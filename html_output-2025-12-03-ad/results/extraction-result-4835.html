<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4835 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4835</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4835</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-0072019b934df4a802c2f948ed090c7745b32dd5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0072019b934df4a802c2f948ed090c7745b32dd5" target="_blank">SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work introduces Spatial Region GPT (SpatialRGPT) to enhance VLMs' spatial perception and reasoning capabilities, and demonstrates that the model significantly enhances performance in spatial reasoning tasks, both with and without local region prompts.</p>
                <p><strong>Paper Abstract:</strong> Vision Language Models (VLMs) have demonstrated remarkable performance in 2D vision and language tasks. However, their ability to reason about spatial arrangements remains limited. In this work, we introduce Spatial Region GPT (SpatialRGPT) to enhance VLMs' spatial perception and reasoning capabilities. SpatialRGPT advances VLMs' spatial understanding through two key innovations: (1) a data curation pipeline that enables effective learning of regional representation from 3D scene graphs, and (2) a flexible plugin module for integrating depth information into the visual encoder of existing VLMs. During inference, when provided with user-specified region proposals, SpatialRGPT can accurately perceive their relative directions and distances. Additionally, we propose SpatialRGBT-Bench, a benchmark with ground-truth 3D annotations encompassing indoor, outdoor, and simulated environments, for evaluating 3D spatial cognition in VLMs. Our results demonstrate that SpatialRGPT significantly enhances performance in spatial reasoning tasks, both with and without local region prompts. The model also exhibits strong generalization capabilities, effectively reasoning about complex spatial relations and functioning as a region-aware dense reward annotator for robotic tasks. Code, dataset, and benchmark are released at https://www.anjiecheng.me/SpatialRGPT</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4835.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4835.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialRGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spatial Region GPT (SpatialRGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A region-aware vision-language model introduced in this paper that integrates region/mask inputs and optional relative-depth via a plugin module to improve 3D spatial perception and reasoning for spatial VQA and region-level tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SpatialRGPT (region-aware VLM with LLM backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal system combining a CLIP-L visual backbone, a region-feature extractor (pixel-level masks or boxes), linear connectors projecting visual features into LLM token space, and a LLaMA2-7B language model as the decoder; includes an optional depth-plugin (depth encoder + depth-to-language connector) initialized from RGB connector and trained on spatial QA. Trained in stages: connector alignment (CC3M), VL pretraining (MMC4/COYO), depth connector training on Open Spatial Dataset (OSD), and instruction-finetuning including region-level QAs.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial VQA / SpatialRGPT-Bench (spatial reasoning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Visual question-answering tasks that require spatial knowledge (relative relations such as left/right/above/below/behind/front; metric relations like direct/horizontal/vertical distances, object width/height, direction). The SpatialRGPT-Bench contains qualitative and quantitative VQA pairs derived from 3D annotations across indoor, outdoor and simulated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Region-conditioned VLM architecture: user-specified region tokens (<region>, <depth>) supply region RGB and depth embeddings to the LLM; training uses a curated Open Spatial Dataset (OSD) with template-based and LLM-generated complex QAs derived from 3D scene graphs; depth is included via a plugin module allowing optional depth inputs; multi-turn prompting interleaves image/region tokens and text; model performs direct generation (no external symbolic solver).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Extensive quantitative and qualitative evaluation on SpatialRGPT-Bench showing strong success rates on relative relations and low absolute relative error on metric questions; examples of multi-hop reasoning (Figure 5) and region-aware dense reward annotation for robotics (Figure 6); ablations show gains from adding metric width/height data and from the depth plugin, demonstrating reliance on region-level and depth cues rather than purely textual priors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SpatialRGPT-7B (mask+depth variant) qualitative average success rate: 91.78%; on individual qualitative categories success rates up to ~99% for left/right/below/above; metric question example: Direct Distance success 41.2% with absolute relative error 0.33 (meters normalized), Horizontal Distance success 65.6% / error 0.25; Direction accuracy 95.3% / angular error ~15.4° (Table 1). SpatialRGPT substantially outperforms baselines such as GPT-4/GPT-4V on these spatial VQA metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relies on axis-aligned bounding boxes (AABBs) leading to label inaccuracies for rotated objects (paper notes AABB limitation); metric depth and point-cloud reconstruction from single view can be noisy (filtering/denoising steps required); some trade-offs observed (e.g., adding width/height data improved size judgments but slightly degraded some distance estimations).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against blind LLMs (GPT-4), vision LLMs (GPT-4V, LLaVA), and region-aware baselines (RegionVILA, KOSMOS-2); SpatialRGPT outperforms these baselines on SpatialRGPT-Bench qualitative and quantitative metrics (e.g., baseline GPT-4 qualitative avg ~57.8% vs SpatialRGPT ~91.8%; GPT-4 direct distance 21.6% vs SpatialRGPT 41.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4835.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4835.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (blind LLM baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (Generative Pre-trained Transformer 4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model used in this paper as a blind-LM baseline (text-only) to measure how much spatial reasoning can be answered from language alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (blind, text-only baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pretrained transformer LLM (OpenAI GPT-4 family); used here in blind mode with language referral (object class names prepended) and no image input to answer spatial VQA questions.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial VQA (text-only / language-referral baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Qualitative and quantitative spatial question-answer pairs where only the textual question (optionally prefixed with object class labels) is provided; tests whether world knowledge or text cues suffice to answer spatial queries.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Pure language generation leveraging internal world knowledge; language referral prepending object classes to questions to supply context; no visual input or grounded region features.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Baseline success on some quantitative QAs (especially width/height questions) suggests LLM world knowledge can sometimes answer metric-like queries, but qualitative spatial relation performance is limited compared to region-aware VLMs; paper uses GPT-4 to score/evaluate model outputs as well.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported on SpatialRGPT-Bench: qualitative category average ~57.83% success; examples: Left/Right 42.85%, Below/Above 64.16%; quantitative example Direct Distance success 21.6% with abs. relative error 1.29 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Without visual grounding, GPT-4 often fails to correctly resolve visually ambiguous spatial relations; prone to leveraging dataset priors or world knowledge rather than perceptual cues; cannot use region masks or depth inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to vision-enabled models (GPT-4V) and region-aware models (SpatialRGPT), GPT-4 performs substantially worse on spatial VQA tasks that require visual or metric evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4835.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4835.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V (vision-enabled GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language variant of GPT-4 used as a vision-capable baseline (VLM) for spatial VQA in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V (vision-enabled LLM baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-capable variant of GPT-4 that accepts image inputs and performs multimodal reasoning; used here as a VLM baseline and also augmented with Set-of-Marks (SoM) to provide region-referring capability in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial VQA (vision baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same spatial VQA tasks requiring visual perception of relative relations and metric estimates from images/regions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Uses image input plus internal multimodal perception; in region-aware baseline experiments augmented with SoM prompting to refer to sets of regions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performs better than text-only LLMs on some spatial tasks but still substantially below SpatialRGPT; adding SoM helps region references but does not attain SpatialRGPT's metric accuracy or low error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On SpatialRGPT-Bench: qualitative average ~58.14%; quantitative examples: Direct Distance 29.7% / error 0.92, Horizontal Distance 25.4% / error 2.75, Direction 43.9% / angular error 69.9° (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lacks explicit region-level pixel-aligned region embeddings and an explicit depth plugin; when compared to SpatialRGPT, it suffers from larger absolute relative errors and lower success rates for metric queries.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Outperformed by SpatialRGPT across most spatial VQA categories (SpatialRGPT qualitative avg ~91.78% vs GPT-4V ~58.14%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4835.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4835.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-70B (QA generation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model used in this work to generate complex spatial reasoning question-answer pairs from scene graph descriptions as part of the dataset creation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-70B (data-generation LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive language model (70B parameters) used offline to convert scene-graph-derived spatial descriptions into diverse, complex QA pairs; not used at inference to answer visual questions.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Dataset generation for Spatial VQA (not a puzzle solver)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Generates complex, multi-step spatial QA examples from textual scene descriptions derived from 3D scene graphs to augment template-based QA data and improve training diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Scene graph -> template-based spatial description -> LLM prompt to produce complex QAs (question + answer) that test multi-hop and advanced spatial reasoning; used to increase QA diversity for training SpatialRGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Used to create harder, multi-hop spatial questions for training; paper demonstrates that mixing template-based and LLM-generated QAs improves model generalization and complex reasoning capability (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes that LLMs can struggle to use raw 3D coordinates effectively, so the pipeline first converts scene graphs into natural-language spatial descriptions before prompting Llama3; generation quality depends on prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not evaluated as a solver; used solely for dataset/question synthesis. No solver performance metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4835.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4835.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-7B (LLM backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2-7B (language model backbone used in SpatialRGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The language model backbone used inside SpatialRGPT for text generation and multi-turn conversation handling, interfacing with vision and region tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-7B (embedded LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 7-billion parameter transformer LLM used as the decoder in SpatialRGPT, receiving interleaved image/region/depth embeddings mapped into token space via connectors for autoregressive generation.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial VQA (internal solver within SpatialRGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Performs spatial question answering and multi-hop reasoning when conditioned on visual and region embeddings; handles region token substitution and auto-regressive answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Receives image-level, region-level RGB and optional depth embeddings (substituted into special tokens) interleaved with text; reasoning occurs via the LLM's autoregressive generation grounded by visual token embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>SpatialRGPT's high performance on SpatialRGPT-Bench and its ability to perform multi-hop reasoning and region-aware reward annotation are the primary evidence that the LLaMA2-7B backbone, when grounded by region and depth embeddings, exhibits spatial reasoning behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>As part of SpatialRGPT-7B: qualitative average ~91.78% on SpatialRGPT-Bench; metric tasks example: Direct Distance 41.2% / error 0.33 (See SpatialRGPT entry).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>On its own, LLaMA2 lacks vision; its spatial capabilities depend on the quality of visual/region/depth embeddings and connector alignment. The paper notes potential failure when depth inputs are absent or noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared implicitly against other VLM backbones (e.g., VILA-based variants) where SpatialRGPT with depth plugin outperforms base VILA pretraining in region/spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4835.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4835.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialVLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpatialVLM: Endowing vision-language models with spatial reasoning capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related prior work that aims to improve VLM spatial reasoning using spatially-aware VQA data; discussed as a comparison point and motivation for region-aware and depth-integrated approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SpatialVLM (related prior VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A VLM that augments training with spatial VQA data (human-labeled spatial annotations) to improve spatial reasoning; relies on language descriptions for object specification and does not support precise region prompts in the same manner as SpatialRGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial VQA / spatial reasoning tasks (prior benchmark/work)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Spatial question-answering on 2D images requiring metric and relative spatial understanding; used as prior-art benchmark and motivation for SpatialRGPT's design choices.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Trains VLMs with spatially-aware VQA data and surface segmentation heuristics; uses text descriptions to identify objects rather than pixel-level region tokens and depth plugins.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Prior results (cited) showed some improvements but limitations: reliance on textual descriptors, inability to specify precise regions, and challenges using 3D coordinates in LLM prompts; these issues motivate SpatialRGPT's region and depth approach.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relies on language descriptions of objects which can be ambiguous when many similar objects appear; lacks explicit region prompting; limited use of depth; training may not force models to learn visual spatial cues as effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>SpatialRGPT argues improvements over SpatialVLM by enabling explicit region prompts, 3D scene-graph-based training data, and a depth plugin, resulting in higher spatial-VQA accuracy in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4835.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4835.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RegionVILA / RegionGPT baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RegionVILA (RegionGPT architecture with VILA pretraining) / RegionGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Region-aware VLM baselines that provide region-level features to LLMs; used for direct comparison to show the effect of specialized spatial training and depth plugin in SpatialRGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Regiongpt: Towards region understanding vision language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RegionVILA-7B / RegionGPT-7B (region-aware VLM baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RegionGPT family provides pixel-level region processing (mask inputs) and connectors to map region features into LLM token space; RegionVILA refers to RegionGPT with VILA pretraining. Used as ablation/baseline without SpatialRGPT's specialized spatial dataset or depth plugin.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Region-level spatial VQA (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Region-aware visual question answering tasks where bounding boxes or masks are provided, but without the specialized metric/depth training pipeline of SpatialRGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Region-pooling/mask pooling to extract region embeddings fed into an LLM via connectors; does not include the relative-depth plugin or OSD training used by SpatialRGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>RegionVILA provides region-level understanding and performs reasonably on region classification and some spatial tasks, but the paper's experiments show lower spatial-VQA accuracy and higher errors on metric queries compared to SpatialRGPT (Table 1 & Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>RegionVILA-7B qualitative average ~40.48% on SpatialRGPT-Bench (Table 1); RegionGPT-7B reported region classification mAP 70.0 and accuracy 80.6 on COCO region classification (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Without specialized spatial QA training and depth integration, region-aware baselines suffer on metric reasoning and complex multi-hop spatial questions; background in bounding boxes can dilute region signals.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>SpatialRGPT (with OSD and depth plugin) outperforms RegionVILA in spatial VQA success rates and metric accuracy; RegionGPT shows good region classification but inferior spatial reasoning on the SpatialRGPT-Bench.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities <em>(Rating: 2)</em></li>
                <li>Regiongpt: Towards region understanding vision language model <em>(Rating: 2)</em></li>
                <li>Blink: Multimodal large language models can see but not perceive <em>(Rating: 1)</em></li>
                <li>Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4835",
    "paper_id": "paper-0072019b934df4a802c2f948ed090c7745b32dd5",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "SpatialRGPT",
            "name_full": "Spatial Region GPT (SpatialRGPT)",
            "brief_description": "A region-aware vision-language model introduced in this paper that integrates region/mask inputs and optional relative-depth via a plugin module to improve 3D spatial perception and reasoning for spatial VQA and region-level tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SpatialRGPT (region-aware VLM with LLM backbone)",
            "model_description": "A multimodal system combining a CLIP-L visual backbone, a region-feature extractor (pixel-level masks or boxes), linear connectors projecting visual features into LLM token space, and a LLaMA2-7B language model as the decoder; includes an optional depth-plugin (depth encoder + depth-to-language connector) initialized from RGB connector and trained on spatial QA. Trained in stages: connector alignment (CC3M), VL pretraining (MMC4/COYO), depth connector training on Open Spatial Dataset (OSD), and instruction-finetuning including region-level QAs.",
            "puzzle_name": "Spatial VQA / SpatialRGPT-Bench (spatial reasoning benchmark)",
            "puzzle_description": "Visual question-answering tasks that require spatial knowledge (relative relations such as left/right/above/below/behind/front; metric relations like direct/horizontal/vertical distances, object width/height, direction). The SpatialRGPT-Bench contains qualitative and quantitative VQA pairs derived from 3D annotations across indoor, outdoor and simulated datasets.",
            "mechanism_or_strategy": "Region-conditioned VLM architecture: user-specified region tokens (&lt;region&gt;, &lt;depth&gt;) supply region RGB and depth embeddings to the LLM; training uses a curated Open Spatial Dataset (OSD) with template-based and LLM-generated complex QAs derived from 3D scene graphs; depth is included via a plugin module allowing optional depth inputs; multi-turn prompting interleaves image/region tokens and text; model performs direct generation (no external symbolic solver).",
            "evidence_of_spatial_reasoning": "Extensive quantitative and qualitative evaluation on SpatialRGPT-Bench showing strong success rates on relative relations and low absolute relative error on metric questions; examples of multi-hop reasoning (Figure 5) and region-aware dense reward annotation for robotics (Figure 6); ablations show gains from adding metric width/height data and from the depth plugin, demonstrating reliance on region-level and depth cues rather than purely textual priors.",
            "performance_metrics": "SpatialRGPT-7B (mask+depth variant) qualitative average success rate: 91.78%; on individual qualitative categories success rates up to ~99% for left/right/below/above; metric question example: Direct Distance success 41.2% with absolute relative error 0.33 (meters normalized), Horizontal Distance success 65.6% / error 0.25; Direction accuracy 95.3% / angular error ~15.4° (Table 1). SpatialRGPT substantially outperforms baselines such as GPT-4/GPT-4V on these spatial VQA metrics.",
            "limitations_or_failure_cases": "Relies on axis-aligned bounding boxes (AABBs) leading to label inaccuracies for rotated objects (paper notes AABB limitation); metric depth and point-cloud reconstruction from single view can be noisy (filtering/denoising steps required); some trade-offs observed (e.g., adding width/height data improved size judgments but slightly degraded some distance estimations).",
            "comparison_baseline": "Compared against blind LLMs (GPT-4), vision LLMs (GPT-4V, LLaVA), and region-aware baselines (RegionVILA, KOSMOS-2); SpatialRGPT outperforms these baselines on SpatialRGPT-Bench qualitative and quantitative metrics (e.g., baseline GPT-4 qualitative avg ~57.8% vs SpatialRGPT ~91.8%; GPT-4 direct distance 21.6% vs SpatialRGPT 41.2%).",
            "uuid": "e4835.0",
            "source_info": {
                "paper_title": "SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4 (blind LLM baseline)",
            "name_full": "GPT-4 (Generative Pre-trained Transformer 4)",
            "brief_description": "A state-of-the-art large language model used in this paper as a blind-LM baseline (text-only) to measure how much spatial reasoning can be answered from language alone.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4 (blind, text-only baseline)",
            "model_description": "Large pretrained transformer LLM (OpenAI GPT-4 family); used here in blind mode with language referral (object class names prepended) and no image input to answer spatial VQA questions.",
            "puzzle_name": "Spatial VQA (text-only / language-referral baseline)",
            "puzzle_description": "Qualitative and quantitative spatial question-answer pairs where only the textual question (optionally prefixed with object class labels) is provided; tests whether world knowledge or text cues suffice to answer spatial queries.",
            "mechanism_or_strategy": "Pure language generation leveraging internal world knowledge; language referral prepending object classes to questions to supply context; no visual input or grounded region features.",
            "evidence_of_spatial_reasoning": "Baseline success on some quantitative QAs (especially width/height questions) suggests LLM world knowledge can sometimes answer metric-like queries, but qualitative spatial relation performance is limited compared to region-aware VLMs; paper uses GPT-4 to score/evaluate model outputs as well.",
            "performance_metrics": "Reported on SpatialRGPT-Bench: qualitative category average ~57.83% success; examples: Left/Right 42.85%, Below/Above 64.16%; quantitative example Direct Distance success 21.6% with abs. relative error 1.29 (Table 1).",
            "limitations_or_failure_cases": "Without visual grounding, GPT-4 often fails to correctly resolve visually ambiguous spatial relations; prone to leveraging dataset priors or world knowledge rather than perceptual cues; cannot use region masks or depth inputs.",
            "comparison_baseline": "Compared to vision-enabled models (GPT-4V) and region-aware models (SpatialRGPT), GPT-4 performs substantially worse on spatial VQA tasks that require visual or metric evidence.",
            "uuid": "e4835.1",
            "source_info": {
                "paper_title": "SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4V (vision-enabled GPT-4)",
            "brief_description": "A vision-language variant of GPT-4 used as a vision-capable baseline (VLM) for spatial VQA in this paper.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4V (vision-enabled LLM baseline)",
            "model_description": "Vision-capable variant of GPT-4 that accepts image inputs and performs multimodal reasoning; used here as a VLM baseline and also augmented with Set-of-Marks (SoM) to provide region-referring capability in comparisons.",
            "puzzle_name": "Spatial VQA (vision baseline)",
            "puzzle_description": "Same spatial VQA tasks requiring visual perception of relative relations and metric estimates from images/regions.",
            "mechanism_or_strategy": "Uses image input plus internal multimodal perception; in region-aware baseline experiments augmented with SoM prompting to refer to sets of regions.",
            "evidence_of_spatial_reasoning": "Performs better than text-only LLMs on some spatial tasks but still substantially below SpatialRGPT; adding SoM helps region references but does not attain SpatialRGPT's metric accuracy or low error rates.",
            "performance_metrics": "On SpatialRGPT-Bench: qualitative average ~58.14%; quantitative examples: Direct Distance 29.7% / error 0.92, Horizontal Distance 25.4% / error 2.75, Direction 43.9% / angular error 69.9° (Table 1).",
            "limitations_or_failure_cases": "Lacks explicit region-level pixel-aligned region embeddings and an explicit depth plugin; when compared to SpatialRGPT, it suffers from larger absolute relative errors and lower success rates for metric queries.",
            "comparison_baseline": "Outperformed by SpatialRGPT across most spatial VQA categories (SpatialRGPT qualitative avg ~91.78% vs GPT-4V ~58.14%).",
            "uuid": "e4835.2",
            "source_info": {
                "paper_title": "SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama3-70B (QA generation)",
            "name_full": "Llama3-70B",
            "brief_description": "A large language model used in this work to generate complex spatial reasoning question-answer pairs from scene graph descriptions as part of the dataset creation pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3-70B (data-generation LLM)",
            "model_description": "Large autoregressive language model (70B parameters) used offline to convert scene-graph-derived spatial descriptions into diverse, complex QA pairs; not used at inference to answer visual questions.",
            "puzzle_name": "Dataset generation for Spatial VQA (not a puzzle solver)",
            "puzzle_description": "Generates complex, multi-step spatial QA examples from textual scene descriptions derived from 3D scene graphs to augment template-based QA data and improve training diversity.",
            "mechanism_or_strategy": "Scene graph -&gt; template-based spatial description -&gt; LLM prompt to produce complex QAs (question + answer) that test multi-hop and advanced spatial reasoning; used to increase QA diversity for training SpatialRGPT.",
            "evidence_of_spatial_reasoning": "Used to create harder, multi-hop spatial questions for training; paper demonstrates that mixing template-based and LLM-generated QAs improves model generalization and complex reasoning capability (Figure 4).",
            "performance_metrics": null,
            "limitations_or_failure_cases": "Paper notes that LLMs can struggle to use raw 3D coordinates effectively, so the pipeline first converts scene graphs into natural-language spatial descriptions before prompting Llama3; generation quality depends on prompt design.",
            "comparison_baseline": "Not evaluated as a solver; used solely for dataset/question synthesis. No solver performance metrics reported.",
            "uuid": "e4835.3",
            "source_info": {
                "paper_title": "SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaMA2-7B (LLM backbone)",
            "name_full": "LLaMA2-7B (language model backbone used in SpatialRGPT)",
            "brief_description": "The language model backbone used inside SpatialRGPT for text generation and multi-turn conversation handling, interfacing with vision and region tokens.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2-7B (embedded LLM)",
            "model_description": "A 7-billion parameter transformer LLM used as the decoder in SpatialRGPT, receiving interleaved image/region/depth embeddings mapped into token space via connectors for autoregressive generation.",
            "puzzle_name": "Spatial VQA (internal solver within SpatialRGPT)",
            "puzzle_description": "Performs spatial question answering and multi-hop reasoning when conditioned on visual and region embeddings; handles region token substitution and auto-regressive answer generation.",
            "mechanism_or_strategy": "Receives image-level, region-level RGB and optional depth embeddings (substituted into special tokens) interleaved with text; reasoning occurs via the LLM's autoregressive generation grounded by visual token embeddings.",
            "evidence_of_spatial_reasoning": "SpatialRGPT's high performance on SpatialRGPT-Bench and its ability to perform multi-hop reasoning and region-aware reward annotation are the primary evidence that the LLaMA2-7B backbone, when grounded by region and depth embeddings, exhibits spatial reasoning behavior.",
            "performance_metrics": "As part of SpatialRGPT-7B: qualitative average ~91.78% on SpatialRGPT-Bench; metric tasks example: Direct Distance 41.2% / error 0.33 (See SpatialRGPT entry).",
            "limitations_or_failure_cases": "On its own, LLaMA2 lacks vision; its spatial capabilities depend on the quality of visual/region/depth embeddings and connector alignment. The paper notes potential failure when depth inputs are absent or noisy.",
            "comparison_baseline": "Compared implicitly against other VLM backbones (e.g., VILA-based variants) where SpatialRGPT with depth plugin outperforms base VILA pretraining in region/spatial tasks.",
            "uuid": "e4835.4",
            "source_info": {
                "paper_title": "SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SpatialVLM",
            "name_full": "SpatialVLM: Endowing vision-language models with spatial reasoning capabilities",
            "brief_description": "A related prior work that aims to improve VLM spatial reasoning using spatially-aware VQA data; discussed as a comparison point and motivation for region-aware and depth-integrated approaches.",
            "citation_title": "Spatialvlm: Endowing vision-language models with spatial reasoning capabilities",
            "mention_or_use": "mention",
            "model_name": "SpatialVLM (related prior VLM)",
            "model_description": "A VLM that augments training with spatial VQA data (human-labeled spatial annotations) to improve spatial reasoning; relies on language descriptions for object specification and does not support precise region prompts in the same manner as SpatialRGPT.",
            "puzzle_name": "Spatial VQA / spatial reasoning tasks (prior benchmark/work)",
            "puzzle_description": "Spatial question-answering on 2D images requiring metric and relative spatial understanding; used as prior-art benchmark and motivation for SpatialRGPT's design choices.",
            "mechanism_or_strategy": "Trains VLMs with spatially-aware VQA data and surface segmentation heuristics; uses text descriptions to identify objects rather than pixel-level region tokens and depth plugins.",
            "evidence_of_spatial_reasoning": "Prior results (cited) showed some improvements but limitations: reliance on textual descriptors, inability to specify precise regions, and challenges using 3D coordinates in LLM prompts; these issues motivate SpatialRGPT's region and depth approach.",
            "performance_metrics": null,
            "limitations_or_failure_cases": "Relies on language descriptions of objects which can be ambiguous when many similar objects appear; lacks explicit region prompting; limited use of depth; training may not force models to learn visual spatial cues as effectively.",
            "comparison_baseline": "SpatialRGPT argues improvements over SpatialVLM by enabling explicit region prompts, 3D scene-graph-based training data, and a depth plugin, resulting in higher spatial-VQA accuracy in experiments.",
            "uuid": "e4835.5",
            "source_info": {
                "paper_title": "SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RegionVILA / RegionGPT baselines",
            "name_full": "RegionVILA (RegionGPT architecture with VILA pretraining) / RegionGPT",
            "brief_description": "Region-aware VLM baselines that provide region-level features to LLMs; used for direct comparison to show the effect of specialized spatial training and depth plugin in SpatialRGPT.",
            "citation_title": "Regiongpt: Towards region understanding vision language model",
            "mention_or_use": "use",
            "model_name": "RegionVILA-7B / RegionGPT-7B (region-aware VLM baselines)",
            "model_description": "RegionGPT family provides pixel-level region processing (mask inputs) and connectors to map region features into LLM token space; RegionVILA refers to RegionGPT with VILA pretraining. Used as ablation/baseline without SpatialRGPT's specialized spatial dataset or depth plugin.",
            "puzzle_name": "Region-level spatial VQA (baseline)",
            "puzzle_description": "Region-aware visual question answering tasks where bounding boxes or masks are provided, but without the specialized metric/depth training pipeline of SpatialRGPT.",
            "mechanism_or_strategy": "Region-pooling/mask pooling to extract region embeddings fed into an LLM via connectors; does not include the relative-depth plugin or OSD training used by SpatialRGPT.",
            "evidence_of_spatial_reasoning": "RegionVILA provides region-level understanding and performs reasonably on region classification and some spatial tasks, but the paper's experiments show lower spatial-VQA accuracy and higher errors on metric queries compared to SpatialRGPT (Table 1 & Table 3).",
            "performance_metrics": "RegionVILA-7B qualitative average ~40.48% on SpatialRGPT-Bench (Table 1); RegionGPT-7B reported region classification mAP 70.0 and accuracy 80.6 on COCO region classification (Table 3).",
            "limitations_or_failure_cases": "Without specialized spatial QA training and depth integration, region-aware baselines suffer on metric reasoning and complex multi-hop spatial questions; background in bounding boxes can dilute region signals.",
            "comparison_baseline": "SpatialRGPT (with OSD and depth plugin) outperforms RegionVILA in spatial VQA success rates and metric accuracy; RegionGPT shows good region classification but inferior spatial reasoning on the SpatialRGPT-Bench.",
            "uuid": "e4835.6",
            "source_info": {
                "paper_title": "SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Spatialvlm: Endowing vision-language models with spatial reasoning capabilities",
            "rating": 2,
            "sanitized_title": "spatialvlm_endowing_visionlanguage_models_with_spatial_reasoning_capabilities"
        },
        {
            "paper_title": "Regiongpt: Towards region understanding vision language model",
            "rating": 2,
            "sanitized_title": "regiongpt_towards_region_understanding_vision_language_model"
        },
        {
            "paper_title": "Blink: Multimodal large language models can see but not perceive",
            "rating": 1,
            "sanitized_title": "blink_multimodal_large_language_models_can_see_but_not_perceive"
        },
        {
            "paper_title": "Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning",
            "rating": 1,
            "sanitized_title": "conceptgraphs_openvocabulary_3d_scene_graphs_for_perception_and_planning"
        }
    ],
    "cost": 0.017188,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SpatialRGPT: Grounded Spatial Reasoning in Vision-Language Models</h1>
<p>An-Chieh Cheng ${ }^{1}$, Hongxu Yin ${ }^{2}$, Yang Fu ${ }^{1}$, Qiushan Guo ${ }^{2}$, Ruihan Yang ${ }^{1}$, Jan Kautz ${ }^{2}$, Xiaolong Wang ${ }^{1,2}$, Sifei Liu ${ }^{2}$<br>${ }^{1}$ UC San Diego, ${ }^{2}$ NVIDIA</p>
<h2>A Relative Relation</h2>
<p>(2) Can you confirm if (3) has a greater width compared to (4)? In fact, (5) might be narrower than (6).
(2) Which of these four, (6), (7), (8), (9) is the tallest?
(2) (3) is the tallest.
(2) From the viewer's perspective, is the position of (3) closer than that of (4)?
(3) No, (4) is closer.</p>
<h2>B Metric Measurement</h2>
<p>(2) If you are at (5), where will you find??
(2) is at 1 o'clock from (4)
(3) What is the height of (4)?
(4) is 1.38 meters tall.
(2) How far apart are (5) and (6) horizontally?
(3) They are 15.33 meters apart horizontally.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<h2>C Spatial Reasoning</h2>
<p>(2) Can you try to estimate how many floors are in (3)? Think step by step. The height of (4) is 204.54 feet. Assuming each floor is about 10 feet high, the total number of floors would be 20.454 . Since you can't have a fraction of a floor, the total number of floors would be approximately 20 .
(3) I'm planning to build a bridge from (4) to (5), is it possible?
(4) The distance between (5) and (6) is 14.82 meters, so it is possible to build a bridge between them.
(2) If you are riding a motorcycle with 36 inches wide, do you think you can pass through the area between (5) and (6)?
(3) The distance between (5) and (6) is 38.95 inches, so yes, you can pass through the area between (5) and (6) since the motorcycle is narrower than the distance between them.</p>
<h2>Abstract</h2>
<p>Vision Language Models (VLMs) have demonstrated remarkable performance in 2D vision and language tasks. However, their ability to reason about spatial arrangements remains limited. In this work, we introduce Spatial Region GPT (SpatialRGPT) to enhance VLMs' spatial perception and reasoning capabilities. SpatialRGPT advances VLMs' spatial understanding through two key innovations: (i) a data curation pipeline that enables effective learning of regional representation from 3D scene graphs, and (ii) a flexible "plugin" module for integrating depth information into the visual encoder of existing VLMs. During inference, when provided with user-specified region proposals, SpatialRGPT can accurately perceive their relative directions and distances. Additionally, we propose SpatialRGBT-Bench, a benchmark with ground-truth 3D annotations encompassing indoor, outdoor, and simulated environments, for evaluating 3D spatial cognition in VLMs. Our results demonstrate that SpatialRGPT significantly enhances performance in spatial reasoning tasks, both with and without local region prompts. The model also exhibits strong generalization capabilities, effectively reasoning about complex spatial relations and functioning as a region-aware dense reward annotator for robotic tasks. Code, dataset, and benchmark are released at https://www.anjiecheng.me/SpatialRGPT.</p>
<p>1 Introduction</p>
<p>Understanding spatial arrangements in both 2D [1, 2] and 3D [3] spaces is crucial for accurately interpreting complex visual environments. Despite the impressive advancements in Vision Language Models (VLMs) across a variety of tasks such as image classification [4], captioning [5], object detection [6], video understanding [7], and document parsing [8], etc., these models still face significant challenges with spatial reasoning. This includes difficulties [9, 10, 11] in distinguishing simple spatial concepts like "left" and "right," "above" and "below," as well as more complex relationships such as "behind" and "in front," "inside" and "outside," and "near" and "far." The ability to comprehend and reason about these spatial relationships is fundamental not only for visual understanding, but also for enabling practical applications in fields like robotics [12, 13] and augmented reality [14], where precise spatial awareness is crucial for tasks such as navigation [15], manipulation [12], and interaction with real-world environments [16].</p>
<p>Recently, several works [11, 17, 18] has advanced VLMs’ spatial reasoning capabilities by introducing a comprehensive data generation pipeline that enables large-scale training with spatially-aware visual question answering (VQA) tasks. This approach is based on the hypothesis that the limited spatial reasoning capabilities of current VLMs are due to a lack of 3D/2D spatial knowledge in their training data. However, two critical challenges remain. First, effective spatial reasoning requires VLMs to accurately parse regional information, particularly the regions of object instances, whereas most existing VLMs are primarily designed to understand the global context of an image. When an image contains numerous instances, it becomes challenging to prompt the model to reason about the spatial relations between specific instances. This is because most VLMs function as global image parsers and do not support specifying regions for which users want to understand spatial relationships. Second, accurately perceiving spatial relations such as direction and distance cannot rely solely on RGB pixel data. Thus, the architecture needs to incorporate 3D inputs, such as depth information.</p>
<p>In this work, we propose SpatialRGPT, leveraging a data curation pipeline, along with a region and 3D-aware visual encoder architecture to improve the spatial reasoning capability of VLMs.</p>
<p>Our data pipeline automatically generates 3D, region-aware annotations from 2D images at scale by constructing a 3D scene graph for each image, where nodes represent object instances and edges denote spatial relationships. This is achieved through three scalable components: (i) open-vocabulary detection and segmentation for instance extraction, (ii) metric depth estimation, and (iii) camera calibration for projecting objects into 3D space. These scene graphs are subsequently transformed into region-aware spatial QA tasks using both template-based and large language model (LLM)-based approaches. This dual approach provides region-based VLMs with the necessary spatial knowledge and advanced reasoning capabilities to interpret complex environments. We use the collected data to train SpatialRGPT. While SpatialRGPT is designed to support region prompts, it effectively avoids the ambiguity issues found in SpatialVLM. In SpatialVLM, multiple similar objects in an image can confuse caption labels. In contrast, our pipeline naturally handles these scenarios without requiring carefully crafted rules or extensive post-processing.</p>
<p>Similar to RGPT [19], SpatialRGPT introduces a region representation module that allows region proposals to be included as additional inputs alongside the image. This approach enables the LLM to leverage both regional and global contexts, allowing the model to reason about relationships between local regions while maintaining an understanding of the overall scene. In addition, we propose a novel architecture that features a flexible “plugin” module for integrating relative-depth information into the visual encoder of existing VLMs. This design allows a pre-trained visual encoder to optionally learn additional depth representation while still functioning effectively when depth inputs are absent. Our experiments demonstrate that this design can substantially improve the spatial reasoning capabilities compared to VLMs that only use RGB images as input. Furthermore, we highlight practical applications enabled by SpatialRGPT, such as serving as a region-aware dense reward annotator and a stand-alone complex spatial reasoner. Our work has four main contributions:</p>
<ol>
<li>We present SpatialRGPT, a framework that enhances region-level spatial reasoning in VLMs by enabling effective representation of regional information and acquisition of spatial knowledge. Our novel architecture also integrates depth information flexibly, significantly improving 3D perception and analysis.</li>
<li>
<p>To facilitate model training, we introduce a scalable data pipeline that constructs regionaware spatial reasoning QAs from existing datasets. With the pipeline, we create the Open Spatial Dataset (OSD), encompassing 8.7M spatial concepts grounded in 5M unique regions.</p>
</li>
<li>
<p>To address the absence of a benchmark for evaluating spatial cognition in VLMs, we present SpatialRGPT-Bench, a comprehensive benchmark based on ground-truth 3D annotations that span indoor, outdoor, and simulated environments.</p>
</li>
<li>We demonstrate downstream applications of SpatialRGPT. Leveraging SpatialRGPT's region capabilities, we develop a region-aware dense reward annotator for robotics. Additionally, we show that SpatialRGPT can function as a stand-alone complex spatial reasoner, as well as its capacity to perform multi-hop reasoning.</li>
</ol>
<h1>2 Related work</h1>
<p>Spatial Reasoning via Large Language Models. Recently, there has been a significant push to obtain spatial reasoning capabilities using LLMs. Initiatives [20, 21] have focused on reconstructing scenes from multi-view images, such as point clouds or neural fields, and enhancing these representations with dense semantic features. The resulting 3D representation and dense features are then integrated into an LLM. However, multi-view images are not always available, and constructing a scene explicitly with dense semantic features is resource-intensive. Additionally, the modal gap between 3D representations and language often results in decreased performance. ConceptGraph [22] avoids directly incorporating 3D representations into LLMs. Instead, it constructs a scene graph and integrates this with the LLM. Yet, recent studies [10] indicate that LLMs struggle to utilize coordinate information effectively when presented in text, which can undermine their ability to understand and reason about spatial relationships. Our research is most aligned with SpatialVLM [17], which uses 2D VLMs to understand spatial relationships and metric distances. Unlike the above approaches, the spatial understanding is encoded implicitly. The VLM directly handles the spatial relationship problem without an explicit 3D representation or scene graph. However, SpatialVLM relies on language descriptions of objects as input, while LLMs can already resolve some spatial queries even without visual data [23]. The responses can be inferred directly from the questions or derived from the world knowledge embedded in LLMs. This reliance on textual cues suggests that the training may not effectively teach VLMs to learn spatial reasoning from visual data. Additionally, SpatialVLM lacks the capability to specify regions precisely. This is particularly problematic in real-world scenarios where describing ambiguous locations or objects in language can be challenging.
Region-level Visual Language Models. KOSMOS-2 [24], Shikra [25], MiniGPT-2 [26], CogVLM [27], SPHINX [28], and LLaVA [29] have enabled MLLMs to achieve region-based image understanding. However, these methods provide region information in textual form, such as bounding box coordinates. This method heavily depends on the language decoder to understand the position. In contrast, VisionLLM [30], GPT4RoI [31], [32], and Ferret [33, 34], along with GLaMM [35], use spatial boxes with ROI-aligned features to map region-level features into the LLM word embedding space. However, bounding boxes can include unwanted background features, leading to inaccurate alignment between region descriptions and text, which complicates spatial reasoning. Recently, RegionGPT [19] and Osprey [36] have introduced visual spatial-aware modules that can directly extract pixel-level features. These models support using input masks that can accommodate regions of any shape. Despite these advancements, none of these approaches specifically focus on enhancing spatial reasoning at the region level in VLMs. Our framework is based on RegionGPT's ability to process pixel-level inputs, with the aim of deepening spatial reasoning within region VLMs.</p>
<h2>3 Method</h2>
<p>SpatialRGPT is a powerful multimodal language model adept at understanding both 2D and 3D spatial arrangements. It can process any region proposal, such as boxes or masks, and provide answers to spatial reasoning questions. While effective training dataset is the key to learn spatial-aware region representation, we introduce: (i) how to build 3D scene Graph from a single image, in Sec. 3.1, and (ii) how to facilitate visual representation learning from these scene graphs in Sec. 3.2. We propose a novel SpatialRGPT visual encoder architecture that flexibly leveraging monocular depth information into an existing 2D VLM, in Sec. 3.3, with training detail explained in Sec. 3.1.</p>
<h3>3.1 3D Scene Graph from Single 2D Images</h3>
<p>Our scene graph construction pipeline (Figure1) begins with a filtering process to remove any unsuitable images (Appx.F.1). Using open-vocabulary models, we identify and ground candidate objects, followed by lifting them into 3D space using metric depth estimation and camera calibration. We then process the point clouds (Appx. F.3) to construct the final 3D scene graph.
Open-Vocabulary Detection \&amp; Segmentation. Segmenting objects is the initial stage of building a scene graph. Our models must satisfy two criteria: (i) object descriptions, e.g., class labels, should</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 1: 3D scene graph construction via automatic data curation pipeline.</p>
<p>adhere to an open-world setting for better generalization; (ii) mask proposals need to be highly accurate, ensuring precise contour outlines. This precision is crucial, as even small deviations can lead to significant inaccuracies in the resulting 3D bounding boxes. To this end, we first employ an open-vocabulary image tagging model [37] to identify all the object classes present in the image. Next, we use GroundingDino [38], an open-vocabulary 2D detector to determine the corresponding object bounding boxes. Finally, we apply segmentation models [39] to refine these bounding boxes into precise masks. We do not use existing dataset annotations since they either fall short due to vocabulary limitations, or use polygon annotations [40] or compressed masks [41] for segmentation.</p>
<p><strong>Metric Depth Estimation.</strong> Several studies have explored the recovery of metric depth from a single image. The main challenge is to address the scale ambiguity, and one common approach [42, 43] is to use relative depth along with metric heads fine-tuned on specific metric datasets. However, these methods may tend to overfit the depth scale for particular datasets such as KITTI [44] or NYU [45], which makes them less robust for in-the-wild images. Recently, Metric3Dv2 [46] takes focal length as input and is trained end-to-end to predict metric depth and surface normals. The model is trained jointly on diverse indoor and outdoor scenes, making it less prone to overfitting to the depth distribution of specific datasets. We adopt Metric3Dv2 as our metric depth estimator and found that Metric3Dv2 together with WildCamera [47]'s camera intrinsic, is robust for images taken in real-world settings. Additionally, thanks to the joint depth-normal optimization training in Metric3Dv2, the recovered geometry is improved particularly around object edges.</p>
<p><strong>Camera Calibration.</strong> Camera calibration includes (i) intrinsic estimation to back-project depth maps to 3D point clouds, and (ii) scene canonicalization to ensure that scene relations are described in a shared space. To estimate the camera intrinsic, we use the WildCamera model [47], which estimates four DoF intrinsic parameters (focal point and focal length in two dimensions). This model excels in real-world scenarios due to its scale-awareness and ability to detect image cropping. To convert the camera coordinates of the point cloud into a canonicalized geodetic coordinate system for each scene, we leverage PerspectiveFields [48], which provides per-pixel up-vectors and latitude values that can be transformed into camera extrinsics, such as pitch and roll. Using these, we derive a rotation matrix to convert the point cloud from camera coordinates to geodetic coordinates. We note that while SpatialVLM [17] uses surface segmentation (e.g., "floor," "tabletop") to identify a horizontal plane and then uses the normal axis of this plane to align the point cloud to the horizontal plane, this approach is limited by the presence of specific classes, such as floors or tables. Additionally, the plane segmentation may fail if there are not enough points for RANSAC.</p>
<p><strong>Constructing 3D Scene Graph.</strong> The 3D scene graph is a collection of tuples where the nodes represent specific 3D object instances, and the edges represent the spatial relationships between the nodes. Each node is defined by the object's class, width, and height in metric scale. To create the node, we start by using the instance mask to deproject the object points from the depth map. Then, we perform canonicalization and denoising, and build 3D axis-aligned bounding boxes for each object. With the 3D bounding box, we calculate the width and height of the objects in</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: Example data entries from our Open Spatial Dataset. The first row contains template-based QAs, and the second row shows LLM-based entries.
real-world units. The edges represent the spatial relationships between the nodes within two types of relations: relative and metric. Relative relations contain left, right, above, below, behind, front, wide, thin, tall, short, big, and small. Metric relations include direction, direct distance, horizontal distance, and vertical distance between the two objects. We then traverse all the object nodes and use the point cloud centroids and bounding boxes to calculate their spatial relationships.</p>
<h1>3.2 Learning Spatial-aware VLMs from 3D Scene Graph</h1>
<p>In this section, we discuss converting the constructed 3D scene graph into textual representations for VLM training. One simple approach is through template-based methods via predefined handcrafted instructions. However, this approach limits the diversity of instructions and hinder the model's reasoning capabilities. Thus, we employ additional complex QAs to enhance the model's reasoning ability. Our results in Figure 4 show that blending these two types of data can lead to a generalized and complex spatial reasoning model.
Template-based Question Answering. These QAs serve as the foundation for learning basic spatial knowledge. We extract information about node attributes such as width and height, as well as relative and metric relations from the edge attributes. We create both qualitative and quantitative templates to generate questions and answers for each type of attribute, using entities in the form of Region [X]. This approach results in examples shown in the first row of Figure 2. We provide detailed templates for each attribute in Appx. F.4.
LLM-based Complex Reasoning Question Answering. We employ Llama3-70B to generate complex spatial reasoning questions to enhance the model's spatial reasoning capabilities. One approach is to input the scene graph directly into the LLMs. However, LLMs struggle to utilize 3D coordinate information effectively [10], so we opt for an alternative approach. We first construct spatial descriptions in a language format. Similar to the template-based approach, we extract attributes from the scene graph and then construct template-based spatial descriptions based on these attributes. We combine the spatial descriptions and the region tags as inputs to the LLM. The LLM is then tasked with creating a complex reasoning question and answer that is based on the description and matches the context. Examples of LLM-generated QAs are shown in the second row of Figure 2. Our LLM prompts for generating QAs are provided in Appx. F.5.
We use our automated annotation pipeline to annotate images from the OpenImages [49] dataset, which covers a wide range of subjects and is of high resolution. The resulting Open Spatial Dataset (OSD) contains 1M unique images and 5M open-vocabulary regions, each associated with a bounding box and segmentation mask. Furthermore, the dataset includes 8M template-based QAs and 700K LLM-based QAs.</p>
<h3>3.3 VLM Architecture</h3>
<p>An overview of SpatialRGPT's VLM architecture is shown in Figure 3. SpatialRGPT consists of a visual encoder (Appx. G.1) to encode vision features, a region-feature extractor [19] to obtain regionlevel embeddings (Appx. G.2), linear connectors (Appx. G.3) to project multi-modal embeddings into</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: An architecture overview of Spatial RGPT. $\bullet$ denotes freezed/trainable parameters.
the word embedding space, and a large language model using LLaMA2-7B for language processing. In this section, we will explain why and how we incorporate depth information into SpatialRGPT, as well as how SpatialRGPT handles tokenizations.
Plugin Module for Relative-depth Inputs. VLMs that learn solely from RGB pixels are ineffective for 3D perception tasks. Direct learning from 3D data, like point clouds, presents challenges due to issues with scale and diversity. To bridge this gap, we propose using relative depth maps, which can be obtained through off-the-shelf models [43], to provide additional depth information alongside RGB images as input to our network. Our goal is to elicit geometric reasoning capability through depth guidance. However, this goal is non-trivial. Most VLM's visual encoders are typically only trained with text and 2D images, and simply concatenating RGB and depth features may negatively impact performance. To address this, we introduce an add-on module that seamlessly incorporates the depth information. We use the same image encoder to process the depth map and generate depth feature maps. Then, we employ an additional depth-to-language connector to project the features into the language domain. The depth connector's weights are initialized from the RGB connector and trained only on spatial-related QAs. This flexible design allows the 2D visual encoder to leverage additional depth representation while still functioning when depth inputs are not presented, thus avoiding the need for a vast amount of training data.
Tokenization and Prompt Format. We generate multi-turn conversation data following [29, 19] for each image and make the image the initial input for the first instruction, providing contextual information. Specifically, we incorporate a prefix prompt: "<image>\n". The <image> is a special token that acts as a placeholder, which would be replaced by the image-level embedding from the vision encoder. When specific mask regions are mentioned in the user input, we use special tokens <region> and <depth> as placeholders. Each region token will be substituted with the corresponding region RGB embedding and depth embedding. All image-level, regionlevel RGB/depth tokens and text tokens are interleaved and fed as the input to the LLM for an auto-regressive generation.</p>
<h1>3.4 Training and Inference Paradigm</h1>
<p>SpatialRGPT training includes three stages [50]: (i) Connector Feature Alignment, (ii) Visual Language Pre-training, and (iii) Visual Instruction-tuning. During the first stage, CC3M imagecaption pairs are used to pretrain the RGB connector as [29, 51, 52]. In the second stage, the visual language corpus from MMC4 [53] and COYO [54] is used to pretrain the LLM and the RGB connector. The RGB connector and LLM parameters are then frozen, with only the depth connector trainable and pre-trained on our OSD dataset. Finally, at stage three, we fine-tune the pre-trained model on visual language instruction-following datasets, using a combination of the instruction tuning dataset from [29], region-level instruction tuning data [19], and our OSD dataset. Detailed data blend of the visual instruction data is in Appx. H.1. For training region-level data and our OSD, we randomly sample from different modalities (e.g., box, mask) for each sample to ensure the model is versatile to the input modality. At inference time, SpatialRGPT can take both boxes or masks as input. For the results shown in the main paper, if the segmentation is available, we use the mask; if not, we use the box provided and apply SAM to segment the corresponding mask.</p>
<h2>4 Experiments</h2>
<p>We evaluate the effectiveness of our proposed SpatialRGPT in three aspects: (1) spatial reasoning benchmarks (Section 4.1), (2) standard vision-language benchmarks (Section 4.2), and (3) real-world applications (Section 4.3).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Below/ <br> Above</th>
<th style="text-align: center;">Left/ <br> Right</th>
<th style="text-align: center;">Big/ <br> Small</th>
<th style="text-align: center;">Tall/ <br> Short</th>
<th style="text-align: center;">Wide/ <br> Thin</th>
<th style="text-align: center;">Behind/ <br> Front</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-4 [55]</td>
<td style="text-align: center;">64.16</td>
<td style="text-align: center;">42.85</td>
<td style="text-align: center;">42.85</td>
<td style="text-align: center;">61.60</td>
<td style="text-align: center;">61.60</td>
<td style="text-align: center;">49.09</td>
<td style="text-align: center;">57.83</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4V [55]</td>
<td style="text-align: center;">63.34</td>
<td style="text-align: center;">46.67</td>
<td style="text-align: center;">64.15</td>
<td style="text-align: center;">60.71</td>
<td style="text-align: center;">68.26</td>
<td style="text-align: center;">45.45</td>
<td style="text-align: center;">58.14</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-v1.6-34B [56]</td>
<td style="text-align: center;">44.16</td>
<td style="text-align: center;">45.71</td>
<td style="text-align: center;">36.79</td>
<td style="text-align: center;">53.57</td>
<td style="text-align: center;">37.50</td>
<td style="text-align: center;">45.45</td>
<td style="text-align: center;">43.98</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4V [55]+SoM [57]</td>
<td style="text-align: center;">75.00</td>
<td style="text-align: center;">55.23</td>
<td style="text-align: center;">42.45</td>
<td style="text-align: center;">54.46</td>
<td style="text-align: center;">49.03</td>
<td style="text-align: center;">47.27</td>
<td style="text-align: center;">54.33</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-v1.6-34B [56]+SoM [57]</td>
<td style="text-align: center;">44.16</td>
<td style="text-align: center;">40.01</td>
<td style="text-align: center;">33.96</td>
<td style="text-align: center;">47.32</td>
<td style="text-align: center;">41.34</td>
<td style="text-align: center;">46.36</td>
<td style="text-align: center;">42.31</td>
</tr>
<tr>
<td style="text-align: center;">KOSMOS-2 [8]</td>
<td style="text-align: center;">28.33</td>
<td style="text-align: center;">15.23</td>
<td style="text-align: center;">4.71</td>
<td style="text-align: center;">26.78</td>
<td style="text-align: center;">12.50</td>
<td style="text-align: center;">12.72</td>
<td style="text-align: center;">17.04</td>
</tr>
<tr>
<td style="text-align: center;">RegionVILA-7B [19]</td>
<td style="text-align: center;">30.83</td>
<td style="text-align: center;">47.61</td>
<td style="text-align: center;">35.84</td>
<td style="text-align: center;">44.64</td>
<td style="text-align: center;">35.57</td>
<td style="text-align: center;">49.09</td>
<td style="text-align: center;">40.48</td>
</tr>
<tr>
<td style="text-align: center;">SpatialRGPT-7B ${ }_{i}$ rgb)</td>
<td style="text-align: center;">99.17</td>
<td style="text-align: center;">99.04</td>
<td style="text-align: center;">79.24</td>
<td style="text-align: center;">89.28</td>
<td style="text-align: center;">83.65</td>
<td style="text-align: center;">87.27</td>
<td style="text-align: center;">89.80</td>
</tr>
<tr>
<td style="text-align: center;">SpatialRGPT-7B</td>
<td style="text-align: center;">99.17</td>
<td style="text-align: center;">99.04</td>
<td style="text-align: center;">80.19</td>
<td style="text-align: center;">91.96</td>
<td style="text-align: center;">87.50</td>
<td style="text-align: center;">91.81</td>
<td style="text-align: center;">91.78</td>
</tr>
<tr>
<td style="text-align: center;">SpatialRGPT-VILA-1.5-3B</td>
<td style="text-align: center;">99.17</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">81.13</td>
<td style="text-align: center;">88.39</td>
<td style="text-align: center;">85.57</td>
<td style="text-align: center;">93.63</td>
<td style="text-align: center;">91.47</td>
</tr>
<tr>
<td style="text-align: center;">SpatialRGPT-VILA-1.5-8B</td>
<td style="text-align: center;">99.17</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">84.90</td>
<td style="text-align: center;">89.28</td>
<td style="text-align: center;">91.34</td>
<td style="text-align: center;">90.90</td>
<td style="text-align: center;">92.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Direct <br> Distance</td>
<td style="text-align: center;">Horizontal <br> Distance</td>
<td style="text-align: center;">Vertical <br> Distance</td>
<td style="text-align: center;">Width</td>
<td style="text-align: center;">Height</td>
<td style="text-align: center;">Direction</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 [55]</td>
<td style="text-align: center;">21.6 / 1.29</td>
<td style="text-align: center;">11.5 / 2.08</td>
<td style="text-align: center;">33.0 / 0.65</td>
<td style="text-align: center;">52.3 / 0.52</td>
<td style="text-align: center;">48.1 / 1.40</td>
<td style="text-align: center;">34.6 / 83.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4V [55]</td>
<td style="text-align: center;">29.7 / 0.92</td>
<td style="text-align: center;">25.4 / 2.75</td>
<td style="text-align: center;">33.0 / 0.48</td>
<td style="text-align: center;">51.1 / 0.37</td>
<td style="text-align: center;">68.4 / 1.57</td>
<td style="text-align: center;">43.9 / 69.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-v1.6-34B [56]</td>
<td style="text-align: center;">24.3 / 0.76</td>
<td style="text-align: center;">24.5 / 1.59</td>
<td style="text-align: center;">30.1 / 0.62</td>
<td style="text-align: center;">30.8 / 0.40</td>
<td style="text-align: center;">42.8 / 1.96</td>
<td style="text-align: center;">33.6 / 78.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4V [55]+SoM [57]</td>
<td style="text-align: center;">25.7 / 1.02</td>
<td style="text-align: center;">22.1 / 2.36</td>
<td style="text-align: center;">33.9 / 0.64</td>
<td style="text-align: center;">45.8 / 0.70</td>
<td style="text-align: center;">62.4 / 1.08</td>
<td style="text-align: center;">54.2 / 55.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-v1.6-34B [56]+SoM [57]</td>
<td style="text-align: center;">12.8 / 1.15</td>
<td style="text-align: center;">20.4 / 1.79</td>
<td style="text-align: center;">11.3 / 0.95</td>
<td style="text-align: center;">9.0 / 0.91</td>
<td style="text-align: center;">7.5 / 3.11</td>
<td style="text-align: center;">12.8 / 33.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">KOSMOS-2 [8]</td>
<td style="text-align: center;">4.1 / $&gt;10$</td>
<td style="text-align: center;">4.91 / $&gt;10$</td>
<td style="text-align: center;">1.9 / 2.26</td>
<td style="text-align: center;">3.0 / 5.42</td>
<td style="text-align: center;">1.5 / 3.82</td>
<td style="text-align: center;">1.9 / 104</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RegionVILA-7B [19]</td>
<td style="text-align: center;">22.3 / 1.30</td>
<td style="text-align: center;">24.6 / 3.26</td>
<td style="text-align: center;">17.9 / $&gt;10$</td>
<td style="text-align: center;">36.8 / $&gt;10$</td>
<td style="text-align: center;">49.6 / 1.61</td>
<td style="text-align: center;">35.5 / 79.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SpatialRGPT-7B ${ }_{i}$ rgb)</td>
<td style="text-align: center;">35.1 / 0.35</td>
<td style="text-align: center;">59.0 / 0.27</td>
<td style="text-align: center;">53.8 / 0.27</td>
<td style="text-align: center;">51.9 / 0.31</td>
<td style="text-align: center;">54.9 / 0.63</td>
<td style="text-align: center;">95.3 / 17.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SpatialRGPT-7B</td>
<td style="text-align: center;">41.2 / 0.33</td>
<td style="text-align: center;">65.6 / 0.25</td>
<td style="text-align: center;">51.9 / 0.27</td>
<td style="text-align: center;">49.6 / 0.31</td>
<td style="text-align: center;">57.9 / 0.61</td>
<td style="text-align: center;">95.3 / 15.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SpatialRGPT-VILA-1.5-3B</td>
<td style="text-align: center;">44.6 / 0.30</td>
<td style="text-align: center;">63.1 / 0.22</td>
<td style="text-align: center;">50.9 / 0.28</td>
<td style="text-align: center;">42.9 / 0.33</td>
<td style="text-align: center;">63.2 / 0.60</td>
<td style="text-align: center;">93.5 / 10.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SpatialRGPT-VILA-1.5-8B</td>
<td style="text-align: center;">45.9 / 0.31</td>
<td style="text-align: center;">68.0 / 0.22</td>
<td style="text-align: center;">56.6 / 0.28</td>
<td style="text-align: center;">48.9 / 0.28</td>
<td style="text-align: center;">61.7 / 0.41</td>
<td style="text-align: center;">95.3 / 9.7</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1: SpatialRGPT-Bench results. $\square$ are Blind LLMs with Language Referral. $\square$ are VLMs with Language Referral. $\square$ are Region-aware VLMs. Numbers in the top table represent success rates ( $\uparrow$ ), while the bottom table includes success rates $(\uparrow)$ and absolute relative error $(\downarrow)$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">VQA $_{v / 2}$</th>
<th style="text-align: center;">GQA</th>
<th style="text-align: center;">SQA $^{I}$</th>
<th style="text-align: center;">VQA $^{T}$</th>
<th style="text-align: center;">POPE</th>
<th style="text-align: center;">MME</th>
<th style="text-align: center;">MMB</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">VILA-1.5-3B</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">$\mathbf{8 5 . 9}$</td>
<td style="text-align: center;">$\mathbf{1 4 4 2}$</td>
<td style="text-align: center;">63.4</td>
</tr>
<tr>
<td style="text-align: left;">SpatialRGPT-VILA-1.5-3B</td>
<td style="text-align: center;">$\mathbf{8 1 . 1}$</td>
<td style="text-align: center;">$\mathbf{6 2 . 3}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 0}$</td>
<td style="text-align: center;">$\mathbf{6 1 . 7}$</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">1424</td>
<td style="text-align: center;">$\mathbf{6 5 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">MMB-CN</td>
<td style="text-align: center;">SEED</td>
<td style="text-align: center;">SEED $^{I}$</td>
<td style="text-align: center;">MMMU $_{V}$</td>
<td style="text-align: center;">MMMU $_{T}$</td>
<td style="text-align: center;">LLaVA $^{B}$</td>
<td style="text-align: center;">MMVet</td>
</tr>
<tr>
<td style="text-align: left;">VILA-1.5-3B</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">$\mathbf{3 3 . 3}$</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">$\mathbf{7 5 . 9}$</td>
<td style="text-align: center;">35.4</td>
</tr>
<tr>
<td style="text-align: left;">SpatialRGPT-VILA-1.5-3B</td>
<td style="text-align: center;">$\mathbf{5 3 . 6}$</td>
<td style="text-align: center;">$\mathbf{6 1 . 8}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 0}$</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">$\mathbf{3 1 . 3}$</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">$\mathbf{3 8 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of SpatialRGPT and base model performance on general VLM benchmarks.</p>
<h1>4.1 3D Spatial Reasoning Benchmarks</h1>
<p>Currently, there are no visual-language benchmarks that specifically focus on VLM's ability to understand 3D spatial concepts like metric distance or size differences between objects. Recently, SpatialVLM created a spatial reasoning VQA benchmark using human labelers to annotate spatial information on 2D images, but this benchmark is not publicly available. To address this gap, we develop SpatialRGPT-Bench, a spatial reasoning VQA benchmark using data from both urban (nuScenes [58], KITTI [59]) and indoor (SUNRGBD [60], ARKitScenes [61]) environments, as well as simulated scenes (Hypersim [62]). These datasets cover various potential applications and include diverse object types, enhancing our benchmark's thoroughness. We use preprocessed 3D cuboids for each object from Omni3D [63], all positioned within a unified 3D camera coordinate system and categorized by object classes. With these 3D cuboid annotations, we developed a conversational benchmark using our data generation pipeline. This benchmark comprises 657 qualitative and 749 quantitative VQA pairs, covering 88 distinct classes. All the samples come from the validation or test splits of the original datasets and are unseen by SpatialRGPT during the training phase. Please see Appx. E for statistics and examples of SpatialRGPT-Bench.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: SpatialRGPT is capable of complex spatial reasoning, addressing gaps that current leading vision language models, such as GPT-4V, struggle with.</p>
<p>We consider three categories of models as baselines:
Blind LLMs w/ Language Referral. The blind [10] LLM model relies solely on text and generates answers using only the content of the question. To enhance this approach, we prepend the object class to each question. This method serves as a baseline to gauge how much spatial reasoning can be derived from purely existing world knowledge. We choose GPT-4 to represent this baseline, as it is the most advanced model for encapsulating comprehensive world knowledge.
VLMs w/ Language Referral. The setup is similar to the blind LLMs but includes access to visual content, which could allow the model to answer better than a blind LLM. We employ current state-of-the-art VLMs, GPT-4V and LLaVA-v1.6-34B [56], as baselines for this category.
Region-aware VLMs. This category explores models with region-level capabilities similar to our method. The models do not receive any language captions or object class information related to the region of interest; they rely solely on their visual processing capabilities. We equip GPT-4V [55] and LLaVA-v1.6-34B with Set of Marks (SoM) [57] to enable region-referring capabilities. Additionally, we include KOSMOS-2 [24], a VLM capable of taking bounding box inputs to reference objects, and RegionVILA (RegionGPT [19] with VILA [50] pre-training). RegionVILA-7B also serves as an ablation baseline to our method; it shares the same model architecture as our SpatialRGPT-7B ${ }_{1} r g b$ ) variant but is trained without our specialized spatial VQA dataset.
We use GPT-4 to evaluate the response for each model; please see Appx. J for details. For qualitative QAs, GPT-4 scores the alignment between the model's response and the correct answer as 0 or 1 . For quantitative QAs, GPT-4 standardizes numerical values across units into meters; we then calculate accuracy and error metrics. We present the results in Table 1. The upper rows of the table show accuracy (correct vs incorrect or failed to answer) for qualitative QAs. The lower rows report on</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>mAP (↑)</th>
<th>Acc. (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>CLIP [64]</td>
<td>58.9</td>
<td>-</td>
</tr>
<tr>
<td>RegionCLIP [65]</td>
<td>58.3</td>
<td>-</td>
</tr>
<tr>
<td>LLaVA-7B [29]</td>
<td>-</td>
<td>40.0</td>
</tr>
<tr>
<td>Shikra-7B [25]</td>
<td>-</td>
<td>53.9</td>
</tr>
<tr>
<td>GPT4Rof-7B [31]</td>
<td>-</td>
<td>64.0</td>
</tr>
<tr>
<td>PVIT-7B [66]</td>
<td>-</td>
<td>64.5</td>
</tr>
<tr>
<td>ASM-7B [32]</td>
<td>69.3</td>
<td>-</td>
</tr>
<tr>
<td>RegionGPT-7B [19]</td>
<td>70.0</td>
<td>80.6</td>
</tr>
<tr>
<td>SpatialRGPT-7B</td>
<td>69.7</td>
<td>79.9</td>
</tr>
<tr>
<td>SpatialRGPT-VILA-1.5-3B</td>
<td>72.5</td>
<td>82.5</td>
</tr>
<tr>
<td>SpatialRGPT-VILA-1.5-8B</td>
<td>72.9</td>
<td>82.9</td>
</tr>
</tbody>
</table>
<p>Table 3: Region-level classification results. We follow the evaluation in RegionCLIP [65] and RegionGPT [19], report the results of object classification with ground-truth box on COCO-2017 validation set.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Acc. (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qwen-VL-Max [67]</td>
<td>58.9</td>
</tr>
<tr>
<td>Gemini Pro [68]</td>
<td>50.0</td>
</tr>
<tr>
<td>Claude 3 OPUS [69]</td>
<td>57.3</td>
</tr>
<tr>
<td>GPT-4V-<em>preview</em> [55]</td>
<td>58.9</td>
</tr>
<tr>
<td>GPT-4V-<em>Turbo</em> [55]</td>
<td>66.9</td>
</tr>
<tr>
<td>GPT-4o [55]</td>
<td>64.5</td>
</tr>
<tr>
<td>InstructBLIP-13B [51]</td>
<td>50.0</td>
</tr>
<tr>
<td>Yi-VL-34B [70]</td>
<td>53.2</td>
</tr>
<tr>
<td>LLaVA-v1.5-13B-xtuner [71]</td>
<td>54.0</td>
</tr>
<tr>
<td>LLaVA-v1.6-34B [56]</td>
<td>64.5</td>
</tr>
<tr>
<td>MiniGPT-4-v2-7B [26]</td>
<td>49.2</td>
</tr>
<tr>
<td>InstructBLIP-7B [51]</td>
<td>50.8</td>
</tr>
<tr>
<td>LLaVA-v1.5-7B-xtuner [71]</td>
<td>50.8</td>
</tr>
<tr>
<td>CogVLM-7B [27]</td>
<td>50.8</td>
</tr>
<tr>
<td>LLaVA-v1.5-7B [72]</td>
<td>51.6</td>
</tr>
<tr>
<td>LLaVA-internLM2-7B [73]</td>
<td>52.4</td>
</tr>
<tr>
<td>SpatialRGPT-7B</td>
<td>82.3</td>
</tr>
<tr>
<td>SpatialRGPT-VILA-1.5-8B</td>
<td>87.9</td>
</tr>
</tbody>
</table>
<p>Table 4: BLINK $<em _="(">{\text{Relative Depth}}$ results.
quantitative QAs, detailing their success rate (answers within $\pm 25\%$ of the ground truth value) and the absolute relative error [43, 42]. We exclude answers that failed to produce a numerical response from the relative error calculations. The results show that SpatialRGPT significantly outperforms baselines in terms of success rate for qualitative QAs and maintains the lowest error rate for quantitative QAs. Interestingly, we found that blind LLMs and VLMs with language referrals achieved commendable success rates for quantitative QAs, especially for questions related to width and height. This suggests that LLMs can accurately answer specific spatial questions using their extensive world knowledge. Additionally, our SpatialRGPT-7B variant demonstrates improved performance over the SpatialRGPT$7\mathrm{B}</em>r g b)$ variant, especially in scenarios where relative depth information can be used to resolve ambiguities, such as distinguishing between behind/front, wide/thin, and estimating distances.</p>
<h3>4.2 Public Vision-language Benchmarks</h3>
<p>General Benchmarks. In this section, we evaluate whether integrating spatial VQA data and depth information affects performance on other VQA tasks. We compared our models with VILA-1.5-3B, which is trained on general VQA datasets. As shown in Table 2, our variants performed similarly to the baselines and slightly better on the VQA-v2 and MMVet datasets. These results align with findings from [17], indicating that VLMs generally underperform on spatial reasoning tasks but can improve with specific spatial VQA training without compromising general VQA performance.</p>
<p>Region &amp; Spatial Benchmarks. We follow the evaluation protocol from RegionGPT [19] and report object classification results using ground-truth boxes on the COCO-2017 validation set. As shown in Table 3, SpatialRGPT outperforms the baselines, demonstrating its strong region cognition capabilities. We further evaluate SpatialRGPT on BLINK [9]'s Relative Depth Benchmark. This benchmark is particularly challenging as it assesses point-level depths, while both the point-level region input and point-level questions were not specifically included in the training of SpatialRGPT. We use bounding boxes to mark the target points and evaluate the test set online with the EvalAI server. As shown in Table 4, SpatialRGPT significantly outperforms the state-of-the-art, achieving over 20% accuracy gain compared to GPT-4V-Turbo. Our model demonstrated strong performance, highlighting its ability to generalize to new tasks without explicit training.</p>
<h3>4.3 Real-world Applications</h3>
<p>Complex Spatial Reasoning. In this application, we aim to explore whether SpatialRGPT can function as a complex spatial reasoner on its own. Unlike the system mentioned in [17], which uses GPT-4 to handle reasoning tasks and employs VLM solely for answering basic spatial queries, SpatialRGPT directly integrates these capabilities. We provide examples in Figure 4, where we compare SpatialRGPT's responses to those from GPT-4V using real-world samples. Our model demonstrates the ability to address complex spatial questions based on its own spatial knowledge. This suggests that SpatialRGPT has developed a robust representation of spatial learning and that this knowledge has effectively generalized to enhance its intrinsic language reasoning abilities.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Examples of SpatialRGPT performing multi-hop reasoning.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: SpatialRGPT functions as a region-aware reward annotator. The estimated distance decreased monotonically as the fingertip moves towards the target.</p>
<p>Multi-hop Reasoning. In Figure 5, we show examples of SpatialRGPT handling multi-hop reasoning. In the upper left sample, the model first identifies what's to the right of Region [0] (a single apple), finds the basket there, determines what's inside the basket, and then provides spatial details about the object inside. Even though our training data doesn't specifically include such multi-hop tasks, SpatialRGPT can still manage them effectively. This indicates that the model has developed a strong understanding of spatial relationships.
Region-aware Dense Reward Annotator. Recently, [17] has shown that VLMs can function as dense reward annotators for robotics tasks by specifying tasks in natural language and having the model annotate rewards for each frame in a trajectory. However, this approach can be constrained by the language's ambiguity, especially when multiple identical objects are present or when targeting a small, specific region in a scene, which can be difficult to describe precisely with language alone. Given that SpatialRGPT is equipped with region-aware capabilities, we can directly specify the regions of interest. To study this application, we conducted a real robot experiment. Specifically, we defined two regions using bounding boxes (one for the fingertip and one for a green cube) and tasked SpatialRGPT to annotate rewards using the distance between the two regions. The results, shown in Figure 6, indicate that the estimated distance between the fingertip and its target cube decreased monotonically as the fingertip moved towards its goal. Also, our depth variant performs slightly better than the RGB variant. This demonstrates SpatialRGPT 's effectiveness as a region-aware dense reward annotator, offering a more precise and efficient alternative to language-only approaches.</p>
<h1>5 Discussion</h1>
<p>Conclusion. We introduce SpatialRGPT, a novel framework designed to enhance the spatial reasoning capabilities of Vision Language Models (VLMs). By integrating a region representation module and a flexible plugin for depth information, SpatialRGPT allows VLMs to effectively perceive spatial arrangement at both local and global scopes. Our data curation pipeline facilitates the learning of 3D spatial knowledge from scene graphs, while SpatialRGPT-Bench provides a comprehensive benchmark for evaluating spatial cognition across diverse environments. The results demonstrate significant improvements in spatial reasoning tasks while showcasing the model's ability to reason complex spatial relations and perform as dense reward annotators for robotic applications.
Limitations. One limitation of our work is the use of Axis-Aligned Bounding Boxes (AABBs), which can result in inaccuracies in label representation. A more accurate alternative is oriented bounding boxes (OBBs), but implementing them requires precise object pose estimation, which remains challenging due to the lack of open-world solutions. The most accurate approach would be human labeling [74], although this requires significant effort. We leave these for future work.</p>
<p>Acknowledgement. This work was supported, in part, by the Qualcomm Innovation Fellowship.</p>
<h2>References</h2>
<p>[1] Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, and Wei Liu. Learning to compose dynamic tree structures for visual contexts. In CVPR, 2019. 2
[2] Long Chen, Hanwang Zhang, Jun Xiao, Xiangnan He, Shiliang Pu, and Shih-Fu Chang. Counterfactual critic multi-agent training for scene graph generation. In ICCV, 2019. 2
[3] Johanna Wald, Helisa Dhamo, Nassir Navab, and Federico Tombari. Learning 3d semantic scene graphs from 3d indoor reconstructions. In CVPR, 2020. 2
[4] Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi. What does a platypus look like? generating customized prompts for zero-shot image classification. In ICCV, 2023. 2
[5] Yuval Alaluf, Elad Richardson, Sergey Tulyakov, Kfir Aberman, and Daniel Cohen-Or. Myvlm: Personalizing vlms for user-specific queries. arXiv preprint arXiv:2403.14599, 2024. 2
[6] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and Anelia Angelova. F-vlm: Openvocabulary object detection upon frozen vision and language models. In ICLR, 2023. 2
[7] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. In ECCV, 2024. 2
[8] Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, et al. Kosmos-2.5: A multimodal literate model. arXiv preprint arXiv:2309.11419, 2023. 2, 7
[9] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. 2, 9
[10] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Karmesh Yadav, Qiyang Li, Ben Newman, Mohit Sharma, Vincent Berges, Shiqi Zhang, Pulkit Agrawal, Yonatan Bisk, Dhruv Batra, Mrinal Kalakrishnan, Franziska Meier, Chris Paxton, Sasha Sax, and Aravind Rajeswaran. Openeqa: Embodied question answering in the era of foundation models. In CVPR, 2024. 2, 3, 5, 8
[11] Jianrui Zhang, Mu Cai, Tengyang Xie, and Yong Jae Lee. Countercurate: Enhancing physical and semantic visio-linguistic compositional reasoning via counterfactual examples. In ACL, 2024. 2
[12] Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh. Physically grounded vision-language models for robotic manipulation. In ICRA, 2024. 2
[13] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, et al. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. arXiv preprint arXiv:2402.07872, 2024. 2
[14] Mikhail Konenkov, Artem Lykov, Daria Trinitatova, and Dzmitry Tsetserukou. Vr-gpt: Visual language model for intelligent virtual reality applications. arXiv preprint arXiv:2405.11537, 2024. 2
[15] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Visual language maps for robot navigation. In ICRA, 2023. 2
[16] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. In ICML, 2023. 2
[17] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In CVPR, 2024. 2, 3, 4, 9, 10, 19, 20, 23
[18] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: A vision-language model for spatial affordance prediction for robotics. In CoRL, 2024. 2</p>
<p>[19] Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei Liu. Regiongpt: Towards region understanding vision language model. In CVPR, 2024. 2, 3, 5, 6, 7, 8, 9, 21, 22
[20] Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan. 3d concept learning and reasoning from multi-view images. In CVPR, 2023. 3
[21] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. In NeurIPS, 2023. 3
[22] Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, et al. Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. In ICRA, 2024. 3,20
[23] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. 3
[24] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. In ICLR, 2024. 3, 8
[25] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. 3, 9
[26] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023. 3, 9
[27] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. 3, 9
[28] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. 3
[29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 3, 6, 9, 22
[30] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. In NeurIPS, 2023. 3
[31] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601, 2023. 3, 9
[32] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, et al. The all-seeing project: Towards panoptic visual recognition and understanding of the open world. In ICLR, 2024. 3, 9
[33] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. In ICLR, 2024. 3
[34] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, TsuJui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferret-v2: An improved baseline for referring and grounding with large language models. arXiv preprint arXiv:2404.07973, 2024. 3, 21
[35] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M Anwer, Erix Xing, Ming-Hsuan Yang, and Fahad S Khan. Glamm: Pixel grounding large multimodal model. In CVPR, 2024. 3
[36] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning. In CVPR, 2024. 3</p>
<p>[37] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: A strong image tagging model. arXiv preprint arXiv:2306.03514, 2023. 4
[38] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 4
[39] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality. In NeurIPS, 2023. 4
[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 4, 22
[41] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. 4
[42] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. Zoedepth: Zero-shot transfer by combining relative and metric depth. In CVPR, 2023. 4, 9
[43] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. 4, 6, 9
[44] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. IJRR, 2013. 4
[45] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. 4
[46] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: A versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. arXiv preprint arXiv:2404.15506, 2024. 4
[47] Shengjie Zhu, Abhinav Kumar, Masa Hu, and Xiaoming Liu. Tame a wild camera: In-the-wild monocular camera calibration. In NeurIPS, 2023. 4
[48] Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver Wang, Kevin Matzen, Matthew Sticha, and David F. Fouhey. Perspective fields for single image camera calibration. In CVPR, 2023. 4
[49] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020. 5, 19, 24
[50] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In CVPR, 2024. 6, 8, 21, 22, 23
[51] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. In NeurIPS, 2023. 6, 9
[52] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 6
[53] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of images interleaved with text. In NeurIPS, 2023. 6
[54] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/ coyo-dataset, 2022. 6
[55] R OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2(5), 2023. 7, 8, 9, 23
[56] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, 2024. 7, 8, 9</p>
<p>[57] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. 7, 8
[58] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In CVPR, 2020. 7
[59] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In CVPR, 2012. 7
[60] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In CVPR, 2015. 7
[61] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Yuri Feigin, Peter Fu, Thomas Gebauer, Daniel Kurz, Tal Dimry, Brandon Joffe, Arik Schwartz, and Elad Shulman. ARKitscenes: A diverse real-world dataset for 3d indoor scene understanding using mobile RGB-d data. In NeurIPS, 2021. 7
[62] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M Susskind. Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding. In ICCV, 2021. 7
[63] Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila Ravi, Justin Johnson, and Georgia Gkioxari. Omni3d: A large benchmark and model for 3d object detection in the wild. In CVPR, 2023. 7, 18
[64] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 9, 21
[65] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In CVPR, 2022. 9
[66] Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, and Yang Liu. Positionenhanced visual instruction tuning for multimodal large language models. arXiv preprint arXiv:2308.13437, 2023. 9
[67] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 9
[68] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 9
[69] Claude-3-family. https://www.anthropic.com/news/ claude-3-family. 9
[70] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. 9
[71] XTuner Contributors. Xtuner: A toolkit for efficiently fine-tuning llm. https://github. com/InternLM/xtuner, 2023. 9
[72] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. 9
[73] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. In arXiv preprint arXiv:2403.17297, 2024. 9
[74] Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David Acuna. Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. arXiv preprint arXiv:2409.09788, 2024. 10, 24
[75] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 35, 2022. 19</p>
<p>[76] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 19
[77] Ruotian Luo, Brian Price, Scott Cohen, and Gregory Shakhnarovich. Discriminability objective for training descriptive captions. In CVPR, 2018. 22
[78] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In CVPR, 2016. 22
[79] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In ECCV, 2020. 22
[80] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017. 22
[81] Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A corpus of natural language for visual reasoning. In NAACL, 2017. 22
[82] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. Visualmrc: Machine reading comprehension on document images. In AAAI, 2021. 22
[83] Desmond Elliott, Stella Frank, Khalil Sima'an, and Lucia Specia. Multi30k: Multilingual english-german image descriptions. In ACL, 2016. 22
[84] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for understanding complex web videos via question answering. In AAAI, 2019. 22
[85] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In CVPR, 2021. 22
[86] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 22
[87] Feng Liu, Tao Xiang, Timothy M Hospedales, Wankou Yang, and Changyin Sun. ivqa: Inverse visual question answering. In CVPR, 2018. 22
[88] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, 2019. 22
[89] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In ICCV, 2019. 22
[90] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Hervé Le Borgne, Romaric Besançon, Jose G. Moreno, and Jesús Lovón Melgarejo. Viquae, a dataset for knowledge-based visual question answering about named entities. In ACM SIGIR, 2022. 22
[91] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. 22
[92] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In CVPR, 2017. 22
[93] Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua Lin. V3det: Vast vocabulary visual detection dataset. In ICCV, 2023. 22
[94] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In CVPR, 2019. 22
[95] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017. 22
[96] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, 2014. 22
[97] Unsplash. https://unsplash.com/. 25</p>
<h1>Appendix Table of Contents</h1>
<p>A Ablation Study on Augmented SpatialRGPT-Bench ..... 17
B Ablation Study on Metric-Scale Width and Height Data ..... 17
C Ablation Study on Bounding Box Types ..... 17
D Ablation Study on Different Input Modalities ..... 18
E Statistics and Samples of SpatialRGPT-Bench ..... 18
F Implementation Details for Data Pipeline ..... 18
G Implementation Details for SpatialRGPT Architecture ..... 21
H Implementation Details for Training SpatialRGPT ..... 22
I Experimental Setting and Details ..... 23
J Benchmark Evaluation Details ..... 23
K More Discussion on Limitations ..... 23
L Broader Impacts ..... 24
M Licenses ..... 24</p>
<h1>A Ablation Study on Augmented SpatialRGPT-Bench</h1>
<p>We conduct additional experiments by augmenting and rephrasing both questions and answers in SpatialRGPT-Bench using GPT-4. The results are shown in Table 5. The results show that SpatialRGPT consistently outperforms the baseline models, even when the questions and answers are different from the training data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Below/ <br> Above</th>
<th style="text-align: center;">Left/ <br> Right</th>
<th style="text-align: center;">Big/ <br> Small</th>
<th style="text-align: center;">Tall/ <br> Short</th>
<th style="text-align: center;">Wide/ <br> Thin</th>
<th style="text-align: center;">Behind/ <br> Front</th>
<th style="text-align: center;">Qualitative <br> Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-4V-Turbo</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">60.5</td>
</tr>
<tr>
<td style="text-align: center;">SpatialRGPT-7B</td>
<td style="text-align: center;">$\mathbf{9 5 . 8}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 7 . 4}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 9}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 7}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 9}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 0}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Direct <br> Distance</td>
<td style="text-align: center;">Horizontal <br> Distance</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Vertical <br> Distance</td>
<td style="text-align: center;">Width</td>
<td style="text-align: center;">Height</td>
<td style="text-align: center;">Direction</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4V-Turbo</td>
<td style="text-align: center;">$30.4 / 0.87$</td>
<td style="text-align: center;">$26.2 / 2.66$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$33.9 / 0.51$</td>
<td style="text-align: center;">$48.8 / 0.35$</td>
<td style="text-align: center;">$\mathbf{6 9 . 1 / 1 . 3 5}$</td>
<td style="text-align: center;">$40.1 / 70.0^{\circ}$</td>
</tr>
<tr>
<td style="text-align: center;">SpatialRGPT-7B</td>
<td style="text-align: center;">$\mathbf{4 3 . 2 / 0 . 3 2}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 9 / 0 . 2 7}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{5 2 . 8 / 0 . 2 6}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 1 / 0 . 3 1}$</td>
<td style="text-align: center;">$54.1 / 1.02$</td>
<td style="text-align: center;">$\mathbf{9 5 . 3 / 1 5 . 3 ^ { \circ }}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Augmented SpatialRGPT-Bench results. Numbers represent success rates $(\uparrow)$ and absolute relative error $(\downarrow)$.</p>
<h2>B Ablation Study on Metric-Scale Width and Height Data</h2>
<p>We conduct an ablation study to see if adding width and height data affects other types of questions. As shown in Table 6, adding this data slightly improved the accuracy for questions about size (like big/small, tall/short, wide/thin) but slightly worsened the accuracy for questions about the distance between objects (horizontal and vertical). This suggests that information about object size helps with size-related questions but might make distance measurements less clear.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Below / Above</th>
<th style="text-align: left;">Left / Right</th>
<th style="text-align: left;">Big / Small</th>
<th style="text-align: left;">Tall / Short</th>
<th style="text-align: left;">Wide / Thin</th>
<th style="text-align: left;">Behind / Front</th>
<th style="text-align: left;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">- width \&amp; height</td>
<td style="text-align: left;">99.1</td>
<td style="text-align: left;">99.0</td>
<td style="text-align: left;">75.8</td>
<td style="text-align: left;">90.8</td>
<td style="text-align: left;">82.8</td>
<td style="text-align: left;">92.1</td>
<td style="text-align: left;">90.5</td>
</tr>
<tr>
<td style="text-align: left;">+ width \&amp; height</td>
<td style="text-align: left;">$99.1+0$</td>
<td style="text-align: left;">$99.0+0$</td>
<td style="text-align: left;">$80.1+4.3$</td>
<td style="text-align: left;">$91.9+1.1$</td>
<td style="text-align: left;">$87.5+4.7$</td>
<td style="text-align: left;">$91.8-0.3$</td>
<td style="text-align: left;">$90.5+1.2$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Direct Distance</td>
<td style="text-align: left;">Horizontal Distance</td>
<td style="text-align: left;">Vertical Distance</td>
<td style="text-align: left;">Width</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Height</td>
<td style="text-align: left;">Direction</td>
</tr>
<tr>
<td style="text-align: left;">- width \&amp; height</td>
<td style="text-align: left;">41.2</td>
<td style="text-align: left;">69.3</td>
<td style="text-align: left;">54.8</td>
<td style="text-align: left;">22.8</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">21.2</td>
<td style="text-align: left;">95.1</td>
</tr>
<tr>
<td style="text-align: left;">+ width \&amp; height</td>
<td style="text-align: left;">$41.2+0$</td>
<td style="text-align: left;">$65.6-3.7$</td>
<td style="text-align: left;">$51.9-2.9$</td>
<td style="text-align: left;">$49.6+26.8$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$57.9+36.7$</td>
<td style="text-align: left;">$95.3+0.2$</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablation study on the impact of width and height data on the performance of other categories. Numbers represent success rates $(\uparrow)$.</p>
<h2>C Ablation Study on Bounding Box Types</h2>
<p>We conduct an ablation study to examine the effect of using axis-aligned bounding boxes (AABB) versus PCA-based oriented bounding boxes (OBB). For this study, we use human-labeled OBBs from the Omni3D test set as the ground truth. We then compare the mean-square error of the width and height measurements for AABBs and PCA-based OBBs labeled by our 3D scene graph pipeline. The results are shown in Table 7. PCA-based OBB often lacks accuracy due to the incomplete and noisy nature of point clouds captured from a single view.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">BBox Type</th>
<th style="text-align: center;">Width $(\downarrow)$</th>
<th style="text-align: center;">Height $(\downarrow)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Oriented BBox</td>
<td style="text-align: center;">17.09</td>
<td style="text-align: center;">4.83</td>
</tr>
<tr>
<td style="text-align: left;">Axis-aligned BBox</td>
<td style="text-align: center;">8.27</td>
<td style="text-align: center;">2.35</td>
</tr>
</tbody>
</table>
<p>Table 7: Ablation study on axis-aligned vs. oriented bounding boxes. Numbers indicate MSE comparing to Omni3D ground truth.</p>
<h1>D Ablation Study on Different Input Modalities</h1>
<p>As mentioned in Section 3.4, SpatialRGPT can take both boxes and masks as input during the inference phase. In this study, we aimed to test the impact of box and mask inputs on our SpatialRGPT-Bench. We presented the results in Table 8, where we observed a slight drop in performance when using boxes, but in general, the performance was very close. This suggests that the random modality strategy used during training is effective.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Below/ <br> Above</th>
<th style="text-align: center;">Left/ <br> Right</th>
<th style="text-align: center;">Big/ <br> Small</th>
<th style="text-align: center;">Tall/ <br> Short</th>
<th style="text-align: center;">Wide/ <br> Thin</th>
<th style="text-align: center;">Behind/ <br> Front</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SpatialRGPT-7B-Mask</td>
<td style="text-align: center;">99.17</td>
<td style="text-align: center;">99.04</td>
<td style="text-align: center;">80.19</td>
<td style="text-align: center;">91.96</td>
<td style="text-align: center;">87.50</td>
<td style="text-align: center;">91.81</td>
<td style="text-align: center;">91.78</td>
</tr>
<tr>
<td style="text-align: left;">SpatialRGPT-7B-Box</td>
<td style="text-align: center;">99.17</td>
<td style="text-align: center;">98.09</td>
<td style="text-align: center;">83.01</td>
<td style="text-align: center;">91.96</td>
<td style="text-align: center;">82.69</td>
<td style="text-align: center;">92.72</td>
<td style="text-align: center;">91.47</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Direct <br> Distance</td>
<td style="text-align: center;">Horizontal <br> Distance</td>
<td style="text-align: center;">Vertical <br> Distance</td>
<td style="text-align: center;">Width</td>
<td style="text-align: center;">Height</td>
<td style="text-align: center;">Direction</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SpatialRGPT-7B-Mask</td>
<td style="text-align: center;">$41.2 / 0.33$</td>
<td style="text-align: center;">$65.6 / 0.25$</td>
<td style="text-align: center;">$51.9 / 0.27$</td>
<td style="text-align: center;">$49.6 / 0.31$</td>
<td style="text-align: center;">$57.9 / 0.61$</td>
<td style="text-align: center;">$95.3 / 15.4^{\circ}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SpatialRGPT-7B-Box</td>
<td style="text-align: center;">$39.2 / 0.35$</td>
<td style="text-align: center;">$63.1 / 0.25$</td>
<td style="text-align: center;">$56.6 / 0.27$</td>
<td style="text-align: center;">$48.8 / 0.36$</td>
<td style="text-align: center;">$60.1 / 1.06$</td>
<td style="text-align: center;">$94.3 / 10.2^{\circ}$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 8: Ablation study on effect of different input modalities to Spatial RGPT. Numbers in the top table represent success rates $(\uparrow)$, while the bottom table includes success rates $(\uparrow)$ and absolute relative error $(\downarrow)$.</p>
<h2>E Statistics and Samples of SpatialRGPT-Bench</h2>
<p>Figure 7 presents key statistics from our SpatialRGPT-Bench, including counts for QA categories, data sources, and objects. We categorize the QA data into 12 distinct types, evenly divided between relative relationships and metric measurements. Notably, some datasets, such as SUNRGBD, emphasize closeobject scenarios. To reduce bias, we source our data from a diverse range of datasets following [63]. We also show six samples from our SpatialRGPT-Bench in Figure 8.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: SpatialRGPT-Bench statistics. Left: Category count and source count. Right: Object count.</p>
<h2>F Implementation Details for Data Pipeline</h2>
<p>In this section, we aim to provide a detailed implementation of our data annotation pipeline and intermediate results obtained through each component.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 8: Samples in SpatialRGPT-Bench.</p>
<h1>F. 1 Filtering.</h1>
<p>Recent VLMs often benefit from the broad capabilities gained through training with large-scale 2D image datasets [75, 49]. However, many images in these datasets are unsuitable for developing spatial reasoning QA. For instance, some images may be computer screenshots, paintings, collages, or simply a piece of text. Similar to SpatialVLM [17], we use a CLIP-based open-vocabulary classification model [76] to identify and exclude these unsuitable images. We follow the labeling used in SpatialVLM but have made a few adaptations to better suit the data distribution of the OpenImage [49] dataset. We show the labels we use in Listing 1. With this process, we filtered out 700 K samples from the 1.7 M OpenImage samples.</p>
<p>Listing 1: CLIP labels used during filtering.</p>
<div class="codehilite"><pre><span></span><code>positive_labels = [
    &quot;a DSLR photo of an indoor scene&quot;,
    &quot;a DSLR of an outdoor scene&quot;,
    &quot;an iphone photo of an indoor scene&quot;,
    &quot;an iphone photo of an outdoor scene&quot;,
]
negative_labels = [
    &quot;a close up shot of a single object&quot;,
    &quot;a product displayed in front of a white back ground&quot;,
    &quot;a painting&quot;,
    &quot;a collage of images&quot;,
    &quot;a screenshot of graphics user interface&quot;,
    &quot;a piece of text&quot;
]
</code></pre></div>

<h2>F. 2 Metric Depth Estimation</h2>
<p>As stated in the main paper, we choose Metric3Dv2 as our metric depth estimator. We have observed that Metric3Dv2 and WildCamera's camera intrinsic perform well on images taken in natural environments. In this section, we present the predicted normal maps from the depth model on OpenImages. These normal maps can be viewed as a proxy to estimate the quality of the reconstructed geometry's edges.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 9: Predicted normal maps using Metric3Dv2 and WildCamera.</p>
<h1>F. 3 Point Cloud Processing</h1>
<p>Here, we detailed how we process the point clouds into scene graphs.
Canonicalization. Our canonicalization method is straightforward. After obtaining the pitch and roll through PerspectiveFields, we transform the point cloud into a canonicalized space using the inverse of the rotation matrix. Figure 10 illustrates the successful alignment of the ground surface with the z-axis angle after canonicalization. This process ensures that the axis-aligned bounding box accurately represents the vertical information of the objects, such as height and vertical distance. Our simple yet effective approach liberates our method from surface segmentation and RANSAC. We have empirically found this procedure robust for most natural images taken by cameras in real-world conditions.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 10: Canonicalization Results.</p>
<p>Denoising and constructing axis-aligned bounding box. The point clouds obtained from singleview depth may contain noise. Following [17, 22], we carry out several denoising steps based on the approach to filter out outliers and unwanted points, thereby improving the robustness and accuracy of the bounding box. Initially, we eliminate statistical outliers from the object points and then downsample the data to a lower resolution. Subsequently, we use DBSCAN to further remove noise. If the points of an object are fewer than ten after DBSCAN clustering, we exclude that object area. Finally, we employ Open3D to create axis-aligned bounding boxes for each object. The pseudocode for our denoising process is as in Listing 2.</p>
<p>Listing 2: Point cloud denoising steps.</p>
<div class="codehilite"><pre><span></span><code>def process_pcd(pcd):
    scale = norm(pcd).std * 3.0 + 1e-6
    [pcd, _ ] = pcd.remove_statistical_outlier(nb_neighbors=10, std_ratio
        =1.2)
    pcd = pcd.voxel_down_sample(voxel_size=max(0.01, scale/40))
    pcd = pcd_denoise_dbscan(
</code></pre></div>

<div class="codehilite"><pre><span></span><code>    pcd, eps=0.2, min_points=10
    )
    return pcd
}
</code></pre></div>

<h1>F. 4 Open Spatial Dataset QA Templates</h1>
<p>We provide samples for each category of QA in the templates that we use to generate QAs mentioned in Section 3.1.</p>
<p>Listing 3: Template for QA synthesis.</p>
<div class="codehilite"><pre><span></span><code><span class="n">distance_template_questions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span>
<span class="n">    &quot;What is the distance between [A</span><span class="o">]</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="vm">?</span><span class="ss">&quot;,</span>
<span class="ss">    &quot;</span><span class="n">How</span><span class="w"> </span><span class="n">far</span><span class="w"> </span><span class="n">away</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="vm">?</span><span class="ss">&quot;,</span>
<span class="ss">    &quot;</span><span class="n">Can</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">provide</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">distance</span><span class="w"> </span><span class="n">measurement</span><span class="w"> </span><span class="ow">between</span><span class="w"> </span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="vm">?</span><span class="ss">&quot;,</span>
<span class="ss">]</span>
<span class="ss">distance_template_answers = [</span>
<span class="ss">    &quot;</span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="o">[</span><span class="n">X</span><span class="o">]</span><span class="w"> </span><span class="n">apart</span><span class="p">.</span><span class="ss">&quot;,</span>
<span class="ss">    &quot;</span><span class="n">A</span><span class="w"> </span><span class="n">distance</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="o">[</span><span class="n">X</span><span class="o">]</span><span class="w"> </span><span class="ow">exists</span><span class="w"> </span><span class="ow">between</span><span class="w"> </span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="p">.</span><span class="ss">&quot;,</span>
<span class="ss">    &quot;</span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="o">[</span><span class="n">X</span><span class="o">]</span><span class="w"> </span><span class="n">apart</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">other</span><span class="p">.</span><span class="ss">&quot;,</span>
<span class="ss">]</span>
<span class="ss">left_predicate_questions = [</span>
<span class="ss">    &quot;</span><span class="k">Is</span><span class="w"> </span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">viewer</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">perspective</span><span class="vm">?</span><span class="ss">&quot;,</span>
<span class="ss">    &quot;</span><span class="n">Does</span><span class="w"> </span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="n">appear</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="n">side</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="vm">?</span><span class="ss">&quot;,</span>
<span class="ss">    &quot;</span><span class="n">Can</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">confirm</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">positioned</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="vm">?</span><span class="ss">&quot;,</span>
<span class="ss">]</span>
<span class="ss">left_true_responses = [</span>
<span class="ss">    &quot;</span><span class="n">Yes</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="p">.</span><span class="ss">&quot;,</span>
<span class="ss">    &quot;</span><span class="n">Indeed</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">positioned</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="n">side</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="p">.</span><span class="ss">&quot;,</span>
<span class="ss">    &quot;</span><span class="n">Correct</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="err">’</span><span class="n">ll</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="p">.</span><span class="ss">&quot;,</span>
<span class="ss">]</span>
<span class="ss">left_false_responses = [</span>
<span class="ss">    &quot;</span><span class="k">No</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="p">.</span><span class="ss">&quot;,</span>
<span class="ss">    &quot;</span><span class="ow">In</span><span class="w"> </span><span class="n">fact</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nf">right</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="p">.</span><span class="ss">&quot;,</span>
<span class="ss">    &quot;</span><span class="n">Incorrect</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="n">side</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="p">.</span><span class="ss">&quot;,</span>
<span class="ss">]</span>
<span class="ss">direction_questions = [</span>
<span class="ss">    &quot;</span><span class="k">If</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="vm">?</span><span class="ss">&quot;</span>
<span class="ss">]</span>
<span class="ss">direction_responses = [</span>
<span class="ss">    &quot;</span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">roughly</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="o">[</span><span class="n">X</span><span class="o">]</span><span class="w"> </span><span class="n">o</span><span class="err">’</span><span class="n">clock</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="p">.</span><span class="ss">&quot;,</span>
<span class="ss">    &quot;</span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="w"> </span><span class="n">around</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="o">[</span><span class="n">X</span><span class="o">]</span><span class="w"> </span><span class="n">o</span><span class="err">’</span><span class="n">clock</span><span class="w"> </span><span class="n">direction</span><span class="p">.</span><span class="err">&quot;</span>
<span class="err">]</span>
</code></pre></div>

<h2>F. 5 LLM Prompts for Complex QA</h2>
<h2>G Implementation Details for SpatialRGPT Architecture</h2>
<h2>G. 1 Visual Backbone.</h2>
<p>We adopt a pre-trained OpenAI CLIP-L model [64] as the visual backbone. We use $336 \times 336$ image resolutions to include more visual details for the model, which can help with vision language tasks that require fine-grained details [50] and are beneficial for region-level representations [34].</p>
<h2>G. 2 Region-feature Extractor.</h2>
<p>We adopt the region feature extraction technique in [19]. To begin with, we use a feature refinement module consisting of a 2-layer deconvolution network designed to upscale the original feature map. Then, we employ MaskPooling to extract and average the refined features from the masked area. Note that we also employ a separate feature refinement module for the depth feature. Similar to the</p>            </div>
        </div>

    </div>
</body>
</html>
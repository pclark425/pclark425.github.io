<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1340 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1340</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1340</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-ac8c7217520abf6ece8d62fee42528f4fd59abdd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ac8c7217520abf6ece8d62fee42528f4fd59abdd" target="_blank">Scaling Local Control to Large-Scale Topological Navigation</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> This work presents an intuitive approach to show that by accurately measuring the capability of a local controller, large-scale visual topological navigation can be achieved while being scalable and robust.</p>
                <p><strong>Paper Abstract:</strong> Visual topological navigation has been revitalized recently thanks to the advancement of deep learning that substantially improves robot perception. However, the scalability and reliability issue remain challenging due to the complexity and ambiguity of real world images and mechanical constraints of real robots. We present an intuitive approach to show that by accurately measuring the capability of a local controller, large-scale visual topological navigation can be achieved while being scalable and robust. Our approach achieves state-of-the-art results in trajectory following and planning in large-scale environments. It also generalizes well to real robots and new environments without retraining or finetuning.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1340",
    "paper_id": "paper-ac8c7217520abf6ece8d62fee42528f4fd59abdd",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.003212,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Scaling Local Control to Large-Scale Topological Navigation</h1>
<p>Xiangyun Meng, Nathan Ratliff, Yu Xiang and Dieter Fox</p>
<h4>Abstract</h4>
<p>Visual topological navigation has been revitalized recently thanks to the advancement of deep learning that substantially improves robot perception. However, the scalability and reliability issue remain challenging due to the complexity and ambiguity of real world images and mechanical constraints of real robots. We present an intuitive approach to show that by accurately measuring the capability of a local controller, large-scale visual topological navigation can be achieved while being scalable and robust. Our approach achieves state-of-theart results in trajectory following and planning in large-scale environments. It also generalizes well to real robots and new environments without retraining or finetuning.</p>
<h2>I. INTRODUCTION</h2>
<p>There has been an emergence of cognitive approaches [1], [2], [3], [4], [5] towards navigation thanks to the advancement of deep learning that substantially improves robot perception. Compared to the traditional mapping, localization and planning approach (SLAM) [6], [7] that builds a metric map, cognitive navigation uses a topological map. This eliminates the need of meticulously reconstructing an environment which requires expensive or bulky hardware such as a laser scanner or a high-resolution camera. Moreover, the fact that humans are able to navigate effortlessly in largescale environments without a metric map is intriguing. By adding this cognitive spatial reasoning capability to robots, we could potentially lower the hardware cost (i.e., using low-resolution cameras), make them work more robustly in dynamic environments and bring insights to more complex tasks such as visual manipulation.</p>
<p>While cognitive navigation has drawn significant attention recently, the problem remains challenging because i) it does not scale well to the size of experiences ii) it is fragile due to actuation noise and dynamic obstacles and iii) it lacks probabilistic interpretation, making it difficult to plan with uncertainty. These problems are exacerbated when using a RGB camera in indoor environments, where partial observability makes it difficult to control a robot to follow a single path $[3],[8]$.</p>
<p>In this paper, we present a simple and intuitive solution for topological navigation. We show that by accurately measuring the capability of a local controller, robust visual topological navigation can be achieved with sparse experiences (Fig.1). In our approach, we do not assume the availability of a global coordinate system or robot poses, nor do we assume noise-free actuation or static environment. This minimalistic representation only has two components: a local controller and a reachability estimator. The controller is responsible for</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Overview of our method. The local controller drives the vehicle towards a given target image, and the reachability estimator plans a path by combining multiple experiences (colored arrows on the map) to provide the controller a sequence of target observations (bottom left) to follow. The vehicle is able to navigate robustly in the real environment (right) while avoiding unseen obstacles (red rectangle and circle). The model is trained entirely in simulation.
local reactive navigation, whereas the reachability estimator measures the capability of the controller for landmark selection and long-term probabilistic planning. To achieve this, we leverage the Riemannian Motion Policy (RMP) framework [9] for robust reactive control and deep learning for learning the capability of the controller from data. We show that with both components working in synergy, a robot can i) navigate robustly with the presence of nonholonomic constraints, actuation noise and obstacles; ii) build a compact spatial memory through adaptive experience sparsification and iii) plan in the topological space probabilistically, allowing robot to generalize to new navigation tasks.</p>
<p>We evaluate our approach in the Gibson simulation environment [10] and on a real RC car. Our test environments contain a diverse set of real-world indoor scenes with presence of strong symmetry and tight spaces. We show that our approach generalizes well to these unseen environments and surprisingly well to real robots without finetuning. Scalability-wise, our spatial memory grows only when new experiences are unseen, making the system spaceefficient and compute-efficient.</p>
<h2>II. Related Work</h2>
<p>Cognitive spatial reasoning has been extensively studied both in neuroscience [11], [12], [13], and robotics [14], [15], [16]. The Spatial Semantic Hierarchy [16] divides the cognitive mapping process into four levels: control, causal, topological and metric. In our method, the local controller operates on the control level, whereas the reachability estimator reasons about causal and topological relationship between observations. We omit metric-level reasoning since we are not concerned about building a metric map.</p>
<p>Experience-driven navigation constructs a topological map for localization and mapping [15], [17], [18], [19], [4]. Unlike SLAM that assumes a static environment, the experience</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: System overview. Given a controller <strong>C</strong>, we train a reachability estimator <strong>RE</strong>. <strong>RE</strong> is used for sparsifying incoming trajectories, building a compact topological map and planning a path. <strong>C</strong> and <strong>RE</strong> work in synergy to robustly follow the planned path.</p>
<p>The graph can also be used for dealing with long-term appearance changes [20]. This line of works mostly focus on appearance-based localization and ignore the control aspect of navigation, and assume that a robot can always follow experiences robustly. This does not usually hold in unstructured indoor environments, where it is crucial to design a good controller while considering its capability.</p>
<p>Semi-Parametric Topological Memory (SPTM) [1], [21] is a recent work that adopts deep learning into topological navigation. Similar to SPTM, we build a topological map through past experiences. Unlike SPTM that uses image similarity as a proxy for reachability, we measure the reachability of a controller directly. This significantly improves robustness and opens opportunities for constructing sparse maps.</p>
<p>There have been recent works studying visual trajectory following that handles obstacles [8], [22], actuation noise [3], or with self-supervision [23]. Our approach differs from them in that our trajectory follower extends seamlessly to probabilistic planning. Our method also handles obstacles and actuation noise well, thanks to the RMP controller that models local geometry and vehicle dynamics.</p>
<p>Recent works on cognitive planning [24], [25] show that a neural planner can be learned from data. However, assumptions such as groundtruth camera poses are available with perfect self-localization are unrealistic. The use of grid map also limits its flexibility. Another line of research uses reinforcement learning to learn a latent map [26], [27], but it is data-inefficient and cannot be easily applied to real robots. In contrast, our planner is general and can adapt to new environments quickly. It bears a resemblance to feedback motion planning system such as LQR-Trees [28], where planning is performed on the topological map connecting reachable state spaces with visual feedback control.</p>
<h2>III. METHOD</h2>
<h3>A. Overview</h3>
<p>We consider the goal-directed navigation problem: a robot is asked to navigate to a goal <strong>G</strong> given an observation <strong>oG</strong> taken at <strong>G</strong>. Robot does not have a map of the environment, but we assume it has collected a set of trajectories (e.g., via self-exploration or following language instructions) as its experiences. Each trajectory is a dense sequence of observations <strong>o1, o2, ..., oN</strong> recorded by its on-board camera. Using its experiences, robot decides the next action to take in order to reach <strong>G</strong>. The action space is continuous (e.g., velocity and steering angle) and robot could be affected by actuation noise and unseen obstacles.</p>
<p>We approach this problem from a cognitive perspective. Robot first builds a topological map from its experiences. The map is a directed graph, with vertices as observations and edges encoding traversability. Then, given its current observation <strong>ocurrent</strong> and goal <strong>oG</strong>, robot searches for a path on the graph and follows that path to reach <strong>G</strong>. Our setup is similar to that of SPTM [1]. The difference is that we design our system to make it generalize to real robots and scale to real environments.</p>
<p>For such a navigation system to work, we first need a target-conditioned <strong>Local Controller</strong> <strong>C</strong>. <strong>C</strong> takes current observation and a target observation, and outputs an action <strong>a = C(ocurrent, otarget)</strong> to drive robot towards the target. The action is executed for a small time step to get an updated <strong>ocurrent</strong> and the process is repeated until <strong>ocurrent</strong> matches <strong>otarget</strong>. Given a path (a sequence of observations) computed by a planner, robot uses <strong>C</strong> to follow the path progressively to reach its final destination.</p>
<p>In practice, robot's experience pool can be large and grow indefinitely, thus the key issue is to build a sparse and scalable representation of an environment given dense, unstructured trajectories. Clearly, adjacent observations in a trajectory is highly correlated and it would be wasteful to keep every observation. One ad-hoc approach to sparsify a trajectory is to take every <strong>nth</strong> observation. However, this assumes that target <strong>n</strong> steps away is always reachable, which is not necessarily true. For example, without occlusion, an observation far away can be confidently reached (e.g., in a straight hallway), whereas an observation nearby may be hidden (thus not reachable) if it is blocked by obstacles. Moreover, motion constraints, sensor field of view, motor noise, etc. can all affect the reachability of a target.</p>
<p>Our intuition is that the sparsification of a trajectory should adapt to the capability of the controller. We propose learning a <strong>Reachability Estimator</strong> <strong>RE</strong> that predicts the probability of <strong>C</strong> successfully reaching a target: <strong>RE(ocurrent, otarget)</strong> = <strong>P(reach|ocurrent, otarget, C)</strong>. We use <strong>RE</strong> as a probabilistic metric throughout the system, illustrated by Fig. 2. Given a controller <strong>C</strong>, we train a corresponding <strong>RE</strong>. The incoming trajectories are first sparsified by <strong>RE</strong> and then interlinked to form a compact topological map. Given <strong>ocurrent</strong> and <strong>oG</strong>, we leverage <strong>RE</strong> to plan a probabilistic path and use <strong>C</strong> and <strong>RE</strong> in synergy to follow the planned path robustly.</p>
<h3>B. Designing a Robust Local Controller</h3>
<p>Real-world robots are subject to disturbances such as motor noise and moving obstacles, which can cause a robot to deviate from planned path and fail. Hence our first objective is to design a sufficiently robust local controller. Contrary to directly predicting low-level controls, we split our controller into two stages: high-level waypoint prediction and low-level</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Architecture of CWP. The architecture of RE is similar, except that it regresses to a single probability and is supervised with cross-entropy loss.</p>
<table>
<thead>
<tr>
<th></th>
<th>Control(k=0)</th>
<th>Ours(k=0)</th>
<th>Ours(k=2)</th>
<th>Ours(k=5)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Success%</td>
<td>46%</td>
<td>88%</td>
<td>91%</td>
<td>95%</td>
</tr>
</tbody>
</table>
<p>TABLE I: Success rate for each controller.</p>
<p>reactive control. The high-level controller CWP predicts a waypoint <em>x</em>, <em>y</em> (in robot's local coordinate system) for the low-level controller. The waypoint needs not be precise, but only serves as a hint for the low-level controller to make progress. Hence CWP is agnostic to robot dynamics (e.g., can be trained with A* waypoints as supervision) and absorbs the effects of actuation noise. For the low-level controller, we adopt the RMP representation [29] as a principled way for obstacle avoidance and vehicle control. Hence we have C(ocurrent, otarget) = C RMP(CWP(ocurrent, otarget)). Note that this allows the same CWP to be applied to different robots by replacing the low-level controller.</p>
<p>Fig. 3 illustrates the design of CWP. The robot state is represented by its current observation ocurrent. Denote <em>i</em>th observation in a trajectory as o<em>i</em>. We represent the corresponding otarget at o<em>i</em> as a sequence of neighbor observations centered at o<em>i</em>:</p>
<p>$$o_{i-k\Delta T}, o_{i-(k-1)\Delta T}, ..., o_{i}, ..., o_{i+(k-1)\Delta T}, o_{i+k\Delta T},$$</p>
<p>where <em>k</em> controls context length and ∆<em>T</em> (set to 3) is the gap between two observations. The past frames expand the field of view of o<em>i</em> which helps controller to do visual closed-loop control. The future frames encode intention at o<em>i</em>, allowing a controller to adjust its waypoint in advance in order to follow subsequent targets smoothly and reliably.</p>
<p>Technically, we extract a feature vector by feeding stacked [ocurrent, oi-k∆T, ocurrent-oi-k∆T] into a sequence of convolutions, followed by combining the 2<em>k</em> + 1 feature vectors through one convolution and multiple fully-connected layers to predict a waypoint <em>x</em>, <em>y</em>. We find this design works much better than featurizing each image or stacking all images together. Additionally, the network predicts the heading difference between ocurrent and o<em>i</em> to help the network anchor the target image in the sequence. Finally, in order to reason about proximity to a target (Sec.III-H), CWP predicts mutual image overlap. Image overlap is a ratio that represents the percentage of content in one image that is visible in another image. Hence mutual image overlap is a pair of ratios (Rcurrent→target, Rtarget→current).</p>
<p>We train CWP in a supervised fashion (Sec.IV). To evaluate our design, we randomly sample ocurrent from a trajectory and otarget being -1.0<em>m</em> behind to 3.0<em>m</em> ahead of ocurrent in 4 unseen environments, and run each controller to see if robot reaches the target. Table I compares the success rate of each controller. Directly predicting low-level controls (forward acceleration and steering velocity) results in much lower success rate than our two-stage design. Compared with directly mapping images to low-level actions, we find it more robust to map images to higher-level abstractions such as waypoints, and then map waypoints to low-level controls using a representation (e.g., RMP) that explicitly models environmental geometry and robot dynamics.</p>
<h3><em>C. Learning the Reachability Estimator</em></h3>
<p>Table I suggests that controller design and parametrization can heavily affect target reachability. Unlike [1] that uses image similarity as a proxy, we learn reachability by explicitly predicting the execution outcome of C. During training, ocurrent and otarget are randomly sampled from demonstration trajectories (Sec. IV) and C is used to drive the robot from ocurrent to otarget to get a binary outcome. The criteria for success is that robot reaches the target within time limit defined as tmax = A<em>(ocurrent, otarget)/vmin, where A</em>(·, ·) computes the A<em> path length and vmin is the minimum velocity. Hence RE measures the probability of C reaching the target </em>efficiently*, which is independent of the temporal and physical distance between ocurrent and otarget. This idea has an interesting connection to feedback motion planning systems [28], as RE can be seen as estimating visual funnels that are locally stable.</p>
<p>The design of RE is almost identical to C, except that it predicts a single probability and is trained with a binary classification loss.</p>
<h3><em>D. Sparsifying a Trajectory</em></h3>
<p>For any observation o<em>i</em> in a dense trajectory, if RE(o<em>i</em>, o<em>i</em>+1), ..., RE(o<em>i</em>, o<em>i</em>+k+1) are sufficiently high, we could confidently discard o<em>i</em>+1, ..., o<em>i</em>+k because C does not need them to reach o<em>i</em>+k+1. Hence a greedy approach to choose the next anchor is</p>
<p>$$\max_{j} \tag{s.t.} \ \RE(o_i, o_k) &gt; p_{\text{sparsify}}, \forall k, i &lt; k \le j$$</p>
<p>where <em>i</em> is previous anchor's position and psparsify is the probability threshold that controls sparsity. Hence a dense trajectory is converted to a sequence of contextified anchor observations ō1, ..., ōm. One may argue that contextification reduces the effective sparsification ratio. Since the time and space complexity is a function of the number of anchors, in practice it significantly saves computation during planning and following a trajectory, allowing our system to run on a robot in real time.</p>
<h3><em>E. Building a Compact Probabilistic Topological Map</em></h3>
<p>Our topological map is a weighted directed graph (Fig. 4a). Vertices are anchor observations and edge weight from ōi to ōj is -log RE(ōi, ōj). Construction is incremental: for an</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: A topological map containing two trajectories. (a) densely connected graph. (b) after pruning low-probability edges. (c) after reusing nodes.</p>
<p>incoming trajectory, we create pairwise edges between every vertex in the graph and every anchor in the new trajectory.</p>
<p>Compared to a graph constructed with dense observations, a graph built from sparsified observations has less than 1/10 of the vertices and 1/100 of the edges. To further improve scalability, we propose the following two optimizations to make the graph grow sublinearly to the number of raw observations, and eventually the size of the graph converges:</p>
<p><strong>Edge pruning.</strong> Low-probability edges with $$ \mathbb{R} \mathbb{E}(\hat{o}<em _text_edge="\text{edge">i, \hat{o}_j) &lt; p</em> $$ are discarded since they contribute little to successful navigation (Fig. 4b).}</p>
<p><strong>Vertex reuse.</strong> It is common for two trajectories to be partially overlapped and storing this overlapping part repeatedly is unnecessary. Hence when adding anchor $$ \hat{o}<em i-1="i-1">i $$ into a graph, we check if there exists a vertex $$ \hat{o} $$ such that $$ \mathbb{R} \mathbb{E}(\hat{o}</em>}, \hat{o}) &gt; p_{\text{reuse}} $$ and $$ \mathbb{R} \mathbb{E}(\hat{o}, \hat{o<em _text_reuse="\text{reuse">{i+1}) &gt; p</em>}} $$. If the condition holds, we discard $$ \hat{o<em i-1="i-1">i $$ and add edges $$ \hat{o}</em> $$, as illustrated in Fig. 4c.} \rightarrow \hat{o} $$ and $$ \hat{o} \rightarrow \hat{o}_{i+1</p>
<p>The graph will converge because for any static environment of finite size, there is a maximum density of anchors. Any additional anchor will pass the vertex reuse check and be discarded. Practically however, an environment may change over time. The solution is to timestamp every observation and discard outdated observations using $$ \mathbb{R} \mathbb{E} $$. We leave the handling of long-term appearance change as future work.</p>
<h3><em>F. Planning</em></h3>
<p>We add an edge (weighted by its negative log probability) from $$ o_{\text{current}} $$ to every vertex in the graph, and from every vertex in the graph to $$ o_G $$. The weighted Dijkstra algorithm computes the path with the lowest negative log probability (i.e., the path that robot is most confident). Robot then decides whether the probability is high enough and may run the trajectory follower proposed in Section III-H.</p>
<h3><em>G. Mitigating Perceptual Aliasing</em></h3>
<p>Practically, $$ o_{\text{current}} $$ may correspond to different locations of similar appearances. Traditional approaches usually formulate this as a POMDP problem [6] and try to resolve the ambiguity by maintaining beliefs over states. This requires having a unique state (e.g., global pose) associated with each observation which is difficult to implement since we do not have any metric information.</p>
<p>We use two techniques to resolve ambiguity. The first is to match a sequence of anchors during search and graph construction. In practice the probability of two segments having similar appearances is much lower than two single observations. Additionally we let robot re-plan a new path if it detects discrepancy (entering <em>Dead reckoning</em> state for too long) while following the previous path. The intuition is that the location where robot detects the discrepancy is likely distinct. See Sec. IV-C.4 for an example. In the worst case where such distinctive anchor is absent, robot might follow a cycle of anchors without making progress. The solution is to count how many times the robot has visited an anchor (i.e., by collecting statistics from <em>last visited anchor</em>). Cyclic behavior can be detected so that the robot can break the loop by biasing its choice in future planning. We leave the handling of this extreme case as future work.</p>
<h3><em>H. Following a Trajectory</em></h3>
<p>Our trajectory follower constantly updates and tracks an active anchor to make progress, while performing dead reckoning to counter local disturbances. Specifically, given a sequence of anchor observations $$ \hat{o}_1, \hat{o}_2, ..., \hat{o}_m $$, the trajectory follower acts as a state machine:</p>
<p><strong>Search</strong>: robot searches for the best anchor: $$ \hat{o}^<em> = \arg\max_{o \in { \hat{o}<em _text_current="\text{current">1, ..., \hat{o}_m }} \mathbb{R} \mathbb{E}(o</em>^}}, o) $$. If $$ \mathbb{R} \mathbb{E}(o_{\text{current}}, \hat{o</em>) &gt; p_{\text{search}} $$, it sets $$ \hat{o}^<em> $$ as current active anchor $$ \hat{o}_{\text{active}} $$ and enters </em>Follow* state, otherwise it gives up and stops.</p>
<p><strong>Follow</strong>: robot computes the next waypoint $$ x, y = \mathbb{C}<em _text_current="\text{current">{\text{WP}}(o</em>}}, \hat{o<em _text_RMP="\text{RMP">{\text{active}}) $$ and uses it to drive $$ \mathbb{C}</em> $$. Meanwhile it tracks and updates the following two values:}</p>
<ul>
<li><em>last visited anchor</em>. Robot uses the predicted mutual image overlap to measure the proximity between $$ o_{\text{current}} $$ and anchors close to $$ \hat{o}<em _text_lastvisited="\text{lastvisited">{\text{active}} $$. The closest anchor is set as $$ o</em> $$. This is a form of approximate localization.}</li>
<li><em>active anchor</em>. If $$ \mathbb{R} \mathbb{E}(o_{\text{current}}, \hat{o}<em _text_follow="\text{follow">{\text{active} + 1}) &gt; p</em>}} $$ and is within proximity, it advances $$ \hat{o<em _text_active="\text{active">{\text{active}} $$ to $$ \hat{o}</em>} + 1} $$, otherwise $$ \hat{o<em _text_lastvisited="\text{lastvisited">{\text{active}} = o</em> $$ that is neither too close nor too far away.} + 1} $$. The intuition is to choose an $$ \hat{o}_{\text{active}</li>
</ul>
<p>Normally robot stays in <em>Follow</em> state. But if moving obstacles or actuation noise cause $$ \mathbb{R} \mathbb{E}(o_{\text{current}}, \hat{o}<em _text_follow="\text{follow">{\text{active}}) &lt; p</em> $$, it enters }<em>Dead reckoning</em> state.</p>
<p><strong>Dead reckoning</strong>: robot tracks the last waypoint computed in the <em>Follow</em> state and uses the waypoint to drive $$ \mathbb{C}<em _text_current="\text{current">{\text{RMP}} $$. The assumption is that disturbances are transient which the robot could escape by following the last successfully computed waypoint. Waypoint tracking can be done by an odometer and needs not be very accurate. While in this state, robot keeps checking if $$ \mathbb{R} \mathbb{E}(o</em>}}, \hat{o<em _text_follow="\text{follow">{\text{active}}) &gt; p</em> $$ and returns to }<em>Follow</em> state if possible.</p>
<h3>IV. EXPERIMENTS</h3>
<p>We trained $$ \mathbb{C}<em _text_RMP="\text{RMP">{\text{WP}} $$, $$ \mathbb{R} \mathbb{E} $$ and all baselines in 12 Gibson environments. 100k training trajectories were generated by running an A<sup></sup> planner (used to provide waypoints) with a laser RMP controller similar to [29]. Simulation step size is 0.1. We use the laser RMP controller as $$ \mathbb{C}</em>}} $$ mostly for efficiency, but in practice an image-based RMP controller can also be used [29]. $$ \mathbb{C<em _text_WP="\text{WP">{\text{WP}} $$ was trained by randomly sampling two images on the same trajectory with certain visual overlap, with the A<sup></sup> waypoint as supervision. After $$ \mathbb{C}</em> $$ to get a binary outcome. Image size is 64 × 64 with 120° horizontal field of view. We augmented the dataset by jittering robot's}} $$ was trained, we trained $$ \mathbb{R} \mathbb{E} $$ by sampling two images that either belong to the same trajectory (prob 0.6) or different trajectories (prob 0.4), and ran a rollout with $$ \mathbb{C</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Trajectory sparsification. Fig. 6: Following a 23m Blue dots: dense observations. long trajectory. Blue trace: Images correspond to numbered groundtruth trajectory. Orange locations. 1D laser scans are visualized from the top view. trace is generated by following the anchor points.</p>
<p>Starting location and orientation to improve generalization. About 1.5M samples were used to train $\mathbb{C}_{\text{WP}}$ and $\mathbb{R}\mathbb{E}$. Our training setup models a real vehicle similar to [30], so that the same model can be used for real experiments.</p>
<p>We present quantitative results in 5 unseen Gibson environments with diverse appearances. Our baseline is based on SPTM. Since SPTM is designed for small synthetic mazes with discrete action space, its original version would perform poorly in our setting. For a fair comparison, we let SPTM use the same controller and trajectory following logic as ours. The main differences between SPTM and ours are thus: i) how reachability is learned and ii) how graph is constructed and used. Our ablation study will thus be in the form of evaluating trajectory following and planning performance in the following sections.</p>
<h3>A. Trajectory Sparsification</h3>
<p>Fig. 5 compares sparsification results of three controllers. The two visual controllers $\mathbb{C}<em k="5">{k=2}$, $\mathbb{C}</em>$ 1D depth as input.}$ differ in their context length. To show that our model is general, we also trained a laser-based controller $\mathbb{C}_{\text{laser}}$ by modifying the input layer in Fig. 3 to take 64-point $240^{\circ</p>
<p>Fig. 5 shows placement of anchors with $p_{\text{sparsify}}=0.99$. Comparing with $\mathbb{C}<em k="2">{k=5}$, $\mathbb{C}</em>}$ requires denser anchors. Since $\mathbb{C<em _text_laser="\text{laser">{k=2}$ uses a shorter context, it is more "local" and has to keep more anchors to follow a path robustly. Nonetheless, anchors are more densely distributed in tight spaces and corners for both controllers, indicating that our sparsification strategy adapts well to environmental geometry. Interestingly, $\mathbb{C}</em>$ shows a more uniform distribution pattern. Since laser scans have a much wider field of view and measures geometry directly, it is not heavily affected by tight spaces and large viewpoint change.}</p>
<h3>B. Trajectory Following</h3>
<p>We randomly generated 500 trajectories in the test environments (Fig. 6) with an average length of 15 m. When following a trajectory, we stop the robot when it diverges from the path or collides with obstacles. We report the cover rate, the percentage of total length of trajectories successfully followed by robot. For our trajectory followers, $p_{\text{search}}=$ $p_{\text{follow}} = 0.92$.</p>
<p>Sparsity is the average ratio of number of images in a sparsified trajectory to the number of images in the corresponding</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 7: Trajectory following cover rate in 5 test environments. Number after $k$ indicates the context length. Data points for Ours-k5 and Ours-k2 are marked with $p_{\text{sparsify}}$.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 8: Example topological maps built from sparsified trajectories. Each trajectory is assigned a different color.</p>
<p>dense trajectory. To change sparsity, we vary $p_{\text{sparsify}}$ for our models. For SPTM we select every $n$th frame and vary $n$. Fig. 7 plots cover rates for varying sparsity conditions. Controllers with contexts (<em>-k2, </em>-k5) achieve higher than 95% cover rate, better than controllers without context (at most 90%). This indicates that having contextual frames can improve robustness. But since contextual frames are used, more observations have to be kept so storage-wise it is not as efficient as (*-k0).</p>
<p>SPTM performs comparably to ours when using a strong controller (<em>-k5), but for all controllers it starts to degrade before ours as sparsity lowers. Due to its fixed-interval subsampling, it does not adapt to controllers' capability well, as can be seen by the increasing gap between ours and the SPTM counterparts when less contextual frames are used (</em>-k2, *-k0).</p>
<p>We also evaluated performance under noisy actuation by multiplying a random scaling factor $s \sim \mathcal{N}(1.0, 0.33)$ to the control output. No noticeable difference was found. This is expected because the local controller runs at a high frequency (10 Hz) and uses visual feedback for closed-loop control.</p>
<h3>C. Planning</h3>
<p>1) Navigation between Places: We built one topological map for each environment (Fig. 8a). A map is constructed from 90 trajectories connecting 10 locations in a pairwise fashion. The locations are selected to make the trajectories cover most of the reachable area.</p>
<p>Robot starts at one of the locations (with jittered position and orientation) and is given an goal image taken at one of the other 9 destinations. Robot has no prior knowledge of its initial location. We re-implemented SPTM's planner and uses the best trajectory follower SPTM-k5 (SecIV-B) to make it a competitive baseline. We set the sub-sampling ratio to 20 and $\Delta T_{l}=1$ to prevent the graph from getting too large.</p>
<table>
<thead>
<tr>
<th></th>
<th>space8</th>
<th>house24</th>
<th>house29</th>
<th>house75</th>
<th>house79</th>
</tr>
</thead>
<tbody>
<tr>
<td>Area</td>
<td>$460m^{2}$</td>
<td>$207m^{2}$</td>
<td>$270m^{2}$</td>
<td>$403m^{2}$</td>
<td>$205m^{2}$</td>
</tr>
<tr>
<td>Images</td>
<td>30,342</td>
<td>31,167</td>
<td>28,679</td>
<td>39,788</td>
<td>33,617</td>
</tr>
<tr>
<td>SPTM</td>
<td>1,648/3,201</td>
<td>1,688/3,668</td>
<td>1,560/3,960</td>
<td>2,116/4,115</td>
<td>1,808/4,756</td>
</tr>
<tr>
<td></td>
<td>48.1%</td>
<td>40.2%</td>
<td>45.6%</td>
<td>51.3%</td>
<td>47.2%</td>
</tr>
<tr>
<td>Ours</td>
<td>974/1,482</td>
<td>900/1,348</td>
<td>901/1,467</td>
<td>1,454/2,275</td>
<td>909/1,524</td>
</tr>
<tr>
<td></td>
<td>86.9%</td>
<td>94.3%</td>
<td>91.2%</td>
<td>84.6%</td>
<td>95.7%</td>
</tr>
</tbody>
</table>
<p>TABLE II: Planning success rate, with #vertices/#edges shown above. Success rate is the outcome of 1,000 navigation trials.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 9: Comparing path probability to empirical success rate. Each bar is the average success rate of paths whose probabilities fall into that range.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 10: Success rate with pruned graphs. Dotted line shows no-generalization for reference.</p>
<p>We performed a hyperparameter search to set s shortcut = 0.99. For our method, we set preuse = p edge = 0.99.</p>
<p>Table II presents the success rate for each environment compared to SPTM. Our method outperforms SPTM with much sparser maps. Graphs built by SPTM have unweighted edges and do not reuse vertices. SPTM also does explicit localization which sometimes causes planning failure. This results in worse scalability and reliability compared with our approach. Note that the slightly lower success rates in space8 and house75 are mostly caused by strong symmetry and rendering artefacts.</p>
<p>2) Comparing Trajectory Probabilities to Empirical Success Rate: To show that path probability is a reasonable indicator of empirical outcome, we let a robot start at a random location (anywhere in a map), plan a path to one of the 10 destinations, and follow the path. 1,000 trajectories were collected in each environment. Fig. 10 shows that path probability strongly corresponds to empirical success rate. This allows a robot to assess the risk before executing a plan, and ask for help if necessary. Note that SPTM does not provide any uncertainty measure.</p>
<p>3) Generalizing to New Navigation Tasks: To test the generalizability of our planner, we randomly pruned the graphs to contain only a subset of the trajectories, and repeated the experiment in IV-C1. Fig. 10 shows that with only 60% of the trajectories, robot already performs close to its peak success rate. In other words, robot is able to combine existing trajectories to solve novel navigation tasks. Fig. 11 shows an example.</p>
<p>4) Resolving Ambiguity: Fig. 12 illustrates how perceptual aliasing is resolved in environments with strong symmetry. Robot initially starts at an ambiguous location (marked "1") and plans a wrong path (red path). While following this path, robot detects the discrepancy at "2" by realizing what is expected to be an office room is actually a hallway. As</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 11: Plans a path from G1 to G4 by combining G1 → G2 and G3 → G4</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Fig. 12: Online planning. Arrows indicate headings. 1a and 2a are the next anchor observations for the two paths respectively.</p>
<p>a result, it plans a new path (green) whereby it successfully reaches the goal.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Fig. 13: Dotted lines show 10x and 20x temporal sub-sampling for reference.</p>
<p>5) Scalability: Fig. 13 shows that map sizes grow sublinearly to the number of raw observations, making our approach scalable to large environments. It also shows that map size is adaptive to an environment. Since house75 has more complex geometry and exhibits more rendering artefacts, denser samples are kept to stay conservative.</p>
<h3>D. Testing in a Real Environment</h3>
<p>Our model trained in simulation generalizes well to real images without finetuning. To map a real environment, we manually drove the RC car to collect 7 trajectories, totalling 3,200 images. The final map contains 206 vertices and 215 edges (Fig.8b). The car is able to plan novel paths between locations and follow the path while avoiding obstacles not seen during mapping (Fig.1). We refer the interested reader to the video supplementary material for more examples.</p>
<h2>V. CONCLUSION</h2>
<p>In this work, we show that by learning the capability of a local controller, robust and scalable visual topological navigation can be achieved. Due to the simplicity and flexibility of our framework, it can be extended to support non-visual sensors and applied to other robotics problems. Future works include combining multiple sensors to improve the controller, developing better algorithms to resolve ambiguity, improving generalization, and extending to manipulation tasks.</p>
<p>The hyperparameters in our approach are mostly probability thresholds, which are easy to interpret and tune. One important scenario our approach does not handle is when robot deviates too much from all vertices in the navigation graph, where it would fail to find a plausible path. A self-exploratory model can help here, and it can also be used for autonomous map construction.</p>
<h2>VI. ACKNOWLEDGEMENTS</h2>
<p>This work was funded in part by ONR grant 63-6094 and by the Honda Curious Minded Sponsored Research Agreement. We thank NVIDIA for generously providing a DGX used for this research via the NVIDIA Robotics Lab and the UW NVIDIA AI Lab (NVAIL).</p>
<h2>REFERENCES</h2>
<p>[1] N. Savinov, A. Dosovitskiy, and V. Koltun, "Semi-parametric topological memory for navigation," in International Conference on Learning Representations (ICLR), 2018.
[2] Y. Wu, Y. Wu, A. Tamar, S. Russell, G. Gkioxari, and Y. Tian, "Bayesian relational memory for semantic visual navigation," in International Conference on Computer Vision (ICCV), 2019.
[3] A. Kumar, S. Gupta, D. Fouhey, S. Levine, and J. Malik, "Visual memory for robust path following," in Advances in Neural Information Processing Systems, 2018, pp. 765-774.
[4] F. Blochliger, M. Fehr, M. Dymczyk, T. Schneider, and R. Siegwart, "Topomap: Topological mapping and navigation based on visual slam maps," in IEEE International Conference on Robotics and Automation (ICRA), 2018.
[5] K. Chen, J. P. de Vicente, G. Sepulveda, F. Xia, A. Soto, M. Vazquez, and S. Savarese, "A behavioral approach to visual navigation with graph localization networks," Robotics Science and Systems (RSS), 2019.
[6] S. Thrun, W. Burgard, and D. Fox, Probabilistic robotics. MIT press, 2005.
[7] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, "Orb-slam: a versatile and accurate monocular slam system," IEEE Transactions on Robotics, vol. 31, no. 5, pp. 1147-1163, 2015.
[8] N. Hirose, F. Xia, R. Martn-Martn, A. Sadeghian, and S. Savarese, "Deep visual mpc-policy learning for navigation," IEEE Robotics and Automation Letters, vol. 4, no. 4, pp. 3184-3191, 2019.
[9] N. D. Ratliff, J. Issac, and D. Kappler, "Riemannian motion policies," CoRR, vol. abs/1801.02854, 2018.
[10] F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese, "Gibson env: Real-world perception for embodied agents," in IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 9068-9079.
[11] J. O'keefe and L. Nadel, The hippocampus as a cognitive map. Oxford: Clarendon Press, 1978.
[12] T. Hafting, M. Fyhn, S. Molden, M.-B. Moser, and E. I. Moser, "Microstructure of a spatial map in the entorhinal cortex," Nature, vol. 436, no. 7052, p. 801, 2005.
[13] C. F. Doeller, C. Barry, and N. Burgess, "Evidence for grid cells in a human memory network," Nature, vol. 463, no. 7281, p. 657, 2010.
[14] S. Thrun, "Learning metric-topological maps for indoor mobile robot navigation," Artificial Intelligence, vol. 99, no. 1, pp. 21-71, 1998.
[15] M. J. Milford, G. F. Wyeth, and D. Prasser, "Ratslam: a hippocampal model for simultaneous localization and mapping," in IEEE International Conference on Robotics and Automation (ICRA), vol. 1, 2004, pp. 403-408.
[16] B. Kuipers, "The spatial semantic hierarchy," Artificial intelligence, vol. 119, no. 1-2, pp. 191-233, 2000.
[17] W. Maddern, M. Milford, and G. Wyeth, "Capping computation time and storage requirements for appearance-based localization with cat-slam," in 2012 IEEE International Conference on Robotics and Automation (ICRA), 2012, pp. 822-827.
[18] F. Fraundorfer, C. Engels, and D. Nistér, "Topological mapping, localization and navigation using image collections," in International Conference on Intelligent Robots and Systems (IROS), 2007.
[19] M. Cummins and P. Newman, "Appearance-only slam at large scale with fab-map 2.0," The International Journal of Robotics Research, 2011.
[20] C. Linegar, W. Churchill, and P. Newman, "Work smart, not hard: Recalling relevant experiences for vast-scale but time-constrained localisation," in IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2015, pp. 90-97.
[21] N. Savinov, A. Raichuk, R. Marinier, D. Vincent, M. Pollefeys, T. Lillicrap, and S. Gelly, "Episodic curiosity through reachability," in International Conference on Learning Representations (ICLR), 2019.
[22] S. Bansal, V. Tolani, S. Gupta, J. Malik, and C. Tomlin, "Combining optimal control and learning for visual navigation in novel environments," Conference on Robot Learning (CoRL), 2019.
[23] D. Pathak, P. Mahmoudieh, G. Luo, P. Agrawal, D. Chen, Y. Shentu, E. Shelhamer, J. Malik, A. A. Efros, and T. Darrell, "Zero-shot visual imitation," in International Conference on Learning Representations (ICLR), 2018.
[24] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik, "Cognitive mapping and planning for visual navigation," in IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 7272-7281.
[25] S. Gupta, D. Fouhey, S. Levine, and J. Malik, "Unifying map and landmark based representations for visual navigation," arXiv preprint arXiv:1712.08125, 2017.
[26] P. Mirowski, M. Grimes, M. Malinowski, K. M. Hermann, K. Anderson, D. Teplyashin, K. Simonyan, A. Zisserman, R. Hadsell, et al., "Learning to navigate in cities without a map," in Advances in Neural Information Processing Systems, 2018, pp. 2419-2430.
[27] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino, M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, et al., "Learning to navigate in complex environments," International Conference on Learning Representations (ICLR), 2018.
[28] R. Tedrake, I. R. Manchester, M. Tobenkin, and J. W. Roberts, "Lqrtrees: Feedback motion planning via sums-of-squares verification," The International Journal of Robotics Research, vol. 29, no. 8, pp. 10381052, 2010.
[29] X. Meng, N. Ratliff, Y. Xiang, and D. Fox, "Neural autonomous navigation with riemannian motion policy," in IEEE International Conference on Robotics and Automation (ICRA), 2019.
[30] "MIT racecar," 2018. [Online]. Available: https://mit-racecar.github.io/</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Xiangyun Meng and Dieter Fox are with the Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington, Seattle, WA 98195, USA {xiangyun, fox}@cs.washington.edu</p>
<p>Nathan Ratliff, Yu Xiang and Dieter Fox are with NVIDIA, Seattle, WA 98105, USA { nratliff, yux, dieterf}@nvidia.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
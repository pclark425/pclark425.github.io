<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8471 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8471</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8471</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-a5cea6716378949a2b73f0401237d29791a6ee6c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a5cea6716378949a2b73f0401237d29791a6ee6c" target="_blank">Offline RL for Natural Language Generation with Implicit Language Q Learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel offline RL method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the flexible utility maximization framework of RL algorithms with the ability of supervised learning to leverage previously collected data, as well as its simplicity and stability.</p>
                <p><strong>Paper Abstract:</strong> Large language models distill broad knowledge from text corpora. However, they can be inconsistent when it comes to completing user specified tasks. This issue can be addressed by finetuning such models via supervised learning on curated datasets, or via reinforcement learning. In this work, we propose a novel offline RL method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the flexible utility maximization framework of RL algorithms with the ability of supervised learning to leverage previously collected data, as well as its simplicity and stability. Our method employs a combination of value conservatism alongside an implicit dataset support constraint in learning value functions, which are then used to guide language model generations towards maximizing user-specified utility functions. In addition to empirically validating ILQL, we present a detailed empirical analysis of situations where offline RL can be useful in natural language generation settings, demonstrating how it can be a more effective utility optimizer than prior approaches for end-to-end dialogue, and how it can effectively optimize high variance reward functions based on subjective judgement, such as whether to label a comment as toxic or not.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8471.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8471.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ILQL-Wordle</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implicit Language Q-Learning agent evaluated on Wordle</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ILQL-trained transformer agent (GPT-2 small) that plays the Wordle text game by treating generation as a token-level POMDP; value functions (Q and V) are predicted per-token from the full token history and used to perturb a supervised behavior model at decode time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ILQL Wordle agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A token-level RL agent implementing ILQL: (1) a supervised behavior model pi_beta (GPT-2 small) finetuned on dataset behavior, (2) a value network (shared transformer with two Q heads and one V head) trained with expectile regression and Bellman backups over token sequences, (3) a Polyak-averaged target network. At inference the agent perturbs pi_beta logits by beta*(Q-V) and samples/greedily decodes.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 small</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-2 small transformer (pretrained weights used for initialization in most experiments); standard causal-transformer architecture with a context window; value heads are MLPs on top of transformer token embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Wordle</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Word-guessing text game where agent has up to 6 attempts to guess a hidden 5-letter word; each guess yields per-letter feedback (colors). Sequential, multi-step decision making with partial observability (history of guesses and color feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory (transformer context / token history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory is the concatenated token history h_t (sequence of previous tokens and environment feedback) used as the transformer's input/context window; Q and V are predicted at each token position from that contextualized representation. No separate external memory module is introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>History (token sequence) is concatenated as the observation input to the transformer; the transformer (causal mask) produces contextualized token representations used by two Q heads and a V head; policy extraction perturbs supervised logits using per-token advantages (Q-V).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Wordle average score: -2.13 ± 0.03 (lower is better, Table 6)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Paper compares ILQL to baselines on Wordle: single-step RL (-2.23 ±0.03), filtered fine-tuning (-2.38 ±0.03), fine-tuning (-2.61 ±0.03), and an upper-bound policy (-1.75 ±0.02). Ablation demonstrates ILQL's multi-step value learning advantage over single-step methods in synthetic distributions designed to expose 'misleading states'. No ablation on explicit memory vs no-memory.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Use the transformer context (per-token token history) as the agent's working memory and perform token-level value learning (per-token Q and V heads) with expectile-based implicit support; extract policy by perturbing a supervised pi_beta with advantage (Q-V).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No explicit external memory module; all memory is bounded by the transformer context window. Paper does not study explicit memory modules or longer-term persistent memory beyond context length. ILQL can be more computationally expensive (requires multiple transformers during training) and may be less effective if datasets are extremely suboptimal.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>For text-game style sequential tasks, prefer per-token RL (token-level POMDP) using sequence-model context as memory, apply expectile-based value fitting (ILQL) for multi-step composition, and perform policy extraction by perturbing a supervised behavior model with Q-V; tune beta at inference to trade off optimization vs diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Offline RL for Natural Language Generation with Implicit Language Q Learning', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8471.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8471.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ILQL-VisualDialogue</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implicit Language Q-Learning agent evaluated on Visual Dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ILQL agent applied to a goal-directed question-asking dialogue benchmark (Visual Dialogue) that uses the transformer's token history as the agent's memory and optimizes diverse reward functions (standard, y/n penalty, conservative y/n) via offline RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ILQL Visual Dialogue agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same ILQL architecture: GPT-2 small as pi_beta and value transformer with two Q heads and one V head; token-level Bellman backups are applied to whole sequences (history) to learn Q/V; policy extraction perturbs pi_beta logits by advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 small</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Causal transformer initialized from GPT-2 small; value heads are 2-layer MLPs on top of token embeddings; target networks use Polyak averaging.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Visual Dialogue (goal-directed question asking)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A multi-turn dialogue game where an agent asks questions about an image to allow an environment model to predict the ground-truth image embedding; episodes end when the image can be predicted well or when dialogue exhausts turns; reward is per-turn negative if prediction not improved.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory (transformer context / token history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Observation is the entire dialogue token history (questions, answers, image caption embedding included in reward computation); transformer context provides memory; Q and V heads compute per-token values conditioned on full history.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Dialogue history is concatenated as input to the transformer; Bellman backups and expectile regression operate over token sequences; policy extraction perturbs pi_beta token logits by Q(h,a)-V(h).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Examples: Visual Dialogue 'standard' reward ILQL: -5.21 ± 0.13; 'y/n' ILQL: -5.57 ± 0.13; 'conservative y/n' ILQL: -6.57 ± 0.18 (Table 1 left). Also Table 5 reports ILQL max score -5.57 ± 0.13 on 'y/n' reward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Extensive ablations: per-token ILQL vs per-utterance ILQL, per-utterance single-step RL, CHAI; per-token ILQL is ~4x faster at inference and outperforms or matches per-utterance baselines. Also compared policy extraction strategies (ILQL's perturbation vs AWR extraction vs GOLD) and found ILQL extraction more stable and effective.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Per-token action space with transformer context as working memory produced best trade-off of speed and performance; combine expectile-based value learning, CQL-style regularizer on Q to push down OOD Q-values, and perturbation-based policy extraction on pi_beta.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Data is near-optimal for 'standard' reward so gains are smaller; ILQL requires tuning of CQL regularizer alpha for some settings, and training uses multiple transformers increasing computational cost; paper does not evaluate explicit long-term memory beyond context window.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Use token-level ILQL with transformer context as memory for dialogue tasks, prefer ILQL's perturbation-based policy extraction over AWR/GOLD for stability and easier tuning, and use the CQL regularizer to mitigate OOD Q-values; tune beta at inference to balance optimization versus diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Offline RL for Natural Language Generation with Implicit Language Q Learning', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8471.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8471.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ILQL-Reddit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implicit Language Q-Learning agent evaluated on Reddit comment generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ILQL agent trained on 4M Reddit comments to optimize subjective, high-variance rewards (toxicity filter, upvote labels) using token-history as memory; ILQL learns safer/high-upvote replies compared to filtered finetuning and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ILQL Reddit agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>ILQL architecture with GPT-2 small supervised policy and value transformer; value heads predict per-token Q and V from concatenated parent comment + reply history; policy extraction perturbs pi_beta logits by advantage to avoid risky tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 small</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Pretrained causal transformer used as pi_beta and as backbone for value heads; token-level context window forms the agent's working memory.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Reddit comment generation (toxicity and upvote rewards)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Open-domain reply generation with highly stochastic, subjective rewards: toxicity filter labels (-10/-5/0) and binary upvote reward (+10/0). Evaluated both on real upvotes and a RoBERTa-based upvote predictor.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory (transformer context / token history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Input is parent comment + generated reply tokens (history) inside transformer context; Q and V heads produce token-level values conditioned on this history. No separate external memory store used.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Full token history concatenated as transformer input; per-token Q/V computed from context; at inference, pi_beta logits are adjusted by beta*(Q-V) to favor safer/higher-reward completions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Toxicity reward: ILQL produced 0.0 toxic outputs (perfect) on main setting; Upvotes real: ILQL 9.83 ± 0.04; Upvotes model: ILQL 10.0 ± 0.0 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to baselines: single-step RL, filtered fine-tuning (top-reward BC), fine-tuning. ILQL outperforms filtered finetuning and fine-tuning on toxicity and upvote rewards. Paper includes an experiment adding noise to toxicity labels showing ILQL remains robust where filtered BC degrades.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Rely on transformer's context (token history) as working memory and learn expected Q-values to represent reward uncertainty; avoid naive filtered BC in stochastic reward regimes by using value-based offline RL (ILQL) with CQL regularizer.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No explicit long-term memory beyond context window; paper does not evaluate integrated multi-session episodic memory. ILQL requires multiple transformer networks during training and tuning of some hyperparameters (tau, alpha, beta) though it is more stable than many baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>To optimize subjective/noisy rewards, use ILQL's value-based offline RL with transformer context as the memory and a CQL-like regularizer to reduce OOD Q-values; prefer ILQL to filtered finetuning in high-variance reward settings and tune beta to trade off diversity vs optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Offline RL for Natural Language Generation with Implicit Language Q Learning', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Offline reinforcement learning with implicit q-learning <em>(Rating: 2)</em></li>
                <li>Decision transformer: Reinforcement learning via sequence modeling <em>(Rating: 2)</em></li>
                <li>Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning <em>(Rating: 2)</em></li>
                <li>Human-centric dialog training via offline reinforcement learning <em>(Rating: 2)</em></li>
                <li>Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8471",
    "paper_id": "paper-a5cea6716378949a2b73f0401237d29791a6ee6c",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "ILQL-Wordle",
            "name_full": "Implicit Language Q-Learning agent evaluated on Wordle",
            "brief_description": "An ILQL-trained transformer agent (GPT-2 small) that plays the Wordle text game by treating generation as a token-level POMDP; value functions (Q and V) are predicted per-token from the full token history and used to perturb a supervised behavior model at decode time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ILQL Wordle agent",
            "agent_description": "A token-level RL agent implementing ILQL: (1) a supervised behavior model pi_beta (GPT-2 small) finetuned on dataset behavior, (2) a value network (shared transformer with two Q heads and one V head) trained with expectile regression and Bellman backups over token sequences, (3) a Polyak-averaged target network. At inference the agent perturbs pi_beta logits by beta*(Q-V) and samples/greedily decodes.",
            "llm_model_name": "GPT-2 small",
            "llm_model_description": "GPT-2 small transformer (pretrained weights used for initialization in most experiments); standard causal-transformer architecture with a context window; value heads are MLPs on top of transformer token embeddings.",
            "benchmark_name": "Wordle",
            "benchmark_description": "Word-guessing text game where agent has up to 6 attempts to guess a hidden 5-letter word; each guess yields per-letter feedback (colors). Sequential, multi-step decision making with partial observability (history of guesses and color feedback).",
            "memory_used": true,
            "memory_type": "working memory (transformer context / token history)",
            "memory_architecture": "Memory is the concatenated token history h_t (sequence of previous tokens and environment feedback) used as the transformer's input/context window; Q and V are predicted at each token position from that contextualized representation. No separate external memory module is introduced.",
            "memory_integration_strategy": "History (token sequence) is concatenated as the observation input to the transformer; the transformer (causal mask) produces contextualized token representations used by two Q heads and a V head; policy extraction perturbs supervised logits using per-token advantages (Q-V).",
            "performance_with_memory": "Wordle average score: -2.13 ± 0.03 (lower is better, Table 6)",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Paper compares ILQL to baselines on Wordle: single-step RL (-2.23 ±0.03), filtered fine-tuning (-2.38 ±0.03), fine-tuning (-2.61 ±0.03), and an upper-bound policy (-1.75 ±0.02). Ablation demonstrates ILQL's multi-step value learning advantage over single-step methods in synthetic distributions designed to expose 'misleading states'. No ablation on explicit memory vs no-memory.",
            "best_memory_strategy": "Use the transformer context (per-token token history) as the agent's working memory and perform token-level value learning (per-token Q and V heads) with expectile-based implicit support; extract policy by perturbing a supervised pi_beta with advantage (Q-V).",
            "limitations_or_failure_cases": "No explicit external memory module; all memory is bounded by the transformer context window. Paper does not study explicit memory modules or longer-term persistent memory beyond context length. ILQL can be more computationally expensive (requires multiple transformers during training) and may be less effective if datasets are extremely suboptimal.",
            "recommendations_or_conclusions": "For text-game style sequential tasks, prefer per-token RL (token-level POMDP) using sequence-model context as memory, apply expectile-based value fitting (ILQL) for multi-step composition, and perform policy extraction by perturbing a supervised behavior model with Q-V; tune beta at inference to trade off optimization vs diversity.",
            "uuid": "e8471.0",
            "source_info": {
                "paper_title": "Offline RL for Natural Language Generation with Implicit Language Q Learning",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "ILQL-VisualDialogue",
            "name_full": "Implicit Language Q-Learning agent evaluated on Visual Dialogue",
            "brief_description": "An ILQL agent applied to a goal-directed question-asking dialogue benchmark (Visual Dialogue) that uses the transformer's token history as the agent's memory and optimizes diverse reward functions (standard, y/n penalty, conservative y/n) via offline RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ILQL Visual Dialogue agent",
            "agent_description": "Same ILQL architecture: GPT-2 small as pi_beta and value transformer with two Q heads and one V head; token-level Bellman backups are applied to whole sequences (history) to learn Q/V; policy extraction perturbs pi_beta logits by advantage.",
            "llm_model_name": "GPT-2 small",
            "llm_model_description": "Causal transformer initialized from GPT-2 small; value heads are 2-layer MLPs on top of token embeddings; target networks use Polyak averaging.",
            "benchmark_name": "Visual Dialogue (goal-directed question asking)",
            "benchmark_description": "A multi-turn dialogue game where an agent asks questions about an image to allow an environment model to predict the ground-truth image embedding; episodes end when the image can be predicted well or when dialogue exhausts turns; reward is per-turn negative if prediction not improved.",
            "memory_used": true,
            "memory_type": "working memory (transformer context / token history)",
            "memory_architecture": "Observation is the entire dialogue token history (questions, answers, image caption embedding included in reward computation); transformer context provides memory; Q and V heads compute per-token values conditioned on full history.",
            "memory_integration_strategy": "Dialogue history is concatenated as input to the transformer; Bellman backups and expectile regression operate over token sequences; policy extraction perturbs pi_beta token logits by Q(h,a)-V(h).",
            "performance_with_memory": "Examples: Visual Dialogue 'standard' reward ILQL: -5.21 ± 0.13; 'y/n' ILQL: -5.57 ± 0.13; 'conservative y/n' ILQL: -6.57 ± 0.18 (Table 1 left). Also Table 5 reports ILQL max score -5.57 ± 0.13 on 'y/n' reward.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Extensive ablations: per-token ILQL vs per-utterance ILQL, per-utterance single-step RL, CHAI; per-token ILQL is ~4x faster at inference and outperforms or matches per-utterance baselines. Also compared policy extraction strategies (ILQL's perturbation vs AWR extraction vs GOLD) and found ILQL extraction more stable and effective.",
            "best_memory_strategy": "Per-token action space with transformer context as working memory produced best trade-off of speed and performance; combine expectile-based value learning, CQL-style regularizer on Q to push down OOD Q-values, and perturbation-based policy extraction on pi_beta.",
            "limitations_or_failure_cases": "Data is near-optimal for 'standard' reward so gains are smaller; ILQL requires tuning of CQL regularizer alpha for some settings, and training uses multiple transformers increasing computational cost; paper does not evaluate explicit long-term memory beyond context window.",
            "recommendations_or_conclusions": "Use token-level ILQL with transformer context as memory for dialogue tasks, prefer ILQL's perturbation-based policy extraction over AWR/GOLD for stability and easier tuning, and use the CQL regularizer to mitigate OOD Q-values; tune beta at inference to balance optimization versus diversity.",
            "uuid": "e8471.1",
            "source_info": {
                "paper_title": "Offline RL for Natural Language Generation with Implicit Language Q Learning",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "ILQL-Reddit",
            "name_full": "Implicit Language Q-Learning agent evaluated on Reddit comment generation",
            "brief_description": "An ILQL agent trained on 4M Reddit comments to optimize subjective, high-variance rewards (toxicity filter, upvote labels) using token-history as memory; ILQL learns safer/high-upvote replies compared to filtered finetuning and other baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ILQL Reddit agent",
            "agent_description": "ILQL architecture with GPT-2 small supervised policy and value transformer; value heads predict per-token Q and V from concatenated parent comment + reply history; policy extraction perturbs pi_beta logits by advantage to avoid risky tokens.",
            "llm_model_name": "GPT-2 small",
            "llm_model_description": "Pretrained causal transformer used as pi_beta and as backbone for value heads; token-level context window forms the agent's working memory.",
            "benchmark_name": "Reddit comment generation (toxicity and upvote rewards)",
            "benchmark_description": "Open-domain reply generation with highly stochastic, subjective rewards: toxicity filter labels (-10/-5/0) and binary upvote reward (+10/0). Evaluated both on real upvotes and a RoBERTa-based upvote predictor.",
            "memory_used": true,
            "memory_type": "working memory (transformer context / token history)",
            "memory_architecture": "Input is parent comment + generated reply tokens (history) inside transformer context; Q and V heads produce token-level values conditioned on this history. No separate external memory store used.",
            "memory_integration_strategy": "Full token history concatenated as transformer input; per-token Q/V computed from context; at inference, pi_beta logits are adjusted by beta*(Q-V) to favor safer/higher-reward completions.",
            "performance_with_memory": "Toxicity reward: ILQL produced 0.0 toxic outputs (perfect) on main setting; Upvotes real: ILQL 9.83 ± 0.04; Upvotes model: ILQL 10.0 ± 0.0 (Table 2).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared to baselines: single-step RL, filtered fine-tuning (top-reward BC), fine-tuning. ILQL outperforms filtered finetuning and fine-tuning on toxicity and upvote rewards. Paper includes an experiment adding noise to toxicity labels showing ILQL remains robust where filtered BC degrades.",
            "best_memory_strategy": "Rely on transformer's context (token history) as working memory and learn expected Q-values to represent reward uncertainty; avoid naive filtered BC in stochastic reward regimes by using value-based offline RL (ILQL) with CQL regularizer.",
            "limitations_or_failure_cases": "No explicit long-term memory beyond context window; paper does not evaluate integrated multi-session episodic memory. ILQL requires multiple transformer networks during training and tuning of some hyperparameters (tau, alpha, beta) though it is more stable than many baselines.",
            "recommendations_or_conclusions": "To optimize subjective/noisy rewards, use ILQL's value-based offline RL with transformer context as the memory and a CQL-like regularizer to reduce OOD Q-values; prefer ILQL to filtered finetuning in high-variance reward settings and tune beta to trade off diversity vs optimization.",
            "uuid": "e8471.2",
            "source_info": {
                "paper_title": "Offline RL for Natural Language Generation with Implicit Language Q Learning",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Offline reinforcement learning with implicit q-learning",
            "rating": 2
        },
        {
            "paper_title": "Decision transformer: Reinforcement learning via sequence modeling",
            "rating": 2
        },
        {
            "paper_title": "Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Human-centric dialog training via offline reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control",
            "rating": 1
        }
    ],
    "cost": 0.01388725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Offline RL for Natural Language Generation with Implicit Language Q Learning</h1>
<p>Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, Sergey Levine<br>UC Berkeley; {csnell22,kostrikov,suyi,sherryy,svlevine}@berkeley.edu}</p>
<h2>ABSTRACT</h2>
<h4>Abstract</h4>
<p>Large language models distill broad knowledge from text corpora. However, they can be inconsistent when it comes to completing user specified tasks. This issue can be addressed by finetuning such models via supervised learning on curated datasets, or via reinforcement learning. In this work, we propose a novel offline RL method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the flexible utility maximization framework of RL algorithms with the ability of supervised learning to leverage previously collected data, as well as its simplicity and stability. Our method employs a combination of value conservation alongside an implicit dataset support constraint in learning value functions, which are then used to guide language model generations towards maximizing user-specified utility functions. In addition to empirically validating ILQL, we present a detailed empirical analysis of situations where offline RL can be useful in natural language generation settings, demonstrating how it can be a more effective utility optimizer than prior approaches for end-to-end dialogue, and how it can effectively optimize high variance reward functions based on subjective judgement, such as whether to label a comment as toxic or not ${ }^{1}$.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models can acquire a remarkable amount of knowledge from large text corpora, and can be applied to a wide range of language-based tasks. However, such models are not designed to optimize any user-specified utility, instead requiring considerable trial and error to design prompts that coerce the models into producing desirable outputs (Liu et al., 2021; Brown et al., 2020; Min et al., 2021). In essence, standard unsupervised language model training only solves part of the problem, being effective at distilling down knowledge in large corpora, but relatively clumsy when applying this knowledge to solve user-specified tasks.</p>
<p>Reinforcement learning (RL) in principle can provide an effective framework for steering language models toward user specified tasks as long as the task can be represented by some utility function (i.e., a reward function); however, as outlined in Figure 2 contemporary methods suffer from high systems complexity and can require expensive human interaction. We need several conditions to make RL practical: (1) Easy to use: the underlying learning algorithm and workflow should be simple, stable, and scalable; (2) Able to optimize user specified rewards: the algorithm should be able to steer a language model toward maximizing any user-defined reward signal, from high-level task goals (e.g., book a flight) to low-level linguistic subtleties (e.g., avoiding rude or toxic speech); (3) Practical for interactive applications: the system should be able to handle a variety of tasks,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>from generating text with desired properties to sequential turn-taking in settings such as dialogue tasks; (4) Able to leverage existing data: such a system should be able to directly utilize the large quantities of existing data, avoiding expensive and time-consuming online human interactions; (5) Temporally compositional <em>Emmons et al., 2021; Rafols et al., 2005)</em>: the method should be able to attain significant improvement over the average behavior in the data – not merely copying the best behaviors in the dataset, but actually distilling out underlying patterns in the relationship between rewards, task dynamics, and language to produce near optimal generations, even when the dataset demonstrates only mediocre task performance.</p>
<p>Offline RL provides a learning paradigm (Figure 1) that combines both supervised learning’s ability to leverage existing data (criteria 4) with RL’s ability to optimize arbitrary rewards and leverage temporal compositionality (criteria 2, 3, 5) <em>Levine et al., 2020; Kostrikov et al., 2021; Kumar et al., 2020; Janner et al., 2021; Chen et al., 2021; Yu et al., 2020; Kidambi et al., 2020)</em>. However, prior offline RL approaches for language tasks are either based on dynamic programming, which enjoy the temporal compositionality (see Appendix A.2) but suffer from high systems complexity, hyper-parameter instability, and slow training times <em>Verma et al., 2022; Jaques et al., 2020, 2017</em> (meets criteria 5, fails 1), or methods based on conditional imitation or dataset value learning that are simple and stable to train, but do not provide the temporal compositionality enjoyed by “full” RL methods (meets criteria 1, fails 5) <em>Chen et al., 2021; Snell et al., 2022; Holtzman et al., 2018; Yang &amp; Klein, 2021; Li et al., 2017; Krause et al., 2021)</em>. Motivated by all these criteria, we design a novel offline RL method based on dynamic programming with an implicit dataset support constraint <em>Kostrikov et al., 2021</em> that enjoys greater stability, fewer training-time dependencies (such as relying on approximate likelihoods from an external language model during training), and a more flexible decoding process than prior approaches (see Sections 4 and 6.4). Our method, ILQL, fine-tunes a transformer language model to predict the state-action $Q$ function and the state value function $V$ at each token. During training we perform iterative policy improvement by fitting a value function to an upper-expectile of the Q function, enabling us to learn policies that leverage temporal compositionality, significantly outperforming the data, while avoiding the need to execute expensive training-time procedures, such as sampling counterfactual utterances from the language model <em>Verma et al., 2022</em> (see Sections 5 and 6). Then at inference time we can simply steer a standard language model towards utility maximizing behavior, by perturbing the predicted likelihoods with our learned values functions (see Figure 3).</p>
<p>Our main contribution is twofold: (1) a novel offline RL algorithm, ILQL, for language models, that employs a stable optimization process that can flexibly learn high-performing policies from sub-optimal data in arbitrary sequential decision making settings, thus meeting each of the conditions laid out above; and (2) a detailed empirical analysis, not only demonstrating ILQL’s ability to more consistently and stably adapt to many different utility functions than prior approaches, but also ILQL’s unique ability to optimize stochastic or subjective reward functions, and its ability to discover optimal behaviors in the face of sub-optimal or unusual data distributions. In particular, in the controlled generation setting of generating non-toxic text, we demonstrate that ILQL, trained on both toxic and non-toxic comments, learns to produce fewer toxic outputs than the more standard approach of performing standard supervised fine-tuning on only non-toxic comments.</p>
<h2>2 RELATED WORK</h2>
<p>A number of prior works have explored combining online RL methods with language models for natural language tasks such as machine translation or summarization <em>Ranzato et al., 2015; Wu et al., 2016; Paulus et al., 2017; Wu &amp; Hu, 2018)</em>. These works have demonstrated that RL can be an effective tool for steering language models towards satisfying utility functions. However, when it comes to settings that require multiple steps of human interaction, e.g., dialogue, these methods can quickly become impractical <em>Verma et al., 2022; Ghasemipour et al., 2020)</em>.</p>
<p>Offline RL addresses this shortcoming by removing all need for environment interaction or user simulators, instead operating purely on static datasets of prior human interaction. Several prior works have applied offline RL to NLP and more broadly sequence generation problems *Jaques et al., 2020; Verma et al., 2022; Jaques et al., 2017; Snell et al., 2022; Janner et al., 2021; Chen</p>
<p>et al., 2021). The most closely related to our work are those methods based on approximate dynamic programming (Jaques et al., 2020; 2017; Verma et al., 2022; Jang et al., 2022). While all these works present promising offline RL methods for NLP tasks, none of them provide a method that achieves the simplicity, stability, and ease-of-use aspect at the level of supervised learning. For example, Verma et al. and Jang et al. (Verma et al., 2022; Jang et al., 2022) define their action space at the "per-utterance" level (Verma et al., 2022), resulting in expensive decoding processes during training (Bender et al., 2021); and while Jaques et al. (Jaques et al., 2020; 2017) remove this issue by defining actions at the "per-token" level, the offline RL algorithm proposed requires querying likelihoods from a language model at RL training time, which adds an additional compounding source of approximation error and increases systems complexity at training time. Our proposed method instead operates both at the "per-token" level and trains in a fully self-contained way, without the need to simulate generation at training time or query likelihoods from a separate language model. This is achieved by combining an implicit dataset support constraint (Kostrikov et al., 2021) with a novel policy extraction method that takes advantage of the discrete "per-token" action space. The result of these design choices is a simple, stable, and effective method that is easy for NLP practitioners to pick up and apply to a variety of language-based tasks. In Section 6.4 we demonstrate our method's effectiveness in meeting these criteria through a series of ablations and comparisons.</p>
<p>Much prior work on steering language models towards desired behavior has done so without an explicit utility function, instead focusing on curating finetuning datasets (Zhang et al., 2018; Zellers et al., 2019; Rajpurkar et al., 2018). A more closely related line of work uses classifiers to guide LMs towards generating desired textual attributes (Yang \&amp; Klein, 2021; Ghazvininejad et al., 2017; Holtzman et al., 2018; Li et al., 2017). These methods are closely related to the prior work on offline RL. In RL parlance, such methods could be considered "policy extraction" methods with Monte Carlo value estimates. This can be interpreted as taking a single step of policy improvement which, though often effective (Brandfonbrener et al., 2021), is known to be suboptimal as compared to full dynamic programming methods (i.e., full Q-learning or actor-critic) (Kostrikov et al., 2021). We will demonstrate empirically in Section 5 that our offline RL method can lead to significant improvements in final performance as compared to such "single step" approaches, particularly when the training data is highly suboptimal for the desired task.</p>
<h1>3 Preliminaries: Language Generation as a Reinforcement Learning Task</h1>
<p>Token-level POMDP. In this work, we formalize language generation tasks as a partially observable Markov decision process (POMDP). We define the POMDP $\mathcal{M}$ at the token level with $\mathcal{M}=$ $(\mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{T}, \mathcal{Z}, \mu_{0}, \mathcal{R}, \gamma)$. We define the agent's observation $h_{t} \in \mathcal{O}$ as a history of tokens with $h_{t}=\left{t_{0}, t_{1}, t_{2}, t_{3}, \ldots t_{t-1}\right}$; the action space $a_{t}=t_{t} \in \mathcal{A}$ is the set of possible next-tokens in our vocabulary which includes the special end-of-turn token $a_{\text {end }}$ (see Figure 3).</p>
<p>Value-based offline RL. In offline RL, the goal is to learn the optimal policy $\pi$ that achieves highest discounted cumulative reward from a static dataset $\mathcal{D}$ that was produced by some potentially suboptimal behavior policy $\pi_{\beta}$. In this work, we build on the implicit Q-learning (IQL) algorithm (Kostrikov et al., 2021), which approximates the Bellman optimality equation constrained to in-dataset actions</p>
<p>$$
Q^{<em>}(s, a)=R(s, a)+\gamma \max <em _pi__beta="\pi_{\beta">{a^{\prime}, \mathrm{s} . \mathrm{l}} \max </em> Q^{}\left(a^{\prime}\right) s^{\prime}&gt;0</em>}\left(s^{\prime}, a^{\prime}\right)
$$</p>
<p>Instead of directly implementing the support constraint, IQL approximates the maximization on the right-hand side of the constrained Bellman operator with expectile regression:</p>
<p>$$
L_{V}(\psi)=\mathbf{E}<em 2="2">{(s, a) \sim D}\left[L</em>(s)\right)\right]
$$}^{\tau}\left(Q_{\hat{\theta}}(s, a)-V_{\psi</p>
<p>where $L_{2}^{\tau}(u)=|\tau-\mathbb{1}(u&lt;0)| u^{2}$. Increasing the hyperparameter $\tau$, more closely approximates the maximum. Then this approximation can be used to estimate TD-targets for the Q-networks:</p>
<p>$$
L_{Q}(\theta)=\mathbf{E}<em _psi="\psi">{\left(s, a, s^{\prime}\right) \sim D}\left[\left(R(s, a)+\gamma V</em>\right]
$$}\left(s^{\prime}\right)-Q_{\theta}(s, a)\right)^{2</p>
<p>IQL was designed for fully observable MDPs. However, in Section 4.1, we discuss how we adapt this formulation to the POMDP setting described above using sequence models.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 3: A diagram of our Implicit Language Q Learning algorithm. Left: ILQL training involves three transformers, each of which is finetuned from a standard pretrained model: (1) A $\pi_{\beta}$ model, finetuned with standard supervised learning. (2) A value function model, with Q and V on two separate heads; the value functions are trained with Bellman backups using a combination of conservatism and and an implicit dataset support constraint. (3) A target value network, which is a Polyak moving average of (2). Right: At inference time, we use our learned value functions to perturb the log probabilities of $\pi_{\beta}$ towards utility maximizing behavior.</p>
<p>Supervised learning baselines. To align with NLP terminology, we refer to $\% \mathrm{BC}$, or supervised learning on curated or filtered data, as "filtered fine-tuning", and we refer to BC, or finetuning on unfiltered data as "fine-tuning". See Appendix A. 4 for filtering details.</p>
<h1>4 IMPLICIT LANGUAGE Q-LEARNING</h1>
<p>Our main technical contribution implicit language Q-learning (ILQL), an offline RL algorithm for NLP tasks. ILQL is specifically designed to enable simple and efficient training of language models with user-specified reward functions, with a workflow that is similar to standard supervised learning.</p>
<p>ILQL builds on the IQL algorithm, extending it to the token-level POMDP that defines NLP tasks via the following modifications: (i) it integrates with sequence models to handle partially observable language generation tasks (Section 4.2); (ii) it utilizes a novel policy extraction method that directly perturbs the behavior policy $\pi_{\beta}$ with our learned value functions, rather than training a separate actor $\pi$, significantly improving performance and stability on NLP tasks (Section 4.1) and (iii) it adds a conservatism loss term (Kumar et al., 2020) to the Q-function, fixing a calibration issue in the policy extraction step. Figure 3 provides an overview of our method.</p>
<h3>4.1 AdAPting IMPLICIT Q-LEARNING TO LANGUAGE MODELS</h3>
<p>Implicit value function learning. Like IQL, our method learns both a value function and a Qfunction, which bootstrap off each other through Bellman backups with implicit maximization through an expectile loss. This recursive process of fitting Q and V corresponds to iterative policy improvement subject to an implicit dataset support constraint, specified by the expectile used to fit V. Due to parameter sharing, we combine Eqn. 1 and 2 into a single loss function:</p>
<p>$$
L_{Q, V}(\theta)=\mathbf{E}<em i="0">{\tau \sim D}\left[\sum</em>\right)\right)\right]
$$}^{T}\left(R\left(h_{i}, a_{i}\right)+\gamma V_{\theta}\left(h_{i+1}\right)-Q_{\theta}\left(h_{i}, a_{i}\right)\right)^{2}+L_{2}^{\tau}\left(Q_{\hat{\theta}}\left(h_{i}, a_{i}\right)-V_{\theta}\left(h_{i</p>
<p>In contrast to IQL, we sample sequences of tokens instead of individual transitions to handle partial observability, such that for each time step, the values are predicted based on a full history.</p>
<p>Policy extraction. IQL (Kostrikov et al., 2021) uses AWR policy extraction (Peng et al., 2019), which distills the Q-function into a policy with a $e^{\beta(Q-V)}$ weighted log-likelihood loss. However, as we discuss in Section 6, we found this somewhat unstable to train on language models, likely due to the high-variance gradients induced by the advantage weights. Fortunately, the value learning procedure in IQL is independent of policy extraction, so instead of attempting to train a model to represent the optimal policy, we use the learned Q and V values to directly perturb samples from a model finetuned via supervised learning to model $\pi_{\beta}$ (see Figure 3). To this end, we compute a modified likelihood for each token by adding its advantage $Q(b, a)-V(h)$ to its logits under the $\pi_{\beta}$ model, with a multiplier $\beta$. We can then renormalize these pseudo-logits and sample them, resulting in</p>
<p>the implicit policy $\pi(a \mid h) \propto \pi_{\beta}(a \mid h) e^{\beta(Q(h, a)-V(h))}=\exp \left(\log \left(\pi_{\beta}(a \mid h)\right)+\beta(Q(h, a)-V(h))\right)$. This does not require training a separate actor, only a behavioral model $\pi_{\beta}$, which can be trained with the standard and stable supervised finetuning objective.</p>
<p>However, naïvely performing policy extraction in this way can perform poorly due to over-smoothed probabilities in $\pi_{\beta}$ that may be nonzero for extremely unlikely tokens. In this case, samples from $\pi_{\beta}$ may be out of distribution for $Q$ and $V$, and might have erroneous values. To fix this calibration issue, we can either further decrease the probability of low probability actions in $\pi_{\beta}$ by performing top-p filtering or tuning a temperature parameter, or we can explicitly push down OOD Q-values during training. We implement the latter by adding a small amount of NLL loss to the Q values, which corresponds to the additional loss terms introduced by CQL (Kumar et al., 2020) with a uniform KL regularizer. Since ILQL actions are discrete tokens, as opposed to the original CQL method (Kostrikov et al., 2021), which operates on continuous action spaces, this CQL loss term is no more expensive than, and in fact equivalent to, a standard cross-entropy loss at the token level. We find that both of these approaches often work in practice, but prefer the latter, finding that it requires less tuning for policy extraction at inference time. Our full loss function is therefore:</p>
<p>$$
L_{Q, V}^{c}(\theta)=L_{Q, V}(\theta)-\alpha \mathbf{E}<em _theta="\theta">{\tau \sim D} \log \left(\frac{e^{Q</em>\right)
$$}\left(s_{i}, a_{i}\right)}}{\sum_{a^{\prime} \in \mathcal{A}} e^{Q_{\theta}\left(s_{i}, a^{\prime}\right)}</p>
<p>In early experiments, we found that decoding using the CQL regularized value functions alone, without $\pi_{\beta}$, required careful tuning of the CQL weight $\alpha$. When the CQL regularized value function is combined with $\pi_{\beta}$ for policy extraction, it mitigates this issue with hyper parameter sensitivity, and simply setting the CQL weight $\alpha$ to an arbitrary small value less than 1 typically works well.</p>
<h1>4.2 ARCHITECTURES FOR IMPLICIT LANGUAGE Q-LEARNING</h1>
<p>We use GPT-2 small as the base model for all transformers in our experiments. Our value function transformer has three MLP heads: two independently initialized and trained Q heads and one V head. Each head has two layers, with a hidden dimension twice that of the embedding dimension. Our target Q value is parameterized as the minimum prediction of both Polyak averaged target Q heads: $\hat{Q}=\min \left(Q_{1}, Q_{2}\right)$ (Fujimoto et al., 2018). As in standard language modeling, the transformer's causal masking enables us to perform Bellman updates over entire sequences in parallel.</p>
<h2>5 Proof of Concept: Multi-Step Offline RL on Wordle</h2>
<p>The lack of configurable task settings and reliable evaluations has arguably slowed down progress in applying sophisticated RL algorithms to language and dialogue tasks (Jiang et al., 2021; Deriu et al., 2021; Curry et al., 2017). To address this, we present the Wordle game (Lokshtanov \&amp; Subercaseaux, 2022) as an easy-to-use but challenging benchmark task to test the capabilities of offline RL algorithms. In this section, we use this task to construct situations where we would expect offline RL to lead to significant improvement over simpler methods based on supervised learning or single-step improvement (e.g., single-step RL-style or filtered supervised learning methods).</p>
<p>Multi-step RL: a motivating example. General value-based RL methods based on solving the Bellman equation described in Section 3 can be viewed as iteratively improving the policy: each update sets the current value $Q\left(h_{t}, a_{t}\right)$ to be the reward plus the maximum possible next time step value according to the current value function. This is in contrast to "single-step" update methods, which do not recursively update the value function, instead only learning to estimate the value of the dataset and then greedily selecting the maximal-value action during inference. Classic examples of such methods use Monte Carlo regression or single-step RL (Sutton et al., 1998) to train the value function, and then greedily choose actions at test-time, though a number of different methods of this sort have been proposed for guided language generation in the literature (Yang \&amp; Klein, 2021; Ghazvininejad et al., 2017; Holtzman et al., 2018; Li et al., 2017). Such methods have been referred to in the NLP literature as "reward models" (Young et al., 2017; Gu et al., 2016; Su et al., 2016) and in the offline RL literature "one step RL" or SARSA (Brandfonbrener et al., 2021). Some works also proposed behavioral cloning methods that filter the training data or use conditioning to clone high-reward trajectories (Chen et al., 2021) - though these methods are based on different principles, they also employ Monte Carlo estimates of the cumulative reward in place of value functions learned with dynamic programming. While in principle such methods should not lead to optimal policies, in practice they often constitute an appealing approximation due to their ease of use.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Left: an abstract depiction of an MDP where single-step RL fails to discover the optimal policy. Right: A notional illustrative example where we might expect full "multi-step" RL methods (such as ILQL) to perform significantly better than "single-step" methods. In this example, good utterances tend to start with "The movie was...", while bad utterances start with "The movie wasn't..." However, the very best examples also start with "The movie wasn't...", requiring multi-step planning or multiple steps of policy improvement to derive effective strategies. Methods that implement just a single step of policy improvement will fail to produce maximally positive sentiment outputs. While this example may appear somewhat contrived, we see in our experiments that multi-step RL methods do lead to improvements in a number of more real settings.
Since ILQL performs multiple steps of policy improvement, it can significantly improve over Monte Carlo estimators or single-step RL when the underlying data is sub-optimal. One example corresponds to the notional task in Figure 4, in which the optimal sequence of actions requires traversing a state that's also frequented by sub-optimal examples. In this case, single-step RL will learn to take actions that appear safer according to the dataset - such as the transition "The movie" $\rightarrow$ "was" in Figure 4 - whereas full ("multi-step") RL methods would recover the optimal policy. We demonstrate this empirically on the Wordle game below.
Wordle dataset. Our Wordle task is designed to allow us to use data from real humans in a sequential decision-making setting while still enabling objective simulated evaluation and the flexibility to compose datasets with different properties, thus providing an effective benchmark for validating a variety of approaches. Wordle is a word guessing game in which the agent gets 6 attempts to guess a certain word, and at each turn receives feedback from the environment about which letters from the guessed word are and are not in the true word (see Appendix A. 5 for more details). While Wordle may appear distinct from many natural language tasks, it shares a number of high-level properties, such as non-deterministic dynamics and a sequential turn-based structure, with more complex language domains like dialogue, making it well suited for a first evaluation of NLP-focused RL methods.
Synthetic Wordle task. We constructed a synthetic Wordle dataset to serve as a benchmark that is specifically intended to evaluate how well a particular method can perform multiple steps of policy improvement. This task intended to specifically bring out the failure mode of single step methods in a setting suitable for testing offline RL methods with sequence models. The dataset consists of data sampled from three behavior policies, each corresponding to one of the branches in Figure 4: (1) $\pi_{\text {upper bound }}$, a high-performing policy, corresponding to the path $S_{0} \rightarrow$ Goal. (2) $\pi_{\text {adversarial }}$, which behaves the same as $\pi_{\text {upper bound }}$ for the first two actions $\left(S_{0} \rightarrow S_{1}\right)$ and then behaves suboptimally ( $S_{0} \rightarrow$ Lava). (3) $\pi_{\text {suboptimal }}$, a policy of moderate performance, corresponding to $S_{0} \rightarrow S_{1}$. Measuring the predicted Q values from our models trained on this distribution, in Figure 6 (right) we observe that ILQL assigns higher values to actions corresponding to the paths towards "misleading states" (i.e. $S_{2}$ ) than those to the "goal states" (i.e. $S_{1}$ ), whereas single-step RL shows the exact opposite preference, confirming both our hypothesis that this type of MDP would be amenable to multiple steps of policy improvement, and that ILQL as an algorithm is able to perform such policy improvement. Of course, the structure of real-world NLP tasks might not necessarily reflect this setting - as we show in the next section, ILQL still often attains improvement over one-step and BC-based methods, though it is more difficult to discern the particular structure that makes this possible in more realistic tasks.
Validating on natural Wordle data. While the synthetic setting explored above was specifically designed to demonstrate a dramatic difference between ILQL and single-step RL, the findings still transfer to more realistic settings. In Table 6, we demonstrate ILQL outperforming single-step RL on a natural dataset of Wordle games scraped from Twitter (see Appendix A. 5 for details).</p>
<h1>6 Natural Language EXPERIMENTS</h1>
<p>Next, we evaluate ILQL on two realistic language tasks. We first identify scenarios in which one might expect offline RL to be particularly beneficial: (1) tasks that demand repeated interactions,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">method</th>
<th style="text-align: center;">standard</th>
<th style="text-align: center;">y/n</th>
<th style="text-align: center;">cons. y/n</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ILQL</td>
<td style="text-align: center;">$-5.21 \pm 0.13$</td>
<td style="text-align: center;">$-\mathbf{5 . 5 7} \pm 0.13$</td>
<td style="text-align: center;">$-\mathbf{6 . 5 7} \pm 0.18$</td>
</tr>
<tr>
<td style="text-align: center;">1-step RL</td>
<td style="text-align: center;">$-5.14 \pm 0.13$</td>
<td style="text-align: center;">$-5.91 \pm 0.14$</td>
<td style="text-align: center;">$-7.63 \pm 0.20$</td>
</tr>
<tr>
<td style="text-align: center;">Filtered FT</td>
<td style="text-align: center;">$-5.07 \pm 0.13$</td>
<td style="text-align: center;">$-7.48 \pm 0.21$</td>
<td style="text-align: center;">$-9.13 \pm 0.22$</td>
</tr>
<tr>
<td style="text-align: center;">FT</td>
<td style="text-align: center;">$-5.25 \pm 0.13$</td>
<td style="text-align: center;">$-10.85 \pm 0.27$</td>
<td style="text-align: center;">$-15.16 \pm 0.35$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">trans/oval</th>
<th style="text-align: center;">standard</th>
<th style="text-align: center;">y/n</th>
<th style="text-align: center;">cons. y/n</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">standard</td>
<td style="text-align: center;">$\mathbf{- 5 . 2 1} \pm 0.13$</td>
<td style="text-align: center;">$-11.12 \pm 0.30$</td>
<td style="text-align: center;">$-14.97 \pm 0.36$</td>
</tr>
<tr>
<td style="text-align: center;">y/n</td>
<td style="text-align: center;">$-5.41 \pm 0.12$</td>
<td style="text-align: center;">$-\mathbf{5 . 5 7} \pm 0.13$</td>
<td style="text-align: center;">$-8.24 \pm 0.22$</td>
</tr>
<tr>
<td style="text-align: center;">cons. y/n</td>
<td style="text-align: center;">$-5.29 \pm 0.13$</td>
<td style="text-align: center;">$-\mathbf{5 . 4 2} \pm 0.13$</td>
<td style="text-align: center;">$-\mathbf{6 . 5 7} \pm 0.18$</td>
</tr>
</tbody>
</table>
<p>Table 1: Left: comparing ILQL to baselines on our Visual Dialogue rewards. "Cons. y/n" refers to the "Conservative y/n" reward. ILQL successfully optimizes for many different rewards, even those for which the data is sub-optimal (e.g. BC performance). Right: Evaluating each ILQL agent on other rewards. Agents generally perform worse on rewards for which they were not trained.
like dialogue; (2) data that is highly sub-optimal under its utility function; (3) settings with highly stochastic rewards based on subjective human judgement (e.g., avoiding toxic language). Taking into account these three scenarios, we evaluate ILQL on (1) a goal-directed question asking task based on Visual Dialogue (Das et al., 2016), where achieving high rewards on a diverse set of metrics during repeated interactions is desirable, and (2) a Reddit comments generation task with highly subjective and noisy reward functions (toxicity ratings or upvotes). For general experiment details see Appendix A.4.</p>
<h1>6.1 Evaluating Diverse Rewards on Visual Dialogue</h1>
<p>Visual Dialogue dataset. We use the Visual Dialogue dataset (Das et al., 2016) to evaluate our algorithm's ability to optimize many different reward functions in complex dialogue settings. The task involves both a question asking and question answering agent, the latter of which is presented with an image and tasked with answering the former's questions about the image. Instead of using this task as a question answering task, we follow Das et al. (Das et al., 2017) and train our agents to ask questions, with rewards based on how well the ground-truth image can be predicted from the resulting dialogue. For evaluation, we use the model from Das et al. (Das et al., 2017) as our environment simulator. To allow our agents to operate entirely in the space of natural language, we treat the image embedding as part of the reward function, using the supervised model proposed by Das et al. (Das et al., 2017) to predict the image embedding from the dialogue. See Figure 5 for example dialogues in this domain. We chose this environment specifically because (1) it has been previously studied in the context of RL (Das et al., 2017); (2) as a dialogue game, automated evaluation is more reliable than other tasks; and (3) the Q\&amp;A structure enables some temporal compositionality (i.e., the answer to one question may prompt new more specific questions).</p>
<p>Visual Dialogue task. The agent receives a reward of -1 for each turn in which the ground truth image can't be predicted accurately from the dialogue, otherwise the agent receives a reward of 0 and the environment ends interaction. For details on the task setup, see Appendix A.6. Since the Visual-Dialogue dataset was largely designed for supervised learning agents, the data is already near optimal for the original task. However, if we shift the reward function such that the data is no longer optimal, we can observe large gains from offline RL. We therefore use this domain to demonstrate offline RL's flexibility to adapt to different reward functions. We consider three rewards: "standard", "y/n", and "conservative y/n". "Standard" is simply the reward described above and detailed in Appendix A.6. "y/n" adds to the "standard" reward a penalty for asking questions that produce yes or no answers, by assigning a reward of -2 each time the other speaker says "yes" or "no". This is challenging, because while the data contains many yes/no questions,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">method</th>
<th style="text-align: right;">toxicity</th>
<th style="text-align: right;">upvotes real</th>
<th style="text-align: right;">upvotes model</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ILQL</td>
<td style="text-align: right;">$\mathbf{0 . 0} \pm 0.0$</td>
<td style="text-align: right;">$\mathbf{9 . 8 3} \pm 0.04$</td>
<td style="text-align: right;">$\mathbf{1 0 . 0} \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">single-step RL</td>
<td style="text-align: right;">$\mathbf{0 . 0} \pm 0.0$</td>
<td style="text-align: right;">$6.23 \pm 0.15$</td>
<td style="text-align: right;">$\mathbf{1 0 . 0} \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">Filtered FT</td>
<td style="text-align: right;">$-0.74 \pm 0.07$</td>
<td style="text-align: right;">$7.06 \pm 0.14$</td>
<td style="text-align: right;">$7.86 \pm 0.13$</td>
</tr>
<tr>
<td style="text-align: center;">FT</td>
<td style="text-align: right;">$-3.51 \pm 0.13$</td>
<td style="text-align: right;">$4.87 \pm 0.16$</td>
<td style="text-align: right;">$4.87 \pm 0.16$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">train/eval</th>
<th style="text-align: center;">toxicity</th>
<th style="text-align: center;">upvotes model</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">toxicity</td>
<td style="text-align: center;">$\mathbf{0 . 0} \pm 0.0$</td>
<td style="text-align: center;">$9.07 \pm 0.09$</td>
</tr>
<tr>
<td style="text-align: center;">upvotes gold</td>
<td style="text-align: center;">$-5.00 \pm 0.00$</td>
<td style="text-align: center;">$9.83 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">upvotes model</td>
<td style="text-align: center;">$-5.00 \pm 0.00$</td>
<td style="text-align: center;">$\mathbf{1 0 . 0} \pm 0.0$</td>
</tr>
</tbody>
</table>
<p>Table 2: Left: A comparison of ILQL against baselines on our Reddit comment rewards. ILQL never generates undesirable comments on 2 out of 3 rewards, whereas fine-tuning on filtered data occasionally does. Right: Evaluating each Reddit ILQL agent on other rewards. Agents trained on one reward are less optimal on others.
the goal is not for the agent itself to avoid those words, but rather avoid utterances that cause the other speaker to use them. Not all simple questions produce literal "yes/no" answers, so our third reward further penalizes brief responses, such as "I can't tell", "no it isn't", "yes it is", "I don't know". This reward function assigns a -2 reward to a set of low-information responses using a handful of conservative string matching heuristics, detailed in Appendix A.6.</p>
<p>Results on optimizing diverse rewards We demonstrate that ILQL learns a policy distinct from the dataset behavior policy and can optimize many different rewards in Table 1 left, where we can see that ILQL is able to outperform baselines on most of our Visual Dialogue reward functions. Agents optimized for each reward learn different behaviors as well: in Table 1 right, we see that offline RL agents trained on one reward function are generally suboptimal on others. Figure 5 demonstrates this qualitatively: without the "yes/no" penalty, our policies ask many "yes/no" questions, but under its presence policies tend to ask other questions instead. Even when the underlying data is highly sub-optimal for a given reward function, ILQL is able to determine the desired behavior. In addition to our reward-based evaluations, we also provide language quality evaluations for all baselines on each reward function in Section A. 11.</p>
<h1>6.2 SUbJECTIVE REWARDS ON REDDIT COMMENTS</h1>
<p>Reddit comments dataset. To evaluate our agents on minimally curated and maximally diverse open-domain text with highly stochastic reward functions based on subjective human judgement, we train ILQL on a large dataset of 4 million Reddit comments from ${ }^{2}$; our agents are given a parent comment or post as context and then trained to produce replies that satisfy one of two rewards: "toxicity" and "upvotes real". Given that this is internet text, the data contains toxic language, so for our "toxicity" reward, we train our agents to satisfy a toxicity filter, which gives rewards $-10,-5$, and 0 for toxic comments, moderately toxic, and non-toxic comments respectively. Our second reward function incentivizes comments that would receive a positive number of upvotes, rewarding +10 for positive upvotes and 0 negative. We automatically evaluate our upvote agents with a finetuned RoBERTa-base model, that predicts whether a comment will receive positive upvotes. We train this model on a held-out split of the data (see Appendix A. 7 for more details). We train agents on both the ground truth upvotes (denoted "upvotes real") and on this upvote model's predicted reward (denoted "upvotes model").
Results on optimizing noisy rewards. In natural language tasks, we may need to optimize stochastic, high-variance reward functions based on subjective judgement, such as whether a Reddit comment should be flagged as toxic. Such stochastic settings should be expected when multiple users with differing opinions provide reward labels. Offline RL, by design, is robust to environment stochasticity, and therefore should be able to optimize such noisy environments. We use the Reddit toxicity and upvote tasks to study how well ILQL can handle such settings. As we can see in Table 2 top, ILQL is surprisingly able to get a perfect or near-perfect score on these more subjective settings, whereas more standard approaches, such as filtered finetuning on only non-toxic or only positive upvote comments, perform significantly worse (i.e. generates more comments flagged as toxic or predicted to have negative upvotes). Additionally, we see in Table 2 bottom that agents trained on one reward function are generally less optimal for others, confirming that ILQL is effectively specializing its behavior for each utility function. We have additional complementary experiments studying this effect in Appendix A.8. In addition to our reward-based evaluations, we also provide language quality evaluations of our agents in Section A.11, and a preliminary user study in Appendix A.12.</p>
<h3>6.3 Choice of Offline RL Algorithm</h3>
<p>We compare ILQL to four other RL methods: a per-token version of CQL, an adaptation of the $\psi$-learning as proposed by Jaques et al. (Jaques et al., 2020; 2017), decision transformer (DT) (Chen et al., 2021), and single-step RL (Yang \&amp; Klein, 2021; Ghazvininejad et al., 2017; Holtzman et al., 2018; Li et al., 2017). In Table 3, we see that ILQL significantly outperforms baselines, and also has</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the second lowest hyper-parameter variance, just behind single-step RL, confirming our hypothesis that ILQL can provide both high relative performance and training stability.</p>
<h1>6.4 ILQL Ablations</h1>
<p>To understand which components of ILQL enable both good results and greater ease-of-use than prior offline RL approaches for language tasks, we abalate ILQL's main design decisions: the choice of a per-token action space, our value learning method, and our policy extraction strategy.</p>
<p>We evaluate these comparisons on the Visual Dialogue "yes/no" reward because (1) it is important to compare offline RL methods on a challenging and realistic sequential decision problem like dialogue, and (2) since the Visual Dialogue data is already "near-expert" for the "standard" reward, the "yes/no" reward is better able to differentiate between methods.
Ablations on per-token vs. per-utterance actions. We hypothesize that, since a per-token Q function enables an efficient search through the utterance action space, performing offline RL at the token level, rather than the utterance level, can yield cheaper inference and better performance.
We compare ILQL to: (1) ILQL (utterance): a per-utterance adaptation of ILQL that removes the conservatism loss term and performs Bellman backups at the utterance level instead of the token level; (2) single-step RL (utterance): a per-utterance version of single-step RL; and (3) CHAI (Verma et al., 2022): an adaptation of CQL for use on language models at the per-utterance action level. For policy extraction, each baseline uses EMAQ (Ghasemipour et al., 2020), where we sample $N$ utterances from a learned behavior policy and then re-rank with the Q function.</p>
<p>We see in Table 5 that ILQL outperforms per-utterance ILQL and single-step RL, while also running inference $\sim 4 \mathrm{x}$ faster on a single T4 GPU. When tuned well, we see that CHAI perform similarly to ILQL. However, CHAI is less stable with respect to hyperparameters and is $&gt;2 \mathrm{x}$ slower at inference time.
Ablations on policy extraction strategies. In adapting IQL (Kostrikov et al., 2021) to sequence models, we design a novel policy extraction strategy, as described in Section 4. We compare our novel extraction procedure to two baselines: (1) the standard AWR-based (Peng et al., 2019) policy extraction method used in IQL (Kostrikov et al., 2021) and a number of other offline RL algorithms (Nair et al., 2020; Wang et al., 2020), and (2) GOLD (Pang \&amp; He, 2020) an off-policy gradient based method for natural language generation. We expect that our approach should generally yield better performance, while also being easier to tune than these baselines. We apply AWR and GOLD extraction to our best performing ILQL value function, denoting this as "ILQL (AWR)" and "ILQL (GOLD)". Table 4 demonstrates that ILQL is both more stable and better at extracting good performance from a given value function than the well established AWR-style extraction and GOLD. Additionally, the AWR and GOLD extraction methods require tuning additional hyper-parameters at training time rather than just at inference time, decreasing flexibility and increasing the time and effort spent tuning parameters and re-training.</p>
<h2>7 CONCLUSION</h2>
<p>We proposed ILQL, an offline RL method for steering language generation to fulfill a variety of desirable conversational behaviors. Through experiments ranging from word games and goal-directed question asking to optimizing upvotes and minimizing toxic language, ILQL shows that offline RL can serve as a strong alternative to the method landscape of language generation dominated by language model finetuning on manually filtered datasets and classifier guidance. We hope the positive results from ILQL will inspire more work on offline RL for dialogue, and lead to more controllable language models that directly optimize user-specified utility functions for a wide range of tasks in text generation. Lastly, we acknowledge that our method is generally more computationally expensive than more standard supervised learning approaches to language generation ( 2 x more training time in our experiments), since it requires 3 separate transformer networks during training and 2 during inference. Future work should study alleviating this cost.</p>
<h1>8 ETHICS STATEMENT</h1>
<p>We acknowledge that any utility optimization method can be used to aid or harm, we hope these future works consider ethical uses of offline RL. Additionally, our method also has its limitations: for example, ILQL may not be effective when datasets are highly suboptimal. Offline RL would also not be ideal in settings which require distributional constraints, such as fairness. We hope that practitioners will take these limitations into account when applying our method.</p>
<h2>9 REPRODUCIbility STATEMENT</h2>
<p>To promote reproducibility, we present extensive results of all hyper-parameter settings we tried for all baselines in the appendix. We also describe all experimental details. Lastly, we have also attached in the supplemental source code for reproducing all the experiments in this submission.</p>
<h2>REFERENCES</h2>
<p>Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT'21, pp. 610-623, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922.</p>
<p>David Brandfonbrener, William F. Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without off-policy evaluation, 2021. URL https://arxiv.org/abs/2106.08909.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https: //arxiv.org/abs/2005.14165.</p>
<p>Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling, 2021. URL https://arxiv.org/abs/2106.01345.</p>
<p>Amanda Cercas Curry, Helen Hastie, and Verena Rieser. A review of evaluation techniques for social dialogue systems. In Proceedings of the 1st ACM SIGCHI International Workshop on Investigating Social Interactions with Artificial Agents, pp. 25-26, 2017.</p>
<p>Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José M. F. Moura, Devi Parikh, and Dhruv Batra. Visual dialog, 2016. URL https://arxiv.org/abs/1611. 08669 .</p>
<p>Abhishek Das, Satwik Kottur, José M. F. Moura, Stefan Lee, and Dhruv Batra. Learning cooperative visual dialog agents with deep reinforcement learning, 2017. URL https://arxiv.org/ abs/1703.06585.</p>
<p>Jan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie Rosset, Eneko Agirre, and Mark Cieliebak. Survey on evaluation methods for dialogue systems. Artificial Intelligence Review, 54(1):755-810, 2021.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2018. URL https://arxiv.org/ abs/1810.04805.</p>
<p>Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for offline rl via supervised learning?, 2021. URL https://arxiv.org/abs/2112.10751.</p>
<p>Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning, 2020. URL https://arxiv.org/abs/2004.07219.</p>
<p>Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods, 2018. URL https://arxiv.org/abs/1802.09477.</p>
<p>Felix A. Gers, Jürgen Schmidhuber, and Fred Cummins. Learning to Forget: Continual Prediction with LSTM. Neural Computation, 12(10):2451-2471, 10 2000. ISSN 0899-7667. doi: 10.1162/ 089976600300015015. URL https://doi.org/10.1162/089976600300015015.</p>
<p>Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expectedmax q-learning operator for simple yet effective offline and online rl, 2020. URL https: //arxiv.org/abs/2007.11091.</p>
<p>Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, and Kevin Knight. Hafez: an interactive poetry generation system. In Proceedings of ACL 2017, System Demonstrations, pp. 43-48, Vancouver, Canada, July 2017. Association for Computational Linguistics. URL https: //aclanthology.org/P17-4008.</p>
<p>Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor O. K. Li. Learning to translate in real-time with neural machine translation, 2016. URL https://arxiv.org/abs/1610.00388.</p>
<p>Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor, 2018. URL https: //arxiv.org/abs/1801.01290.</p>
<p>Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. Learning to write with cooperative discriminators, 2018. URL https://arxiv.org/abs/1805. 06087 .</p>
<p>Youngsoo Jang, Jongmin Lee, and Kee-Eung Kim. GPT-critic: Offline reinforcement learning for end-to-end task-oriented dialogue systems. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=qaxhBG1UUaS.</p>
<p>Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. In Advances in Neural Information Processing Systems, 2021.
N. Jaques, S. Gu, D. Bahdanau, J. M. Hernandez-Lobato, R. E. Turner, and D. Eck. Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control. International Conference on Machine Learning (ICML), 2017.</p>
<p>Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Shane Gu, and Rosalind Picard. Human-centric dialog training via offline reinforcement learning, 2020. URL https://arxiv.org/abs/2010.05848.</p>
<p>Haoming Jiang, Bo Dai, Mengjiao Yang, Tuo Zhao, and Wei Wei. Towards automatic evaluation of dialog systems: A model-free off-policy evaluation approach. arXiv preprint arXiv:2102.10242, 2021.</p>
<p>Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel : Modelbased offline reinforcement learning, 2020. URL https://arxiv.org/abs/2005.05951.</p>
<p>Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning, 2021. URL https://arxiv.org/abs/2110.06169.</p>
<p>Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. GeDi: Generative discriminator guided sequence generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 4929-4952, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.424. URL https://aclanthology.org/2021. findings-emnlp. 424.</p>
<p>Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.</p>
<p>Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems, 2020. URL https://arxiv.org/abs/2005. 01643 .</p>
<p>Jiwei Li, Will Monroe, and Dan Jurafsky. Learning to decode for future success, 2017. URL https://arxiv.org/abs/1701.06549.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing, 2021. URL https://arxiv.org/abs/2107.13586.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019. URL https://arxiv.org/abs/1907.11692.</p>
<p>Daniel Lokshtanov and Bernardo Subercaseaux. Wordle is np-hard, 2022. URL https://arxiv. org/abs/2203.16713.</p>
<p>Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Noisy channel language model prompting for few-shot text classification, 2021. URL https://arxiv.org/abs/ 2108.04106 .</p>
<p>Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets, 2020. URL https://arxiv.org/abs/2006. 09359 .</p>
<p>Richard Yuanzhe Pang and He He. Text generation by learning from demonstrations, 2020. URL https://arxiv.org/abs/2009.07839.</p>
<p>Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization, 2017. URL https://arxiv.org/abs/1705.04304.</p>
<p>Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning, 2019. URL https://arxiv.org/ abs/1910.00177.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Eddie Rafols, Anna Koop, and Richard S Sutton. Temporal abstraction in temporal-difference networks. In Y. Weiss, B. Schölkopf, and J. Platt (eds.), Advances in Neural Information Processing Systems, volume 18. MIT Press, 2005. URL https://proceedings.neurips.cc/ paper/2005/file/12311d05c9aa67765703984239511212-Paper.pdf.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad, 2018. URL https://arxiv.org/abs/1806.03822.</p>
<p>Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks, 2015. URL https://arxiv.org/abs/1511.06732.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347.</p>
<p>Charlie Snell, Sherry Yang, Justin Fu, Yi Su, and Sergey Levine. Context-aware language modeling for goal-oriented dialogue systems, 2022. URL https://arxiv.org/abs/2204.10198.</p>
<p>Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, and Steve Young. On-line active reward learning for policy optimisation in spoken dialogue systems, 2016. URL https://arxiv.org/abs/1605.07669.</p>
<p>Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning. 1998.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017. URL https://arxiv.org/ abs/1706.03762.</p>
<p>Siddharth Verma, Justin Fu, Mengjiao Yang, and Sergey Levine. Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning, 2022. URL https://arxiv.org/abs/2204. 08426 .</p>
<p>Ziyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, and Nando de Freitas. Critic regularized regression, 2020. URL https://arxiv.org/abs/2006.15134.</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google's neural machine translation system: Bridging the gap between human and machine translation, 2016. URL https://arxiv.org/abs/1609.08144.</p>
<p>Yuxiang Wu and Baotian Hu. Learning to extract coherent summary via deep reinforcement learning, 2018. URL https://arxiv.org/abs/1804.07036.</p>
<p>Kevin Yang and Dan Klein. FUDGE: Controlled text generation with future discriminators. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.naacl-main.276. URL https://doi.org/10.18653\%2Fv1\% 2F2021.naacl-main. 276 .</p>
<p>Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. Recent trends in deep learning based natural language processing, 2017. URL https://arxiv.org/abs/1708.02709.</p>
<p>Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization, 2020. URL https://arxiv. org/abs/2005.13239.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence?, 2019. URL https://arxiv.org/abs/1905.07830.</p>
<p>Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. Personalizing dialogue agents: I have a dog, do you have pets too?, 2018. URL https:// arxiv.org/abs/1801.07243.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 JUSTIFICATIONS FOR OffLINE RL IN DIALOGUE</h2>
<p>Dialogue tasks are one of the most rich and interactive settings in NLP, and as we will argue, these properties make it an ideal target for applying offline RL. RL in general presents an elegant and highly desirable utility optimization framework for sequential decision making settings, such as dialogue. However, solving realistic interactive tasks with online RL requires either repeated realworld interaction or building a realistic simulator of the environment. In the case of dialogue, such online interaction means communicating with real humans, which may be impractically expensive and time-consuming with contemporary sample-inefficient online RL methods (Schulman et al., 2017; Haarnoja et al., 2018), and building a realistic simulator of human responses may be largely intractable in sufficiently rich or complex dialogue settings. Offline RL, on the other hand, avoids both of these heavy requirements, by, just as many recent breakthroughs in the field of NLP (Brown et al., 2020; Radford et al., 2019; Devlin et al., 2018), operating purely on previously collected data, which is wildly available on the internet in general. Offline-RL therefore presents an ideal approach for flexibly steering language models towards the successful completion of dialogue tasks in a way that effectively leverages existing data, just as supervised learning does.</p>
<h1>A. 2 Temporal Compositionality</h1>
<p>It has been well studied in the RL literature that value-function based RL methods are capabale of a form of temporal compositionality (Emmons et al., 2021; Rafols et al., 2005) or "stitching" in their utility optimization. Specifically this refers to an RL algorithm's ability to stitch together the locally optimal parts of suboptimal trajectories to find globally optimal behavior. The result of this stiching is a learning algorithm which can distill the optimal behavior out of a dataset that contains only suboptimal demonstrations.</p>
<h2>A. 3 Full Token-Level POMDP Formulation</h2>
<p>We expand on the POMDP definition presented in Section 3. In order to apply RL to interactive language settings, we need to formalize dialogue generation and other NLP tasks as a partially observable Markov decision processes (POMDP). We define the POMDP $\mathcal{M}$ at the token level with $\mathcal{M}=(\mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{T}, \mathcal{Z}, \mu_{0}, \mathcal{R}, \gamma)$. We define the agent's observation $h_{t} \in \mathcal{O}$ to be a history of tokens with $h_{t}=\left{t_{0}, t_{1}, t_{2}, t_{3}, \ldots t_{t-1}\right}$; the action space $a_{t}=t_{t} \in \mathcal{A}$ is defined to be the set of possible next-tokens in our vocabulary, which includes the special end-of-turn token $a_{\text {end }}$. The agent's policy then corresponds to a mapping $\pi: \mathcal{O} \rightarrow \mathcal{P}(\mathcal{A})$. Many tasks such as dialogue have an underlying state $s_{t}$ that goes beyond just the sequence history, which can encompass things like the speaker's mental state. The environment transitions $\mathcal{T}\left(\cdot \mid s_{t}, a_{t}\right)$ are defined as a function of this $s_{t}$. In particular, in domains, such as dialogue, the dynamics are trivial within the agent's utterance (the selected token is deterministically appended to the history), but when the policy outputs a special "end of turn" token, the other speaker gets a turn, which is subsequently appended to the history. In other tasks, where the goal is to generate a single utterance, such as generating a summary or a single Reddit comment, the episode ends when the policy produces the end token. The agent receives a reward, defined by $R\left(s_{t}, a_{t}\right) \rightarrow \mathcal{R}$, after each action taken. However, in all the settings we consider, the agent receives non-zero reward $r_{t}$ only after producing an "end of turn" token, rather than densely at every token in the agent's utterances.</p>
<p>While some prior works (Verma et al., 2022; Jang et al., 2022) have considered actions at the utterance level, defining decision processes at the token level can yield a more effective search over the exponentially large utterance action space, simply by selecting tokens with high estimated values. Typically, searching over a per-utterance action space requires a Monte Carlo process of sampling multiple full utterances and then re-ranking with estimated values, which can generally bring additional computational complexity at both training and inference time. In Section 6.4, we demonstrated the effectiveness of learning at the token level through an ablation study that compares with learning at the utterance level.</p>
<h2>A. 4 General Experiment Details</h2>
<p>Here we outline architecture and hyper-parameter details of all our models and baselines.
ILQL experiment details. We run all of our experiments on GPT-2 small transformer architectures, with the supervised learning policy on one transformer and Q and V heads on a separate one. The target Q network is also on a separate transformer. In all our experiments we initialize with GPT-2 pre-trained weights, except in the case of Wordle, where we initialize randomly. Additionally, Wordle uses a different token set: the set of 26 characters, plus an additional token for each "color". We train all RL baselines with double-Q learning, using two separate heads on the same transformer model as the two Q-functions. Our target Q networks are Polyak-averaged with decay factor 0.005 for both the transformer and the Q function head. We use $\gamma=0.99$ for all offline-RL experiments. All value function heads are two layer MLPs with hidden dimension twice that of the transformer's embedding dimension. Our MLPs used ReLU non-linearities and no dropout. We used the AdamW optimizer for all experiments, with a learning rate of 1e-4 on the Reddit and Visual Dialogue tasks and 1e-5 on the Wordle task. We used no weight decay in the training any of our models, and we used a dropout rate of 0.1 inside the transformer. We trained all Wordle models with a batch size of 1024, all Visual Dialogue models with a batch size of 64, and all Reddit models with a batch size of 32. We always truncate token sequences to length 1024, except on Reddit tasks, in which we truncate to length 512.</p>
<p>Except on the Reddit comment task, we train ILQL on each of $\tau={0.7,0.8,0.9}$, and we also evaluate each on $\beta={4,8,16, \infty}$. Where $\beta=\infty$ refers to acting greedily according to only</p>
<p>the Q function. On the Reddit comment tasks, we only train with $\tau=0.6$ and evaluate on $\beta=$ ${1,2,4,8,16,32}$. On all tasks, we report the setting with the greatest performance.</p>
<p>For the NLL (CQL) loss term applied to ILQL, we used a weight $\alpha$ of 1.0 on all VisualDialogue experiments, 0.25 on all Reddit Comment experiments, and 0.0001 on Wordle. These values were tuned by hand. Generally, we find this loss parameter to not be too critical to performance; we tune it a little at first for each task and then don't worry about it.</p>
<p>All ILQL models and all baselines were trained on a single GPU until convergence. Training never exceeded three days (or 72 V100 hours).</p>
<p>Evaluation details. During evaluation, we use greedy decoding to generate utterances on all tasks and baselines, except the Reddit Comments tasks, where we sample instead. All experiments are evaluated on 1024 task-instances from an unseen evaluation set. We use our BC baseline model as $\pi_{\beta}$ for guiding ILQL's perturbation-based policy extraction.</p>
<p>Fine-tuning (BC) baselines. We train our fine-tuning baselines with the same optimization parameters (i.e., weight decay, dropout, learning rate, batch size) and initialization as ILQL. We use early stopping: when the validation loss exceeds the training loss, we stop training. Unlike our ILQL value function models, we use a linear head on top of the transformer to parameterize our BC policy, as is standard for language model finetuning. The only difference between our fine-tuning and standard language model training is that instead of finetuning the model to predict the whole sequence of states and actions, we only finetune the model to predict the agent's own actions or utterances.</p>
<p>Filtered Fine-tuning (\%BC) baselines. For our filtered fine-tuning baselines, we train models on datasets filtered for the top reward trajectories. Specifically we filter for the top ${10 \%, 30 \%, 50 \%}$ for Wordle, ${10 \%, 20 \%, 30 \%}$ for Visual Dialogue, and for Reddit, since our rewards are discrete, we define filtered fine-tuning to mean just training on the data-points with the maximum reward label. For each task, we train models on each of these percentages and report the performance of the best setting found. We use the same hyper-parameters as our Fine-tuning (BC) baselines for training these models.</p>
<p>Decision transformer baseline. Our decision transformer baseline follows from Chen et al. (Chen et al., 2021), except we initialize with pretrained GPT2 weights. All hyperparameters are identical to those used to train our BC baselines. To evaluate decision transformer on our Visual Dialogue " $\mathrm{y} / \mathrm{h}$ " reward, we swept over a broad range of conditional reward-to-go values: ${-11,-10,-9,-8,-7,-6,-5,-4,-3,-2,-1,0}$. We report the setting with the best performance.
single-step RL baselines. single-step RL baselines are implemented as ILQL with $\tau=0.5$ and all other hyper-parameters are identical to those used with ILQL as described above. Except on the Reddit comment tasks, we evaluate all single-step RL models on $\beta={4,8,16}$, and report the setting with the greatest performance. On the Reddit comment tasks, we show the best performance from $\beta={1,2,4,8,16,32}$.</p>
<p>Per-utterance ILQL. For "ILQL (utterance)", we train models with $\tau={0.7,0.8,0.9}$. We also evaluate each model with number of EMAQ-style (Ghasemipour et al., 2020) samples chosen from $\mathrm{N}={4,8,16}$. We report the setting with the best task performance. "single-step RL (utterance)" is a special case of "ILQL (utterance)" with $\tau=0.5$, which we evaluate on each of $\mathrm{N}={4,8,16}$ and report the setting with the best performance. The architecture for "ILQL (utterance)" and "single-step RL (utterance)" is largely identical to that of per-token ILQL, with the main difference being that Bellman backups are performed at the utterance level instead of the token-level. As a result of this difference, Q-heads map to a scalar at the end of an utterance instead of a vector at every token with length equal to the size of the vocabulary. We include comparisons to Per-utterance ILQL in Table 5.</p>
<p>CHAI baseline. Our CHAI baseline is adopted from Verma et al. (Verma et al., 2022). The tasks we consider only require utterance actions; no axuiliary actions, like the price proposal action required by that of Verma et al. (Verma et al., 2022)'s bargaining task. We therefore only adopt the components from CHAI relevant to utterance level actions. In order to compute CHAI's CQL loss at the utterance</p>
<table>
<thead>
<tr>
<th>method</th>
<th>max score</th>
<th>$\sigma$ w.r.t bparams</th>
<th>inference time per-dialogue (sec)</th>
</tr>
</thead>
<tbody>
<tr>
<td>ILQL</td>
<td>-5.57 $\pm 0.13$</td>
<td>0.46</td>
<td>5.10 $\pm 0.12$</td>
</tr>
<tr>
<td>ILQL (utterance)</td>
<td>-5.89 $\pm 0.14$</td>
<td>0.51</td>
<td>$22.1 \pm 0.47$</td>
</tr>
<tr>
<td>single-step RL (utterance)</td>
<td>-7.35 $\pm 0.17$</td>
<td>0.21</td>
<td>20.38 $\pm 0.41$</td>
</tr>
<tr>
<td>CHAI</td>
<td>-5.57 $\pm 0.13$</td>
<td>1.11</td>
<td>$12.13 \pm 0.25$</td>
</tr>
</tbody>
</table>
<p>Table 5: On the VisualDialogue "y/n" reward, we compare per-token ILQL to per-utterance ILQL, per-utterance single-step RL, and CHAI (Verma et al., 2022). We observe that per-token ILQL is generally much faster at inference time than per-utterance methods, while also outperforming or performing equivalently to all perutterance baselines. All evaluations were performed on a single T4 GPU. All baseline implementations build on the same core code for sampling utterances, with a handful of method specific runtime optimizations in each case.
level, we need to sample counterfactual utterances for each action in the training data, which can be highly expensive and can greatly slow training. Following Verma et al. (Verma et al., 2022), we amortize this cost at the risk of inducing some bias by caching 5 counterfactual samples for each action in the training data as a preprocessing step. In our case, this preprocessing step took over 30 hours to execute on a V-100 GPU for the full the Visual Dialogue training set. As in all our other experiments, we train two Q networks (Fujimoto et al., 2018), where our target Q value is parameterized as the minimum of both Polyak averaged target Q heads. We train our CHAI models with a batch size of 16 and otherwise all other hyperparameters are identical to those used with ILQL. We train models with CQL $\alpha={0.1,1.0,10.0}$, and we evaluate each model with the number of EMAQ-style (Ghasemipour et al., 2020) samples chosen from $\mathrm{N}={4,8,16}$. We report the setting with the best performance. We include comparisons to CHAI in Table 5.</p>
<p>Per-token dynamic programming baselines. For our per-token CQL and $\psi$-learning baselines in Table 4, we tuned the CQL loss weight with $\alpha={0.1,1.0,10.0}$, and the $\psi$-learning reward scale with $c={0.1,1.0,10.0}$. For each baseline agent, we evaluated using ILQL's policy extraction with $\beta={4,8,16}$ and also evaluated by greedily selecting tokens with the $Q$ function by itself. We report the setting with the best performance for each baseline.
Our implementation of per-token CQL is identical to ILQL with the only exception being that for per-token CQL the loss function is defined as:</p>
<p>$$
L_{Q, V}(\theta)=\mathbf{E}<em i="0">{\tau \sim D}\left[\sum</em>\right)+\gamma \max }^{T}\left(R\left(h_{i}, a_{i<em t_1="t+1">{a</em>\right]
$$} \in \mathcal{A}} Q_{\hat{\theta}}\left(h_{i+1}, a_{t+1}\right)-Q_{\theta}\left(h_{i}, a_{i}\right)\right)^{2</p>
<p>Our implementation of $\psi$-learning is adapted from Jaques et al. (Jaques et al., 2020; 2017) for use on transformer language models (Vaswani et al., 2017; Radford et al., 2019) instead of RNNs (Gers et al., 2000). The architecture is identical to that of ILQL, the main difference is in the loss function:</p>
<p>$$
L_{Q, V}(\theta)=\mathbf{E}<em i="0">{\tau \sim D}\left[\sum</em>\right)\right)\right]
$$}^{T} L_{\delta}\left(\frac{R\left(h_{i}, a_{i}\right)}{c}+\log \left(\pi_{\beta}\left(h_{i}, a_{i}\right)\right)+\gamma \log \left(\sum_{a_{t+1} \in \mathcal{A}} \exp Q_{\hat{\theta}}\left(h_{i+1}, a_{t+1}\right)\right)-Q_{\theta}\left(h_{i}, a_{i</p>
<p>Where $\pi_{\beta}$ is our BC baseline model: a transformer language model trained with supervised learning. And $L_{\gamma}$ defines the Huber loss; we use $\gamma=1$ in our experiments.</p>
<p>In both baselines, we also fit a value function head to the mean of the Q functions, as in ILQL with $\tau=0.5$. Additionally, for both baselines, aside from the parameters mentioned, all other parameters are identical to those used with ILQL, as described above. The only exception being that for $\psi$-learning, we used a learning rate of $1 \mathrm{e}-5$ instead of $1 \mathrm{e}-4$ due to training instability with the higher learning rate.</p>
<p>We generally found $\psi$-learning to be highly unstable to train in our experiments, often producing incomprehensible outputs. It is possible that the baseline could work better with even more careful tuning.</p>
<p>AWR extraction abalation details. For our "ILQL (AWR)" ablation, we extracted a policy with AWR extraction using the best performing ILQL value function out of those trained with $\tau=$ ${0.7,0.8,0.9}$. We performed AWR extraction from this value function using 3 different settings</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 6: We empirically validate the setting depicted in Figure 4 on a Wordle task. Left: a visualization of the synthetic dataset distribution we constructed to evaluate the benefits of ILQL's multiple steps of policy improvement over "single step" methods, such as single-step RL. Right: a plot showing that ILQL's Q function learns to more often assign higher Q values to optimal actions than single-step RL.
for beta: $\beta={4,8,16}$. Using the same value function, we performed ILQL extraction with $\tau={4,8,16}$. For each, we reported the setting with the best performance.</p>
<p>GOLD baseline. Our GOLD baseline is adopted from Pang et al. (Pang \&amp; He, 2020), with some small modifications. In particular, we use our best performing ILQL Q function as the Q function for GOLD policy learning, and instead of using a manually tuned constant baseline, we use our ILQL value-function, V, as the baseline. We updated our target policy every 1.5 k steps, as was done on several tasks in the original gold paper. We trained 4 models one with the target-policy weight lower-bound hyperparameter u set to each of ${0.00,0.10,0.15,0.20}$. We reported the setting with the best performance.</p>
<h1>A. 5 Wordle Task Details</h1>
<h2>Evaluating ILQL on the synthetic Wordle task.</h2>
<p>Wordle Background. In the game of Wordle, the agent gets 6 turns to guess a 5 letter word randomly selected from a vocabulary, and the environment responds with one of three "colors" for each letter in the guessed word: "black" meaning the guessed letter is not in the environment's word, "yellow" meaning the guessed letter is in the word but not in the right location, and "green" meaning the guessed letter is in the right location. We give a reward of -1 for each incorrect guess and a reward of 0 for a correct guess, at which point environment interaction ends; the agent's goal is therefore to guess the correct word in as few turns as possible, a task for which computing optimal behavior has previously been proven to be an intractable NP-Hard problem (Lokshtanov \&amp; Subercaseaux, 2022).</p>
<p>Environment details. Our agents observe the game-state as a history of alternating sequences of 5 letter tokens followed by 5 color tokens. Unlike the actual Wordle game, we do not prevent the agent from generating words that aren't in the vocabulary (i.e., the agent is free to produce any sequence of 5 letters). For our synthetic experiments, we chose to use the full word list given at https://gist.github.com/cfreshman/a03ef2cba789d8cf00c08f767e0fad7bdue to its relatively large size and its use in the actual Wordle game. However, the environment can be configured with any provided list of 5 letter words.</p>
<p>Wordle Twitter dataset details. We outline the details of our natural Wordle dataset scraped from Twitter, introduced in Section 5. Due to Wordle's popularity, we have access to a large amount of natural human data for Wordle (specifically 214,930 games), scraped from tweets ${ }^{3}$. Existing offline RL benchmarks are composed of purely synthetic data (Fu et al., 2020), and as a result, it may be unclear how well offline RL algorithms work on more natural data distributions. We therefore present this human Wordle dataset as a more naturalistic offline RL task. While the scraped Tweets don't display the actual words used by human players, only the sequence of transition colors given by the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">method</th>
<th style="text-align: center;">Wordle Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ILQL</td>
<td style="text-align: center;">$\mathbf{- 2 . 1 3} \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">single-step RL</td>
<td style="text-align: center;">$-2.23 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">Filtered Fine-tuning</td>
<td style="text-align: center;">$-2.38 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuning</td>
<td style="text-align: center;">$-2.61 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">$\pi_{\text {upper bound }}$</td>
<td style="text-align: center;">$-1.75 \pm 0.02$</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparing ILQL to baselines on Wordle human data. Even on realistic human data, ILQL outperforms "single step" single-step RL.
environment, we can retrofit valid words onto these tweets to produce a dataset of full trajectories. Note that the words we retrofit may not necessarily be the natural words human players would have used, but the dataset still represents the average performance of human players, since the number of turns remains unchanged in this retrofitting process. Additionally, this retrofitting allows us to further multiply the size of the dataset, since typically several different sequences of words can be valid for a given tweet. We can also partially control the difficulty level of the task and dataset by specifying the size and composition of the vocabulary used to retrofit words onto Tweets. In our Wordle human experiments in Table 6, we use a random subset of 200 words from the word list given at https://gist.github.com/cfreshman/a03ef2cba789d8cf00c08f767e0fad7b.</p>
<p>Synthetic Wordle Evaluation. For our synthetic Wordle task, we constructed a training distribution in which demonstrate ILQL's multiple steps of policy improvement significantly outperforms methods which only perform a single-step of improvement, such as single-step RL. As described in Secrion 5 our training distribution consists of a mixture of 3 policies, each of which represents one of the paths through the MDP in Figure 4: (1). $\pi_{\text {upper bound }}$, a high-performing policy which myopically selects the word with the highest information gain, representing the path from $S_{0} \rightarrow$ Goal. (2). $\pi_{\text {adversarial }}$, which behaves the same as $\pi_{\text {upper bound }}$ for the first two actions ( $S_{0} \rightarrow S_{1}$ ) and then subsequently repeats these first two words ( $S_{0} \rightarrow$ Lava). (3). $\pi_{\text {suboptimal }}$, which selects a random word $50 \%$ of the time, and the other $50 \%$ randomly selects a word that meets all known letter constraints, representing the path from $S_{0} \rightarrow S_{1}$. The relative performance of these policies is ordered according to $\pi_{\text {upper bound }}&gt;\pi_{\text {suboptimal }}&gt;\pi_{\text {adversarial }}$. We construct our dataset with $9 \%$ of the data coming from $\pi_{\text {upper bound }}, 45.5 \%$ from $\pi_{\text {suboptimal }}$, and $45.5 \%$ from $\pi_{\text {adversarial }}$. In Figure 6 (right), we show that when trained on this distribution, ILQL assigns higher Q values to actions corresponding to the paths to "misleading states" (i.e. $S_{2}$ ) than those to the "goal states" (i.e. $S_{1}$ ), whereas single-step RL shows the exact opposite preference, just as our hypothesis predicted.</p>
<p>Human Wordle Evaluation. In Figure 6, we compare ILQL against baselines on our dataset of Wordle games scraped from Twitter. We see that even on realistic data, ILQL's multiple steps of policy improvement outperforms methods which employ just a single-step of improvement.</p>
<h1>A. 6 Visual Dialogue Task Details</h1>
<p>Here we detail the task setup and reward functions used in our Visual Dialogue experiments in Section 6.1. We use Das et al.'s code ${ }^{4}$ to produce generations and to predict image embeddings from the provided supervised learning answer and question bots, respectively. We integrate these components of Das et al.'s codebase into ours by wrapping the relevant functionality of Das et al.'s codebase in a flask webserver interface that is then queried by our system.</p>
<p>As described in Section 6.1, our "standard" reward function gives a reward of -1 for each turn in which the true image is sufficiently difficult to predict from the dialogue, otherwise the agent receives a reward of 0 and the environment interaction ends. We firstly formalize this notion of "sufficiently difficult to predict".</p>
<p>The standard reward is based on the relative percentile ranking of the ground truth image's distance from the predicted embedding among a set of images taken from the evaluation set. We give a -1 reward to our agent for every turn in which $\left(1-p_{t}\right)&lt;\left(1-p_{0}\right) * 0.5$, where $p_{t}$ is the ground truth image's percentile rank at dialogue turn $t$ and $p_{0}$ is the ground truth image's percentile rank at the beginning of the dialogue, when only the image caption is observed. Otherwise, the agent gets a reward of 0 and the episode ends. This condition effectively rewards the agent once the ground truth image is preferred over $50 \%$ of the images that were preferred over it at initialization. The agent should learn to ask as many good questions as possible to get the episode to successfully end as early</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">method</th>
<th style="text-align: center;">toxicity</th>
<th style="text-align: center;">noised toxicity</th>
<th style="text-align: center;">upvotes real</th>
<th style="text-align: center;">upvotes model</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ILQL</td>
<td style="text-align: center;">$\mathbf{0 . 0} \pm 0.0$</td>
<td style="text-align: center;">$\mathbf{0 . 0} \pm 0.0$</td>
<td style="text-align: center;">$\mathbf{9 . 8 3} \pm 0.04$</td>
<td style="text-align: center;">$\mathbf{1 0 . 0} \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">single-step RL</td>
<td style="text-align: center;">$\mathbf{0 . 0} \pm 0.0$</td>
<td style="text-align: center;">$\mathbf{0 . 0} \pm 0.0$</td>
<td style="text-align: center;">$6.23 \pm 0.15$</td>
<td style="text-align: center;">$\mathbf{1 0 . 0} \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">Filtered Fine-tuning</td>
<td style="text-align: center;">$-0.74 \pm 0.07$</td>
<td style="text-align: center;">$-1.61 \pm 0.11$</td>
<td style="text-align: center;">$7.06 \pm 0.14$</td>
<td style="text-align: center;">$7.86 \pm 0.13$</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuning</td>
<td style="text-align: center;">$-3.51 \pm 0.13$</td>
<td style="text-align: center;">$-3.48 \pm 0.15$</td>
<td style="text-align: center;">$4.87 \pm 0.16$</td>
<td style="text-align: center;">$4.87 \pm 0.16$</td>
</tr>
</tbody>
</table>
<p>Table 7: A comparison of ILQL against baselines on the various Reddit comments reward functions. ILQL manages to never generate undesirable comments on 3 out of 4 reward functions, whereas fine-tuning on filtered data occasionally does.
as possible. In initial experiments, we found it took a very long time to train good value functions for rewards based on the absolute Euclidean distance alone, as used by Das et al. (Das et al., 2017), so to make it faster to iterate, we used the relative distance formulation described above.</p>
<p>Our " $y / n$ " reward adds, on top of the "standard" reward, a reward of -2 for every question that results in a response that exactly matches the strings "yes" or "no".</p>
<p>Our "conservative y/n" reward instead aims to provide a more conservative, higher-recall lowerprecision penalty to any question which might be a yes/no question. This accounts for the fact that people often answer yes/no questions with longer phrases (e.g., "It appears so"). This reward function provides a reward of -2 if any of the following words are sub-strings of the response to the agent's question: "not", "don't", "can't", "cannot", "fairly", "could", "think so", "okay", "maybe", "yes", "no", "looks", "appears", "tell", "mostly just". All of these were determined by hand to be words/phrases that occur often in answers to questions that are effectively yes/no questions.</p>
<h1>A. 7 Reddit Reward Model Details</h1>
<p>We outline the details of our reward functions for the Reddit tasks presented in Section 6.2.
Toxicity Reward. Our toxicity filter reward uses OpenAI's API ${ }^{5}$, which provides a free toxicity filter, meant for developers building applications off the GPT3 API to use to block toxic inputs or generations. We assign a reward of -10 for comments labeled as toxic (scored as 2 ), -5 for comments labeled as moderately toxic (scored as 1), and 0 for comments labeled as non-toxic (scored as 0 ).</p>
<p>Upvote Model Reward. Our upvote reward function is finetuned from RoBERTa-base (Liu et al., 2019) with a learning rate of $1 \mathrm{e}-5$ and a batch size of 64 . Since our reward functions are binary, we train with binary cross entropy loss. Like our value function heads in ILQL, we predict the reward as a scalar from a 2-layer MLP on top of the RoBERTa transformer, with hidden dimension twice that of the transformer, ReLU non-linearity, and no dropout. We truncate token sequences to maximum length 256. At inference time, we predict a reward of +10 if the model's reward logit is $\geq 0$ and a reward of 0 otherwise. We used binary (positive or negative) rewards for upvotes instead of the more natural cardinal numeric representation, because different sub-reddits can have drastically different upvote counts depending on the sub-reddit's population, and our binarization (positive or negative upvotes) is invariant to these differences in scale. However, this binarization is not the only normalization that we could have used to overcome this issue.</p>
<h2>A. 8 Noisy Rewards</h2>
<p>In figure 7, we present a more detailed visual explanation for why offline RL outperforms filtered finetuning on our Reddit tasks in section 6.2. As our results in Table 2 show, offline RL consistently outperforms finetuning on filtered data on this task. We hypothesize that this is due to offline RL's ability to effectively reason about the inherently stochastic and subjective rewards functions present in these tasks. In these high-variance reward settings, simply filtering or curating datasets for exclusively high-reward examples can fail to produce desirable outputs, since such filtered finetuning approaches do not make the model aware of the reward uncertainty. Put another way, training on curated datasets that exclude low-reward examples doesn't teach the model about what not to generate, whereas models that are aware of the reward, such as Q-learning, directly learn to relate actions to their expected reward values, averaging out uncertainty and stochasticity. This can enable models to avoid</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 7: A visual explanation of offline RL's ability to optimize high variance reward functions based on subjective judgement, such as whether to label a comment as an example of toxic speech or not. Finetuning on filtered data accidentally generalizes into producing undesirable outputs, whereas offline RL is able to find the "safe" outputs. Top: In the case of stochastic rewards, offline RL learns to avoid the highly stochastic regions of the action space, whereas filtered finetuning will explicitly learn to imitate undesirable outputs that were stochastically given a positive reward in the training data, thus leading to suboptimal behavior. Right: In the case of non-stochastic but sharp-boundary reward functions, ILQL is still able to integrate into its Q values uncertainty about actions near the sharper parts of the reward function's decision boundary, thus avoiding these regions. Finetuning on filtered data expresses no such preference and thus risks generalizing into occasionally producing undesirable outputs.</p>
<p>ILQL per-token advantages for toxic comments generated by filtered finetuning model</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 8: Two toxic comments incidentally generated by the filtered fine-tuning model. ILQL assigns negative advantages to many tokens, demonstrating how ILQL is more effectively able to avoid such generations.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: By varying $\beta$ at inference time, we can interpolate the trade-off between offline RL "optimization" and output diversity.
outputs that have low but non-trivial probability of undesirable outcomes (e.g., toxicity). Offline RL in this sense is able to find the safest outputs, whereas fine-tuning on filtered data does not explicitly express such a preference.
To further test this hypothesis, we artificially add further noise to the toxicity reward function. The standard toxicity reward assigns all comments one of three reward values: 0 , indicating the comment is non-toxic; -5 , indicating the comment is moderately toxic; -10 , indicating the comment is highly toxic. We now relabel the reward for all comments originally given a -5 reward, randomly to either 0 or 10 with equal probability.
Since some of the moderately toxic comments get relabeled with reward=0, they would be included in the $\% \mathrm{BC}$ training set, whereas offline RL should learn to represent uncertainty about such comments and thus push away from the stochastic "middle ground" of this reward function. In Table A. 8 in the "noised toxicity" column, we see that our offline RL agents learn to never generate toxic outputs despite the additional noise, and in Figure 8 we can see qualitatively that offline RL assigns low advantages to potentially negative or toxic words/phrases that were incidentally generated by the $\% \mathrm{BC}$ model. All of this goes to support our hypothesis that the advantage of offline RL over filtered supervised learning in these settings lies in its improved ability to handle reward uncertainty.</p>
<h1>A. 9 Trading Off Output Diversity for Optimization</h1>
<p>An advantage of our novel policy extraction mechanism is that we can flexibly tune the parameter $\beta$ at inference time, directly trading off between random generation and optimality. As discussed in Section 9, this parameter controls a constraint on our policy's deviation from the data distribution. As we increase $\beta$, the resulting policy will be more strongly influenced by the Q-function, and as we decrease $\beta$, it should approach the data distribution. Beyond the risk in diverging too far from the data, another potential downside to increasing $\beta$ is that the resulting policy distribution will become more deterministic. In some settings, such as chit-chat dialogue, we may desire policies capable of producing diverse and interesting outputs, so such a deterministic, highly-optimized agent would be undesirable.
We demonstrate in Figure 9, using the Reddit Toxicity task, how varying $\beta$ can modulate the diversity of the generations produced by our policy, as measured by its entropy, at the cost of a small decrease in performance. We show that as we increase $\beta$, while the policy's performance generally increases, the entropy decreases and subsequently so does the interestingness and diversity of the language</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5} \mathrm{https}: / /$ openai.com/api/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3536 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3536</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3536</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-196cc546041cb6db167784f632037f0a1dcf4a79</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/196cc546041cb6db167784f632037f0a1dcf4a79" target="_blank">Generating Natural Language Proofs with Verifier-Guided Search</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A novel stepwise method, NLProofS (Natural Language Proof Search), which learns to generate relevant steps conditioning on the hypothesis, which improves the correctness of predicted proofs from 27.7% to 33.3% in the distractor setting of EntailmentBank, demonstrating the effectiveness of NL proofS in generating challenging human-authored proofs.</p>
                <p><strong>Paper Abstract:</strong> Reasoning over natural language is a challenging problem in NLP. In this work, we focus on proof generation: Given a hypothesis and a set of supporting facts, the model generates a proof tree indicating how to derive the hypothesis from supporting facts. Compared to generating the entire proof in one shot, stepwise generation can better exploit the compositionality and generalize to longer proofs but has achieved limited success on real-world data. Existing stepwise methods struggle to generate proof steps that are both logically valid and relevant to the hypothesis. Instead, they tend to hallucinate invalid steps given the hypothesis. In this paper, we present a novel stepwise method, NLProofS (Natural Language Proof Search), which learns to generate relevant steps conditioning on the hypothesis. At the core of our approach, we train an independent verifier to check the validity of the proof steps to prevent hallucination. Instead of generating steps greedily, we search for proofs maximizing a global proof score judged by the verifier. NLProofS achieves state-of-the-art performance on EntailmentBank and RuleTaker. Specifically, it improves the correctness of predicted proofs from 27.7% to 33.3% in the distractor setting of EntailmentBank, demonstrating the effectiveness of NLProofS in generating challenging human-authored proofs.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3536.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3536.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLProofS (EntailmentBank)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NLProofS (Natural Language Proof Search) evaluated on EntailmentBank</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stepwise proof-generation system that conditions a T5-based stepwise prover on the hypothesis, uses a RoBERTa-based verifier to score individual steps, and performs non-local proof search to find high-scoring proof trees.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large (prover) + RoBERTa-large (verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prover: T5-large fine-tuned to generate candidate proof steps stepwise (text-to-text). Verifier: RoBERTa-large fine-tuned as a binary/score producer to estimate validity of (multi-premise -> conclusion) steps; step and prover scores are averaged to score candidates used during proof graph search.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5-large ~737M (prover); RoBERTa-large (verifier; size not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank (Tasks 1/2/3)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Proof generation in natural language: given a hypothesis and a set of supporting facts (with varying amounts of distractors), generate a tree-structured natural-language proof (leaf selection, intermediate conclusions, and proof steps). Task 1: no distractors; Task 2: 25 facts including distractors; Task 3: full corpus with retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Stepwise generation conditioned on hypothesis + independently trained verifier to score steps + symbolic proof-graph search (verifier-guided search) aggregating step validity into global proof score.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Task 2 (distractor) test: Overall-AllCorrect = 33.3% ± 1.5; Leaves-AllCorrect = 58.8% ± 1.8; Steps-AllCorrect = 34.4% ± 1.7; Intermediates-AllCorrect = 37.8% ± 1.6. Task 1 (no distractor) Overall-AllCorrect = 38.9% ± 0.7; Task 3 (full-corpus, using provided retrieval) Overall-AllCorrect = 6.9% ± 0.7. Reported averages are over 5 runs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>EntailmentWriter (single-shot T5-large) Task 2 Overall-AllCorrect = 20.9%; EntailmentWriter (T5-11B) Task 2 Overall-AllCorrect = 25.6%; MetGen Task 2 Overall-AllCorrect = 27.7%. (Values from Table 2 in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>On Task 2, NLProofS improves Overall-AllCorrect from 20.9% (EntailmentWriter T5-large) to 33.3% (absolute +12.4 percentage points), and from MetGen's 27.7% to 33.3% (+5.6 points). Also large improvements on Leaves and Steps metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still substantial failure modes: performance degrades with proof length (longer proofs much harder); prover is a bottleneck (misses ground-truth steps); hallucination remains possible though reduced; decoder beam search yields low diversity and permutation-duplicate candidates; scaling issues when concatenating many support facts; average inference time ~4.4s per example (2x stepwise baseline). Human eval: percentage of valid generated steps = 77% (NLProofS full), vs 43% for EntailmentWriter.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations show proof search is beneficial: w/o search Overall-AllCorrect drops (w/o search = 31.8% vs full 33.3%). Removing verifier or prover scores individually degrades performance (both scores needed). Oracle experiments: replacing prover with oracle yields large boost (Overall-AllCorrect on validation Task2 from 37.1% to 66.3%); oracle verifier alone gives small gain (~+0.8 overall), but combining oracle prover+verifier reaches 75.2% indicating both components matter and prover is currently the main bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Natural Language Proofs with Verifier-Guided Search', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3536.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3536.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLProofS (RuleTaker)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NLProofS (Natural Language Proof Search) evaluated on RuleTaker (OWA D0-D3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same NLProofS system evaluated on the synthetic RuleTaker benchmark; the verifier-guided search approach is applied to provable/disprovable hypothesis classification and proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large (prover) + RoBERTa-large (verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prover: T5-large fine-tuned for stepwise step generation in synthetic RuleTaker sentences. Verifier: RoBERTa-large trained with pseudo-negatives suitable for RuleTaker (including negations) to judge step validity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5-large ~737M (prover); RoBERTa-large (verifier; size not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RuleTaker (OWA), D0-D3 subset</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Synthetic template-based multi-step deduction dataset where hypotheses can be provable, disprovable, or neither. Requires producing a proof when provable/disprovable and classifying the hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Adapt proof generation to RuleTaker by generating proofs for both h and ¬h and training a linear classifier on the two proof scores; use NLProofS proof search with verifier to produce calibrated proof scores.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Test set (D0-D3): Answer accuracy = 99.5%; Proof accuracy (Overall across set) = 99.5 (Table 4 shows high accuracy broken down by proof depth: near 100% for shallow proofs and slightly lower for depth 3).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>ProofWriter (reported) achieves overall answer accuracy 99.7 and proof accuracy 99.7; FaiRR reported answer accuracy 99.6 and proof accuracy 99.6 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>NLProofS is competitive with prior methods (ProofWriter, FaiRR) on RuleTaker; not a large improvement but achieves comparable near-state-of-the-art numbers on synthetic dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>On RuleTaker, verifiers alone are sufficient in many cases (high verifier accuracy on synthetic data) and adding prover score made little difference; however, systems without verifiers fail to assign low scores to invalid hypotheses and perform poorly when adaptation requires distinguishing invalid hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations show verifier score alone suffices on RuleTaker (due to high verifier accuracy), whereas on EntailmentBank both verifier and prover scores are necessary; systems without verifiers perform much worse for classification of valid/invalid hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Natural Language Proofs with Verifier-Guided Search', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3536.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3536.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EntailmentWriter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EntailmentWriter (single-shot entailment tree generator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-shot T5-based model that generates entire entailment trees in one sequence; used as a strong baseline on EntailmentBank and compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explaining answers with entailment trees</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EntailmentWriter (T5-large / T5-11B variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single-shot sequence-to-sequence T5 models finetuned to map (hypothesis + context) to a serialized proof tree; evaluated in T5-large and T5-11B sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5-large ~737M; T5-11B ~11B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank (Tasks 1/2/3)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same proof-generation benchmark of expert-authored entailment trees; requires generating structured proofs from fact sets.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Single-shot generation of full proof trees using a pretrained large seq2seq model (T5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported (Task 2 distractor) Overall-AllCorrect = 20.9% (T5-large) and 25.6% (T5-11B). Leaves-AllCorrect = 35.6% (T5-large) and 48.8% (T5-11B).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Used as the primary baseline in this paper; NLProofS (T5-large + verifier-guided search) outperforms both T5-large and T5-11B EntailmentWriter in Task 2.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>NLProofS (T5-large) beats EntailmentWriter (T5-large) by +12.4 percentage points in Overall-AllCorrect on Task 2; even outperforms EntailmentWriter T5-11B by a substantial margin.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Tends to hallucinate invalid conclusions (examples shown); when failing to reason, model sometimes copies premises rather than deriving conclusions; single-shot models may struggle with compositional generalization to longer proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper shows that stepwise generation plus verifier-guided search (NLProofS) outperforms single-shot EntailmentWriter, and replicating single-shot in their unified codebase still underperforms stepwise baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Natural Language Proofs with Verifier-Guided Search', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3536.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3536.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetGen (module-based entailment tree generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A module-based, stepwise (or module-factored) entailment tree generator that uses additional data and reasoning-type annotations to guide generation; used as a baseline on EntailmentBank.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MetGen: A module-based entailment tree generation framework for answer explanation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MetGen (T5-large based, trained with extra data and annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MetGen uses modular components and collects additional annotation/data (templates from Wikipedia and manually annotated reasoning types) to help generation; reported as stronger prior work on EntailmentBank but requiring extra resources.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Reported variants include a larger prefixed model; in paper comparison MetGen reported but NLProofS uses only T5-large.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank (Tasks 1/2/3)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same entailment-tree proof generation benchmark; MetGen is designed to better model reasoning types and uses extra training data.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Modular architecture with reasoning-type conditioning and additional Wikipedia-derived training data plus manual annotation of reasoning types for some training steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Task 2 (distractor) Overall-AllCorrect = 27.7% (reported in Table 2). Task 1 (no-distractor) Overall-AllCorrect = 36.5% (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared against EntailmentWriter and NLProofS; MetGen performs well but requires extra data/annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>NLProofS (T5-large) outperforms MetGen on Task 2 (33.3% vs 27.7%) despite MetGen using more data and annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Relies on additional data collection and manual annotation of reasoning types; not strictly comparable resource-wise to methods trained only on EntailmentBank.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper notes that MetGen's improvements come in part from extra data and annotated reasoning types; NLProofS achieves better performance without that extra annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Natural Language Proofs with Verifier-Guided Search', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3536.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3536.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProofWriter / FaiRR (baselines on RuleTaker)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProofWriter and FaiRR (iterative/stepwise reasoning baselines evaluated on RuleTaker)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior stepwise proof generation methods (ProofWriter and FaiRR) built on T5 that have reported strong performance on synthetic RuleTaker and are used as baselines in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ProofWriter: Generating implications, proofs, and abductive statements over natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ProofWriter / FaiRR (T5-large based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ProofWriter: iterative stepwise generation of proofs with constraints; FaiRR: constraint-based forward reasoning that forbids access to hypothesis when generating intermediate steps (to avoid hallucination). Both operate in synthetic rule/fact settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5-large (used in reported comparisons); exact sizes as reported in the referenced works.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RuleTaker (OWA D0-D3)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Synthetic multi-step deduction tasks with provable/disprovable/hypothesis classification requiring logical entailment from template-generated rules and facts.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Stepwise generation with hypotheses-access restrictions (FaiRR) or iterative constrained decoding (ProofWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported (Table 4): ProofWriter overall proof accuracy ~99.8 and answer accuracy 99.7; FaiRR reported answer accuracy 99.6 and proof accuracy 99.6. NLProofS achieves comparable results (answer accuracy 99.5, proof accuracy ~99.5).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>These are the competitive baselines on RuleTaker; NLProofS is roughly competitive with them on synthetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>NLProofS does not substantially outperform these baselines on RuleTaker but is competitive while also being applicable to human-authored proofs (EntailmentBank) where some prior methods do not scale.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Prior baselines often tailored to RuleTaker assumptions (e.g., distinguishing rules vs facts) and may not extend well to human-authored natural language proofs; methods that ban hypothesis access produce many valid but irrelevant steps on real-world data.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper discusses why some prior methods work well on RuleTaker but fail on EntailmentBank: synthetic templates and simpler language make hallucination and relevance issues less severe in RuleTaker.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Natural Language Proofs with Verifier-Guided Search', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3536.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3536.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (few-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (text-davinci-002) evaluated with few-shot in-context prompting for proof generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large autoregressive LM (OpenAI GPT-3) tested via few-shot prompting to generate entailment trees; included to test whether very large LMs can perform the task out-of-the-box.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-002) via few-shot in-context prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 family autoregressive transformer evaluated with 7 in-context examples prompting to produce proof tree outputs; no fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in paper (GPT-3 family sizes are known externally, but not restated here); treated as large few-shot model.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank (Task 2) validation set, few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Attempt to generate full entailment trees via in-context examples without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Few-shot chain-of-thought style prompting with 7 in-context examples; output is parsed as serialized proof tree.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Validation Task 2: Overall-AllCorrect = 12.3% ± 1.4; Leaves-AllCorrect = 15.3% ± 1.9; Steps-AllCorrect = 12.3% ±1.4 (Table B). Performance substantially worse than finetuned methods.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to NLProofS validation (Overall-AllCorrect 37.1% ±1.5) and EntailmentWriter (T5-large) validation ~27.3% (Table B), GPT-3 performs much worse.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>No improvement; few-shot GPT-3 is far inferior to finetuned models for this structured proof generation task.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Few-shot prompting with GPT-3 struggles to produce structured proof trees and hallucinates; not suitable out-of-the-box for this strict logical proof generation task without task-specific fine-tuning or verifier/search interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper reports mean/std over three prompt-sampling runs and notes GPT-3/Codex cannot easily solve proof generation via few-shot prompting in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Natural Language Proofs with Verifier-Guided Search', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3536.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3536.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex (few-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Codex (code-davinci-002) evaluated with few-shot in-context prompting for proof generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-specialized large LM (Codex) also evaluated with few-shot prompting for entailment tree generation; performed slightly better than GPT-3 but still far behind finetuned systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating large language models trained on code</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex (code-davinci-002) via few-shot in-context prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Code-specialized autoregressive Transformer used with 7 in-context examples to attempt proof generation; no fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank (Task 2) validation set, few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same as GPT-3 entry; evaluation of few-shot performance on structured proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Few-shot prompting with examples; outputs parsed to extract proof tree structure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Validation Task 2: Overall-AllCorrect = 14.4% ± 1.4; Leaves-AllCorrect = 19.8% ± 3.2; Steps-AllCorrect = 14.6% ± 1.7 (Table B).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Significantly below NLProofS and EntailmentWriter finetuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>No improvement; Codex slightly better than GPT-3 but still poor relative to finetuned methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Few-shot Codex fails to produce accurate structured natural-language proofs; not adequate for this strict logical reasoning task without task-specific training.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors note Codex performs better than GPT-3 here (consistent with other findings) but still inadequate for proof generation in EntailmentBank.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Natural Language Proofs with Verifier-Guided Search', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3536.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3536.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verifier-guided search (method)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Verifier-guided proof search (core NLProofS intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The central intervention: generate multiple candidate proof steps via a stepwise prover conditioned on hypothesis, score steps with an independent verifier, then perform search in a proof graph to maximize a global proof score aggregated from step scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Technique combining T5-large (prover) and RoBERTa-large (verifier) within NLProofS</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Independent verifier trained to classify/score step validity (RoBERTa-large); stepwise T5 generates beam of candidates; scores (prover likelihood + verifier output) averaged per candidate; proof graph search iteratively expands/updates to find max-scoring proof tree.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Depends on component models (T5-large ~737M; RoBERTa-large unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank and RuleTaker proof generation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Structured proof generation requiring strict logical validity of multi-premise -> conclusion steps and selection of relevant supporting facts among distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Verifier to reduce hallucination + non-local search to aggregate local step scores into a global proof objective (min-based aggregation across tree nodes) and escape greedy local decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Substantial improvement over baselines on EntailmentBank Task 2 (Overall-AllCorrect 33.3% vs 20.9% EntailmentWriter T5-large), better leaf selection and reduced hallucination (human-eval valid-step rates: NLProofS 77% vs EntailmentWriter 43%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to stepwise prover without verifier/search: stepwise baseline Overall-AllCorrect (w/o search) = 31.8% (validation/test differences reported); single-shot baseline overall ~27.1% in unified implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Search + verifier yields largest gains versus stepwise-alone and single-shot methods, especially on real human-authored proofs with distractors; ablation shows both prover and verifier scores are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Search adds runtime (approx 2x slower than stepwise baseline) and depends on quality/diversity of candidate steps from prover; verifier training used pseudo-negatives, which may be imperfect; scoring aggregation (min operator) is a simple choice—other aggregations unexplored.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations: removing search reduces overall performance; removing verifier or prover score harms results; oracle-prover ablation indicates prover coverage is major bottleneck; oracle-verifier helps but less so unless prover can propose ground-truth steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Natural Language Proofs with Verifier-Guided Search', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Explaining answers with entailment trees <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>FaiRR: Faithful and robust deductive reasoning over natural language <em>(Rating: 2)</em></li>
                <li>Natural language deduction through search over statement compositions <em>(Rating: 2)</em></li>
                <li>PRover: Proof generation for interpretable reasoning over rules <em>(Rating: 1)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3536",
    "paper_id": "paper-196cc546041cb6db167784f632037f0a1dcf4a79",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "NLProofS (EntailmentBank)",
            "name_full": "NLProofS (Natural Language Proof Search) evaluated on EntailmentBank",
            "brief_description": "A stepwise proof-generation system that conditions a T5-based stepwise prover on the hypothesis, uses a RoBERTa-based verifier to score individual steps, and performs non-local proof search to find high-scoring proof trees.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-large (prover) + RoBERTa-large (verifier)",
            "model_description": "Prover: T5-large fine-tuned to generate candidate proof steps stepwise (text-to-text). Verifier: RoBERTa-large fine-tuned as a binary/score producer to estimate validity of (multi-premise -&gt; conclusion) steps; step and prover scores are averaged to score candidates used during proof graph search.",
            "model_size": "T5-large ~737M (prover); RoBERTa-large (verifier; size not specified in paper)",
            "reasoning_task_name": "EntailmentBank (Tasks 1/2/3)",
            "reasoning_task_description": "Proof generation in natural language: given a hypothesis and a set of supporting facts (with varying amounts of distractors), generate a tree-structured natural-language proof (leaf selection, intermediate conclusions, and proof steps). Task 1: no distractors; Task 2: 25 facts including distractors; Task 3: full corpus with retrieval.",
            "method_or_intervention": "Stepwise generation conditioned on hypothesis + independently trained verifier to score steps + symbolic proof-graph search (verifier-guided search) aggregating step validity into global proof score.",
            "performance": "Task 2 (distractor) test: Overall-AllCorrect = 33.3% ± 1.5; Leaves-AllCorrect = 58.8% ± 1.8; Steps-AllCorrect = 34.4% ± 1.7; Intermediates-AllCorrect = 37.8% ± 1.6. Task 1 (no distractor) Overall-AllCorrect = 38.9% ± 0.7; Task 3 (full-corpus, using provided retrieval) Overall-AllCorrect = 6.9% ± 0.7. Reported averages are over 5 runs.",
            "baseline_performance": "EntailmentWriter (single-shot T5-large) Task 2 Overall-AllCorrect = 20.9%; EntailmentWriter (T5-11B) Task 2 Overall-AllCorrect = 25.6%; MetGen Task 2 Overall-AllCorrect = 27.7%. (Values from Table 2 in paper.)",
            "improvement_over_baseline": "On Task 2, NLProofS improves Overall-AllCorrect from 20.9% (EntailmentWriter T5-large) to 33.3% (absolute +12.4 percentage points), and from MetGen's 27.7% to 33.3% (+5.6 points). Also large improvements on Leaves and Steps metrics.",
            "limitations_or_failures": "Still substantial failure modes: performance degrades with proof length (longer proofs much harder); prover is a bottleneck (misses ground-truth steps); hallucination remains possible though reduced; decoder beam search yields low diversity and permutation-duplicate candidates; scaling issues when concatenating many support facts; average inference time ~4.4s per example (2x stepwise baseline). Human eval: percentage of valid generated steps = 77% (NLProofS full), vs 43% for EntailmentWriter.",
            "ablation_or_analysis": "Ablations show proof search is beneficial: w/o search Overall-AllCorrect drops (w/o search = 31.8% vs full 33.3%). Removing verifier or prover scores individually degrades performance (both scores needed). Oracle experiments: replacing prover with oracle yields large boost (Overall-AllCorrect on validation Task2 from 37.1% to 66.3%); oracle verifier alone gives small gain (~+0.8 overall), but combining oracle prover+verifier reaches 75.2% indicating both components matter and prover is currently the main bottleneck.",
            "uuid": "e3536.0",
            "source_info": {
                "paper_title": "Generating Natural Language Proofs with Verifier-Guided Search",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "NLProofS (RuleTaker)",
            "name_full": "NLProofS (Natural Language Proof Search) evaluated on RuleTaker (OWA D0-D3)",
            "brief_description": "Same NLProofS system evaluated on the synthetic RuleTaker benchmark; the verifier-guided search approach is applied to provable/disprovable hypothesis classification and proof generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-large (prover) + RoBERTa-large (verifier)",
            "model_description": "Prover: T5-large fine-tuned for stepwise step generation in synthetic RuleTaker sentences. Verifier: RoBERTa-large trained with pseudo-negatives suitable for RuleTaker (including negations) to judge step validity.",
            "model_size": "T5-large ~737M (prover); RoBERTa-large (verifier; size not specified in paper)",
            "reasoning_task_name": "RuleTaker (OWA), D0-D3 subset",
            "reasoning_task_description": "Synthetic template-based multi-step deduction dataset where hypotheses can be provable, disprovable, or neither. Requires producing a proof when provable/disprovable and classifying the hypothesis.",
            "method_or_intervention": "Adapt proof generation to RuleTaker by generating proofs for both h and ¬h and training a linear classifier on the two proof scores; use NLProofS proof search with verifier to produce calibrated proof scores.",
            "performance": "Test set (D0-D3): Answer accuracy = 99.5%; Proof accuracy (Overall across set) = 99.5 (Table 4 shows high accuracy broken down by proof depth: near 100% for shallow proofs and slightly lower for depth 3).",
            "baseline_performance": "ProofWriter (reported) achieves overall answer accuracy 99.7 and proof accuracy 99.7; FaiRR reported answer accuracy 99.6 and proof accuracy 99.6 (Table 4).",
            "improvement_over_baseline": "NLProofS is competitive with prior methods (ProofWriter, FaiRR) on RuleTaker; not a large improvement but achieves comparable near-state-of-the-art numbers on synthetic dataset.",
            "limitations_or_failures": "On RuleTaker, verifiers alone are sufficient in many cases (high verifier accuracy on synthetic data) and adding prover score made little difference; however, systems without verifiers fail to assign low scores to invalid hypotheses and perform poorly when adaptation requires distinguishing invalid hypotheses.",
            "ablation_or_analysis": "Ablations show verifier score alone suffices on RuleTaker (due to high verifier accuracy), whereas on EntailmentBank both verifier and prover scores are necessary; systems without verifiers perform much worse for classification of valid/invalid hypotheses.",
            "uuid": "e3536.1",
            "source_info": {
                "paper_title": "Generating Natural Language Proofs with Verifier-Guided Search",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "EntailmentWriter",
            "name_full": "EntailmentWriter (single-shot entailment tree generator)",
            "brief_description": "A single-shot T5-based model that generates entire entailment trees in one sequence; used as a strong baseline on EntailmentBank and compared in this paper.",
            "citation_title": "Explaining answers with entailment trees",
            "mention_or_use": "use",
            "model_name": "EntailmentWriter (T5-large / T5-11B variants)",
            "model_description": "Single-shot sequence-to-sequence T5 models finetuned to map (hypothesis + context) to a serialized proof tree; evaluated in T5-large and T5-11B sizes.",
            "model_size": "T5-large ~737M; T5-11B ~11B",
            "reasoning_task_name": "EntailmentBank (Tasks 1/2/3)",
            "reasoning_task_description": "Same proof-generation benchmark of expert-authored entailment trees; requires generating structured proofs from fact sets.",
            "method_or_intervention": "Single-shot generation of full proof trees using a pretrained large seq2seq model (T5).",
            "performance": "Reported (Task 2 distractor) Overall-AllCorrect = 20.9% (T5-large) and 25.6% (T5-11B). Leaves-AllCorrect = 35.6% (T5-large) and 48.8% (T5-11B).",
            "baseline_performance": "Used as the primary baseline in this paper; NLProofS (T5-large + verifier-guided search) outperforms both T5-large and T5-11B EntailmentWriter in Task 2.",
            "improvement_over_baseline": "NLProofS (T5-large) beats EntailmentWriter (T5-large) by +12.4 percentage points in Overall-AllCorrect on Task 2; even outperforms EntailmentWriter T5-11B by a substantial margin.",
            "limitations_or_failures": "Tends to hallucinate invalid conclusions (examples shown); when failing to reason, model sometimes copies premises rather than deriving conclusions; single-shot models may struggle with compositional generalization to longer proofs.",
            "ablation_or_analysis": "Paper shows that stepwise generation plus verifier-guided search (NLProofS) outperforms single-shot EntailmentWriter, and replicating single-shot in their unified codebase still underperforms stepwise baselines.",
            "uuid": "e3536.2",
            "source_info": {
                "paper_title": "Generating Natural Language Proofs with Verifier-Guided Search",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "MetGen",
            "name_full": "MetGen (module-based entailment tree generation)",
            "brief_description": "A module-based, stepwise (or module-factored) entailment tree generator that uses additional data and reasoning-type annotations to guide generation; used as a baseline on EntailmentBank.",
            "citation_title": "MetGen: A module-based entailment tree generation framework for answer explanation",
            "mention_or_use": "mention",
            "model_name": "MetGen (T5-large based, trained with extra data and annotations)",
            "model_description": "MetGen uses modular components and collects additional annotation/data (templates from Wikipedia and manually annotated reasoning types) to help generation; reported as stronger prior work on EntailmentBank but requiring extra resources.",
            "model_size": "Reported variants include a larger prefixed model; in paper comparison MetGen reported but NLProofS uses only T5-large.",
            "reasoning_task_name": "EntailmentBank (Tasks 1/2/3)",
            "reasoning_task_description": "Same entailment-tree proof generation benchmark; MetGen is designed to better model reasoning types and uses extra training data.",
            "method_or_intervention": "Modular architecture with reasoning-type conditioning and additional Wikipedia-derived training data plus manual annotation of reasoning types for some training steps.",
            "performance": "Task 2 (distractor) Overall-AllCorrect = 27.7% (reported in Table 2). Task 1 (no-distractor) Overall-AllCorrect = 36.5% (reported).",
            "baseline_performance": "Compared against EntailmentWriter and NLProofS; MetGen performs well but requires extra data/annotations.",
            "improvement_over_baseline": "NLProofS (T5-large) outperforms MetGen on Task 2 (33.3% vs 27.7%) despite MetGen using more data and annotations.",
            "limitations_or_failures": "Relies on additional data collection and manual annotation of reasoning types; not strictly comparable resource-wise to methods trained only on EntailmentBank.",
            "ablation_or_analysis": "Paper notes that MetGen's improvements come in part from extra data and annotated reasoning types; NLProofS achieves better performance without that extra annotation.",
            "uuid": "e3536.3",
            "source_info": {
                "paper_title": "Generating Natural Language Proofs with Verifier-Guided Search",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "ProofWriter / FaiRR (baselines on RuleTaker)",
            "name_full": "ProofWriter and FaiRR (iterative/stepwise reasoning baselines evaluated on RuleTaker)",
            "brief_description": "Prior stepwise proof generation methods (ProofWriter and FaiRR) built on T5 that have reported strong performance on synthetic RuleTaker and are used as baselines in this paper.",
            "citation_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "mention_or_use": "mention",
            "model_name": "ProofWriter / FaiRR (T5-large based)",
            "model_description": "ProofWriter: iterative stepwise generation of proofs with constraints; FaiRR: constraint-based forward reasoning that forbids access to hypothesis when generating intermediate steps (to avoid hallucination). Both operate in synthetic rule/fact settings.",
            "model_size": "T5-large (used in reported comparisons); exact sizes as reported in the referenced works.",
            "reasoning_task_name": "RuleTaker (OWA D0-D3)",
            "reasoning_task_description": "Synthetic multi-step deduction tasks with provable/disprovable/hypothesis classification requiring logical entailment from template-generated rules and facts.",
            "method_or_intervention": "Stepwise generation with hypotheses-access restrictions (FaiRR) or iterative constrained decoding (ProofWriter).",
            "performance": "Reported (Table 4): ProofWriter overall proof accuracy ~99.8 and answer accuracy 99.7; FaiRR reported answer accuracy 99.6 and proof accuracy 99.6. NLProofS achieves comparable results (answer accuracy 99.5, proof accuracy ~99.5).",
            "baseline_performance": "These are the competitive baselines on RuleTaker; NLProofS is roughly competitive with them on synthetic data.",
            "improvement_over_baseline": "NLProofS does not substantially outperform these baselines on RuleTaker but is competitive while also being applicable to human-authored proofs (EntailmentBank) where some prior methods do not scale.",
            "limitations_or_failures": "Prior baselines often tailored to RuleTaker assumptions (e.g., distinguishing rules vs facts) and may not extend well to human-authored natural language proofs; methods that ban hypothesis access produce many valid but irrelevant steps on real-world data.",
            "ablation_or_analysis": "Paper discusses why some prior methods work well on RuleTaker but fail on EntailmentBank: synthetic templates and simpler language make hallucination and relevance issues less severe in RuleTaker.",
            "uuid": "e3536.4",
            "source_info": {
                "paper_title": "Generating Natural Language Proofs with Verifier-Guided Search",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "GPT-3 (few-shot prompting)",
            "name_full": "GPT-3 (text-davinci-002) evaluated with few-shot in-context prompting for proof generation",
            "brief_description": "Large autoregressive LM (OpenAI GPT-3) tested via few-shot prompting to generate entailment trees; included to test whether very large LMs can perform the task out-of-the-box.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-002) via few-shot in-context prompting",
            "model_description": "GPT-3 family autoregressive transformer evaluated with 7 in-context examples prompting to produce proof tree outputs; no fine-tuning.",
            "model_size": "Not specified in paper (GPT-3 family sizes are known externally, but not restated here); treated as large few-shot model.",
            "reasoning_task_name": "EntailmentBank (Task 2) validation set, few-shot prompting",
            "reasoning_task_description": "Attempt to generate full entailment trees via in-context examples without fine-tuning.",
            "method_or_intervention": "Few-shot chain-of-thought style prompting with 7 in-context examples; output is parsed as serialized proof tree.",
            "performance": "Validation Task 2: Overall-AllCorrect = 12.3% ± 1.4; Leaves-AllCorrect = 15.3% ± 1.9; Steps-AllCorrect = 12.3% ±1.4 (Table B). Performance substantially worse than finetuned methods.",
            "baseline_performance": "Compared to NLProofS validation (Overall-AllCorrect 37.1% ±1.5) and EntailmentWriter (T5-large) validation ~27.3% (Table B), GPT-3 performs much worse.",
            "improvement_over_baseline": "No improvement; few-shot GPT-3 is far inferior to finetuned models for this structured proof generation task.",
            "limitations_or_failures": "Few-shot prompting with GPT-3 struggles to produce structured proof trees and hallucinates; not suitable out-of-the-box for this strict logical proof generation task without task-specific fine-tuning or verifier/search interventions.",
            "ablation_or_analysis": "Paper reports mean/std over three prompt-sampling runs and notes GPT-3/Codex cannot easily solve proof generation via few-shot prompting in this setting.",
            "uuid": "e3536.5",
            "source_info": {
                "paper_title": "Generating Natural Language Proofs with Verifier-Guided Search",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Codex (few-shot prompting)",
            "name_full": "Codex (code-davinci-002) evaluated with few-shot in-context prompting for proof generation",
            "brief_description": "A code-specialized large LM (Codex) also evaluated with few-shot prompting for entailment tree generation; performed slightly better than GPT-3 but still far behind finetuned systems.",
            "citation_title": "Evaluating large language models trained on code",
            "mention_or_use": "use",
            "model_name": "Codex (code-davinci-002) via few-shot in-context prompting",
            "model_description": "Code-specialized autoregressive Transformer used with 7 in-context examples to attempt proof generation; no fine-tuning.",
            "model_size": "Not specified in paper",
            "reasoning_task_name": "EntailmentBank (Task 2) validation set, few-shot prompting",
            "reasoning_task_description": "Same as GPT-3 entry; evaluation of few-shot performance on structured proof generation.",
            "method_or_intervention": "Few-shot prompting with examples; outputs parsed to extract proof tree structure.",
            "performance": "Validation Task 2: Overall-AllCorrect = 14.4% ± 1.4; Leaves-AllCorrect = 19.8% ± 3.2; Steps-AllCorrect = 14.6% ± 1.7 (Table B).",
            "baseline_performance": "Significantly below NLProofS and EntailmentWriter finetuned models.",
            "improvement_over_baseline": "No improvement; Codex slightly better than GPT-3 but still poor relative to finetuned methods.",
            "limitations_or_failures": "Few-shot Codex fails to produce accurate structured natural-language proofs; not adequate for this strict logical reasoning task without task-specific training.",
            "ablation_or_analysis": "Authors note Codex performs better than GPT-3 here (consistent with other findings) but still inadequate for proof generation in EntailmentBank.",
            "uuid": "e3536.6",
            "source_info": {
                "paper_title": "Generating Natural Language Proofs with Verifier-Guided Search",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Verifier-guided search (method)",
            "name_full": "Verifier-guided proof search (core NLProofS intervention)",
            "brief_description": "The central intervention: generate multiple candidate proof steps via a stepwise prover conditioned on hypothesis, score steps with an independent verifier, then perform search in a proof graph to maximize a global proof score aggregated from step scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Technique combining T5-large (prover) and RoBERTa-large (verifier) within NLProofS",
            "model_description": "Independent verifier trained to classify/score step validity (RoBERTa-large); stepwise T5 generates beam of candidates; scores (prover likelihood + verifier output) averaged per candidate; proof graph search iteratively expands/updates to find max-scoring proof tree.",
            "model_size": "Depends on component models (T5-large ~737M; RoBERTa-large unspecified)",
            "reasoning_task_name": "EntailmentBank and RuleTaker proof generation",
            "reasoning_task_description": "Structured proof generation requiring strict logical validity of multi-premise -&gt; conclusion steps and selection of relevant supporting facts among distractors.",
            "method_or_intervention": "Verifier to reduce hallucination + non-local search to aggregate local step scores into a global proof objective (min-based aggregation across tree nodes) and escape greedy local decisions.",
            "performance": "Substantial improvement over baselines on EntailmentBank Task 2 (Overall-AllCorrect 33.3% vs 20.9% EntailmentWriter T5-large), better leaf selection and reduced hallucination (human-eval valid-step rates: NLProofS 77% vs EntailmentWriter 43%).",
            "baseline_performance": "Compared to stepwise prover without verifier/search: stepwise baseline Overall-AllCorrect (w/o search) = 31.8% (validation/test differences reported); single-shot baseline overall ~27.1% in unified implementation.",
            "improvement_over_baseline": "Search + verifier yields largest gains versus stepwise-alone and single-shot methods, especially on real human-authored proofs with distractors; ablation shows both prover and verifier scores are needed.",
            "limitations_or_failures": "Search adds runtime (approx 2x slower than stepwise baseline) and depends on quality/diversity of candidate steps from prover; verifier training used pseudo-negatives, which may be imperfect; scoring aggregation (min operator) is a simple choice—other aggregations unexplored.",
            "ablation_or_analysis": "Ablations: removing search reduces overall performance; removing verifier or prover score harms results; oracle-prover ablation indicates prover coverage is major bottleneck; oracle-verifier helps but less so unless prover can propose ground-truth steps.",
            "uuid": "e3536.7",
            "source_info": {
                "paper_title": "Generating Natural Language Proofs with Verifier-Guided Search",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Explaining answers with entailment trees",
            "rating": 2
        },
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2
        },
        {
            "paper_title": "FaiRR: Faithful and robust deductive reasoning over natural language",
            "rating": 2
        },
        {
            "paper_title": "Natural language deduction through search over statement compositions",
            "rating": 2
        },
        {
            "paper_title": "PRover: Proof generation for interpretable reasoning over rules",
            "rating": 1
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1
        }
    ],
    "cost": 0.020562249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Generating Natural Language Proofs with Verifier-Guided Search</h1>
<p>Kaiyu Yang and Jia Deng and Danqi Chen<br>Department of Computer Science<br>Princeton University<br>{kaiyuy,jiadeng,danqic}@cs.princeton.edu</p>
<h4>Abstract</h4>
<p>Reasoning over natural language is a challenging problem in NLP. In this work, we focus on proof generation: Given a hypothesis and a set of supporting facts, the model generates a proof tree indicating how to derive the hypothesis from supporting facts. Compared to generating the entire proof in one shot, stepwise generation can better exploit the compositionality and generalize to longer proofs but has achieved limited success on real-world data. Existing stepwise methods struggle to generate proof steps that are both logically valid and relevant to the hypothesis. Instead, they tend to hallucinate invalid steps given the hypothesis. In this paper, we present a novel stepwise method, NLProofS (Natural Language Proof Search), which learns to generate relevant steps conditioning on the hypothesis. At the core of our approach, we train an independent verifier to check the validity of the proof steps to prevent hallucination. Instead of generating steps greedily, we search for proofs maximizing a global proof score judged by the verifier. NLProofS achieves state-of-the-art performance on EntailmentBank and RuleTaker. Specifically, it improves the correctness of predicted proofs from $27.7 \%$ to $33.3 \%$ in the distractor setting of EntailmentBank, demonstrating the effectiveness of NLProofS in generating challenging human-authored proofs. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>A fundamental goal of AI since its inception is automated reasoning (McCarthy et al., 1960): given explicitly provided knowledge as assumptions, we want the system to draw logically valid conclusions. Research in automated reasoning has traditionally focused on structured domains such as formal logic (Robinson and Voronkov, 2001). On the other hand, recent work suggests that free-form natural language can be a suitable vehicle for reasoning (Clark et al., 2020; Dalvi et al., 2021), because natural language represents knowledge</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>without requiring labour-intensive formalization. However, reasoning in natural language is challenging, as it requires compositional generalization to novel examples (Ruis et al., 2020)—a capability that state-of-the-art large language models struggle with (Rae et al., 2021).</p>
<p>In this work, we focus on proof generation in natural language (Fig. 1): given a hypothesis and a set of supporting facts in natural language, the model generates a proof tree indicating how the hypothesis is derived from a subset of the supporting facts. The proof tree may contain intermediate conclusions, which need to be generated by the model. Existing methods generate the proof either in a single shot or step by step. Stepwise methods leverage the compositionality of proofs, making it easier for the model to learn and generalize to longer proofs (Tafjord et al., 2021).</p>
<p>However, existing stepwise methods suffer from a trade-off between generating valid steps and relevant steps. Prior works (Sanyal et al., 2022; Bostrom et al., 2022) have observed that, given the hypothesis, the model often learns to hallucinate invalid proof steps leading to the hypothesis, instead of performing valid logical inference (see examples in Table 3). To mitigate this issue, previous attempts have restricted the model from accessing the hypothesis, forcing it to generate conclusions based solely on known premises. However, without the hypothesis, the model tends to generate many valid but irrelevant steps. This problem is especially prominent for real-world natural language proofs. Due to the inherent ambiguity of natural language, the search space for each proof step is much larger than that of simple synthetic tasks. That may explain why stepwise methods have demonstrated superior performance on the simple, synthetic RuleTaker dataset (Clark et al., 2020) but not on the more realistic, human-authored EntailmentBank dataset (Dalvi et al., 2021), which is the gap we aim to bridge.</p>
<p>We introduce NLProofS, a novel method for stepwise proof generation. It generates proof steps conditioning on the hypothesis, enabling the model to learn to generate only relevant steps. To prevent hallucination, it trains an independent verifier based on RoBERTa (Liu et al., 2019), which takes a single step (including multiple premises and one conclusion) as input and produces a score indicating its validity. During inference, instead of generating steps greedily, NLProofS searches for proofs</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: <em>Top</em>: In proof generation, given a hypothesis and multiple supporting facts (potentially with redundant facts), the model generates a proof tree, including both the tree structure and the intermediate conclusions (<em>int1</em> and <em>int2</em>). A common approach encodes the input/output as text sequences and generates the proof in a single-shot (<em>Middle</em>) or generates the proof step by step (<em>Bottom</em>, showing only one of the three steps) using text-to-text models.</p>
<p>that maximize a proof score aggregating the validity scores of all steps.</p>
<p>We evaluate NLProofS on two benchmarks: Rule-Taker (Clark et al., 2020) and EntailmentBank (Dalvi et al., 2021). RuleTaker consists of simple, synthetic English sentences generated from templates. In contrast, proofs in EntailmentBank are authored by human experts and are more challenging. They are in unconstrained natural language and exhibit considerable fuzziness and ambiguity, as is typical for reasoning in natural language. NLProofS achieves state-of-the-art performance on both datasets. On EntailmentBank, it outperforms previous best results by a large margin. For example, in the distractor task setting, it improves the accuracy of generating complete proofs from 27.7% to 33.3% and the accuracy of identifying relevant supporting facts from 46.1% to 58.8%, which demonstrates the effectiveness of our method in generating challenging human-authored proofs.</p>
<p>In addition, we conduct extensive ablations to gain insights. First, we show that the verifier plays a crucial role in generating proofs by providing well-calibrated validity scores. Without the verifier, our method performs worse on EntailmentBank and fails completely on RuleTaker. Second, while generating long proofs remains a major challenge, NLProofS leads to large improvements for long proofs. Third, there is still a large room for future improvement, e.g., by generating more accurate and diverse proof steps as candidates for the search algorithm to explore.</p>
<p><strong>Contributions.</strong> In summary, our contributions are two-fold. First, we introduce NLProofS, a stepwise proof generation method that searches for proofs whose validity is scored by a verifier. It substantially advances state-of-the-art performance on the challenging EntailmentBank dataset. Second, through extensive analyses and ablations, we shed light on the performance improvement and reveal the current bottleneck. Our work is a first step exploring the interplay between verifiers and proof search in generating natural language proofs, and we expect further advancements down the road.</p>
<h2>2 Related Work</h2>
<p><strong>Proof generation in natural language.</strong> Table 1 summarizes existing methods for generating natural language proofs, including single-shot and stepwise methods. Single-shot methods generate the entire proof tree in one shot, enforcing structural constraints explicitly via linear integer programming (Saha et al., 2020; Sun et al., 2021) or implicitly via pretrained text-to-text transformers (Gontier et al., 2020; Dalvi et al., 2021) (Fig. 1 <em>Middle</em>). In contrast, stepwise methods generate the proof as individual proof steps, forward (Tafjord et al., 2021; Sanyal et al., 2022; Bostrom et al., 2022), backward (Liang et al., 2021; Qu et al., 2022; Dalvi et al., 2022), or both (Hong et al., 2022). Our method generates proofs stepwise, in the forward direction.</p>
<p>When generating a proof step, prior work has observed that if the hypothesis is available, the model often uses it to hallucinate the intermediate conclusion instead of drawing valid logical inferences (Table 3). Therefore, ProofWriter (Tafjord et al., 2021), FaiRR (Sanyal et al., 2022), and SCSearch (Bostrom et al., 2022) explicitly ban the model from accessing the hypothesis when generating intermediate conclusions, forcing it to draw</p>
<p><sup>2</sup>The hypothesis may be used for premise selection.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Stepwise</th>
<th style="text-align: center;">Proof <br> direction</th>
<th style="text-align: center;">Generate intermediates <br> w/ hypothesis</th>
<th style="text-align: center;">Verifier</th>
<th style="text-align: center;">Non-local <br> search</th>
<th style="text-align: center;">Evaluated on <br> human-authored proofs</th>
<th style="text-align: center;">No external <br> data</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PRover</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">No intermediates</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">EntailmentWriter</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">ProofWriter</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">FaiRR</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">SCSearch</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">MetGen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Both</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Dalvi et al. (2022)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\leftarrow$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">NLProofS (ours)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1: A comparison of NLProofS with existing methods for proof generation: PRover (Saha et al., 2020), EntailmentWriter (Dalvi et al., 2021), ProofWriter (Tafjord et al., 2021), FaiRR (Sanyal et al., 2022), SCSearch (Bostrom et al., 2022), MetGen (Hong et al., 2022), and a concurrent work (Dalvi et al., 2022) marked with $\dagger . \rightarrow$ and $\leftarrow$ denote forward/backward stepwise proof generation.
inference from known premises only. However, without the hypothesis, the model may generate many valid proof steps irrelevant to the hypothesis. Unlike other forward stepwise methods, our model has access to the hypothesis but relies on a verifier to check the validity of proof steps and prevent hallucination.</p>
<p>Dalvi et al. (2022) is a concurrent work that also uses a verifier to score multiple candidate proof steps generated by the model. However, they use the scores to make a greedy local decision, selecting the best step and discarding others, whereas we search for proofs with the maximum aggregated scores. Besides, they train the verifier on additionally annotated negative examples, whereas we train on pseudo-negative examples generated automatically without additional annotation efforts (Sec. 4.2). Other stepwise methods in Table 1 do not have verifiers, and they make local decisions.</p>
<p>PRover (Saha et al., 2020), ProofWriter, and FaiRR have only evaluated on the simple RuleTaker dataset (Clark et al., 2020). And it is nontrivial to extend them to real-world data. For example, FaiRR assumes sentences fall into two categories: rules and facts, which are tailored for RuleTaker. Dalvi et al. (2021) introduce EntailmentBank, a challenging benchmark of proofs authored by human experts, which is used to evaluate EntailmentWriter, their method for single-shot proof generation. SCSearch and Dalvi et al. (2022) also use EntailmentBank but focus on different task settings that do not quantitatively evaluate the generated proofs.</p>
<p>Reasoning in other NLP tasks. Multi-hop reasoning can also be found in open-domain QA (Yang et al., 2018), fact verification (Jiang et al., 2020), and reading comprehension (Min et al., 2019; Sinha et al., 2019; Jiang et al., 2019). Compared to proof generation, reasoning chains in these tasks are much simpler, often consisting of only 2-3 supporting facts. Also, they are more coarse-grained, involving large chunks of texts such as passages instead of simple, short sentences.</p>
<p>Bostrom et al. (2021) generate conclusions from premises. Their method can potentially be a component in proof generation but does not consider whether the generated conclusions are relevant. In math word
problems, Cobbe et al. (2021) demonstrate the benefits of using a verifier to re-rank the model's predicted solutions. However, these solutions are unconstrained texts, whereas proofs in our task are structured trees/graphs. Further, we use the verifier during proof generation rather than merely to rank the solutions post hoc. Our verifier is also related to natural language inference (Bowman et al., 2015), especially the multipremises setting in Lai et al. (2017). Recently, large language models have shown the ability to solve multi-step reasoning through chain-of-thought prompting (Wei et al., 2022; Kojima et al., 2022) on arithmetic, symbolic and commonsense reasoning tasks.</p>
<p>Symbolic reasoning. Classical AI has invested significant efforts in reasoning in symbolic domains, e.g., automated theorem proving (ATP) (Kovács and Voronkov, 2013; Yang and Deng, 2019; Polu and Sutskever, 2020). Researchers have attempted to apply ATP to natural language through semantic parsing (Mineshima et al., 2015; Saparov and Mitchell, 2022). However, it is challenging (if not impossible) for semantic parsers to cover the full complexity of natural language. Therefore, researchers have developed reasoning approaches bypassing semantic parsing (Angeli et al., 2016; Kalyanpur et al., 2022; Yang and Deng, 2021).</p>
<p>One promising example is neurosymbolic reasoning. It uses neural networks to handle the complexity of natural language but incorporates inductive biases inspired by symbolic reasoning (Weber et al., 2019; Smolensky, 1990; Kathryn and Mazaitis, 2018; Lee et al., 2016). Our method also falls into this broad category. It uses large language models to generate individual reasoning steps but chains the steps together into a coherent, tree-structured proof using symbolic search algorithms.</p>
<h2>3 Generating Natural Language Proofs</h2>
<p>Task definition. Now we define the proof generation task. As in Fig. 1 (Top), the input consists of a hypothesis $h$ and a set of supporting facts $C=$ $\left{\operatorname{sent}<em 2="2">{1}, \operatorname{sent}</em>}, \ldots, \operatorname{sent<em i="i">{n}\right}$. Both $h$ and $\operatorname{sent}</em>$ are natural language sentences. $h$ can be derived from a subset of $C$ through reasoning of one or multiple steps.</p>
<p>The output is a proof tree $T$ specifying how $h$ is derived from $C$. The tree has $h$ as its root and $\operatorname{sent}_{i}$ as leaf nodes. The intermediate nodes are intermediate conclusions generated by the model. Each non-leaf node $u$ corresponds to a reasoning step with $u$ as the conclusion and its children as premises. To successfully perform the task, the model must select relevant sentences from $C$, use them as leaf nodes to compose a valid proof tree leading to $h$, and fill in all the intermediate conclusions.</p>
<p>Singles-shot vs. stepwise generation. A simple and effective method for proof generation, popularized by ProofWriter (Tafjord et al., 2021), is to finetune a pretrained T5 model (Raffel et al., 2020) to map the input ( $h$ and $C$ ) to the output ( $T$ ), either in a single shot or stepwise. To that end, the input/output must be encoded as text sequences, e.g., encoding the input by concatenating $h$ and $C$ as illustrated in Fig. 1. ${ }^{3}$</p>
<p>The output proof tree can be encoded by post-order traversal. As in Fig. 1, nodes are labeled with identifiers: sent<em> for leaf nodes, int</em> for intermediate nodes, and hypothesis for the root. The output sequence is produced by traversing the tree in post-order, generating one proof step at each non-leaf node, using \&amp; for "and" and $\rightarrow$ for "entails". The tree may correspond to multiple valid sequences due to different ordering between proof steps and between premises within a step. Nevertheless, the evaluation metric can be calculated from the reconstructed trees instead of the raw text sequences.</p>
<p>In single-shot generation, the model generates the output sequence of the entire proof (Fig. 1 Middle), whereas in stepwise generation, each time the model takes the current partial proof as input (besides $h$ and $C$ ) and generates only the next step (Fig. 1 Bottom).</p>
<h2>4 Our Method: NLProofS</h2>
<p>Now we present NLProofS, our method for generating natural language proofs. It has three main components: (1) a stepwise prover for generating candidate proof steps; (2) a verifier for scoring the validity of proofs; (3) an algorithm for searching for proofs that have high aggregated proof scores.</p>
<h3>4.1 Stepwise Prover</h3>
<p>Like prior work (Tafjord et al., 2021), we implement the stepwise prover by finetuning a pretrained T5 model. The training data is extracted from the steps in ground truth proofs. Let $T$ be a proof tree and $u \in T$ be a nonleaf node corresponding to a step we want to extract. Take node int1 in Fig. 1 as an example of $u$. Non-leaf nodes in $T$ can be categorized into (1) $u$ 's descendants, e.g., none in Fig. 1; (2) $u$ itself and its ancestors, e.g., int1 and $h$ in Fig. 1; (3) neither, e.g., int2 in Fig. 1. The partial proof must include all of (1) but none of (2). It may or may not include nodes in (3). Therefore, for this particular example, the partial proof cannot include</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>int1 or h but has a free choice about whether to include int2. When preprocessing the training data, we make these choices randomly as a form of data augmentation.</p>
<p>During inference, the prover may generate syntactically ill-formed proof steps. For the example in Fig. 1 (Bottom), "int1 \&amp; int2 -&gt; hypothesis;" is ill-formed, since the premise int2 is not available in the current partial proof. We mitigate the issue by generating multiple proof steps from the model via beam search and using heuristics to filter out ill-formed ones, e.g., those with syntactical errors or unavailable premises.</p>
<h3>4.2 Verifier</h3>
<p>Scoring a proof step. We introduce an independent verifier, which is trained to check the validity of proof steps and prevent the prover from hallucinating invalid steps based on the hypothesis. A proof step has multiple premises and one conclusion. The verifier takes them as input and produces a continuous validity score in $[0,1]$.</p>
<p>We implement the verifier by finetuning a pretrained RoBERTa model (Liu et al., 2019) to classify proof steps as valid or invalid. The input premises are shuffled randomly and concatenated with the conclusion. For training data, positive examples (valid steps) can be extracted from ground-truth proofs; however, there are no negative examples readily available. Instead of annotating additional negative examples as in Dalvi et al. (2022), we generate pseudo-negative examples automatically. Please refer to Appendix C for details.</p>
<p>Aggregating scores for the entire proof. Step scores are aggregated to produce the score of the entire proof tree. We associate scores with all nodes in the tree recursively. All leaves have a score of 1.0 , as they are explicitly provided assumptions that always hold. Each non-leaf node $u$ corresponds to a proof step $s$ from its children $v_{1}, v_{2}, \ldots, v_{l}$ and has a score defined as</p>
<p>$$
\operatorname{scr}<em s="s">{n}(u)=\min \left(\operatorname{scr}</em>}(s), \operatorname{scr<em 1="1">{n}\left(v</em>}\right), \ldots, \operatorname{scr<em l="l">{n}\left(v</em>\right)\right)
$$</p>
<p>where $\operatorname{scr}<em n="n">{s}(s)$ is the step score, e.g., produced by a verifier. Intuitively, $\operatorname{scr}</em>}(u)$ reflects our confidence in $u$, and it is monotonically non-increasing w.r.t. the step score and the scores of its children. Eqn. 1 is just one simple way of defining $\operatorname{scr<em n="n">{n}(u)$, and we leave a more thorough exploration of scoring options for future work. Finally, the proof score is $\operatorname{scr}</em>(h)$ : the root's score.</p>
<h3>4.3 Proof Search</h3>
<p>Now we combine the prover and the verifier in our proof search algorithm, which looks for proofs with optimal proof scores. Our method is inspired by automated reasoning in formal logic (Russell and Norvig, 2002), where proofs are found by searching in a large space efficiently. Instead of greedy stepwise proof generation, we search for proof trees in a large proof graph (Fig. 2), allowing the model to explore different paths, recover from errors, and ultimately find better proofs.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An iteration in the proof search. Nodes for proof step $(S)$ are omitted for simplicity of illustration. (1) Sample a partial proof (red) from the proof graph. (2) Use the stepwise prover to generate potential steps (blue, only showing one but generating multiple) and score them using the verifier. (3) Execute the steps to update the graph.</p>
<p>Definition 4.1 (Proof graph). A proof graph is a directed acyclic graph with the following properties:</p>
<ul>
<li>Nodes: It has four types of nodes $(C, I, S, h)$, where $C$ corresponds to supporting facts, $I$ corresponds to intermediate conclusions, $S$ consists of proof steps, and $h$ is the hypothesis. Nodes in ${h} \bigcup I \bigcup C$ are associated with unique sentences.</li>
<li>Edges: For any proof step node $s \in S$, it has one or more inbound edges, all of which originate from $I \bigcup C$. It has exactly one outbound edge, which points to a node in $I \bigcup{h}$. Besides these edges, the graph contains no additional edges. Any node $u \in I \bigcup{h}$ has at most one inbound edge.</li>
<li>Scores: All nodes are associated with scores in $[0,1]$. For any sentence $\operatorname{sent}<em n="n">{i} \in C, \operatorname{scr}</em>\left(\right.$ sent $\left.<em n="n">{i}\right)=$ 1. For any node $u \in I \bigcup{h}, \operatorname{scr}</em>(u)$ is defined by Eqn. 1. Scores of proof step nodes in $S$ are provided externally by the algorithm that operates on the proof graph.}(u)=0$ if it has no inbound edge. Otherwise, it must have exactly one inbound edge from $s \in S$, and $s$ has inbound edges from $\left{v_{1}, \ldots, v_{l}\right} \subseteq I \bigcup C . \operatorname{scr}_{n</li>
</ul>
<p>Proof trees correspond to paths in proof graphs (treating $S$ as "and" nodes in and-or graphs). Therefore, our task is to search for a path from $C$ to $h$ that maximizes $\operatorname{scr}_{n}(h)$. The search algorithm is outlined in Fig. 2 and Algorithm 1. Proof search takes place only in inference. In training, we train a stepwise prover $\mathcal{P}$ and a verifier $\mathcal{V}$. In inference, we use them to iteratively expand the proof graph and update the node scores until the graph can no longer be updated. At that point, we extract the best proof of $h$ found so far.</p>
<p>Initialization (line 1-3 in Algorithm 1). We initialize the proof graph using the greedy proof generated by $\mathcal{P}$. We could also start from scratch, i.e., $I=S=\varnothing$ and $\operatorname{scr}(h)=0$, but the initialization accelerates proof search by providing a non-zero initial score for $h$, which can be used to prune unpromising paths during search.</p>
<p>Iteration (line 5-13 in Algorithm 1). We use $\mathcal{P}$ to generate proof steps for updating the graph. $\mathcal{P}$ is trained on partial proof trees rather than graphs. So in each iteration, we first sample a new partial proof tree from the graph as the candidate for expansion (details in Appendix E). Then, we use $\mathcal{P}$ to generate multiple proof</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="n">Proof</span><span class="w"> </span><span class="n">search</span><span class="o">.</span>
<span class="w">    </span><span class="n">Input</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">Hypothesis</span><span class="w"> </span>\<span class="p">(</span><span class="n">h</span>\<span class="p">),</span><span class="w"> </span><span class="n">supporting</span><span class="w"> </span><span class="n">facts</span><span class="w"> </span>\<span class="p">(</span><span class="n">C</span>\<span class="p">),</span>
<span class="w">        </span><span class="n">stepwise</span><span class="w"> </span><span class="n">prover</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">verifier</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">V</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">Output</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">Proof</span><span class="w"> </span><span class="n">tree</span><span class="w"> </span>\<span class="p">(</span><span class="n">T</span>\<span class="p">)</span>
<span class="w">    </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">G</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">generate_greedy</span><span class="w"> </span>\<span class="p">((</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="p">},</span><span class="w"> </span><span class="n">h</span><span class="p">,</span><span class="w"> </span><span class="n">C</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="w"> </span><span class="n">G</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">initialize_graph</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
<span class="w">    </span><span class="n">explored</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">varnothing</span>\<span class="p">)</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="bp">true</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="n">partial_proof</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">sample_new</span><span class="w"> </span>\<span class="p">((</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="w"> </span><span class="n">G</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">explored</span><span class="p">)</span>
<span class="w">        </span><span class="n">explored</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">explored</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">cup</span>\<span class="p">)</span><span class="w"> </span>\<span class="p">{</span><span class="n">partial_proof</span>\<span class="p">}</span>
<span class="w">        </span><span class="n">steps</span><span class="p">,</span><span class="w"> </span><span class="n">p_scrs</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">generate</span><span class="p">(</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">partial_proof</span><span class="p">)</span>
<span class="w">        </span><span class="n">v_scrs</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">verify</span><span class="w"> </span>\<span class="p">((</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">V</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">steps</span><span class="p">)</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">scrs</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mathrm</span><span class="p">{</span><span class="n">p</span><span class="p">}</span><span class="w"> </span>\<span class="n">_</span>\<span class="n">right</span><span class="o">.</span>\<span class="p">)</span><span class="n">scrs</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">v_scrs</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="w"> </span><span class="n">G</span><span class="p">}</span><span class="o">^</span><span class="p">{</span>\<span class="n">prime</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">update</span><span class="p">(</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="w"> </span><span class="n">G</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">steps</span><span class="p">,</span><span class="w"> </span><span class="n">scrs</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="w"> </span><span class="n">G</span><span class="p">}</span><span class="o">^</span><span class="p">{</span>\<span class="n">prime</span><span class="p">}</span><span class="o">=</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="w"> </span><span class="n">G</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">then</span>
<span class="w">            </span><span class="k">break</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="w"> </span><span class="n">G</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="w"> </span><span class="n">G</span><span class="p">}</span><span class="o">^</span><span class="p">{</span>\<span class="n">prime</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">extract_proof</span><span class="w"> </span>\<span class="p">((</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="w"> </span><span class="n">G</span><span class="p">})</span>\<span class="p">)</span>
</code></pre></div>

<p>steps $s_{1}, s_{2}, \ldots, s_{k}$ through beam search followed by filtering as discussed in Sec. 4.1. We calculate step scores $\operatorname{scr}<em 1="1">{s}\left(s</em>}\right), \operatorname{scr<em 2="2">{s}\left(s</em>}\right), \ldots, \operatorname{scr<em k="k">{s}\left(s</em>$, which are the likelihood scores in beam search.}\right)$ by averaging verifier scores v_scrs from $\mathcal{V}$ (Sec. 4.2) and prover scores p_scrs from $\mathcal{P</p>
<p>Then we try to update the proof graph by executing these steps. Assume a step $s_{i}$ has premises $v_{1}, \ldots, v_{l}$ and a conclusion $u$. First, we use Eqn. 1 to calculate a tentative score $\widehat{\operatorname{scr}}<em n="n">{n}(u)$. If $u$ is an existing node in the graph with $\operatorname{scr}</em>}(u) \geq \widehat{\operatorname{scr}<em n="n">{n}(u)$, the step becomes a noop, and we do not perform any update. Otherwise, there are two cases: (1) If $u$ is not in the graph (Fig. 2), we just create a new node for it with $\operatorname{scr}</em>}(u)=\widehat{\operatorname{scr}<em n="n">{n}(u)$; (2) If $u$ is in the graph and $\operatorname{scr}</em>}(u)&lt;\widehat{\operatorname{scr}<em i="i">{n}(u)$, we update $u$ by replacing the existing proof step leading to it with the new step $s</em>}$ with $\operatorname{scr<em n="n">{n}(u)=\widehat{\operatorname{scr}}</em>(u)$. According to Eqn. 1, the score change may affect $u$ 's successors, so we propagate the change to all of them.</p>
<p>Proof extraction (line 14 in Algorithm 1). When all proof steps in an iteration are no-op, we stop and extract the best proof of $h$ found so far, which simply consists of all predecessors of $h$. The result is guaranteed to be a tree, as we prove in Appendix D.</p>
<h2>5 Main Results</h2>
<h3>5.1 Experimental Setup</h3>
<p>We evaluate NLProofS on proof generation using two benchmarks: a real-world benchmark EntailmentBank (Dalvi et al., 2021) and a synthetic benchmark RuleTaker (Clark et al., 2020). Training and inference details are in Appendix F. Bostrom et al. (2022) also evaluated on EntailmentBank but deviated from the original setting, instead formulating the task as distinguishing verifiable hypotheses from unverifiable ones. In order to have a fair comparison with their work, we also evaluate under their setting in Appendix G.</p>
<p>EntailmentBank. EntailmentBank consists of 1,840 proof trees constructed by expert annotators ( 1,313 for training, 187 for validation, and 340 for testing). It comes with three variants of the proof generation task (Sec. 3) with varying numbers of distractors in supporting facts $C$. Task 1 does not have any distractor, i.e., $C$ consists of exactly the leaf nodes of the ground truth proof tree. In Task 2, $C$ always has 25 sentences, including ground truth supporting facts as well as distractors. In Task 3, $C$ is a large corpus of 12 K sentences derived from WorldTree V2 (Xie et al., 2020), requiring the model to retrieve relevant supporting facts from the corpus. We evaluate on all three tasks. Our method is directly applicable to Task 1 and Task 2. For Task 3, Dalvi et al. (2021) retrieve 25 supporting facts for each hypothesis. We use the same retrieved supporting facts and focus solely on proof generation. ${ }^{4}$ And following their practice, we train the model on Task 2 and evaluate its zero-shot performance on Task 3.</p>
<p>A generated proof tree $\hat{T}$ is compared against the ground truth $T$ using official metrics developed by EntailmentBank. In summary, the leaves, proof steps, and intermediate conclusions in $\hat{T}$ are compared against those in $T$ to produce two metrics: the F1 score, and the AllCorrect score which evaluates exact matches. ${ }^{5}$ In addition, the Overall-AllCorrect metric measures whether $\hat{T}$ is identical to $T$. As a caveat, these metrics do not account for the existence of multiple valid proof trees. Metrics for evaluating leaves are less impacted by this issue, as multiple valid trees often have the same set of leaves. Please refer to Appendix A and EntailmentBank for additional details. We report results produced by their official evaluation code. ${ }^{6}$</p>
<p>RuleTaker. To demonstrate the broad applicability of NLProofS to different reasoning datasets, we also evaluate on RuleTaker. In RuleTaker, $h$ can be either proved, disproved, or neither. The model has to do two things: (1) predict the answer as one of those three categories and (2) generate a proof when $h$ can be proved or</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>disproved. Unlike EntailmentBank, examples in RuleTaker are made of simple, synthetic English sentences generated by templates. We use the OWA (open-world assumption) version of the dataset introduced by Tafjord et al. (2021). Following the setup in Sanyal et al. (2022), we train and test on the D0-D3 subset, which consists of proofs of depth $\leq 3$. It has 129 K examples -90 K for training, 13 K for validation, and 26 K for testing.</p>
<p>The predicted answer is evaluated using accuracy, whereas proofs are evaluated using Overall-AllCorrect but ignoring the intermediate conclusions. ${ }^{7}$</p>
<h3>5.2 Proof Generation on EntailmentBank</h3>
<p>Table 2 shows test results on EntailmentBank. We compare with EntailmentWriter (Dalvi et al., 2021) and MetGen (Hong et al., 2022): two prior state-of-the-art methods that also finetune a T5 model to generate proofs. EntailmentWriter generates the entire proof in a single shot, whereas MetGen generates the proof stepwise. EntailmentWriter has two versions, one with T5-large ( 737 million parameters) and the other with T5-11B (11 billion parameters). All other methods, including ours, use only T5-large due to computational constraints.</p>
<p>NLProofS significantly outperforms EntailmentWriter across the board. Take Task 2 as an example. First, it generates more correct proofs overall, improving the Overall-AllCorrect metric from $20.9 \%$ to $33.3 \%$. Second, it identifies relevant supporting facts more effectively, improving the Leaves-AllCorrect from 35.6\% to $58.8 \%$. Third, it generates more accurate proof steps and intermediate conclusions, as demonstrated by the Steps and Intermediates metrics. Moreover, our method with T5-large even outperforms EntailmentWriter with T5-11B by a large margin.</p>
<p>Compared to MetGen, we perform competitively on Task 1 and Task 3 but much better on Task 2, improving the Overall-AllCorrect metric from 27.7\% to 33.3\%. Note that our model is trained only on EntailmentBank, whereas MetGen requires much more data annotation efforts (Sec. 4.1.2 in Hong et al. (2022)). First, the MetGen authors manually design templates of different reasoning types and use them to collect additional training data from Wikipedia. Second, they manually annotate the reasoning types of 400 training proof steps in EntailmentBank. MetGen needs these annotations since the model takes the reasoning type as input.</p>
<p>We also examine whether the proof can be generated in a single shot by very large language models such as GPT-3 (Brown et al., 2020) or Codex (Chen et al., 2021), through prompting with in-context examples. Results in Appendix H show that in-context prompting performs substantially worse than our method.</p>
<p>Table 3 shows two examples of invalid steps generated by EntailmentWriter but avoided by NLProofS , likely due to its verifier. In the first example, "June" in EntailmentWriter's conclusion is hallucinated based on the hypothesis, as the word does not appear in the</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Leaves</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Steps</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Intermediates</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;">AllCorrect</td>
</tr>
<tr>
<td style="text-align: center;">Task 1</td>
<td style="text-align: center;">EntailmentWriter</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">33.5</td>
</tr>
<tr>
<td style="text-align: center;">(no-distractor)</td>
<td style="text-align: center;">EntailmentWriter (T5-11B)</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">35.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MetGen ${ }^{\dagger}$</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">36.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NLProofS (ours)</td>
<td style="text-align: center;">$97.8 \pm 0.2$</td>
<td style="text-align: center;">$90.1 \pm 1.2$</td>
<td style="text-align: center;">$55.6 \pm 0.6$</td>
<td style="text-align: center;">$42.3 \pm 0.4$</td>
<td style="text-align: center;">$72.4 \pm 0.5$</td>
<td style="text-align: center;">$40.6 \pm 0.7$</td>
<td style="text-align: center;">$38.9 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;">Task 2</td>
<td style="text-align: center;">EntailmentWriter</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">20.9</td>
</tr>
<tr>
<td style="text-align: center;">(distractor)</td>
<td style="text-align: center;">EntailmentWriter (T5-11B)</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">25.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MetGen ${ }^{\dagger}$</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">27.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NLProofS (ours)</td>
<td style="text-align: center;">$\mathbf{9 0 . 3} \pm \mathbf{0 . 4}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 8} \pm \mathbf{1 . 8}$</td>
<td style="text-align: center;">$\mathbf{4 7 . 2} \pm \mathbf{1 . 7}$</td>
<td style="text-align: center;">$\mathbf{3 4 . 4} \pm \mathbf{1 . 7}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 2} \pm \mathbf{0 . 5}$</td>
<td style="text-align: center;">$\mathbf{3 7 . 8} \pm \mathbf{1 . 6}$</td>
<td style="text-align: center;">$\mathbf{3 3 . 3} \pm \mathbf{1 . 5}$</td>
</tr>
<tr>
<td style="text-align: center;">Task 3</td>
<td style="text-align: center;">EntailmentWriter</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">2.4</td>
</tr>
<tr>
<td style="text-align: center;">(full-corpus)</td>
<td style="text-align: center;">EntailmentWriter (T5-11B)</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">2.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MetGen ${ }^{\dagger}$</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">8.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NLProofS (ours)</td>
<td style="text-align: center;">$\mathbf{4 3 . 2} \pm \mathbf{0 . 6}$</td>
<td style="text-align: center;">$8.2 \pm 0.7$</td>
<td style="text-align: center;">$\mathbf{1 1 . 2} \pm \mathbf{0 . 6}$</td>
<td style="text-align: center;">$6.9 \pm 0.7$</td>
<td style="text-align: center;">$\mathbf{4 2 . 9} \pm \mathbf{1 . 0}$</td>
<td style="text-align: center;">$17.3 \pm 0.5$</td>
<td style="text-align: center;">$6.9 \pm 0.7$</td>
</tr>
</tbody>
</table>
<p>Table 2: Test results of proof generation on EntailmentBank (Dalvi et al., 2021). ${ }^{\dagger}$ : MetGen (Hong et al., 2022) is trained on additional data collected from Wikipedia, whereas other methods are trained only on EntailmentBank. Here we report the results of the MetGen-prefixed model, as the other MetGen-separated model performs slightly better but is 5 times larger. All methods are based on T5-large (Raffel et al., 2020) unless otherwise noted. For our method, we report the average performance and the standard deviation for 5 independent runs. Bold and underlined texts highlight the best method and the runner-up.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hypothesis</th>
<th style="text-align: left;">Premises</th>
<th style="text-align: left;">Conclusions generated by models</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">The next new moon will <br> occur on June 30.</td>
<td style="text-align: left;">1. A new moon is a kind of <br> phase of the moon. 2. A moon <br> phase occurs 28 days after the <br> last time it occurs.</td>
<td style="text-align: left;">EntailmentWriter: The next new moon <br> will occur 28 days after June 2.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">NLProofS (ours): The next new moon will <br> occur 28 days after the last new moon.</td>
</tr>
<tr>
<td style="text-align: left;">Planting trees prevents <br> soil from washing away.</td>
<td style="text-align: left;">1. Planting trees increases the <br> amount of trees in an <br> environment. 2. Tree roots <br> decrease / reduce soil erosion.</td>
<td style="text-align: left;">EntailmentWriter: Plants trees increases <br> the amount of trees in an environment.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">NLProofS (ours): Planting trees decreases <br> soil erosion.</td>
</tr>
</tbody>
</table>
<p>Table 3: Examples of invalid proof steps generated by EntailmentWriter (Dalvi et al., 2021) but not our method. In the first example, "June" in the conclusion is hallucinated rather than derived from the premises. In the second example, EntailmentWriter simply copies one of the premises without performing any meaningful reasoning.
premises. The second example is a typical undesirable behavior also observed by Bostrom et al. (2022). When the model has difficulties in generating a conclusion, it falls back into copying one of the premises. Our method generates reasonable conclusions in these two examples.</p>
<h3>5.3 Generating Answers and Proofs on RuleTaker</h3>
<p>Hypotheses in RuleTaker can be provable, disprovable, or neither. To benchmark on RuleTaker, we use a similar scheme to Bostrom et al. (2022) to adapt any proof generation system capable of producing proof scores. In training, we (1) discard hypotheses that are neither provable nor disprovable and (2) convert disprovable hypotheses into provable ones by negating them. We negate sentences by adding an "I don't think" prefix.</p>
<p>In testing, given a hypothesis $h$, we try to generate proofs and the associated scores for both $h$ and its negation $\neg h$. Then we train a linear classifier on top of the two scores to predict the answer. Depending on the predicted answer, we take the generated proof to be the proof of $h, \neg h$, or neither.</p>
<p>Results are in Table 4. ProofWriter is the iterative model in Tafjord et al. (2021). It generates proofs step-
wise based on T5. Our method performs competitively with ProofWriter and FaiRR (Sanyal et al., 2022).</p>
<h2>6 Analyses</h2>
<h3>6.1 Ablation Studies</h3>
<p>EntailmentBank. Our full model searches for stepwise proofs, relying on both the verifier and the prover for producing scores. We conduct ablation studies on Task 2 of EntailmentBank to better understand the empirical gains coming from each of these components.</p>
<p>First, we compare the full model with the stepwise prover without search (Sec. 4.1). Results in Table 5 show that the full model significantly improves upon this stepwise baseline across the board, demonstrating the benefits of searching for proofs at inference time.</p>
<p>Note that the stepwise baseline without search also performs significantly better than EntailmentWriter ( $31.8 \%$ vs. $20.9 \%$ in Overall-AllCorrect). We ask how much of the improvement is due to stepwise proof generation as opposed to implementation details and hyperparameters. ${ }^{8}$. In Table 5, we replicate EntailmentWriter</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Answer accuracy</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Proof accuracy</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">All</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">3</td>
</tr>
<tr>
<td style="text-align: left;">FaiRR ${ }^{\dagger}$</td>
<td style="text-align: left;">$\underline{99.6}$</td>
<td style="text-align: left;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: left;">99.7</td>
<td style="text-align: left;">98.9</td>
<td style="text-align: left;">$\underline{96.6}$</td>
<td style="text-align: left;">99.2</td>
<td style="text-align: left;">$\underline{99.6}$</td>
<td style="text-align: left;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: left;">99.5</td>
<td style="text-align: left;">97.2</td>
<td style="text-align: left;">$\underline{95.3}$</td>
</tr>
<tr>
<td style="text-align: left;">ProofWriter ${ }^{\dagger}$</td>
<td style="text-align: left;">$\mathbf{9 9 . 7}$</td>
<td style="text-align: left;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: left;">$\underline{99.9}$</td>
<td style="text-align: left;">$\mathbf{9 9 . 7}$</td>
<td style="text-align: left;">$\mathbf{9 9 . 7}$</td>
<td style="text-align: left;">$\mathbf{9 9 . 8}$</td>
<td style="text-align: left;">$\mathbf{9 9 . 7}$</td>
<td style="text-align: left;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: left;">$\underline{99.9}$</td>
<td style="text-align: left;">$\mathbf{9 9 . 4}$</td>
<td style="text-align: left;">$\mathbf{9 9 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">NLProofS (ours)</td>
<td style="text-align: left;">99.5</td>
<td style="text-align: left;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: left;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: left;">$\underline{99.4}$</td>
<td style="text-align: left;">96.4</td>
<td style="text-align: left;">$\underline{99.3}$</td>
<td style="text-align: left;">99.5</td>
<td style="text-align: left;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: left;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: left;">$\mathbf{9 9 . 4}$</td>
<td style="text-align: left;">95.1</td>
</tr>
</tbody>
</table>
<p>Table 4: Test results on RuleTaker (OWA) (Tafjord et al., 2021). Models are trained and tested on the D0-D3 subset. Methods with $\dagger$ are reported by Sanyal et al. (2022). All methods are based on T5-large. The 'All' columns are accuracies on the entire testing set. ' 0 ', ' 1 ', ' 2 ', and ' 3 ' are accuracies broken down by the length of testing proofs. 'N/A' includes testing examples without ground truth proofs since they can be neither proved nor disproved.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Leaves</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Steps</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Intermediates</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Overall</th>
<th style="text-align: left;">Time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">AllCorrect</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">AllCorrect</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">AllCorrect</td>
<td style="text-align: left;">AllCorrect</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">NLProofS (full model)</td>
<td style="text-align: left;">$\mathbf{9 0 . 3} \pm \mathbf{0 . 4}$</td>
<td style="text-align: left;">$\mathbf{5 8 . 8} \pm \mathbf{1 . 8}$</td>
<td style="text-align: left;">$\mathbf{4 7 . 2} \pm \mathbf{1 . 7}$</td>
<td style="text-align: left;">$\mathbf{3 4 . 4} \pm \mathbf{1 . 7}$</td>
<td style="text-align: left;">$\mathbf{7 0 . 2} \pm \mathbf{0 . 5}$</td>
<td style="text-align: left;">$\mathbf{3 7 . 8} \pm \mathbf{1 . 6}$</td>
<td style="text-align: left;">$\mathbf{3 3 . 3} \pm \mathbf{1 . 5}$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">4.4</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">w/o search</td>
<td style="text-align: left;">$89.7 \pm 0.6$</td>
<td style="text-align: left;">$56.5 \pm 1.7$</td>
<td style="text-align: left;">$45.9 \pm 1.3$</td>
<td style="text-align: left;">$33.7 \pm 1.4$</td>
<td style="text-align: left;">$67.4 \pm 2.3$</td>
<td style="text-align: left;">$36.4 \pm 1.5$</td>
<td style="text-align: left;">$31.8 \pm 1.4$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">2.2</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">w/o search w/o stepwise</td>
<td style="text-align: left;">$86.9 \pm 0.6$</td>
<td style="text-align: left;">$45.6 \pm 1.5$</td>
<td style="text-align: left;">$42.6 \pm 1.6$</td>
<td style="text-align: left;">$29.7 \pm 1.3$</td>
<td style="text-align: left;">$64.6 \pm 1.4$</td>
<td style="text-align: left;">$32.2 \pm 2.1$</td>
<td style="text-align: left;">$27.1 \pm 1.5$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathbf{1 . 9}$</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">w/o prover score</td>
<td style="text-align: left;">$\mathbf{9 0 . 3} \pm \mathbf{0 . 7}$</td>
<td style="text-align: left;">$57.8 \pm 1.9$</td>
<td style="text-align: left;">$43.9 \pm 0.9$</td>
<td style="text-align: left;">$30.4 \pm 0.5$</td>
<td style="text-align: left;">$68.9 \pm 0.5$</td>
<td style="text-align: left;">$35.3 \pm 1.2$</td>
<td style="text-align: left;">$29.7 \pm 0.8$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">4.6</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">w/o verifier score</td>
<td style="text-align: left;">$89.7 \pm 0.6$</td>
<td style="text-align: left;">$55.8 \pm 2.2$</td>
<td style="text-align: left;">$45.8 \pm 1.4$</td>
<td style="text-align: left;">$33.8 \pm 1.5$</td>
<td style="text-align: left;">$68.5 \pm 0.4$</td>
<td style="text-align: left;">$36.1 \pm 1.4$</td>
<td style="text-align: left;">$31.9 \pm 1.3$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">3.3</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 5: Ablation results on Task 2 of the test set of EntailmentBank. The last column shows the average inference time (in seconds) per test example when running with a batch size of 1 and a beam width of 10 .
(Ours w/o search w/o stepwise) in a unified implementation with our other methods. It outperforms EntailmentWriter ( $27.1 \%$ vs. $20.9 \%$ in Overall-AllCorrect) but still falls behind the stepwise baseline, demonstrating the effectiveness of stepwise proof generation.</p>
<p>Instead of averaging the scores from the prover and the verifier, what if we use only one of them? Can we still produce accurate and well-calibrated scores that are useful in proof search? In Table 5, we experiment with two additional versions of NLProofS: one without the prover score and the other without the verifier score. Results show that they fail to improve upon the stepwise baseline without search, demonstrating the necessity of combining the verifier and the prover.</p>
<p>Table 5 also includes different methods' average inference time per test example, measured with batch size 1 and beam width 10. NLProofS takes 4.4 seconds to process a test example, which is $2 x$ slower than the stepwise baseline. Longer run time is a natural consequence of proof search, and $2 x$ is a modest slow down.</p>
<p>RuleTaker. We perform similar ablation experiments also on RuleTaker. Results in Table 6 show similar patterns as the ablations on EntailmentBank. However, the main difference is that proof search leads to a much larger improvement on RuleTaker, and the two baselines without search perform much lower than prior methods (ProofWriter in Table 4). This is due to how we adapt proof generation systems to the task of RuleTaker.</p>
<p>As described in Sec. 5.3, the answer is produced by a linear classifier over proof scores of the hypothesis $h$ and its negation $\neg h$. To perform well, we need the proof generation system to (1) assign high scores to valid hypotheses and (2) assign low scores to invalid hypotheses. However, proof generation systems without</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>verifiers-such as the two baselines-have never seen invalid proof steps in training. They are good at (1) but not necessarily (2); this is sufficient for EntailmentBank (with only valid hypotheses) but not RuleTaker. In contrast, proof generation systems with verifierssuch as our full model-are good at both (1) and (2). In other words, NLProofS can generate more accurate proof scores for both valid and invalid hypotheses.</p>
<p>In addition, Table 6 shows that the verifier score alone is sufficient for RuleTaker; adding the prover score does not make much difference. This is because verifiers trained on RuleTaker are highly accurate, and they do not need to be supplemented by the prover.</p>
<h3>6.2 NLProofS with Oracles</h3>
<p>The stepwise prover and the verifier are two major components in NLProofS. To analyze which one is the bottleneck, we construct "oracle" versions of them, both of which have access to ground-truth information for better predictions. Given a partial proof, the oracle prover generates multiple potential proof steps just like a regular prover. But it additionally includes all ground truth steps that are valid for the partial proof, i.e., steps whose premises have been satisfied by the partial proof. The oracle verifier also builds on a regular verifier but always assigns the highest score (1.0) to proof steps in the ground truth. Note that we call them "oracles", but neither of them is perfect. For example, the oracle verifier cannot reliably tell whether a proof step is valid if the step deviates even slightly from the ground truth.</p>
<p>Table 7 shows the validation results on Task 2 of EntailmentBank. The oracle prover alone improves the performance significantly (e.g., a boost of 29.2 in OverallAllCorrect), demonstrating that the prover is a major bottleneck. In contrast, the oracle verifier alone does not help much, improving only 0.8 in Overall-AllCorrect.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Answer accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Proof accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">All</td>
</tr>
<tr>
<td style="text-align: center;">NLProofS (full model)</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">99.2</td>
</tr>
<tr>
<td style="text-align: center;">w/o search</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">84.2</td>
</tr>
<tr>
<td style="text-align: center;">w/o search w/o stepwise</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">72.7</td>
</tr>
<tr>
<td style="text-align: center;">w/o prover score</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">99.0</td>
</tr>
<tr>
<td style="text-align: center;">w/o verifier score</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">91.4</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablation results on the D0-D3 test set of RuleTaker (OWA) (Tafjord et al., 2021).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Leaves</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Steps</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Intermediates</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AllCorrect</td>
</tr>
<tr>
<td style="text-align: left;">NLProofS (no oracle)</td>
<td style="text-align: center;">$89.4 \pm 0.8$</td>
<td style="text-align: center;">$56.0 \pm 0.7$</td>
<td style="text-align: center;">$50.4 \pm 1.9$</td>
<td style="text-align: center;">$38.4 \pm 1.3$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$71.9 \pm 1.4$</td>
<td style="text-align: center;">$41.3 \pm 1.4$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$37.1 \pm 1.5$</td>
</tr>
<tr>
<td style="text-align: left;">oracle verifier</td>
<td style="text-align: center;">$90.0 \pm 0.5$</td>
<td style="text-align: center;">$56.9 \pm 1.6$</td>
<td style="text-align: center;">$51.3 \pm 2.2$</td>
<td style="text-align: center;">$39.1 \pm 2.1$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$72.9 \pm 1.5$</td>
<td style="text-align: center;">$42.0 \pm 2.1$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$37.9 \pm 2.2$</td>
</tr>
<tr>
<td style="text-align: left;">oracle prover</td>
<td style="text-align: center;">$94.6 \pm 0.3$</td>
<td style="text-align: center;">$76.2 \pm 1.9$</td>
<td style="text-align: center;">$75.8 \pm 0.9$</td>
<td style="text-align: center;">$67.0 \pm 1.8$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$85.3 \pm 0.5$</td>
<td style="text-align: center;">$67.5 \pm 1.6$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$66.3 \pm 1.9$</td>
</tr>
<tr>
<td style="text-align: left;">oracle prover + verifier</td>
<td style="text-align: center;">$\mathbf{9 5 . 7} \pm \mathbf{0 . 7}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 7} \pm \mathbf{2 . 7}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 0} \pm \mathbf{3 . 1}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 4} \pm \mathbf{3 . 9}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{8 8 . 1} \pm \mathbf{1 . 8}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 7} \pm \mathbf{4 . 1}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{7 5 . 2} \pm \mathbf{3 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Validation results on EntailmentBank (Task 2) of replacing the prover/verifier with oracles.</p>
<p>However, 0.8 might underestimate the importance of the verifier, as the oracle verifier is not useful if the prover fails to generate the ground truth proof step in the first place. Actually, adding the oracle verifier to the oracle prover improves Overall-AllCorrect by 8.9, demonstrating that the verifier also bears room for improvement.</p>
<p>Leaves-AllCorrect
w/o search w/o stepwise w/o search NLProofS
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Test results on Task 2 (distractor) broken down by the length of the ground truth proof. Here we show the Leaves-AllCorrect metric.</p>
<h3>6.3 Impact of Proof Length</h3>
<p>Prior work has demonstrated that proof generation models struggle with long proofs (Dalvi et al., 2021). In Fig. 3, we break down the test performance on Task 2 of EntailmentBank by proof length, i.e., the number of steps in the ground truth proof. Here we show only the Leaves-AllCorrect metric. Leaves metrics are relatively robust against the issue of multiple valid proof trees per example. Appendix H include figures of other metrics. But they may exaggerate the difficulty with long proofs, as the issue of multiple valid proofs is particularly prominent for long proofs. Nevertheless, we still see a significant performance drop in Fig. 3 when the proof length exceeds $1-2$, suggesting that generating
long proofs remains a challenge. However, we also see the benefits of proof search since its improvements over the stepwise baseline are more evident for long proofs.</p>
<p>In addition, NLProofS tends to generate longer proofs compared to the baselines. On the validation set of Task 2 , the ground truth proofs have an average length of 3.2 steps, whereas the average lengths of the generated proofs are $2.6,2.7$, and 2.9 for the single-shot baseline, the stepwise baseline, and NLProofS.</p>
<h3>6.4 Reduced Hallucination</h3>
<p>The verifier in NLProofS aims to prevent the model from hallucinating invalid proof steps. However, it is difficult to evaluate hallucination automatically: when the model generation deviates from ground truth, it is difficult to evaluate whether it is a valid proof step. Therefore, besides qualitative examples in Table 3, we also perform a human evaluation similar to Bostrom et al. (2022).</p>
<p>We compare three models: EntailmentWriter, NLProofS w/o search (our model without the verifierguided search), and NLProofS (our full model). For each model, we sample 100 generated proof steps and manually annotate them as valid/invalid. The percentage of valid steps is $43 \%, 65 \%$, and $77 \%$, demonstrating the effectiveness of NLProofS in mitigating hallucination.</p>
<h2>7 Conclusion</h2>
<p>We have introduced NLProofS for stepwise proof generation in natural language. It learns to generate relevant proof steps conditioning on the hypothesis. To prevent hallucination, NLProofS searches for proofs that maximize a validity score judged by a verifier. Our method has achieved state-of-the-art performance on EntailmentBank and RuleTaker, demonstrating the promise of stepwise proof generation for human-authored proofs. In the future, we hope to see increasing applications of verifiers and proof search in various reasoning tasks.</p>
<h2>Limitations</h2>
<p>Despite the strong performance on two benchmarks, our method still has substantial room for future improvement. Currently, the prover (Sec. 4.1) uses beam search as the decoding algorithm, which has two problems: First, it generates equivalent proof steps such as "sent1 \&amp; sent2 -&gt; hypothesis" and "sent2 \&amp; sent1 -&gt; hypothesis". It would be more efficient if we make the decoding invariant to the permutation of premises. Second, the generated proof steps lack diversity. Since the verifier can filter out invalid proof steps, it is more important for the prover to have coverage and diversity than precision. It would be interesting to try more advanced decoding algorithms such as Diverse Beam Search (Vijayakumar et al., 2018). Like prior work (Tafjord et al., 2021; Dalvi et al., 2021), our prover concatenates all supporting facts into a long text sequence and applies a Transformer encoder to it. This could be an inefficient use of computation and may have problems scaling to longer sentences or a larger number of supporting facts. Solutions like Fusion-in-Decoder (Izacard and Grave, 2021) may help solve this problem.</p>
<h2>Ethical Considerations</h2>
<p>Machine learning and NLP are moving from lab curiosity into real-world systems that make critical decisions in areas such as hiring, loan approval, and college admission. It is imperative that these decisions are interpretable to humans. Proof generation enhances interpretability by requiring the model to produce not only the final decision but also an explicit proof. However, the interpretability is jeopardized if the model learns to hallucinate invalid proof steps, like a person trying to find unfaithful excuses to justify a decision. Our method uses an independently trained verifier to check the validity of proof steps, which effectively reduces hallucination and enables the generated proof to explain the decision more faithfully.</p>
<h2>Acknowledgements</h2>
<p>This work is partially supported by the Office of Naval Research under Grant N00014-20-1-2634. We gratefully acknowledge financial support from the Schmidt DataX Fund at Princeton University made possible through a major gift from the Schmidt Futures Foundation. We also thank Darby Haller, Jane Pan, Shunyu Yao, and the members of the Princeton NLP group for helpful discussion and valuable feedback.</p>
<h2>References</h2>
<p>Gabor Angeli, Neha Nayak, and Christopher D Manning. 2016. Combining natural logic and shallow reasoning for question answering. In Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, and Greg Durrett. 2022. Natural language deduction through search over statement compositions. arXiv preprint arXiv:2201.06028.</p>
<p>Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. 2021. Flexible generation of natural language deductions. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. In International Joint Conference on Artificial Intelligence (IJCAI).</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. 2021. Explaining answers with entailment trees. arXiv preprint arXiv:2104.08661.</p>
<p>Bhavana Dalvi, Oyvind Tafjord, and Peter Clark. 2022. Towards teachable reasoning systems. arXiv preprint arXiv:2204.13074.</p>
<p>Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Chris Pal. 2020. Measuring systematic generalization in neural proof generation with transformers. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Ruixin Hong, Hongming Zhang, Xintong Yu, and Changshui Zhang. 2022. MetGen: A module-based entailment tree generation framework for answer explanation. In Findings of the North American Chapter of the Association for Computational Linguistics: NAACL.</p>
<p>Gautier Izacard and Édouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In European Chapter of the Association for Computational Linguistics (EACL), pages $874-880$.</p>
<p>Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020. HoVer: A dataset for many-hop fact extraction and claim verification. In Findings of the Association for Computational Linguistics: EMNLP.</p>
<p>Yichen Jiang, Nitish Joshi, Yen-Chun Chen, and Mohit Bansal. 2019. Explore, propose, and assemble: An interpretable model for multi-hop reading comprehension. In Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Aditya Kalyanpur, Tom Breloff, David Ferrucci, Adam Lally, and John Jantos. 2022. Braid: Weaving symbolic and neural knowledge into coherent logical explanations. In AAAI Conference on Artificial Intelligence.</p>
<p>William W Cohen Fan Yang Kathryn and Rivard Mazaitis. 2018. TensorLog: Deep learning meets probabilistic databases. Journal of Artificial Intelligence Research, 1:1-15.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Laura Kovács and Andrei Voronkov. 2013. First-order theorem proving and Vampire. In International Conference on Computer Aided Verification (CAV).</p>
<p>Alice Lai, Yonatan Bisk, and Julia Hockenmaier. 2017. Natural language inference from multiple premises. In International Joint Conference on Natural Language Processing (IJCNLP).</p>
<p>Moontae Lee, Xiaodong He, Wen-tau Yih, Jianfeng Gao, Li Deng, and Paul Smolensky. 2016. Reasoning in vector space: An exploratory study of question answering. In International Conference on Learning Representations (ICLR).</p>
<p>Zhengzhong Liang, Steven Bethard, and Mihai Surdeanu. 2021. Explainable multi-hop verbal reasoning through internal monologue. In Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR).</p>
<p>Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. 2022. Language models of code are few-shot commonsense learners. arXiv preprint arXiv:2210.07128.</p>
<p>John McCarthy et al. 1960. Programs with common sense. RLE and MIT Computation Center.</p>
<p>Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019. Multi-hop reading comprehension through question decomposition and rescoring. In Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Koji Mineshima, Pascual Martínez-Gómez, Yusuke Miyao, and Daisuke Bekki. 2015. Higher-order logical inference with compositional semantics. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Stanislas Polu and Ilya Sutskever. 2020. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393.</p>
<p>Hanhao Qu, Yu Cao, Jun Gao, Liang Ding, and Ruifeng Xu. 2022. Interpretable proof generation via iterative backward reasoning. In Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis \&amp; insights from training Gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR), 21:1-67.</p>
<p>Danilo Ribeiro, Shen Wang, Xiaofei Ma, Rui Dong, Xiaokai Wei, Henry Zhu, Xinchi Chen, Zhiheng Huang, Peng Xu, Andrew Arnold, et al. 2022. Entailment tree explanations via iterative retrieval-generation reasoner. In Findings of the North American Chapter of the Association for Computational Linguistics: NAACL.</p>
<p>Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends ${ }^{\circledR}$ in Information Retrieval, 3(4):333-389.</p>
<p>Alan JA Robinson and Andrei Voronkov. 2001. Handbook of automated reasoning, volume 1. Elsevier.</p>
<p>Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, and Brenden M Lake. 2020. A benchmark for systematic generalization in grounded language understanding. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Stuart Russell and Peter Norvig. 2002. Artificial intelligence: a modern approach.</p>
<p>Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, and Mohit Bansal. 2020. PRover: Proof generation for interpretable reasoning over rules. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Soumya Sanyal, Harman Singh, and Xiang Ren. 2022. FaiRR: Faithful and robust deductive reasoning over natural language. In Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Abulhair Saparov and Tom M Mitchell. 2022. Towards general natural language understanding with probabilistic worldbuilding. Transactions of the Association for Computational Linguistics (TACL), 10:325342 .</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L Hamilton. 2019. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Paul Smolensky. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence, 46:159216.</p>
<p>Changzhi Sun, Xinbo Zhang, Jiangjie Chen, Chun Gan, Yuanbin Wu, Jiaze Chen, Hao Zhou, and Lei Li. 2021. Probabilistic graph reasoning for natural proof generation. In Findings of the Association for Computational Linguistics: ACL.</p>
<p>Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL.</p>
<p>Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. 2018. Diverse Beam Search: Decoding diverse solutions from neural sequence models. In AAAI Conference on Artificial Intelligence.</p>
<p>Leon Weber, Pasquale Minervini, Jannes Münchmeyer, Ulf Leser, and Tim Rocktäschel. 2019. NLProlog: Reasoning with weak unification for question answering in natural language. In Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Zhengnan Xie, Sebastian Thiem, Jaycie Martin, Elizabeth Wainwright, Steven Marmorstein, and Peter Jansen. 2020. WorldTree V2: A corpus of sciencedomain structured explanations and inference patterns supporting multi-hop inference. In International Conference on Language Resources and Evaluation (LREC).</p>
<p>Kaiyu Yang and Jia Deng. 2019. Learning to prove theorems via interacting with proof assistants. In International Conference on Machine Learning (ICML).</p>
<p>Kaiyu Yang and Jia Deng. 2021. Learning symbolic rules for reasoning in quasi-natural language. arXiv preprint arXiv:2111.12038.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<h2>A Evaluation Metrics on EntailmentBank</h2>
<p>We evaluate on EntailmentBank using their official evaluation metrics calculated by their evaluation code. Below is a summary; please refer to the EntailmentBank paper for further details.</p>
<p>Let $\hat{T}$ be a generated proof tree, with $T$ being the ground truth. First, nodes in $\hat{T}$ are aligned with nodes in $T$ using a tree alignment algorithm based on the "sent*" labels. Once aligned, it is scored using 4 types of metrics-Leaves, Steps, Intermediates, and Overall.</p>
<ul>
<li>Leaves (F1, AllCorrect): The Leaves metrics compare the leaf nodes of $\hat{T}$ and $T$ to calculate an F1 score and an "AllCorrect" score, which means all predicted nodes are correct. In other words, AllCorrect $=1$ if $\mathrm{F} 1=1$, and AllCorrect $=0$ if $\mathrm{F} 1&lt;1$.</li>
<li>Steps (F1, AllCorrect): The Steps metrics measure whether predicted proof steps are structurally correct. A predicted step corresponds to an internal node $u \in$ $\hat{T}$ (aligned to $v \in T$ ). It is structurally correct if the children of $u$ and $v$ are also perfectly aligned. Since there are multiple steps in $\hat{T}$ and $T$, we can calculate F1 and AllCorrect.</li>
<li>Intermediates (F1, AllCorrect): An intermediate conclusion $u \in \hat{T}$ (aligned to $v \in T$ ) is correct if the BLEURT (Sellam et al., 2020) ${ }^{9}$ score between $u$ and $v$ is greater than 0.28 . We calculate F1 and AllCorrect from all intermediate conclusions in $\hat{T}$ and $T$.</li>
<li>Overall (AllCorrect): The Overall metric evaluates whether the leaves, steps, and intermediates are all correct, i.e., AllCorrect $=1$ if and only if $\hat{T}$ matches completely with $T$.</li>
</ul>
<h2>B Different Input Formats</h2>
<p>We use a slightly different input format from EntailmentWriter (Dalvi et al., 2021), as their format had not been released when we developed our method.</p>
<p>Consider the example in Fig. 1. The input to our single-shot baseline (NLProofS w/o search w/o stepwise in Table 5) is "\$hypothesis\$ = solar is a kind of renewable energy for heating homes ; \$context\$ = sent1: homes are buildings sent2: solar is renewable . . ;", whereas the their input is "\$proof\$ ; \$question\$ = As a kind of renewable energy, what can solar be used for? ; \$answer\$ = heating homes ; \$hypothesis\$ = solar is a kind of renewable energy for heating homes ; \$context\$ = sent1: homes are buildings sent2: solar is renewable . . . ;", which includes more information (\$question\$ and \$answer\$) than ours.</p>
<p>We experiment with single-shot methods implemented in our codebase using their input format. Results in Table A indicate no significant difference.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>C Pseudo-negative Examples for Training the Verifier</h2>
<p>As mentioned in Sec. 4.2, the negative examples used for training the verifier are constructed automatically using the procedure below:</p>
<ul>
<li>As in Fig. A, for each positive example consisting of premises and a conclusion, we either remove some premises or replacing one premise with a distractor retrieved from $C$ using BM25 (Robertson et al., 2009).</li>
<li>For EntailmentBank, as in Fig. B, we generate additional pseudo-negatives by copying one of the premises as the conclusion.</li>
<li>For RuleTaker, as in Fig. C, we generate additional pseudo-negatives by negating the conclusion.</li>
</ul>
<h2>D Proof Graphs are Loopless</h2>
<p>We prove that the proof graph in Algorithm 1 is loopless. Intuitively, for a proof graph $(C, I, S, h)$, as we traverse along any path, the node scores in $C \cup I \cup{h}$ are non-increasing due to Eqn. 1, which prevents loops.
Lemma D.1. Let $\mathcal{G}$ be a proof graph with nodes $(C, I, S, h)$. For any $s \in S$ and $v, u \in C \cup I \cup{h}$ s.t. edges $(v, s)$ and $(s, u)$ exist, we have $\operatorname{scr}<em n="n">{n}(u) \leq$ $\operatorname{scr}</em>(v)$.</p>
<p>Proof. In this case, $s$ must be a proof step with $u$ as the conclusion and $v$ as one of its premises. According to Definition 4.1 and Eqn. 1, we have</p>
<p>$$
\operatorname{scr}<em s="s">{n}(u)=\min \left(\operatorname{scr}</em>(v), \ldots\right)
$$}(s), \operatorname{scr}_{n</p>
<p>Therefore, $\operatorname{scr}<em n="n">{n}(u) \leq \operatorname{scr}</em>(v)$.
Lemma D.2. Let $\mathcal{G}$ be a proof graph with nodes $(C, I, S, h)$. For any $v, u \in C \cup I \cup{h}$ s.t. $v$ is a predecessor of $u$, we have $\operatorname{scr}<em n="n">{n}(u) \leq \operatorname{scr}</em>(v)$.
Proof. According to Definition 4.1, there exists a path $v \rightarrow s_{1} \rightarrow w_{1} \rightarrow s_{2} \rightarrow w_{2} \rightarrow, \ldots, \rightarrow s_{k} \rightarrow u$, where $\forall i, s_{i} \in S, w_{i} \in C \cup I \cup{h}$. The lemma can be proved by performing induction on the path and applying Lemma D.1.</p>
<p>Theorem D.3. In Algorithm 1, if the proof graph is loopless after initialization, then it will remain loopless.</p>
<p>Proof. We just need to prove that it is impossible to introduce a loop during any iteration in Algorithm 1. We prove it by contradiction, assuming we could introduce a loop in an iteration, as in Fig. D. We have two nodes $v, u \in C \cup I \cup{h}$, and $v$ is a predecessor of $u$ before introducing the loop. Further assume that the loop is introduced as a result of executing a proof step $s$, which created the edges $(u, s)$ and $(s, v)$ (the blue arrow in Fig. D; $s$ is omitted). In this hypothetical scenario, the loop would be $v \rightarrow \cdots \rightarrow u \rightarrow s \rightarrow v$.</p>
<p>Apply Lemma D. 2 to the path from $v$ to $u$, and we have $\operatorname{scr}<em n="n">{n}(u) \leq \operatorname{scr}</em>(v)$ before introducing the loop.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Leaves</th>
<th></th>
<th>Steps</th>
<th></th>
<th>Intermediates</th>
<th></th>
<th>Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>F1</td>
<td>AllCorrect</td>
<td>F1</td>
<td>AllCorrect</td>
<td>F1</td>
<td>AllCorrect</td>
<td>AllCorrect</td>
</tr>
<tr>
<td>Our format</td>
<td>$86.9 \pm 0.6$</td>
<td>$45.6 \pm 1.5$</td>
<td>$\mathbf{4 2 . 6} \pm \mathbf{1 . 6}$</td>
<td>$\mathbf{2 9 . 7} \pm \mathbf{1 . 3}$</td>
<td>$\mathbf{6 4 . 6} \pm \mathbf{1 . 4}$</td>
<td>$32.2 \pm 2.1$</td>
<td>$27.1 \pm 1.5$</td>
</tr>
<tr>
<td>EntailmentWriter format</td>
<td>$\mathbf{8 7 . 6} \pm \mathbf{0 . 5}$</td>
<td>$\mathbf{4 7 . 1} \pm \mathbf{2 . 1}$</td>
<td>$42.2 \pm 1.1$</td>
<td>$\mathbf{2 9 . 7} \pm \mathbf{1 . 6}$</td>
<td>$64.5 \pm 0.6$</td>
<td>$\mathbf{3 2 . 3} \pm \mathbf{1 . 7}$</td>
<td>$\mathbf{2 7 . 5} \pm \mathbf{1 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table A: Test results of single-shot models on EntailmentBank (Dalvi et al., 2021) (Task 2) with different input formats. All methods are based on T5-large (Raffel et al., 2020).
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure A: Pseudo-negative examples constructed by perturbing positive examples.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure B: EntailmentBank pseudo-negative examples constructed by copying premises. The step is technically valid but not useful.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure C: RuleTaker pseudo-negative examples constructed by negating the conclusion.</p>
<p>Remember how the step $s$ is executed (Sec. 4.3), the tentative score $\widehat{\operatorname{scr}}<em s="s">{n}(v)=\min \left(\operatorname{scr}</em>}(s), \operatorname{scr<em n="n">{n}(u), \ldots\right) \leq$ $\operatorname{scr}</em>(v)$. The tentative score is not greater than the original score of $v$. So the step is a no-op that should not be executed. Therefore, it is impossible to introduce loops.}(u) \leq \operatorname{scr}_{n</p>
<h2>E Procedure for Sampling Partial Proofs</h2>
<p>The sample_new function in Algorithm 1 samples a partial proof tree from the proof graph. First, the graph is a DAG, so we can visit nodes in the order of a topological sort-successors before predecessors. Second, when visiting a node, if it is not already a part of the partial proof, we add it with a probability of 0.5 . Third, whenever we add a node, we also add all of its predecessors. This ensures the result is a valid partial proof.</p>
<h2>F Training and inference details</h2>
<p>We use T5-large (Raffel et al., 2020) for the prover and RoBERTa-large (Liu et al., 2019) for the verifier. All experiments are run on machines with 2 CPUs, 16GB
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure D: A hypothetical loop in the proof graph. Nodes for proof step $(S)$ are omitted for simplicity of illustration. The blue arrow is a hypothetical proof step that introduces the loop, which should actually be a no-op because the tentative score $\widehat{\operatorname{scr}}<em n="n">{n}(v)$ not greater than the existing score $\operatorname{scr}</em>(v)$ (Sec. 4.3).
memory, and one NVIDIA A6000 GPU. Models are optimized using AdamW (Loshchilov and Hutter, 2019). The learning rate warms up linearly from 0 to a maximum value and then decays following the cosine schedule. Hyperparameters are tuned on the validation data separately for each task/method. We report test results of models trained on the training set alone, excluding the validation set. We report the average performance and the standard deviation for 5 independent runs.</p>
<p>Our results on EntailmentBank are produced by the official evaluation code. ${ }^{10}$ The code had a bug fix in May 2022 that impacted the Intermediate-AllCorrect metric of methods evaluated earlier, including IRGR (Ribeiro et al., 2022) and arXiv versions v1, v2 of EntailmentWriter (Dalvi et al., 2021). We evaluate NLProofS using the evaluation code after the bug fix. And we report the EntailmentBank numbers based on their fixed arXiv version v3 that was released on May 28, 2022. The numbers in the IRGR paper have not been updated yet, so we report its Intermediates-AllCorrect metric based on private correspondence with the authors.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Leaves</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Steps</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Intermediates</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AllCorrect</td>
</tr>
<tr>
<td style="text-align: center;">EntailmentWriter</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">27.3</td>
</tr>
<tr>
<td style="text-align: center;">EntailmentWriter (T5-11B)</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">32.1</td>
</tr>
<tr>
<td style="text-align: center;">NLProofS (ours)</td>
<td style="text-align: center;">$\mathbf{8 9 . 4} \pm \mathbf{0 . 8}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 0} \pm \mathbf{0 . 7}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 4} \pm \mathbf{1 . 9}$</td>
<td style="text-align: center;">$\mathbf{3 8 . 4} \pm \mathbf{1 . 3}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{7 1 . 9} \pm \mathbf{1 . 4}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 3} \pm \mathbf{1 . 4}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{3 7 . 1} \pm \mathbf{1 . 5}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 (Brown et al., 2020)</td>
<td style="text-align: center;">$64.2 \pm 2.3$</td>
<td style="text-align: center;">$15.3 \pm 1.9$</td>
<td style="text-align: center;">$17.6 \pm 0.6$</td>
<td style="text-align: center;">$12.3 \pm 1.4$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$53.6 \pm 1.4$</td>
<td style="text-align: center;">$22.3 \pm 1.1$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$12.3 \pm 1.4$</td>
</tr>
<tr>
<td style="text-align: center;">Codex (Chen et al., 2021)</td>
<td style="text-align: center;">$68.9 \pm 3.7$</td>
<td style="text-align: center;">$19.8 \pm 3.2$</td>
<td style="text-align: center;">$21.4 \pm 3.0$</td>
<td style="text-align: center;">$14.6 \pm 1.7$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$55.6 \pm 2.2$</td>
<td style="text-align: center;">$23.2 \pm 1.9$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$14.4 \pm 1.4$</td>
</tr>
</tbody>
</table>
<p>Table B: Validation results of proof generation on EntailmentBank (Dalvi et al., 2021). Results of GPT-3 and Codex are based on prompting with 7 in-context examples randomly sampled from the training data.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Answer accuracy</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Proof accuracy</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">All</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">All</td>
</tr>
<tr>
<td style="text-align: left;">NLProofS</td>
<td style="text-align: left;">99.7</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">97.0</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">99.7</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">96.1</td>
<td style="text-align: left;">99.3</td>
</tr>
</tbody>
</table>
<p>Table C: Validation results on the D0-D3 subset of RuleTaker (OWA) (Tafjord et al., 2021).</p>
<h2>G Distinguishing Valid and Invalid Hypotheses</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Task 1</th>
<th style="text-align: center;">Task 2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Learned (Goal) + PPM ${ }^{\dagger}$</td>
<td style="text-align: center;">$82.0 \pm 1.0$</td>
<td style="text-align: center;">$86.0 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: left;">EntailmentWriter ${ }^{\dagger}$</td>
<td style="text-align: center;">$53.0 \pm 2.0$</td>
<td style="text-align: center;">$65.0 \pm 2.0$</td>
</tr>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: center;">$\mathbf{8 2 . 4} \pm \mathbf{0 . 8}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 9} \pm \mathbf{1 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table D: Area under the ROC curve (AUROC) of distinguishing valid/invalid hypotheses on EntailmentBank (Dalvi et al., 2021) validation set. Methods with $\dagger$ are reported by Bostrom et al. (2022). All methods are based on T5-large (Raffel et al., 2020).
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure E: The Leaves-F1 metric of test results on task 2 (distractor) broken down by the length of the ground truth proof.</p>
<p>We also evaluate on distinguishing valid/invalid hypotheses introduced by Bostrom et al. (2022). In this task, the model is given a hypothesis $h$ and supporting facts $C$. But unlike in proof generation, here $h$ can be either valid or invalid w.r.t. $C$. And the model has to classify $h$ as valid/invalid. We use the dataset Bostrom et al. (2022) constructed from EntailmentBank: Examples with valid hypotheses come directly from EntailmentBank. Examples with invalid hypotheses are
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure F: The Steps-F1 metric of test results on task 2 (distractor) broken down by the length of the ground truth proof.
constructed by pairing the supporting facts in one example with the hypothesis in another random example.</p>
<p>NLProofS is developed for proof generation, and it has seen only valid hypotheses in training. So we follow Bostrom et al. (2022) to adapt proof generation systems to this new task: (1) Train the system to generate proofs for valid hypotheses. (2) Apply the system to generate proof scores for both valid and valid hypotheses. (3) Train a linear classifier on top of the scores to predict the validity of hypotheses. It requires the system to be able to produce proof scores. For our method, we use $\operatorname{scr}_{n}(h)$ defined in Eqn 1 as the proof score.</p>
<p>Results in Table D show that our method compares favorably with SCSearch, whereas EntailmentWriter falls behind. The results suggest that proof scores generated by us are more well-calibrated: they are high for valid hypotheses and low for invalid ones. This is largely attributed to our verifier, which prevents the model from hallucinating invalid proofs with confidence.</p>
<p>However, results on this task should be interpreted</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure G: The Steps-AllCorrect metric of test results on task 2 (distractor) broken down by the length of the ground truth proof.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure H: The Intermediates-F1 metric of test results on task 2 (distractor) broken down by the length of the ground truth proof.
with caution. First, they do not reflect the performance on proof generation, and SCSearch has not been evaluated on proof generation. Second, none of the methods are explicitly optimized for this task. They see only valid hypotheses during training but are asked to distinguish valid/invalid hypotheses during inference. Third, the particular dataset constructed by Bostrom et al. (2022) is too easy. An invalid hypothesis has very little lexical overlap with the supporting facts, which can be used as a cue for classifying hypotheses accurately. As a result, a simple RoBERTa baseline directly optimized for classifying the hypothesis can solve this task to almost $100 \%$.</p>
<h2>H Additional Experimental Results</h2>
<p>Validation results. Table B shows our proof generation results on the validation set of EntailmentBank (Task 2) (Dalvi et al., 2021), corresponding to Table 2. Table C shows the validation results on RuleTaker (OWA) (Tafjord et al., 2021), corresponding to Table 4.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure I: The Intermediates-AllCorrect metric of test results on task 2 (distractor) broken down by the length of the ground truth proof.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure J: The Overall-AllCorrect metric of test results on task 2 (distractor) broken down by the length of the ground truth proof.</p>
<p>Few-shot prompting with GPT-3 or Codex. We investigate whether proof generation can be solved out of the box by prompting GPT-3 (Brown et al., 2020) or Codex (Chen et al., 2021) with few-shot examples. Fig. K shows an example prompt consisting of 7 incontext examples randomly sampled from the training set of EntailmentBank (Task 2), as well as a validation example for which we want to make predictions.</p>
<p>Table B includes the results on the full validation set. They were obtained on October 20, 2022 using the model text-davinci-002 for GPT-3 and code-davinci-002 for Codex. We report the mean and standard deviation from 3 independent runes with different in-context examples in the prompt. GPT-3 and Codex perform substantially worse than other methods, demonstrating that we cannot easily solve proof generation through few-shot prompting. In addition, Codex performs better than GPT-3, which is consistent with the observations in Madaan et al. (2022) though we do not format the output as Python programs.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure K: A prompt for GPT-3 and Codex. Each example has 25 supporting facts. We only show 3 for simplicity.</p>
<p>Test results by different proof length. Fig. E, F, G, H, I, and J are EntailmentBank (Task 2) test results broken down by proof length (also Fig. 3).</p>
<p>Improving the retriever. For Task 3 of EntailmentBank, all methods in Table 2 use the same retrieved supporting facts in Dalvi et al. (2021) and focus solely on proof generation. An orthogonal direction is improving the retriever. IRGR (Ribeiro et al., 2022) designs a multi-step retriever, which obtains significant improvements on Task $3(11.8 \%$ on the Overall-AllCorrect metric) but worse results on Task 1 and Task 2 compared to the EntailmentWriter baseline. We do not compare with IRGR, since improving the retriever is orthogonal to our contributions.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ https://github.com/allenai/entailment_bank&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ The metric was introduced by PRover (Saha et al., 2020).&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
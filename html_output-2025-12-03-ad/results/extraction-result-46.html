<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-46 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-46</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-46</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-4.html">extraction-schema-4</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models perform arithmetic tasks using algorithmic or step-by-step prompting, including model details, prompting methods, performance metrics, error types, generalization ability, and comparisons to other prompting strategies.</div>
                <p><strong>Paper ID:</strong> paper-62176de125738e3b95850d1227bac81fd646b78e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/62176de125738e3b95850d1227bac81fd646b78e" target="_blank">Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with “Let’s think step by step” as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e46.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e46.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models perform arithmetic tasks using algorithmic or step-by-step prompting, including model details, prompting methods, performance metrics, error types, generalization ability, and comparisons to other prompting strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model developed by OpenAI, known for its ability to perform a wide range of natural language processing tasks, including arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 is a transformer-based model that utilizes deep learning techniques to generate human-like text based on the input it receives.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>multi-step arithmetic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot Plan-and-Solve (PS+) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_description</strong></td>
                            <td>The prompt instructs the model to first devise a plan to solve the problem, extract relevant variables, calculate intermediate results, and then solve the problem step by step.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>PS+ prompting improves accuracy over Zero-shot-CoT by at least 5% across all arithmetic datasets, achieving an average accuracy of 76.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>error_analysis</strong></td>
                            <td>Common errors include calculation errors (5%), missing-step errors (7%), and semantic misunderstanding errors (27%).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_ability</strong></td>
                            <td>The model demonstrates the ability to generalize to complex arithmetic problems by effectively following the structured prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>PS+ prompting consistently outperforms Zero-shot-CoT and is comparable to few-shot Manual-CoT prompting, indicating its effectiveness in eliciting reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_of_model_size</strong></td>
                            <td>The paper does not explicitly discuss the impact of model size on arithmetic performance under algorithmic prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_of_training_data</strong></td>
                            <td>The influence of training data composition on arithmetic performance is not detailed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tools</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prompt_sensitivity</strong></td>
                            <td>The model's performance is sensitive to prompt correctness, as variations in prompt structure can lead to significant differences in output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insights</strong></td>
                            <td>The structured prompting approach helps the model simulate arithmetic algorithms internally, leading to improved reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>The model still struggles with semantic misunderstanding errors, which remain a challenge despite improvements in calculation and reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>automation_of_prompting</strong></td>
                            <td>The paper suggests that the prompting strategy can be customized but does not provide evidence of automation in generating prompts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-46",
    "paper_id": "paper-62176de125738e3b95850d1227bac81fd646b78e",
    "extraction_schema_id": "extraction-schema-4",
    "extracted_data": [
        {
            "name_short": "GPT-3",
            "name_full": "Generative Pre-trained Transformer 3",
            "brief_description": "A large language model developed by OpenAI, known for its ability to perform a wide range of natural language processing tasks, including arithmetic reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "GPT-3 is a transformer-based model that utilizes deep learning techniques to generate human-like text based on the input it receives.",
            "model_size": "175B",
            "arithmetic_task_type": "multi-step arithmetic reasoning",
            "prompting_strategy": "Zero-shot Plan-and-Solve (PS+) prompting",
            "prompt_description": "The prompt instructs the model to first devise a plan to solve the problem, extract relevant variables, calculate intermediate results, and then solve the problem step by step.",
            "performance_metrics": "PS+ prompting improves accuracy over Zero-shot-CoT by at least 5% across all arithmetic datasets, achieving an average accuracy of 76.7%.",
            "error_analysis": "Common errors include calculation errors (5%), missing-step errors (7%), and semantic misunderstanding errors (27%).",
            "generalization_ability": "The model demonstrates the ability to generalize to complex arithmetic problems by effectively following the structured prompting.",
            "comparison_to_baselines": "PS+ prompting consistently outperforms Zero-shot-CoT and is comparable to few-shot Manual-CoT prompting, indicating its effectiveness in eliciting reasoning.",
            "impact_of_model_size": "The paper does not explicitly discuss the impact of model size on arithmetic performance under algorithmic prompting.",
            "impact_of_training_data": "The influence of training data composition on arithmetic performance is not detailed in the paper.",
            "use_of_external_tools": false,
            "tool_description": null,
            "prompt_sensitivity": "The model's performance is sensitive to prompt correctness, as variations in prompt structure can lead to significant differences in output quality.",
            "mechanistic_insights": "The structured prompting approach helps the model simulate arithmetic algorithms internally, leading to improved reasoning.",
            "limitations_and_challenges": "The model still struggles with semantic misunderstanding errors, which remain a challenge despite improvements in calculation and reasoning steps.",
            "automation_of_prompting": "The paper suggests that the prompting strategy can be customized but does not provide evidence of automation in generating prompts.",
            "uuid": "e46.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
            "rating": 1
        }
    ],
    "cost": 0.0031215,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</h1>
<p>Lei Wang ${ }^{1}$ Wanyu Xu ${ }^{2}$ Yihuai Lan Zhiqiang Hu ${ }^{3}$ Yunshi Lan ${ }^{4}$<br>Roy Ka-Wei Lee ${ }^{3}$ Ee-Peng Lim ${ }^{1 *}$<br>${ }^{1}$ Singapore Management University<br>${ }^{2}$ Southwest Jiaotong University<br>${ }^{3}$ Singapore University of Technology and Design<br>${ }^{4}$ East China Normal University</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zero-shot-CoT concatenates the target problem statement with "Let's think step by step" as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8 -shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) (Brown et al., 2020; Thoppilan et al., 2022; Chowdhery et al., 2022) have recently proven highly effective in various NLP tasks. Unlike the previous pre-trained language models (PTMs) (Devlin et al., 2019; Liu</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Error analysis of 46 GSM8K problems with incorrect answers returned by Zero-shot-CoT using GPT-3 LLM. Following Wei et al. (2022b) and Wang et al. (2022a), we assign "Calculation Error" (7\%), "Step Missing Error" (12\%), or "Semantic misunderstanding Error" (27\%) to each incorrect answer.
et al., 2019), these LLMs are typically provided as a service, with no access to model parameters due to commercial considerations and potential risks of misuse (Sun et al., 2022). Thus, it is challenging to fine-tune LLMs for downstream tasks (He et al., 2021; Houlsby et al., 2019; Devlin et al., 2019). Instead, we leverage LLMs to solve complex reasoning problems by eliciting their strong reasoning abilities over their embedded knowledge using instructions (or trigger sentences). So far, LLMs have shown impressive abilities to solve new reasoning problems by simply conditioning them on a few illustrative examples (i.e., few-shot learning) or a prompt to solve new problems without illustrative examples (i.e., zero-shot learning).</p>
<p>To tackle multi-step complex reasoning tasks using LLMs, Wei et al. (2022b) proposes few-shot chain-of-thought (CoT) prompting, which enables LLMs to explicitly generate the intermediate reasoning steps before predicting the final answer with a few manual step-by-step reasoning demonstration examples. In (Kojima et al., 2022), Zero-shot CoT eliminates the need for manually crafted examples in prompts by appending "Let's think step by step" to the target problem fed to LLMs such</p>
<p>as GPT-3. This simple prompting strategy surprisingly enables LLMs to yield performance similar to few-shot CoT prompting.</p>
<p>Despite the remarkable success of Zero-shotCoT in solving multi-step reasoning tasks, its results on a sample of 100 arithmetic test examples still point to three pitfalls (as shown in Figure 1): (i) Calculation errors (in $7 \%$ of test examples): These are errors in the calculation leading to wrong answers; (ii) Missing Step errors (in $12 \%$ of test examples): These occur when some intermediate reasoning step(s) is missed-out especially when there are many steps involved; (iii) Semantic misunderstanding (in $27 \%$ of test examples): There are other errors in semantic understanding of the problem and coherence of reasoning steps likely to be caused by the insufficient capability of LLMs.</p>
<p>To address the issue of Zero-shot-CoT caused by missing reasoning steps, we propose Plan-andSolve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. In our experiments, we simply replace "Let's think step by step" of Zero-shot-CoT with "Let's first understand the problem and devise a plan to solve the problem. Then, let's carry out the plan and solve the problem step by step" (see Figure 2 (b)).</p>
<p>To address the calculation errors of Zero-shotCoT and improve the quality of generated reasoning steps, we add more detailed instructions to PS prompting. Specifically, we extend it with "extract relevant variables and their corresponding numerals" and "calculate intermediate results (pay attention to calculation and commonsense)" instructions. This prompting variant is called the PS+ prompting strategy (see Figure 3 (b)). Despite its simplicity, PS+ strategy greatly improves the quality of the generated reasoning process. Moreover, this prompting strategy can be easily customized to solve a variety of problems other than math reasoning, such as commonsense and symbolic reasoning problems.</p>
<p>We evaluate our proposed prompting on six math reasoning datasets, including AQuA (Ling et al., 2017), GSM8K (Cobbe et al., 2021), MultiArith, AddSub, SingleEq, and SVAMP (Patel et al., 2021), two commonsense reasoning datasets (CommonsenseQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021)), and two symbolic reasoning datasets (Last Letter and Coin Flip (Wei
et al., 2022b)). The results of our experiments with GPT-3 show that our proposed Zero-shot-PS+ prompting consistently outperforms Zero-shot-CoT across all reasoning problems and datasets by a large margin, and is comparable to or exceeds Zero-shot-Program-of-Thought (PoT) Prompting (Chen et al., 2022)). Furthermore, although PS+ prompting does not require manual demonstration examples, it has a performance similar to an 8 -shot CoT prompting in arithmetic reasoning.</p>
<p>Overall, our results suggest that (a) Zero-shot PS prompting is capable of generating a higher-quality reasoning process than Zero-shot-CoT prompting, as the PS prompts provide more detailed instructions guiding the LLMs to perform correct reasoning tasks; (b) Zero-shot PS+ prompting outperforms Few-shot manual-CoT prompting on some datasets, indicating that in some instances it has the potential to outperform manual Few-shot CoT prompting, which hopefully will spark further development of new CoT prompting approaches to elicit reasoning in LLMs.</p>
<h2>2 Plan-and-Solve Prompting</h2>
<p>Overview. We introduce PS prompting, a new zero-shot CoT prompting method, which enables LLMs to explicitly devise a plan for solving a given problem and generate the intermediate reasoning process before predicting the final answer for the input problem. As opposed to prior few-shot CoT approaches where step-by-step few-shot demonstration examples are included in the prompt, the zero-shot PS prompting method does not require demonstration examples, and its prompt covers the problem itself and a simple trigger sentence. Similar to Zero-shot-CoT, Zero-shot PS prompting consists of two steps. In step 1, the prompt first makes an inference using the proposed prompting template to generate the reasoning process and the answer to a problem. In step 2, it extracts the answer for evaluation by using the answer extraction prompting, such as "Therefore, the answer (arabic numerals) is".</p>
<h3>2.1 Step 1: Prompting for Reasoning Generation</h3>
<p>To solve the input problem while avoiding errors resulting from incorrect calculation and missing reasoning steps, this step aims to construct templates to meet the following two criteria:</p>
<ul>
<li>The templates should elicit LLMs to deter-</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example inputs and outputs of GPT-3 with (a) Zero-shot-CoT prompting, (b) Plan-and-Solve (PS) prompting, and (c) answer extraction prompting. While Zero-shot-CoT encourages LLMs to generate multi-step reasoning with "<em>Let's think step by step</em>", it may still generate wrong reasoning steps when the problem is complex. Unlike Zero-shot-CoT, PS prompting first asks LLMs to devise a plan to solve the problem by generating a step-bystep plan and carrying out the plan to find the answer.</p>
<p>mine subtasks and accomplish the subtasks.</p>
<ul>
<li>The templates should guide LLMs to pay more attention to calculations and intermediate results and to ensure that they are correctly performed as much as possible.</li>
</ul>
<p>To meet the first criterion, we follow Zero-shot-CoT and first convert the input data example into a prompt with a simple template "Q: [X]. A: [T]". Specifically, the input slot [X] contains the input problem statement and a hand-crafted instruction is specified in the input slot [T] to trigger LLMs to generate a reasoning process that includes a plan and steps to complete the plan. In Zero-shot-CoT, the instruction in the input slot [T] includes the trigger instruction "<em>Let's think step by step</em>". Our Zero-shot PS prompting method instead includes the instructions "<em>devise a plan</em>" and "<em>carry out the plan</em>" as shown in Figure 2(b). Thus, the prompt would be "Q: [X]. A: <em>Let's first understand the problem and devise a plan to solve the problem. Then, let's carry out the plan and solve the problem step by step</em>."</p>
<p>We then pass the above prompt to the LLM which subsequently outputs a reasoning process. In accordance with Zero-shot-CoT, our method uses the greedy decoding strategy (1 output chain) for generating output by default.</p>
<p>To meet the second criterion, we extend the plan-based trigger sentence with more detailed instructions. Specifically, "<em>pay attention to calculation</em>" is added to the trigger sentence to request the LLMs to perform calculations as accurately as possible. To reduce errors resulting from missing necessary reasoning steps, we include "<em>extract relevant variables and their corresponding numerals</em>" to explicitly instruct the LLMs not to ignore relevant information in the input problem statement. We hypothesize that if the LLMs leave out the relevant and important variables, it is more likely to miss out relevant reasoning steps. Correlation analysis of generated content of variable and the missing reasoning step errors, shown in Figure 5, empirically supports this hypothesis (correlation value is less than 0). Additionally, we add "<em>calculate intermediate results</em>" to the prompt to enhance LLM's ability to generate relevant and important reasoning steps. The specific example is illustrated in Figure 3(b). At the end of Step 1, LLM generates the reasoning text which includes the answer. For example, the generated reasoning text in Figure 3(b) includes "<em>Combined weight of Grace and Alex = 125 + 498 = 623 pounds</em>". The strategy of adding specific descriptions to the trigger sentence represents a new way to improve zero-shot performance on complex reasoning.</p>
<h3>2.2 Step 2: Prompting for Answer Extraction</h3>
<p>Similar to Zero-shot-CoT, we devise another prompt in Step 2 to get the LLM to extract the final numerical answer from the reasoning text gener-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example inputs and outputs of GPT-3 with (a) Plan-and-Solve (PS) Prompting and (b) Plan-and-Solve prompting with more detailed instructions (PS+ prompting). PS+ prompting greatly improves the quality of the generated reasoning process.
ated in Step 1. This prompt includes the answer extraction instruction appended to the first prompt followed by the LLM generated reasoning text. This way, LLM is expected to return the final answer in the desired form.</p>
<p>Based on the example in Figure 3(b), the prompt used in Step 2 will include "Q: Grace weighs 125 pounds ... Variables: Grace: 125 pounds ... Answer: Combined weight of Grace and Alex $=125+498=623$ pounds. Therefore, the answer (arabic numerals) is". For this example, the final answer returned by LLM is " 623 ".</p>
<h2>3 Experimental Setup</h2>
<h3>3.1 Benchmarks</h3>
<p>The proposed method is evaluated on the ten benchmark datasets from three categories of reasoning problems: Arithmetic Reasoning: (1) the GSM8K (Cobbe et al., 2021) dataset of high quality linguistically diverse grade school math word problems created by human problem writers, (2) the SVAMP (Patel et al., 2021) benchmark of oneunknown arithmetic word problems for up-to-4 grade level students by making simple changes to a set of problems from another existing dataset, (3) the MultiArith (Roy and Roth, 2016) dataset
of math word problems requiring multiple reasoning steps and operations, (4) the AddSub (Hosseini et al., 2014) dataset of addition and subtraction arithmetic word problems, (5) the AQUA (Ling et al., 2017) dataset of algebraic word problems with natural language rationales, and (6) the SingleEq (Koncel-Kedziorski et al., 2015) dataset of single-equation grade-school algebra word problems with multiple math operations over nonnegative rational numbers and one variable; Commonsense Reasoning: (7) the CSQA (Talmor et al., 2019) benchmark dataset of multiple-choice questions that require different types of commonsense knowledge to obtain the correct answers; and (8) the StrategyQA (Geva et al., 2021) benchmark dataset with questions requiring multi-step reasoning but the reasoning steps are not given. Hence, they are to be inferred; Symbolic Reasoning: (9) the Last Letter Concatenation (Wei et al., 2022b) dataset of questions requiring the last letters of words in a name to be concatenated (e.g., "James Brown" $\rightarrow$ "sn"), and (10) the Coin Flip (Wei et al., 2022b) dataset of questions on whether a coin is still heads up after it is flipped or not flipped based on steps given in the questions. Table 1 shows dataset statistics.</p>
<p>Table 1: Details of datasets being evaluated. Math: arithmetic reasoning. CS: commonsense reasoning. Sym.: symbolic reasoning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;"># Samples</th>
<th style="text-align: center;">Ave. words</th>
<th style="text-align: center;">Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MultiArith</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">Number</td>
</tr>
<tr>
<td style="text-align: left;">AddSub</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">395</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">Number</td>
</tr>
<tr>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">1319</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">Number</td>
</tr>
<tr>
<td style="text-align: left;">AQUA</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">254</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">Option</td>
</tr>
<tr>
<td style="text-align: left;">SingleEq</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">508</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">Number</td>
</tr>
<tr>
<td style="text-align: left;">SVAMP</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">Number</td>
</tr>
<tr>
<td style="text-align: left;">CSQA</td>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">1221</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">Option</td>
</tr>
<tr>
<td style="text-align: left;">StrategyQA</td>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">2290</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">Yes / No</td>
</tr>
<tr>
<td style="text-align: left;">Last Letters</td>
<td style="text-align: center;">Sym.</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">String</td>
</tr>
<tr>
<td style="text-align: left;">Coin Flip</td>
<td style="text-align: center;">Sym.</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">Yes / No</td>
</tr>
</tbody>
</table>
<h3>3.2 Zero-shot and Few-shot Baselines</h3>
<p>We compare our proposed zero-shot PS and PS+ prompting methods with three types of prompting baselines: (1) Zero-shot baselines. We include zero-shot-CoT (Kojima et al., 2022) and zero-shot-PoT (Chen et al., 2022). The former appends "Let's think step by step" to the prompt without any demonstration examples. The latter uses LLM (mainly OpenAI Codex ${ }^{1}$ ) to generate a Python program and then derive an answer by executing the generated program on a Python interpreter; (2) Few-shot with manual demonstrations. ManualCoT (Wei et al., 2022b) creates eight hand-crafted examples as demonstrations. (3) Few-shot with automatic demonstrations. Auto-CoT (Zhang et al., 2022) automatically selected examples by clustering with diversity and generates reasoning chains using zero-shot-CoT to construct demonstrations.</p>
<h3>3.3 Implementations</h3>
<p>Following Auto-CoT (Zhang et al., 2022), we use the public GPT-3 (Brown et al., 2020) (175B) as the backbone language model, which is one of the most widely-used LLMs with public APIS ${ }^{2}$. Since text-davinci-003 is an upgraded version of text-davinci-002, which can produce higher-quality writing, accommodate more complex instructions, and perform better at longerform content generation, We report the results using text-davinci-003 engine for GPT-3 in the main paper. We set the temperature to 0 (argmax sampling) throughout our experiments for the greedy decoding strategy. We also include two few-shot baselines, Manual-CoT and Auto-CoT, we use 8 demonstration examples for MultiArith, GSM8K, AddSub, SingleEq, and SVAMP, 4 ex-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>amples for AQuA and Last Letters, 7 examples for CSQA, and 6 examples for StrategyQA as suggested in the original papers, Wei et al. (2022b) and Zhang et al. (2022). Evaluation metrics wise, we follow Manual-CoT (Wei et al., 2022b) and report the accuracy of all methods across datasets.</p>
<h2>4 Experimental Results</h2>
<h3>4.1 Main Results</h3>
<p>Arithmetic Reasoning. Table 2 reports the accuracy comparison of our method and existing zeroshot and few-shot methods on the arithmetic reasoning datasets. In the zero-shot setting, our PS+ prompting (i.e., PS prompting with more detailed instructions) consistently outperforms Zero-shotCoT across all arithmetic reasoning datasets by a large margin. Specifically, PS+ prompting improves the accuracy over Zero-shot CoT by at least $5 \%$ for all datasets except GSM8K which sees a $2.9 \%$ improvement. The exception could be due to GSM8K being a more challenging dataset from the linguistics complexity aspect. PS prompting also outperforms Zero-shot-CoT across all datasets, and enjoys $2.5 \%$ higher average accuracy than that of Zero-shot CoT.</p>
<p>Compared with another competitive Zero-shot baseline, PoT, the performance of PS(+) and PS promptings are still impressive. PS+ prompting outperforms PoT on five out of six arithmetic datasets. PS prompting also outperforms PoT on three arithmetic datasets. The results suggest that adding more detailed instructions to the prompt can effectively elicit higher-quality reasoning steps from LLMs.</p>
<p>Compared with the few-shot methods, Manual CoT and Auto-CoT, PS+ prompting yields an average accuracy ( $76.7 \%$ ) slightly lower than ManualCoT ( $77.6 \%$ ) but higher than Auto-CoT ( $75.9 \%$ ). While this is an unfair comparison, this result indicates that zero-shot prompting can outperform fewshot CoT prompting, which hopefully will spark further development of new ways with a less manual effort to effectively elicit reasoning in LLMs.</p>
<p>Commmonsense Reasoning. Table 3 shows the results on commonsense reasoning datasets: CommonsenseQA and StrategyQA. We only include our better zero-shot PS+ prompting strategy in this comparison. Zero-shot PoT is excluded as it does not work on this problem. While PS+ prompting underperforms Few-Shot-CoT(Manual) on this</p>
<p>Table 2: Accuracy comparison on six math reasoning datasets. The best and second best results are boldfaced and underlined respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setting</th>
<th style="text-align: left;">Method (text-davinci-003)</th>
<th style="text-align: center;">MultiArith</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">AddSub</th>
<th style="text-align: center;">AQuA</th>
<th style="text-align: center;">SingleEq</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Zero-Shot</td>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">70.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">PoT</td>
<td style="text-align: center;">$\mathbf{9 2 . 2}$</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">$\underline{43.9}$</td>
<td style="text-align: center;">$\underline{91.7}$</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">$\underline{73.5}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">PS (ours)</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">$\underline{58.2}$</td>
<td style="text-align: center;">$\underline{88.1}$</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">$\underline{72.0}$</td>
<td style="text-align: center;">72.9</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">PS+ (ours)</td>
<td style="text-align: center;">$\underline{91.8}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 3}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 2}$</td>
<td style="text-align: center;">$\mathbf{4 6 . 0}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 7}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 7}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot</td>
<td style="text-align: left;">Manual-CoT</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">77.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Auto-CoT</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">75.9</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy on commonsense reasoning datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">CSQA</th>
<th style="text-align: center;">StrategyQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Few-Shot-CoT (Manual)</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">71.2</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot-CoT</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">63.8</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot-PS+ (ours)</td>
<td style="text-align: center;">$\mathbf{7 1 . 9}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy on symbolic reasoning datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Last Letter</th>
<th style="text-align: center;">Coin Flip</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Few-Shot-CoT (Manual)</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot-CoT</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">96.8</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot-PS+ (ours)</td>
<td style="text-align: center;">$\mathbf{7 5 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 6}$</td>
</tr>
</tbody>
</table>
<p>problem, it consistently outperforms Zero-shotCoT on CommonsenseQA ( $71.9 \%$ vs. $65.2 \%$ ) and StrategyQA ( $65.4 \%$ vs. $63.8 \%$ ) datasets.</p>
<p>Symbolic Reasoning. Table 4 shows the accuracy of PS+ prompting against Zero-shot-CoT and Few-shot-CoT on symbolic reasoning datasets: Last Letters and Coin Flip. Zero-shot PoT is again excluded as it is not designed for the problem. On Last Letters, our Zero-shot PS+ prompting (75.2\%) outperforms Manual-CoT (70.6\%) and Zero-shotCoT (65.2\%). On Coin Flip, Zero-shot PS+ prompting ( $99.6 \%$ ) is slightly worse than Manual-CoT (100.0\%) but outperforms Zero-shot-CoT by a good margin ( $96.8 \%$ ). More examples from the experiment results can be found in Appendix A.2.</p>
<h3>4.2 Analysis</h3>
<p>Results of Prompting with Self-Consistency. Self-consistency (Wang et al., 2022b) (SC) is proposed to reduce randomness in LLM's output by generating $N$ reasoning results and determining the final answer by majority voting. With SC, the methods' results are usually expected to be consistent and better. Hence, we evaluate Zero-shot PS+ prompting with SC on GSM8K and SVAMP datasets. We set the temperature to 0.7 and $N$ to 10 for experiments with SC. Figure 4 shows that PS+ prompting with SC ( $73.7 \%$ and $84.4 \%$ ) substantially outperforms that without SC ( $58.7 \%$ and
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results of methods with and without selfconsistency (SC) on GSM8K and SVAMP.
$75.7 \%$ ) on GSM8K and SVAMP, respectively. The former also consistently outperforms Zero-shotCoT with SC ( $70.7 \%$ and $81.7 \%$ ) on GSM8K and SVAMP, respectively, although Zero-shot CoT also enjoys improvement with the self consistency approach.</p>
<p>Effect of Prompts. Table 5 demonstrates a comparison of the performance of 6 different input prompts. Prompts 1 and 2 are used in Zero-shot CoT and Zero-shot PoT respectively. The rest are variations of prompts used in Step 1 of the Zeroshot PS+ prompting strategies with greedy decoding. We observe that Prompt 3 with variables and numeral extraction performs worse than Prompt 1 of Zero-shot-CoT. The reason is that Prompt 3 doesn't include instructions for devising and completing a plan. However, the other prompts of Zero-shot-PS+ perform well as we add more instructions about intermediate results calculation, plan design, and implementation. The above results conclude that LLMs are capable of generating high-quality reasoning text when the prompts include more detailed instructions to guide the LLMs. More prompts for different reasoning problems can be found in Appendix A.1.</p>
<p>Error Analysis. To qualitatively evaluate the impact of the Zero-shot-PS+ prompting on calculation errors and reasoning steps missing errors, we examine the distribution of errors on the GSM8K dataset. We first randomly sample 100 problems</p>
<p>Table 5: Performance comparison of trigger sentences measured on GSM8K and SVAMP datasets with text-davinci-003 except for No. 2 (code-davinci-002). (<em>1) means the trigger sentence used in Zero-shot-CoT (Kojima et al., 2022). (</em>2) means the trigger sentence used in Zero-shot-PoT (Chen et al., 2022).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">No.</th>
<th style="text-align: center;">Trigger Sentence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">SVAMP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Let's think step by step.</td>
<td style="text-align: center;">(*1)</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">69.9</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">import math <br> import numpy as np <br> # Question: example['question'] <br> # Answer this question by implementing a solver() function. <br> def solver(): <br> # Let's write a Python program step by step, and then return the answer <br> # Firstly, we need define the following variable:</td>
<td style="text-align: center;">(*2)</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">70.8</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Extract variables and assign their corresponding numerals to these variables first and then solve the problem step by step.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">69.5</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Firstly, extract variables and their corresponding numerals. Then, calculate intermediate variables. Finally, solve the problem step by step.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">70.8</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Let's first understand the problem and devise a plan to solve the problem. Then, let's carry out the plan and solve the problem step by step.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">72.0</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Let's first understand the problem, extract relevant variables and their corresponding numerals, and make a plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">59.3</td>
<td style="text-align: center;">75.7</td>
</tr>
</tbody>
</table>
<p>Table 6: Distribution of error types of 100 examples from GSM8K where Zero-shot-CoT, zero-shot PS (Zero-shot-PS) prompting, and zero-shot PS+ prompting get incorrect final answers.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Calculation</th>
<th style="text-align: center;">Missing</th>
<th style="text-align: center;">Semantic</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Zero-shot-CoT</td>
<td style="text-align: center;">$7 \%$</td>
<td style="text-align: center;">$12 \%$</td>
<td style="text-align: center;">$27 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot-PS</td>
<td style="text-align: center;">$7 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$26 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot-PS+</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">$7 \%$</td>
<td style="text-align: center;">$27 \%$</td>
</tr>
</tbody>
</table>
<p>from GSM8K, generate the reasoning text, and extract answers using Zero-Shot-CoT, Zero-shotPS, and Zero-shot-PS+ prompting strategies. Zero-Shot-CoT generated incorrect final answers for 46 of the problems, 43 for Zero-shot-PS, and 39 for Zero-shot-PS+. Subsequently, we analyze and determine the error types of all these problems as shown in Table 6.</p>
<p>The analysis results show that PS+ prompting achieves the least calculation (5\%) and missingstep (7\%) errors, and semantic understanding errors comparable to Zero-shot-CoT. Zero-shot-PS has slightly more errors but is still better than Zero-shot-CoT. Their plan-and-solve prompts thus effectively guide the LLMs to generate clear and complete reasoning steps. Moreover, the additional detailed instructions in PS+ prompting (i.e., "extract relevant variables and their corresponding numerals" and "calculate intermediate variables") enable the LLMs to generate high-quality reason-
ing steps leading to fewer calculation errors.</p>
<h2>Correlation Analysis of Generated Reasoning</h2>
<p>and Error Types. To obtain deeper insight into the impact of PS+ prompting on error types, we examine the correlation between the sub-parts of the generated reasoning and error types. Specifically, we analyze the existence of variable definition, reasoning plan, and solution in the generated reasoning text and correlate them with the three error types. The set of problems used for this analysis study is the same as that used in the earlier error type analysis. Figure 5 shows the correlation matrix among the existence of variable definitions, plans, solutions and three different types of errors. It is observed that both variable definition and plan existences have a negative correlation with calculation errors and missing-reasoning-step errors. The Zero-shot-PS+ prompt can further improve the performance of LLMs on mathematical reasoning problems by reducing calculation errors and missing-reasoning-step errors.</p>
<h2>Exploring the Presence of Plans in PS Predic-</h2>
<p>tions. To ascertain the presence of a plan in each prediction made by PS, we conducted a random sampling of 100 data examples and examined their corresponding predictions. Our analysis reveals that 90 of the 100 predictions indeed incorporated a plan. This observation indicates the emergence</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Correlation analysis of generated reasoning and error types of randomly sampled 100 data examples from GSM8K for Zero-shot-PS+.
of strong planning abilities in recent LLMs such as GPT-3.5 and GPT-4.</p>
<h2>5 Related Work</h2>
<h3>5.1 Reasoning in NLP</h3>
<p>It is well known that complex reasoning problems are challenging for NLP models, and such problems include mathematical reasoning (Cobbe et al., 2021; Patel et al., 2021; Ling et al., 2017; Koncel-Kedziorski et al., 2016) (requiring the ability to understand mathematical concepts, calculation, and multi-step reasoning), commonsense reasoning (Talmor et al., 2019; Geva et al., 2021) (requiring the ability to make judgments based on commonsense knowledge), and logical reasoning (Wei et al., 2022b) (requiring the ability to manipulate symbols by applying formal logical rules). Before the advent of Large Language models (LLMs), Talmor et al. (2019) trained the NLP model using explanations generated by the finetuned GPT model and found that the trained model yields better performance on commonsense QA problems. Hendrycks et al. (2021) attempted to fine-tune pretrained language models with labeled rationale, but found out that these fine-tuned models could not easily generate high-quality reasoning steps. Recent work by Wei et al. (2022a) showed that LLMs demonstrates strong reasoning ability when scaled up to tens of billions of parameters, such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022). These LLMs with a few demonstration exemplars can yield impressive performance across different NLP tasks. However, these models still perform poorly in problems that require multi-step reasoning. This may be due to the fact that the few exemplars provided are insufficient to unlock the LLMs' capabilities.</p>
<h3>5.2 Prompting Methods</h3>
<p>To exploit the reasoning ability in LLMs, Wei et al. (2022b) propose Chain-of-Thought prompting, appending multiple reasoning steps before the answer to the input question. With this simple few-shot prompting strategy, LLMs are able to perform much better in complex reasoning problems. Subsequently, many works (Wang et al., 2022a; Suzgun et al., 2022; Shaikh et al., 2022; Saparov and $\mathrm{He}, 2022$ ) propose to further improve CoT prompting in different aspects, including prompt format (Chen et al., 2022), prompt selection (Lu et al., 2022), prompt ensemble (Wang et al., 2022b; Li et al., 2022; Weng et al., 2022; Fu et al., 2022), problem decomposition (Zhou et al., 2022; Khot et al., 2022; Dua et al., 2022; Press et al., 2022), and planning (Yao et al., 2022; Huang et al., 2022; Wang et al., 2023; Liu et al., 2023; Sun et al., 2023; Yao et al., 2023). Chen et al. (2022) introduced PoT prompting to use LLMs with code pre-training to write a program as a rationale for disentangling computation from reasoning. To do away with manual effort, Kojima et al. (2022) proposed Zero-shotCoT to elicit reasoning step generation without exemplars. To leverage the benefit of demonstration examples and minimize manual effort, Zhang et al. (2022) designed Auto-CoT. It first automatically obtains $k$ examples by clustering the given dataset. It then follows Zero-shot-CoT to generate rationales for the selected examples. Finally, demonstration examples are constructed by adding the generated rationales to selected examples as CoT prompts. Our work is different from the above works by focusing on eliciting multi-step reasoning by LLMs in a zero-shot approach. We ask LLMs to write a plan to decompose a complex reasoning task into multiple reasoning steps. Furthermore, we introduce detailed instructions to the prompt to avoid obvious errors in the reasoning steps. We refer readers to the survey (Huang and Chang, 2022) for more related works.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we find that Zero-shot-CoT still suffers from three pitfalls: calculation errors, missing-reasoning-step errors, and semantic understanding errors. To address these issues, we introduce plan-and-solve prompting strategies (PS and PS+ prompting). They are new zero-shot prompting methods that guide LLMs to devise a plan that divides the entire task into smaller subtasks and then</p>
<p>carries out the subtasks according to the plan. Evaluation on ten datasets across three types of reasoning problems shows PS+ prompting outperforms the previous zero-shot baselines and performs on par with few-shot CoT prompting on multiple arithmetic reasoning datasets. Overall, our results suggest that (a) Zero-shot PS+ prompting can generate a high-quality reasoning process than Zero-shotCoT prompting since the PS prompts can provide more detailed instructions guiding the LLMs to perform correct reasoning; (b) Zero-shot PS+ prompting has the potential to outperform manual Fewshot CoT prompting, which hopefully will spark further development of new CoT prompting approaches to elicit reasoning in LLMs. Moreover, PS(+) prompting is a general idea that can be used for non-reasoning tasks, and refining the plan is also an interesting idea. We leave them for future work.</p>
<h2>7 Limitations</h2>
<p>There are two limitations to this work. First, it takes effort to design the prompt to guide the LLMs to generate correct reasoning steps. The GPT-3 models are sensitive to the expressions in prompts. Thus we need to carefully design the prompts. Second, the proposed plan-and-solve prompting can help address the calculation errors and missing-reasoningstep errors, but the semantic misunderstanding errors still remain. We will explore how to address semantic misunderstanding errors by prompting instead of upgrading LLMs in the future.</p>
<h2>8 Ethics</h2>
<p>We experiment on six math reasoning datasets, including AQuA (Ling et al., 2017), GSM8K (Cobbe et al., 2021), MultiArith, AddSub, SingleEq, and SVAMP (Patel et al., 2021), two commonsense reasoning tasks (CommonsenseQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021)), and two symbolic tasks (Last Letter and Coin Flip (Wei et al., 2022b)), where GSM8K and SVAMP use the MIT License code, AQUA and StrategyQA use the Apache-2.0 code, the remaining datasets are unspecified.</p>
<p>The proposed prompts do not collect and use personal information about other individuals. The prompts we used are listed in Appendix. The prompts in this work do not contain any words that discriminate against any individual or group. In this work, prompts would not negatively impact
other people's safety.</p>
<h2>References</h2>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL, pages 41714186.</p>
<p>Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2022. Successive prompting for decomposing complex questions. arXiv preprint arXiv:2212.04092.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. TACL, 9:346-361.</p>
<p>Junxian He, Chunting Zhou, Xuezhe Ma, Taylor BergKirkpatrick, and Graham Neubig. 2021. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In EMNLP, pages 523-533.</p>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790-2799. PMLR.</p>
<p>Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403.</p>
<p>Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118-9147. PMLR.</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585-597.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In Proceedings of NAACL, pages 1152-1157.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167.</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2022. Dynamic prompt learning
via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of NAACL, pages 2080-2094.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.</p>
<p>Subhro Roy and Dan Roth. 2016. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413.</p>
<p>Abulhair Saparov and He He. 2022. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240.</p>
<p>Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang. 2022. On second thought, let's not think step by step! bias and toxicity in zeroshot reasoning. arXiv preprint arXiv:2212.08061.</p>
<p>Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, and Mohit Iyyer. 2023. Pearl: Prompting large language models to plan and execute actions over long documents.</p>
<p>Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service. arXiv preprint arXiv:2201.03514.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of NAACL-HLT, pages 41494158 .</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.</p>
<p>Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2022a. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. 2023. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022).</p>
<p>Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. 2022. Large language models are reasoners with self-verification. arXiv preprint arXiv:2212.09561.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. ArXiv, abs/2210.03629.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.</p>
<h1>A Appendix</h1>
<p>This section includes two parts: (1) Results of all prompts we have tried; (2) Example texts generated by Zero-shot-PS+. Unless otherwise mentioned, we use GPT3 (text-davinci-003) model.</p>
<h2>A. 1 Results of All Trigger Sentences</h2>
<p>Tables 7 to 16 list the results of all prompts we have tried for each dataset.</p>
<h2>A. 2 Example Outputs by Zero-shot-PS+</h2>
<p>Tables 17 to 25 list example outputs generated by Zero-shot-PS+ for each dataset.</p>
<p>Table 7: Performance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with text-davinci-003 on AQuA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">No.</th>
<th style="text-align: left;">Trigger Setence</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Let's first understand the problem and devise a plan to solve the problem. Then, let's carry out the plan <br> to solve the problem step by step.</td>
<td style="text-align: center;">42.5</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">Let's first understand the problem, extract all relevant variables and their corresponding numerals <br> carefully, and devise a plan. Then, let's carry out the plan, calculate intermediate variables (pay attention <br> to correct numerical calculation and common sense), solve the problem step by step carefully, and show <br> the answer.</td>
<td style="text-align: center;">42.9</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Let's first understand the problem, extract relevant correct variables and their correct corresponding <br> numerals, and devise complete plans. Then, let's carry out the plan, calculate intermediate variables <br> including extracted variables (pay attention to correct numerical calculation and common sense), solve <br> the problem by single equations, and show the answer.</td>
<td style="text-align: center;">43.7</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: left;">Let's first understand the problem, extract relevant variables and their corresponding numerals, and <br> make a complete plan.Then, let's carry out the plan, calculate intermediate variables (pay attention to <br> correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</td>
<td style="text-align: center;">46.0</td>
</tr>
</tbody>
</table>
<p>Table 8: Performance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with text-davinci-003 on GSM8K.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">No.</th>
<th style="text-align: left;">Trigger Setence</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Let's first understand the problem and devise a plan to solve the problem. Then, let's carry out the plan <br> to solve the problem step by step.</td>
<td style="text-align: center;">58.2</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">Let's first understand the problem, extract relevant variables and their corresponding numerals, and <br> devise a plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct <br> numeral calculation and commonsense), solve the problem step by step, and show the answer.</td>
<td style="text-align: center;">58.7</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Let's first understand the problem, extract relevant variables and their corresponding numerals, and <br> make a complete plan.Then, let's carry out the plan, calculate intermediate variables (pay attention to <br> correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</td>
<td style="text-align: center;">59.3</td>
</tr>
</tbody>
</table>
<p>Table 9: Performance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with text-davinci-003 on MultiArith.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">No.</th>
<th style="text-align: left;">Trigger Setence</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Let's first understand the problem and devise a plan to solve the problem. Then, let's carry out the plan <br> to solve the problem step by step.</td>
<td style="text-align: center;">87.2</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">Let's first understand the problem, extract relevant variables and their corresponding numerals, and <br> devise a plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct <br> numeral calculation and commonsense), solve the problem step by step, and show the answer.</td>
<td style="text-align: center;">88.3</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Let's first understand the problem, extract relevant variables and their corresponding numerals, and <br> devise a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention <br> to the correctness of the calculation and common sense), solve the problem step by step, and show <br> the answer.</td>
<td style="text-align: center;">90.5</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: left;">Let's first understand the problem, extract relevant variables and their corresponding numerals, and <br> devise a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to <br> correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</td>
<td style="text-align: center;">91.8</td>
</tr>
</tbody>
</table>
<p>Table 10: Performance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with text-davinci-003 on SVAMP.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">No.</th>
<th style="text-align: left;">Trigger Setence</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Let's first understand the problem and devise a plan to solve the problem. Then, let's carry out the plan <br> to solve the problem step by step.</td>
<td style="text-align: center;">72.0</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">Let's first understand the problem, extract relevant variables and their corresponding numerals, and <br> devise a plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct <br> numeral calculation and commonsense), solve the problem step by step, and show the answer.</td>
<td style="text-align: center;">75.4</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Let's first understand the problem, extract relevant variables and their corresponding numerals, and <br> make a complete plan.Then, let's carry out the plan, calculate intermediate variables (pay attention to <br> correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</td>
<td style="text-align: center;">75.7</td>
</tr>
</tbody>
</table>
<p>Table 11: Performance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with text-davinci-003 on AddSub.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">No.</th>
<th style="text-align: left;">Trigger Setence</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Let's first understand the problem and devise a plan to solve the problem. Then, let's carry out the plan <br> to solve the problem step by step.</td>
<td style="text-align: center;">87.3</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">Let's first understand the problem, extract relevant variables and their corresponding numerals, and <br> devise a complete plan.Then, let's carry out the plan, calculate intermediate variables (pay attention to <br> correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</td>
<td style="text-align: center;">87.8</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Let's first understand the problem, extract relevant variables and their corresponding numerals, and <br> devise a plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct <br> numeral calculation and commonsense), solve the problem step by step, and show the answer.</td>
<td style="text-align: center;">92.2</td>
</tr>
</tbody>
</table>
<p>Table 12: Performance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with text-davinci-003 on SingleEq.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">No.</th>
<th style="text-align: left;">Trigger Setence</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Let's first understand the problem and devise a plan to solve the problem. Then, let's carry out the plan <br> to solve the problem step by step.</td>
<td style="text-align: center;">92.3</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">Let's first understand the problem, extract relevant variables and their corresponding numerals, and <br> devise a plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct <br> numeral calculation and commonsense), solve the problem step by step, and show the answer.</td>
<td style="text-align: center;">94.7</td>
</tr>
</tbody>
</table>
<p>Table 13: Performance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with text-davinci-003 on CSQA.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">No.</th>
<th style="text-align: left;">Trigger Setence</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">Let's devise a plan and solve the problem step by step.</td>
<td style="text-align: center;">67.4</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">Let's first understand the problem, extract relevant variables and their corresponding numerals, and <br> devise a complete plan.Then, let's carry out the plan, calculate intermediate variables (pay attention to <br> correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</td>
<td style="text-align: center;">71.9</td>
</tr>
</tbody>
</table>
<p>Table 14: Performance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with text-davinci-003 on StrategyQA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">No.</th>
<th style="text-align: left;">Trigger Setence</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Let's devise a plan and solve the problem step by step.</td>
<td style="text-align: center;">61.5</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">Let's devise a complete plan. Then, let's carry out the plan, solve the problem step by step, and <br> show the answer.</td>
<td style="text-align: center;">63.0</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Let's first prepare relevant information and make a plan. Then, let's answer the question step by step <br> (pay attention to commonsense and logical coherence).</td>
<td style="text-align: center;">65.4</td>
</tr>
</tbody>
</table>
<p>Table 15: Performance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with text-davinci-003 on Last Letters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">No.</th>
<th style="text-align: left;">Trigger Setence</th>
<th style="text-align: left;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">Let's devise a plan and solve the problem step by step.</td>
<td style="text-align: left;">75.2</td>
</tr>
</tbody>
</table>
<p>Table 16: Performance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with text-davinci-003 on Coin Flip.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">No.</th>
<th style="text-align: center;">Trigger Setence</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Let's devise a complete plan. Then, let's carry out the plan, solve the problem step by step, and show the answer.</td>
<td style="text-align: center;">70.6</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Let's first devise a plan, then solve the problem step by step.</td>
<td style="text-align: center;">72.6</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Let's first devise a plan, then solve the problem step by step.(Distinguish between tail up and head up)</td>
<td style="text-align: center;">84.0</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Let's first understand the problem and devise a complete plan.Then, let's carry out the plan, reason problem step by step. Every step answer the subquestion "does the person flip and what is the coin current state?", and according to the last coin state, give the final answer (pay attention that the coin turns state at every flip). <br> Plan: <br> Step 1:</td>
<td style="text-align: center;">94.2</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Let's first understand the problem and devise a complete plan. Then, let's carry out the plan, reason problem step by step. Every step answer the subquestion "does the person flip and what is the coin current state?", and according to the coin last state give the final answer(pay attention to every flip, the coin turns state). <br> Plan: <br> Step 1:</td>
<td style="text-align: center;">99.0</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Let's first understand the problem and devise a complete plan. Then, let's carry out the plan and reason problem step by step. Every step answer the subquestion, "does the person flip and what is the coin's current state?". According to the coin's last state, give the final answer (pay attention to every flip and the coin's turning state).</td>
<td style="text-align: center;">99.6</td>
</tr>
</tbody>
</table>
<p>Table 17: Example outputs by Zero-shot-PS+ for AQuA.</p>
<p>Q: The average wages of a worker during a fortnight comprising of 15 consecutive working days was $\$ 90$ per day. During the first 7 days, his average wage was $\$ 87$ per day and the average wage during the last 7 days was $\$ 92$ per day. What was his wage on the 8 th day? Answer Choices: (A) $\$ 83$ (B) $\$ 92$ (C) $\$ 90$ (D) $\$ 97$ (E) $\$ 104$</p>
<p>A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and make a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</p>
<h1>Output:</h1>
<p>Variables:
Average wage during the first 7 days $=87$
Average wage during the last 7 days $=92$
Average wage during the fortnight $=90$
Plan:
Calculate the total wages earned during
the first 7 days.
Calculate the total wages earned during
the last 7 days.
Calculate the total wages earned during
the fortnight.
Calculate the wage on the 8th day.
Calculation:
Total wages earned during the first 7 days $=7 \times 87=609$
Total wages earned during the last 7 days $=7 \times 92=644$
Total wages earned during the fortnight $=15 \times 90=1350$
Wage on the 8th day $=(1350-609-644) / 1=97$
Answer: (D) $\$ 97$.
Q: If 27 bottles of soda cost A cents, how much will B bottles cost in dollars? Answer Choices: (A) $\mathrm{AB} / 2700$ (B) $27 / \mathrm{AB}$ (C) $\mathrm{AB} / 270$ (D) $2700 / \mathrm{AB}$ (E) $100 \mathrm{AB} / 27$</p>
<p>A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and make a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</p>
<h2>Output:</h2>
<p>Given:
27 bottles of soda cost A cents
B bottles of soda
Plan:</p>
<ol>
<li>Convert A cents to dollars</li>
<li>Calculate the cost of B bottles of soda</li>
</ol>
<p>Solution:</p>
<ol>
<li>Convert A cents to dollars: A cents $=\mathrm{A} / 100$ dollars</li>
<li>Calculate the cost of B bottles of soda: B bottles of soda $=(\mathrm{A} / 100) *(\mathrm{~B} / 27)=\mathrm{AB} / 2700$ dollars Answer: (A) $\mathrm{AB} / 2700$</li>
</ol>
<p>Table 18: Example outputs by Zero-shot-PS+ for GSM8K.</p>
<p>Q: James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total meters does he run a week?</p>
<p>A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and make a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</p>
<h1>Output:</h1>
<p>Given:
James runs 3 sprints 3 times a week.
Each sprint is 60 meters.
Plan:
We need to calculate the total meters run by James in a week.
Calculation:
Total number of sprints run by James in a week $=3$ sprints x 3 times $=9$ sprints
Total meters run by James in a week $=9$ sprints x 60 meters $=540$ meters
Answer:
James runs 540 meters in a week.
Q: In a dance class of 20 students, $20 \%$ enrolled in contemporary dance, $25 \%$ of the remaining enrolled in jazz dance, and the rest enrolled in hip-hop dance. What percentage of the entire students enrolled in hip-hop dance?</p>
<p>A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and make a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</p>
<h2>Output:</h2>
<p>Given:
Total number of students $=20$
Percentage of students enrolled in contemporary dance $=20 \%$
Percentage of students enrolled in jazz dance $=25 \%$
Plan:</p>
<ol>
<li>Calculate the number of students enrolled in contemporary dance.</li>
<li>Calculate the number of students remaining after subtracting the number of students enrolled in contemporary dance.</li>
<li>Calculate the number of students enrolled in jazz dance.</li>
<li>Calculate the number of students enrolled in hip-hop dance.</li>
<li>Calculate the percentage of students enrolled in hip-hop dance.</li>
</ol>
<p>Calculation:</p>
<ol>
<li>Number of students enrolled in contemporary dance $=20 \%$ of $20=20 \% \times 20=4$</li>
<li>Number of students remaining after subtracting the number of students enrolled in contemporary dance $=20-4=16$</li>
<li>Number of students enrolled in jazz dance $=25 \%$ of $16=25 \% \times 16=4$</li>
<li>Number of students enrolled in hip-hop dance $=16-4=12$</li>
<li>Percentage of students enrolled in hip-hop dance $=12 / 20 \times 100 \%=60 \%$</li>
</ol>
<p>Answer:
$60 \%$ of the entire students enrolled in hip-hop dance.</p>
<p>Table 19: Example outputs by Zero-shot-PS+ for MultiArith.</p>
<p>Q: The school cafeteria ordered 42 red apples and 7 green apples for students lunches. But, if only 9 students wanted fruit, how many extra did the cafeteria end up with?</p>
<p>A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and devise a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</p>
<h1>Output:</h1>
<p>Variables:
Red apples $=42$
Green apples $=7$
Students wanting fruit $=9$
Plan:
Calculate the total number of apples ordered by the cafeteria
Subtract the total number of apples ordered by the cafeteria from the number of students wanting fruit Calculation:
Total number of apples ordered by the cafeteria $=42+7=49$
Answer:
Extra apples $=49-9=40$
Q: Carol and her mom were picking carrots from their garden. Carol picked 29 and her mother picked 16. If only 38 of the carrots were good, how many bad carrots did they have?</p>
<p>A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and devise a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</p>
<h2>Output:</h2>
<p>Variables:
Carol: 29
Mom: 16
Good Carrots: 38
Plan:
We can calculate the number of bad carrots by subtracting the number of good carrots from the total number of carrots picked.
Calculation:
Total Carrots Picked $=$ Carol + Mom $=29+16=45$
Bad Carrots $=$ Total Carrots Picked - Good Carrots $=45-38=7$
Answer:
There were 7 bad carrots.</p>
<p>Table 20: Example outputs by Zero-shot-PS+ for SVAMP.</p>
<p>Q: Mom buys 70 white $t$ - shirts in total. If white $t$ - shirts can be purchased in packages and mom buys 14 packages. How many white $t$ - shirts does each package have?</p>
<p>A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and make and device a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</p>
<h1>Output:</h1>
<p>Variables:
Total number of white $t$-shirts $=70$
Number of packages $=14$
Plan:
We can use the equation:
Total number of white $t$-shirts $=$ Number of packages $x$ Number of $t$-shirts per package
Calculation:
Number of $t$-shirts per package $=70 / 14=5$
Answer:
Each package has 5 white $t$-shirts.
Q: Jesse's rooms are 19 feet long and 18 feet wide. If she has 20 rooms in all. How much carpet does she need to cover the floors of all rooms?</p>
<p>A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and make and device a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</p>
<h2>Output:</h2>
<p>Relevant Variables:
Number of rooms: 20
Length of each room: 19 feet
Width of each room: 18 feet
Plan:
Step 1: Calculate the area of each room.
Step 2: Multiply the area of each room by the number of rooms.
Step 3: Show the answer.
Calculation:
Step 1: Area of each room $=$ Length x Width $=19$ feet $\times 18$ feet $=342$ square feet
Step 2: Total area of all rooms $=$ Area of each room x Number of rooms $=342$ square feet $\times 20$ rooms $=6840$ square feet
Answer: Jesse needs 6840 square feet of carpet to cover the floors of all rooms.</p>
<p>Table 21: Example outputs by Zero-shot-PS+ for AddSub.</p>
<p>Q: There were 28 bales of hay in the barn. Tim stacked bales in the barn today. There are now 54 bales of hay in the barn. How many bales did he store in the barn ?</p>
<p>A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numeral calculation and commonsense), solve the problem step by step, and show the answer.</p>
<h1>Output:</h1>
<p>Given:
Initial number of bales of hay in the barn $=28$
Number of bales of hay in the barn after Tim stacked them $=54$
Plan:
Calculate the number of bales of hay Tim stored in the barn by subtracting the initial number of bales of hay from the number of bales of hay after Tim stacked them.
Calculation:
Number of bales of hay Tim stored in the barn $=54-28=26$
Answer:
Tim stored 26 bales of hay in the barn.
Q: There were 6 roses in the vase. Mary cut some roses from her flower garden. There are now 16 roses in the vase. How many roses did she cut?</p>
<p>A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numeral calculation and commonsense), solve the problem step by step, and show the answer.</p>
<h2>Output:</h2>
<p>Given:
Initial number of roses in the vase $=6$
Final number of roses in the vase $=16$
Plan:
Subtract the initial number of roses from the final number of roses to calculate the number of roses Mary cut.
Calculation:
Number of roses Mary cut $=16-6=10$
Answer:
Mary cut 10 roses from her flower garden.</p>
<p>Table 22: Example outputs by Zero-shot-PS+ for SingleEq.</p>
<p>Q: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each person paid 45 dollars, what was the total of the bill ?</p>
<p>A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numeral calculation and commonsense), solve the problem step by step, and show the answer.</p>
<h1>Output:</h1>
<p>Problem: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each person paid 45 dollars, what was the total of the bill?
Variables:</p>
<ul>
<li>Total of the bill (T)</li>
<li>Number of people (N)</li>
<li>Amount paid by each person (A)</li>
</ul>
<p>Numerals:
$-\mathrm{N}=3$
$-\mathrm{A}=45$
Plan:</p>
<ul>
<li>Calculate the total of the bill (T) by multiplying the number of people ( N ) by the amount paid by each person (A).
Calculation:
$\mathrm{T}=\mathrm{N} \times \mathrm{A}$
$\mathrm{T}=3 \times 45$
$\mathrm{T}=135$
Answer: The total of the bill was 135 dollars.
Q: Mary is baking a cake. The recipe wants 8 cups of flour. She already put in 2 cups. How many more cups does she need to add ?</li>
</ul>
<p>A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numeral calculation and commonsense), solve the problem step by step, and show the answer.</p>
<h2>Output:</h2>
<p>Mary needs to add 8 cups of flour to the cake. She has already put in 2 cups.
Plan:
Subtract the amount of flour Mary has already put in from the total amount of flour needed.
Calculation:
8 cups (total amount of flour needed) - 2 cups (amount of flour Mary has already put in) $=6$ cups (amount of flour Mary needs to add)
Answer: Mary needs to add 6 more cups of flour.</p>
<p>Table 23: Example outputs by Zero-shot-PS+ for CommonsenseQA.</p>
<p>Q: What is a place that has a bench nestled in trees? Answer Choices: (A) state park (B) bus stop (C) bus depot (D) statue (E) train station</p>
<p>A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and devise a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</p>
<h1>Output:</h1>
<p>The question is asking for a place that has a bench nestled in trees. The answer choices are (A) state park, (B) bus stop, (C) bus depot, (D) statue, and (E) train station.
The answer is (A) state park. State parks are typically located in natural areas with trees, and they often have benches for visitors to sit and enjoy the scenery.
Q: Where is a good idea but not required to have a fire extinguisher? Answer Choices: (A) school bus (B) boat (C) house (D) hospital (E) school</p>
<p>A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and devise a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</p>
<h2>Output:</h2>
<p>The question is asking for a good idea to have a fire extinguisher, but it is not required. The answer choices are (A) school bus, (B) boat, (C) house, (D) hospital, and (E) school.
The best answer is (C) house. Having a fire extinguisher in a house is a good idea, but it is not required. Fire extinguishers are not typically required in school buses, boats, hospitals, or schools.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://openai.com/blog/openai-codex/
${ }^{2}$ https://beta.openai.com/docs/models/gpt-3&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
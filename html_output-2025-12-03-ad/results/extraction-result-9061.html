<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9061 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9061</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9061</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-7ef9aafc68511afab5b287e62b754576ea37b4ce</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7ef9aafc68511afab5b287e62b754576ea37b4ce" target="_blank">Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Cognitive Science Society</p>
                <p><strong>Paper TL;DR:</strong> A hybrid Parse-and-Solve model is proposed, which augments distributional LLMs with a structured symbolic reasoning module, which shows more robust adaptation to out-of-distribution planning problems, demonstrating the promise of hybrid AI models for more human-like reasoning.</p>
                <p><strong>Paper Abstract:</strong> Human language offers a powerful window into our thoughts -- we tell stories, give explanations, and express our beliefs and goals through words. Abundant evidence also suggests that language plays a developmental role in structuring our learning. Here, we ask: how much of human-like thinking can be captured by learning statistical patterns in language alone? We first contribute a new challenge benchmark for comparing humans and distributional large language models (LLMs). Our benchmark contains two problem-solving domains (planning and explanation generation) and is designed to require generalization to new, out-of-distribution problems expressed in language. We find that humans are far more robust than LLMs on this benchmark. Next, we propose a hybrid Parse-and-Solve model, which augments distributional LLMs with a structured symbolic reasoning module. We find that this model shows more robust adaptation to out-of-distribution planning problems, demonstrating the promise of hybrid AI models for more human-like reasoning.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9061.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9061.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (Part I)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (Generative Pre-trained Transformer 3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive large transformer language model from OpenAI used in this paper as the end-to-end LLM baseline to generate natural-language plans and causal explanations for an iterative linguistic reasoning benchmark (planning and explanation domains).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLM (OpenAI) used with few-shot prompting to produce free-form natural-language plans and explanations; seeded with human goal:plan or scenario:explanation examples and sampled from the predictive distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Iterative Linguistic Reasoning Benchmark (Planning & Explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Custom linguistic benchmark developed in this paper assessing goal-directed planning and causal explanation generation under three conditions: baseline (unconstrained), single-most-common-noun-constrained, and all-initial-nouns-constrained. Human-generated constraints force out-of-distribution solutions; responses were rated on a 7-point Likert scale by blind human evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3 generations received lower mean human-evaluated 'goodness' ratings than human responses in every domain and condition; the difference was statistically significant (humans vs. LLM: p < 0.001). GPT-3 performance degraded strongly under constrained (out-of-distribution) conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human-generated plans/explanations received higher mean goodness ratings on the same 7-point Likert scale; humans significantly outperformed GPT-3 across all conditions (statistical tests reported p < 0.001). Exact mean rating numbers are not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM below human baseline: GPT-3 performed significantly worse than humans overall and was less robust to constraints designed to force out-of-distribution responses.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot prompting: seeded with human examples (n=12 goals and n=15 explanations maximum due to token limits). GPT-3 prompted with temperature 0.5, max_tokens 300; 30 rollouts per prompt were generated, degenerate outputs filtered by human 'humanness' raters (responses scoring <=2 excluded), resulting in 20 retained LLM-generated responses per prompt; blind comparative human evaluation used mixed sets of 10 human and 10 LLM responses rated by N=393 evaluators on a 1-7 Likert scale. Linear mixed effects models were used to compare sources and conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>The benchmark is a linguistic reasoning task (planning/explanation), not a canonical cognitive-psychology test battery; no raw numeric mean scores for humans vs. GPT-3 are reported (only statistical significance). Human prescreening of LLM outputs for 'humanness' may bias retained samples toward more human-like responses. Model size and exact variant of GPT-3 are not specified in the paper. Comparisons rely on human subjective ratings (likert) rather than objective task success metrics; constrained conditions use negative constraints derived from human responses which may differentially affect distributional LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9061.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9061.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-Neo (Part II)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-Neo (Large Scale Autoregressive Language Modeling with Mesh-TensorFlow)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open autoregressive transformer LLM (GPT-Neo) used in the simulated planning experiment as an LLM-as-Planner baseline that generates natural-language plans; its outputs were parsed into executable actions and evaluated in a synthetic stacking domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLM (EleutherAI / GPT-Neo family) used as an LLM-as-Planner baseline in a simulated synthetic planning domain; prompted few-shot to output action sequences in natural language that were then automatically parsed into programmatic actions for execution in the simulated environment.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Simulated Synthetic Planning Task (stacking domain)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A synthetic, fully-specified planning domain (object-stacking grammar mapped to PDDL) with 100 initial configurations and three constraint conditions (initial, +single constraint, +many constraints). Success is binary: whether the generated plan achieves the target goal when simulated.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-Neo (LLM-as-Planner) produced many invalid or unsuccessful plans; the LLM solved none of the problems in the most constrained condition and had substantially lower overall success than the hybrid Parse-and-Solve (P+S) model. Statistical test finds overall difference vs. P+S: p < 0.001.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not applicable — this simulated planning evaluation compared LLM-as-Planner to a hybrid model (P+S), not to human planner performance; human performance on the simulated tasks is not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM below the hybrid baseline: GPT-Neo performed substantially worse than the Parse-and-Solve hybrid (which uses Codex to parse and a symbolic planner to solve), and GPT-Neo failed entirely in the most constrained condition.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot prompting with a fixed header of n=3 goal/plan examples (disjoint from evaluation set); temperature 0.05. GPT-Neo-generated natural-language plans were parsed via the synthetic grammar into programmatic actions; unparseable or invalid plans were marked unsuccessful. P+S used Codex to parse natural language into PDDL predicates and a symbolic planner (Pyperplan-based solver) to produce executable plans. 100 test configurations with constrained variants (including injection of out-of-distribution objects) were evaluated; success coded binary and analyzed via mixed-effects models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>The simulated tasks are synthetic and more constrained than real-world planning; the LLM-as-Planner's failures include generating valid actions but invalid plans (violating goal constraints). Model size/variant of GPT-Neo is unspecified. The LLM baseline was judged on executable success after automatic parsing of natural-language plans, which introduces parsing failure modes distinct from planning competence; the P+S advantage conflates better parsing-to-program mapping plus symbolic search, not purely 'reasoning' by distributional models. Small synthetic domain may not generalize to richer real-world tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Evaluating large language models trained on code <em>(Rating: 2)</em></li>
                <li>GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow <em>(Rating: 1)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Skill induction and planning with latent language <em>(Rating: 2)</em></li>
                <li>PIQA: reasoning about physical commonsense in natural language <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9061",
    "paper_id": "paper-7ef9aafc68511afab5b287e62b754576ea37b4ce",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "GPT-3 (Part I)",
            "name_full": "GPT-3 (Generative Pre-trained Transformer 3)",
            "brief_description": "An autoregressive large transformer language model from OpenAI used in this paper as the end-to-end LLM baseline to generate natural-language plans and causal explanations for an iterative linguistic reasoning benchmark (planning and explanation domains).",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "Autoregressive transformer LLM (OpenAI) used with few-shot prompting to produce free-form natural-language plans and explanations; seeded with human goal:plan or scenario:explanation examples and sampled from the predictive distribution.",
            "model_size": null,
            "test_battery_name": "Iterative Linguistic Reasoning Benchmark (Planning & Explanation)",
            "test_description": "Custom linguistic benchmark developed in this paper assessing goal-directed planning and causal explanation generation under three conditions: baseline (unconstrained), single-most-common-noun-constrained, and all-initial-nouns-constrained. Human-generated constraints force out-of-distribution solutions; responses were rated on a 7-point Likert scale by blind human evaluators.",
            "llm_performance": "GPT-3 generations received lower mean human-evaluated 'goodness' ratings than human responses in every domain and condition; the difference was statistically significant (humans vs. LLM: p &lt; 0.001). GPT-3 performance degraded strongly under constrained (out-of-distribution) conditions.",
            "human_baseline_performance": "Human-generated plans/explanations received higher mean goodness ratings on the same 7-point Likert scale; humans significantly outperformed GPT-3 across all conditions (statistical tests reported p &lt; 0.001). Exact mean rating numbers are not reported in the paper.",
            "performance_comparison": "LLM below human baseline: GPT-3 performed significantly worse than humans overall and was less robust to constraints designed to force out-of-distribution responses.",
            "experimental_details": "Few-shot prompting: seeded with human examples (n=12 goals and n=15 explanations maximum due to token limits). GPT-3 prompted with temperature 0.5, max_tokens 300; 30 rollouts per prompt were generated, degenerate outputs filtered by human 'humanness' raters (responses scoring &lt;=2 excluded), resulting in 20 retained LLM-generated responses per prompt; blind comparative human evaluation used mixed sets of 10 human and 10 LLM responses rated by N=393 evaluators on a 1-7 Likert scale. Linear mixed effects models were used to compare sources and conditions.",
            "limitations_or_caveats": "The benchmark is a linguistic reasoning task (planning/explanation), not a canonical cognitive-psychology test battery; no raw numeric mean scores for humans vs. GPT-3 are reported (only statistical significance). Human prescreening of LLM outputs for 'humanness' may bias retained samples toward more human-like responses. Model size and exact variant of GPT-3 are not specified in the paper. Comparisons rely on human subjective ratings (likert) rather than objective task success metrics; constrained conditions use negative constraints derived from human responses which may differentially affect distributional LLMs.",
            "uuid": "e9061.0",
            "source_info": {
                "paper_title": "Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "GPT-Neo (Part II)",
            "name_full": "GPT-Neo (Large Scale Autoregressive Language Modeling with Mesh-TensorFlow)",
            "brief_description": "An open autoregressive transformer LLM (GPT-Neo) used in the simulated planning experiment as an LLM-as-Planner baseline that generates natural-language plans; its outputs were parsed into executable actions and evaluated in a synthetic stacking domain.",
            "citation_title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow",
            "mention_or_use": "use",
            "model_name": "GPT-Neo",
            "model_description": "Autoregressive transformer LLM (EleutherAI / GPT-Neo family) used as an LLM-as-Planner baseline in a simulated synthetic planning domain; prompted few-shot to output action sequences in natural language that were then automatically parsed into programmatic actions for execution in the simulated environment.",
            "model_size": null,
            "test_battery_name": "Simulated Synthetic Planning Task (stacking domain)",
            "test_description": "A synthetic, fully-specified planning domain (object-stacking grammar mapped to PDDL) with 100 initial configurations and three constraint conditions (initial, +single constraint, +many constraints). Success is binary: whether the generated plan achieves the target goal when simulated.",
            "llm_performance": "GPT-Neo (LLM-as-Planner) produced many invalid or unsuccessful plans; the LLM solved none of the problems in the most constrained condition and had substantially lower overall success than the hybrid Parse-and-Solve (P+S) model. Statistical test finds overall difference vs. P+S: p &lt; 0.001.",
            "human_baseline_performance": "Not applicable — this simulated planning evaluation compared LLM-as-Planner to a hybrid model (P+S), not to human planner performance; human performance on the simulated tasks is not reported.",
            "performance_comparison": "LLM below the hybrid baseline: GPT-Neo performed substantially worse than the Parse-and-Solve hybrid (which uses Codex to parse and a symbolic planner to solve), and GPT-Neo failed entirely in the most constrained condition.",
            "experimental_details": "Few-shot prompting with a fixed header of n=3 goal/plan examples (disjoint from evaluation set); temperature 0.05. GPT-Neo-generated natural-language plans were parsed via the synthetic grammar into programmatic actions; unparseable or invalid plans were marked unsuccessful. P+S used Codex to parse natural language into PDDL predicates and a symbolic planner (Pyperplan-based solver) to produce executable plans. 100 test configurations with constrained variants (including injection of out-of-distribution objects) were evaluated; success coded binary and analyzed via mixed-effects models.",
            "limitations_or_caveats": "The simulated tasks are synthetic and more constrained than real-world planning; the LLM-as-Planner's failures include generating valid actions but invalid plans (violating goal constraints). Model size/variant of GPT-Neo is unspecified. The LLM baseline was judged on executable success after automatic parsing of natural-language plans, which introduces parsing failure modes distinct from planning competence; the P+S advantage conflates better parsing-to-program mapping plus symbolic search, not purely 'reasoning' by distributional models. Small synthetic domain may not generalize to richer real-world tasks.",
            "uuid": "e9061.1",
            "source_info": {
                "paper_title": "Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 2
        },
        {
            "paper_title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow",
            "rating": 1
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "Skill induction and planning with latent language",
            "rating": 2
        },
        {
            "paper_title": "PIQA: reasoning about physical commonsense in natural language",
            "rating": 2
        }
    ],
    "cost": 0.00994875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks</h1>
<p>Katherine M. Collins ${ }^{1,2 * \dagger}$, Catherine Wong ${ }^{2 *}$, Jiahai Feng ${ }^{2}$, Megan Wei ${ }^{2}$, and Joshua B. Tenenbaum ${ }^{2}$<br>${ }^{1}$ University of Cambridge, ${ }^{2}$ MIT<br>${ }^{\dagger} \mathrm{kmc} 61 @ c a m . a c . u k$</p>
<h4>Abstract</h4>
<p>Human language offers a powerful window into our thoughts - we tell stories, give explanations, and express our beliefs and goals through words. Abundant evidence also suggests that language plays a developmental role in structuring our learning. Here, we ask: how much of human-like thinking can be captured by learning statistical patterns in language alone? We first contribute a new challenge benchmark for comparing humans and distributional large language models (LLMs). Our benchmark contains two problem-solving domains (planning and explanation generation) and is designed to require generalization to new, out-of-distribution problems expressed in language. We find that humans are far more robust than LLMs on this benchmark. Next, we propose a hybrid Parse-and-Solve model, which augments distributional LLMs with a structured symbolic reasoning module. We find that this model shows more robust adaptation to out-of-distribution planning problems, demonstrating the promise of hybrid AI models for more human-like reasoning. Keywords: language; problem-solving; programs; language of thought; neuro-symbolic models</p>
<h2>Introduction</h2>
<p>Language expresses the rich internal landscape of our thinking in a form that can be shared externally with others. We tell stories about real (what did I do today?) and hypothetical (what would I do if I won the lottery?) situations; give instructions for achieving goals ranging from the mundane (how do I put away the dishes?) to the complex (how do I fix a carburetor?); and propose explanations for both everyday events (why isn't the light bulb turning on?) and novel observations (what's that strange beeping sound?). Learning language and learning from language also play crucial roles in the development of children's thinking (Gopnik \&amp; Meltzoff, 1997; Carey, 2009; Harris et al., 2018). But what, in computational terms, is the relationship between language and thought, and between learning language and learning to think?</p>
<p>Classical theories draw a stark division between thinking as the manipulation of structured representations in an internal symbol system or language of thought (LOT) (Fodor, 1975), and language as a system of mappings between those representations and outwardly expressed</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>forms (e.g., sounds, text). Under this view, learning language plays at best a supporting role in learning to think. Recently however, a new generation of statistical language learning systems in AI has put forth a serious challenge to this view. So-called large language models (LLMs) (Brown et al., 2020; Rae et al., 2021) have demonstrated such striking success in realistic language production that they often appear to be "thinking" - and yet they are driven solely by neural networks trained to predict the distribution of next words in long text sequences from very large corpora of human language. Other work has proposed using LLMs as a universal foundation for emulating many human reasoning abilities - including capacities as diverse as physical reasoning (Bisk et al., 2019), tasklevel planning (Sharma et al., 2021; Huang et al., 2022), and even mathematical reasoning (Cobbe et al., 2021) simply by re-framing them as linguistic prediction. Under this view, "all you need is language": learning to think requires little more than learning (the statistics of) language, or learning only the latent structure sufficient to produce the most probable next word in any linguistic context.</p>
<p>In this paper, our goal is to critically assess how close modern LLMs come to actually learning to think, and to sketch out an alternative hybrid view of the languagethought interface that integrates elements of the classical LOT and recent LLM paradigms. In Part I, we describe a new, generic approach for constructing linguistic reasoning prompts that measure flexible, creative thinking abilities in novel situations, as opposed to the ability to retrieve familiar patterns of thought for familiar situations. We use an iterative constraint generation paradigm that extends initial linguistic prompts using linguistic constraints that restrict production of the most common human responses, forcing responses that require novel language production - and, we argue, a greater degree of thinking. We compare LLMs to humans using this benchmark on two domains plan and explanation generation - and find that humans both significantly outperform LLMs in general, and are comparatively more robust to prompts that extend beyond the standard distribution of human language. In Part II, we propose an alternative computational approach that leverages an LLM to map natural language into a space</p>
<p>of structured programs, such that reasoning problems can be solved by powerful, scalable symbolic algorithms rather than the purely neural form of end-to-end LLMs alone. We implement and demonstrate this model in a simplified synthetic language setting designed to emulate the planning domain in Part I. Our results suggest that such hybrid approaches are a promising way forwards, albeit still rich with potential for future improvement.</p>
<h2>Part I: Linguistic reasoning benchmark for humans and language models</h2>
<p>The first core motivation of this work is to evaluate the extent to which modeling the predictive distribution of language actually captures the underlying reasoning latent in human language. Towards this end, we propose a benchmark task (Fig. 1) based on two core reasoning abilities - goal-based planning and causal explanation - using an iterative design to challenge models which simply learn predictable responses from prior language.</p>
<h2>Methods</h2>
<p>We benchmark human and language model performance using a two-stage experimental design. In the first stage, an iterative human language production experiment (Fig. 1B), we collect human responses on two domains (planning and explanations) under three progressively more challenging conditions: a baseline initial prompt condition using a collecting of linguistic reasoning prompts; and two constrained conditions which restrict the use of common answers to each prompt, in order to encourage participants to generate novel linguistic solutions. In the second stage, we evaluate a large language model (LLM) on the same prompts, and collect responses by sampling from its predictive distribution. We describe each stage in more detail below.</p>
<h2>Human language production experiment</h2>
<p>Participants 240 participants recruited from Prolific (2 domains x 3 conditions x 40 participants) completed the task. Base pay was $\$ 15 / \mathrm{hr}$, with a $\$ 1$ quality bonus.</p>
<p>Condition 1: initial reasoning prompts To measure baseline performance, our first reasoning condition elicits human responses to initial prompts (Fig. 1B, Condition 1) on each grounding domain. We construct 28 goal prompts for the planning domain (Fig. 1A, top), designed to elicit a concrete linguistic plan and to vary in their base typicality (eg. ranging from clean the dirty dishes to get a sofa on the roof). We also construct 28 causal event prompts of varying typicality for the explanations domain (Fig. 1A, bottom), inspired by the "unusual event"
prompts in (Korman \&amp; Khemlani, 2020): each event begins with an inciting cause and its usual consequence, then poses a counterfactual.</p>
<p>Participants in this condition responded to a random batch $(\mathrm{n}=7)$ of prompts from a single domain, resulting in 10 unique responses per prompt. After responding to all prompts, we also ask participants to score base typicality for each prompt of the goal (on planning) or inciting event (on explanations) using a 7-point Likert scale.</p>
<p>Condition 2 and 3: constrained reasoning prompts In the subsequent conditions (Fig. 1B, Condition 2, 3), we evaluate the human ability to flexibly generate more novel plans and explanations for the same initial prompts, by restricting their responses to prevent subjects from falling back on the most common solutions. Specifically, we use subject responses from Condition 1 to determine common (and likely highly predictable) components of plans and explanations for each prompt. We construct linguistic constraints by extracting concrete nouns from all responses to a given prompt (using an expert human tagger, who also lemmatizes and standarizes the form of each noun). We then extend each initial prompt in two more challenging conditions: in the most common noun constrained condition, we restrict responses which use the single most common noun; in the all initial nouns constrained, we restrict all nouns which appear in the initial responses.</p>
<p>A new set of participants responded to a random batch $(\mathrm{n}=7)$ of prompts in a single domain and condition, again resulting in 10 unique responses per prompt and condition that reflect these linguistic constraints.</p>
<p>Language model matched production experiment Our human experiment yields a series of linguistic prompts, in which individual goal and explanation prompts are extended across two more challenging conditions through linguistic constraints that restrict the usage of the most common responses to each.</p>
<p>We use these same prompts to construct a benchmark language production task for our artificial language model. We evaluate our prompts on the state-of-the-art model GPT-3 (Brown et al., 2020), using the few-shot prompting technique introduced in (Brown et al., 2020) for generating predictive language for particular tasks. Specifically, we seed the model with a small number of examples ( $\mathrm{n}=12$ goals, and $\mathrm{n}=15$ explanations: the maximum number of examples the model allowed, based on token limits) pairing heldout prompts and human-generated text, then elicit generated responses for each prompt across all conditions.</p>
<p>To eliminate purely degenerate text, we also prescreen the samples by asking human evaluators ( $\mathrm{N}=370$; recruited from Prolific) to score responses for surface language errors alone, and remove the lowest scoring re-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Iterative reasoning task overview. A) Sample goals and scenarios for the planning and explanation domains, respectively, illustrating the range of base typicality of our stimuli; B) Formation of constraints from human-generated language, where constraints are selected based on frequency, with sample human generations (blue text) C) LLMgenerations (gray text) in response to the same prompts.
sponses. After screening, we collect a total of 20 LLMgenerated responses for each prompt in each condition.</p>
<p>Blind comparative human evaluation Having collected human and LLM responses to the same linguistic prompts across all conditions, we now benchmark their relative performance using blind human evaluators ( $\mathrm{N}=393$; recruited from Prolific) asked to evaluate responses in a single domain and condition a 7-point Likert scale (1: worst; 7: best). Subjects rated responses for a random batch of prompts, scoring a (randomly shuffled) set of human $(\mathrm{n}=10)$ and LLM $(\mathrm{n}=10)$ responses for each.</p>
<h2>Results</h2>
<p>Representative human responses and language model responses across both domains and conditions are depicted in Fig. 2. To investigate comparative performance, we fit linear mixed effects regression (LMER) models predicting the human-evaluated score and use a corresponding likelihood ratio test (LRT) between an ablated model to determine the significance of the fixed effects. Fig. 3 shows results of the blind human evaluation, and depicts statistical significance within and across conditions.</p>
<h2>People outperform the LLM within each reasoning</h2>
<p>condition We first fit a LMER predicting the human evaluated score from the source language generator (human or LLM), with random effects for the individual
raters and prompts (syntax: score $\sim$ source + (1 | rater_id) + (1 | prompt)). Our LRT finds that there is a significant effect $(p&lt;0.001)$ of the language source (humans vs. LLM) in both domains and in each condition (3, black indicators), humans outperform the LLM in every condition, across both domains.</p>
<p>People are more robust to out-of-distribution prompts with constraints We next consider our more central question: how well do language models perform specifically on our more constrained conditions, designed explicitly to force both humans and models to generate novel solutions to our underlying reasoning task? We expect humans to not only outperform language models in a direct comparison across individual prompts, but also to be comparatively more robust to prompts which restrict highly predictable answers, and require responses beyond the distribution of standard human language.</p>
<p>An initial LMER with a fixed effect for the condition (unconstrained, most common constraint, or many constraints) suggests that both humans and LLMs are sensitive to the added constraints, though we find a strongly significant effect of condition on performance for LLMs ( $p&lt;0.001$ ); and a weakly significant effect $(p=0.03)$ for humans in the planning domain but strongly significant for explanations $(p&lt;0.001)$.</p>
<p>However, a subsequent LMER with an interaction term</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Representative plans (A) and explanations (B), per constraint condition, generated by humans and an end-to-end LLM. Average goodness rating, over the human evaluators for each generation, is shown in orange.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Mean overall goodness rating over plans (left) and explanations (right), show across all three constraint conditions. Humans (blue boxes) significantly outperform the LLM (gray boxes) in every condition (black, lower bars) and in successive pairwise conditions (red, upper bars).
for the language source (humans or LLMs) and condition (fit pairwise across each successive set of conditions) indicates that humans and LLMs are not equally sensitive to constraints: we find strongly signficant interaction terms (Fig. 3, red) indicating that humans are more robust to added constraints across each condition. This supports our central hypothesis: language models are increasingly poor at solving the underlying task once the prompts are
constrained to restrict predictable responses.
People are more robust to goal typicality We also investigate whether another measure of linguistic predictability - the atypicality of our base prompts - also impacts LLM performance relative to humans. We fit a final LMER model with an interaction term for source and human typicality scores elicited in our initial experi-</p>
<p>ment. Interestingly, we find a significant interaction effect of typicality $(p&lt;0.001)$ for the planning domain, but not for explanations. As assessing typicality for these prompts is more complex, further work (such as linguistic measures of prompt typicality) are necessary to better assess the explanations domains. This finding further supports our broader hypothesis: that LLMs are less robust to responding to out-of-distribution scenarios which pose novel, but solvable, planning problems.</p>
<p>Qualitative analysis of commonsense failures in LLM reasoning Do large language models suffer from distinctively different patterns of errors? An initial, qualitative examination suggests that large language models are particularly prone to errors indicating a more fundamental lack of "common sense" understanding: of the underlying task, or the world knowledge required to solve it. A preliminary examination suggests that language models struggle particularly in generating coherent, realistic solutions for problems that require novel but concrete physical reasoning: as in the sofa on a roof goals in Fig. 2; or failures to understand color (The carpet was white, so the blue dye did not show up); water (the grass is not made of water and so it does not absorb the water); or gravity and material (eg. someone failing to scrape their knees after falling in pants that were made of paper). Taken together, our reasoning experiment suggests that despite the surface plausibilty of their generated text, large language models generally struggle to emulate the latent reasoning that backs human responses - once problems expressed in language require solutions beyond the standard, and most predictable, distribution of prior language, the apparent "reasoning" abilities of these models deteriorate sharply.</p>
<h2>Part II: Integrating language with structured reasoning models</h2>
<p>Our results in Part I suggest that even very large language models may not capture the characteristic flexibility of human reasoning: they struggle to produce language reflecting novel computation over an underlying task.</p>
<p>Here, we propose an alternate computational approach for reasoning about problems posed in language. Rather than hoping to simulate latent computations (like planning) by directly predicting output language, we propose a simple (but demonstrative) parse-and-symbolic planner $(\mathrm{P}+\mathrm{S})$ model which grounds language in an explicit "language-of-thought" (Fodor, 1975): a formal program expressing the meaning of the linguistic prompt, which interfaces with a symbolic computational solver (Fig. 4B).</p>
<h2>Simulated planning experiment</h2>
<p>We introduce a simulated planning domain to benchmark our parse-and-symbolic planner model against a standard LLM (here, GPT-Neo (Black, Gao, Wang, Leahy,
\&amp; Biderman, 2021)), using a restricted set of prompts designed to emulate the core properties of the broader planning domain in Part I. We focus on planning here for a straightforward metric of comparative performance: accuracy of our restricted plans can be evaluated directly on an explicit world model.</p>
<p>Initial and constrained synthetic planning prompts As with Part I, our simulated experiment benchmarks model performance under three progressively more challenging conditions: responses to an initial set of linguistic goal prompts (Fig. 4A, Condition 1); and two constrained conditions which introduce new linguistic constraints over the initial goal (Fig. 4B, Condition 2, 3). As is obvious from Fig. 4B, our conditions differ from Part I in one important respect: we extend our initial goals with positive constraints, rather than the negative constraints in Part I. This format permits a more direct, albeit simplified, evaluation of the core task - fully simulating restrictions on initial resources would require modeling (and communicating) all possible alternative ways to achieve a goal in a simulated environment - while still requiring models to reason about complex, out-of-distribution language.</p>
<p>We generate initial and constrained goal prompts along with a linguistic initial condition completely specifying the starting planning state for each prompt - from a synthetic grammar over a simple object-stacking domain (Gupta \&amp; Nau, 1992), in which each goal is a target stack of objects on a table (Fig. 4). Initial prompts involve goals with a single common household object; these are extended with both a single constraint and many constraints $(\mathrm{n}=4)$ that introduce additional, unusual objects into the initial goal. In total, we sample $\mathrm{n}=100$ initial goals and then sample constraints for both extended conditions.</p>
<p>Parse-and-solve model Fig. 4B depicts a schematic of our parse-and-solve model, designed to disentangle language from the underlying computation required to solve planning tasks expressed in language. Our model integrates two distinct components. First, it parses language into a formal program representing the initial problem state and goal (using the PDDL planning language (McDermott et al., 1998)). For more direct comparison with a benchmark LLM, we also use a large language model as our surface parser: we use the Codex (Chen et al., 2021) model (a GPT-3 model fine-tuned on a joint distribution of language and symbolic programs), which can "parse" language into programs using an analogous fewshot prompting technique (seeded with coupled examples of text and code). Unlike our comparison model, however, we employ distributional prediction only for a more constrained task: emulating the joint variation between a natural and formal language. The parsed programs are passed to our model's second core component: a symbolic</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Simulated iterative planning task overview. A) Example progressively-constrained goal stimuli; B) Evaluation compares plans generated directly from an LLM (left) with plans generated from P+S (right); C) Success rate of P+S model (purple) vs. LLM (gray); P+S statistically significantly outperforms the LLM under each condition (black bars).</p>
<p><em>Solver</em>, modeled with a search-based planner <em>Alkhazraji et al. (2020)</em> which attempts to generate a symbolic plan over a restricted set of actions (<em>moving</em> objects from one location to the next) to solve the parsed goal.</p>
<h3>Plan simulation environment</h3>
<p>Unlike in Part I, plans using the restricted space of actions in this domain can be simulated directly to assess accuracy. The P+S model outputs executable PDDL actions; the LLM-as-planner baseline outputs language which we reparse by inverting the synthetic grammar into PDDL actions. For both models, we mark unparseable or invalid plans as unsuccessful.</p>
<h3>Results</h3>
<p>Analogous analyses to those in Part I (Fig. 4C) – measuring the comparative performance of our model with an LLM, as well as its <em>robustness</em> to constraints – suggest that our hybrid model, which uses predictive modeling only to transform language into a structured interface to an underlying symbolic planner, vastly improves its ability to adapt to complexly constrained goals.</p>
<h3>Parse-and-solve model outperforms LLM</h3>
<p>An LMER comparing our two models (P+S and LLM) finds a strongly significant difference in overall performance (<em>p</em> &lt; 0.001; Fig. 4C): indeed, the LLM solves <em>none</em> of the problems in our most constrained condition.</p>
<h3>Comparative robustness to constraints</h3>
<p>Interestingly, a pairwise LMER testing for an interaction between source and condition does not find a significant interaction effect, suggesting that both models decline similarly in relative performance between conditions. One likely possibility is that this is an artifact of our restricted experiment size: the LLM simply can perform no worse in the final condition. However, these results could also suggest that the parsing approach we use here – which employs distributional models to map language into programs – may itself struggle to generalize; a hybrid <em>parser</em>, which itself draws on more structured representations (like classical <em>linguistic</em> grammars), might be better suited to parsing our most challenging compositional goals.</p>
<h3>Discussion</h3>
<p>Human language provides a richly structured window into how we think about the world. Our results, however, suggest that modeling the distribution of language alone may not be sufficient to capture the computations underlying planning, explanations, and other forms of reasoning which ground the language we produce. Instead, we propose an alternative approach: hybrid models which use distributional prediction to map language into structured formal representations of meaning that interface directly with structured symbolic algorithms <em>Ellis et al. (2020); Wong et al. (2021); Nye et al. (2021)</em>. Our contributions here leave much open for future work: to more systematically characterize regimes under which simply producing probable language closely approximates, and <em>deviates</em>, from human reasoning, and go beyond the simple demonstration model we have provided towards broader-coverage models for more realistic reasoning domains.</p>
<p>An important next step will be building on the qualitative analyses in Part I to disentangle the many factors (e.g., <em>accuracy</em>, <em>semantic coherence</em>, and <em>concision</em>) that may separate human performance from purely predictive responses. In tandem, the hybrid model we propose here offers a promising, albeit highly restricted, step towards emulating human-like reasoning over language. How do we <em>learn</em> the structured world models, or even sophisticated planning algorithms, that our simple model builds</p>
<p>upon? Our core modeling approach suggests a path towards these more fundamental learning problems: using language to construct, or guide discovery, of programs which represent novel environments, actions, and even algorithms for operating over such worlds.</p>
<h2>Acknowledgments</h2>
<p>We thank Laura Schulz, Junyi Chu, Alex Lew, Joao Loula Guimaraes de Campos, Max Nye, and the rest of the GPS Community for many thrilling, inspiring conversations, as well as practical advice with our project. We are also deeply grateful for Pratyusha Sharma, Jacob Andreas, and Noa Korneev for their thoughtful suggestions and support. We thank Yoni Friedman for his fantastic help with human annotator recruitment. Additionally, we thank the OpenAI team for increasing our quota to enable us to run more GPT-3 rollouts for Part I, and our Anonymous Reviewers for useful comments.</p>
<p>KMC is supported by a Marshall Scholarship and conducted work on the project under a Goldwater Scholarship. CW and JBT are supported by AFOSR#FA9550-19-1-0269, the MIT Quest for Intelligence, the MIT-IBM Watson AI Lab, ONR Science of AI, and DARPA Machine Common Sense.</p>
<h2>References</h2>
<p>Alkhazraji, Y., Frorath, M., Grützner, M., Helmert, M., Liebetraut, T., Mattmüller, R., ... Wülfing, J. (2020). Pyperplan. https://doi.org/10.5281/ zenodo. 3700819 . Zenodo. Retrieved from https:// doi.org/10.5281/zenodo. 3700819 doi: 10.5281/ zenodo. 3700819
Bisk, Y., Zellers, R., Bras, R. L., Gao, J., \&amp; Choi, Y. (2019). PIQA: reasoning about physical commonsense in natural language. CoRR, abs/1911.11641. Retrieved from http://arxiv.org/abs/1911.11641
Black, S., Gao, L., Wang, P., Leahy, C., \&amp; Biderman, S. (2021, March). GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. Zenodo. Retrieved from https://doi.org/10.5281/zenodo . 5297715 (If you use this software, please cite it using these metadata.) doi: 10.5281/zenodo. 5297715
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... Amodei, D. (2020). Language models are few-shot learners. CoRR, abs/2005.14165. Retrieved from https://arxiv.org/abs/2005.14165
Carey, S. (2009). Where our number concepts come from. The Journal of philosophy, 106(4), 220.
Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., ... Zaremba, W. (2021). Evaluating large language models trained on code. CoRR,
abs/2107.03374. Retrieved from https://arxiv .org/abs/2107.03374
Cobbe, K., Kosaraju, V., Bavarian, M., Hilton, J., Nakano, R., Hesse, C., \&amp; Schulman, J. (2021). Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.
Ellis, K., Wong, C., Nye, M. I., Sablé-Meyer, M., Cary, L., Morales, L., ... Tenenbaum, J. B. (2020). Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep bayesian program learning. CoRR, abs/2006.08381. Retrieved from https:// arxiv.org/abs/2006.08381
Fodor, J. A. (1975). The language of thought. Harvard University Press.
Gopnik, A., \&amp; Meltzoff, A. N. (1997). Words, thoughts, and theories. Mit Press.
Gupta, N., \&amp; Nau, D. S. (1992). On the complexity of blocks-world planning. Artificial Intelligence, 56(2-3), $223-254$.
Harris, P. L., Koenig, M. A., Corriveau, K. H., \&amp; Jaswal, V. K. (2018). Cognitive foundations of learning from testimony. Annual Review of Psychology, 69, 251-273.
Huang, W., Abbeel, P., Pathak, D., \&amp; Mordatch, I. (2022). Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
Korman, J., \&amp; Khemlani, S. (2020). Explanatory completeness. Acta Psychologica, 209, 103139. Retrieved from https://www.sciencedirect.com/science/ article/pii/S0001691819303531 doi: https://doi .org/10.1016/j.actpsy.2020.103139
McDermott, D., Ghallab, M., Howe, A., Knoblock, C., Ram, A., Veloso, M., ... Wilkins, D. (1998). Pddl - the planning domain definition language (Tech. Rep. No. TR-98-003). Yale Center for Computational Vision and Control,.
Nye, M. I., Tessler, M. H., Tenenbaum, J. B., \&amp; Lake, B. M. (2021). Improving coherence and consistency in neural sequence models with dual-system, neurosymbolic reasoning. CoRR, abs/2107.02794. Retrieved from https://arxiv.org/abs/2107.02794
Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., ... others (2021). Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446.
Sharma, P., Torralba, A., \&amp; Andreas, J. (2021). Skill induction and planning with latent language.
Wong, C., Ellis, K., Tenenbaum, J. B., \&amp; Andreas, J. (2021). Leveraging language to learn program abstractions and search heuristics. CoRR, abs/2106.11053. Retrieved from https://arxiv.org/abs/2106.11053</p>
<h1>Supplemental: Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks</h1>
<p>Here, we provide additional details on the human experiments, models, stimuli, and analyses described in Parts I and II of our main text. Further details can be found in our repository. A pre-registration logged for Part I of our study can be found at here.</p>
<h2>S1. Part I Supplemental Details</h2>
<p>We first expand on the experimental design and results discussed in Part I: Linguistic reasoning benchmark for humans and language models. Code for these experiments can be found in the "Part_I" directory of our repository.</p>
<h2>Creation of constraints</h2>
<p>Constraints used in Condition 2 and 3 per domain were constructed by an expert human tagger from the plans and explanations produced by humans in Condition 1. The human tagger extracted all concrete nouns mentioned in the human generations. We allow the expert tagger to collapse multiple semantically identical phrases into a single noun, based on prior world knowledge; e.g., collapsing "put a life net", "fly an outstretched net", and "allow them to land in the net", into a single phrase "the net."</p>
<p>In the explanation domain, constraints were constructed with respect to the full set of $N=10$ unconstrained examples. In the planning domain, plans were generated in stages. In the first stage, approximately $4-7$ humans generated plans per goal, and in the second stage, additional humans were recruited to ensure at least 10 saw each plan. Constraints for the planning domain were constructed only with respect to this first batch.</p>
<h2>Human experiments</h2>
<p>We now provide additional details on the three flavors of human experiments run in this work: 1) generation of plans and explanations, 2) rating humanness of LLMgenerated language, and 3) rating the overall goodness of mixed human- and LLM-generated language. In all experiments, participants were recruited from Prolific via Cognition. Participants were based in the United States and had to be at least 18 years old and speak English. Participants were not allowed to partake in more than one flavor of experiment.</p>
<p>Human language production and typicality rating Language production tasks were designed to last 30 minutes. Participants were asked to provide plans or explanations for seven goals or scenarios within a single condition.</p>
<p>Participants in Condition 1 (the initial prompt without constraints), also were asked to provide typicality ratings for all goals or scenarios that they had seen. All such ratings were conducted after all seven language production tasks were completed. For the planning domain, participants were asked to mark on a 1-7 Likert scale "How frequently you think people try to achieve each goal," where 1 = "Most people do this on a daily basis." and 7 $=$ "I don't see how this is even possible to do or try to do." Participants in the explanation domain rated prompts along two dimensions of typicality: 1) the base rate of the incident, X (e.g., "How frequently do you think someone observes this event in the actual world?"), and 2) the rate of the effect, Y, given the cause-event X (e.g., "Assuming that this initial event happens...how frequently do you think this results?").</p>
<p>Rating humanness To give the LLM the fairest comparison, we employ human annotators to pre-screen, or filter, LLM generations based on their "humanness." Annotators were asked to rate the likelihood that the language could have been generated by a person on a 1-7 Likert scale ("How plausible is it that a human would have generated the language of the explanation/plan?") with 1 being "completely implausible" and 7 being "completely plausible". Each rater saw a random subset of between 42-45 generations from within the same domain and associated constraint condition. The subset was randomly selected over the entire space of goals or scenarios, with the exception that a pilot check of the experiment was run for the planning domain only, where participants saw plans randomized over only three goals. We retained only explanations and plans which surpassed a score of 2 to remove degenerate responses.</p>
<p>Blind comparative human evaluation Each participant rated language generated within the same domain, and within the same condition in said domain. A sub-</p>
<p>set of 10 of the 20 LLM-generated plans or explanations was shuffled with all 10 human plans. We divided the 20 LLM generations into two sets of 10 in advance. Each participant hence saw 20 total ( 10 human, 10 LLM) generated plans or explanations - depending on the domain assigned - for 3 separate goals or scenarios, within the same condition. Participants had a 15 second break after rating all language for a single goal or scenario, before continuing to the next. In the planning domain, annotators were asked: "How good is this plan overall? Assign it a single score that summarizes how good it is for this goal," and in the explanation domain: "How good is this explanation overall? Assign it a single score that summarizes how good it is for this scenario." Ratings were marked on a 7-point Likert scale as noted in Part I.</p>
<h2>Prompting GPT-3</h2>
<p>We next expand on the prompting regime used to glean plans and explanations from GPT-3 (Brown et al., 2020). As an example, to generate plans for the goal "Cool down in a record-breaking heat wave, without using an air conditioner," we sample $N=15$ of the remaining 27 goals within the same condition - and for each of this $N$ goals, we randomly sample one of the 10 human-generated plan written to achieve that goal.</p>
<p>We concatenate these goal:plan pairs in the following format:</p>
<div class="codehilite"><pre><span></span><code><span class="err">&quot;</span><span class="nx">Goal</span><span class="p">:</span><span class="w"> </span><span class="nx">Help</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">local</span><span class="w"> </span><span class="nx">town</span><span class="w"> </span><span class="nx">mayor</span>
<span class="nx">win</span><span class="w"> </span><span class="nx">re</span><span class="o">-</span><span class="nx">election</span><span class="p">,</span><span class="w"> </span><span class="nx">without</span><span class="w"> </span><span class="nx">using</span>
<span class="nx">billboards</span><span class="p">.</span><span class="err">\</span><span class="nx">nPlan</span><span class="p">:</span><span class="w"> </span><span class="err">\&quot;</span><span class="nx">Organize</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">team</span><span class="w"> </span><span class="nx">of</span>
<span class="nx">people</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">campaign</span><span class="w"> </span><span class="nx">door</span><span class="o">-</span><span class="nx">to</span><span class="o">-</span><span class="nx">door</span><span class="p">.</span><span class="w"> </span><span class="nx">We</span><span class="w"> </span><span class="nx">can</span>
<span class="nx">also</span><span class="w"> </span><span class="nx">print</span><span class="w"> </span><span class="nx">flyers</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">pass</span><span class="w"> </span><span class="nx">out</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">streets</span>
<span class="k">and</span><span class="w"> </span><span class="nx">put</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">cars</span><span class="p">.</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="nx">Facebook</span><span class="w"> </span><span class="nx">ad</span><span class="w"> </span><span class="nx">would</span><span class="w"> </span><span class="nx">be</span>
<span class="nx">useful</span><span class="p">,</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">well</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">radio</span><span class="w"> </span><span class="nx">interview</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nx">we</span>
<span class="nx">can</span><span class="w"> </span><span class="nx">set</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">up</span><span class="p">.</span><span class="w"> </span><span class="nx">Finally</span><span class="p">,</span><span class="w"> </span><span class="nx">booking</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">debate</span>
<span class="nx">with</span><span class="w"> </span><span class="nx">his</span><span class="w"> </span><span class="nx">opponent</span><span class="w"> </span><span class="nx">would</span><span class="w"> </span><span class="nx">help</span><span class="p">.</span><span class="err">\&quot;\</span><span class="nx">nGoal</span><span class="p">:</span><span class="w"> </span><span class="nx">Build</span>
<span class="nx">a</span><span class="w"> </span><span class="nx">float</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">dazzle</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">crowd</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">Macy</span><span class="err">&#39;</span><span class="nx">s</span>
<span class="nx">Day</span><span class="w"> </span><span class="nx">Parade</span><span class="p">,</span><span class="w"> </span><span class="nx">without</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">trailer</span><span class="p">.</span><span class="err">\</span><span class="nx">nPlan</span><span class="p">:</span>
<span class="err">\&quot;</span><span class="nx">Research</span><span class="w"> </span><span class="nx">different</span><span class="w"> </span><span class="nx">types</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">floats</span><span class="w"> </span><span class="nx">that</span>
<span class="nx">are</span><span class="w"> </span><span class="nx">seen</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">Macy</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">Day</span><span class="w"> </span><span class="nx">Parade</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">see</span><span class="w"> </span><span class="k">if</span>
<span class="nx">any</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">used</span><span class="w"> </span><span class="nx">without</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">trailer</span><span class="p">.</span><span class="err">\&quot;\</span><span class="nx">nGoal</span><span class="p">:</span>
<span class="nx">Order</span><span class="w"> </span><span class="nx">food</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">restaurant</span><span class="p">,</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="nx">you</span>
<span class="nx">don</span><span class="err">&#39;</span><span class="nx">t</span><span class="w"> </span><span class="nx">speak</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">native</span><span class="w"> </span><span class="nx">language</span><span class="p">,</span><span class="w"> </span><span class="nx">without</span>
<span class="nx">using</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">translator</span><span class="w"> </span><span class="nx">app</span><span class="p">.</span><span class="err">\</span><span class="nx">nPlan</span><span class="p">:</span><span class="w"> </span><span class="err">\&quot;</span><span class="nx">The</span><span class="w"> </span><span class="nx">plan</span>
<span class="nx">would</span><span class="w"> </span><span class="nx">involve</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">gesture</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">pointing</span>
<span class="nx">at</span><span class="w"> </span><span class="nx">items</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">menu</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">describe</span><span class="w"> </span><span class="nx">what</span>
<span class="nx">you</span><span class="w"> </span><span class="nx">would</span><span class="w"> </span><span class="k">like</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">order</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">could</span><span class="w"> </span><span class="nx">also</span>
<span class="nx">find</span><span class="w"> </span><span class="nx">someone</span><span class="w"> </span><span class="nx">who</span><span class="w"> </span><span class="nx">speaks</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">language</span><span class="w"> </span><span class="k">and</span>
<span class="nx">persuade</span><span class="w"> </span><span class="nx">them</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">order</span><span class="o">/</span><span class="nx">translate</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">you</span><span class="p">.</span><span class="err">\&quot;</span>
<span class="p">[</span><span class="o">...</span><span class="p">]</span><span class="w"> </span><span class="err">\</span><span class="nx">nGoal</span><span class="p">:</span><span class="w"> </span><span class="nx">Jump</span><span class="w"> </span><span class="nx">over</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">six</span><span class="w"> </span><span class="nx">foot</span><span class="w"> </span><span class="nx">tall</span>
<span class="nx">man</span><span class="p">,</span><span class="w"> </span><span class="nx">without</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">trampoline</span><span class="p">.</span><span class="err">\</span><span class="nx">nPlan</span><span class="p">:</span>
<span class="err">\&quot;</span><span class="nx">Fashion</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">catapult</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">resistance</span>
<span class="nx">band</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">tree</span><span class="w"> </span><span class="nx">trunk</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">launch</span><span class="w"> </span><span class="nx">yourself</span>
<span class="nx">over</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">man</span><span class="p">.</span><span class="err">\&quot;\</span><span class="nx">nGoal</span><span class="p">:</span><span class="w"> </span><span class="nx">Build</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">bookshelf</span><span class="p">,</span>
</code></pre></div>

<p>without using wood.\nPlan: \"You need to go and purchase cinder blocks and sheets of plastic. Stack the cinder blocks as a base. You should do 3 rows for stability. You can then use the plastic sheets for shelves and the backing. \"\nGoal: Stop your canoe from falling down the waterfall, without using a paddle.\nPlan: \"1. Survey area for other things to grab onto . 2. Use hands in the water as makeshift paddle to maneuver your way to the possible objects (rocks, tree branches, etc). 3. Reach out to and grab onto said objects. \"[...]\nGoal: Fix a flat tire, without using a spare tire.\nPlan: \"Call roadside assistance or someone you know and have them bring a tire that fits you car. Second option is to call a towing company and have your car towed to a tire shop. \"\nGoal: Escape quicksand, without using a branch.\nPlan: \"I would",</p>
<p>Note, human-generated plans are couched in quotations and always end with a punctuation mark; periods were added to any human-generated plan that ended with a letter instead. New lines demarcated these goal:plan pairs. We therefore queried GPT-3 by starting on an open parenthesis and used "." \n" as our stop token.</p>
<p>A comparable process was employed for the explanation domain, with the modification that the number of seed prompts was set to $N=12$ due to context window constraints (explanation stimuli tended to be longer than the goals used in the planning domain). Moreover, GPT-3 was instead seeded with pairs of the form: "Scenario:" and "Explanation: This could have happened because".</p>
<p>GPT-3 was prompted using a temperature of 0.5 and queried for a maximum of 300 tokens per rollout. We define "rollout" as a single forward generation from the LLM. We set the maximum number of tokens to be greater than the most tokens used by any human participant to ensure that GPT-3 was given a fair chance to generate a sufficiently long plan or explanation.</p>
<p>We collected 30 plans and explanations per linguistic prompt. We manually removed rollouts which included derogatory language and regenerated as needed to ensure we had 30 generations per prompt. Generations which did not end on a period, but reached the maximum number of tokens were spliced to end on the last period. If no period was generated in the rollout, the generation was discarded and re-generated.</p>
<p>As there is the possibility that such parsing may have impacted the semantics - in addition to broadly degenerate, un-humanlike behavior potentially impacting ratings - we reasoned that our prescreen stage would naturally ensure that only the most human-like language was maintained in order to give GPT-3 the fairest chance possible</p>
<p>in our later evaluations.</p>
<h2>Statistical analyses</h2>
<p>We include the full syntax for all linear regression models employed to run the statistical tests conducted in Part I:</p>
<ul>
<li>Within-group (within humans, and within LLM) sensitivity to constraints: score $\sim$ condition + (1 | rater_id) + (1 | prompt)</li>
<li>Between-group (humans vs. LLM) sensitivity to constraints: score $\sim$ (source * condition) + source + condition + (1 | rater_id) + (1 | prompt)</li>
<li>Robustness to prompt typicality: score $\sim$ (source * typicality_score) + source + typicality_score + (1 | rater_id) + (1 | prompt)</li>
</ul>
<h2>S2. Part II Supplemental Details</h2>
<p>We next clarify the stimuli used, models compared, and evaluation set-up employed in Part II: Integrating language with structured reasoning models. Associated code can be found under the "Part_II" directory of our repository.</p>
<h2>Set-up and stimuli creation</h2>
<p>We design a problem setting to mimic the planning domain of Part I; however, here, we need to design tasks such that we can exactly verify whether a plan successfully solved a goal. The open-ended nature of the planning problems of Part I prohibits this degree of control, which is a necessary step to design a solid testbed to compare models. To that end, we construct a synthetic grammar over both goals and actions - that can be directly mapped into formal predicates for goals, and PDDL actions. This enables us to execute generated plans and evaluate success, as discussed in the section entitled "Plan simulation enviornment".</p>
<p>Possible actions in our grammar include: stack, unstack, and stackfromtable. Example initial configurations and goals took the form of those shown in Fig. 4 of the main text. Configurations entail stacking problems, where each stacking problem includes a random set of $N=4$ items selected from a pre-defined vocabulary of everyday household items (e.g., "plate", "keyboard"). We generate 100 test configurations using this grammar, and for each configuration, build three increasingly constrained settings - as discussed in Part II. Constraints are formed by sampling one of the constraints that fully specifies the goal condition and adding this to the goal. For every object mentioned in additional constraints (we disclude the first constraint), we swap the object with an out-ofdistribution object (where out-of-distribution is defined
with respect to what may not usually be found on a household table; e.g., "meteorite", "corduroy pants") to inject a dimension of atypicality to mirror stimuli used in Part I.</p>
<p>A listing of all stimuli used can be found in our repository.</p>
<h2>Prompting</h2>
<p>We next discuss how we prompt the LLMs used in this section. We employ an LLM-as-Planner (GPT-Neo (Black, Gao, Wang, Leahy, \&amp; Biderman, 2021) which generates plans in natural language (NL) to directly mirror the LLM set-up used in Part I. Additionally, P+S relies on an LLM (Codex (Chen et al., 2021)) to parse the initial configurations and goal specification into formal predicates.</p>
<p>LLM-as-Planner For the LLM-as-Planner model (a vanilla LLM prompted to produce an entire plan, consisting of a sequence of actions, given a linguistic goal), we construct a few-shot prompt analogous to those in Part I consisting of a header with a set of "training" example (goal, plan) pairs separated by the same delimiter, and then ending in the desired goal for which the LLM should produce a plan. For all goals, we use a header consisting of the same sequence of $\mathrm{n}=3$ (goal, plan) examples (which are disjoint from the goals we evaluate. A sample prompt containing these training example used can be found in the README within the "Part_II" directory. Solutions are structured to follow "Actions:", and "Initially:" is used as the stop token, as it would indicate the start of a new planning problem. Generations were run with a temperature of 0.05 .</p>
<p>P+S prompting The LLM-model used in the P+S model is only used as a "parser": it transduces linguistic goals into a symbolic program containing the formal environment predicates for this goal.</p>
<p>Therefore, the LLM in this example is prompted with a header with a set of "training" example (goal, parsed_goal_program_predicate) pairs separated by the same delimiter, and then ending in the desired goal for which the LLM should produce a parse. For all goals, we use a header consisting of the same sequence of $\mathrm{n}=3$ (goal, parsed_goal_program_predicate) examples, which are drawn from the same set of training examples as in the LLM-as-planner baseline prompt (but these are shown only with goal parses, not complete plans).</p>
<p>An open parentheses cues Codex to start generating PDDL, and ";" is used as a stop token. A sample prompt can be found in the same README.</p>
<h2>Plan simulation environment</h2>
<p>Our set-up consists of an LLM-as-Planner generating plans in natural language, and our $\mathrm{P}+\mathrm{S}$ model generating plans as programs (e.g., PDDL). We therefore needed to</p>
<p>design a scheme to compare such natural language against plans - namely, our "Plan simulation enviornment". Our set-up automatically parses LLM-generated language into a program using our synthetic grammar. Both programs are then evaluated as to whether they achieved the goal. Planning success is coded in a binary $(1=$ success, $0=$ fail) fashion. If the goal state is not reached, then the plan is deemed to have failed.</p>
<h2>Statistical analyses</h2>
<p>The statistical analyses run in Part II used the following syntax:</p>
<ul>
<li>Between-group (LLM-as-Planner vs. P+S) performance: succeed $\sim$ method $+(1 \mid$ id)</li>
<li>Between-group (LLM-as-Planner vs. P+S) sensitivity to constraints: succeed $\sim$ (method * constraints) + method + constraints + (1 id)</li>
</ul>
<p>We conducted an initial analysis (succeed $\sim$ constraints + (1 | id)) to see if both models were impacted by constraints, akin to Part I; we find that the LLM-as-Planner and P+S are indeed both impacted by constraints ( $p&lt;0.001$ ).</p>
<h2>Qualitative analysis of failure modes</h2>
<p>We found that the LLM-as-Planner was able to generate valid actions, but not consistently valid plans. For instance, when prompted with the following initial configuration and goal:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Initially</span><span class="o">:</span>
<span class="n">The</span><span class="w"> </span><span class="n">writing</span><span class="w"> </span><span class="n">pad</span><span class="w"> </span><span class="n">rests</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">table</span><span class="o">.</span>
<span class="n">The</span><span class="w"> </span><span class="n">notebook</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">writing</span><span class="w"> </span><span class="n">pad</span><span class="o">.</span>
<span class="n">The</span><span class="w"> </span><span class="n">tissue</span><span class="w"> </span><span class="n">box</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">notebook</span><span class="o">.</span>
<span class="n">There</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">nothing</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">tissue</span><span class="w"> </span><span class="n">box</span><span class="o">.</span>
<span class="n">The</span><span class="w"> </span><span class="n">tablet</span><span class="w"> </span><span class="n">rests</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">table</span><span class="o">.</span>
<span class="n">There</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">nothing</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">tablet</span><span class="o">.</span>
<span class="n">Goal</span><span class="o">:</span>
<span class="n">There</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">nothing</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">notebook</span><span class="o">.</span>
</code></pre></div>

<p>The LLM generated the plan: Move the tablet onto the notebook., a direct violation of the goal constraint; therefore, an invalid plan.</p>
<h2>References</h2>
<p>Black, S., Gao, L., Wang, P., Leahy, C., \&amp; Biderman, S. (2021, March). GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. Zenodo. Retrieved from https://doi.org/10.5281/zenodo . 5297715 (If you use this software, please cite it using these metadata.) doi: 10.5281/zenodo. 5297715
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... Amodei, D. (2020). Language models are few-shot learners. CoRR, abs/2005.14165. Retrieved from https://arxiv.org/abs/2005.14165</p>
<p>Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., ... Zaremba, W. (2021). Evaluating large language models trained on code. CoRR, $a b s / 2107.03374$. Retrieved from https://arxiv .org/abs/2107.03374</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Contributed equally.
${ }^{\ddagger}$ Data and code for the project can be found at: https://github.com/collinskatie/structured_flexible_and_robust</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
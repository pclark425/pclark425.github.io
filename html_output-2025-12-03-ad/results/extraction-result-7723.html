<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7723 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7723</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7723</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-271909637</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.10729v1.pdf" target="_blank">Towards Efficient Large Language Models for Scientific Text: A Review</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have ushered in a new era for processing complex information in various fields, including science. The increasing amount of scientific literature allows these models to acquire and understand scientific knowledge effectively, thus improving their performance in a wide range of tasks. Due to the power of LLMs, they require extremely expensive computational resources, intense amounts of data, and training time. Therefore, in recent years, researchers have proposed various methodologies to make scientific LLMs more affordable. The most well-known approaches align in two directions. It can be either focusing on the size of the models or enhancing the quality of data. To date, a comprehensive review of these two families of methods has not yet been undertaken. In this paper, we (I) summarize the current advances in the emerging abilities of LLMs into more accessible AI solutions for science, and (II) investigate the challenges and opportunities of developing affordable solutions for scientific domains using LLMs.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7723.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7723.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioKnowPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioKnowPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-tuning framework tailored for extracting relations from biomedical texts by incorporating imprecise domain knowledge into prompt verbalizers to improve relation extraction in biomedical corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bioknowprompt: Incorporating imprecise knowledge into prompt-tuning verbalizer with biomedical text for relation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Bioknowprompt: Incorporating imprecise knowledge into prompt-tuning verbalizer with biomedical text for relation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Qing Li et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>BioKnowPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses prompt-tuning (verbalizer-style) with injected (possibly imprecise) biomedical knowledge to extract relation triples from biomedical text, leveraging an LLM/PLM as the extractor while guiding it with domain-aware prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Biomedical text (papers/clinical text) as relation extraction corpora</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured relation triples (entity-relation-entity) / extracted relations</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Prompt-tuning (verbalizer), instruction-style prompts</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Biomedical text corpora (as reported by the paper; unspecified in the review)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authors (as summarized in the review) note prompting can be challenging for certain phenomena and may struggle with highly imbalanced training data.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Efficient Large Language Models for Scientific Text: A Review', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7723.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7723.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioMedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioMedGPT (Open multimodal generative pretrained transformer for biomedicine)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal generative pretrained transformer tailored for biomedical applications that uses knowledge distillation to connect complex biological information to natural language, intended to accelerate tasks like drug and target discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Biomedgpt: Open multimodal generative pretrained transformer for biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Biomedgpt: Open multimodal generative pretrained transformer for biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Yizhen Luo et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>BioMedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A multimodal generative LLM designed for biomedicine that relies on knowledge distillation and multimodal pretraining to encode complex biological knowledge into language-capable models, enabling generation of domain-relevant hypotheses and facilitating discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Multimodal biomedical data (text, possibly other modalities); scientific biomedical corpora</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated biomedical outputs (textual knowledge, hypotheses, generative responses) and distilled biomedical knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Instruction-tuning / distillation-based training (as summarized in review)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioMedGPT (proposed model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Biomedical corpora (unspecified in the review summary)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported by review as facilitating substantial advancements in drug and therapeutic target discovery (qualitative summary); quantitative details not provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Efficient Large Language Models for Scientific Text: A Review', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7723.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7723.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DARWIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DARWIN series (domain-specific LLMs for natural science)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of domain-specific LLMs (material science, chemistry, physics) built on LLaMA that automate generation of scientific instruction text and improve performance on scientific tasks while reducing reliance on closed-source LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Darwin series: Domain specific large language models for natural science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Darwin series: Domain specific large language models for natural science</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Tong Xie et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DARWIN</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Adapts a general LLaMA model to multiple scientific domains via domain-specific pretraining and automated generation of scientific instruction-style training data, effectively synthesizing domain knowledge into the model.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Domain-specific scientific corpora (materials, chemistry, physics texts) and automatically generated instruction text</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Domain-adapted LLMs capable of scientific text generation and task performance; synthetic instruction datasets</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Automated instruction generation / instruction-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-based DARWIN models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>LLaMA-7B base for reported versions (as summarized)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Domain-specific corpora collected for materials, chemistry, physics (details in original paper)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Review states DARWIN achieved notable advances in automating scientific text instruction and improved performance in scientific tasks; numeric results not provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Efficient Large Language Models for Scientific Text: A Review', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7723.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7723.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciGLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciGLM (Scientific Generalist Language Models via self-reflective instruction annotation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of scientific language models trained with a self-reflective instruction annotation framework to address data scarcity and improve models' ability on scientific discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sciglm: Training scientific language models with self-reflective instruction annotation and tuning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Sciglm: Training scientific language models with self-reflective instruction annotation and tuning</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Dan Zhang et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciGLM (self-reflective instruction annotation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Generates and refines instruction-style training data via a self-reflective annotation loop (using LLMs to create and improve instructions), then instruction-tunes base models to improve scientific reasoning and discovery capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Scientific texts and synthesized instruction annotations (generated/refined by LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Instruction-tuned scientific LLMs and synthetic instruction datasets to support scientific discovery tasks</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Self-reflective instruction generation (LLM-generated annotations), instruction-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Synthetic instruction datasets generated with self-reflection; scientific corpora (unspecified details in review)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human evaluations and benchmark improvements (as summarized; exact metrics not listed in review)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported improvements: improved base models (e.g., ChatGLM variants) by several percentage points on instruction-following and scientific reasoning tasks per review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Efficient Large Language Models for Scientific Text: A Review', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7723.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7723.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Prop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-Prop (Predicting properties from text descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses large language models to predict physical and electronic properties of crystalline solids from textual descriptions, showing LLMs can generate structured property predictions from descriptive text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>LLM-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Andre Niyongabo Rubungo et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-Prop</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Converts textual descriptions of crystal structures into predictions of physical/electronic properties using LLMs, effectively distilling structured material property knowledge from descriptive text.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Textual descriptions of crystal structures (TextEdge dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured property predictions (physical and electronic properties)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>LLM-based mapping from descriptive text to property outputs (prompting/regression via LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>TextEdge (benchmark of text descriptions of crystal structures) as reported in the review</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported to outperform a domain-specific fine-tuned BERT model (MatBERT) despite using significantly fewer parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Efficient Large Language Models for Scientific Text: A Review', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7723.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7723.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Magdi (Structured Distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Magdi: Structured distillation of multi-agent interaction graphs improves reasoning in smaller language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured distillation approach that distills multi-agent interaction graphs into smaller language models to improve reasoning capabilities, i.e., transferring structured interaction knowledge into compact models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Magdi: Structured distillation of multi-agent interaction graphs improves reasoning in smaller language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Magdi: Structured distillation of multi-agent interaction graphs improves reasoning in smaller language models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Justin Chih-Yao Chen et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Magdi (structured distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Performs structured distillation by converting multi-agent interaction traces/graphs into distilled supervisory signals to train smaller language models, improving their reasoning by injecting structured interaction knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Multi-agent interaction graphs / structured interaction traces (as used in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Smaller LMs with improved reasoning (distilled structured knowledge embedded in model)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Structured distillation (graph-to-model distillation); likely multi-step supervised distillation rather than simple prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Efficient Large Language Models for Scientific Text: A Review', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7723.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7723.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FreeAL / LLMaAA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FreeAL (and LLMaAA) - LLMs as annotators for active learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Frameworks that use large language models as automatic annotators within active learning loops to label or select high-quality samples from unlabeled corpora, reducing human annotation requirements and synthesizing labeled datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FreeAL: Towards human-free active learning in the era of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>FreeAL: Towards human-free active learning in the era of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Ruixuan Xiao et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>FreeAL (and LLMaAA variants)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses LLMs to generate annotations/in-context labels and an SLM to filter/refine high-quality samples in an active learning loop, enabling generation of labeled data from unlabeled corpora without human annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Unlabeled corpora / text datasets (could include scientific texts)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Annotated labeled datasets (structured labels, NER/relation labels, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>LLM-as-annotator with in-context learning, active selection and filtering strategies</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs used as annotators (unspecified; review mentions robust LLMs like ChatGPT for related approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Evaluated on benchmark NLP datasets (per review summary); scientific-specific corpora not specified</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Zero-shot performance improvements on downstream tasks (as summarized in review)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>FreeAL significantly enhanced zero-shot performance for both SLMs and LLMs without human supervision in experiments across eight benchmark datasets (per review summary).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on quality of LLM-generated annotations and SLM filtering; depends on LLM capabilities and may propagate LLM biases.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Efficient Large Language Models for Scientific Text: A Review', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7723.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7723.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BabyLM distillation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BabyLM / BabyLLaMA knowledge distillation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Distillation approach training an ensemble of teachers (GPT-2 and small LLaMA) on a small curated dataset and distilling them into a compact student model that preserves or improves performance on small-corpus tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Inar Timiryasov et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>BabyLLaMA (BabyLM distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Trains multiple teacher models on a limited corpus and performs knowledge distillation into a small student model, demonstrating distilled models can exceed or match teachers on small-data benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Small curated text corpus (BabyLM dataset, ~10M words)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Compact student LLM (distilled model) retaining knowledge from teachers</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Knowledge distillation (teacher-student training), ensemble distillation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 and small LLaMA as teachers; distilled small LLaMA student</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Student: ~58M parameters (as reported in reference summary)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>BabyLM 10M-word dataset</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Distilled 58M-parameter LLaMA student outperformed original teachers and comparable models trained without distillation on small-data benchmarks (per review summary).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Efficient Large Language Models for Scientific Text: A Review', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Biomedgpt: Open multimodal generative pretrained transformer for biomedicine <em>(Rating: 2)</em></li>
                <li>Bioknowprompt: Incorporating imprecise knowledge into prompt-tuning verbalizer with biomedical text for relation extraction <em>(Rating: 2)</em></li>
                <li>Sciglm: Training scientific language models with self-reflective instruction annotation and tuning <em>(Rating: 2)</em></li>
                <li>Darwin series: Domain specific large language models for natural science <em>(Rating: 2)</em></li>
                <li>LLM-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions <em>(Rating: 2)</em></li>
                <li>Magdi: Structured distillation of multi-agent interaction graphs improves reasoning in smaller language models <em>(Rating: 2)</em></li>
                <li>FreeAL: Towards human-free active learning in the era of large language models <em>(Rating: 2)</em></li>
                <li>Baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7723",
    "paper_id": "paper-271909637",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "BioKnowPrompt",
            "name_full": "BioKnowPrompt",
            "brief_description": "A prompt-tuning framework tailored for extracting relations from biomedical texts by incorporating imprecise domain knowledge into prompt verbalizers to improve relation extraction in biomedical corpora.",
            "citation_title": "Bioknowprompt: Incorporating imprecise knowledge into prompt-tuning verbalizer with biomedical text for relation extraction",
            "mention_or_use": "mention",
            "paper_title": "Bioknowprompt: Incorporating imprecise knowledge into prompt-tuning verbalizer with biomedical text for relation extraction",
            "authors": "Qing Li et al.",
            "year": 2022,
            "method_name": "BioKnowPrompt",
            "method_description": "Uses prompt-tuning (verbalizer-style) with injected (possibly imprecise) biomedical knowledge to extract relation triples from biomedical text, leveraging an LLM/PLM as the extractor while guiding it with domain-aware prompts.",
            "input_type": "Biomedical text (papers/clinical text) as relation extraction corpora",
            "output_type": "Structured relation triples (entity-relation-entity) / extracted relations",
            "prompting_technique": "Prompt-tuning (verbalizer), instruction-style prompts",
            "model_name": null,
            "model_size": null,
            "datasets_used": "Biomedical text corpora (as reported by the paper; unspecified in the review)",
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": "Authors (as summarized in the review) note prompting can be challenging for certain phenomena and may struggle with highly imbalanced training data.",
            "counterpoint": false,
            "uuid": "e7723.0",
            "source_info": {
                "paper_title": "Towards Efficient Large Language Models for Scientific Text: A Review",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "BioMedGPT",
            "name_full": "BioMedGPT (Open multimodal generative pretrained transformer for biomedicine)",
            "brief_description": "A multimodal generative pretrained transformer tailored for biomedical applications that uses knowledge distillation to connect complex biological information to natural language, intended to accelerate tasks like drug and target discovery.",
            "citation_title": "Biomedgpt: Open multimodal generative pretrained transformer for biomedicine",
            "mention_or_use": "mention",
            "paper_title": "Biomedgpt: Open multimodal generative pretrained transformer for biomedicine",
            "authors": "Yizhen Luo et al.",
            "year": 2023,
            "method_name": "BioMedGPT",
            "method_description": "A multimodal generative LLM designed for biomedicine that relies on knowledge distillation and multimodal pretraining to encode complex biological knowledge into language-capable models, enabling generation of domain-relevant hypotheses and facilitating discovery.",
            "input_type": "Multimodal biomedical data (text, possibly other modalities); scientific biomedical corpora",
            "output_type": "Generated biomedical outputs (textual knowledge, hypotheses, generative responses) and distilled biomedical knowledge",
            "prompting_technique": "Instruction-tuning / distillation-based training (as summarized in review)",
            "model_name": "BioMedGPT (proposed model)",
            "model_size": null,
            "datasets_used": "Biomedical corpora (unspecified in the review summary)",
            "evaluation_metric": null,
            "reported_results": "Reported by review as facilitating substantial advancements in drug and therapeutic target discovery (qualitative summary); quantitative details not provided in the review.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7723.1",
            "source_info": {
                "paper_title": "Towards Efficient Large Language Models for Scientific Text: A Review",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "DARWIN",
            "name_full": "DARWIN series (domain-specific LLMs for natural science)",
            "brief_description": "A suite of domain-specific LLMs (material science, chemistry, physics) built on LLaMA that automate generation of scientific instruction text and improve performance on scientific tasks while reducing reliance on closed-source LLMs.",
            "citation_title": "Darwin series: Domain specific large language models for natural science",
            "mention_or_use": "mention",
            "paper_title": "Darwin series: Domain specific large language models for natural science",
            "authors": "Tong Xie et al.",
            "year": 2023,
            "method_name": "DARWIN",
            "method_description": "Adapts a general LLaMA model to multiple scientific domains via domain-specific pretraining and automated generation of scientific instruction-style training data, effectively synthesizing domain knowledge into the model.",
            "input_type": "Domain-specific scientific corpora (materials, chemistry, physics texts) and automatically generated instruction text",
            "output_type": "Domain-adapted LLMs capable of scientific text generation and task performance; synthetic instruction datasets",
            "prompting_technique": "Automated instruction generation / instruction-tuning",
            "model_name": "LLaMA-based DARWIN models",
            "model_size": "LLaMA-7B base for reported versions (as summarized)",
            "datasets_used": "Domain-specific corpora collected for materials, chemistry, physics (details in original paper)",
            "evaluation_metric": null,
            "reported_results": "Review states DARWIN achieved notable advances in automating scientific text instruction and improved performance in scientific tasks; numeric results not provided in review.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7723.2",
            "source_info": {
                "paper_title": "Towards Efficient Large Language Models for Scientific Text: A Review",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "SciGLM",
            "name_full": "SciGLM (Scientific Generalist Language Models via self-reflective instruction annotation)",
            "brief_description": "A suite of scientific language models trained with a self-reflective instruction annotation framework to address data scarcity and improve models' ability on scientific discovery tasks.",
            "citation_title": "Sciglm: Training scientific language models with self-reflective instruction annotation and tuning",
            "mention_or_use": "mention",
            "paper_title": "Sciglm: Training scientific language models with self-reflective instruction annotation and tuning",
            "authors": "Dan Zhang et al.",
            "year": 2024,
            "method_name": "SciGLM (self-reflective instruction annotation)",
            "method_description": "Generates and refines instruction-style training data via a self-reflective annotation loop (using LLMs to create and improve instructions), then instruction-tunes base models to improve scientific reasoning and discovery capabilities.",
            "input_type": "Scientific texts and synthesized instruction annotations (generated/refined by LLMs)",
            "output_type": "Instruction-tuned scientific LLMs and synthetic instruction datasets to support scientific discovery tasks",
            "prompting_technique": "Self-reflective instruction generation (LLM-generated annotations), instruction-tuning",
            "model_name": null,
            "model_size": null,
            "datasets_used": "Synthetic instruction datasets generated with self-reflection; scientific corpora (unspecified details in review)",
            "evaluation_metric": "Human evaluations and benchmark improvements (as summarized; exact metrics not listed in review)",
            "reported_results": "Reported improvements: improved base models (e.g., ChatGLM variants) by several percentage points on instruction-following and scientific reasoning tasks per review summary.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7723.3",
            "source_info": {
                "paper_title": "Towards Efficient Large Language Models for Scientific Text: A Review",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "LLM-Prop",
            "name_full": "LLM-Prop (Predicting properties from text descriptions)",
            "brief_description": "A method that uses large language models to predict physical and electronic properties of crystalline solids from textual descriptions, showing LLMs can generate structured property predictions from descriptive text.",
            "citation_title": "LLM-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions",
            "mention_or_use": "mention",
            "paper_title": "LLM-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions",
            "authors": "Andre Niyongabo Rubungo et al.",
            "year": 2024,
            "method_name": "LLM-Prop",
            "method_description": "Converts textual descriptions of crystal structures into predictions of physical/electronic properties using LLMs, effectively distilling structured material property knowledge from descriptive text.",
            "input_type": "Textual descriptions of crystal structures (TextEdge dataset)",
            "output_type": "Structured property predictions (physical and electronic properties)",
            "prompting_technique": "LLM-based mapping from descriptive text to property outputs (prompting/regression via LLM)",
            "model_name": null,
            "model_size": null,
            "datasets_used": "TextEdge (benchmark of text descriptions of crystal structures) as reported in the review",
            "evaluation_metric": null,
            "reported_results": "Reported to outperform a domain-specific fine-tuned BERT model (MatBERT) despite using significantly fewer parameters.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7723.4",
            "source_info": {
                "paper_title": "Towards Efficient Large Language Models for Scientific Text: A Review",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Magdi (Structured Distillation)",
            "name_full": "Magdi: Structured distillation of multi-agent interaction graphs improves reasoning in smaller language models",
            "brief_description": "A structured distillation approach that distills multi-agent interaction graphs into smaller language models to improve reasoning capabilities, i.e., transferring structured interaction knowledge into compact models.",
            "citation_title": "Magdi: Structured distillation of multi-agent interaction graphs improves reasoning in smaller language models",
            "mention_or_use": "mention",
            "paper_title": "Magdi: Structured distillation of multi-agent interaction graphs improves reasoning in smaller language models",
            "authors": "Justin Chih-Yao Chen et al.",
            "year": 2024,
            "method_name": "Magdi (structured distillation)",
            "method_description": "Performs structured distillation by converting multi-agent interaction traces/graphs into distilled supervisory signals to train smaller language models, improving their reasoning by injecting structured interaction knowledge.",
            "input_type": "Multi-agent interaction graphs / structured interaction traces (as used in the cited work)",
            "output_type": "Smaller LMs with improved reasoning (distilled structured knowledge embedded in model)",
            "prompting_technique": "Structured distillation (graph-to-model distillation); likely multi-step supervised distillation rather than simple prompting",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7723.5",
            "source_info": {
                "paper_title": "Towards Efficient Large Language Models for Scientific Text: A Review",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "FreeAL / LLMaAA",
            "name_full": "FreeAL (and LLMaAA) - LLMs as annotators for active learning",
            "brief_description": "Frameworks that use large language models as automatic annotators within active learning loops to label or select high-quality samples from unlabeled corpora, reducing human annotation requirements and synthesizing labeled datasets.",
            "citation_title": "FreeAL: Towards human-free active learning in the era of large language models",
            "mention_or_use": "mention",
            "paper_title": "FreeAL: Towards human-free active learning in the era of large language models",
            "authors": "Ruixuan Xiao et al.",
            "year": 2023,
            "method_name": "FreeAL (and LLMaAA variants)",
            "method_description": "Uses LLMs to generate annotations/in-context labels and an SLM to filter/refine high-quality samples in an active learning loop, enabling generation of labeled data from unlabeled corpora without human annotators.",
            "input_type": "Unlabeled corpora / text datasets (could include scientific texts)",
            "output_type": "Annotated labeled datasets (structured labels, NER/relation labels, etc.)",
            "prompting_technique": "LLM-as-annotator with in-context learning, active selection and filtering strategies",
            "model_name": "LLMs used as annotators (unspecified; review mentions robust LLMs like ChatGPT for related approaches)",
            "model_size": null,
            "datasets_used": "Evaluated on benchmark NLP datasets (per review summary); scientific-specific corpora not specified",
            "evaluation_metric": "Zero-shot performance improvements on downstream tasks (as summarized in review)",
            "reported_results": "FreeAL significantly enhanced zero-shot performance for both SLMs and LLMs without human supervision in experiments across eight benchmark datasets (per review summary).",
            "limitations": "Relies on quality of LLM-generated annotations and SLM filtering; depends on LLM capabilities and may propagate LLM biases.",
            "counterpoint": false,
            "uuid": "e7723.6",
            "source_info": {
                "paper_title": "Towards Efficient Large Language Models for Scientific Text: A Review",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "BabyLM distillation",
            "name_full": "BabyLM / BabyLLaMA knowledge distillation",
            "brief_description": "Distillation approach training an ensemble of teachers (GPT-2 and small LLaMA) on a small curated dataset and distilling them into a compact student model that preserves or improves performance on small-corpus tasks.",
            "citation_title": "Baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty",
            "mention_or_use": "mention",
            "paper_title": "Baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty",
            "authors": "Inar Timiryasov et al.",
            "year": 2023,
            "method_name": "BabyLLaMA (BabyLM distillation)",
            "method_description": "Trains multiple teacher models on a limited corpus and performs knowledge distillation into a small student model, demonstrating distilled models can exceed or match teachers on small-data benchmarks.",
            "input_type": "Small curated text corpus (BabyLM dataset, ~10M words)",
            "output_type": "Compact student LLM (distilled model) retaining knowledge from teachers",
            "prompting_technique": "Knowledge distillation (teacher-student training), ensemble distillation",
            "model_name": "GPT-2 and small LLaMA as teachers; distilled small LLaMA student",
            "model_size": "Student: ~58M parameters (as reported in reference summary)",
            "datasets_used": "BabyLM 10M-word dataset",
            "evaluation_metric": null,
            "reported_results": "Distilled 58M-parameter LLaMA student outperformed original teachers and comparable models trained without distillation on small-data benchmarks (per review summary).",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7723.7",
            "source_info": {
                "paper_title": "Towards Efficient Large Language Models for Scientific Text: A Review",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Biomedgpt: Open multimodal generative pretrained transformer for biomedicine",
            "rating": 2,
            "sanitized_title": "biomedgpt_open_multimodal_generative_pretrained_transformer_for_biomedicine"
        },
        {
            "paper_title": "Bioknowprompt: Incorporating imprecise knowledge into prompt-tuning verbalizer with biomedical text for relation extraction",
            "rating": 2,
            "sanitized_title": "bioknowprompt_incorporating_imprecise_knowledge_into_prompttuning_verbalizer_with_biomedical_text_for_relation_extraction"
        },
        {
            "paper_title": "Sciglm: Training scientific language models with self-reflective instruction annotation and tuning",
            "rating": 2,
            "sanitized_title": "sciglm_training_scientific_language_models_with_selfreflective_instruction_annotation_and_tuning"
        },
        {
            "paper_title": "Darwin series: Domain specific large language models for natural science",
            "rating": 2,
            "sanitized_title": "darwin_series_domain_specific_large_language_models_for_natural_science"
        },
        {
            "paper_title": "LLM-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions",
            "rating": 2,
            "sanitized_title": "llmprop_predicting_physical_and_electronic_properties_of_crystalline_solids_from_their_text_descriptions"
        },
        {
            "paper_title": "Magdi: Structured distillation of multi-agent interaction graphs improves reasoning in smaller language models",
            "rating": 2,
            "sanitized_title": "magdi_structured_distillation_of_multiagent_interaction_graphs_improves_reasoning_in_smaller_language_models"
        },
        {
            "paper_title": "FreeAL: Towards human-free active learning in the era of large language models",
            "rating": 2,
            "sanitized_title": "freeal_towards_humanfree_active_learning_in_the_era_of_large_language_models"
        },
        {
            "paper_title": "Baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty",
            "rating": 2,
            "sanitized_title": "baby_llama_knowledge_distillation_from_an_ensemble_of_teachers_trained_on_a_small_dataset_with_no_performance_penalty"
        }
    ],
    "cost": 0.018109499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Efficient Large Language Models for Scientific Text: A Review</p>
<p>Huy Quoc To 
Ming Liu m.liu@deakin.edu.au 
Guangyan Huang guangyan.huang@deakin.edu.au 
Christopher Berner 
Lenny Bogdonoff 
Oleg Boiko 
Madelaine Boyd 
Anna-Luisa Brakman 
Greg Brock- Man 
Tim Brooks 
Miles Brundage 
Kevin Button 
Trevor Cai 
Rosie Campbell 
Andrew Cann 
Brittany Carey 
Chelsea Carlson 
Rory Carmichael 
Brooke Chan 
Che Chang 
Fotis Chantzis 
Derek Chen 
Sully Chen 
Ruby Chen 
Jason Chen 
Mark Chen 
Ben Chess 
Chester Cho 
HyungCasey Chu 
Won Chung 
Dave Cummings 
Jeremiah Currier 
Yunxing Dai 
Tarun Goel 
Gabriel Gogineni 
Rapha Goh 
Jonathan Gontijo- Lopes 
Morgan Gordon 
Scott Grafstein 
Ryan Gray 
Joshua Greene 
ShixiangShane Gross 
Yufei Gu 
Chris Guo 
Jesse Hallacy 
Jeff Han 
Yuchen Harris 
Mike He 
Johannes Heaton 
Chris Heidecke 
Alan Hesse 
Wade Hickey 
Peter Hickey 
Brandon Hoeschele 
Kenny Houghton 
Shengli Hsu 
Xin Hu 
Joost Hu 
Shantanu Huizinga 
Shawn Jain 
Joanne Jain 
Angela Jang 
Roger Jiang 
Haozhun Jiang 
Denny Jin 
Shino Jin 
Billie Jomoto 
Hee- Woo Jonn 
Tomer Jun 
ukasz Kaftan 
Ali Kaiser 
Ingmar Ka- Mali 
Kanitscheider 
Shirish Nitish 
Tabarak Keskar 
Logan Khan 
Jong Wook Kilpatrick 
Christina Kim 
Yongjik Kim 
Jan Hendrik Kim 
Jamie Kirch- Ner 
Matt Kiros 
Daniel Knight 
ukasz Kokotajlo 
Andrew Kondraciuk 
Aris Kondrich 
Kyle Kon- Stantinidis 
Gretchen Kosic 
Vishal Krueger 
Michael Kuo 
Ikai Lampe 
Teddy Lan 
Jan Lee 
Jade Leike 
Daniel Leung 
ChakMing Levy 
Rachel Li 
Molly Lim 
Stephanie Lin 
Mateusz Lin 
Theresa Litwin 
Ryan Lopez 
Patricia Lowe 
Anna Lue 
Kim Makanju 
Sam Malfacini 
Todor Manning 
Yaniv Markov 
Bianca Markovski 
Katie Martin 
Andrew Mayer 
Bob Mayne 
Scott Mayer Mcgrew 
Christine Mckinney 
Paul Mcleavey 
Jake Mcmillan 
David Mcneil 
Aalok Medina 
Jacob Mehta 
Luke Menick 
Andrey Metz 
Pamela Mishchenko 
Vinnie Mishkin 
Evan Monaco 
Daniel Morikawa 
Tong Mossing 
Mira Mu 
Oleg Murati 
David Murk 
Ashvin Mly 
Reiichiro Nair 
Rajeev Nakano 
Arvind Nayak 
Richard Neelakantan 
Hyeonwoo Ngo 
Long Noh 
Cullen Ouyang 
Jakub O'keefe 
Alex Pachocki 
Joe Paino 
Ashley Palermo 
Pantuliano 
Carl Ross 
Bob Rotsted 
Henri Roussez 
Nick Ry- Der 
Mario Saltarelli 
Ted Sanders 
Shibani Santurkar 
Girish Sastry 
Heather Schmidt 
David Schnurr 
John Schulman 
Daniel Selsam 
Kyla Sheppard 
Toki Sherbakov 
Jessica Shieh 
Sarah Shoker 
Pranav Shyam 
Szymon Sidor 
Eric Sigler 
Maddie Simens 
Jordan Sitkin 
Katarina Slama 
Ian Sohl 
Benjamin Sokolowsky 
Yang Song 
Natalie Staudacher </p>
<p>School of Information Technology
Deakin University
Australia</p>
<p>Cory Decareaux
Thomas Degry
Noah Deutsch
Arka Dhar, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam FedusDamien Deville, David Dohan, Steve Dowling, Niko Felix</p>
<p>Simn Posada Fishman
Juston Forte</p>
<p>Isabella Ful-ford
Elie GeorgesLeo Gao, Christian Gibson, Vik</p>
<p>Giambat-tista Parascandolo
Joel Parish
Emy Parparita, Mikhail Pavlov, Andrew PengAlex Passos</p>
<p>Filipe de Avila Belbute Peres
Adam Perel-man
Michael Petrov</p>
<p>Henrique Ponde de Oliveira Pinto
Poko-rnyMichael</p>
<p>Michelle Pokrass
Vitchyr H. Pong, Tolly Pow-ell, Boris PowerAlethea Power</p>
<p>Elizabeth Proehl
Raul Puri, Alec Radford, Jack Rae, Aditya RameshCameron Raymond</p>
<p>Francis Real
Kendra Rimbach</p>
<p>Towards Efficient Large Language Models for Scientific Text: A Review
2C2497ACE40CFE2A9C0FC5D630747B6D
Large language models (LLMs) have ushered in a new era for processing complex information in various fields, including science.The increasing amount of scientific literature allows these models to acquire and understand scientific knowledge effectively, thus improving their performance in a wide range of tasks.Due to the power of LLMs, they require extremely expensive computational resources, intense amounts of data, and training time.Therefore, in recent years, researchers have proposed various methodologies to make scientific LLMs more affordable.The most well-known approaches align in two directions.It can be either focusing on the size of the models or enhancing the quality of data.To date, a comprehensive review of these two families of methods has not yet been undertaken.In this paper, we (I) summarize the current advances in the emerging abilities of LLMs into more accessible AI solutions for science, and (II) investigate the challenges and opportunities of developing affordable solutions for scientific domains using LLMs.</p>
<p>Introduction</p>
<p>Recently, the advancement of large language models (LLMs) has equipped us with the capability to address complex tasks that demand an understanding of both structure and language.The key factors that make LLMs so rapid are the huge amount of generated data and the advancement in computational architectures.With regard to scientific data itself, this domain has witnessed a constantly and rapidly increase in number of publications.For example, there were more than 2.4 million scholarly papers on ArXiv1 (up to 2024) and 36 million publications on PubMeb2 (up to 2022).The exponential growth enables us to leverage the success of language models to effectively learn scientific knowledge.Recently, (Ho et al., 2024) reported that there are about 117 language models constructed for the scientific domain.Tasks such as Text Classification, Summarization, or Named-Entity Recognition are effectively handled by most of these models, which have shown impressive performance on various benchmarks.</p>
<p>In order to perform sophisticated problemsolving tasks, the scientific language models are designed to have complex structures with vast scale.In particular, recent LLMs for science such as Galactica (Taylor et al., 2022) are equipped with groundbreaking architectures.They surpass most of the evaluations on reasoning, problem solving, and knowledge understanding.However, these LLMs face inevitable drawbacks, as they require a substantial amount of resources, for example, a large-scale high-quality dataset and a high training or inference cost (OpenAI et al., 2024).Whereas, these resources are not available in many cases, such as low-resource languages or small organizations with limited computational access.Therefore, limitations related to accessibility, cost, and adaptability pose substantial challenges to fully utilize the capabilities of scientific LLMs.In this review, we present two main contributions:</p>
<p> We provide a comprehensive overview of the latest developments of the application of Large Language Models (LLMs) in scientific fields.This includes discussing how LLMs have been tailored to solve complex scientific problems, and their integration into existing studies.</p>
<p> We delve into examining the technical and economic barriers to deploying LLMs for science, exploring cost-effective strategies and innovations, and identifying opportunities for 1 arXiv:2408.10729v1[cs.CL] 20 Aug 2024</p>
<p>reducing expenses without compromising performance.</p>
<p>Related surveys</p>
<p>There are few surveys on pre-trained language models (PLM) for science (Ho et al., 2024;Kalyan et al., 2022;Wang et al., 2023) and to make LLMs more accessible (Wan et al., 2024b;Xu et al., 2024).</p>
<p>Regarding scientific language models, (Ho et al., 2024) presented the first comprehensive review of scientific language models (SciLM), describing more than 110 models, evaluating their performance across various domains and tasks, and addressing future research challenges.The study analyzed six key dimensions: scope, target language models, domains, scientific texts, languages, and modalities, offering a distinctive evolutionary perspective on SciLMs over recent years.Specifically, in the biomedical sector, (Wang et al., 2023) reviewed the latest advancements of PLMs in the biomedical field and their applications in downstream biomedical tasks.The authors explored the motivations for PLMs in the biomedical sector, outlined key concepts, and proposed a taxonomy that classifies existing biomedical PLMs from multiple perspectives.In a different related survey by (Kalyan et al., 2022), researchers explored the core principles of transformer-based PLMs, such as pre-training techniques, pre-training tasks, finetuning strategies, and embedding types tailored to the biomedical domain.The study presented a classification system for transformer-based PLMs, evaluated all existing models, identified several challenges, and proposed possible solutions.</p>
<p>According to (Wan et al., 2024b), while LLMs are at the forefront of the AI revolution, their impressive abilities require significant resources.As model sizes increase, the GPU hours needed for training increase exponentially, enhancing performance but also increasing costs.Furthermore, inference operations significantly add to the financial burden of running LLMs.Although enlarging the size of the model improves performance, it reduces inference throughput (increases inference latency), which poses obstacles in extending their adoption to a wider range of customers and applications affordably.The substantial resource requirements of LLMs underscore the critical necessity of devising methods that improve their efficiency.In the survey of (Wan et al., 2024b), a fairly detailed number of approaches based on three aspects is listed: model-centric, data-centric, and frameworks.However, their survey lacks investigation on the application of listed methods in different domains.Additionally, the survey conducted by (Xu et al., 2024) emphasizes harnessing the capabilities of proprietary LLMs (like those from the GPT family) through knowledge distillation.This technique involves transferring the implicit 'knowledge' from proprietary models into open-source language models.The goal of these methods is to narrow the performance gap between state-of-the-art proprietary and open-source LLMs.By using the advanced features of leading proprietary models such as GPT-4 (OpenAI et al., 2024) as benchmarks, knowledge distillation aims to enhance the performance of open-source LLMs.This method resembles an experienced instructor transferring expertise to a student, with the student models adopting the performance traits of the teacher LLMs.</p>
<p>Despite the existing surveys on making LLMs more accessible, these works presented methods and techniques primarily in a broader domain.Meanwhile, in previous reviews on scientific language models, the authors encouraged finding efficient and low-cost solutions for scientific adaptation and leveraging LLMs for science.Therefore, our review focuses on investigating recent efficient approaches for scientific LLMs and potential research directions.This section discusses the latest developments in the application of Large Language Models (LLMs) within the scientific field.The purpose of this study is to investigate the capabilities of LLMs in scientific research.In this review, we attempt to en-compass a broad range of science-related topics, including biology, biomedicine, mathematics, geoscience, ocean science, and other natural sciences.Figure 1 shows the distribution of efficient methods leveraging LLMs for each scientific domain in our review.</p>
<p>Biology</p>
<p>In the field of biology, there has been a trend towards studying increasingly large language models.Yet, the substantial computational and memory requirements for fine-tuning these models pose significant challenges for many academic laboratories and small biotechnology firms.(Sledzieski et al., 2023) implemented parameter-efficient fine-tuning (PEFT) on ESM2 model (Lin et al., 2023) to predict protein-protein interactions.Employing the PEFT technique LoRA, the model surpassed the performance of fully fine-tuned model while consuming less memory, illustrating that effective deployment of large protein language models is feasible even for groups with constrained computational resources.The study further highlighted that the efficacy of this method could be enhanced by utilizing more informative embeddings produced by LLMs.Research on the PEFT LoRA method adapted for the ESM2 model was also conducted in (Zeng et al., 2023b) focusing on signal peptides (SP) prediction.Other PEFT techniques such as Adapter Tuning and Prompt Tuning were also explored.It was noted that Prompt Tuning underperformed compared to other previous models, likely due to the size of the model.While Adapter Tuning improved performance, it required a considerably larger number of training parameters relative to LoRA.Future enhancements were suggested, including combining PEFT techniques to improve interpretability for identifying SP-related motifs and integrating structure-aware language models to include protein structural data.Another PEFT approach, adaptive LoRA (AdaLoRA), was utilized in the study by (Zhan and Zhang, 2023).This study introduced AdaLoRa with random sampling (AdaLoRA-RS) on OPT-350M to enhance the understanding of genomic language complexities.When compared to other models, DNABERT and Nucleotide Transformer, AdaLoRA+RS demonstrated performance on par with fully fine-tuned models across 13 genomic datasets while using less than 2% of the training parameters.The experimental findings further showed that pre-trained language models such as OPT-125M outperformed the specialized DNA model HR-500M, utilizing only 25% of the parameters.</p>
<p>Biomedical domain</p>
<p>The progress of LLMs has significantly influenced biomedical research.In the last ten years, extensive unlabelled datasets like PubMed, PMC, MIMIC, and ScienceDirect have been made accessible in the biomedical field.Models such as GPT-4 and Med-PaLM 2 have demonstrated outstanding performance in a variety of biomedical NLP tasks.Nonetheless, these models, with their vast number of parameters, are costly in terms of computational resources, necessitate data transmission over the Internet, and are trained on proprietary datasets.</p>
<p>In 2022, (Li et al., 2022) leveraged this unlabelled information to introduce BioKnowPrompt, a prompt-tuning PLM framework tailored for extracting relationships from biomedical texts.Additionally, prompting can be challenging for certain phenomena and may struggle with highly imbalanced training data.Follow up, with the introduction of ChatGPT and GPT-4 (OpenAI et al., 2024), many researches have leveraged its power for data augmentation.(Zhang et al., 2023a) created HuatuoGPT based on LlaMa model, employing both refined data from ChatGPT and data from doctors for health consultations.This model is superior at producing patient-friendly and doctorlike responses and outperformed existing medical open-source LLMs.DoctorGLM presented by (Xiong et al., 2023) also used the data generated by ChatGPT for medical dialogues in Chinese.The training process of DoctorGLM can handle a considerable number of question-answer pairs per hour per GPU, with a relatively low cost per training session.Furthermore, the inference operations of DoctorGLM demand minimal GPU memory, enabling execution on standard consumer hardware, thus making it accessible for numerous research facilities and healthcare centers.They also mentioned that the model can be deployed on even more affordable GPU when applying PEFT method such as LoRA.The superiority of GPT-4 also demonstrated by (Hsueh et al., 2023).The authors succeeded in using prompt engineering for ChatGPT(GPT-4) to generate answers for biomedical questions.Although their method outperformed the fine-tuned BioBERT model, they discussed that there were rooms for improvements such as determining key information before prompting.(Bolton et al., 2024) presented BioMedLM, a GPT-style autoregressive model with 2.7 billion parameters, trained solely on PubMed abstracts and full articles.Their findings emphasize that smaller models can offer transparent, privacy-preserving, costeffective, and eco-friendly solutions for biomedical applications.Another approach using GPT-3.5 for biomedical purposes was presented by (Bao et al., 2023).The researchers employed GPT-3.5 to extract medical knowledge triples from a knowledge graph through a department-focused method based on patient query patterns from real-world consultations, producing 50,000 samples.Additionally, (Liu et al., 2023) addressed the issue of securely managing medical data in the modern digital age, where confidentiality is a major concern.Utilizing advancements in large language models such as ChatGPT and GPT-4, the researchers introduced DeID-GPT, an innovative framework designed to automatically identify and mask personal information in medical texts.Their method not only achieved high accuracy in maintaining text integrity but also set a new standard for the application of LLMs in healthcare settings focused on privacy protection.</p>
<p>While proprietary LLMs are usually huge, untrainable and their architecture are unclear, researchers adapt instruction-tuning technique to open-source smaller LLMs for solving biomedical problems.(Wu et al., 2023) systematically adapted the open-source general LLM, LLaMA, for biomedical tasks by injecting domain-specific data and instruction-tuning tailored to medical contexts.The PCM-LLaMA model, an open-source language model designed for medical purposes, demonstrates superior results on various medical benchmarks, surpassing both ChatGPT and LLaMA-2 while utilizing considerably fewer parameters.Additionally, (Luo et al., 2023b) presented BioMedGPT, a multi-modal generative pretrained model tailored for biomedical applications.The design of BioMedGPT highlights the critical importance of knowledge distillation in bridging complex biological information with natural language, enabling substantial advancements in the discovery of drugs and therapeutic targets.(Peng et al., 2024) conducted comparison on GatorTron using soft-prompting in various configurations.The study revealed that soft prompting surpassed hard prompting, unfrozen Large Language Models (LLMs) display robust few-shot learning abilities and adaptability across different institutions, using frozen LLMs reduces computational costs to be-tween 2.5% and 6% relative to earlier methods that utilized unfrozen LLMs, while still attaining optimal outcomes with large-scale unfrozen LLMs.To enhance performance and generalizability beyond traditional benchmarks, (Zhang et al., 2024b) introduced MedInstruct-52k, a diverse dataset generated with GPT-4 and ChatGPT.Fine-tuning LLaMAseries models on this dataset resulted in AlpaCare, which outperformed previous medical LLMs by up to 38.1% in medical instruction-following tasks and showed consistent improvements in general domain benchmarks based on human evaluations.Parameter-efficient fine-tuning is also an effective approach to reduce the training time and cost when performance domain adaptation.(Han et al., 2023) utilized the LLaMA foundation models with 7 billion and 13 billion parameters, fine-tuning them over five epochs with learning rates specifically adjusted for each model variant.They applied Low-Rank Adaptation (LoRA) to improve efficiency by lowering GPU memory usage and reducing training time.Additionally, the author incorporated 8-bit matrix multiplication to further decrease computational requirements, making it more feasible to deploy these models in medical applications with strict resource limitations.</p>
<p>Clinical domain</p>
<p>The clinical domain has also experienced a transition from traditional pre-trained language models to the effective use of LLMs.(Gema et al., 2024) introduced a two-step PEFT framework based on the LLaMA model, which was evaluated within the clinical domain.This framework integrates a specialized PEFT adapter layer for clinical domain adaptation with another adapter for downstream tasks.It was tested on various datasets for clinical outcome prediction and compared to language models trained specifically for clinical purposes.This research is the first to propose a comprehensive empirical analysis of the interaction between PEFT techniques and domain adaptation in the crucial real-world setting of clinical applications.(Goswami et al., 2024) investigated the utility of prompt engineering and parameterefficient fine-tuning for summarizing hospital discharge summary (HDS) articles.The goal was to ensure these models correctly interpret medical terminology and contexts, generate brief summaries, and extract key themes.The research employed LLaMA-2 as the base model and fine-tuned it using QLoRA (Quantized Low-Rank Adapters) to re-duce memory consumption while maintaining data quality.Chinese patent medicine (CPM), an essential part of traditional Chinese medicine (TCM) utilizing Chinese herbs, was studied by (Liu et al., 2024) with LLMs.The researchers created the first CPM instructions (CPMI) dataset and fine-tuned the ChatGLM-6B base model, resulting in CPMI-ChatGLM.They employed parameter-efficient finetuning with consumer-grade graphics cards and investigated LoRA, P-Tuning v2, along with various data scales and configurations.Comparative experiments with similar-size LLMs demonstrated the leading performance of CPMI-ChatGLM in recommending CPM, highlighting its potential for clinical support and data analysis in TCM research.</p>
<p>Mathematics</p>
<p>Large language models such as GPT-4 have shown remarkable abilities in complex mathematical reasoning.However, open-source models are generally pre-trained on extensive internet data without specific tuning for mathematical tasks.To address this gap, (Luo et al., 2023a) introduced WizardMath, which improves mathematical reasoning in LLaMa-2 using Reinforcement Learning from Evol-Instruct Feedback (RLEIF).Wizard-Math outperformed ChatGPT-3.5,Claude Instant-1, PaLM-2, and Minerva on GSM8k, as well as Text-davinci-002, PaLM-1, and GPT-3 on MATH, demonstrating the effectiveness of RLEIF.Building on this groundwork, (Yue et al., 2023) presented MAmmoTH, a series of open-source LLMs designed specifically for mathematics.MAmmoTH-7B achieved a 33% accuracy rate on MATH, exceeding WizardMath-7B by 23%, highlighting the significance of diverse problem coverage and hybrid rationales in the development of advanced mathematical models.</p>
<p>Additionally, (Gou et al., 2024) presented TORA, integrating natural language reasoning with external computational tools like computation libraries and symbolic solvers to tackle challenging mathematical problems.TORA models significantly outperformed existing open-source models on ten mathematical reasoning datasets, achieving average improvements of 13%-19%.TORA-7B achieved 44.6% accuracy on the competition-level MATH dataset, outperforming WizardMath-70B by 22% absolute, demonstrating the effectiveness of integrating computational tools with language models for mathematical problem-solving.</p>
<p>Geoscience</p>
<p>In geoscience, (Deng et al., 2023) unveiled K2, the pioneering LLM specifically designed for geoscience applications.The researchers created essential resources to advance LLM research in geoscience, such as GeoSignal, the first dataset for geoscience instruction tuning, and GeoBench, the first benchmark for evaluating LLMs in this field.Their study described the adaptation of a pretrained general-domain LLM, namely the LLaMA-7B model, to the geoscience domain by further training it on a 5.5 billion token corpus of geoscience texts and fine-tuning it with supervised data from GeoSignal.Additionally, the authors outlined a protocol for efficiently collecting and constructing domain-specific supervised data, even with limited resources.The experimental results on GeoBench demonstrated the effectiveness of their approach and datasets in enhancing the understanding and application of geoscience knowledge, representing a significant breakthrough in the integration of LLMs into geoscientific research and practice.</p>
<p>Chemistry</p>
<p>In the effort to improve crystal property prediction, recent research has focused on using textual descriptions of crystal structures.Traditional methods primarily use graph neural networks (GNNs) to model these structures (Huang et al., 2024b;Ruff et al., 2023;Yan et al., 2024), but they often struggle with the complex interactions between atoms and molecules.An innovative approach by (Rubungo et al., 2024) involves creating a benchmark dataset named TextEdge, which provides comprehensive text descriptions of crystal structures along with their properties.Furthermore, the authors propose LLM-Prop, a novel method employing large language models (LLMs) to predict the physical and electronic properties of crystals based on their textual descriptions.Remarkably, it outperforms a domain-specific fine-tuned BERT model, MatBERT, despite having significantly fewer parameters.</p>
<p>Ocean Science</p>
<p>Ocean science, essential for comprehending the vast reservoirs of life and biodiversity that cover over 70% of our planet, has not yet fully reaped the benefits of advancements in large language models (LLMs).Although LLMs have been success-ful in various domains, they often fall short in addressing the specialized requirements of oceanographers due to the complexity and richness of ocean data.To bridge this gap, (Bi et al., 2024) introduced OCEANGPT, the first LLM specifically designed for ocean science.Extensive experiments revealed that OCEANGPT not only exhibited a high level of expertise in ocean science but also demonstrated initial capabilities in embodied intelligence for ocean technology.Additionally, (Zheng et al., 2023) introduced MarineGPT, the first visionlanguage model specifically crafted for the marine domain.MarineGPT, developed using the Marine-5M dataset containing over 5 million marine imagetext pairs, aimed to make ocean knowledge more accessible and enhance marine vision and language alignment, addressing the shortcomings of existing general-purpose MLLMs in understanding and responding to domain-specific intents.</p>
<p>Multi-scientific domains</p>
<p>In their respective studies, (Xie et al., 2023) introduced DARWIN, a series of tailored LLMs optimized specifically for scientific disciplines such as material science, chemistry, and physics.Built upon the foundational LLaMA-7B model, DAR-WIN achieved significant advances in automating the generation of scientific text instruction, thus improving its performance in various scientific tasks and reducing the dependency on closed-source LLMs.Similarly, (Zhang et al., 2024a) presented SciGLM, a suite of scientific language models designed for college-level scientific reasoning.Using a self-reflective instruction annotation framework, SciGLM addressed data scarcity challenges in the science domain by improving both base models like ChatGLM3-6B-Base by 4.87% and largerscale models by 2.67%.This approach enhances the model's ability to conduct diverse scientific discovery tasks while preserving its language understanding capabilities.</p>
<p>Challenges and future directions</p>
<p>Current studies on the application of LLMs in science have made significant progress.We summarize the existing methods and scientific LLMs in Table 1.Most of these studies have initially harnessed the power of LLMs to address problems in scientific fields such as biology and biomedicine.However, many issues remain unresolved.This section will present some research gaps with potential for further exploration.</p>
<p>Data Collection</p>
<p>Challenges The lack of labeled data is a common issue faced by researchers when training language models in various scientific fields.Despite the abundance of unlabeled scientific data, it is not utilized efficiently to train language models.(Ho et al., 2024) summarized that among 117 language models for scientific fields, most previous work focused on the biomedical domain, with more than 87% pre-trained language models in this area.The author also noted that these language models typically have fewer than 1 billion parameters (e.g., BERT-based models) and do not leverage opensource LLMs.This leads to the issue of underutilization of unlabeled data in various scientific domains.The process of gathering high-quality labeled data for training models is time-consuming and labor-intensive.</p>
<p>Potential directions Current approaches like active learning for small language models (SLMs) and in-context learning for large language models (LLMs) have partially alleviated the shortage of labeled data, yet they still depend heavily on human involvement.(Xiao et al., 2023) tackled this problem by proposing FreeAL, a collaborative learning framework where an LLM serves as an active annotator and an SLM filters high-quality in-context samples for label refinement.Comprehensive experiments on eight benchmark datasets demonstrated that FreeAL significantly enhanced zero-shot performance for both SLMs and LLMs without human supervision.(Zhang et al., 2023c) introduced LLMaAA, which uses LLMs as annotators in an active learning loop to efficiently select data for annotation, demonstrating superior performance in named entity recognition and relation extraction tasks with fewer annotated examples.(Huang et al., 2024a) tackled the challenge of high quality annotations under limited budgets with SANT, a selective annotation framework utilizing error-aware triage and bi-weighting mechanisms, setting a new benchmark for triage-based annotation studies.</p>
<p>Data Selection</p>
<p>Challenges Determining the optimal data volume crucial for maximizing the effectiveness of Large Language Models (LLMs) remains a persistent challenge, necessitating further research to</p>
<p>Methods</p>
<p>Models</p>
<p>Efficient Fine-tuning ESM2-LoRA (Sledzieski et al., 2023), PEFT-SP (Zeng et al., 2023b), AdaLoRA+RS (Zhan and Zhang, 2023), BioMedGPT (Luo et al., 2023b), MedAlpaca (Han et al., 2023), Clinical LLaMA-LoRA (Gema et al., 2024), LLaMa-QLoRA (Goswami et al., 2024), CPMI-ChatGLM (Liu et al., 2024)</p>
<p>Instruction Tuning</p>
<p>BioKnowPrompt (Li et al., 2022), NCU-IISR (Hsueh et al., 2023), Alpacare (Zhang et al., 2024b), GatorTron (Peng et al., 2024), K2 (Deng et al., 2023), WizardMath (Luo et al., 2023a), MAmmoTH (Yue et al., 2023), TORA (Gou et al., 2024), OCEANGPT (Bi et al., 2024), MarineGPT (Zheng et al., 2023), SciGLM (Zhang et al., 2024a) Knowledge distillation Black box HuatouGPT (Zhang et al., 2023a), DoctorGLM (Xiong et al., 2023), DISC-MedLLM (Bao et al., 2023), DeID-GPT (Liu et al., 2023) White box BioMedLM (Bolton et al., 2024), PCM-LLaMA (Wu et al., 2023), LLM-Prop (Rubungo et al., 2024), DARWIN (Xie et al., 2023) Table 1: Summary of previous work on efficient LLMs for science.</p>
<p>establish clear guidelines.Additionally, developing robust methodologies to filter out low-quality data continues to be an ongoing concern in leveraging LLMs effectively.</p>
<p>Potential Directions</p>
<p>In general domain, (Zhou et al., 2023) proposed that a minimum of 1000 well-curated, high-quality data samples could be sufficient to align LLMs, as pre-training already provides essential knowledge.(Chen et al., 2024b) introduced a new data selection method using a robust LLM such as ChatGPT to independently filter out low-quality data.They developed AlpaGasus, a model refined with just 9,000 high-quality samples from the initial dataset.More recently, (Li et al., 2024) presented Superfiltering, which used smaller models such as GPT-2 to extract a high-quality subset from a dataset.Despite these advancements, the challenges of selecting optimal data for refining LLMs and determining the necessary data volume persist because of the abundance of unlabeled scientific data.</p>
<p>Utilizing multiple LLMs</p>
<p>Challenges The majority of current models originate from a single LLM, yet it is commonly recognized that models trained with diverse data sources possess distinct advantages.Consequently, the question arises: Can knowledge from multiple LLMs be integrated into a single smaller model?</p>
<p>Potential directions To develop a "BabyLM," (Timiryasov and Tastet, 2023) trained a combination of GPT-2 and small LLaMA models on the 10M-word BabyLM dataset, subsequently distilling this ensemble into a compact, 58M-parameter LLaMA model.The distilled model surpassed both its original models and a comparable model trained without distillation, indicating that distillation can preserve and even enhance the performance of teacher models, especially on small datasets.(Wan et al., 2024a)</p>
<p>Multimodality</p>
<p>Challenges Within the scientific field, there is increasing enthusiasm for multi-modal models (Ho et al., 2024), which are created by further training mono-modal or multi-modal models from general domains, capitalizing on their robust performance.Nonetheless, numerous challenges remain.The scientific domain often suffers from a scarcity of data compared to general domains, complicating the process of effectively training or fine-tuning multi-modal language models.Integrating this multi-modal information into scientific language models is essential for research progress.</p>
<p>Potential directions Several research efforts are dedicated to creating adapters that transform nonlanguage data to be interpreted within the same embedding space as language (Dai et al., 2023;Zhu et al., 2023).These frameworks are designed to manage non-language information while maintaining the strong problem-solving abilities of LLMs.</p>
<p>Although proprietary LLMs such as GPT-4 can handle various scientific data types, utilizing these models demands substantial resources.Consequently, it is advisable to develop efficient strategies to enhance the accessibility of LLMs and incorporate multimodality in scientific domains, thereby unlocking the full potential of multi-modal models in the scientific field.</p>
<p>Further reduce the cost</p>
<p>Challenges Despite the impressive capabilities of modern LLMs, their substantial resource demands highlight the critical need for effective solutions to address these challenges.Based on Table 1, in previous work within the scientific domain, common ways to reduce costs have included Instruction Tuning and Efficient Fine-Tuning.Continued research and development in other methodologies are crucial to making LLMs more accessible and sustainable.</p>
<p>Potential directions In other fields, numerous efficient methods have been explored, including Quantization (Frantar et al., 2023;Kim et al., 2023;Tao et al., 2022), Parameter Pruning (Ma et al., 2023;Zhang et al., 2023b), and Memory Efficient Fine-Tuning (Dettmers et al., 2023;Malladi et al., 2023).The challenge of further reducing the cost of LLMs remains unresolved.For example, Memory Efficient Fine-Tuning approaches, such as QLoRA (Goswami et al., 2024), which enhances memory efficiency during fine-tuning, also present viable solutions.</p>
<p>Conclusion</p>
<p>The swift progress of large language models (LLMs) has greatly improved our capability to tackle intricate tasks that demand profound linguistic and structural comprehension.The growth of scientific data has enabled effective learning of scientific knowledge through LLMs.However, despite their impressive performance in tasks like reasoning and problem-solving, these models remain resource-intensive and often inaccessible to smaller organizations and low-resource languages.Our review highlighted various cost-effective techniques for utilizing LLMs in scientific domains.We address the challenges in fully harness the potential of LLMs for science and ensure their broader accessibility and applicability in scientific research.</p>
<p>Limitations</p>
<p>Our work is based on results and suggestions of as many papers as possible we can find.We also mostly emphasize text-based scientific information, setting aside other forms such as images, videos, audio, and structured knowledge like knowledge graphs (KGs) and databases for future consideration.Our review primarily highlights the most recent advancements in the last three years, specif-ically from 2023 and 2024.However, our review may hold a potential of missed out some the most recent studies.We leave this as future improvements.Moreover, due to space limitations, we provide only concise summaries of the reviewed methods.</p>
<p>Figure 1 :
1
Figure 1: Distribution of efficient LLMs for Science.</p>
<p>https://arxiv.org/stats/monthly_submissions
https://www.nlm.nih.gov/bsd/medline_pubmed_ production_stats.html</p>
<p>Disc-medllm: Bridging general large language models and real-world medical consultation. Zhijie Bao, Wei Chen, Shengze Xiao, Kuang Ren, Jiaao Wu, Cheng Zhong, Jiajie Peng, Xuanjing Huang, Zhongyu Wei, arXiv:2308.143462023Preprint</p>
<p>Oceangpt: A large language model for ocean science tasks. Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, Huajun Chen, arXiv:2310.020312024Preprint</p>
<p>Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, Christopher D Manning, arXiv:2403.18421Biomedlm: A 2.7b parameter language model trained on biomedical text. 2024Preprint</p>
<p>Magdi: Structured distillation of multi-agent interaction graphs improves reasoning in smaller language models. Justin Chih-Yao Chen, Swarnadeep Saha, Elias Stengel-Eskin, Mohit Bansal, arXiv:2402.016202024aPreprint</p>
<p>Alpagasus: Training a better alpaca with fewer data. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, Hongxia Jin, arXiv:2307.087012024bPreprint</p>
<p>Lifelong language pretraining with distributionspecialized experts. Wuyang Chen, Yanqi Zhou, Nan Du, Yanping Huang, James Laudon, Zhifeng Chen, Claire Cu, arXiv:2305.122812023Preprint</p>
<p>Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, arXiv:2305.065002023Preprint</p>
<p>K2: A foundation language model for geoscience knowledge understanding and utilization. Cheng Deng, Tianhang Zhang, Zhongmou He, Yi Xu, Qiyuan Chen, Yuanyuan Shi, Luoyi Fu, Weinan Zhang, Xinbing Wang, Chenghu Zhou, Zhouhan Lin, Junxian He, arXiv:2306.050642023Preprint</p>
<p>Optimal brain compression: A framework for accurate post-training quantization and pruning. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, arXiv:2208.11580Thirty-seventh Conference on Neural Information Processing Systems. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. 2023. 2023PreprintQLoRA: Efficient finetuning of quantized LLMs</p>
<p>Parameterefficient fine-tuning of llama for the clinical domain. Aryo Pradipta, Gema , Pasquale Minervini, Luke Daines, Tom Hope, Beatrice Alex, arXiv:2307.030422024Preprint</p>
<p>Parameterefficient fine-tuning large language model approach for hospital discharge paper summarization. Joyeeta Goswami, Kaushal Kumar Prajapati, Ashim Saha, Apu Kumar Saha, 10.1016/j.asoc.2024.111531Applied Soft Computing. 1571115312024</p>
<p>Tora: A tool-integrated reasoning agent for mathematical problem solving. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, arXiv:2309.174522024Preprint</p>
<p>Medalpaca -an open-source collection of medical conversational ai models and training data. Tianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Lser, Daniel Truhn, Keno K Bressem, arXiv:2304.082472023Preprint</p>
<p>A survey of pre-trained language models for processing scientific text. Xanh Ho, Anh Khoa, Duong Nguyen, An Tuan Dao, Junfeng Jiang, Yuki Chida, Kaito Sugimoto, Quoc Huy, Florian To, Akiko Boudin, Aizawa, arXiv:2401.178242024Preprint</p>
<p>Ncu-iisr: Prompt engineering on gpt-4 to stove biological problems in bioasq 11b phase b. Chun Yu Hsueh, Yu Zhang, Yu Wei Lu, Jen Chieh Han, Wilailack Meesawad, Richard Tzong, Han Tsai, date: 18-09-2023 Through 21-09-2023Publisher Copyright:  2023 Copyright for this paper by its authors.; 24th Working Notes of the Conference and Labs of the Evaluation Forum. 2023. 20233497CEUR Workshop Proceedings</p>
<p>Selective annotation via data allocation: These data should be triaged to experts for annotation rather than the model. Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Ido Dagan, arXiv:2405.120812024aPreprint</p>
<p>Ada-gnn: Atom-distance-angle graph neural network for crystal material property prediction. Jiao Huang, Qianli Xing, Jinglong Ji, Bo Yang, arXiv:2401.117682024bPreprint</p>
<p>Ammu: A survey of transformer-based biomedical pretrained language models. Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, Sivanesan Sangeetha, 10.1016/j.jbi.2021.103982Journal of Biomedical Informatics. 1261039822022</p>
<p>Finequant: Unlocking efficiency with fine-grained weight-only quantization for llms. Jin Young, Rawn Kim, Raffy Henry, Hany Fahim, Hassan Awadalla, arXiv:2308.097232023Preprint</p>
<p>Superfiltering: Weak-to-strong data filtering for fast instruction-tuning. Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, Tianyi Zhou, arXiv:2402.005302024Preprint</p>
<p>Bioknowprompt: Incorporating imprecise knowledge into prompt-tuning verbalizer with biomedical text for relation extraction. Qing Li, Yichen Wang, Tao You, Yantao Lu, 10.1016/j.ins.2022.10.063Information Sciences. 6172022</p>
<p>Evolutionary-scale prediction of atomic-level protein structure with a language model. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos, Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives, 10.1126/science.ade2574Science. 37966372023</p>
<p>Cpmi-chatglm: parameter-efficient finetuning chatglm with chinese patent medicine instructions. Can Liu, Kaijie Sun, Qingqing Zhou, Yuchen Duan, Jianhua Shu, Hongxing Kan, Zongyun Gu, Jili Hu, 10.1038/s41598-024-56874-wScientific Reports. 142024</p>
<p>Zhengliang Liu, Yue Huang, Xiaowei Yu, Lu Zhang, Zihao Wu, Chao Cao, Haixing Dai, Lin Zhao, Yiwei Li, Peng Shu, Fang Zeng, Lichao Sun, Wei Liu, Dinggang Shen, Quanzheng Li, Tianming Liu, arXiv:2303.11032Dajiang Zhu, and Xiang Li. 2023. Deid-gpt: Zero-shot medical text de-identification by gpt-4. Preprint</p>
<p>Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, arXiv:2308.095832023aPreprint</p>
<p>Biomedgpt: Open multimodal generative pretrained transformer for biomedicine. Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, Zaiqing Nie, arXiv:2308.094422023bPreprint</p>
<p>LLM-pruner: On the structural pruning of large language models. Xinyin Ma, Gongfan Fang, Xinchao Wang, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Fine-tuning language models with just forward passes. Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi Chen, Sanjeev Arora, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Red Anadkat, Igor Avila, Suchir Babuschkin, Valerie Balaji, Paul Balcom, Haiming Baltescu, Mohammad Bao, Jeff Bavarian, Irwan Belgum, Jake Bello, Gabriel Berdine, Bernadett-Shapiro, Petroski Lipe, Natalie Such, Ilya Summers, Jie Sutskever, Nikolas Tang, Madeleine B Tezak, Phil Thompson, Amin Tillet, Elizabeth Tootoonchian, Preston Tseng, Nick Tuggle, Jerry Turley, Juan Tworek, Felipe Cern, Andrea Uribe, Vallone, Wei, Akila Cj Weinmann, Peter Welihinda, Jiayi Welinder, Lilian Weng, Matt Weng, Dave Wiethoff, Clemens Willner, Samuel Winter, Hannah Wolrich, Lauren Wong, Sherwin Workman, Jeff Wu, Michael Wu, Kai Wu, Tao Xiao, Sarah Xu, Kevin Yoo, Qiming Yu, Wojciech Yuan, Rowan Zaremba, Chong Zellers, Marvin Zhang, Shengjia Zhang, Tianhao Zhao, Juntang Zheng, William Zhuang, Zhuk, arXiv:2303.08774Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, JasonPreprintand Barret Zoph. 2024. Gpt-4 technical report</p>
<p>Model tuning or prompt tuning? a study of large language models for clinical concept and relation extraction. Cheng Peng, Xi Yang, Kaleb E Smith, Zehao Yu, Aokun Chen, Jiang Bian, Yonghui Wu, 10.1016/j.jbi.2024.104630Journal of Biomedical Informatics. 1531046302024</p>
<p>LLM-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions. Andre Niyongabo Rubungo, Craig Arnold, Barry Rand, Adji Bousso, Dieng , 2024</p>
<p>Connectivity optimized nested graph networks for crystal structures. Robin Ruff, Patrick Reiser, arXiv:2302.14102Jan Sthmer, and Pascal Friederich. 2023Preprint</p>
<p>Democratizing protein language models with parameter-efficient fine-tuning. Samuel Sledzieski, Meghana Kshirsagar, Minkyung Baek, Bonnie Berger, Rahul Dodhia, Juan Lavista, Ferres , 10.1101/2023.11.09.566187bioRxiv. 2023</p>
<p>Compression of generative pre-trained language models via quantization. Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong, 10.18653/v1/2022.acl-long.331Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, arXiv:2211.09085Galactica: A large language model for science. 2022Preprint</p>
<p>Baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty. Inar Timiryasov, Jean-Loup Tastet, 10.18653/v1/2023.conll-babylm.24Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. the BabyLM Challenge at the 27th Conference on Computational Natural Language LearningSingaporeAssociation for Computational Linguistics2023</p>
<p>Knowledge fusion of large language models. Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, Shuming Shi ; Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, Mi Zhang, arXiv:2312.03863The Twelfth International Conference on Learning Representations. 2024a. 2024bPreprintEfficient large language models: A survey</p>
<p>Pretrained language models in biomedical domain: A systematic survey. Benyou Wang, Qianqian Xie, Jiahuan Pei, Zhihong Chen, Prayag Tiwari, Zhao Li, Jie Fu, arXiv:2110.050062023Preprint</p>
<p>Pmc-llama: Towards building open-source language models for medicine. Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie, arXiv:2304.144542023Preprint</p>
<p>FreeAL: Towards human-free active learning in the era of large language models. Ruixuan Xiao, Yiwen Dong, Junbo Zhao, Runze Wu, Minmin Lin, Gang Chen, Haobo Wang, 10.18653/v1/2023.emnlp-main.896Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Imran Razzak, and Bram Hoex. 2023. Darwin series: Domain specific large language models for natural science. Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit, Clara Grazian, Wenjie Zhang, arXiv:2308.13565Preprint</p>
<p>Doctorglm: Fine-tuning your chinese doctor is not a herculean task. Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Linlin Huang, Qian Wang, Dinggang Shen, arXiv:2304.010972023Preprint</p>
<p>A survey on knowledge distillation of large language models. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou, arXiv:2402.131162024Preprint</p>
<p>Complete and efficient graph transformers for crystal material property prediction. Keqiang Yan, Cong Fu, Xiaofeng Qian, Xiaoning Qian, Shuiwang Ji, arXiv:2403.118572024Preprint</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.056532023Preprint</p>
<p>Continual learning with dirichlet generative-based rehearsal. Min Zeng, Wei Xue, Qifeng Liu, Yike Guo, arXiv:2309.069172023aPreprint</p>
<p>Peftsp: Parameter-efficient fine-tuning on large protein language models improves signal peptide prediction. Shuai Zeng, Duolin Wang, Dong Xu, 10.1101/2023.11.04.565642bioRxiv. 2023b</p>
<p>Parameterefficient fine-tune on open pre-trained transformers for genomic sequence. Huixin Zhan, Zijun Frank Zhang, Generative AI and Biology. 2023. 2023GenBio) Workshop</p>
<p>Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, Jie Tang, arXiv:2401.079502024aPreprint</p>
<p>HuatuoGPT, towards taming language model to be a doctor. Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Guiming Chen, Jianquan Li, Xiangbo Wu, Zhang Zhiyi, Qingying Xiao, Xiang Wan, Benyou Wang, Haizhou Li, 10.18653/v1/2023.findings-emnlp.725Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023a</p>
<p>Loraprune: Pruning meets lowrank parameter-efficient fine-tuning. Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang, arXiv:2305.184032023bPreprint</p>
<p>Ruoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, Lei Zou, arXiv:2310.19596Llmaaa: Making large language models as active annotators. 2023cPreprint</p>
<p>Alpacare:instruction-tuned large language models for medical application. Xinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang Chen, Zekun Li, Linda Ruth Petzold, arXiv:2310.145582024bPreprint</p>
<p>CITB: A benchmark for continual instruction tuning. Zihan Zhang, Meng Fang, Ling Chen, Mohammad-Reza Namazi-Rad, 10.18653/v1/2023.findings-emnlp.633Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023d</p>
<p>Marinegpt: Unlocking secrets of ocean to the public. Ziqiang Zheng, Jipeng Zhang, Tuan-Anh Vu, Shizhe Diao, Yue Him, Wong Tim, Sai-Kit Yeung, arXiv:2310.135962023Preprint</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy, arXiv:2305.11206Less is more for alignment. Lima2023Preprint</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.105922023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
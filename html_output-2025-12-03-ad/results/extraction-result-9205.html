<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9205 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9205</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9205</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-3718212</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1803.02395v1.pdf" target="_blank">Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM</a></p>
                <p><strong>Paper Abstract:</strong> We propose a simple mathematical definition and new neural architecture for finding anomalies within discrete sequence datasets. Our model comprises of a modified LSTM autoencoder and an array of One-Class SVMs. The LSTM takes in elements from a sequence and creates context vectors that are used to predict the probability distribution of the following element. These context vectors are then used to train an array of One-Class SVMs. These SVMs are used to determine an outlier boundary in context space.We show that our method is consistently more stable and also outperforms standard LSTM and sliding window anomaly detection systems on two generated datasets.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9205.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9205.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZBLSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero Boundary LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A custom architecture that combines an LSTM encoder trained to predict next-element distributions with an array of per-symbol One-Class SVMs operating on LSTM context vectors to detect discrete-sequence anomalies defined as next-element probability zero.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Zero Boundary LSTM (custom)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LSTM encoder + MLP decoder with an array of One-Class SVMs (per-symbol density estimators)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>discrete sequences of bytes/characters (next-element prediction / context vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>synthetic datasets: generated IPv4 address strings and generated JSON strings</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>structural and syntactic anomalies (outlier sequences, invalid tokens, malformed structure, length anomalies, misplaced punctuation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train an LSTM encoder E(x; θ_E) to predict the next-element distribution via an MLP decoder D; extract context vectors E(x)_i (encoding up to position i) and collect per-symbol context sets Y_σ; train a One-Class SVM O_σ on each Y_σ to estimate a decision boundary such that if O_{x_i}(E(x)_{i-1}) < t_σ for any position i, the sequence is flagged anomalous. Thresholds t_σ are set per-symbol (slightly below zero) to control Type I error.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Standard LSTM next-character probability thresholding; n-gram / sliding-window anomaly detector (variable window sizes), naive sliding-window frequency / n-gram model</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>True positives, true negatives, accuracy (per-experiment reporting of TP/TN counts), training stability across epochs (consistency of decision boundary), and qualitative comparisons; thresholds and cutoffs reported for experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Trained on 10,000 synthetic IPv4 strings and on generated JSON strings. Paper reports that Zero Boundary LSTM outperforms the standard LSTM and the n-gram sliding-window on the IPv4 experiment across anomaly categories, and nearly matches the normal LSTM on the JSON experiment (best-model comparisons after hyperparameter search). Exact numeric scores are shown in figures in the paper but not enumerated in the main text; cutoff and training details: OCSVM ν=0.001, cutoff t_σ set to lowest value from training/validation, standard-LSTM cutoff set to 1/8103 in IPv4 experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Overall better than or matching baselines: outperforms standard LSTM and sliding-window on IPv4 anomalies; nearly matches standard LSTM on JSONs; main advantage is much greater stability of decision boundary across training epochs compared with standard LSTM next-character probability thresholding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Highly sensitive to presence of anomalies in training set (requires nonzero ν as a workaround); using ν>0 increases false positives for context vectors near the decision boundary; OCSVM training scales poorly (O(n^3)), making the approach difficult to scale to very large datasets; threshold selection per-symbol is required and may be dataset-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Flipping the conditional to train per-symbol density estimators on fixed-size context vectors allows use of standard density/boundary estimators for detecting zero-probability next elements; combining LSTM-predicted context vectors with an array of OCSVMs produces a stable decision boundary that avoids the high variance in predicted probabilities for rare tokens that plagues standard LSTM next-token probability thresholding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9205.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9205.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standard LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard LSTM next-character prediction (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conventional LSTM trained to predict the probability distribution of the next element in a discrete sequence; anomalies are flagged when predicted next-element probability falls below a threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Standard LSTM next-character predictor</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LSTM (multi-layer recurrent network) with MLP output</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>discrete sequences of bytes/characters</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>synthetic IPv4 and JSON string datasets (same as Zero Boundary LSTM experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>outlier or unlikely next-token events, syntactic/structural errors detectable via low next-token probability</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train LSTM with cross-entropy next-token loss; at detection time compute P(next token | prefix) and flag sequence as anomalous if probability < cutoff threshold (cutoff tuned/selected).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Used as primary baseline against Zero Boundary LSTM; also compared to n-gram sliding-window model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Same metrics as Zero Boundary LSTM: true positives, true negatives, accuracy, and stability across epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Under same architectures and training budget, the standard LSTM shows higher variance in predicted probabilities for infrequent values, producing less stable decision boundaries; on IPv4 it was outperformed by Zero Boundary LSTM; on JSON the best-tuned standard LSTM matched or slightly exceeded ZBLSTM in some cases, but required extensive hyperparameter search.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>This is the baseline; compared to ZBLSTM it had worse stability and higher variance in deciding anomaly thresholds, leading to more false positives or missed anomalies depending on threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High prediction variance for infrequent tokens when trained with SGD and cross-entropy, making threshold selection difficult and decision boundary unstable for alerting systems.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Standard next-token probability thresholding suffers from instability due to SGD variance for rare tokens; this motivates learning a context-space boundary (ZBLSTM) rather than relying solely on probability magnitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9205.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9205.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>n-gram (sliding window)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Naive sliding-window n-gram anomaly detector</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A frequency-based sliding-window model that inspects fixed-length subsequences (n-grams) and flags anomalies when observed n-gram frequencies are inconsistent with training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>n-gram sliding-window model</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>n-gram / sliding-window frequency model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>discrete sequences (fixed-size subsequences/windows)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>synthetic IPv4 and JSON strings (experiments used window sizes tuned per dataset: IPv4 used window=4, JSON used window=3)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>local anomalies within short windows (local token errors, malformed local patterns)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Count/estimate frequency of n-grams in training set; at test time slide window across sequence and flag sequences containing low-frequency or unseen n-grams as anomalous.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Serves as a simple baseline for local anomaly detection compared with LSTM-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>True positives/negatives and accuracy per anomaly category.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On IPv4 digit-anomaly category the n-gram model performed best because it can effectively memorize acceptable numeric groupings within its fixed window; however n-gram models with small windows fail to detect long-range anomalies (e.g., too many digit groupings) that LSTM-based methods detect.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Stronger on strictly local anomaly types due to memorization; weaker on long-range or structural anomalies due to limited window size.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cannot capture long-term dependencies without exponentially increasing window sizes or data; higher false positives or false negatives when window size is mismatched to anomaly types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9205.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9205.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OCSVM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-Class Support Vector Machine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A density-boundary estimator that constructs a decision boundary around training vectors in fixed-dimensional space to detect outliers; here used per-symbol on LSTM context vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Support vector method for novelty detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>One-Class SVM (per-symbol)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>kernelized One-Class SVM with Gaussian (RBF) kernel</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>fixed-size continuous context vectors (embeddings) produced by LSTM encoder</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>context-vector space derived from synthetic sequence datasets (IPv4, JSON)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>outliers in context vector space corresponding to impossible/zero-probability next-symbol events</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Collect context vectors Y_σ = {E(x)_i : x_i = σ} for each alphabet symbol σ, then train an OCSVM O_σ on Y_σ with parameter ν (here ν=0.001) and RBF kernel; at detection evaluate the OCSVM decision function for the context vector preceding each token and compare to a threshold t_σ to flag anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Used in combination with LSTM context vectors as the primary density/boundary estimator; contrasted with direct next-token probability thresholding and n-gram frequency models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Decision values from OCSVMs used with per-symbol thresholds; global metrics (TP/TN, accuracy) reported for overall anomaly detection when combined with LSTM encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>When combined with LSTM context vectors, per-symbol OCSVMs produce stable decision boundaries and lead to improved anomaly detection performance compared with standard LSTM probability-thresholding on IPv4; training cost and scalability are limiting factors (O(n^3) training complexity reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Using OCSVMs on learned context vectors reduces variance issues seen with direct LSTM probability thresholds and improves stability; however, OCSVMs are computationally expensive compared to more scalable density estimators.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cubic training time O(n^3) makes OCSVMs impractical on very large datasets; intolerance to anomalies in the training set (requires ν>0 workaround) which increases false positives; per-symbol decision thresholds needed and some context vectors near the boundary cause increased false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>One class support vector machines for detecting anomalous windows registry accesses <em>(Rating: 2)</em></li>
                <li>Support vector method for novelty detection <em>(Rating: 2)</em></li>
                <li>Anomaly detection: A survey <em>(Rating: 2)</em></li>
                <li>Deep structured energy based models for anomaly detection <em>(Rating: 1)</em></li>
                <li>Variational Autoencoder based Anomaly Detection using Reconstruction Probability <em>(Rating: 1)</em></li>
                <li>Anomaly detection in aircraft data using Recurrent Neural Networks (RNN) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9205",
    "paper_id": "paper-3718212",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "ZBLSTM",
            "name_full": "Zero Boundary LSTM",
            "brief_description": "A custom architecture that combines an LSTM encoder trained to predict next-element distributions with an array of per-symbol One-Class SVMs operating on LSTM context vectors to detect discrete-sequence anomalies defined as next-element probability zero.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Zero Boundary LSTM (custom)",
            "model_type": "LSTM encoder + MLP decoder with an array of One-Class SVMs (per-symbol density estimators)",
            "model_size": null,
            "data_type": "discrete sequences of bytes/characters (next-element prediction / context vectors)",
            "data_domain": "synthetic datasets: generated IPv4 address strings and generated JSON strings",
            "anomaly_type": "structural and syntactic anomalies (outlier sequences, invalid tokens, malformed structure, length anomalies, misplaced punctuation)",
            "method_description": "Train an LSTM encoder E(x; θ_E) to predict the next-element distribution via an MLP decoder D; extract context vectors E(x)_i (encoding up to position i) and collect per-symbol context sets Y_σ; train a One-Class SVM O_σ on each Y_σ to estimate a decision boundary such that if O_{x_i}(E(x)_{i-1}) &lt; t_σ for any position i, the sequence is flagged anomalous. Thresholds t_σ are set per-symbol (slightly below zero) to control Type I error.",
            "baseline_methods": "Standard LSTM next-character probability thresholding; n-gram / sliding-window anomaly detector (variable window sizes), naive sliding-window frequency / n-gram model",
            "performance_metrics": "True positives, true negatives, accuracy (per-experiment reporting of TP/TN counts), training stability across epochs (consistency of decision boundary), and qualitative comparisons; thresholds and cutoffs reported for experiments",
            "performance_results": "Trained on 10,000 synthetic IPv4 strings and on generated JSON strings. Paper reports that Zero Boundary LSTM outperforms the standard LSTM and the n-gram sliding-window on the IPv4 experiment across anomaly categories, and nearly matches the normal LSTM on the JSON experiment (best-model comparisons after hyperparameter search). Exact numeric scores are shown in figures in the paper but not enumerated in the main text; cutoff and training details: OCSVM ν=0.001, cutoff t_σ set to lowest value from training/validation, standard-LSTM cutoff set to 1/8103 in IPv4 experiment.",
            "comparison_to_baseline": "Overall better than or matching baselines: outperforms standard LSTM and sliding-window on IPv4 anomalies; nearly matches standard LSTM on JSONs; main advantage is much greater stability of decision boundary across training epochs compared with standard LSTM next-character probability thresholding.",
            "limitations_or_failure_cases": "Highly sensitive to presence of anomalies in training set (requires nonzero ν as a workaround); using ν&gt;0 increases false positives for context vectors near the decision boundary; OCSVM training scales poorly (O(n^3)), making the approach difficult to scale to very large datasets; threshold selection per-symbol is required and may be dataset-specific.",
            "unique_insights": "Flipping the conditional to train per-symbol density estimators on fixed-size context vectors allows use of standard density/boundary estimators for detecting zero-probability next elements; combining LSTM-predicted context vectors with an array of OCSVMs produces a stable decision boundary that avoids the high variance in predicted probabilities for rare tokens that plagues standard LSTM next-token probability thresholding.",
            "uuid": "e9205.0",
            "source_info": {
                "paper_title": "Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "Standard LSTM",
            "name_full": "Standard LSTM next-character prediction (baseline)",
            "brief_description": "A conventional LSTM trained to predict the probability distribution of the next element in a discrete sequence; anomalies are flagged when predicted next-element probability falls below a threshold.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Standard LSTM next-character predictor",
            "model_type": "LSTM (multi-layer recurrent network) with MLP output",
            "model_size": null,
            "data_type": "discrete sequences of bytes/characters",
            "data_domain": "synthetic IPv4 and JSON string datasets (same as Zero Boundary LSTM experiments)",
            "anomaly_type": "outlier or unlikely next-token events, syntactic/structural errors detectable via low next-token probability",
            "method_description": "Train LSTM with cross-entropy next-token loss; at detection time compute P(next token | prefix) and flag sequence as anomalous if probability &lt; cutoff threshold (cutoff tuned/selected).",
            "baseline_methods": "Used as primary baseline against Zero Boundary LSTM; also compared to n-gram sliding-window model.",
            "performance_metrics": "Same metrics as Zero Boundary LSTM: true positives, true negatives, accuracy, and stability across epochs.",
            "performance_results": "Under same architectures and training budget, the standard LSTM shows higher variance in predicted probabilities for infrequent values, producing less stable decision boundaries; on IPv4 it was outperformed by Zero Boundary LSTM; on JSON the best-tuned standard LSTM matched or slightly exceeded ZBLSTM in some cases, but required extensive hyperparameter search.",
            "comparison_to_baseline": "This is the baseline; compared to ZBLSTM it had worse stability and higher variance in deciding anomaly thresholds, leading to more false positives or missed anomalies depending on threshold.",
            "limitations_or_failure_cases": "High prediction variance for infrequent tokens when trained with SGD and cross-entropy, making threshold selection difficult and decision boundary unstable for alerting systems.",
            "unique_insights": "Standard next-token probability thresholding suffers from instability due to SGD variance for rare tokens; this motivates learning a context-space boundary (ZBLSTM) rather than relying solely on probability magnitudes.",
            "uuid": "e9205.1",
            "source_info": {
                "paper_title": "Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "n-gram (sliding window)",
            "name_full": "Naive sliding-window n-gram anomaly detector",
            "brief_description": "A frequency-based sliding-window model that inspects fixed-length subsequences (n-grams) and flags anomalies when observed n-gram frequencies are inconsistent with training data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "n-gram sliding-window model",
            "model_type": "n-gram / sliding-window frequency model",
            "model_size": null,
            "data_type": "discrete sequences (fixed-size subsequences/windows)",
            "data_domain": "synthetic IPv4 and JSON strings (experiments used window sizes tuned per dataset: IPv4 used window=4, JSON used window=3)",
            "anomaly_type": "local anomalies within short windows (local token errors, malformed local patterns)",
            "method_description": "Count/estimate frequency of n-grams in training set; at test time slide window across sequence and flag sequences containing low-frequency or unseen n-grams as anomalous.",
            "baseline_methods": "Serves as a simple baseline for local anomaly detection compared with LSTM-based methods.",
            "performance_metrics": "True positives/negatives and accuracy per anomaly category.",
            "performance_results": "On IPv4 digit-anomaly category the n-gram model performed best because it can effectively memorize acceptable numeric groupings within its fixed window; however n-gram models with small windows fail to detect long-range anomalies (e.g., too many digit groupings) that LSTM-based methods detect.",
            "comparison_to_baseline": "Stronger on strictly local anomaly types due to memorization; weaker on long-range or structural anomalies due to limited window size.",
            "limitations_or_failure_cases": "Cannot capture long-term dependencies without exponentially increasing window sizes or data; higher false positives or false negatives when window size is mismatched to anomaly types.",
            "uuid": "e9205.2",
            "source_info": {
                "paper_title": "Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "OCSVM",
            "name_full": "One-Class Support Vector Machine",
            "brief_description": "A density-boundary estimator that constructs a decision boundary around training vectors in fixed-dimensional space to detect outliers; here used per-symbol on LSTM context vectors.",
            "citation_title": "Support vector method for novelty detection",
            "mention_or_use": "use",
            "model_name": "One-Class SVM (per-symbol)",
            "model_type": "kernelized One-Class SVM with Gaussian (RBF) kernel",
            "model_size": null,
            "data_type": "fixed-size continuous context vectors (embeddings) produced by LSTM encoder",
            "data_domain": "context-vector space derived from synthetic sequence datasets (IPv4, JSON)",
            "anomaly_type": "outliers in context vector space corresponding to impossible/zero-probability next-symbol events",
            "method_description": "Collect context vectors Y_σ = {E(x)_i : x_i = σ} for each alphabet symbol σ, then train an OCSVM O_σ on Y_σ with parameter ν (here ν=0.001) and RBF kernel; at detection evaluate the OCSVM decision function for the context vector preceding each token and compare to a threshold t_σ to flag anomalies.",
            "baseline_methods": "Used in combination with LSTM context vectors as the primary density/boundary estimator; contrasted with direct next-token probability thresholding and n-gram frequency models.",
            "performance_metrics": "Decision values from OCSVMs used with per-symbol thresholds; global metrics (TP/TN, accuracy) reported for overall anomaly detection when combined with LSTM encoder.",
            "performance_results": "When combined with LSTM context vectors, per-symbol OCSVMs produce stable decision boundaries and lead to improved anomaly detection performance compared with standard LSTM probability-thresholding on IPv4; training cost and scalability are limiting factors (O(n^3) training complexity reported).",
            "comparison_to_baseline": "Using OCSVMs on learned context vectors reduces variance issues seen with direct LSTM probability thresholds and improves stability; however, OCSVMs are computationally expensive compared to more scalable density estimators.",
            "limitations_or_failure_cases": "Cubic training time O(n^3) makes OCSVMs impractical on very large datasets; intolerance to anomalies in the training set (requires ν&gt;0 workaround) which increases false positives; per-symbol decision thresholds needed and some context vectors near the boundary cause increased false positives.",
            "uuid": "e9205.3",
            "source_info": {
                "paper_title": "Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM",
                "publication_date_yy_mm": "2018-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "One class support vector machines for detecting anomalous windows registry accesses",
            "rating": 2,
            "sanitized_title": "one_class_support_vector_machines_for_detecting_anomalous_windows_registry_accesses"
        },
        {
            "paper_title": "Support vector method for novelty detection",
            "rating": 2,
            "sanitized_title": "support_vector_method_for_novelty_detection"
        },
        {
            "paper_title": "Anomaly detection: A survey",
            "rating": 2,
            "sanitized_title": "anomaly_detection_a_survey"
        },
        {
            "paper_title": "Deep structured energy based models for anomaly detection",
            "rating": 1,
            "sanitized_title": "deep_structured_energy_based_models_for_anomaly_detection"
        },
        {
            "paper_title": "Variational Autoencoder based Anomaly Detection using Reconstruction Probability",
            "rating": 1,
            "sanitized_title": "variational_autoencoder_based_anomaly_detection_using_reconstruction_probability"
        },
        {
            "paper_title": "Anomaly detection in aircraft data using Recurrent Neural Networks (RNN)",
            "rating": 1,
            "sanitized_title": "anomaly_detection_in_aircraft_data_using_recurrent_neural_networks_rnn"
        }
    ],
    "cost": 0.009192249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM</p>
<p>Chase Roberts roberc4@rpi.edu 
Rensselaer Polytechnic Institute</p>
<p>Manish Nair Bloomberg 
Rensselaer Polytechnic Institute</p>
<p>Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM</p>
<p>We propose a simple mathematical definition and new neural architecture for finding anomalies within discrete sequence datasets. Our model comprises of a modified LSTM autoencoder and an array of One-Class SVMs. The LSTM takes in elements from a sequence and creates context vectors that are used to predict the probability distribution of the following element. These context vectors are then used to train an array of One-Class SVMs. These SVMs are used to determine an outlier boundary in context space. We show that our method is consistently more stable and also outperforms standard LSTM and sliding window anomaly detection systems on two generated datasets. * Work done while an intern at Bloomberg</p>
<p>Introduction</p>
<p>Discrete sequence anomaly detection is the process of discovering sequences in a dataset that do not conform to the pattern seen in a normal dataset. Detecting anomalies in discrete sequence data, especially text or byte datasets, is an incredibly difficult but valuable problem. Discovering these anomalies can help in many domains such as cybersecurity [3] and flight safety [4]. However, implementing these detection systems has proven to be a challenge. This is mainly because formally defining what makes a sequence anomalous is a non-trivial task. As such, metrics for an algorithm's success becomes ill-defined, mainly relying on human validation. In this paper, we give a theoretical definition of anomalies for discrete sequences and develop a machine learning architecture that is derived directly from this definition.</p>
<p>Prior Work</p>
<p>Existing discrete sequence anomaly detection systems are comprised of naive sliding window, Markov chain, or subsequence frequency analysis [1]. While these systems are effective in catching local anomalies within a sequence, they tend to to miss long-term dependencies on more structured sequences. Errors like grammatical mistakes in sentences may require looking back several dozen characters to determine an anomaly. Increasing window sizes for an n-gram model will either require significantly more training data or cause a huge increase in false positives.</p>
<p>Recently, more modern deep learning approaches have consisted of training an LSTM to predict the probability distribution of the next element in a sequence [14], and flagging that sequence as an anomaly if the predicted probability is below a certain threshold. However, if we use standard minibatch and cross entropy loss to train our LSTM, then the predict variance of infrequent values becomes quite large. This makes deciding the threshold for anomalies very difficult, and grays out the boundary between real and anomalous sequences. The instability of this decision bound makes this system difficult to use as an alerting system, as it either raises too many false positive alerts or misses an unacceptable number of true positives.</p>
<p>Background</p>
<p>Autoencoders</p>
<p>Autoencoders are an unsupervised way of decreasing the dimensionality of a given input sequence. They have been shown to outperform other dimensionality reduction techniques, such as PCA, for feature extraction [15]. Researchers have used autoencoders in the past for anomaly detection [5], image generation [6], and various input interpolation, including sentence interpolation [7].</p>
<p>Autoencoders work by training two functions: the "encoder" Enc(x) : R n → R m and the "decoder" Dec(x) : R m → R n where n &gt;&gt; m. These two functions are trained simultaneously by decreasing the loss function L = d(Dec(Enc(x)), x) where d is some reconstruction distance metric such as mean squared error or cross entropy. By forcing the neural network to reconstruct its own input through a bottleneck, the vector at the bottleneck becomes a low-dimensional rich description of the input data.</p>
<p>Past methods of anomaly detection using autoencoders would use the reconstruction loss as the metric for detection [5]. However, Vincent [8] showed that the reconstruction loss is actually equal to the gradient of the energy function for the underlying probability distribution. This causes certain anomalous inputs to be misclassified as normal inputs if they happen to have a near zero gradient. Energy-based models [9] have been developed to correct for this, but these models do not extend to discrete sequences.</p>
<p>One Class SVMs</p>
<p>One-Class SVMs (OCSVMs) [10] are density boundary estimators for fixed space inputs. Given a set of training vectors X and a value ν between 0 and 1, OCSVMs are able to create a boundary where a lower bound of 1 − ν of the vectors will be given a positive value and an upper bound of ν of the vectors will be given a negative value. Any input that is assigned a value less than 0 will be considered an outlier or anomalous input.</p>
<p>Training a OCSVM requires solving the quadratic programming problem:
min A ij α i α j K(x i , x j ) subject to 0 ≤ α i ≤ 1 νl , i α i = 1
where A is a vector of scalars α ∈ A, K is the Gaussian kernel K(x, y) = e |x−y| 2 /γ , x ∈ X are our datapoints , and l = |X|. The support vector expansion is then:
SV (y) = i α i K(x i , y) − ρ where ρ = j α j K(x j , x i ) for any given i where α i ≈ 0.
The decision function is usually the sign of SV (y). However, for our method we will be using a variable cutoff that may not be zero.</p>
<p>Proposed Method</p>
<p>Theoretical Foundation</p>
<p>Let Σ represent our finite alphabet. Let x ∈ Σ n represent a discrete sequence (we implicitly assume that each of our sequences start and end with a special delimiter element). Let L = {x 1 , x 2 , ..., x m } be the language of our system. We define x as an anomaly simply if:</p>
<p>x ∈ L Equivalently, it also follows that x is a anomaly if there exists x i ∈ x such that:
P (x i |x 1 , ..., x i−1 ; L) = 0
From this definition, we wish to build a neural network architecture to approximate the function f (x; L) where:
f (x; L) = 1, if ∃x i ∈ x : P (x i |x 1 , ..., x i−1 ; L) = 0 0, otherwise
Since we require a probability of zero, it follows that:
P (x i |x 1 , ..., x i−1 ; L) = P (x 1 , ..., x i−1 |x i ; L) = 0
Because we have now flipped the conditional of the probability distribution, we can now use a separate probability distribution for each element in our alphabet. Because of this, f (x) is equivalently:
f (x; L) = 1, if ∃x i ∈ x : P xi (x 1 , ..., x i−1 ; L) = 0 0, otherwise
where P x is the probability distribution under a specified byte x. If we are able to reduce the input for our probability distribution {x 1 , ..., x i−1 } into a vector of fixed size, then we are able to use standard density estimators for P x . In the following section, we describe exactly how we achieved this. We define:</p>
<p>Zero Boundary LSTM
E(x; θ E ) : Σ n → R n×e D(y; θ D ) : R e → R |Σ| O σ (cv) : R e → R where E(x; θ E )
is an LSTM encoder, D(y; θ D ) is a MLP decoder, and O σ (cv) is the OCSVM for element σ ∈ Σ, n is the length of the sequence x, and e is the size of our bottleneck. We define the OCSVM array as O = {O σ ∀σ ∈ Σ}. We also define E(x; θ E ) i as the ith row of E(x; θ E ). This vector will also be referenced as a context vector, as it encodes the context up to the ith element.</p>
<p>Normal LSTM autoencoders are trained by reconstructing their own input. However, as a modification, our method outputs the expected probability distribution for the i+1 element given the first i elements. Thus, we can then learn θ E and θ D by using stochastic gradient decent where our loss is the cross entropy loss of the predicted character.
L = n−1 i=1 − log P (x i+1 |D(E(x; θ E ) i ; θ D ))
We will now describe how to train the OCSVM array. Let θ E and θ D be pretrained parameters. Let X = {x 1 , ..., x N } ⊆ L be our training set. Let Y = {Y σ ∀σ ∈ Σ} where Y σ is a set of context vectors such that:
Y σ = {E(x; θ E ) i : x i = σ ∀x ∈ X}
We can then train each OCSVM O σ ∈ O with the context vector set from the corresponding Y σ ∈ Y using the method from Schölkopf [10].</p>
<p>We now describe how to approximate the function f (x). Given, θ E and O, we define g ≈ f as
g(x) = 1, if ∃x i ∈ x : O xi (E(x; θ E ) i−1 ) &lt; t σ 0
, otherwise where t σ is a threshold hyperparameter used to control the Type I error for each OCSVM. The value tends to be slightly less than zero.</p>
<p>The motivation behind our architecture comes from Heller et al. [2]. They stated that the failure of their window registry anomaly detection came from the fact that their kernel was unable to incorporate prior distributions of features. By pretraining our LSTM autoencoder to predict the distribution of the following element, the vector at the bottleneck now incorporates all of our prior information of both the sequence and the data distribution.</p>
<p>Experiments</p>
<p>We compare our method against two other anomaly detection algorithms: standard LSTM next character prediction and naive sliding window. Our experiments consist of two generated toy datasets. The toy datasets are generated IPv4 addresses and generated string only JSON objects.</p>
<p>Our LSTMs were implemented using tensorflow [11] and we used the scikit learn implementation of OCSVM [12]. Our OCSVMs were all trained with ν = 0.001 and the remaining parameters were left as their defaults. For both of our LSTMs, we used an embedding size of 128 for each of the elements in our alphabet. Our LSTMs used 5 layered cells with 128 hidden units in each layer. For the Zero Boundary LSTM, we used an MLP bottleneck with seven hidden layers of shape {128, 128, 64, 32, 64, 128, 256}. Each layer used the ReLU activation function except for the bottleneck layer which used linear activation. The standard LSTM used as a baseline has the same architecture and training procedure as the Zero Boundary LSTM, except we use two hidden layers of size 256 rather than a bottleneck. Both LSTMs used a logits of size 257 (256 possible bytes + 1 for special start/end of sequence element). We trained both networks with the Adam optimizer [16] with default parameters. The cutoff used for anomaly detection is variable and we explicitly state the cutoff used for each experiment. Our naive sliding window used a different window size for each experiment and we only publish on the window size that had the best results.</p>
<p>IPv4</p>
<p>In our first experiment, all of our models were trained on 10,000 randomly generated IPv4 addresses. These strings were generated by selecting four random integers between 0 -255 uniformly and placing a dot between each number. We then created four different classes of anomalies to try to detect: trivial, length, digit, and dot placement. Trivial anomalies are anomalies that contain characters other than a digit or a dot. Length anomalies are anomalies where the IP address was either cut off too soon or extends for more than four digit groupings. Digit anomalies are strings that contain digit groupings of values larger than 255. Finally, dot placement anomalies are anomalies where a string either contains a double dot (for example: "123.123..123.123") or either starts or ends with a dot.</p>
<p>The t σ values of our Zero Boundary LSTM were set to the lowest value calculated from our training and validation sets. The cutoff for the standard LSTM was set to be 1 in 8103. Both LSTMs were trained with two epochs of the training data. The n-gram model used a window of size four. The results for this experiment can be seen in Figure 2.</p>
<p>JSONs</p>
<p>The next toy dataset experiment used randomly generated JSON strings. These strings contain nested objects up to four levels deep. Each object contains between zero and four random entries. The In this experiment, we trained both LSTMs for four epochs of the data. We used an n-gram window size of three. The results of this experiment can be seen in Figure 3. Our Zero Boundary LSTM was able to nearly match the performance of the normal LSTM. However, these results are from highest performing models after many attempts and hyper parameter searches. One of the main advantages of our system is the stability we see with training.</p>
<p>In the next experiment, we showcased the models trained with different epoch values. As you can see in Figure 4, the Zero Boundary LSTM has a much easier time consistently finding a stable decision boundary than the normal LSTM.</p>
<p>Future Work</p>
<p>One of the largest drawbacks to our system is its intolerance to having anomalies within our training set. To get around this, we use a ν value that is not zero. However, this workaround also tends to increase false positive rating, especially for sequences that have context vectors right on the edge of the decision bound. Also, using OCSVMs as our density estimators makes scaling to larger datasets difficult, as OCSVM have a training run time of O(n 3 ). Solving these two issues will be a great improvement to our system and may even make unsupervised anomaly detection in areas like network traffic finally viable.</p>
<p>Conclusion</p>
<p>In this paper, we developed a mathematical definition of anomalies for discrete sequence datasets and created a neural network architecture to approximate this definition. Our method is able to give a stable decision boundary that does not suffer from the variance of SGD training while also  outperforming or matching the performance of both the sliding window and standard unsupervised LSTM algorithms. Even with these successes, our system still suffers heavily when scaled to very large datasets due to the cubic nature of OCSVMs. In order to use this system for more practical situations, this will need to be solved.</p>
<p>Figure 1 :
1A diagram of our neural network architecture. Here, to detect if element x 5 is an anomaly, we calculate the context vector up to x 5 and pass it into the corresponding OCSVM.</p>
<p>Figure 2 :
2Experimental results with our IPv4 dataset. Here, we see that our Zero Boundary LSTM outperforms the standard LSTM algorithm in all categories. The Zero boundary does not perform as well as the n-gram model for anomalous digit dataset, as the ngram has the capacity to memorize all possible acceptable numbers. However, due to the window size being so short, n-grams can not detect anomalies where there are too many digit groupings, unlike the Zero Boundary and normal LSTMs. entries for any given JSON object are either lowercase strings with lowercase string keys, or nested JSON objects with lowercase string keys. The probability of nesting for any given entry was set to be 1 in 5. We then created four different classes of anomalies to detect: colon, comma, quote, and nesting. Colon anomalies are anomalies where a colon is misplaced. Comma anomalies are when commas are either incorrectly placed or missing. Quote anomalies are where quote markers are either misplaced or completely missing. Nesting anomalies are when the curly braces do not add up correctly.</p>
<p>Figure 4 :
4Training stability results from our Zero Boundary LSTM and normal LSTM models. Accuracies are determined from the same datasets used for the other JSONs experiment.</p>
<p>Figure 3: Experimental results with our JSON dataset. Results are from the best models that we could train.Zero Boundary True Positives Zero Boundary True NegativesNormal True Positives Normal True NegativesColon </p>
<p>Comma 
Nesting 
Quote 
Test Set </p>
<p>960 </p>
<p>980 </p>
<p>1,000 </p>
<p>Dataset Type </p>
<p>Count </p>
<p>JSONs </p>
<p>Zero Boundary 
Normal LSTM 
n-gram-3 </p>
<p>Epoch 1 
Epoch 2 
Epoch 3 
Epoch 4 </p>
<p>0.4 </p>
<p>0.6 </p>
<p>0.8 </p>
<p>1 </p>
<p>Epochs </p>
<p>Percentage </p>
<p>JSON Training Stability </p>
<p>AcknowledgmentsWe would like to thank Raphael Meyer, Connie Yee and Sheryl Zhang of Bloomberg for their valuable contributions to this work.
Anomaly detection: A survey. Varun Chandola, Arindam Banerjee, Vipin Kumar, ACM computing surveys (CSUR). 4115Chandola, Varun, Arindam Banerjee, and Vipin Kumar. "Anomaly detection: A survey." ACM computing surveys (CSUR) 41.3 (2009): 15.</p>
<p>One class support vector machines for detecting anomalous windows registry accesses. Katherine A Heller, Proc. of the workshop on Data Mining for Computer Security. of the workshop on Data Mining for Computer Security9Heller, Katherine A., et al. "One class support vector machines for detecting anomalous windows registry accesses." Proc. of the workshop on Data Mining for Computer Security. Vol. 9. 2003.</p>
<p>Anomalous payload-based network intrusion detection. Ke Wang, Salvatore J Stolfo, 4Wang, Ke, and Salvatore J. Stolfo. "Anomalous payload-based network intrusion detection." RAID. Vol. 4. 2004.</p>
<p>Discovering atypical flights in sequences of discrete flight parameters. Budalakoti, Ashok N Suratna, Ram Srivastava, Akella, Aerospace Conference. IEEEBudalakoti, Suratna, Ashok N. Srivastava, and Ram Akella. "Discovering atypical flights in sequences of discrete flight parameters." Aerospace Conference, 2006 IEEE. IEEE, 2006.</p>
<p>Variational Autoencoder based Anomaly Detection using Reconstruction Probability. Jinwon An, Sungzoon Cho, Technical ReportAn, Jinwon, and Sungzoon Cho. Variational Autoencoder based Anomaly Detection using Reconstruction Probability. Technical Report, 2015.</p>
<p>Optimizing neural networks that generate images. Tijmen Tieleman, Diss. University of Toronto (CanadaTieleman, Tijmen. Optimizing neural networks that generate images. Diss. University of Toronto (Canada), 2014.</p>
<p>Generating sentences from a continuous space. Samuel R Bowman, arXiv:1511.06349arXiv preprintBowman, Samuel R., et al. "Generating sentences from a continuous space." arXiv preprint arXiv:1511.06349 (2015).</p>
<p>A connection between score matching and denoising autoencoders. Pascal Vincent, Neural computation. 23Vincent, Pascal. "A connection between score matching and denoising autoencoders." Neural computation 23.7 (2011): 1661-1674.</p>
<p>Deep structured energy based models for anomaly detection. Shuangfei Zhai, International Conference on Machine Learning. Zhai, Shuangfei, et al. "Deep structured energy based models for anomaly detection." International Confer- ence on Machine Learning. 2016.</p>
<p>Advances in neural information processing systems. Bernhard Schölkopf, Support vector method for novelty detectionSchölkopf, Bernhard, et al. "Support vector method for novelty detection." Advances in neural information processing systems. 2000.</p>
<p>Tensorflow: Large-scale machine learning on heterogeneous distributed systems. Martín Abadi, arXiv:1603.04467arXiv preprintAbadi, Martín, et al. "Tensorflow: Large-scale machine learning on heterogeneous distributed systems." arXiv preprint arXiv:1603.04467 (2016).</p>
<p>Scikit-learn: Machine learning in Python. Fabian Pedregosa, Journal of Machine Learning Research. 12Pedregosa, Fabian, et al. "Scikit-learn: Machine learning in Python." Journal of Machine Learning Research 12.Oct (2011): 2825-2830.</p>
<p>SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient. Lantao Yu, Yu, Lantao, et al. "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient." AAAI. 2017.</p>
<p>Anomaly detection in aircraft data using Recurrent Neural Networks (RNN). Anvardh Nanduri, Lance Sherry, IEEEIntegrated Communications Navigation and Surveillance (ICNSNanduri, Anvardh, and Lance Sherry. "Anomaly detection in aircraft data using Recurrent Neural Networks (RNN)." Integrated Communications Navigation and Surveillance (ICNS), 2016. IEEE, 2016.</p>
<p>Unsupervised learning of invariant feature hierarchies with applications to object recognition. Fu Huang, Y-Lan Jie, Yann Boureau, Lecun, IEEEComputer Vision and Pattern RecognitionHuang, Fu Jie, Y-Lan Boureau, and Yann LeCun. "Unsupervised learning of invariant feature hierarchies with applications to object recognition." Computer Vision and Pattern Recognition, 2007. CVPR'07. IEEE Conference on. IEEE, 2007.</p>
<p>Adam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, arXiv:1412.6980arXiv preprintKingma, Diederik, and Jimmy Ba. "Adam: A method for stochastic optimization." arXiv preprint arXiv:1412.6980 (2014).</p>            </div>
        </div>

    </div>
</body>
</html>
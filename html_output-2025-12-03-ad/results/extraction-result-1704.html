<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1704 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1704</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1704</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-260438420</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.01399v2.pdf" target="_blank">Learning to Model the World with Language</a></p>
                <p><strong>Paper Abstract:</strong> To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents can learn to execute simple language instructions, we aim to build agents that leverage diverse language -- language like"this button turns on the TV"or"I put the bowls away"-- that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. While current methods that learn language-conditioned policies degrade in performance with more diverse types of language, we show that Dynalang learns to leverage environment descriptions, game rules, and instructions to excel on tasks ranging from game-playing to navigating photorealistic home scans. Finally, we show that our method enables additional capabilities due to learning a generative model: Dynalang can be pretrained on text-only data, enabling learning from offline datasets, and generate language grounded in an environment.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1704.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1704.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynalang</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynalang (multimodal world model agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based embodied agent that learns a multimodal generative world model predicting future image and text representations, and uses imagined rollouts from that model to train actor-critic policies; it can be pretrained on text-only data and finetuned on embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Dynalang</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A DreamerV3-based Recurrent State Space Model (RSSM) world model that encodes per-timestep image frames and language tokens into discrete latent codes z_t, predicts future latents ẑ_{t+1} conditioned on actions and recurrent state h_t, decodes image/text/reward/continue signals, and trains an actor-critic policy from imagined latent rollouts. The model uses a variational discrete latent bottleneck, GRU sequence model, CNN image encoder/decoder, and MLPs for language processing and policy/value heads. It supports one-hot token or pretrained token embeddings and can output language tokens as part of its action space.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>text-only corpora (in-domain manuals and general-domain short-story corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Two text-only pretraining sources evaluated: (1) in-domain text comprising game manuals from Messenger Stage 2 / Stage 1 games (used as 'in-domain' manual data for the Messenger benchmark); (2) TinyStories, a domain-general dataset of 2M short stories (generated by a large LM) containing ~500M tokens. Pretraining was performed by zeroing image and action inputs and disabling image/reward/continue decoder losses so the world model learned text representations and text dynamics (next-representation prediction). Exact token counts: TinyStories ≈ 500M tokens; Messenger in-domain manual corpus size not quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>VLN-CE (Vision-Language Navigation in Continuous Environments); also evaluated on HomeGrid, Messenger, LangRoom</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>VLN-CE: a realistic 3D photorealistic navigation task using Matterport3D scenes (real homes), where the agent receives natural language navigation instructions and egocentric RGB+depth observations and must take low-level discrete navigation actions (forward 0.25m, turn ±15° and stop) to reach a goal and emit stop at the correct location. HomeGrid: partially-observed pixel gridworld with multitask object/trash interactions and streaming language hints. Messenger: symbolic gridworld where agents read manuals describing randomized rules/dynamics. LangRoom: small embodied question-answering room where agent can move and emit language tokens as actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>During text-only pretraining there are no actions used (image and action inputs zeroed); language is provided token-by-token as observations. When language is present in environment interaction, language observations are single tokens per timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete low-level motor actions in the embodied tasks: e.g. VLN-CE uses discrete low-level navigation actions (move forward 0.25 m, turn left/right 15°, stop); HomeGrid uses cardinal moves and discrete object/interact operations (pick up, drop, get, pedal, grasp, lift); Messenger uses discrete move actions on a symbolic grid; LangRoom includes movement and discrete language-token outputs as actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>No explicit learned mapping from text actions to low-level motor commands; instead, Dynalang learns a joint multimodal latent dynamics model where language, vision, and actions are encoded into latent representations and the sequence model predicts future latents conditioned on actions. Policies are trained on imagined latent rollouts to map latent state (h_t, z_t) to discrete actions. For language generation as actions (LangRoom) the policy's language-action distribution is regularized with a KL term toward the world model's predicted next-token distribution to bias generation toward plausible utterances.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Depends on task: VLN-CE requires egocentric RGB and depth (photorealistic) perception; HomeGrid requires RGB pixels with partial observability; Messenger uses symbolic grid observations; LangRoom uses RGB pixels (partial view) and token-based language observations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Qualitative improvement reported on Messenger S2 when the Dynalang world model was pretrained on text-only corpora: (1) even small in-domain text pretraining closes much of the gap between learned token embeddings from scratch and using pretrained T5 embeddings; (2) pretraining on TinyStories (~500M tokens) exceeded final performance of using T5 embeddings. Exact numeric performance values for the pretraining vs non-pretraining runs are not provided in tabular form in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>When training text embeddings from scratch on Messenger S2 (no text pretraining), models struggled to learn the complex language in S2 without S1 pretraining and performed worse than using pretrained T5 embeddings or TinyStories pretraining. Exact numeric baselines are not provided in the text beyond qualitative curves/plots.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Reported embodied training budgets used in finetuning experiments (Table I.3): Messenger S2 finetuning used 25M environment steps; VLN-CE training used 30M environment steps; HomeGrid used 50M env steps. The paper reports that pretraining reduced the amount of online learning needed to reach competitive performance on Messenger S2, but does not provide a precise sample-count-to-threshold comparison number.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Same finetuning budgets were used in the experiments from scratch (e.g., Messenger S2 with learned embeddings required more environment interaction and S1 initialization to match performance), but explicit counts showing how many fewer steps were required with pretraining are not provided numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitative gain: text-only pretraining (both small in-domain manuals and larger TinyStories) produced faster and higher final performance on downstream Messenger tasks compared to training embeddings from scratch, with TinyStories outperforming pretrained T5 embeddings; the paper does not report a single scalar such as 'x-fold' sample reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Key enabling factors noted by the paper: (1) the generative multimodal world-model objective that predicts future text and images (shared predictive objective) allows learning language dynamics offline; (2) decoupling world modeling from policy learning enables pretraining without actions and later finetuning; (3) discrete latent codes and next-representation prediction let the model learn text dynamics that transfer to vision+action tasks; (4) using pretrained token embeddings (T5) or offline text pretraining improves text representations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limitations discussed or implied: (1) perception gap between text-only pretraining and visually-grounded embodied inputs (image encoders must be learned/finetuned at deployment); (2) mismatch between text dynamics and spatial/physical dynamics in 3D environments; (3) scale — the paper's text-only pretraining was small-scale relative to SOTA LLM pretraining and larger-scale pretraining may be required for stronger transfer; (4) paper reports that despite benefits, pretraining does not make Dynalang state-of-the-art on VLN-CE, suggesting further architectural or data-scale challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dynalang's multimodal generative world model can be pretrained on text-only corpora (in-domain game manuals and a ~500M-token TinyStories corpus) by disabling image/action losses, and this offline text pretraining improves downstream embodied RL performance (notably on Messenger S2), often matching or exceeding the benefit of using pretrained T5 embeddings; pretraining enables learning text dynamics offline and yields qualitative improvements in sample efficiency and final performance, though precise numeric sample-efficiency gains and comprehensive evaluations on large-scale 3D tasks remain limited in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Model the World with Language', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1704.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1704.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-small embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-small token embeddings (Raffel et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained transformer-based language model token embeddings (T5-small, ~60M parameters) used to embed textual tokens before feeding into the world model; served as a pretrained language baseline and comparison for text-only pretraining of Dynalang.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>T5-small embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Frozen T5-small model used to produce token-level embeddings (last hidden layer) for input sentences/tokens. Employed in Messenger and VLN-CE experiments to provide pretrained semantic embeddings and as a baseline compared to training embeddings from scratch or text-only pretraining of Dynalang.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>large-scale language model pretraining on web-scale corpora (original T5 pretraining; used here as frozen embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper uses pretrained T5-small (60M parameters) embeddings as input representations. The T5 pretraining corpus details are referenced via Raffel et al. (2020) but not re-described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Messenger (S2/S1) and VLN-CE (used as an embedding baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>See Dynalang entry: Messenger is a gridworld with textual manuals describing game rules; VLN-CE is vision-language navigation in photorealistic Matterport3D continuous environments.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A for T5 pretraining in this paper (T5 is used as an observation encoder, not as an action policy).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A (T5 provides embeddings for agents acting in discrete action spaces of the embodied tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Not applicable: T5 provides fixed token embeddings that are fed into the world model / policy; there is no explicit mapping from T5 outputs to motor commands beyond the standard policy MLP that maps latent representations to discrete actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>T5 is text-only; perception modalities in the embodied tasks remain RGB/depth or symbolic as required by the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Using pretrained T5 embeddings improved baseline learning compared to training token embeddings from scratch on some tasks (Messenger S1/S2), but offline Dynalang pretraining on TinyStories (≈500M tokens) exceeded final performance of using T5 embeddings on Messenger S2 according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>When embeddings are learned from scratch without pretrained T5, models (especially on Messenger S2) struggle to learn complex language dynamics and perform worse than with T5 or Dynalang text pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not reported quantitatively; using T5 embeddings reduced the need for in-environment language dynamics learning compared to learned-from-scratch embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported quantitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitative improvement vs learned-from-scratch embeddings; exact numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Pretrained semantic representations and token-level contextualization from large-scale language model pretraining provide better initialization for language understanding in embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Fixed text embeddings are not sufficient to ground language to perceptual observations and physical dynamics; they may be outperformed by models that learn text dynamics jointly with visual grounding or by large-scale task-specific pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained token embeddings (T5-small) provide a strong baseline for language inputs in embodied RL, but Dynalang's text-only world-model pretraining (especially on a ~500M-token TinyStories corpus) can surpass the utility of frozen T5 embeddings by learning text dynamics useful for downstream multimodal prediction and policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Model the World with Language', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1704.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1704.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E (an embodied multimodal language model) - referenced in related work</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in related work as an example of a large pretrained language model used for embodied multimodal tasks; cited to contrast pretrained LLM approaches with Dynalang's learned multimodal world model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PaLM-E (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Mentioned as prior work where large language models are augmented or adapted for embodied tasks; the paper references these approaches to highlight differences (e.g., LLMs not directly grounded to environment observations or hard to update online).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>pretrained on large web/text multimodal corpora (as a large LLM / multimodal model) — detailed pretraining specifics not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Referenced generally as applied to embodied control or multimodal tasks (not used in experiments here).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Mentioned generally: LLMs encode world knowledge via next-token prediction which can be leveraged for planning or as priors, but have grounding and online-update limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Paper notes LLMs are not grounded to real environment observations and are difficult to update with environment feedback; representing visual inputs as text is low-bandwidth.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced to motivate Dynalang's approach: unlike LLM-based priors, Dynalang learns a multimodal generative model from experience that can be pretrained on text-only data and finetuned from embodied experience.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Model the World with Language', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1704.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1704.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2 / RT2-like (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2 / Rt-2-like vision-language-action models (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced in related work as models that transfer web-scale language/vision knowledge to robotic control; cited to situate Dynalang with respect to supervised, demonstration-driven approaches that use pretrained language/video models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RT-2 (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Mentioned as an example of vision-language-action models that use pretraining on Web-scale data to inform robotic control; not used in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale web/text/vision pretraining (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Referenced generally: benefits from large web-scale pretraining and alignment of vision-language representations for downstream robotic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not discussed in detail in this paper; RT-2-like methods often rely on supervised demonstration data and may not learn online as easily as model-based RL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Included in related work to contrast demonstration- or LLM-based methods with Dynalang's world-model pretraining and online finetuning approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Model the World with Language', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Palm-e: An embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>Rt-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 2)</em></li>
                <li>Learning universal policies via text-guided video generation <em>(Rating: 2)</em></li>
                <li>Reading to learn: Constructing features from semantic abstracts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1704",
    "paper_id": "paper-260438420",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "Dynalang",
            "name_full": "Dynalang (multimodal world model agent)",
            "brief_description": "A model-based embodied agent that learns a multimodal generative world model predicting future image and text representations, and uses imagined rollouts from that model to train actor-critic policies; it can be pretrained on text-only data and finetuned on embodied tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "Dynalang",
            "model_agent_description": "A DreamerV3-based Recurrent State Space Model (RSSM) world model that encodes per-timestep image frames and language tokens into discrete latent codes z_t, predicts future latents ẑ_{t+1} conditioned on actions and recurrent state h_t, decodes image/text/reward/continue signals, and trains an actor-critic policy from imagined latent rollouts. The model uses a variational discrete latent bottleneck, GRU sequence model, CNN image encoder/decoder, and MLPs for language processing and policy/value heads. It supports one-hot token or pretrained token embeddings and can output language tokens as part of its action space.",
            "pretraining_data_type": "text-only corpora (in-domain manuals and general-domain short-story corpus)",
            "pretraining_data_details": "Two text-only pretraining sources evaluated: (1) in-domain text comprising game manuals from Messenger Stage 2 / Stage 1 games (used as 'in-domain' manual data for the Messenger benchmark); (2) TinyStories, a domain-general dataset of 2M short stories (generated by a large LM) containing ~500M tokens. Pretraining was performed by zeroing image and action inputs and disabling image/reward/continue decoder losses so the world model learned text representations and text dynamics (next-representation prediction). Exact token counts: TinyStories ≈ 500M tokens; Messenger in-domain manual corpus size not quantified in the paper.",
            "embodied_task_name": "VLN-CE (Vision-Language Navigation in Continuous Environments); also evaluated on HomeGrid, Messenger, LangRoom",
            "embodied_task_description": "VLN-CE: a realistic 3D photorealistic navigation task using Matterport3D scenes (real homes), where the agent receives natural language navigation instructions and egocentric RGB+depth observations and must take low-level discrete navigation actions (forward 0.25m, turn ±15° and stop) to reach a goal and emit stop at the correct location. HomeGrid: partially-observed pixel gridworld with multitask object/trash interactions and streaming language hints. Messenger: symbolic gridworld where agents read manuals describing randomized rules/dynamics. LangRoom: small embodied question-answering room where agent can move and emit language tokens as actions.",
            "action_space_text": "During text-only pretraining there are no actions used (image and action inputs zeroed); language is provided token-by-token as observations. When language is present in environment interaction, language observations are single tokens per timestep.",
            "action_space_embodied": "Discrete low-level motor actions in the embodied tasks: e.g. VLN-CE uses discrete low-level navigation actions (move forward 0.25 m, turn left/right 15°, stop); HomeGrid uses cardinal moves and discrete object/interact operations (pick up, drop, get, pedal, grasp, lift); Messenger uses discrete move actions on a symbolic grid; LangRoom includes movement and discrete language-token outputs as actions.",
            "action_mapping_method": "No explicit learned mapping from text actions to low-level motor commands; instead, Dynalang learns a joint multimodal latent dynamics model where language, vision, and actions are encoded into latent representations and the sequence model predicts future latents conditioned on actions. Policies are trained on imagined latent rollouts to map latent state (h_t, z_t) to discrete actions. For language generation as actions (LangRoom) the policy's language-action distribution is regularized with a KL term toward the world model's predicted next-token distribution to bias generation toward plausible utterances.",
            "perception_requirements": "Depends on task: VLN-CE requires egocentric RGB and depth (photorealistic) perception; HomeGrid requires RGB pixels with partial observability; Messenger uses symbolic grid observations; LangRoom uses RGB pixels (partial view) and token-based language observations.",
            "transfer_successful": true,
            "performance_with_pretraining": "Qualitative improvement reported on Messenger S2 when the Dynalang world model was pretrained on text-only corpora: (1) even small in-domain text pretraining closes much of the gap between learned token embeddings from scratch and using pretrained T5 embeddings; (2) pretraining on TinyStories (~500M tokens) exceeded final performance of using T5 embeddings. Exact numeric performance values for the pretraining vs non-pretraining runs are not provided in tabular form in the paper.",
            "performance_without_pretraining": "When training text embeddings from scratch on Messenger S2 (no text pretraining), models struggled to learn the complex language in S2 without S1 pretraining and performed worse than using pretrained T5 embeddings or TinyStories pretraining. Exact numeric baselines are not provided in the text beyond qualitative curves/plots.",
            "sample_complexity_with_pretraining": "Reported embodied training budgets used in finetuning experiments (Table I.3): Messenger S2 finetuning used 25M environment steps; VLN-CE training used 30M environment steps; HomeGrid used 50M env steps. The paper reports that pretraining reduced the amount of online learning needed to reach competitive performance on Messenger S2, but does not provide a precise sample-count-to-threshold comparison number.",
            "sample_complexity_without_pretraining": "Same finetuning budgets were used in the experiments from scratch (e.g., Messenger S2 with learned embeddings required more environment interaction and S1 initialization to match performance), but explicit counts showing how many fewer steps were required with pretraining are not provided numerically.",
            "sample_complexity_gain": "Qualitative gain: text-only pretraining (both small in-domain manuals and larger TinyStories) produced faster and higher final performance on downstream Messenger tasks compared to training embeddings from scratch, with TinyStories outperforming pretrained T5 embeddings; the paper does not report a single scalar such as 'x-fold' sample reduction.",
            "transfer_success_factors": "Key enabling factors noted by the paper: (1) the generative multimodal world-model objective that predicts future text and images (shared predictive objective) allows learning language dynamics offline; (2) decoupling world modeling from policy learning enables pretraining without actions and later finetuning; (3) discrete latent codes and next-representation prediction let the model learn text dynamics that transfer to vision+action tasks; (4) using pretrained token embeddings (T5) or offline text pretraining improves text representations.",
            "transfer_failure_factors": "Limitations discussed or implied: (1) perception gap between text-only pretraining and visually-grounded embodied inputs (image encoders must be learned/finetuned at deployment); (2) mismatch between text dynamics and spatial/physical dynamics in 3D environments; (3) scale — the paper's text-only pretraining was small-scale relative to SOTA LLM pretraining and larger-scale pretraining may be required for stronger transfer; (4) paper reports that despite benefits, pretraining does not make Dynalang state-of-the-art on VLN-CE, suggesting further architectural or data-scale challenges.",
            "key_findings": "Dynalang's multimodal generative world model can be pretrained on text-only corpora (in-domain game manuals and a ~500M-token TinyStories corpus) by disabling image/action losses, and this offline text pretraining improves downstream embodied RL performance (notably on Messenger S2), often matching or exceeding the benefit of using pretrained T5 embeddings; pretraining enables learning text dynamics offline and yields qualitative improvements in sample efficiency and final performance, though precise numeric sample-efficiency gains and comprehensive evaluations on large-scale 3D tasks remain limited in this paper.",
            "uuid": "e1704.0",
            "source_info": {
                "paper_title": "Learning to Model the World with Language",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "T5-small embeddings",
            "name_full": "T5-small token embeddings (Raffel et al., 2020)",
            "brief_description": "Pretrained transformer-based language model token embeddings (T5-small, ~60M parameters) used to embed textual tokens before feeding into the world model; served as a pretrained language baseline and comparison for text-only pretraining of Dynalang.",
            "citation_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "mention_or_use": "use",
            "model_agent_name": "T5-small embeddings",
            "model_agent_description": "Frozen T5-small model used to produce token-level embeddings (last hidden layer) for input sentences/tokens. Employed in Messenger and VLN-CE experiments to provide pretrained semantic embeddings and as a baseline compared to training embeddings from scratch or text-only pretraining of Dynalang.",
            "pretraining_data_type": "large-scale language model pretraining on web-scale corpora (original T5 pretraining; used here as frozen embeddings)",
            "pretraining_data_details": "The paper uses pretrained T5-small (60M parameters) embeddings as input representations. The T5 pretraining corpus details are referenced via Raffel et al. (2020) but not re-described in this paper.",
            "embodied_task_name": "Messenger (S2/S1) and VLN-CE (used as an embedding baseline)",
            "embodied_task_description": "See Dynalang entry: Messenger is a gridworld with textual manuals describing game rules; VLN-CE is vision-language navigation in photorealistic Matterport3D continuous environments.",
            "action_space_text": "N/A for T5 pretraining in this paper (T5 is used as an observation encoder, not as an action policy).",
            "action_space_embodied": "N/A (T5 provides embeddings for agents acting in discrete action spaces of the embodied tasks)",
            "action_mapping_method": "Not applicable: T5 provides fixed token embeddings that are fed into the world model / policy; there is no explicit mapping from T5 outputs to motor commands beyond the standard policy MLP that maps latent representations to discrete actions.",
            "perception_requirements": "T5 is text-only; perception modalities in the embodied tasks remain RGB/depth or symbolic as required by the environment.",
            "transfer_successful": true,
            "performance_with_pretraining": "Using pretrained T5 embeddings improved baseline learning compared to training token embeddings from scratch on some tasks (Messenger S1/S2), but offline Dynalang pretraining on TinyStories (≈500M tokens) exceeded final performance of using T5 embeddings on Messenger S2 according to the paper.",
            "performance_without_pretraining": "When embeddings are learned from scratch without pretrained T5, models (especially on Messenger S2) struggle to learn complex language dynamics and perform worse than with T5 or Dynalang text pretraining.",
            "sample_complexity_with_pretraining": "Not reported quantitatively; using T5 embeddings reduced the need for in-environment language dynamics learning compared to learned-from-scratch embeddings.",
            "sample_complexity_without_pretraining": "Not reported quantitatively.",
            "sample_complexity_gain": "Qualitative improvement vs learned-from-scratch embeddings; exact numbers not provided.",
            "transfer_success_factors": "Pretrained semantic representations and token-level contextualization from large-scale language model pretraining provide better initialization for language understanding in embodied tasks.",
            "transfer_failure_factors": "Fixed text embeddings are not sufficient to ground language to perceptual observations and physical dynamics; they may be outperformed by models that learn text dynamics jointly with visual grounding or by large-scale task-specific pretraining.",
            "key_findings": "Pretrained token embeddings (T5-small) provide a strong baseline for language inputs in embodied RL, but Dynalang's text-only world-model pretraining (especially on a ~500M-token TinyStories corpus) can surpass the utility of frozen T5 embeddings by learning text dynamics useful for downstream multimodal prediction and policy learning.",
            "uuid": "e1704.1",
            "source_info": {
                "paper_title": "Learning to Model the World with Language",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "PaLM-E (mentioned)",
            "name_full": "PaLM-E (an embodied multimodal language model) - referenced in related work",
            "brief_description": "Mentioned in related work as an example of a large pretrained language model used for embodied multimodal tasks; cited to contrast pretrained LLM approaches with Dynalang's learned multimodal world model.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "PaLM-E (referenced)",
            "model_agent_description": "Mentioned as prior work where large language models are augmented or adapted for embodied tasks; the paper references these approaches to highlight differences (e.g., LLMs not directly grounded to environment observations or hard to update online).",
            "pretraining_data_type": "pretrained on large web/text multimodal corpora (as a large LLM / multimodal model) — detailed pretraining specifics not given in this paper.",
            "pretraining_data_details": null,
            "embodied_task_name": "Referenced generally as applied to embodied control or multimodal tasks (not used in experiments here).",
            "embodied_task_description": null,
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Mentioned generally: LLMs encode world knowledge via next-token prediction which can be leveraged for planning or as priors, but have grounding and online-update limitations.",
            "transfer_failure_factors": "Paper notes LLMs are not grounded to real environment observations and are difficult to update with environment feedback; representing visual inputs as text is low-bandwidth.",
            "key_findings": "Referenced to motivate Dynalang's approach: unlike LLM-based priors, Dynalang learns a multimodal generative model from experience that can be pretrained on text-only data and finetuned from embodied experience.",
            "uuid": "e1704.2",
            "source_info": {
                "paper_title": "Learning to Model the World with Language",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "RT-2 / RT2-like (mentioned)",
            "name_full": "RT-2 / Rt-2-like vision-language-action models (referenced)",
            "brief_description": "Referenced in related work as models that transfer web-scale language/vision knowledge to robotic control; cited to situate Dynalang with respect to supervised, demonstration-driven approaches that use pretrained language/video models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "RT-2 (referenced)",
            "model_agent_description": "Mentioned as an example of vision-language-action models that use pretraining on Web-scale data to inform robotic control; not used in experiments in this paper.",
            "pretraining_data_type": "Large-scale web/text/vision pretraining (not detailed in this paper).",
            "pretraining_data_details": null,
            "embodied_task_name": null,
            "embodied_task_description": null,
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Referenced generally: benefits from large web-scale pretraining and alignment of vision-language representations for downstream robotic tasks.",
            "transfer_failure_factors": "Not discussed in detail in this paper; RT-2-like methods often rely on supervised demonstration data and may not learn online as easily as model-based RL.",
            "key_findings": "Included in related work to contrast demonstration- or LLM-based methods with Dynalang's world-model pretraining and online finetuning approach.",
            "uuid": "e1704.3",
            "source_info": {
                "paper_title": "Learning to Model the World with Language",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Palm-e: An embodied multimodal language model",
            "rating": 2,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        },
        {
            "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 2,
            "sanitized_title": "rt2_visionlanguageaction_models_transfer_web_knowledge_to_robotic_control"
        },
        {
            "paper_title": "Learning universal policies via text-guided video generation",
            "rating": 2,
            "sanitized_title": "learning_universal_policies_via_textguided_video_generation"
        },
        {
            "paper_title": "Reading to learn: Constructing features from semantic abstracts",
            "rating": 1,
            "sanitized_title": "reading_to_learn_constructing_features_from_semantic_abstracts"
        }
    ],
    "cost": 0.018120749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning to Model the World With Language
31 May 2024</p>
<p>Jessy Lin <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#106;&#101;&#115;&#115;&#121;&#95;&#108;&#105;&#110;&#64;&#98;&#101;&#114;&#107;&#101;&#108;&#101;&#121;&#46;&#101;&#100;&#117;">&#106;&#101;&#115;&#115;&#121;&#95;&#108;&#105;&#110;&#64;&#98;&#101;&#114;&#107;&#101;&#108;&#101;&#121;&#46;&#101;&#100;&#117;</a>. 
UC Berkeley</p>
<p>UC Berkeley</p>
<p>Yuqing Du 
UC Berkeley</p>
<p>Olivia Watkins 
UC Berkeley</p>
<p>Danijar Hafner 
UC Berkeley</p>
<p>Pieter Abbeel 
UC Berkeley</p>
<p>Dan Klein 
UC Berkeley</p>
<p>Anca Dragan 
UC Berkeley</p>
<p>Learning to Model the World With Language
31 May 2024BF96C045665E4C5FD27BB26342F6D4ECarXiv:2308.01399v2[cs.CL]
To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world.While current agents can learn to execute simple language instructions, we aim to build agents that leverage diverse languagelanguage like "this button turns on the TV" or "I put the bowls away"-that conveys general knowledge, describes the state of the world, provides interactive feedback, and more.Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will be rewarded.This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective.We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts.While current methods that learn language-conditioned policies degrade in performance with more diverse types of language, we show that Dynalang learns to leverage environment descriptions, game rules, and instructions to excel on tasks ranging from game-playing to navigating photorealistic home scans.Finally, we show that our method enables additional capabilities due to learning a generative model: Dynalang can be pretrained on text-only data, enabling learning from offline datasets, and generate language grounded in an environment.</p>
<p>Introduction</p>
<p>A long-standing goal of artificial intelligence is to develop agents that can use language to interact naturally with people in the physical world (Winograd, 1972).Current embodied agents can follow basic instructions like "bring me the apple" (Driess et al., 2023).However, the full potential of Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024.Copyright 2024 by the author(s).language affords much richer communication beyond task specification.Consider a household robot-beyond delegating tasks to it one after another ("clean the dishes...pick up the toy..."), one would imagine that a truly natural interaction would allow us to communicate beyond the "here and now" (Hockett &amp; Hockett, 1960): sharing knowledge such as "the top left button turns off the TV," providing situational information such as "we're out of milk," and coordinating by saying "I already vacuumed the living room."Language communicates what we know about the state of the world or how things work, not just what to do.</p>
<p>It is unclear how to best integrate diverse kinds of language with vision and action in a single agent.In many ways, current methods are specialized for learning from language instructions, e.g. by embedding a task description ("pick up the blue block") at the beginning of an episode and outputting a sequence of motor controls (Brohan et al., 2023;Nair et al., 2021).To freely collaborate with humans, agents ideally would continuously integrate language inputs while acting in the environment.This natural interaction requires us to move beyond language "prompting" towards methods that input and output language continuously, along with action and video.More crucially, we hypothesize that directly mapping diverse language to optimal actions is a difficult learning problem, because language and optimal actions may only be weakly correlated if their dependency is complex.Consider the example "I put the bowls away": if the task at hand is cleaning up, the agent should respond by moving on to the next cleaning step, whereas if it is serving dinner, the agent should retrieve the bowls.The key question is thus what the right learning signal is for understanding the full range of language while acting in the world.</p>
<p>In this work, we propose that agents can ground diverse kinds of language by using it to predict the future.We instantiate this idea with Dynalang, an agent that learns a joint generative model of language and vision (i.e., a multimodal world model; Ha &amp; Schmidhuber (2018)), and uses the model to plan and act.We build on the DreamerV3 algorithm (Hafner et al., 2023), training the world model to predict future latent representations of all observation modalities, using experience collected from acting in the environment.In contrast to directly predicting what to do with a language-conditioned policy, Dynalang decouples learning to model the world with language (supervised learning Figure 1.Dynalang learns to use language to make predictions about future (text + image) observations and rewards.Here, we show real model predictions in the HomeGrid environment.From the past text "the bottle is in the living room", the agent predicts at timesteps 61-65 that it will see the bottle in the final corner of the living room.From the text "get the bottle" describing the task, the agent predicts that it will be rewarded for picking up the bottle.The agent can also predict future text observations: given the prefix "the plates are in the" and the plates it observed on the counter at timestep 30, the model predicts the most likely next token is "kitchen."</p>
<p>with prediction objectives) from learning to act given that model (reinforcement learning with task rewards).Future prediction provides a rich grounding signal for learning what language utterances mean, which in turn equip the agent with a richer understanding of the world to solve complex tasks.Intuitively, the utterance "I put the bowls away" helps agents make better predictions about future observations (i.e., that if it opens the cabinet, it will observe the bowls there).Prior knowledge such as "wrenches can be used to tighten nuts" helps agents predict environment dynamics.</p>
<p>We can also formulate instruction following predictivelyinstructions help agents predict how they will be rewarded.</p>
<p>World models are well-studied in model-based RL (Ha &amp; Schmidhuber, 2018;Hafner et al., 2023)-our goal is not to introduce a new world modeling architecture or to demonstrate that world models can be augmented with language, but to investigate whether learning languageconditioned world models enable agents to scale to more diverse language use, compared to language-conditioned policies.First, we explore the design space for augmenting model-based agents with language, demonstrating that many ways of fusing language and vision in a world model underperform the simple and effective architecture in Dynalang.Then, we investigate whether Dynalang can indeed learn from diverse kinds of language, using language beyond instructions to better solve tasks, while maintaining strong performance on instruction following benchmarks.</p>
<p>Our evaluation focuses on scaling up language complexity in a controlled way (as opposed to task or visual complexity) across a wide range of simulated environments.We build a home cleanup environment focused on testing agents' ability to understand diverse language types, HomeGrid, where Dynalang learns to use language hints about future observations, environment dynamics, and corrections, outperform-ing strong model-free language-conditioned policies whose performance degrades with more diverse language.On the Messenger benchmark (Hanjie et al., 2021), we show that Dynalang can read game manuals to fit the most challenging stage of the game, outperforming task-specific architectures.</p>
<p>In vision-language navigation (Krantz et al., 2020), we show that Dynalang can also follow instructions in a visually and linguistically complex domain.</p>
<p>Additionally, we explore whether Dynalang enables additional scalability benefits and capabilities due to learning a generative model.While our previous experiments train agents from scratch, pretraining on offline text data is known to be an important ingredient for scaling models to opendomain language.We validate that we can pretrain Dynalang on single-modality text-only data without actions or rewards at a small scale, demonstrating that both in-domain text and a general-domain text dataset of ∼500M tokens improves downstream RL performance.Finally, we explore whether we can leverage the generative language-vision world model to generate language, enabling the agent to speak.In our framework, what the agent sees can inform future token predictions; by augmenting the action space with language and regularizing it towards the generative model, we demonstrate that Dynalang can gather information to answer questions about a simple visual environment.</p>
<p>Our contributions:</p>
<p>• We propose that predicting the future in a multimodal world model allows embodied agents to ground diverse types of language to visual experience.We implement Dynalang to instantiate this idea.</p>
<p>• We explore the design space for fusing language and vision in a world model, finding a simple and effective architecture.</p>
<p>LangRoom HomeGrid</p>
<p>Messenger Habitat</p>
<p>Environment</p>
<p>Inputs</p>
<p>Actions</p>
<p>Pixels Instructions</p>
<p>Walk into the living room and turn right.Stop by the end table.</p>
<p>Motor Stop</p>
<p>Positions Rules Figure 2. We consider a range of environments that feature visual inputs and diverse types of language.We introduce HomeGrid, a challenging visual gridworld with instructions and diverse hints.Messenger is a benchmark with symbolic inputs and hundreds of human-written game manuals that require multi-hop reasoning.Habitat simulates photorealistic 3D homes for vision-language navigation in hundreds of scenes and crowdsourced language.LangRoom is a simple visual grid world with partial observability, where the agent needs to produce both motor actions and language.</p>
<p>• We find that the performance of language-conditioned policies degrades when faced with more diverse language, while Dynalang learns to utilize many kinds of language to excel on a broad range of tasks.</p>
<p>• We show that learning a generative model of language and vision enables additional capabilities, such as embodied language generation and text-only pretraining without actions or rewards.</p>
<p>Related Work</p>
<p>Much work has focused on teaching reinforcement learning agents to utilize language to solve tasks by directly conditioning policies on language (Lynch &amp; Sermanet, 2021;Shridhar et al., 2022;Abramson et al., 2020).More similar to our work, recent work proposes text-conditioning a video model trained on expert demonstrations and using the model for planning (Du et al., 2023b;Yang et al., 2023).However, language in these settings has thus far been limited to short instructions, and only a few works investigate how RL agents can learn from other kinds of language like descriptions of how the world works, in simple settings (Zhong et al., 2020;Hanjie et al., 2021).In reality, human language is far richer than imperative commands.Other work looks to diversify the utterances that agents can understand with supervised learning on human-annotated or expert datasets in embodied environments (Das et al., 2018;Thomason et al., 2019;Bara et al., 2021) or pretrained large language models (LLMs) (Huang et al., 2022a;Li et al., 2022;Ahn et al., 2022;Driess et al., 2023).These works approach the realism of natural language in the diversity of utterances and roles of language in the world they consider.However, supervised approaches rely on expensive human data (often with aligned language annotations and expert demonstrations), and both these approaches have limited ability to improve their behaviors and language understanding online.</p>
<p>In contrast to previous RL agents, Dynalang takes a step towards more diverse language understanding by making predictions in a world model, providing a learning signal that is both rich (unlike rewards), self-supervised (unlike demonstrations), and compatible with offline pretraining.We refer to Appendix C for a more detailed discussion of related work.</p>
<p>Dynalang</p>
<p>Dynalang utilizes diverse types of language in visual environments by encoding multiple modalities into learned representations and then predicting the sequence of future representations given actions.For our algorithm, we build on the model-based algorithm DreamerV3 (Hafner et al., 2023)  environments with multimodal observations o t = (x t , l t ), containing both visual x t and textual input l t at each time step.The agent's goal is to choose actions that maximize the expected discounted sum of rewards E[ T t=1 γ t r t ], where γ &lt; 1 is a discount factor, T is the episode length, and c T = 0 signals the episode end.In most of our experiments, the actions a t are integers in a categorical action space.However, we also consider factorized action spaces where the agent outputs both a discrete movement command and a language token at each time step.</p>
<p>Multimodal alignment While previous work assumes that language such as instructions arrive at the beginning of an episode, we consider a diverse range of environments, summarized in Figure 2, where agents receive a continuous stream of video and text observations.For humans, reading, listening, and speaking extends over time, during which we receive new visual inputs and can perform motor actions.Analogously, at each time step we provide Dynalang with one video frame x t and one language token l t as input (with zero or padding if no video frame or token is available at that time step), and the agent outputs one motor action (and in applicable environments, a language token).While a natural question is whether the language token l t needs to be semantically aligned with the video frame x t , Dynalang does not assume or require that modalities are aligned.Intuitively, the prediction problem at each time step is to predict the future given past inputs from all modalities, and it does not matter how modalities were aligned in the past as long as information from each modality is represented in the model state.We can thus use a simple architecture that does not require more complex temporal segmentation, while outperforming other ways of fusing modalities (Section 4.1) and enabling language model-style pretraining (Section 4.6).</p>
<p>World Model Learning</p>
<p>The world model learns representations of all sensory modalities that the agent receives and then predicts the sequence of these latent representations given actions.Predicting future representations not only provides a rich learning signal to ground language in visual experience but also allows planning and policy optimization from imagined sequences.The world model is shown in Figure 3a.At each time step, it receives an image x t , a language token l t , and an action a t .The image and language observations are compressed into a discrete representation z t and fed together with the action into the sequence model to predict the next representation ẑt+1 .The multimodal world model consists of the following components, where h t is a recurrent state:
Sequence model: ẑt , h t = seq(z t−1 , h t−1 , a t−1 )
Multimodal encoder:
z t ∼ enc(x t , l t , h t ) Multimodal decoder: xt , lt , rt , ĉt = dec(z t , h t )
We implement the world model as a Recurrent State Space Model (RSSM Hafner et al., 2018), where the sequence model is implemented as GRU (Cho et al., 2014) with recurrent state h t , but other sequence models such as Transformers can also be used as the backbone (Robine et al., 2023).</p>
<p>The decoder is trained to reconstruct observations and other information, thus shaping the model representations.The world model is trained jointly to minimize a representation learning loss L repr and a future prediction loss L pred , which we describe below.</p>
<p>Multimodal representations</p>
<p>The world model learns to compress inputs images x t and language tokens l t into stochastic latent representations z t through a variational autoencoding objective (Kingma &amp; Welling, 2013;Rezende et al., 2014).Reconstructing the input observations encourages the model to compress information from all modalities into its representations.We also predict the reward, rt , and whether the episode continues, ĉt , so that the policy can be learned directly on top of the latent representations, as discussed in the next section.Finally, the representations are regularized towards the predicted distribution over ẑt as a prior, essentially regularizing the representations to be predictable.We denote the categorical cross entropy loss as catxent, the binary cross entropy loss as binxent, the stop gradient operator as sg, and β reg = 0.1 is a hyperparameter.The representation learning loss L repr is thus the sum of:
a 2 z 2 a 1 z 1 a 3 z 3 ẑ 2 ẑ 3 ẑ 4 r ̂2 r ̂3 r ̂4 World Model x 2 l 2 x 1 l 1 x 3 l 3 (a) World Model Learning World Model r ̂2 v 2 a 2 a 1 z 1 ẑ 2 r ̂3 v 3 a 3 ẑ 3 r ̂4 v 4 a 4 ẑ 4 x 1 l 1(
Image loss:
L x = ∥x t − x t ∥ 2 2
Language loss:
L l = catxent( lt , l t )
Reward loss:
L r = catxent(r t , twohot(r t ))
Continue loss:
L c = binxent(ĉ t , c t )
Regularizer:
L reg = β reg max(1, D KL [z t || sg(ẑ t )])
We choose a strided CNN image encoder, a strided CNN as image decoder, and MLPs for all other model components.We evaluate our method both with one-hot token observations (i.e., learning the embeddings from scratch) and pretrained embeddings from T5 (Raffel et al., 2020).</p>
<p>One-hot representations are reconstructed with the cross entropy loss above and pretrained embeddings are reconstructed with a squared error.For more details on world model learning, refer to Appendix A.</p>
<p>Future prediction</p>
<p>The world model learns to predict the sequence of multimodal representations, which enables it to plan and ground language.The sequence model produces ẑt from the current model state (z t−1 , h t−1 ) and the current action a t−1 , which is trained to match the actual representation at the next timestep z t .Concretely, the future prediction objective is:
Prediction loss: L pred = β pred max(1, D KL [sg(z t )||ẑ t ])
where the gradient around the target distribution for z t is stopped since it is also a learned representation and β pred = 0.5 is a hyperparameter.Intuitively, the codes z t contains both information from current observation and additional information that may be required to predict the reward and episode continuation.By training the world model to make predictions ẑt of its future representations, it effectively learns to predict future images, language, and rewards, encouraging the agent to extract information from language and learn the correlations between its multiple modalities.For example, when the language input describes that "the book is in the bedroom" and the agent later on visually observes the book, the agent will learn this multimodal association even if the reward signal does not directly relate the two.The world model is trained to optimize the overall loss L repr + L pred with respect to all its parameters.</p>
<p>Single-Modality Pretraining One potential benefit of separating world modeling from policy learning is that the world model can be trained offline, benefitting from largescale text-only and video-only datasets without actions.To pretrain the world model with text-only data as in Section 4.6, we zero out the image and action inputs and set the image, reward, and continuation decoder loss coefficients to 0 so the pretraining focuses on learning to represent text and text dynamics (i.e.language modeling).Dynalang can then be finetuned on experience with all modalities (language, images, and actions) by initializing the actor and critic from scratch, while continuing to train the world model.Note that unlike the typical language modeling objective, the model is not explicitly trained to predict the next token from the prefix, except through the next-representation prediction.</p>
<p>Policy Learning</p>
<p>To select actions, we train an actor critic algorithm (Williams, 1992) purely from imagined sequences of multimodal representations (Sutton, 1991), as shown in Figure 3b.</p>
<p>The critic estimates the discounted sum of future rewards for each state to guide actor learning.Both networks are MLPs:
Actor net: π(a t |h t , z t ) Critic net: V(h t , z t )
We do not modify the policy learning algorithm of Dream-erV3 and refer to Appendix B for details.</p>
<p>Experiments</p>
<p>Our experiments test the following hypotheses:</p>
<p>H1) Aligning image and language as single (image, token) pairs per timestep outperforms other methods for incorporating language into DreamerV3 (Section 4.1).</p>
<p>H2) Dynalang can better utilize diverse types of language to improve task performance over language-conditioned policies.To test this, we investigate whether Dynalang performance improves when provided with different kinds of language hints in HomeGrid (Section 4.2) and game manuals in Messenger (Section 4.3), compared to model-free RL baselines.</p>
<p>H3) Incorporating instructions into a world model is no worse than directly learning a language-conditioned policy.To test this, we compare performance to baselines with task-only language in HomeGrid and on vision-language navigation (Section 4.4).</p>
<p>H4) The multimodal generative model enables Dynalang to handle tasks that require grounded language generation (Section 4.5) and pretraining on offline text-only data (Section 4.6).</p>
<p>Language encodings</p>
<p>We tokenize all text with the T5 tokenizer (Raffel et al., 2020), with a vocabulary size of 32,100.In HomeGrid we use one-hot token encodings.In Messenger and VLN-CE, where agents must generalize to synonyms and linguistic variations, we embed each sentence with T5-small (60M parameters) and use the last hidden layer representation for each token.</p>
<p>Baselines We compare against two off-policy modelfree RL baselines: IMPALA (Espeholt et al., 2018) and R2D2 (Kapturowski et al., 2019).The architecture for both algorithms consists of an LSTM that takes in input embeddings from a CNN image encoder and an MLP language encoder.We use the implementations from the SeedRL repository (Espeholt et al., 2019).We pass the same language observations to the baselines as to our method (token embeddings or one-hot encodings).We also try providing the baselines with sentence embeddings from a pretrained all-distilroberta-v1 model from the Sentence Transformers library (Reimers &amp; Gurevych, 2019) and did not find a consistent improvement across our tasks.Both baseline models are ∼10M parameters, and we did not find that these models benefit from scaling parameter count.</p>
<p>Aligning Language, Vision, and Action in a World Model</p>
<p>First, we isolate the effect of our design choices for the multimodal architecture in Dynalang from the base DreamerV3 architecture by comparing to other ways of conditioning DreamerV3 on language.We use Messenger's Stage 1 as a testbed, where the agent must learn to read a text manual in order to achieve high reward (see Section 4.3 for details).We implement several common language conditioning methods in DreamerV3 from previous work, comparing their performance to Dynalang in Figure 4:</p>
<ol>
<li>Language-Conditioned Policy Embed tokens with a GRU and condition the policy on the final hidden state.This baseline is similar to how model-free approaches implement language conditioning (Shridhar et al., 2022), but the agent still learns to model images with a world model.This ablates the effect of learning joint representations of language and images in the world model.</li>
</ol>
<p>Sentence Embed</p>
<p>Input text into the world model one sentence at a time, using SentenceBERT to embed each sentence of the manual.This ablates the effect of inputting one token per timestep.</p>
<p>T5 with Image Adapter</p>
<p>Train the image encoder to map image features to the embedding space of a pretrained T5 model, following multimodal models such as LLaVA (Liu et al., 2023).This allows the agent to leverage the pretrained representations of the LLM.We map the CNN output at each timestep to 10 language tokens with an MLP, attend over the image tokens and the tokens of the language input with a frozen T5 encoder, and condition the world model on the last hidden state of T5.</p>
<ol>
<li>
<p>T5 with Cross-Attention Embed the entire manual with T5 and then fuse with the image observation by attending to the sequence of token embeddings with the image embedding output by the image encoder, similar to the original Messenger baseline (Hanjie et al., 2021).</p>
</li>
<li>
<p>Finetuned T5 with Cross-Att.(3.), but finetune T5.</p>
</li>
<li>
<p>T5 with Two-Way Cross-Att.(3.), but additionally map images to a fixed number of latents and attend to it with the pooled text embedding, following multimodal models such as Lu et al. (2016).</p>
</li>
</ol>
<p>We find that Dynalang outperforms these alternatives even with token embeddings initialized from scratch while also being simple and efficient to train, supporting H1.Dynalang outperforms language-conditioned IMPALA and R2D2, as well as the task-specific EMMA architecture, fitting the most complex stage of the game where other methods fail to achieve non-trivial performance.</p>
<p>HomeGrid: Language Hints</p>
<p>As most standard RL benchmarks do not provide language beyond instructions, we introduce a new environment, HomeGrid, that evaluates how well agents can ground diverse types of language to solve tasks.HomeGrid is a multitask gridworld where agents receive language task specifications and language hints.Hints provide prior knowledge about world dynamics, information about world state, or corrections that assist the agent.The agent can otherwise acquire the same information through its own interaction with the environment, as in standard RL.Agents can achieve higher performance if they learn to ground language.</p>
<p>There are five task types involving objects and trash bins (find, get, clean up, rearrange, open) for a total of 38 tasks.</p>
<p>Agents get pixel observations with a partially observed view of the environment and can move and interact with objects and trash bins.Object locations, bin locations, and bin dynamics (i.e., which action correctly opens the bin) are randomized on each episode.Objects are also randomly moved throughout the episode.Agents receive task specifications in language.When a task is completed, the agent gets a reward of 1 and a new task is sampled.To achieve a high score, agents must complete as many tasks as possible before the episode terminates in 100 steps.We also provide hints at random points throughout the episode that are provided token-by-token while the agent continues to act.We script the following language hints, with examples shown in Figure D.1 in the appendix:</p>
<p>• Future observations Descriptions of where objects are in the world.Without language, the agent must explore the environment to find objects.</p>
<p>• Dynamics Descriptions of the correct action to open each trash bin.Without language, the agent can try all the different actions, although taking the wrong action can disable the trash can for a variable number of timesteps.</p>
<p>• Corrections Tell the agent "no, turn around" when its distance to the current goal object is increasing.Without language, the agent must explore on its own.Section 4.1 shows that Dynalang benefits from all kinds of language, achieving higher scores with hints relative to just using instructions.Notably, agents never receive direct supervision about what the hints mean in HomeGrid, and hints are often far removed from the objects or observations they refer to.We show the agent's imagined rollouts in Appendix F, which provide qualitative evidence that Dynalang learns to ground language to the environment purely via the future prediction objective.IMPALA struggles to learn the task at all, while R2D2 learns to use the types of language that are correlated with reward (tasks and corrections).Interestingly, we find that while R2D2's performance drops as it receives more diverse language, while Dynalang improves with more language information, supporting H2 and our initial motivation that learning actions from diverse language is a difficult learning problem.</p>
<p>Dynalang also outperforms baselines even with task-only language, supporting H3.Language-conditioned policies in prior work often rely on supervised learning on expensive language-action datasets; while Dynalang may also benefit from offline data to further boost performance, it can achieve non-trivial performance even when learning from scratch.</p>
<p>Messenger: Game Manuals</p>
<p>Next, we evaluate Dynalang on the Messenger game environment (Hanjie et al., 2021), which tests whether agents can read text manuals describing game dynamics to achieve high scores.In Messenger, the agent must retrieve a message from one of the entities in the environment and deliver it to another entity, while avoiding enemies.In each episode, the agent receives a manual describing the randomized entity roles and movement dynamics as a series of rules, requiring multi-hop reasoning over both visual and text inputs (e.g.combining the manual information that the goal entity is a "fleeing wizard" with observations of entity identities and movement dynamics).Messenger has three stages of increasing length and difficulty.</p>
<p>In addition to the baselines above, we compare Dynalang to EMMA, the original baseline for the benchmark that uses a specialized grid-based architecture and learns a languageconditioned policy with PPO (Schulman et al., 2017).As seen in Figure 6, Dynalang achieves higher performance and learns more efficiently than EMMA, IMPALA and R2D2.While other methods fail to fit S3 at all, our method learns to interpret the manuals to achieve non-trivial performance on the most challenging stage, further supporting H2.</p>
<p>Vision-Language Navigation: Instruction Following</p>
<p>To evaluate how Dynalang performs in more complex environments, we apply it to the popular Vision-Language Navigation (VLN) (Anderson et al., 2018) benchmark.Agents must navigate Matterport3D panoramas captured in real homes (Chang et al., 2017), following crowd-sourced natural language instructions that indicate where the agent should navigate to, such as "Go past the bed to the door.Enter the hallway,..." We focus on the more challenging variant, VLN in Continuous Environments (VLN-CE) (Krantz et al., 2020), in which agents take low-level discrete actions (left, forward, ...) rather than relying on a waypoint navigation graph as in the original VLN task.In this task, our goal is to demonstrate that Dynalang can learn to follow language instructions (e.g. via predicting future rewards).</p>
<p>Each episode randomly samples a language instruction and corresponding scene from the training dataset out of 10,819 unique instructions total.The agent gets a dense reward based on relative position to the current goal, a success reward when taking the stop action at the correct location, and a penalty otherwise.</p>
<p>As shown in Figure 7, Dynalang succeeds at significantly more instructions compared to the model-free R2D2 baseline, supporting H3.While Dynalang successfully learns to ground instructions from scratch, performance is not yet competitive with state-of-the-art VLN methods (many of which use expert demonstrations or specialized architectures), and further work is needed to close the gap.</p>
<p>LangRoom: Embodied Question Answering</p>
<p>Next, show how Dynalang can also generate language in the same framework.On the other benchmarks, language is used to inform agents' future predictions about the world, but perception can also inform future predictions about what might be said.For example, agents could predict that they will hear descriptive utterances such as "the stove is on" that are consistent with its own observations of the burner producing flames.</p>
<p>We introduce the LangRoom embodied question answering environment to demonstrate a proof-of-concept of this capability.We expand the action space to include language 0 1.5M 3M by allowing the agent to output one language token per timestep as an action.The environment contains a room with objects with fixed positions but randomized colors.The language observations from the environment are questions "what color is the <object>?".The agent must move to the object and emit a language action saying the correct color.Results are shown in Figure 8.The agent learns to answer more questions correctly with task reward by taking information gathering actions to observe the color of the object and generating text consistent with the world state.</p>
<p>Critically, learning an generative model of language is important for scaling up to larger vocabularies, as seen in Figure 9, supporting H4.Naively increasing the vocabulary size to 10k (by adding dummy tokens) fails to learn, likely because selecting actions from such a large action space with RL is intractable.We recover performance by using the world model to guide language generation, regularizing the language action towards the predicted next token in the world model by adding D KL [π(l act t | h t , z t )|| sg(p( lt+1 | ĥt+1 , ẑt+1 ))] to the entropy regularizer.Intuitively, this regularizes the generated language actions towards valid and likely utterances in the current context, similar to how LLMs are trained with RL (Ziegler et al., 2019).</p>
<p>Text-only Pretraining</p>
<p>Our experiments thus far have investigated how Dynalang can learn from multimodal experience online, demonstrating that the world modeling objective enables the agent to understand language in the context of vision and action even with no prior or pretrained initialization.However, we expect that large-scale offline training will be necessary to scale to realistic settings.Thus, in this section, we investigate whether the generative model in Dynalang can be pretrained on single-modality data to benefit downstream learning.</p>
<p>To evaluate this capability, we zero out the other modality and action inputs and pretrain Dynalang from scratch on text-only corpora: (1) in-domain text with manuals from Messenger S2 games (2) domain-general text with TinyStories (Eldan &amp; Li, 2023), a dataset of 2M short stories (∼500M tokens) generated by GPT-4.We evaluate on Messenger S2, where models that learn to embed one-hot token observations from scratch struggle to learn the complex language in S2 without pretraining on S1.We compare S2 task performance with learned embeddings to using pretrained T5 embeddings, training all methods from scratch on S2 without initializing from S1. Results are shown in Figure 10.Dynalang is able to benefit from offline pretraining, supporting H4.Even a small amount of in-domain text closes much of the gap between training text embeddings from scratch and using T5 embeddings.Furthermore, pretraining on TinyStories exceeds the final performance of using T5 embeddings, likely because pretraining allows the model to learn text dynamics offline rather than during environment interaction.Although the model is not trained explicitly to do language modeling except through next-representation prediction, we show generated language samples in Appendix E. These results suggest that our paradigm can leverage the benefits of large-scale offline pretraining, providing a way to unify offline and online data with language, vision, and action in a single agent.</p>
<p>Conclusion</p>
<p>Taken together, our results suggest a way forward for multimodal agents that can understand language and vision and act in the world.We presented Dynalang, an agent that does so through future prediction as a rich self-supervised objective.In contrast to model-free methods that struggle with increased language perplexity, we demonstrated in four diverse environments how Dynalang can understand various types of language and what they mean in the world, even when learning from scratch from its own experiences.Additionally, we show the same world model architecture can be pretrained on offline text data in a way that benefits downstream learning from experience, paving the way towards a self-improving multimodal agent that interacts with humans in the world.</p>
<p>A World Model Learning</p>
<p>Representation Learning The discrete codes z t are vectors of one-hot categoricals that are sampled during the forward pass and optimized using straight-through gradients on the backward pass (Bengio et al., 2013;Hafner et al., 2020).</p>
<p>Two-hot Reward Prediction</p>
<p>We follow DreamerV3 in predicting rewards using a softmax classifier with exponentially spaced bins that regresses the twohot encoding of the real-valued rewards and in clipping the regularizer at 1 free nat (Kingma et al., 2016).The two-hot regression decouples the gradient scale from the arbitrary scale of the rewards and free nats prevent over-regularization, known as posterior collapse.</p>
<p>B Actor Critic Learning</p>
<p>Because we optimize the policy from imagined rollouts, all involved quantities are predictions rather than environment observations.For simplicity, we omit the hats from the notation now and e.g.write z t instead of ẑt .To train the actor and critic networks, we predict a sequence of T = 15 representations z t by sampling from the world model and the actor network.The sequences start at all representations computed from the world model training step.From a sequence of representations z t and recurrent states h t , we fill in the rewards r t and episode continuation flags c t by applying their two MLPs, without invoking the image or language decoders.Given the quantities, we compute a λ-return (Sutton &amp; Barto, 2018) that estimates the discounted sum of future rewards:
R t = r t + γc t (1 − λ)V (z t+1 , h t+1 ) + λR t+1 R T . = V (z T , h T ) (1)
The return estimate R t serves as a prediction target for the critic network, which uses discrete regression using a categorical cross entropy loss towards the twohot encoded targets.The actor network is trained to maximize the return estimates subject to an entropy regularizer on the action distribution:
L V = catxent(V t (h t , z t ), sg(twohot(R t ))) L π = − sg(R t − V (z t , h t ))/ max(1, S) log π(a t | h t , z t ) − ηE &lt; H &gt;&lt; <a href="2">π(a t | h t , z t )</a>
To trade off the two actor loss terms without having to tune hyperparameters, the actor loss normalized returns that exceed a magnitude of 1 are normalized by an exponential moving average of the 5th to 95th percentile range of returns, S = ema(per(R t , 95) − per(R t , 5)).When interacting with the environment, we choose actions by incorporating the new observation into the world model representation and then sampling from the actor network.</p>
<p>C Detailed Related Work</p>
<p>Language and Embodied Agents Language can be used in embodied settings in a variety of ways (Luketina et al., 2019).</p>
<p>In instruction following, agents must interpret language specifications of high-level goals or step-by-step guidance (Branavan et al., 2010;Andreas &amp; Klein, 2015;Anderson et al., 2018;Shridhar et al., 2020a;Lynch &amp; Sermanet, 2021).Language can also be used as an abstraction to assist learning or decision-making, e.g. for planning by decomposing high-level tasks into low-level subgoals (Andreas et al., 2017;Jiang et al., 2019;Ahn et al., 2022;Huang et al., 2022a;Li et al., 2022;Sharma et al., 2021) or guiding exploration (Mirchandani et al., 2021;Tam et al., 2022;Mu et al., 2022;Du et al., 2023a).Instead of using language as a scaffolding mechanism for planning or exploration, our model treats language as another modality in observation space and plans in latent space.Additionally, human language is far richer than imperative commands.Other work looks to diversify the utterances that agents can understand with supervised learning on human-annotated or expert datasets in embodied environments, e.g. to understand more complex instructions (Ku et al., 2020;Shridhar et al., 2020a) or environment descriptions (Zhong et al., 2022), ask for assistance in simulated navigation or household tasks (Thomason et al., 2019;Padmakumar et al., 2022;Abramson et al., 2020), answer questions (Das et al., 2018), communicate domain knowledge (Eisenstein et al., 2009;Branavan et al., 2010;Narasimhan et al., 2018;Zhong et al., 2020), or collaborate dynamically on a shared goal (Bara et al., 2021).While these works consider more realistic natural language, they often rely on supervised approaches and expensive human data (often with the assumption of aligned language annotations and expert demonstrations).</p>
<p>Our work investigates how to unify these settings so that agents can learn from all kinds of language they might encounter in the world, including instructions and descriptions.While most of these works directly condition policies on language to generate actions (model-free), our algorithm uses language for future prediction, learning a world model that is then used for planning and acting.</p>
<p>Multimodal Models Developing agents that can leverage both vision and text observations requires training multimodal models.Previous works develop vision-language models (VLMs) by augmenting LLMs with visual encoders (Alayrac et al., 2022;Li et al., 2023b;Chen et al., 2022;Guo et al., 2023) or training models jointly over all modalities (Lu et al., 2022) However, because VLMs are prohibitively expensive to query and finetune, recent work on using VLMs as policies has focused on supervised learning from demonstrations (Driess et al., 2023;Jiang et al., 2022), rather than using them in embodied agents that can learn online.Reed et al. (2022) trains a multimodal embodied agent across various tasks, modalities, and embodiments by additionally learning to generate actions.Perhaps most similar, a recent line of work (Du et al., 2023b;Yang et al., 2023) trains text-conditioned video models for planning.Their approach works by using a video model trained on expert demonstrations to generate a video plan conditioned on a text instruction, and then imputing the actions to execute that plan.From a generative modeling perspective, our approach differs in that it also learns to generate language instead of being solely input text-conditioned, enabling text-only pretraining, interactive dialogue, and future possibilities for learning shared representations of text and video.Additionally beyond both VLMs and text-conditioned video models, our approach enables learning from online experience in addition to offline pretraining, allowing agents to improve their behaviors and understanding of the world autonomously rather than being inherently limited to an offline dataset.</p>
<p>Decision-making with Large Language Models Large language models (LLMs) learn about the world via next-token prediction on web-text, implicitly modeling world state (Li et al., 2021;2023c) and relations between concepts (Piantadosi &amp; Hill, 2022).When acting in purely text-based or symbolic environments, language models can be used as complete world models (Ammanabrolu &amp; Riedl, 2018;Singh et al., 2021).In visual environments, LLMs can be used to break down complex language context into simple instructions for low-level policies (Huang et al., 2022a;Li et al., 2022;Ahn et al., 2022), integrate knowledge from text (Wu et al., 2023), or directly serve as the policy (Driess et al., 2023;Wang et al., 2023;Carta et al., 2023).However, LLMs are not grounded to real environment observations and cannot directly take actions unless observations are translated to text (Shridhar et al., 2020b;Huang et al., 2022b;Dasgupta et al., 2023), and representing visual inputs as text is inherently low bandwidth.Additionally, while LLMs can be used as a prior over actions or observations (Li et al., 2023a), they are difficult to update with feedback from the environment except in limited cases (Carta et al., 2023;Dagan et al., 2023).In contrast, we learn a single multimodal world model from experience with autoregressive prediction on both text and images (predicting both modalities in the future from both modalities as input), thus grounding language to experience (Bisk et al., 2020).Our model can also be trained on text-only data as a language model or video-only data as a video prediction model.</p>
<p>D Environment Details</p>
<p>D.1 HomeGrid</p>
<p>The HomeGrid environment is a grid with different objects, receptacles, and rooms.Agents receive pixel observations of 3x3 grid cells centered on the current agent position.The action space is: movement (left, right, up, down), object interaction (pick up, drop), and trash bin interaction (get, pedal, grasp, lift).The agent can carry one object in its inventory by executing the pick up action in front of an object or the get action in front of a trash bin with an object inside.</p>
<p>There are three rooms (living room, dining room, kitchen) indicated by different flooring textures, three possible trash bin types with different colors (blue recycling, black trash, green compost) and four possible trash object types (bottle, fruit, papers, plates).Trash bins can be open, closed, or knocked over (represented visually as toppled over sideways).Each trash bin can be opened with a specific action that is randomly selected from {pedal, grasp, lift} in each episode.If agents apply the wrong action on a bin, it becomes broken and cannot be interacted with further until reset by the environment.When a trash bin is open, one object can be dropped into the bin with the drop action and the current object in the bin (if any) can be retrieved into the agent's inventory with get.</p>
<p>For each episode, the environment is randomly initialized with two objects and two trash bins in random positions.Trash bins are initialized in the open state with probability 0.5.One bin is irreversibly broken if the wrong action is applied and the other bin is reset after 5 timesteps if broken.At each timestep, each object is moved to a new position with probability 0.05 and new objects are spawned with probability 0.1 * num_remaining_unique_objects at a random position.</p>
<p>In our experiments, agents are evaluated on setups with different language inputs: task instructions, task instructions + dynamics, task instructions + future observations, and task instructions + corrections.Language for each type is generated with templates from the underlying environment state, with the following semantics: Language is provided to the agent one token per timestep.All language are provided while the agent acts and the environment state is changing, except for dynamics descriptions (which apply to the whole episode).For dynamics descriptions, we randomly shuffle all possible descriptions and input them to the agent in sequence up to a maximum of 28 tokens while the agent is fixed in place.For language provided during the episode, on each timestep, if there is not currently an utterance being provided to the agent, either (1) the task instruction is repeated, every 20 timesteps (2) an utterance describing one of the events that occurred at this timestep is provided (i.e.objects moved or spawned) (3) a description of future observations or dynamics is provided (4) a correction is provided, with probability 0.1.If there is a new task instruction (i.e. the agent just completed the last task), any currently streaming sentence will be interrupted and the agent will immediately receive the tokens of the new instruction.All evaluation setups share the same underlying environment dynamics and parameters (e.g. each trash bin must be operated with the correct action even if the agent does not receive hints about dynamics).</p>
<p>D.2 Messenger</p>
<p>Language in Messenger is generated from human-written templates, resulting in diverse sentences with multiple ways of referring to each entity and a total vocabulary size of 1,125.Observations are a symbolic grid of entity IDs, and the agent takes discrete actions to move.We input the manual into the world model token-by-token before the episode begins.</p>
<p>The EMMA baseline provides a gridworld-specific inductive bias that each text token should map to some region in the current observation, and assumes that the model has access to the spatial locations of entities in the scene.As in the original benchmark, we initialize all models from the converged model trained on the previous game stage.</p>
<p>D.3 VLN-CE</p>
<p>The best-performing methods on VLN-CE use expert demonstrations (An et al., 2023) or train navigation-specialized hierarchical agents (Krantz et al., 2021).The VLN-CE training set consists of 10,819 unique natural instructions total, spread across 61 scenes.The instruction and corresponding scene are randomly sampled per episode.In addition to language, the agent observes an egocentric RGB and depth image at each timestep.Agents have access to discrete low-level actions (moving forward 0.25 meters, turning left or right 15 degrees), as well as a stop action.Crucially, the agent must learn to take the stop action when it thinks it has reached the goal to indicate that it recognizes the goal position.This makes the task more challenging, as the agent must learn to only terminate the episode at the appropriate goal locations.The agent receives a dense reward at every timestep based on the delta in position from the goal.Following (Krantz et al., 2021), we provide an additional success reward of 1000 when the agent takes the stop action at the correct location, and a penalty of −10 when the agent takes the stop action elsewhere.</p>
<p>D.4 LangRoom</p>
<p>In LangRoom, the environment contains four objects in the corners of a room.The positions of the objects are fixed but the colors are randomized.The action space for the agent includes the four cardinal movement actions, stay, and 15 tokens that the agent can say.The language observations from the environment are questions "what color is the <object>?"followed by a random silence duration (allowing the agent to find out the answer), followed by the answer "it is <color>".After each question and answer, the colors are randomized and the environment asks a new question, up to a fixed episode length of 200 timesteps.Agents are rewarded +1 for saying the correct "<color>" token at the same timestep that the environment produces the "<color>" token, −0.1 for saying the wrong color at that timestep, −0.01 for speaking at other timesteps, and 0 for saying nothing.The agent only has a partial view over the environment, so it must move to the object before the environment starts prompting it for the answer.</p>
<p>E Text Pretraining: Text Generation Samples</p>
<p>Dynalang is not explicitly trained on the language modeling objective, but we can still generate text from the model by sampling rollouts from the world model and decoding the token from the latent representation at each timestep.Here, we show sampled 10-token generations conditioned on a prefix of 50 tokens for validation examples in TinyStories.In one rollout (top), the agent predicts the papers are on the table and correctly predicts it will get rewarded for picking it up.In the second rollout (bottom), it predicts that the bottle is on the table and that it will not get rewarded.</p>
<p>H.2 Model Scaling for Baselines</p>
<p>We find that scaling the baseline R2D2 and IMPALA models does not improve their performance.Stage 2 runs were initialized from scratch.</p>
<p>H.3 Auxiliary Reconstruction Loss for Baselines</p>
<p>We tried adding an auxiliary loss for reconstructing the visual and language observations at the current timestep.The loss was implemented by adding a linear layer that predicts each auxiliary target from the LSTM hidden state.The loss used is MSE (for continuous values) or cross-entropy (for discrete language vocab tokens).The auxiliary loss was added to the RL loss with a loss scale of 1.This did not meaningfully change performance.</p>
<p>0.0 1.5 3.0 4.5</p>
<p>Env Steps 1e7</p>
<p>I.2 Dynalang Hyperparameters</p>
<p>We use the default model hyperparameters for the XL DreamerV3 model unless otherwise specified below.For VLN, we use a larger GRU deterministic state and a bottleneck layer of size 1024 between timesteps.To process both one-hot and embedding language inputs, we use a 5-layer MLP with 1024 MLP units in each layer.All models were trained on NVIDIA A100 GPUs.</p>
<p>Figure 3 .
3
Figure 3.During world model learning, the model compresses observations of image frames and text to a latent representation.The model is trained to predict the next representation and reconstruct observations from the representation.During policy learning, imagined rollouts are sampled from the world model and the policy is trained to maximize imagined rewards.</p>
<p>-Att.Finetuned T5 + Cross-Att.T5 + Two-Way Cross-Att.</p>
<p>Figure 4 .
4
Figure 4. Comparison of ways to equip the world model with language inputs on Messenger S1.We compare ways of conditioning DreamerV3 on language and find that Dynalang substantially outperforms other model-based approaches, even those based on pretrained T5, despite training from scratch.</p>
<p>Figure 5 .Figure 6 .
56
Figure5.HomeGrid performance after 50M steps (2 seeds).Dynalang learns to use all types of language hints to score higher than when just provided with the task information, outperforming language-conditioned IM-PALA and R2D2, where we see performance decrease with language hints.</p>
<p>Figure 7 .Figure 8 .
78
Figure 7. VLN-CE results.(left) A portion of a trained agent trajectory, given the instruction "Exit the bedroom, go straight down the hallway, make a right into the doorway of the bathroom and stop".(right) Success rate during RL training, averaged across 3 seeds for Dynalang and 2 seeds for R2D2.</p>
<p>Figure D. 1 .
1
Figure D.1.HomeGrid provides language hints and task specifications.We show real trajectories from a trained agent.</p>
<p>Figure F.1.Imagined rollouts from the world model.Conditioned on a language description, the task, and the same action sequence, we sample rollouts of the world model's imagined trajectories.Since the papers and bottle can be in any of multiple possible locations in the living room, the model samples exhibit uncertainty over the possible futures.In one rollout (top), the agent predicts the papers are on the table and correctly predicts it will get rewarded for picking it up.In the second rollout (bottom), it predicts that the bottle is on the table and that it will not get rewarded.</p>
<p>Figure F. 1
1
Figure F.1 shows that we can interpret what the model has learned by rolling out the world model state into the future and reconstructing observations from the latent state, conditioned on some history.We can see that the model represents the information and correctly grounds it to observations: given the information that the papers and bottle are in the living room, different samples from the world model represent different possible futures, both of which are consistent with the text.The model also correctly predicts that in the future where the papers are on the table, it will receive a reward of +1 for doing a pickup action, and that it will not be rewarded if it picks up the bottle.</p>
<p>Figure H. 1 .
1
Figure H.1.Token vs. sentence embedding performance for IMPALA and R2D2 on all tasks, averaged across 3 seeds.Sentence embeddings help R2D2 perform better on Messenger S1 and S2 but does not help consistently across tasks and methods.</p>
<p>Figure H. 3 .
3
Figure H.3. Model scaling curves for IMPALA.</p>
<p>Figure H. 4 .
4
Figure H.4. Model-free R2D2 performance with an auxiliary reconstruction loss.</p>
<p>Dynalang define rewards r t , episode continue flag c t , images x t , language tokens l t , actions a t , model state (h t , z t ).while acting do Step environment r t , c t , x t , l t ← env(a t−1 ).Encode observations z t ∼ enc(x t , l t , h t ).Execute action a t ∼ π(a t | h t , z t ).Add transition (r t , c t , x t , l t , a t ) to replay buffer.while training do Draw batch {(r t , c t , x t , l t , a t )} from replay buffer.Use world model to compute multimodal representations z t , future predictions ẑt+1 , and decode xt , lt , rt , ĉt .Update world model to minimize L pred + L repr .Imagine rollouts from all z t using π.Update actor to minimize L π .Update critic to minimize L V .while text pretraining do Sample text batch {l t } from dataset.Create zero images x t and actions a t .Use world model to compute representations z t , future predictions ẑt+1 , and decode lt .Update world model to minimize L pred + L l .
Algorithm 1:and extend it to process and optionally produce lan-guage. The world model is trained from a replay buffer ofpast experience while the agent is interacting with the en-vironment. It can additionally be pretrained from text-onlydata. To select actions, we train an actor-critic model fromsequences of representations imagined by the world model.The algorithm is summarized in Algorithm 1.Problem setting To perform interactive tasks, an agentchooses actions a t to interact with an environment that re-sponds with rewards r t , a flag for whether the episode con-tinues c t , and observations o t . In this work, we consider</p>
<p>She wanted to climb down the hill.</s>friends and It was a steep tree, but she was not She wanted to own it, but it was too hot She thought it would be fun banana scary, andffy She wanted to skip through the.But Once upon Prompt: Once upon a time, there was a thoughtful girl named Sue.Sue loved to help her mom around the house.One day, her mom asked her to wipe the table after they ate their lunch.Sue was happy to help.As True: Sue was wiping the table, she saw Once upon a time, there was a little girl named Lucy.She had a pet cat named Tom.They loved to play together in the big green park near their house.One sunny day, they went to the park to play.True: While playing, Tom saw a big s Samples: </s> Once upon a time, there was scarf </s> " Jenny, you are my sweet.You must </s> Once heard a kind girl and asked Mom to store.He was very happy and ran to the store.Inside the store, Tim met True: a strange man.The man said, " Samples: a nice lady named Sue.</s>The thing the a tall named Max.</s>that the clever a girl friend named Sue.They said, " a big dog named Theffy.</s> said a new prize car two cars.things.</s>Prompt: Once upon a time, there was a big, heavy alligator.He lived near a small pond.He was very hungry and wanted to eat something.
Samples: they her big room. playly remembered her she was placing", she saw a she got being, she saw hera all she on she was organizing, she saw the pin case in the she was their best delicate turkey on, she saw True: pond. The alligator saw the bun Samples: flower. The bunny said, "Hello kitchen. He thisly and said, "This bunny and askede "Do, smell you sunflower. The bun said, "Stop, sunset Prompt: One day, a little bunny came close to the bunny. The bunny said, "
Prompt: Once upon a time, in a big forest, there lived a rhinoceros named Roxy.Roxy loved to climb.She climbed trees, rocks, and hills.One day, Roxy found an icy hill.True: She had never seen anything like it before.It Samples:</p>
<p>Model Size LSTM hidden size Language MLP size CNN hidden size Policy/Value Hidden Size Table H.1.R2D2 architecture sizes for model scaling experiment.Figure H.2. Model scaling curves for R2D2.Model Size LSTM hidden size Language MLP hidden size CNN hidden size Policy/Value Head Table H.2. IMPALA architecture sizes for model scaling experiment.
1.7M256256[16, 32, 32]None (linear)10M1024512[16, 32, 32][512]37M20481024[64, 64, 64][1024, 1024]Score1.0 0.5 0.0 0.5 1.00 500K 1M 1.5M Env Steps Stage 1Env Steps 0 10M 20M 1.0 Stage 2 1.0 0.5 0.0 0.5R2D2 (1.7M)R2D2 (10M)R2D2 (37M)1.5M512[64][16, 32, 32]None (linear)8.8M1024[512][16, 32, 32][512]34M2048[1024][16, 32, 32][1024, 1024]</p>
<p>Table I.3.Dynalang hyperparameters and training information for each environment.
HomeGridMsgr S1Msgr S2Msgr S3VLNLangRoomTotal model parameters281M148M148M148M268M243MLanguage inputsOne-hotT5 EmbedT5 EmbedT5 EmbedT5 EmbedOne-hotVocabulary size32100n/an/an/an/a15Language MLP layers555555Language MLP units102410241024102410241024Image inputPixelSymbolSymbolSymbolPixelPixelImage size(64, 64, 3)(16, 16, 17) (16, 16, 17) (16, 16, 17) (64, 64, 3)(64, 64, 3)Train ratio326464323216Batch size16162424816Batch length25625651251225664GRU recurrent units409640964096409681926144Bottleneck unitsn/an/an/an/a10242048Env steps50M1M25M50M30M45MNumber of envs6616166684Training time (GPU days)3.752.51624162
AcknowledgmentsWe thank Aditi Raghunathan, Michael Chang, Sanjay Subramanian, and Nicholas Tomlin for helpful discussions, and Kuba Grudzien, Jacob Steinhardt, Meena Jagadeesan, and Alex Wei for draft feedback.We thank the TPU Research Cloud (TRC) program and Danat Pomeranets from Google for access to compute resources.This work was funded in part by The Berkeley Center for Human Compatible AI (CHAI) and the Office of Naval Research (ONR-YIP).Impact StatementThis paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</s> When taking a small, thin thing he </s> The.lesson its if can improve and Prompt: Once upon a time, there was a little boy named Tom.He loved to play with his red ball.One sunny day, Tom went outside to play with his ball in the land near his home.Tom kicked the ball high in True: the sky.The ball went far, far away Samples: the sky and ity it."</s>Once day, the air and loved then it rain outside.We can the sky, but was enormous diary to with baby the sky.</s> red ball went and all game, the air and ran after to catchMoo.His was Prompt: Once upon a time, there was a girl named Mia.Mia loved her jewelry.She had a big box full of pretty things.She liked to wear them all day.But at night, she had to sleep.One True: day, Mia met a talking cat named Samples: day, shea was mad.She did not want night, shea socks out wanted to hurt up.day, could not find her skirt dress She day, hera's mom came to her. day, Miaa fell her hair could.It Prompt: Once upon a time, there was a little boy named Tom.Tom had a special belt that he loved to wear.One day, he could not find his belt and felt very sad.Tom's mom saw him and True: asked, "Why are you sad, Tom?" Samples: frustrated and asked him what was rude.</s>Once upon asked, "Why are you sad, Tom?"</s> asked, "Howeny, I did, get said, "Don't worry, Tom.We said, "To tree, you look be in Prompt: Once upon a time, in a small house, there lived a kind and honest girl named Lily.She loved to bake cakes for her friends and family.One day, she made a big, yummy cake for her best friend True: 's birthday.Lily carefully put the cake Samples: , Ben.</s> Tom went Mike opened the and, , Tom.</s> Oneo decided the biggest ow , Tim.</s> Once upon a time, there , Tim.</s> lady.</s></s> and Lily , Tom.</s> Once upon a time, there Prompt: One day, a young boy named Tim found a dull, round rock.He picked it up and looked at it.He thought it was not very fun, but he took it with him to the park.At the park, Tim True: saw a girl named Sue.She had Samples: he met.favorite friend He put it his to met a girl named Sue.Sue saw the ball saw a stick top Sam.He kept playing with played with his friends and but they friends!"</s>Li met a girl named Lily.</s>ly saw Prompt: Once upon a time, there was a little boy named Tim.Tim loved candy more than anything else.One day, Tim saw a big candy
J Abramson, A Ahuja, I Barr, A Brussee, F Carnevale, M Cassin, R Chhaparia, S Clark, B Damoc, A Dudzik, arXiv:2012.05672Imitating interactive intelligence. 2020arXiv preprint</p>
<p>Do as I can and not as I say: Grounding language in robotic affordances. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, K Gopalakrishnan, K Hausman, A Herzog, D Ho, J Hsu, J Ibarz, B Ichter, A Irpan, E Jang, R J Ruano, K Jeffrey, S Jesmonth, N Joshi, R Julian, D Kalashnikov, Y Kuang, K.-H Lee, S Levine, Y Lu, L Luu, C Parada, P Pastor, J Quiambao, K Rao, J Rettinghouse, D Reyes, P Sermanet, N Sievers, C Tan, A Toshev, V Vanhoucke, F Xia, T Xiao, P Xu, S Xu, M Yan, arXivpreprintarXiv:2204.016912022</p>
<p>J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, arXiv:2204.14198A visual language model for fewshot learning. 2022arXiv preprint</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. P Ammanabrolu, M O Riedl, arXiv:1812.016282018arXiv preprint</p>
<p>D An, H Wang, W Wang, Z Wang, Y Huang, K He, L Wang, Etpnav, arXiv:2304.03047Evolving topological planning for vision-language navigation in continuous environments. 2023arXiv preprint</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N Sünderhauf, I D Reid, S Gould, Van Den, A Hengel, 10.1109/CVPR.2018.003872018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018. Salt Lake City, UT, USAIEEE Computer SocietyJune 18-22, 2018. 2018</p>
<p>Alignment-based compositional semantics for instruction following. Andreas , J Klein, D , 10.18653/v1/D15-1138Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>Modular multitask reinforcement learning with policy sketches. J Andreas, D Klein, S Levine, Proceedings of the 34th International Conference on Machine Learning. D Precup, Y W Teh, the 34th International Conference on Machine LearningSydney, NSW, AustraliaPMLR2017. 6-11 August 2017. 201770of Proceedings of Machine Learning Research</p>
<p>Theory of mind modeling for situated dialogue in collaborative tasks. C.-P Bara, S Ch-Wang, J Chai, Mindcraft, 10.18653/v1/2021.emnlp-main.85Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. M.-F Moens, X Huang, L Specia, S W Yih, -T, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 2021Online and Punta Cana</p>
<p>Estimating or propagating gradients through stochastic neurons for conditional computation. Y Bengio, N Léonard, A Courville, arXiv:1308.34322013arXiv preprint</p>
<p>Experience grounds language. Y Bisk, A Holtzman, J Thomason, J Andreas, Y Bengio, J Chai, M Lapata, A Lazaridou, J May, A Nisnevich, N Pinto, J Turian, 10.18653/v1/2020.emnlp-main.703Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsNovember 2020</p>
<p>Reading between the lines: Learning to map high-level instructions to commands. S Branavan, L Zettlemoyer, R Barzilay, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. the 48th Annual Meeting of the Association for Computational LinguisticsUppsala, SwedenAssociation for Computational Linguistics2010</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Grounding large language models in interactive environments with online reinforcement learning. T Carta, C Romac, T Wolf, S Lamprier, O Sigaud, P.-Y Oudeyer, Proceedings of the 40th International Conference on Machine Learning. A Krause, E Brunskill, K Cho, B Engelhardt, S Sabato, J Scarlett, the 40th International Conference on Machine LearningPMLRJul 2023202</p>
<p>A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, Matterport3d, arXiv:1709.06158Learning from rgb-d data in indoor environments. 2017arXiv preprint</p>
<p>Visualgpt: Data-efficient adaptation of pretrained language models for image captioning. J Chen, H Guo, K Yi, B Li, M Elhoseiny, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Learning phrase representations using rnn encoderdecoder for statistical machine translation. K Cho, B Van Merriënboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, arXiv:1406.10782014arXiv preprint</p>
<p>Learning the effects of physical actions in a multi-modal environment. G Dagan, F Keller, A Lascarides, Findings of the Association for Computational Linguistics: EACL 2023. Dubrovnik, CroatiaAssociation for Computational LinguisticsMay 2023</p>
<p>Embodied Question Answering. A Das, S Datta, G Gkioxari, S Lee, D Parikh, D Batra, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2018</p>
<p>I Dasgupta, C Kaeser-Chen, K Marino, A Ahuja, S Babayan, F Hill, R Fergus, arXiv:2302.00763Collaborating with language models for embodied reasoning. 2023arXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, arXiv:2303.033782023arXiv preprint</p>
<p>Guiding pretraining in reinforcement learning with large language models. Y Du, O Watkins, Z Wang, C Colas, T Darrell, P Abbeel, A Gupta, Andreas , J , 2023a</p>
<p>Learning universal policies via text-guided video generation. Y Du, M Yang, B Dai, H Dai, O Nachum, J B Tenenbaum, D Schuurmans, P Abbeel, 2023b2302arXiv e-prints</p>
<p>Reading to learn: Constructing features from semantic abstracts. J Eisenstein, J Clarke, D Goldwasser, D Roth, 2009</p>
<p>Conference on Empirical Methods in Natural Language Processing. SingaporeAssociation for Computational Linguistics2009</p>
<p>Tinystories: How small can language models be and still speak coherent english?. R Eldan, Y Li, 2023</p>
<p>Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. L Espeholt, H Soyer, R Munos, K Simonyan, V Mnih, T Ward, Y Doron, V Firoiu, T Harley, I Dunning, International conference on machine learning. PMLR2018</p>
<p>L Espeholt, R Marinier, P Stanczyk, K Wang, M Michalski, arXiv:1910.06591Seed rl: Scalable and efficient deeprl with accelerated central inference. 2019arXiv preprint</p>
<p>From images to textual prompts: Zero-shot visual question answering with frozen large language models. J Guo, J Li, D Li, A M H Tiong, B Li, D Tao, S Hoi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Recurrent world models facilitate policy evolution. D Ha, J Schmidhuber, Advances in Neural Information Processing Systems. 201831</p>
<p>D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, arXiv:1811.04551Learning latent dynamics for planning from pixels. 2018arXiv preprint</p>
<p>Mastering atari with discrete world models. D Hafner, T Lillicrap, M Norouzi, J Ba, arXiv:2010.021932020arXiv preprint</p>
<p>Mastering diverse domains through world models. D Hafner, J Pasukonis, J Ba, T Lillicrap, arXiv:2301.041042023arXiv preprint</p>
<p>Grounding language to entities and dynamics for generalization in reinforcement learning. A W Hanjie, V Zhong, K Narasimhan, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. M Meila, T Zhang, the 38th International Conference on Machine Learning, ICML 2021PMLR18-24 July 2021. 2021139of Proceedings of Machine Learning Research</p>
<p>The origin of speech. C F Hockett, C D Hockett, Sci. Am. 20331960</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning. PMLR2022a</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, P Sermanet, N Brown, T Jackson, L Luu, S Levine, K Hausman, B Ichter, arXivpreprintarXiv:2207.056082022b</p>
<p>Language as an abstraction for hierarchical deep reinforcement learning. Y Jiang, S S Gu, K P Murphy, C Finn, Advances in Neural Information Processing Systems. 201932</p>
<p>Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, L Fan, Vima, arXiv:2210.03094General robot manipulation with multimodal prompts. 2022arXiv preprint</p>
<p>Recurrent experience replay in distributed reinforcement learning. S Kapturowski, G Ostrovski, J Quan, R Munos, W Dabney, International conference on learning representations. 2019</p>
<p>D P Kingma, M Welling, arXiv:1312.6114Auto-encoding variational bayes. 2013arXiv preprint</p>
<p>Improved variational inference with inverse autoregressive flow. D P Kingma, T Salimans, R Jozefowicz, X Chen, I Sutskever, M Welling, Advances in neural information processing systems. 292016</p>
<p>Beyond the nav-graph: Vision-and-language navigation in continuous environments. J Krantz, E Wijmans, A Majumdar, D Batra, S Lee, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 202016</p>
<p>Waypoint models for instruction-guided navigation in continuous environments. J Krantz, A Gokaslan, D Batra, S Lee, O Maksymets, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. A Ku, P Anderson, R Patel, E Ie, J Baldridge, 10.18653/v1/2020.emnlp-main.356Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. B Webber, T Cohn, Y He, Y Liu, the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsNovember 2020</p>
<p>Implicit representations of meaning in neural language models. B Z Li, M Nye, Andreas , J , 10.18653/v1/2021.acl-long.143Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 20211</p>
<p>Language models as probabilistic priors for perception and action. B Z Li, W Chen, P Sharma, Andreas , J Lampp, 2023aarXiv e-prints</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, arXiv:2301.125972023barXiv preprint</p>
<p>Emergent world representations: Exploring a sequence model trained on a synthetic task. K Li, A K Hopkins, D Bau, F Viégas, H Pfister, M Wattenberg, The Eleventh International Conference on Learning Representations. 2023c</p>
<p>Pre-trained language models for interactive decision-making. S Li, X Puig, Y Du, C Wang, E Akyurek, A Torralba, J Andreas, I Mordatch, arXiv:2202.017712022arXiv preprint</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, NeurIPS2023</p>
<p>Hierarchical question-image co-attention for visual question answering. J Lu, J Yang, D Batra, D Parikh, Advances in Neural Information Processing Systems. D Lee, M Sugiyama, U Luxburg, I Guyon, R Garnett, Curran Associates, Inc201629</p>
<p>Unified-io: A unified model for vision, language, and multi-modal tasks. J Lu, C Clark, R Zellers, R Mottaghi, A Kembhavi, arXiv:2206.089162022arXiv preprint</p>
<p>A survey of reinforcement learning informed by natural language. J Luketina, N Nardelli, G Farquhar, J N Foerster, J Andreas, E Grefenstette, S Whiteson, T Rocktäschel, 10.24963/ijcai.2019/880Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019. S Kraus, the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019Macao, ChinaAugust 10-16, 2019. 2019</p>
<p>Language conditioned imitation learning over unstructured data. C Lynch, P Sermanet, Robotics: Science and Systems. 2021</p>
<p>Exploration through learned language abstraction. S Mirchandani, S Karamcheti, D Sadigh, Ella, Advances in Neural Information Processing Systems. 202134</p>
<p>Improving intrinsic exploration with language abstractions. J Mu, V Zhong, R Raileanu, M Jiang, N D Goodman, T Rocktäschel, E Grefenstette, NeurIPS2022</p>
<p>Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. S Nair, E Mitchell, K Chen, B Ichter, S Savarese, C Finn, Conference on Robot Learning. 2021</p>
<p>Grounding language for transfer in deep reinforcement learning. K Narasimhan, R Barzilay, T Jaakkola, Journal of Artificial Intelligence Research. 632018</p>
<p>TEACh: Task-driven Embodied Agents that Chat. A Padmakumar, J Thomason, A Shrivastava, P Lange, A Narayan-Chen, S Gella, R Piramuthu, Gokhan Tur, D H .-T, Conference on Artificial Intelligence (AAAI). 2022</p>
<p>Meaning without reference in large language models. S T Piantadosi, F Hill, ArXiv, abs/2208.029572022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, Journal of Machine Learning Research. 211402020</p>
<p>S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, arXiv:2205.06175A generalist agent. 2022arXiv preprint</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. N Reimers, I Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics201911</p>
<p>D J Rezende, S Mohamed, D Wierstra, arXiv:1401.4082Stochastic backpropagation and approximate inference in deep generative models. 2014arXiv preprint</p>
<p>Transformer-based world models are happy with 100k interactions. J Robine, M Höftmann, T Uelwer, S Harmeling, arXiv:2303.071092023arXiv preprint</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>P Sharma, A Torralba, Andreas , J , arXiv:2110.01517Skill induction and planning with latent language. 2021arXiv preprint</p>
<p>ALFRED: A benchmark for interpreting grounded instructions for everyday tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, 10.1109/CVPR42600.2020.010752020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020. Seattle, WA, USAIEEEJune 13-19, 2020. 2020a</p>
<p>M Shridhar, X Yuan, M.-A Côté, Y Bisk, A Trischler, M Hausknecht, Alfworld, arXiv:2010.03768Aligning text and embodied environments for interactive learning. 2020barXiv preprint</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Conference on Robot Learning. PMLR2022</p>
<p>Pre-trained language models as prior knowledge for playing text-based games. I Singh, G Singh, A Modi, arXiv:2107.084082021arXiv preprint</p>
<p>an integrated architecture for learning, planning, and reacting. R S Sutton, Dyna, ACM SIGART Bulletin. 241991</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>Semantic exploration from language abstractions and pretrained representations. A C Tam, N C Rabinowitz, A K Lampinen, N A Roy, S C Y Chan, D Strouse, J Wang, A Banino, F Hill, NeurIPS. 2022</p>
<p>Vision-and-dialog navigation. J Thomason, M Murray, M Cakmak, L Zettlemoyer, Conference on Robot Learning. 2019</p>
<p>Voyager: An openended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, arXiv: Arxiv-2305.162912023arXiv preprint</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. R J Williams, Machine learning. 83-41992</p>
<p>Understanding natural language. T Winograd, org/10.1016/0010-0285(72)90002-3Cognitive Psychology. 0010-0285311972</p>
<p>URL. </p>
<p>SPRING: Studying papers and reasoning to play games. Y Wu, S Y Min, S Prabhumoye, Y Bisk, R Salakhutdinov, A Azaria, T Mitchell, Y Li, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>M Yang, Y Du, K Ghasemipour, J Tompson, D Schuurmans, P Abbeel, arXiv:2310.06114Learning interactive real-world simulators. 2023arXiv preprint</p>
<p>RTFM: generalising to new environment dynamics via reading. V Zhong, T Rocktäschel, E Grefenstette, 8th International Conference on Learning Representations. Addis Ababa, EthiopiaApril 26-30, 2020. 20202020OpenReview.net</p>
<p>Improving policy learning via language dynamics distillation. V Zhong, J Mu, L Zettlemoyer, E Grefenstette, T Rocktaschel, Thirty-sixth Conference on Neural Information Processing Systems. 2022</p>
<p>Finetuning language models from human preferences. D M Ziegler, N Stiennon, J Wu, T B Brown, A Radford, D Amodei, P Christiano, G Irving, ArXiv, abs/1909.085932019</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2036 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2036</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2036</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-49.html">extraction-schema-49</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-279251658</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.06632v1.pdf" target="_blank">Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> We aim to improve the reasoning capabilities of language models via reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1 have demonstrated reasoning abilities on mathematical and coding tasks. However, prior studies suggest that using RL alone to improve reasoning on inherently difficult tasks is less effective. Here, we draw inspiration from curriculum learning and propose to schedule tasks from easy to hard (E2H), allowing LLMs to build reasoning skills gradually. Our method is termed E2H Reasoner. Empirically, we observe that, although easy tasks are important initially, fading them out through appropriate scheduling is essential in preventing overfitting. Theoretically, we establish convergence guarantees for E2H Reasoner within an approximate policy iteration framework. We derive finite-sample complexity bounds and show that when tasks are appropriately decomposed and conditioned, learning through curriculum stages requires fewer total samples than direct learning. Experiments across multiple domains show that E2H Reasoner significantly improves the reasoning ability of small LLMs (1.5B to 3B), which otherwise struggle when trained with vanilla RL alone, highlighting the effectiveness of our method.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2036.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2036.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E2H Reasoner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Easy 2 Hard Reasoner (E2H)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum-reinforcement-learning (CRL) post-training framework that decomposes hard reasoning tasks into easier subtasks (trivial, easy, medium) and schedules them from easy to hard using probabilistic samplers to improve LLM reasoning and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>difficulty-based ordering with probabilistic scheduling</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Tasks are decomposed into ordered difficulty levels (trivial, easy, medium, hard) using either human difficulty annotations (when available) or model zero-shot error rates (quartile bucketization). A probabilistic scheduler progressively shifts sampling probability from easier to harder tasks over training steps (two concrete schedulers: cosine-based and Gaussian-based). Intermediate tasks serve as dense-reward stages to reduce reward sparsity and bridge the pretraining-to-target distribution gap.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen 2.5/1.5B Instruct, Qwen 2.5/3B Instruct, LLaMA 3.2 3B Instruct (models used for experiments and for zero-shot difficulty estimation in some splits; Qwen 1.5B used for RQ1/RQ2 analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Text-based reasoning and planning (mathematical reasoning: MATH, GSM8K, AQuA; planning: Blocksworld; Countdown arithmetic planning)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Tasks span single-step to multi-step planning and arithmetic: Blocksworld plans up to 8 steps (training on 1,2,4,6; OOD=8), Countdown uses 2–6 input numbers (Trivial=2 up to OOD=6), MATH problems Levels 1–5 (train on 1–4, OOD=5); subtasks require compositional skills (basic arithmetic, estimation, backtracking, multi-step planning).</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>balanced (mixed-difficulty random sampling), traditional sequential curriculum learning (fixed-stage switch), GRPO baseline (balanced scheduling default), training on Hard-only, supervised fine-tuning (SFT), Chain-of-Thought (CoT) prompting reference</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Across multiple models and datasets E2H variants (cosine E2H-C and Gaussian E2H-G) improved HARD and OOD accuracy compared to balanced GRPO and CL baselines. Representative highlights: Qwen 2.5 3B Instruct — GRPO (All) Blocksworld Hard=38.3% OOD=14.6%; E2H-G (best) Blocksworld Hard=53.3% OOD=23.2%. Qwen 2.5 1.5B Instruct — GRPO (All) Blocksworld Hard=21.1% OOD=2.6%; E2H-G improved final hard accuracy (e.g., medium/ hard improvements shown up to +~11-32 percentage points depending on dataset). Detailed per-difficulty tables reported in paper (Tables 1-4,8).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>No direct wall-clock convergence curves reported; all post-training runs used up to 1600 GRPO steps and LoRA; theoretical analysis shows CRL can require fewer total samples than direct learning under specified geometric allocation conditions (see Theorem 3.2); empirically E2H reached better final accuracy within the fixed training budget (1600 steps) used for all methods.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>E2H improves out-of-distribution (OOD) generalization (e.g., Blocksworld OOD accuracy increases: Qwen 3B E2H-G OOD=23.2% vs GRPO(All) OOD=14.6%; similar OOD gains reported for MATH and Countdown). The paper emphasizes stronger OOD performance as evidence of improved reasoning/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not quantitatively formalized beyond using multiple difficulty buckets and datasets; diversity implicitly encoded by decomposition into multiple difficulty levels and by mixing schedules, but no separate metricized task diversity comparison is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Prerequisites are identified via human difficulty labels (when available) or via zero-shot error rates of base LLMs (quartile bucketing) to form trivial→easy→medium→hard curricula; no automated structure discovery beyond these heuristics was used.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes — intermediate (trivial/easy/medium) subtasks were explicitly constructed from dataset annotations (plan length, number of operands, MATH levels) or by bucketing examples by model error; these intermediate tasks were effective in providing dense rewards and enabling transfer to harder tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>No LLM-generated curriculum (in the sense of using an LLM to propose new tasks) was used; observed model limitations include reward-hacking/overfitting to trivial tasks and task forgetting under rigid curricula; E2H-C (cosine) can overexpose trivial examples and cause overfitting in sparse-reward domains, which E2H-G (Gaussian) mitigates by faster decay of trivial-task probability.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training reported on up to three 80GB A100 GPUs; experiments required ~16–18 hours on average per run for 3B models; all methods used same GRPO budget (up to 1600 steps), so curriculum generation overhead (bucketing by error or using human labels) is minimal compared to model training and was not separately timed.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No human expert scoring of curricula was reported; human labels used only where datasets already included difficulty annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Scheduling tasks from easy to hard with probabilistic samplers (E2H) significantly improves reasoning and OOD generalization for small LLMs (1.5–3B) compared to balanced sampling, direct hard-only training, and traditional fixed-stage curricula; cosine scheduling (E2H-C) is simple and effective for datasets with similar zero-shot performance across levels, while Gaussian scheduling (E2H-G) provides hyperparameterized control to avoid trivial-task overfitting in sparse-reward domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2036.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2036.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E2H-C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>E2H Cosine Sampler (E2H-C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-free non-parametric scheduler that interpolates sampling probabilities for K ordered tasks across training steps using a cosine function, initially favoring easiest tasks and ending favoring hardest tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>difficulty-based ordering with cosine interpolation (non-parametric)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>At step t the sampler computes α_t = 0.5*(1 + cos(pi * t / T)) and assigns raw weights proportional to α_t*(K - k - 1) + (1 - α_t)*k for task index k, then normalizes to a sampling distribution; effectively places highest sampling prob on easiest tasks at start and on hardest tasks at end, with smooth cosine interpolation in between.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Text-based reasoning/planning datasets used across experiments (Blocksworld, Countdown, MATH, GSM8K, AQuA).</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Same task decomposition as E2H: trivial→easy→medium→hard; complexity increases by more steps, operands, or problem level.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>balanced, traditional CL, E2H-G (Gaussian), hard-only, GRPO baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>E2H-C often improved early learning and final performance in datasets where zero-shot performance is similar across difficulty levels (e.g., MATH): examples include Qwen 3B on MATH E2H-C Trivial/Easy/Med/Hard/OOD approx = 90.4/80.0/73.4/58.9/38.1 (see Table 8) and LLaMA 3.2 variants where cosine worked well for MATH but overfitted in Blocksworld sparse-reward settings.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>No explicit convergence time numbers; used same GRPO steps; E2H-C can require careful fading of trivial tasks to avoid overfitting according to ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Improved generalization on some datasets (notably MATH); however in highly sparse-reward planning tasks (Blocksworld) E2H-C sometimes overfits to trivial tasks, degrading OOD results compared to Gaussian variant.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not separately quantified; cosine sampler mixes tasks smoothly over time.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Relies on provided difficulty labels or zero-shot buckets (not adaptive).</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Uses predefined intermediate tasks (trivial/easy/medium) from dataset splits.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>N/A (scheduler is algorithmic); observed limitation is tendency to keep trivial tasks active for too long, promoting reward-hacking behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Scheduler is parameter-free and negligible computational overhead compared to training; no extra cost reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>None reported</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Cosine scheduling is simple and effective when zero-shot performance across difficulties is similar (e.g., MATH), but can overexpose trivial examples leading to overfitting in sparse-reward planning tasks; thus it works well in some domains but not universally.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2036.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2036.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E2H-G</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>E2H Gaussian Sampler (E2H-G)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameterized Gaussian-based scheduler that models task sampling probabilities as Gaussians over a training-position axis and moves sampling position along training steps; allows control of exposure concentration and speed through σ and β.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>difficulty-based ordering with Gaussian mixture-inspired scheduling</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Assume each task k corresponds to a Gaussian centered at µ_k (µ_k = k-1) with shared variance σ; sampling position x_t = (t/T) * β * (K-1). Sampling probability for task k is proportional to exp(-(x_t - µ_k)^2/(2σ^2)), normalized across tasks. Hyperparameters β (speed) and σ (concentration) control how quickly sampling shifts and how narrowly it focuses on specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Text reasoning and planning (Blocksworld, Countdown, MATH, GSM8K, AQuA).</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Same multi-level difficulty decomposition; E2H-G settings tuned per-dataset to avoid trivial-task overfitting (e.g., β=0.25,σ=0.75; β=0.5,σ=0.5; β=0.75,σ=0.25 were evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>balanced, CL, E2H-C, hard-only, GRPO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>E2H-G produced the strongest and most consistent improvements in sparse-reward domains: example Qwen 2.5 3B Instruct on Blocksworld: E2H-G (best hyperparams) Trivial/Easy/Med/Hard/OOD = 100.0/100.0/96.4/53.3/23.2 versus GRPO(All) 100.0/100.0/94.1/38.9/13.3 (see Tables 4 and 8). On Countdown and MATH similar consistent gains on HARD and OOD slices were reported; E2H-G hyperparameter sweep results in Appendix (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>No explicit episode/step convergence graphs; within the fixed 1600 GRPO-step budget E2H-G consistently yielded higher final accuracies in sparse-reward tasks compared to E2H-C and balanced baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>E2H-G improved OOD generalization across datasets (Blocksworld OOD gains shown above; similar patterns for Countdown and MATH hard/OOD splits).</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not directly measured; Gaussian sampler controls concentration (σ) resulting in more focused exposure windows to particular difficulty levels, reducing trivial-task overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Uses dataset labels or zero-shot error quartiles to set ordering; does not automatically discover prerequisites.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Intermediate tasks are dataset-driven (same as E2H); Gaussian schedule generates progressive bridging exposures with tunable durations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable to generation but E2H-G requires tuning two hyperparameters (σ, β) and selection of best setting reported; improper settings can underexpose easier tasks or still overexpose them.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Scheduler overhead negligible; extra compute only from running multiple hyperparameter settings (paper reports best of 3 λ-settings; full sweep reported in Appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>None reported</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Gaussian scheduling provides fine-grained control to prevent trivial-task overfitting and usually outperforms cosine and balanced samplers in sparse-reward, compositional planning domains; tuning β and σ is important.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2036.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2036.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Traditional CL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Traditional Sequential Curriculum Learning (CL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fixed-stage curriculum where training proceeds deterministically from one difficulty stage to the next for preset epochs/steps (hard switch), used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>hand-crafted fixed-stage ordering (easy→hard) with stage thresholds</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>For K tasks, define thresholds τ_k and switch sampling to only the k-th task between τ_k and τ_{k+1}; this is the conventional 'block-wise' curriculum used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Same reasoning/planning datasets</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Same difficulty levels; CL exposes entire blocks of training to a single difficulty stage before switching.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>compared against balanced, E2H-C, E2H-G, GRPO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Traditional CL often led to task forgetting or overfitting; example: Qwen 1.5B CL results on Blocksworld/Easy/Med/Hard showed collapses (CL sometimes had 0% on many intermediate levels depending on dataset), and overall performed worse on OOD compared to E2H variants (see Tables 1-4).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>CL can produce initial fast gains on mid-range tasks but suffers later forgetting; no quantitative convergence-time advantage shown under fixed budget.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Generally worse OOD generalization than probabilistic E2H samplers due to forgetting and reward-hacking.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>CL provides low instantaneous task diversity (only one stage active), which the paper links to forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Relies on manual ordering; no automated detection.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Uses same dataset-defined intermediate tasks; no dynamic intermediate generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>N/A (method is scheduler); observed disadvantage is causing forgetting and overfitting when switching is rigid.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>No extra cost beyond training; identical training budget used.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>None reported</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Fixed-stage CL can cause forgetting and overfitting to earlier stages; probabilistic scheduling (E2H-C/E2H-G) alleviates these issues and yields better final and OOD performance under the same training budget.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2036.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2036.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Balanced Scheduler</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Balanced Scheduling (mixed-difficulty random sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that samples uniformly across all difficulty levels at every training step (S_balanced(t,k) = 1/K), equivalent to no curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>random uniform sampling across difficulty buckets</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>At each training step sample uniformly from the K difficulty buckets, providing maximum task diversity per step but no structured progression.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Text reasoning/planning datasets</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Uniform exposure to all levels (trivial→hard).</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>compared to CL, E2H-C, E2H-G, hard-only, SFT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Balanced GRPO baseline is strong in many settings (e.g., Qwen 1.5B GRPO(All) Blocksworld Trivial/Easy/Med/Hard/OOD = 98.0/100/83.3/21.1/2.6), but often suboptimal on hardest and OOD slices compared to E2H-G/C; balanced helps avoid forgetting but exposes hard tasks too early, causing sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Provides steady learning but can be slower to improve hard-task performance due to early sparse-reward exposure; no numeric convergence times.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Moderate OOD performance; outperformed by E2H variants on hard/OOD slices in many experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Maximizes instantaneous task diversity by design.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>None — no ordering used.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not applicable (uses existing splits).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>N/A (scheduler approach).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Negligible extra overhead; identical training budget.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>None reported</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Balanced sampling is a robust baseline that avoids forgetting but is often suboptimal for learning very hard, sparse-reward tasks compared to curated E2H probabilistic schedules.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2036.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2036.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRPO baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GRPO (policy optimization algorithm used for RL post-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The GRPO reinforcement learning algorithm (used as the main RL optimizer in experiments); by default employs balanced scheduling and is used as the primary RL baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>none (optimizer-level; default balanced sampling in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>GRPO is used as the underlying policy optimization algorithm for all RL post-training runs; curriculum effects are implemented at data sampling level independent of the optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>LLM post-training for reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Used across same range of multi-step reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Used as baseline; SFT and CoT also compared</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GRPO with balanced sampling (GRPO All) reported per-dataset/per-difficulty accuracies; E2H variants always use GRPO as optimizer, showing improvements over GRPO(All) when paired with E2H schedules.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>All RL experiments used GRPO with identical hyperparameters and training steps (max 1600), enabling fair curriculum comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>GRPO(All) generalizes reasonably but is outperformed by E2H schedules on many HARD and OOD metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Balanced GRPO provides high diversity; GRPO combined with curricula modifies sampling externally.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not provided by optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not provided by optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable beyond optimizer behavior described in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training hyperparameters: learning_rate=1e-6, max_steps=1600; runs took ~16–18 hours on 2–3x A100 80GB for 3B models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>None</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>GRPO is a reliable RL optimizer used across experiments; curriculum/scheduling choices on top of GRPO drive the observed improvements rather than changing the optimizer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2036.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2036.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard supervised fine-tuning baseline (imitative learning) that trains models to mimic provided reasoning examples, compared against RL-based post-training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>none (SFT uses static training data; in appendices SFT used same configuration for fair comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>SFT trains models to imitate labeled solutions; no curriculum scheduling beyond data composition; reported for comparison to show RL advantages in sparse/OOD settings.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Same reasoning/planning datasets</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>SFT performs well when train/test distributions closely match but poorly on broad or OOD distributions (e.g., Countdown OOD).</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to GRPO(All) and E2H variants in Table 7 and Appendix</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SFT shows variable performance: Blocksworld SFT Trivial/Easy/Med/Hard/OOD = 100.0/97.8/88.1/55.3/16.5 (example row), but fails on some OOD splits (e.g., Countdown OOD=0% in reported example); RL/E2H often more consistent across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not directly comparable to RL within paper; SFT can converge quickly when data distribution is close to test distribution but generalizes poorly to OOD sparse-reward tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Inconsistent; SFT generalizes poorly to broad or OOD distributions in e.g., Countdown but can do well in Blocksworld when distributions align.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not part of SFT protocol in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not applied.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>SFT can overfit to surface patterns and not learn transferable reasoning strategies, consistent with prior literature cited.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Same GRPO training config used for SFT for fair comparison; specific SFT compute not separately detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>None reported</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>SFT performance varies by task and distribution alignment; RL-based curricula (E2H) provide more consistent gains on sparse and OOD reasoning tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2036.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2036.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRL Theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theoretical Analysis of Curriculum Reinforcement Learning (API viewpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An API-based theoretical framework analyzing convergence and finite-sample complexity of curriculum RL showing conditions where CRL requires fewer total samples than direct learning on the hard task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>analytical (theoretical decomposition of curriculum into interpolated MDP/task distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Formalize K interpolated MDPs {M_k} with distributions {d_k} between source d1 and target dK; apply Approximate Policy Iteration (API) with approximate evaluation/improvement errors to derive a bound on final performance gap (Theorem 3.1) and a finite-sample complexity result (Theorem 3.2). Show M_CR L = O(sum_k (...) ) and establish a sufficient condition (Equation 2) under geometric error/L_k allocation for M_CR L < M_Direct.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Abstract MDP view general to LLM token-generation MDPs used for reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Analysis explicitly tracks curriculum approximation error ∥Q*_K - Q*_k∥ and function approximation terms; accounts for multi-step returns and sparse terminal rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Theoretical comparison between CRL (sequential/interpolated learning) and direct learning on final task; no empirical baselines in theory section.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Theorem 3.1 bounds final performance gap E_K by sum of actor convergence bias (γ^T terms), evaluation errors δ_k, policy update error β terms, and curriculum approximation error K−1 Σ ∥Q*_K - Q*_k∥; Theorem 3.2 provides a sample complexity order: O(Σ_k log^3(1/ϵ_k)/ϵ_k^2 • Õ(L_k^2 n (1-γ)^-7 (1-γ_c)^-3 λ_min^-3)). Derived condition (Equation 2) gives when M_CR L < M_Direct under geometric allocations.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Theoretical claim: CRL requires fewer total samples than direct learning when curriculum tasks are appropriately chosen and error allocations optimized; specific inequality provided (Equation 2) and example numeric instantiation (K=3, e*l=1.4, m=1.8) shows practical plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>The theory links smoother curriculum trajectories (small ∥Q*_K - Q*_k∥) and smaller approximation errors to better final generalization, providing formal justification for empirical OOD gains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Curriculum discrepancy measured via ∥Q*_K - Q*_k∥ terms; analysis encourages small curriculum drift for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Theorem assumes existence of interpolating distributions d_k but does not provide an algorithm to identify prerequisites automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Theory presumes intermediate tasks M_k exist to interpolate distributions; effectiveness depends on how well these are chosen (small curriculum approximation error).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>N/A (theoretical analysis abstracts away model-specific failures but accounts for function approximation error terms).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Theoretical sample complexity scales are provided; no wall-clock measurements in theory.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>None</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Provides convergence guarantees for API-style CRL and finite-sample bounds; theoretically demonstrates that carefully designed curricula can be more sample-efficient than direct training on hard tasks, aligning with empirical results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepseek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Curriculum learning <em>(Rating: 2)</em></li>
                <li>Automated curriculum learning for neural networks <em>(Rating: 2)</em></li>
                <li>Curriculum learning for reinforcement learning domains: A framework and survey <em>(Rating: 2)</em></li>
                <li>WISDOM: Progressive curriculum synthesis makes LLMs better mathematical reasoner <em>(Rating: 2)</em></li>
                <li>Online difficulty filtering for reasoning oriented reinforcement learning <em>(Rating: 2)</em></li>
                <li>Logic-rl: Unleashing LLM reasoning with rule-based reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2036",
    "paper_id": "paper-279251658",
    "extraction_schema_id": "extraction-schema-49",
    "extracted_data": [
        {
            "name_short": "E2H Reasoner",
            "name_full": "Easy 2 Hard Reasoner (E2H)",
            "brief_description": "A curriculum-reinforcement-learning (CRL) post-training framework that decomposes hard reasoning tasks into easier subtasks (trivial, easy, medium) and schedules them from easy to hard using probabilistic samplers to improve LLM reasoning and generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "difficulty-based ordering with probabilistic scheduling",
            "curriculum_method_description": "Tasks are decomposed into ordered difficulty levels (trivial, easy, medium, hard) using either human difficulty annotations (when available) or model zero-shot error rates (quartile bucketization). A probabilistic scheduler progressively shifts sampling probability from easier to harder tasks over training steps (two concrete schedulers: cosine-based and Gaussian-based). Intermediate tasks serve as dense-reward stages to reduce reward sparsity and bridge the pretraining-to-target distribution gap.",
            "llm_model_used": "Qwen 2.5/1.5B Instruct, Qwen 2.5/3B Instruct, LLaMA 3.2 3B Instruct (models used for experiments and for zero-shot difficulty estimation in some splits; Qwen 1.5B used for RQ1/RQ2 analyses)",
            "domain_environment": "Text-based reasoning and planning (mathematical reasoning: MATH, GSM8K, AQuA; planning: Blocksworld; Countdown arithmetic planning)",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Tasks span single-step to multi-step planning and arithmetic: Blocksworld plans up to 8 steps (training on 1,2,4,6; OOD=8), Countdown uses 2–6 input numbers (Trivial=2 up to OOD=6), MATH problems Levels 1–5 (train on 1–4, OOD=5); subtasks require compositional skills (basic arithmetic, estimation, backtracking, multi-step planning).",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "balanced (mixed-difficulty random sampling), traditional sequential curriculum learning (fixed-stage switch), GRPO baseline (balanced scheduling default), training on Hard-only, supervised fine-tuning (SFT), Chain-of-Thought (CoT) prompting reference",
            "performance_metrics": "Across multiple models and datasets E2H variants (cosine E2H-C and Gaussian E2H-G) improved HARD and OOD accuracy compared to balanced GRPO and CL baselines. Representative highlights: Qwen 2.5 3B Instruct — GRPO (All) Blocksworld Hard=38.3% OOD=14.6%; E2H-G (best) Blocksworld Hard=53.3% OOD=23.2%. Qwen 2.5 1.5B Instruct — GRPO (All) Blocksworld Hard=21.1% OOD=2.6%; E2H-G improved final hard accuracy (e.g., medium/ hard improvements shown up to +~11-32 percentage points depending on dataset). Detailed per-difficulty tables reported in paper (Tables 1-4,8).",
            "learning_speed_comparison": "No direct wall-clock convergence curves reported; all post-training runs used up to 1600 GRPO steps and LoRA; theoretical analysis shows CRL can require fewer total samples than direct learning under specified geometric allocation conditions (see Theorem 3.2); empirically E2H reached better final accuracy within the fixed training budget (1600 steps) used for all methods.",
            "generalization_performance": "E2H improves out-of-distribution (OOD) generalization (e.g., Blocksworld OOD accuracy increases: Qwen 3B E2H-G OOD=23.2% vs GRPO(All) OOD=14.6%; similar OOD gains reported for MATH and Countdown). The paper emphasizes stronger OOD performance as evidence of improved reasoning/generalization.",
            "task_diversity_analysis": "Not quantitatively formalized beyond using multiple difficulty buckets and datasets; diversity implicitly encoded by decomposition into multiple difficulty levels and by mixing schedules, but no separate metricized task diversity comparison is provided.",
            "prerequisite_identification": "Prerequisites are identified via human difficulty labels (when available) or via zero-shot error rates of base LLMs (quartile bucketing) to form trivial→easy→medium→hard curricula; no automated structure discovery beyond these heuristics was used.",
            "intermediate_task_generation": "Yes — intermediate (trivial/easy/medium) subtasks were explicitly constructed from dataset annotations (plan length, number of operands, MATH levels) or by bucketing examples by model error; these intermediate tasks were effective in providing dense rewards and enabling transfer to harder tasks.",
            "llm_limitations_observed": "No LLM-generated curriculum (in the sense of using an LLM to propose new tasks) was used; observed model limitations include reward-hacking/overfitting to trivial tasks and task forgetting under rigid curricula; E2H-C (cosine) can overexpose trivial examples and cause overfitting in sparse-reward domains, which E2H-G (Gaussian) mitigates by faster decay of trivial-task probability.",
            "computational_cost": "Training reported on up to three 80GB A100 GPUs; experiments required ~16–18 hours on average per run for 3B models; all methods used same GRPO budget (up to 1600 steps), so curriculum generation overhead (bucketing by error or using human labels) is minimal compared to model training and was not separately timed.",
            "human_expert_evaluation": "No human expert scoring of curricula was reported; human labels used only where datasets already included difficulty annotations.",
            "key_findings_summary": "Scheduling tasks from easy to hard with probabilistic samplers (E2H) significantly improves reasoning and OOD generalization for small LLMs (1.5–3B) compared to balanced sampling, direct hard-only training, and traditional fixed-stage curricula; cosine scheduling (E2H-C) is simple and effective for datasets with similar zero-shot performance across levels, while Gaussian scheduling (E2H-G) provides hyperparameterized control to avoid trivial-task overfitting in sparse-reward domains.",
            "uuid": "e2036.0"
        },
        {
            "name_short": "E2H-C",
            "name_full": "E2H Cosine Sampler (E2H-C)",
            "brief_description": "A parameter-free non-parametric scheduler that interpolates sampling probabilities for K ordered tasks across training steps using a cosine function, initially favoring easiest tasks and ending favoring hardest tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "difficulty-based ordering with cosine interpolation (non-parametric)",
            "curriculum_method_description": "At step t the sampler computes α_t = 0.5*(1 + cos(pi * t / T)) and assigns raw weights proportional to α_t*(K - k - 1) + (1 - α_t)*k for task index k, then normalizes to a sampling distribution; effectively places highest sampling prob on easiest tasks at start and on hardest tasks at end, with smooth cosine interpolation in between.",
            "llm_model_used": null,
            "domain_environment": "Text-based reasoning/planning datasets used across experiments (Blocksworld, Countdown, MATH, GSM8K, AQuA).",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Same task decomposition as E2H: trivial→easy→medium→hard; complexity increases by more steps, operands, or problem level.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "balanced, traditional CL, E2H-G (Gaussian), hard-only, GRPO baseline",
            "performance_metrics": "E2H-C often improved early learning and final performance in datasets where zero-shot performance is similar across difficulty levels (e.g., MATH): examples include Qwen 3B on MATH E2H-C Trivial/Easy/Med/Hard/OOD approx = 90.4/80.0/73.4/58.9/38.1 (see Table 8) and LLaMA 3.2 variants where cosine worked well for MATH but overfitted in Blocksworld sparse-reward settings.",
            "learning_speed_comparison": "No explicit convergence time numbers; used same GRPO steps; E2H-C can require careful fading of trivial tasks to avoid overfitting according to ablations.",
            "generalization_performance": "Improved generalization on some datasets (notably MATH); however in highly sparse-reward planning tasks (Blocksworld) E2H-C sometimes overfits to trivial tasks, degrading OOD results compared to Gaussian variant.",
            "task_diversity_analysis": "Not separately quantified; cosine sampler mixes tasks smoothly over time.",
            "prerequisite_identification": "Relies on provided difficulty labels or zero-shot buckets (not adaptive).",
            "intermediate_task_generation": "Uses predefined intermediate tasks (trivial/easy/medium) from dataset splits.",
            "llm_limitations_observed": "N/A (scheduler is algorithmic); observed limitation is tendency to keep trivial tasks active for too long, promoting reward-hacking behavior.",
            "computational_cost": "Scheduler is parameter-free and negligible computational overhead compared to training; no extra cost reported.",
            "human_expert_evaluation": "None reported",
            "key_findings_summary": "Cosine scheduling is simple and effective when zero-shot performance across difficulties is similar (e.g., MATH), but can overexpose trivial examples leading to overfitting in sparse-reward planning tasks; thus it works well in some domains but not universally.",
            "uuid": "e2036.1"
        },
        {
            "name_short": "E2H-G",
            "name_full": "E2H Gaussian Sampler (E2H-G)",
            "brief_description": "A parameterized Gaussian-based scheduler that models task sampling probabilities as Gaussians over a training-position axis and moves sampling position along training steps; allows control of exposure concentration and speed through σ and β.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "difficulty-based ordering with Gaussian mixture-inspired scheduling",
            "curriculum_method_description": "Assume each task k corresponds to a Gaussian centered at µ_k (µ_k = k-1) with shared variance σ; sampling position x_t = (t/T) * β * (K-1). Sampling probability for task k is proportional to exp(-(x_t - µ_k)^2/(2σ^2)), normalized across tasks. Hyperparameters β (speed) and σ (concentration) control how quickly sampling shifts and how narrowly it focuses on specific tasks.",
            "llm_model_used": null,
            "domain_environment": "Text reasoning and planning (Blocksworld, Countdown, MATH, GSM8K, AQuA).",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Same multi-level difficulty decomposition; E2H-G settings tuned per-dataset to avoid trivial-task overfitting (e.g., β=0.25,σ=0.75; β=0.5,σ=0.5; β=0.75,σ=0.25 were evaluated).",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "balanced, CL, E2H-C, hard-only, GRPO",
            "performance_metrics": "E2H-G produced the strongest and most consistent improvements in sparse-reward domains: example Qwen 2.5 3B Instruct on Blocksworld: E2H-G (best hyperparams) Trivial/Easy/Med/Hard/OOD = 100.0/100.0/96.4/53.3/23.2 versus GRPO(All) 100.0/100.0/94.1/38.9/13.3 (see Tables 4 and 8). On Countdown and MATH similar consistent gains on HARD and OOD slices were reported; E2H-G hyperparameter sweep results in Appendix (Table 8).",
            "learning_speed_comparison": "No explicit episode/step convergence graphs; within the fixed 1600 GRPO-step budget E2H-G consistently yielded higher final accuracies in sparse-reward tasks compared to E2H-C and balanced baselines.",
            "generalization_performance": "E2H-G improved OOD generalization across datasets (Blocksworld OOD gains shown above; similar patterns for Countdown and MATH hard/OOD splits).",
            "task_diversity_analysis": "Not directly measured; Gaussian sampler controls concentration (σ) resulting in more focused exposure windows to particular difficulty levels, reducing trivial-task overfitting.",
            "prerequisite_identification": "Uses dataset labels or zero-shot error quartiles to set ordering; does not automatically discover prerequisites.",
            "intermediate_task_generation": "Intermediate tasks are dataset-driven (same as E2H); Gaussian schedule generates progressive bridging exposures with tunable durations.",
            "llm_limitations_observed": "Not applicable to generation but E2H-G requires tuning two hyperparameters (σ, β) and selection of best setting reported; improper settings can underexpose easier tasks or still overexpose them.",
            "computational_cost": "Scheduler overhead negligible; extra compute only from running multiple hyperparameter settings (paper reports best of 3 λ-settings; full sweep reported in Appendix).",
            "human_expert_evaluation": "None reported",
            "key_findings_summary": "Gaussian scheduling provides fine-grained control to prevent trivial-task overfitting and usually outperforms cosine and balanced samplers in sparse-reward, compositional planning domains; tuning β and σ is important.",
            "uuid": "e2036.2"
        },
        {
            "name_short": "Traditional CL",
            "name_full": "Traditional Sequential Curriculum Learning (CL)",
            "brief_description": "A fixed-stage curriculum where training proceeds deterministically from one difficulty stage to the next for preset epochs/steps (hard switch), used as a baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "hand-crafted fixed-stage ordering (easy→hard) with stage thresholds",
            "curriculum_method_description": "For K tasks, define thresholds τ_k and switch sampling to only the k-th task between τ_k and τ_{k+1}; this is the conventional 'block-wise' curriculum used as baseline.",
            "llm_model_used": null,
            "domain_environment": "Same reasoning/planning datasets",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Same difficulty levels; CL exposes entire blocks of training to a single difficulty stage before switching.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "compared against balanced, E2H-C, E2H-G, GRPO",
            "performance_metrics": "Traditional CL often led to task forgetting or overfitting; example: Qwen 1.5B CL results on Blocksworld/Easy/Med/Hard showed collapses (CL sometimes had 0% on many intermediate levels depending on dataset), and overall performed worse on OOD compared to E2H variants (see Tables 1-4).",
            "learning_speed_comparison": "CL can produce initial fast gains on mid-range tasks but suffers later forgetting; no quantitative convergence-time advantage shown under fixed budget.",
            "generalization_performance": "Generally worse OOD generalization than probabilistic E2H samplers due to forgetting and reward-hacking.",
            "task_diversity_analysis": "CL provides low instantaneous task diversity (only one stage active), which the paper links to forgetting.",
            "prerequisite_identification": "Relies on manual ordering; no automated detection.",
            "intermediate_task_generation": "Uses same dataset-defined intermediate tasks; no dynamic intermediate generation.",
            "llm_limitations_observed": "N/A (method is scheduler); observed disadvantage is causing forgetting and overfitting when switching is rigid.",
            "computational_cost": "No extra cost beyond training; identical training budget used.",
            "human_expert_evaluation": "None reported",
            "key_findings_summary": "Fixed-stage CL can cause forgetting and overfitting to earlier stages; probabilistic scheduling (E2H-C/E2H-G) alleviates these issues and yields better final and OOD performance under the same training budget.",
            "uuid": "e2036.3"
        },
        {
            "name_short": "Balanced Scheduler",
            "name_full": "Balanced Scheduling (mixed-difficulty random sampling)",
            "brief_description": "A baseline that samples uniformly across all difficulty levels at every training step (S_balanced(t,k) = 1/K), equivalent to no curriculum.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "random uniform sampling across difficulty buckets",
            "curriculum_method_description": "At each training step sample uniformly from the K difficulty buckets, providing maximum task diversity per step but no structured progression.",
            "llm_model_used": null,
            "domain_environment": "Text reasoning/planning datasets",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Uniform exposure to all levels (trivial→hard).",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "compared to CL, E2H-C, E2H-G, hard-only, SFT",
            "performance_metrics": "Balanced GRPO baseline is strong in many settings (e.g., Qwen 1.5B GRPO(All) Blocksworld Trivial/Easy/Med/Hard/OOD = 98.0/100/83.3/21.1/2.6), but often suboptimal on hardest and OOD slices compared to E2H-G/C; balanced helps avoid forgetting but exposes hard tasks too early, causing sparse rewards.",
            "learning_speed_comparison": "Provides steady learning but can be slower to improve hard-task performance due to early sparse-reward exposure; no numeric convergence times.",
            "generalization_performance": "Moderate OOD performance; outperformed by E2H variants on hard/OOD slices in many experiments.",
            "task_diversity_analysis": "Maximizes instantaneous task diversity by design.",
            "prerequisite_identification": "None — no ordering used.",
            "intermediate_task_generation": "Not applicable (uses existing splits).",
            "llm_limitations_observed": "N/A (scheduler approach).",
            "computational_cost": "Negligible extra overhead; identical training budget.",
            "human_expert_evaluation": "None reported",
            "key_findings_summary": "Balanced sampling is a robust baseline that avoids forgetting but is often suboptimal for learning very hard, sparse-reward tasks compared to curated E2H probabilistic schedules.",
            "uuid": "e2036.4"
        },
        {
            "name_short": "GRPO baseline",
            "name_full": "GRPO (policy optimization algorithm used for RL post-training)",
            "brief_description": "The GRPO reinforcement learning algorithm (used as the main RL optimizer in experiments); by default employs balanced scheduling and is used as the primary RL baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "none (optimizer-level; default balanced sampling in experiments)",
            "curriculum_method_description": "GRPO is used as the underlying policy optimization algorithm for all RL post-training runs; curriculum effects are implemented at data sampling level independent of the optimizer.",
            "llm_model_used": null,
            "domain_environment": "LLM post-training for reasoning tasks",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Used across same range of multi-step reasoning tasks",
            "is_curriculum_adaptive": null,
            "baseline_comparisons": "Used as baseline; SFT and CoT also compared",
            "performance_metrics": "GRPO with balanced sampling (GRPO All) reported per-dataset/per-difficulty accuracies; E2H variants always use GRPO as optimizer, showing improvements over GRPO(All) when paired with E2H schedules.",
            "learning_speed_comparison": "All RL experiments used GRPO with identical hyperparameters and training steps (max 1600), enabling fair curriculum comparisons.",
            "generalization_performance": "GRPO(All) generalizes reasonably but is outperformed by E2H schedules on many HARD and OOD metrics.",
            "task_diversity_analysis": "Balanced GRPO provides high diversity; GRPO combined with curricula modifies sampling externally.",
            "prerequisite_identification": "Not provided by optimizer.",
            "intermediate_task_generation": "Not provided by optimizer.",
            "llm_limitations_observed": "Not applicable beyond optimizer behavior described in literature.",
            "computational_cost": "Training hyperparameters: learning_rate=1e-6, max_steps=1600; runs took ~16–18 hours on 2–3x A100 80GB for 3B models.",
            "human_expert_evaluation": "None",
            "key_findings_summary": "GRPO is a reliable RL optimizer used across experiments; curriculum/scheduling choices on top of GRPO drive the observed improvements rather than changing the optimizer.",
            "uuid": "e2036.5"
        },
        {
            "name_short": "SFT baseline",
            "name_full": "Supervised Fine-Tuning (SFT)",
            "brief_description": "Standard supervised fine-tuning baseline (imitative learning) that trains models to mimic provided reasoning examples, compared against RL-based post-training.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "none (SFT uses static training data; in appendices SFT used same configuration for fair comparison)",
            "curriculum_method_description": "SFT trains models to imitate labeled solutions; no curriculum scheduling beyond data composition; reported for comparison to show RL advantages in sparse/OOD settings.",
            "llm_model_used": null,
            "domain_environment": "Same reasoning/planning datasets",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "SFT performs well when train/test distributions closely match but poorly on broad or OOD distributions (e.g., Countdown OOD).",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Compared to GRPO(All) and E2H variants in Table 7 and Appendix",
            "performance_metrics": "SFT shows variable performance: Blocksworld SFT Trivial/Easy/Med/Hard/OOD = 100.0/97.8/88.1/55.3/16.5 (example row), but fails on some OOD splits (e.g., Countdown OOD=0% in reported example); RL/E2H often more consistent across domains.",
            "learning_speed_comparison": "Not directly comparable to RL within paper; SFT can converge quickly when data distribution is close to test distribution but generalizes poorly to OOD sparse-reward tasks.",
            "generalization_performance": "Inconsistent; SFT generalizes poorly to broad or OOD distributions in e.g., Countdown but can do well in Blocksworld when distributions align.",
            "task_diversity_analysis": "Not applicable.",
            "prerequisite_identification": "Not part of SFT protocol in paper.",
            "intermediate_task_generation": "Not applied.",
            "llm_limitations_observed": "SFT can overfit to surface patterns and not learn transferable reasoning strategies, consistent with prior literature cited.",
            "computational_cost": "Same GRPO training config used for SFT for fair comparison; specific SFT compute not separately detailed.",
            "human_expert_evaluation": "None reported",
            "key_findings_summary": "SFT performance varies by task and distribution alignment; RL-based curricula (E2H) provide more consistent gains on sparse and OOD reasoning tasks.",
            "uuid": "e2036.6"
        },
        {
            "name_short": "CRL Theory",
            "name_full": "Theoretical Analysis of Curriculum Reinforcement Learning (API viewpoint)",
            "brief_description": "An API-based theoretical framework analyzing convergence and finite-sample complexity of curriculum RL showing conditions where CRL requires fewer total samples than direct learning on the hard task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "analytical (theoretical decomposition of curriculum into interpolated MDP/task distributions)",
            "curriculum_method_description": "Formalize K interpolated MDPs {M_k} with distributions {d_k} between source d1 and target dK; apply Approximate Policy Iteration (API) with approximate evaluation/improvement errors to derive a bound on final performance gap (Theorem 3.1) and a finite-sample complexity result (Theorem 3.2). Show M_CR L = O(sum_k (...) ) and establish a sufficient condition (Equation 2) under geometric error/L_k allocation for M_CR L &lt; M_Direct.",
            "llm_model_used": null,
            "domain_environment": "Abstract MDP view general to LLM token-generation MDPs used for reasoning tasks",
            "is_interactive_text_environment": null,
            "is_compositional": null,
            "task_complexity_description": "Analysis explicitly tracks curriculum approximation error ∥Q*_K - Q*_k∥ and function approximation terms; accounts for multi-step returns and sparse terminal rewards.",
            "is_curriculum_adaptive": null,
            "baseline_comparisons": "Theoretical comparison between CRL (sequential/interpolated learning) and direct learning on final task; no empirical baselines in theory section.",
            "performance_metrics": "Theorem 3.1 bounds final performance gap E_K by sum of actor convergence bias (γ^T terms), evaluation errors δ_k, policy update error β terms, and curriculum approximation error K−1 Σ ∥Q*_K - Q*_k∥; Theorem 3.2 provides a sample complexity order: O(Σ_k log^3(1/ϵ_k)/ϵ_k^2 • Õ(L_k^2 n (1-γ)^-7 (1-γ_c)^-3 λ_min^-3)). Derived condition (Equation 2) gives when M_CR L &lt; M_Direct under geometric allocations.",
            "learning_speed_comparison": "Theoretical claim: CRL requires fewer total samples than direct learning when curriculum tasks are appropriately chosen and error allocations optimized; specific inequality provided (Equation 2) and example numeric instantiation (K=3, e*l=1.4, m=1.8) shows practical plausibility.",
            "generalization_performance": "The theory links smoother curriculum trajectories (small ∥Q*_K - Q*_k∥) and smaller approximation errors to better final generalization, providing formal justification for empirical OOD gains.",
            "task_diversity_analysis": "Curriculum discrepancy measured via ∥Q*_K - Q*_k∥ terms; analysis encourages small curriculum drift for efficiency.",
            "prerequisite_identification": "Theorem assumes existence of interpolating distributions d_k but does not provide an algorithm to identify prerequisites automatically.",
            "intermediate_task_generation": "Theory presumes intermediate tasks M_k exist to interpolate distributions; effectiveness depends on how well these are chosen (small curriculum approximation error).",
            "llm_limitations_observed": "N/A (theoretical analysis abstracts away model-specific failures but accounts for function approximation error terms).",
            "computational_cost": "Theoretical sample complexity scales are provided; no wall-clock measurements in theory.",
            "human_expert_evaluation": "None",
            "key_findings_summary": "Provides convergence guarantees for API-style CRL and finite-sample bounds; theoretically demonstrates that carefully designed curricula can be more sample-efficient than direct training on hard tasks, aligning with empirical results.",
            "uuid": "e2036.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Curriculum learning",
            "rating": 2
        },
        {
            "paper_title": "Automated curriculum learning for neural networks",
            "rating": 2
        },
        {
            "paper_title": "Curriculum learning for reinforcement learning domains: A framework and survey",
            "rating": 2
        },
        {
            "paper_title": "WISDOM: Progressive curriculum synthesis makes LLMs better mathematical reasoner",
            "rating": 2
        },
        {
            "paper_title": "Online difficulty filtering for reasoning oriented reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Logic-rl: Unleashing LLM reasoning with rule-based reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.02323275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning
7 Jun 2025</p>
<p>Shubham Parashar 
Department of Computer Science &amp; Engineering
Texas A&amp;M University</p>
<p>Shurui Gui 
Department of Computer Science &amp; Engineering
Texas A&amp;M University</p>
<p>Xiner Li 
Department of Computer Science &amp; Engineering
Texas A&amp;M University</p>
<p>Hongyi Ling 
Department of Computer Science &amp; Engineering
Texas A&amp;M University</p>
<p>Sushil Vemuri 
Department of Electrical &amp; Computer Engineering
Texas A&amp;M University</p>
<p>Blake Olson 
Department of Computer Science &amp; Engineering
Texas A&amp;M University</p>
<p>Eric Li 
Department of Computer Science &amp; Engineering
Texas A&amp;M University</p>
<p>Yu Zhang 
Department of Computer Science &amp; Engineering
Texas A&amp;M University</p>
<p>James Caverlee 
Department of Computer Science &amp; Engineering
Texas A&amp;M University</p>
<p>Dileep Kalathil 
Department of Electrical &amp; Computer Engineering
Texas A&amp;M University</p>
<p>Shuiwang Ji 
Department of Computer Science &amp; Engineering
Texas A&amp;M University</p>
<p>Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning
7 Jun 2025F237455F67428940BE3E9FB4428CEFBAarXiv:2506.06632v1[cs.LG]Unable to Learn Pretraining Distribution RL Task Distribution Trivial Tasks Easy Tasks Medium Tasks Hard Tasks Trivial Tasks Easy Tasks Medium Tasks Hard Tasks Pretraining Distribution RL Trivial Tasks Easy Tasks Medium Tasks Hard Tasks Pretraining Distribution RL Trivial Tasks Easy Tasks Medium Tasks Hard Tasks Pretraining Distribution RL Trivial Tasks Easy Tasks Medium Tasks Hard Tasks Pretraining Distribution Pretraining Distribution
We aim to improve the reasoning capabilities of language models via reinforcement learning (RL).Recent RL post-trained models like DeepSeek-R1 have demonstrated reasoning abilities on mathematical and coding tasks.However, prior studies suggest that using RL alone to improve reasoning on inherently difficult tasks is less effective.Here, we draw inspiration from curriculum learning and propose to schedule tasks from easy to hard (E2H), allowing LLMs to build reasoning skills gradually.Our method is termed E2H Reasoner.Empirically, we observe that, although easy tasks are important initially, fading them out through appropriate scheduling is essential in preventing overfitting.Theoretically, we establish convergence guarantees for E2H Reasoner within an approximate policy iteration framework.We derive finite-sample complexity bounds and show that, when tasks are appropriately decomposed and conditioned, learning through curriculum stages requires fewer total samples than direct learning.Experiments across multiple domains show that E2H Reasoner significantly improves the reasoning ability of small LLMs (1.5B to 3B), which otherwise struggle when trained with vanilla RL alone, highlighting the effectiveness of our method.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated reasoning capabilities in tasks such as multistep arithmetic, symbolic planning and code generation.However, the notion of reasoning in LLMs remains vague, with some works equating it to generating intermediate steps during problem solving [1,2].This view, although intuitive, blurs the line between genuine reasoning and surfacelevel pattern recognition [3,4].Therefore, we adopt a functional view focusing on generalization, defining reasoning as the ability to extract principles from simpler tasks and apply them to harder ones.Supporting this capability requires training methods that go beyond imitation and help models learn underlying problem-solving strategies.</p>
<p>In this direction, the success of DeepSeek R1 [5] and OpenAI o1 [6] shows that reinforcement learning (RL) based post-training enhances reasoning.RL uses task-specific rewards based on output correctness, unlike supervised fine-tuning (SFT), which trains models to imitate fixed input-output examples [7].However, RL struggles on harder tasks on which pre-trained models have low zero shot performance [8,9] and since rewards are granted only for correct answers, resulting in sparse learning signals.</p>
<p>Motivated by these limitations, we introduce the Easy 2 Hard Reasoner (E2H), a CRL approach for LLM post-training.Using E2H, we show LLMs can learn tasks they initially failed in the zeroshot setting, challenging the belief reinforced by pass@k evaluations that base models outperform RL-trained ones at a higher k [8,10] (Fig. 1).We further show that LLaMA models and small (b) Pass@k on Blocksworld Q: Using the numbers [89,48,99,28,5,39], create an equation that equals 14 <think>First, we can add 5 and 99 to get 104, then subtract 48 from that result to get 56, then subtract 28 from that to get 28, then subtract 39 from that to get -11, then subtract 5 from that to get -16, then subtract 89 from that to get -105, which is not equal to 14. Next, we can subtract 99 from 48 to get -51, then add 89 to get 38, then subtract 5 from that to get 33, then subtract 28 from that to get 5, then add 39 to get 44, which is not equal to 14.We can also try subtracting 48 from 89 to get 41, then subtracting 99 from that to get -58, then adding 5 to get -53, then adding 28 to get -25, then adding 39 to get 14.</think> <answer>89 -48 -99 + 28 + 5 + 39</answer> <answer> I am not able to find an equation </answer> E2H Reasoner (LLaMA 3. accuracy at low k values in pass@k evaluation [5,10], we show that our method, E2H Reasoner, a curriculumbased RL (CRL) approach, enables LLMs to solve tasks they previously could not, outperforming base models even at higher k.(c) Prior work [11] suggests otherwise, we show that LLaMA 3.2 3B can learn to reason effectively with E2H Reasoner.</p>
<p>models like Qwen 1.5B, often considered incapable of reasoning due to their pre-training data [11] or insufficient scale [5], can reason effectively when post-trained with E2H (Fig. 1).This capability comes from using a probabilistic scheduler in E2H that gradually shifts focus from easy to hard tasks, allowing LLMs to build foundational skills on simpler tasks and progressively solve harder ones, leading to improved generalization.On the theoretical front, we provide a comprehensive analysis of curriculum reinforcement learning through the lens of Approximate Policy Iteration.We establish the convergence guarantee for final performance gap and finite-sample analysis for sample complexity bound.Importantly, we prove a sample efficiency comparison that CRL requires fewer total samples than direct learning under appropriate design and learning of curriculum stages, which aligns with our empirical observations.Empirically, we demonstrate that E2H achieves state-of-the-art performance across five reasoning tasks, including planning tasks such as Blocksworld [12] and Countdown [13], and three arithmetic reasoning tasks ( [14], [15], and [2]), outperforming RL and curriculum baselines.</p>
<p>Background and Related Work</p>
<p>Reasoning in Large Language Models remains loosely defined, with interpretations varying by tasks and contexts [16].Prior work describes it as generating logical chains of thought [1,17], performing multi-step deductions [18,19,20], or simulating human-like problem solving [21].These views, lack clear boundaries between reasoning and pattern recognition.To address this gap, we are inspired by prior work that frames reasoning as generalization or abstraction [12,4], building on the idea that reasoning involves learning core principles and applying them broadly [3,22].</p>
<p>Post-training of Large Language Models has emerged as a popular approach to improve reasoning [23].These methods are grouped into Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) based techniques.In SFT, the model is trained to imitate outputs from carefully curated human-like reasoning examples [7,24].However, studies have shown that SFT can lead models to overfit to surface-level patterns [25], limiting generalization.In contrast, RL-based post-training uses task-specific rewards and updates the model through policy optimization algorithms [26,27,8], instead of imitation.This approach has shown greater potential in improving reasoning performance, as demonstrated by the success of models fine-tuned with RL [5,6].Still, for inherently difficult tasks that LLMs struggle to solve in zero-shot settings, post-training with RL alone has been insufficient [8,9].</p>
<p>Curriculum Learning is a training paradigm that organizes tasks in increasing order of difficulty to facilitate smoother and more effective learning [28,29].In the context of RL, it has been applied to help agents acquire complex behaviors by first mastering simpler tasks [30].Despite its potential, the application of curriculum-based RL to LLM post-training remains underexplored.Recently efforts have been made to investigate how curriculum-based RL can enhance reasoning and generalization in LLMs [31,32,9].To support gradual learning, some works generate Chain-of-Thought (CoT) data spanning a range of difficulty levels [31].Others improve learning by removing examples that are too easy or too hard [32], or by maintaining a balanced mix of task difficulties [9].Other recent efforts have implemented manual curricula that switch from easy to hard tasks after a fixed number of training iterations [33,34].In contrast, our work schedules tasks probabilistically from easy to hard, improving reasoning and generalization to out-of-distribution tasks.</p>
<p>Method</p>
<p>RL for LLM reasoning.We formulate the reasoning process of LLMs as a RL problem defined over a discounted Markov Decision Process (MDP) M = (S, A, P, r, γ), where S is the state space, A is the finite action space, P : S × A → ∆(S) is the transition kernel, r :
S × A → [0, R max ]
is the reward function, and γ ∈ (0, 1) is the discount factor.The state space S consists of all valid token prefixes, where each state s t = (x 0 , x 1 , . . ., x t ) is a sequence of tokens from the vocabulary Σ.The action space is the vocabulary itself, A = Σ.A policy π θ corresponds to the pre-trained LLM, which defines a distribution over the next token conditioned on the current prefix: π θ (x t+1 |s t ) = p θ (x t+1 |x 0 , . . ., x t ).We adopt the tags used in [5], i.e., <think> </think> and <answer> </answer>, to distinguish intermediate reasoning from final answers.The reward function is sparse: r(s, a, s ′ ) = 0 for all intermediate states, and r(s, a, s ′ ) = r(y) only when the final predicted answer y wrapped by <answer> </answer> is completed.The goal is to optimize the policy to maximize the expected cumulative reward, encouraging the model to generate correct and well-formatted answers through effective reasoning.</p>
<p>Task decomposition for RL post-training</p>
<p>While SFT provides strong supervision signals, [25] suggest that reasoning and generalization ability are more effectively enhanced through RL-based post-training.However, applying RL techniques similar to DeepSeek-R1-Zero [5] to learn complicated reasoning tasks remains challenging.In this work, we analyze these challenges in two key aspects, the distribution gap and reward design.</p>
<p>Challenge 1: Distribution gap.Learning tasks that exceed the base LLM's reasoning capabilities introduce significant learning challenges.These challenges are often caused by a non-trivial distribution gap between the model pre-training source data distribution d 0 and the target data distribution d K .As shown in Fig. 2, because rewards are only given for correct outputs, large distribution shifts can lead to low accuracy and sparse reward signals [8,9].Moreover, fitting the model to a single target distribution can lead to overfitting and memorization, undermining the model's generalization and reasoning ability.</p>
<p>Challenge 2: Reward design.Challenging reasoning tasks often require LLMs to combine multiple skills to arrive at a solution.While designing a fine-grained, step-by-step reward function could potentially guide the model effectively, such design is generally task-specific and labor-intensive.For example, a computer science student is supposed to learn basic mathematics and linear algebra before learning machine learning.Similarly, a typical Countdown [13] task involves skills like basic arithmetic, estimating the distance to the goal, and backtracking.While it is possible to include a supervision signal for each intermediate step, doing so is not sustainable and generalizable across diverse tasks.</p>
<p>To overcome these RL learning challenges, we propose decomposing a hard reasoning task into multiple easier reasoning tasks according to either human task difficulty annotations or difficulty evaluations given model performance, as shown in Fig. 2. Aligning with curriculum learning, this challenge requires us to apply data interpolation between the pre-training source data distribution d 0 and the task distribution d K , i.e., {d k } K k=1 , leading to smoother distribution changes.From the distribution gap perspective, this method mitigates the distribution gap between the model pretraining data distribution and the target task distribution, so that the reward signal becomes less sparse, improving training and sampling efficiency.From the reward design perspective, decomposing difficult tasks into simpler tasks breaks learning multiple skills into smaller steps.For instance, the goal of the 6-number countdown problem is to use 6 integers and 4 operators, + − ×÷, to reach a given target number, which requires the model to learn skills including basic arithmetic, estimating the distance to the goal, and backtracking.However, when we decompose this complex learning task into a 2-number countdown task where only 2 integers are provided, we allow LLMs to learn basic arithmetic well before tackling more complex reasoning abilities that multi-number countdown tasks require.Thus, task decomposition alleviates the burden of manual reward function design and enhances the transferability of the method across tasks.</p>
<p>In this work, we simplify a hard reasoning task into 3 progressively challenging tasks, namely, trivial, easy, and medium, then we adapt our pre-trained LLM to these tasks in a sequential curriculum.We adopt the reward design from [5], where outputs with the correct format receive partial rewards, and full rewards are given only when both format and answer are correct.In Section 3.3, we provide a theoretical justification that, under fixed sampling and training resources, learning step by step leads to better performance than training directly on the hard task.While task decomposition simplifies RL post-training, traditional curriculum learning poses two main challenges, mainly task forgetting and overfitting, due to the rigid progression through tasks in a fixed task order.</p>
<p>Training schedulers for LLM reasoning ability
Cosine Sampler
Challenges: Task forgetting and overfitting.The first challenge, task forgetting, refers to the degradation in performance on earlier (easier) tasks as the model adapts to later (harder) tasks.According to the traditional model generalization literature [35,36], the task distribution shifts are considered as explicit signals for the model generalization direction; thus, retaining strong performance across all task distributions is essential for generalization.Therefore, the task forgetting will undermine the model's generalization capability, i.e., the reasoning ability.Task overfitting, the second challenge, arises when the model overfits to trivial tasks and prefers simplistic patterns or short answers that bypass meaningful reasoning.This phenomenon is called reward hacking [37], where the model exploits shortcuts on easy tasks and fails to learn harder ones, resulting in poor reasoning performance.To address both challenges, we explore different training sampling techniques, forming four different scheduling strategies as follows.</p>
<p>Traditional scheduling.We first formulate the traditional sequential curriculum learning sampler with T training steps as S trad (t, k) = 1 when τ k ≤ t ≤ τ k+1 , otherwise, 0, where t denotes the current training step; for K tasks k = 1, . . ., K, τ k denotes the threshold when the curriculum learning proceeds to the k-th stage, while τ 1 = 0 and τ K+1 = T .The output of the sampler denotes the probability of sampling data from the k-th task; therefore, at the t step, the sampling distribution will be [S trad (t, 1), S trad (t, 2), . . ., S trad (t, K)].</p>
<p>Balanced scheduling.To avoid forgetting, the simplest way is to mix all data with different difficulties together and sample randomly, which can be considered as a trivial case of curriculum learning.Alternatively, this can be interpreted as the default behavior of any policy optimization algorithm [8,26], where training occurs without considering task difficulty.This balanced sampler can be written as: S balanced (t, k) = 1 K , where each task difficulties have the same probability to be selected at each training step.Although this is an efficient way to avoid forgetting, this sampling introduces harder tasks too early, leading to sparser rewards and suboptimal CRL.</p>
<p>Cosine scheduling (E2H-C).</p>
<p>To alleviate both the reward sparsity and forgetting issues, we propose a non-parametric scheduling strategy, namely, cosine sampling.This strategy can be written as:
S cosine (t, k) = α t • (K − k − 1) + (1 − α t ) • k, and α t = 0.5 • (1 + cos( πt T ))
, where the resulting probabilities need to be normalized before sampling.Intuitively, this cosine sampler sets both the initial and ending sampling probabilities simply according to their ordinal number and interpolates   the intermediate probabilities using a cosine function.As shown in Figure 3, the easiest task has the highest probability of being sampled at the beginning, and has the lowest probability of being sampled at the end.
y R G G k 7 E i D M v X 3 R Y q F 1 l M R 2 E 2 B z V g v e 3 P x X y 8 Q S 8 k m r P s p k 3 F i q C S L 4 D D h y E R o 3 g M a M k W J 4 V N L M F H M / o 7 I G C t M j G 2 r Z E v x l i t Y J e 2 L q l e r 1 m 4 v K 4 1 6 X k 8 R T u A U z s G D K 2 j A D T S h B Q R G 8 A w v 8 O o 8 O W / O u / O x W C 0 4 + c 0 x / I H z + Q N L 9 J R G &lt; / l a t e x i t &gt;G r s G G 7 F y Q y W Q / L q 5 q j p m d Q k = " &gt; A A A B + n i c b V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r s i S c q A j W X E v C B Z w u x k N h k y M 7 v M z I p h z S f Y a m 8 n t v 6 M r V / i J N l C E w 9 c O J x z L + d y g p g z b V z 3 y 8 l t b G 5 t 7 + R 3 C 3 v 7 B 4 d H x e O T t o 4 S R W i L R D x S 3 Q B r y p m k L c M M p 9 1 Y U S w C T j v B 5 G b u d x 6 o 0 i y S T T O N q S / w S L K Q E W y s d P 8 4 a A 6 K J b f s L o D W i Z e R E m R o D I r f / W F E E k G l I R x r 3 f P c 2 P g p V o Y R T m e F f q J p j M k E j 2 j P U o k F 1 X 6 6 e H W G L q w y R G G k 7 E i D F u r v i x Q L r a c i s J s C m 7 F e 9 e b i v 1 4 g V p J N W P N T J u P E U E m W w W H C k Y n Q v A c 0 Z I o S w 6 e W Y K K Y / R 2 R M V a Y G N t W w Z b i r V a w T t p X Z a 9 S r t x d l + q 1 r J 4 8 n M E 5 X I I H V a j D L T S g B Q R G 8 A w v 8 O o 8 O W / O u / O x X M 0 5 2 c 0 p / I H z + Q O E x J R q &lt; / l a t e x i t &gt; x T (a) Gaussian sampler &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s V 8 / Y o U c f s 4 1 g U p 7 u G R L Y U A b T P A = " &gt; A A A C D n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u A q J a H U j F N 2 4 r G A v 0 I Q y m Z 6 0 Q 2 e S M D M R S u g 7 + A p u d e 9 O 3 P o K b n 0 S p 2 0 W 2 v r D g Y / / n M M 5 / G H K m d K u + 2 U t L a + s r q 2 X N s q b W 9 s 7 u / b e f l M l m a T Q o A l P Z D s k C j i L o a G Z 5 t B O J R A R c m i F w 9 t J v / U I U r E k f t C j F A J B + j G L G C X a W F 3 7 0 A 9 B k 2 v X u f C x r 1 h f T L l r V 1 z H n Q o v g l d A B R W q d + 1 v v 5 f Q T E C s K S d K d T w 3 1 U F O p G a U w 7 j s Z w p S Q o e k D x 2 D M R G g g n z 6 / h i f G K e H o 0 S a i j W e u r 8 3 c i K U G o n Q T A q i B 2 q + N z H / 7 Y V i 7 r K O r o K c x W m m I a a z w 1 H G s U 7 w J B v c Y x K o 5 i M D h E p m f s d 0 Q C S h 2 i R Y N q F 4 8 x E s Q v P M 8 a p O 9 f 6 8 U r s p 4 i m h I 3 S M T p G H L l E N 3 a E 6 a i C K c v S M X t C r 9 W S 9 W e / W x 2 x 0 y S p 2 D t A f W Z 8 / T H i a r w = = &lt; / l a t e x i t &gt; ω = 0.5 ε = 0.5 (c) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q 8 6 K w J c Z 0 W P J Y y g 3 D k B q o K P 2 U y g = " &gt; A A A C E H i c b V C 7 T s M w F H X K q 5 R X g I G B x a J C Y o q S i r Y s S B U s j E W i D 6 m J K s d 1 W q u 2 E 9 k O U l X 1 J / g F V t j Z E C t / w M q X 4 L Q Z o O V I V z o + 5 1 5 d 3 x M m j C r t u l 9 W Y W 1 9 Y 3 O r u F 3 a 2 d 3 b P 7 A P j 9 o q T i U m L R y z W H Z D p A i j g r Q 0 1 Y x 0 E 0 k Q D x n p h O P b z O 8 8 E q l o L B 7 0 J C E B R 0 N B I 4 q R N l L f P v F D o t G 1 6 1 S q P v Q V H f L s U a / 2 7 b L r u H P A V e L l p A x y N P v 2 t z + I c c q J 0 J g h p X q e m + h g i q S m m J F Z y U 8 V S R A e o y H p G S o Q J y q Y z g + Y w X O j D G A U S 1 N C w 7 n 6 e 2 K K u F I T H p p O j v R I L X u Z + K 8 X 8 q X N O r o K p l Q k q S Y C L x Z H K Y M 6 h l k 6 c E A l w Z p N D E F Y U v N 3 i E d I I q x N h i U T i r c c w S p p V x y v 5 t T u L 8 u N m z y e I j g F Z + A C e K A O G u A O N E E L Y D A D z + A F v F p P 1 p v 1 b n 0 s W g t W P n M M / s D 6 / A F I h Z s s &lt; / l a t e x i t &gt; ω = 0.25 ε = 0.75 (b) (d) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J i f d y G v P S 2 G e K b 5 E 6 V 8 f g w d u L M Y = " &gt; A A A C E H i c b V C 7 T s M w F H X K q 5 R X g I G B x a J C Y o q S i r Y s S B U s j E W i D 6 m J K s d 1 W q u 2 E 9 k O U l X 1 J / g F V t j Z E C t / w M q X 4 L Q Z o O V I V z o + 5 1 5 d 3 x M m j C r t u l 9 W Y W 1 9 Y 3 O r u F 3 a 2 d 3 b P 7 A P j 9 o q T i U m L R y z W H Z D p A i j g r Q 0 1 Y x 0 E 0 k Q D x n p h O P b z O 8 8 E q l o L B 7 0 J C E B R 0 N B I 4 q R N l L f P v F D o t G 1 6 9 S r P v Q V H f L s U a n 2 7 b L r u H P A V e L l p A x y N P v 2 t z + I c c q J 0 J g h p X q e m + h g i q S m m J F Z y U 8 V S R A e o y H p G S o Q J y q Y z g + Y w X O j D G A U S 1 N C w 7 n 6 e 2 K K u F I T H p p O j v R I L X u Z + K 8 X 8 q X N O r o K p l Q k q S Y C L x Z H K Y M 6 h l k 6 c E A l w Z p N D E F Y U v N 3 i E d I I q x N h i U T i
Gaussian scheduling (E2H-G).Although cosine scheduling addresses reward sparsity and forgetting, the parameter-free design limits flexibility in handling issues like trivial task overfitting and finegrained control over different learning stages.Empirically, while adding the trivial task can boost the model performance, it is also easy for the model to overfit to trivial tasks.To overcome this challenge, we propose a Gaussian scheduling strategy inspired by the Gaussian mixture model [38].</p>
<p>As shown in Figure 4 (a), in a one-dimensional space, we assume the data distributions of tasks follow Gaussian distributions with the same variance σ.The means of the adjacent task's Gaussian distributions are assumed to have the same distance 1, i.e., µ k = k − 1.Then the sampling probability is defined as the likelihood of a given position x t belonging to different task Gaussian distributions, similar to the expectation-maximization algorithm [39].Therefore, the Gaussian scheduling strategy can be expressed as:
S Gaussian (t, k) = exp − (x t − µ k ) 2 2σ 2
, and
x t = t T β (K − 1) ,(1)
where we ignore the normalization term, and the probabilities will be normalized for sampling.In this sampling scheduler, we only have two hyperparameters, i.e., σ and β.While the variance σ controls the sampling concentration, β &gt; 0 controls the sampling position x t 's moving speed.When σ is smaller, the training is more focused, more similar to traditional curriculum learning.When β &lt; 1, the sampling process will assign fewer training steps focusing on easier tasks and train harder tasks longer, avoiding easy task overfitting.As shown in Figure 4 (bcd), we use three typical hyperparameter settings for this Gaussian sampler, i.e., β = 0.25 σ = 0.75, β = 0.75 σ = 0.25, and β = 0.5 σ = 0.5.</p>
<p>Theoretical Analysis</p>
<p>In this section, we analyze our curriculum reinforcement learning framework for LLMs under the lens of Approximate Policy Iteration (API).We follow the theoretical structure of [40,41], adapting it to our curriculum setting.Specifically, we show how sequentially solving interpolated curriculum distributions enables convergence guarantees and improved sample complexity bounds, under function approximation errors.Our analysis considers action-value functions and explicitly tracks the impact of approximation and distribution shift across curriculum stages.All proofs are presented in Appx. A.</p>
<p>Theoretical Setup</p>
<p>Recall our MDP M = (S, A, P, r, γ).We denote the policy space as Π, where each policy π ∈ Π is a mapping from states to distributions over actions, π : S → ∆(A).In our curriculum setting, we introduce a sequence of MDPs {M k } K k=1 where each MDP shares the same state and action spaces but may differ in reward functions and/or transition dynamics.Each MDP induces a state visitation distribution d k under the optimal policy for that curriculum, with {d k } K k=1 interpolating between an easy source distribution d 1 and the final hard task d K .Let π k be the learned policy at curriculum step k, and let π * K be the optimal policy under the final target task.The goal is to learn a sequence of policies {π k } K k=1 such that the final policy π K performs well under the target distribution d K .The action-value function of a policy π is defined as
Q π (s, a) := E [ ∞ t=0 γ t r(s t , a t )|s 0 = s, a 0 = a, a t ∼ π(•|s t ), s t+1 ∼ P (•|s t , a t )]
. Let T π be the Bellman operator for policy π, defined as
T π Q(s, a) := r(s, a) + γE s ′ ∼P (•|s,a),a ′ ∼π(•|s ′ ) [Q(s ′ , a ′ )] .
We define the optimal Bellman operator T , T Q(s, a) := r(s, a)
+ γE s ′ ∼P (•|s,a) [max a ′ Q(s ′ , a ′ )] .
The fixed point of the optimal Bellman operator is
T Q * = Q * .
The Approximate Policy Iteration (API) framework is a generalization of classical policy iteration that accommodates function approximation and inexact updates.It serves as a foundational tool for analyzing practical reinforcement learning algorithms including actor-critic and deep RL methods.The API algorithm alternates between two stages.At iteration k, the algorithm performs two algorithmic steps.(i) Policy Evaluation: Given policy π k , compute an approximate estimate Q k of its action-value function Q π k , (ii) Policy Improvement: Update the policy to π k+1 , which is greedy or approximately greedy with respect to Q k .In the context of CRL, API provides a natural structure to analyze how sequentially adapted policies over interpolated curriculum distributions evolve toward the optimal policy and influence the final policy π K .</p>
<p>We adopt the following API assumptions from [40,41], adapted to the curriculum setting.Let Q k := Q π k be the action-value function for the policy π k at curriculum k.
Approximate Policy Evaluation. At each iteration k, a function approximator Q k is used to estimate Q k . The estimated Q-function Q π k k satisfies ∥ Q π k k − Q π k k ∥ ∞ ≤ δ k .
Approximate Greedy Policy Improvement.Let π k+1 be an ϵ k -greedy policy with respect to Q k :
E s∼µ k [Q π k+1 (s, π k+1 (s))] ≥ E s∼µ k max a Q k (s, a) − ϵ k , for a sampling distribution µ k at step k, i.e., ∥T Q π k k − Q π k+1 k ∥ ∞ ≤ ϵ k .
Distribution Mismatch (Concentrability).Let µ k be the sampling distribution for policy improvement at step k, and d k the true task distribution.We assume the distribution mismatch between µ k and d k is bounded by C k , i.e., sup s∈S
d k (s) µ k (s) ≤ C k .
Bounded Curriculum Drift.For the weighted norm,
∥Q∥ d k = E s∼d k [max a Q(s, a) 2 ]. The deviation between successive optimal Q-functions satisfies ∥Q * K − Q * k ∥ d K is bounded for all k. 3.3.2 Convergence Guarantee Let Q *
K be the optimal Q-function for the final task under distribution d K , and let Q π K be the Q-function of the policy learned at the final step.Define the performance loss of the final policy compared to the optimal target policy as:
L K := ∥Q * K − Q π K ∥ d K . Theorem 3.1 (CRL Performance Guarantee).
Let T be the number of API policy updates within each task.β &gt; 0 is a tunable parameter for stepsizes specified in [40].Under the approximate greedy update and evaluation error assumptions above, the final performance gap L K satisfies:
E K ≤ K k=1 γ T η k + 2γ(1 − γ T ) (1 − γ) 2 δ k + 2γ β(1 − γ) 2 + K−1 k=1 ∥Q * K − Q * k ∥ d K ,
where
η k := ∥Q * k − Q π k k ∥ ∞ is the per-task Bellman error, δ k is the evaluation error, and ϵ k is absorbed in η k .
The first term represents the convergence bias of the actor, and goes to zero geometrically fast as T goes to infinity.The second term captures accumulated evaluation errors, which involves the stochastic error due to sampling and the error due to function approximation The third term captures the error introduced by the policy update rule, which can be made arbitrarily small by using large enough β.Alternatively, we can use geometrically increasing stepsizes, in which case the third term goes to zero at a geometric rate.The last term captures the deviation in optimal Q-functions across curriculum stages, representing the cumulative gap between intermediate curriculum-optimal values and the final optimal value, which we refer to as the curriculum approximation error.This decomposition highlights the dual effect of CRL in improving sample efficiency (small δ k , ϵ k ) and ensuring smooth interpolation (small
∥Q * K − Q * k ∥).</p>
<p>Finite Sample Approximation Error Analyses</p>
<p>In this section, we perform finite-sample analysis.Considering the critic, i.e., how to obtain an estimate of Q π , we can estimate the Q-function Q π of a given target policy π using TD-learning.In TD-learning, especially when the state-action space size is large, the use of function approximation is natural.In linear function approximation, a set of basis vectors is selected to approximate the target value function Q π using linear combinations of the basis vectors.Let Φ ∈ R |S||A|×d be the matrix of basis vectors
Φ = [ϕ 1 , • • • , ϕ d ].
Then, the goal is to find from the linear subspace
Q = { Qw = Φw | w ∈ R d } the "best" approximation of the Q-function Q π , where w ∈ R d is the weight vector.
Following Section 3 and 4 of [40], we derive the finite-sample theorem for CRL.Theorem A.1 and its analyses are provided in Appx.A.1.1.The result of Theorem A.1 suggests that smoother curriculum trajectories and smaller approximation errors under finite samples, which is closely associated with the stepsize and number of updates per iteration, can lead to better final policy performance.</p>
<p>Following [40], let K SA ∈ R |S||A|×|S||A| be a diagonal matrix and let K SA,min be the minimal diagonal entry.J k is the number of critic updates per policy iteration at curriculum k.The bootstrapping parameter n is chosen such that γ c :=
γ n / K SA,min &lt; 1. L k is the parameter defined by 1 + (γρ max,k ) n , E approx,k := sup π Q π k c,ρ − Φw π k c,ρ ∞ is the critic's function approximation error, ∥Q * K − Q * k ∥ ∞
is the gap between the curriculum subtask and the final task, and λ min is the mininum eigenvalue of the positive definite matrix
Φ T K SA Φ. The term N 2,1 = K k=1 2γE approx,k
(1−γ) 2 in Theorem A.1 is the function approximation error and equals zero when using a complete basis.A term of similar form is present in all existing work studying RL with function approximation [42,15].Based on Theorem A.1, we next derive the sample complexity result.Theorem 3.2 (Sample Complexity).For a given accuracy level ϵ &gt; 0, to achieve E
[∥Q * K − Q π K ∥ ∞ ] ≤ ϵ + N 2,1 + K−1 k=1 ∥Q * K − Q * k ∥ d K
, the total number of samples (i.e., the integer T K k=1 J k ) required across all K curriculum stages is of the order:
O K k=1 log 3 (1/ϵ k ) ϵ 2 k • Õ L 2 k n (1 − γ) 7 (1 − γ c ) 3 λ 3 min ,
where ϵ k is the target accuracy for curriculum k, and
K k=1 ϵ k ≤ ϵ. Let M CRL = K k=1
M k be the total number of samples needed by CRL with K curriculum steps to achieve error ϵon the final task, and let M Direct = m * M K be the number of samples needed by direct learning, where m &gt; 1 represents the relative difficulty factor of direct learning.Under geometric error and L k allocation ϵ k = ϵ K • e K−k and L k = L K l K−k for curriculums, we have:
M CRL &lt; M Direct ⇐⇒ (e * l) 2(1−K) − 1 1 − (e * l) 2 &lt; m − 1.(2)
The geometric error and L k allocation reflect the curriculum gradually increases in difficulty while allowing larger errors in earlier stages.Since the function f (x) = x 1−K −1 1−x 2 monotonically decrease for x &gt; 1 when K is a integer larger than 1, the final condition can be reasonably satisfied, e.g., with K = 3, e * l = 1.4,and m = 1.8 in practice.</p>
<p>The first half of Theorem 3.2 analysis highlights the dual benefit of curriculum design in CRL.First, by constructing intermediate distributions d 1 , . . ., d K−1 close to d K , the curriculum error term ∥Q * K − Q * k ∥ can be made small.Second, easier curriculum tasks improve estimation accuracy and yield more stable approximate greedy updates, enhancing sample efficiency and policy improvement.For the second half of Theorem 3.2, the final condition in Eq. 2 is satisfied when curriculums are appropriately learned, the allocation of accuracy targets ϵ k is gradually optimized across curriculum steps, and the designed curriculums with increasing difficulties effectively bridge the gap between source and target distributions.This mathematical derivation shows CRL can theoretically require fewer total samples than direct learning on the final task, which aligns with experimental observations.</p>
<p>Experiments</p>
<p>In practice, we conduct experiments to investigate the following research questions.RQ1: What role does task decomposition play in RL-based post-training?RQ2: How does task scheduling impact the learning process?RQ3: Can small-scale LLMs learn to reason on hard tasks?</p>
<p>Experimental Setup</p>
<p>We evaluate our method on a diverse set of reasoning and planning tasks, covering both datasets with and without human-annotated difficulty levels.For datasets with difficulty labels, such as Blocksworld (using plan length) [4], Countdown (using number of operations) [11], and MATH (using problem levels) [14], we use the provided annotations and define an out-of-distribution (OOD) split to assess generalization.For datasets without explicit difficulty, namely GSM8K [2] and AQuA [43], we estimate difficulty using the zero shot error rate of the base model.Specifically, for each question in the training set, we generate 20 responses and compute the error rate as 1 − Number of Correct Responses 20 and bucket the samples into trivial, easy, medium, and hard, based on quartiles Fig. 5.More dataset details are in the Appendix.We conduct experiments using three LLMs spanning 1.5 to 3 billion parameters, namely Qwen 2.5/1.5BInstruct, Qwen 2.5/3B Instruct [44], and LLaMa 3.2 3B Instruct [45].We use Qwen 1.5B for research questions RQ1 and RQ2.</p>
<p>Baselines</p>
<p>Firt, we report the Chain of Thought [1] (CoT) performance for all models.All post-training experiments use GRPO [8] as the reinforcement learning algorithm (c.f.Appendix for implementation).GRPO, by default, employs balanced scheduling over all tasks, which we use as our baseline and refer to as GRPO in Table 3.We also assess whether models can learn directly from the most challenging examples by training only on the Hard and OOD subsets.In addition, we evaluate traditional curriculum learning (CL), which introduces tasks in a fixed order from easy to hard.Finally, we provide a comparison against SFT in the Appendix.</p>
<p>Experimental Results</p>
<p>We report results addressing the research questions listed above.We examine how task decomposition impacts LLM post-training using Qwen-1.5B-Instruct(RQ1) with a balanced scheduler (Table 1).We find that including trivial and easy examples helps the model build core skills.This enables effective transfer from simpler to harder tasks and better OOD performance, consistent with our view of reasoning as learning core principles and applying them to harder tasks.Practically, easier tasks yield dense rewards, helping the model begin learning effectively and gradually adapt to harder tasks.</p>
<p>Next, we examine how scheduling impacts post-training (RQ2).Balanced scheduling over mixeddifficulty data serves as a strong baseline but lacks structure, leading to suboptimal learning.Traditional curriculum learning (CL) schedules tasks in a fixed order, which can cause forgetting of earlier tasks or overfitting to easier ones.In our experiments, easier tasks help initiate learning due to their dense rewards, but overexposure hinders generalization to harder, sparse-reward tasks.This motivates the need for more effective scheduling and we address these limitations with our proposed E2H-C and E2H-G (see Section 3.2).For tasks like MATH, where models perform reasonably well across all difficulty levels (as seen with CoT in Table 2), the cosine scheduling in E2H-C is effective, beginning with a focus on trivial and easy tasks and gradually shifting toward harder ones.However, on tasks like Blocksworld, where rewards are sparse and harder examples are more challenging, this leads to overfitting and degraded performance.E2H-G addresses this issue by using a Gaussian schedule that quickly decays the sampling probability of trivial and easy tasks.As shown in Fig. 4, it provides enough exposure early on to support initial learning while rapidly shifting focus to harder Table 3: Results of E2H Reasoner across three models on Blocksworld [4], Countdown [13] and MATH [14].</p>
<p>Our method consistently improves performance especially on HARD and OOD tasks, demonstrating effective reasoning with small-scale LLMs.examples.This prevents overfitting and improves generalization in sparse-reward settings where E2H-C struggles.Further qualitative analyses are provided in the Appendix.</p>
<p>Building on these insights, we investigate whether models can learn to reason directly from difficult examples (RQ3).As shown in Table 3, the answer is largely no.For instance, Qwen-2.5 1.5B when trained on Level 5 MATH directly, underperforms the CoT baseline on the model.Moreover, stronger models like Qwen-2.5 3B can learn Hard Blocksworld tasks directly but fail at the trivial and easy tasks, indicating a lack of basic understanding.These failures highlight the limitations of relying on difficult examples alone.In contrast, E2H Reasoner guides learning by scheduling tasks from easy to hard, improving generalization, as reflected in stronger OOD performance (RQ3).Note that for E2H-G we report the best numbers out of our 3 parameter settings with extensive results in the Appendix.Our results show that small-scale LLMs can learn to reason if taught with task decomposition and appropriate scheduling.Finally, E2H Reasoner remains effective even without human difficulty labels by using error rates as a proxy for difficulty (Table 4).</p>
<p>Summary and Limitations</p>
<p>We introduce the E2H Reasoner (E2H), a CRL-based method for LLM post-training.E2H enables models to learn tasks they initially failed at by scheduling tasks from easy to hard.E2H challenges the assumption that small LLMs cannot reason and demonstrates strong empirical performance supported by theoretical analysis, offering convergence guarantees and improved sample efficiency over direct RL.Despite the strengths of our method, it has limitations, we explore only simple, intuitive schedulers, and more adaptive, advantage-based strategies could yield further gains.Additionally, we</p>
<p>have not yet scaled post-training to larger models.In summary, E2H provides a scalable, theoretically grounded, and practical method for LLM reasoning.</p>
<p>A Proofs</p>
<p>We now present the proofs for the main theoretical results stated above.Our analysis closely follows the derivation style of [40,41], extending it to the curriculum setting.</p>
<p>A.1 Proof of Theorem 3.1</p>
<p>Proof.We aim to derive a tight bound for the final performance error in task T K :
E K := ∥Q * K − Q π K K ∥ d K , where Q *
K is the optimal Q-function for the final task T K , and Q π K K is the Q-function under the learned policy π K .</p>
<p>We begin by analyzing the policy performance error under API.For each task k, we define:
η k := ∥Q * k − Q π k k ∥ ∞ .
By norm monotonicity, we have:
∥Q * K − Q π K K ∥ d K ≤ ∥Q * K − Q π K K ∥ ∞ .
We proceed by recalling the result from Theorem 2.1 of [40] for a fixed task k, applied with a step size β:
∥Q * k − Q π k k ∥ ∞ ≤ γ T ∥Q * k − Q π 0 k k ∥ ∞ + 2γ 1 − γ T −1 t=0 γ T −1−t δ k + 2γ β(1 − γ) 2 ,
where δ k is the approximate value evaluation error at curriculum step k; γ
T ∥Q * k − Q π 0 k k ∥ ∞ reflects initialization at π 0 k ;
and β &gt; 0 is the API soft update step size parameter.The finite geometric series
T −1 t=0 γ T −1−t has closed-form expression T −1 i=0 γ i = 1 − γ T 1 − γ for γ ̸ = 1 and T −1 i=0 γ i = 1, for γ = 1.
Thus we simplify with
T −1 i=0 γ i = 1 − γ T 1 − γ .
Since we denote:
η k := ∥Q * k − Q π k k ∥ ∞ ;
then, the bound becomes:
η k ≤ γ T η k + 2γ(1 − γ T ) (1 − γ) 2 δ k + 2γ β(1 − γ) 2 .
We isolate η k :
η k (1 − γ T ) ≤ 2γ(1 − γ T ) (1 − γ) 2 δ k + 2γ β(1 − γ) 2 ⇒ η k ≤ 1 1 − γ T 2γ(1 − γ T ) (1 − γ) 2 δ k + 2γ β(1 − γ) 2 .
We want to bound:
∥Q * K − Q π K K ∥ d K ;
using triangle inequality:
∥Q * K − Q π K K ∥ d K ≤ ∥Q * K − Q π K K ∥ ∞ ≤ η K + K−1 k=1 ∥Q * K − Q * k ∥ d K .
Let us now plug in the bound for η K :
∥Q * K − Q π K K ∥ d K ≤ 1 1 − γ T 2γ(1 − γ T ) (1 − γ) 2 δ k + 2γ β(1 − γ) 2 + K−1 k=1 ∥Q * K − Q * k ∥ d K .
The distribution mismatch is defined by
C k := d K d k ∞
. If d k is constructed by curriculum to smoothly interpolate toward d K , and both d K and d k are supported on the same or growing state space.In our analysis, we use:
∥Q * k − Q π k k ∥ d K ≤ C k • ∥Q * k − Q π k k ∥ ∞ , where ∥ • ∥ d K is the L 2 norm under the distribution d K and ∥ • ∥ ∞ is the sup norm (worst-case).
In general, for any probability distribution µ, and any function f ,
∥f ∥ L2(µ) = f (x) 2 dµ(x) 1/2 ≤ ∥f ∥ ∞ .
So, in fact, we have the reverse:
∥f ∥ d K ≤ ∥f ∥ ∞ ⇒ C k ≤ 1.</p>
<p>If we do want to write
∥f ∥ d K ≤ C k • ∥f ∥ ∞ ,
then the tightest possible value for C k is exactly 1, and any value C k &lt; 1 may be possible depending on the support of f under d K .This holds when later curriculum stages subsume earlier ones, a design principle in CRL.Hence, using C k ≤ 1 simplifies bounds without loosening them unnecessarily.</p>
<p>Because curriculum proceeds sequentially, and assuming that earlier tasks are easier and learned more accurately, we can sum the per-step bounds for curriculum steps:
E K = ∥Q * K − Q π K K ∥ d K ≤ K k=1 1 1 − γ T 2γ(1 − γ T ) (1 − γ) 2 δ k + 2γ β(1 − γ) 2 + K−1 k=1 ∥Q * K − Q * k ∥ d K .
This completes the proof.</p>
<p>Optionally, assuming γ T is small for large T , 1/(1 − γ T ) ≤ 2 leads to:
E K ≲ K k=1 4γ(1 − γ T ) (1 − γ) 2 δ k + 4γ β(1 − γ) 2 + K−1 k=1 ∥Q * K − Q * k ∥ d K .
A.1.1 Theorem A.1 and Proof Theorem A.1 (Finite-Sample Guarantee).Consider the sequence of tasks {T k } K k=1 , where the final task T K has optimal action-value function Q * K .Suppose that π K is the final policy produced by CRL and assumptions hold for all curriculum stages.Following [40], let K SA ∈ R |S||A|×|S||A| be a diagonal matrix and let K SA,min be the minimal diagonal entry.J k is the number of critic updates per policy iteration at curriculum k.The bootstrapping parameter n is chosen such that γ c := γ n / K SA,min &lt; 1.Then, when using constant stepsize α satisfying α(t α + n + 1)
≤ (1−γc)λmin 130L 2 , we have for all J k ≥ t α + n + 1: E [∥Q * K − Q π K ∥ ∞ ] ≤ γ T K ∥Q * K − Q π0 ∥ ∞ N1 + K k=1 ( 2γE approx,k (1 − γ) 2 N2,1 + 2γ 2 E bias,k (1 − γ) 4 N2,2 ) + K k=1 6(1 − (1 − γ c )λ min α) 1 2 [J k −(tα+n+1)] (1 − γ) 3 (1 − γ c ) 1/2 λ 1/2 min N2,3 + K k=1 70L k [α(t α + n + 1)] 1/2 λ min (1 − γ c )(1 − γ) 3 N2,4 + 2γβK (1 − γ) 2 N3 + K−1 k=1 ∥Q * K − Q * k ∥ d K Curriculum discrepancy
, where c(•, •) and ρ(•, •) are generalized importance sampling factors, L k is the parameter defined by
1 + (γρ max,k ) n , E approx,k := sup π Q π k c,ρ − Φw π k c,ρ ∞ is the critic's function approximation error, E bias,k := max 0≤t≤T max s∈S (1 − λ(s)) ∥π k,t (•|s) − π k,b (•|s)∥ 1 is the importance sampling bias at curriculum k, ∥Q * K − Q * k ∥ ∞
is the gap between the curriculum subtask and the final task, and λ min is the mininum eigenvalue of the positive definite matrix Φ T K SA Φ.</p>
<p>The terms N 1 and N 3 are the same as appeared in Thm.3.1, and together capture the error in actor update.Term N 2,1 is the function approximation error and equals zero when using a complete basis.A term of similar form is present in all existing work studying RL with function approximation [42,15].Term N 2,2 is the bias introduced by generalized importance sampling factors c(•, •) and ρ(•, •), and N 2,2 = 0 when c(s, a) = ρ(s, a) = π(a|s)/π b (a|s).Term N 2,3 represents the convergence bias in the critic and goes to zero geometrically fast as the inner loop iteration index J k goes to infinity.The term N 2,4 represents the variance in the critic and is proportional to
√ αt α = O( α log(1/α)), thus
arbitrarily small given small enough stepsize α.Together, {N 2,i } i=1∼4 correspond to the second term in Thm.3.1.Finally, the curriculum approximation error is the same as the last term in in Theorem 3.1.This result suggests that smoother curriculum trajectories and smaller approximation errors under finite samples, which is closely related with the stepsize and number of updates per iteration, can lead to better final policy performance.</p>
<p>Proof.We proceed by mathematical induction across the curriculum steps.Let π k,T denote the final policy after T iterations at curriculum step k.Base case: For the first curriculum step (k = 1), we directly apply Theorem 4.1 from [40]:
E[∥Q 1 − Q π 1,T ∥ ∞ ] ≤ γ T ∥Q 1 − Q π0 1 ∥ ∞(3)+ 2γE approx,1 (1 − γ) 2 + 2γ 2 E bias,1 (1 − γ) 4(4)+ 6(1 − (1 − γ c )λ min α) 1 2 [J1−(tα+n+1)] (1 − γ) 3 (1 − γ c ) 1/2 λ 1/2 min(5)+ 70L 1 [α(t α + n + 1)] 1/2 λ min (1 − γ c )(1 − γ) 3 + 2γβ (1 − γ) 2(6)
Inductive step: Assume that for curriculum step k, we have:
E[∥Q k − Q π k,T ∥ ∞ ] ≤ γ T k ∥Q 1 − Q π0 1 ∥ ∞ + k j=1 2γE approx,j (1 − γ) 2 + 2γ 2 E bias,j (1 − γ) 4 + 6(1 − (1 − γ c )λ min α) 1 2 [Jj −(tα+n+1)] (1 − γ) 3 (1 − γ c ) 1/2 λ 1/2 min + 70L j [α(t α + n + 1)] 1/2 λ min (1 − γ c )(1 − γ) 3 + 2γβ (1 − γ) 2 + k−1 j=1 ∥Q k − Q j ∥ d k
For curriculum step k + 1, we initialize with policy π k,T and apply Theorem 4.1 from [40]:
E[∥Q k+1 − Q π k+1,T ∥ ∞ ] ≤ γ T ∥Q k+1 − Q π k,T k+1 ∥ ∞ + 2γE approx,k+1 (1 − γ) 2 + 2γ 2 E bias,k+1 (1 − γ) 4 + 6(1 − (1 − γ c )λ min α) 1 2 [J k+1 −(tα+n+1)] (1 − γ) 3 (1 − γ c ) 1/2 λ 1/2 min + 70L k+1 [α(t α + n + 1)] 1/2 λ min (1 − γ c )(1 − γ) 3 + 2γβ (1 − γ) 2 We need to relate ∥Q * k+1 − Q π k,T
k+1 ∥ ∞ to our induction hypothesis.By triangle inequality:
∥Q k+1 − Q π k,T k+1 ∥ ∞ ≤ ∥Q k+1 − Q k ∥ ∞ + ∥Q k − Q π k,T k ∥ ∞ + ∥Q π k,T k − Q π k,T k+1 ∥ ∞ ≤ ∥Q k+1 − Q k ∥ ∞ + ∥Q * k − Q π k,T k ∥∞ + γ 1 − γ ∥r k − r k+1 ∥ ∞
where the last inequality follows from the performance difference lemma with respect to rewards.For curriculum learning, we design the reward functions to satisfy ∥r k − r k+1 ∥ ∞ ≤ δ r for some small δ r .Thus:
∥Q k+1 − Q π k,T k+1 ∥ ∞ ≤ ∥Q k+1 − Q k ∥ ∞ + ∥Q k − Q π k,T k ∥ ∞ + γδ r 1 − γ Substituting our induction hypothesis: E[∥Q k+1 − Q π k+1,T ∥ ∞ ] ≤ γ T E[∥Q k+1 − Q k ∥ ∞ + ∥Q k − Q π k,T k ∥ ∞ + γδ r 1 − γ ] + 2γE approx,k+1 (1 − γ) 2 + 2γ 2 E bias,k+1 (1 − γ) 4 + 6(1 − (1 − γ c )λ min α) 1 2 [J k+1 −(tα+n+1)] (1 − γ) 3 (1 − γ c ) 1/2 λ 1/2 min + 70L k+1 [α(t α + n + 1)] 1/2 λ min (1 − γ c )(1 − γ) 3 + 2γ β(1 − γ) 2 ≤ γ T ∥Q k+1 − Q k ∥ ∞ + γ T E[∥Q * k − Q π k,T k ∥ ∞ ] + γ T +1 δ r 1 − γ + 2γE approx,k+1 (1 − γ) 2 + 2γ 2 E bias,k+1 (1 − γ) 4 + 6(1 − (1 − γ c )λ min α) 1 2 [J k+1 −(tα+n+1)] (1 − γ) 3 (1 − γ c ) 1/2 λ 1/2 min + 70L k+1 [α(t α + n + 1)] 1/2 λ min (1 − γ c )(1 − γ) 3 + 2γβ (1 − γ) 2
Applying the induction hypothesis:
E[∥Q k+1 − Q π k+1,T ∥ ∞ ] ≤ γ T ∥Q k+1 − Q k ∥ ∞ + γ T • γ T k ∥Q 1 − Q π0 1 ∥ ∞ + γ T k j=1 2γE approx,j (1 − γ) 2 + 2γ 2 E bias,j (1 − γ) 4 + 6(1 − (1 − γ c )λ min α) 1 2 [Jj −(tα+n+1)] (1 − γ) 3 (1 − γ c ) 1/2 λ 1/2 min + 70L j [α(t α + n + 1)] 1/2 λ min (1 − γ c )(1 − γ) 3 + 2γ β(1 − γ) 2 + γ T k−1 j=1 ∥Q k − Q j ∥ d k + γ T +1 δ r 1 − γ + 2γE approx,k+1 (1 − γ) 2 + 2γ 2 E bias,k+1 (1 − γ) 4 + 6(1 − (1 − γ c )λ min α) 1 2 [J k+1 −(tα+n+1)] (1 − γ) 3 (1 − γ c ) 1/2 λ 1/2 min + 70L k+1 [α(t α + n + 1)] 1/2 λ min (1 − γ c )(1 − γ) 3 + 2γβ (1 − γ) 2 Simplifying: E[∥Q k+1 − Q π k+1,T ∥ ∞ ] ≤ γ T (k+1) ∥Q 1 − Q π0 1 ∥ ∞ + k+1 j=1 2γE approx,j (1 − γ) 2 + 2γ 2 E bias,j (1 − γ) 4 + 6(1 − (1 − γ c )λ min α) 1 2 [Jj −(tα+n+1)] (1 − γ) 3 (1 − γ c ) 1/2 λ 1/2 min + 70L j [α(t α + n + 1)] 1/2 λ min (1 − γ c )(1 − γ) 3 + 2γβ (1 − γ) 2 + k j=1 ∥Q k+1 − Q j ∥ d k+1
where we used the fact that
∥Q * k+1 − Q * k ∥ d k+1 + γ T k−1 j=1 ∥Q * k − Q * j ∥ d k ≤ k j=1 ∥Q * k+1 − Q * j ∥ d k+1(7)
due to the triangle inequality and the curriculum design.By induction, for the final curriculum step K, we have:
E[∥Q K − Q π K ∥ ∞ ] ≤ γ T K ∥Q 1 − Q π0 1 ∥ ∞ + K k=1 2γE approx,k (1 − γ) 2 + 2γ 2 E bias,k (1 − γ) 4 + 6(1 − (1 − γ c )λ min α) 1 2 [J k −(tα+n+1)] (1 − γ) 3 (1 − γ c ) 1/2 λ 1/2 min + 70L k [α(t α + n + 1)] 1/2 λ min (1 − γ c )(1 − γ) 3 + 2γβ (1 − γ) 2 + K−1 k=1 ∥Q K − Q k ∥ d K
This completes the proof.</p>
<p>A.1.2 Proof of Theorem 3.2</p>
<p>Proof.From the Finite-Sample Theorem for CRL, we aim to control all error terms to achieve the desired accuracy.We allocate the total error budget ϵ across the K curriculum steps, with ϵ k being the error allocation for step k, such that K k=1 ϵ k ≤ ϵ.For each curriculum k, we need to control the following terms:
γ T K ∥Q * 1 − Q π0 1 ∥ ∞ ≤ ϵ 4 (8) 6(1 − (1 − γ c )λ min α) 1 2 [J k −(tα+n+1)] (1 − γ) 3 (1 − γ c ) 1/2 λ 1/2 min ≤ ϵ k 4 (9) 70L k [α(t α + n + 1)] 1/2 λ min (1 − γ c )(1 − γ) 3 ≤ ϵ k 4(10)2γβ (1 − γ) 2 ≤ ϵ k 4(11)
We can solve each of these constraints.For the first constraint, we need:
T ≥ 1 K log γ ϵ 4∥Q * 1 − Q π0 1 ∥ ∞ Since ∥Q * 1 − Q π0 1 ∥ ∞ ≤ 1 1−γ , we have: T ≥ 1 K log γ ϵ(1 − γ) 4 = O log(1/ϵ) K
For the second constraint, we need:
(1 − (1 − γ c )λ min α) 1 2 [J k −(tα+n+1)] ≤ ϵ k (1 − γ) 3 (1 − γ c ) 1/2 λ 1/2 min 24 ,
thus by taking log,
1 2 [J k − (t α + n + 1)] log(1 − (1 − γ c )λ min α) ≤ log ϵ k (1 − γ) 3 (1 − γ c ) 1/2 λ 1/2 min 24 , thus J k ≥ (t α + n + 1) + 2 log ϵ k (1−γ) 3 (1−γc) 1/2 λ 1/2 min 24 log(1 − (1 − γ c )λ min α) .
Using log(1 − x) ≈ −x for small x, and α(t α + n + 1)
≤ (1−γc)λmin 130L 2 k
, we have:
J k ≥ (t α + n + 1) − 2 log 24 ϵ k (1−γ) 3 (1−γc) 1/2 λ 1/2 min (1 − γ c )λ min α = (t α + n + 1) + 2 log ϵ k (1−γ) 3 (1−γc) 1/2 λ 1/2 min 24 (1 − γ c )λ min α
For the third constraint, we need:
α(t α + n + 1) ≤ ϵ 2 k (1 − γ) 6 (1 − γ c ) 2 λ 2 min 4900L 2 k = O ϵ 2 k (1 − γ) 6 (1 − γ c ) 2 λ 2 min L 2 k
For the fourth constraint, we need:
β ≤ ϵ k (1 − γ) 2 8γ
Combining these constraints, the dominant factor in the sample complexity comes from the third constraint, which gives us:
α ≤ O ϵ 2 k (1 − γ) 6 (1 − γ c ) 2 λ 2 min L 2 k (t α + n + 1)
The mixing time t α is O(log(1/α)), which gives us:
α ≤ O ϵ 2 k (1 − γ) 6 (1 − γ c ) 2 λ 2 min L 2 k (log(1/α) + n + 1) This implies: α = O ϵ 2 k (1 − γ) 6 (1 − γ c ) 2 λ 2 min L 2 k log(1/α) α log(1/α) = O ϵ 2 k (1 − γ) 6 (1 − γ c ) 2 λ 2 min L 2 k
Using the Lambert W function, we can solve for α:
α = Θ   ϵ 2 k (1 − γ) 6 (1 − γ c ) 2 λ 2 min L 2 k log L 2 k ϵ 2 k (1−γ) 6 (1−γc) 2 λ 2 min   = Θ ϵ 2 k (1 − γ) 6 (1 − γ c ) 2 λ 2 min L 2 k
where Θ hides logarithmic factors.The number of samples required for curriculum step k is:
M k = J k • T = O (t α + n + 1) + log(1/ϵ k ) (1 − γ c )λ min α • O log(1/ϵ) K = O log(1/α) + n + 1 K log(1/ϵ) + log(1/ϵ k ) log(1/ϵ) K(1 − γ c )λ min α = O log(1/ϵ) log L 2 k ϵ 2 k (1−γ) 6 (1−γc) 2 λ 2 min + n log(1/ϵ) K + log(1/ϵ k ) log(1/ϵ)L 2 k log L 2 k ϵ 2 k (1−γ) 6 (1−γc) 2 λ 2 min Kϵ 2 k (1 − γ) 6 (1 − γ c ) 3 λ 3 min = Õ n log(1/ϵ) K + log(1/ϵ k ) log(1/ϵ)L 2 k Kϵ 2 k (1 − γ) 6 (1 − γ c ) 3 λ 3 min
The second term dominates, giving us:
M k = Õ log 2 (1/ϵ k ) log(1/ϵ)L 2 k Kϵ 2 k (1 − γ) 6 (1 − γ c ) 3 λ 3 min (12)
The total number of samples across all curriculum steps is:
M CRL = K k=1 M k = K k=1 Õ log 2 (1/ϵ k ) log(1/ϵ)L 2 k Kϵ 2 k (1 − γ) 6 (1 − γ c ) 3 λ 3 min = Õ log(1/ϵ) K K k=1 log 2 (1/ϵ k )L 2 k ϵ 2 k (1 − γ) 6 (1 − γ c ) 3 λ 3 min
Assuming ϵ k = ϵ K for all k (uniform error allocation), we get:
M CRL = Õ log(1/ϵ) K K k=1 log 2 (K/ϵ)L 2 k (ϵ/K) 2 (1 − γ) 6 (1 − γ c ) 3 λ 3 min = Õ log(1/ϵ) log 2 (K/ϵ)K ϵ 2 (1 − γ) 6 (1 − γ c ) 3 λ 3 min K k=1 L 2 k = Õ log 3 (1/ϵ)K ϵ 2 (1 − γ) 6 (1 − γ c ) 3 λ 3 min K k=1 L 2 k
Adding the dependence on the bootstrapping parameter n, we have:
M CRL = O K k=1 log 3 (1/ϵ k ) ϵ 2 k • Õ L 2 k n (1 − γ) 7 (1 − γ c ) 3 λ 3 min (13)
This completes the proof for the first half of the Theorem.Now we move on to prove the second half of the Theorem.</p>
<p>The sample complexity for the final task K is:
M K = O log 3 (1/ϵ K ) ϵ 2 K • L 2 K • C (1 − γ) 7 (1 − γ c ) 3 λ 3 min .
We can factor this term out of the total sum:
M CRL = M K • 1 + K−1 k=1 L 2 k • log 3 (1/ϵ k ) • ϵ 2 K L 2 K • log 3 (1/ϵ K ) • ϵ 2 k .
Define the curriculum efficiency factor:
CEF = 1 + K−1 k=1 L 2 k • log 3 (1/ϵ k ) • ϵ 2 K L 2 K • log 3 (1/ϵ K ) • ϵ 2 k .
This factor represents the ratio of total curriculum sample complexity to the sample complexity of just the final task.We make two structure assumptions about our curriculum design, geometric error allocation and L k progression,
ϵ k = ϵ K • e K−k and L k = L K /l K−k
, where e &gt; 1.This reflect a curriculum that gradually increases in difficulty while allowing larger errors in earlier stages.For a well-designed curriculum where early tasks are simpler than later ones, l &gt; 1.</p>
<p>For the error ratio term:
ϵ 2 K ϵ 2 k = ϵ 2 K (ϵ K • e K−k ) 2 = e −2(K−k) ,
and for the L k ratio term:
L 2 k L 2 K = L 2 k (L k • l K−k ) 2 = l −2(K−k) .
For the logarithmic term:
log(1/ϵ k ) = log(1/(ϵ K • e K−k )) = log(1/ϵ K ) − (K − k) log(e).
Substituting these expressions into CEF :
CEF = 1 + K−1 k=1 l −2(K−k) • [log(1/ϵ K ) − (K − k) log(e)] 3 • e −2(K−k) log 3 (1/ϵ K ) .
Since e &gt; 1 and K ≥ k, (K − k) log(e) ≥ 0. Thus we have
CEF = 1 + K−1 k=1 [log(1/ϵ K ) − (K − k) log(e)] 3 log 3 (1/ϵ K ) • (el) −2(K−k) ≤ 1 + K−1 k=1 (el) −2(K−k) .
The sum is a geometric series:
K−1 k=1 (el) −2(K−k) = K−1 k=1 (el) 2(k−K) = (el) 2(1−K) 1 − (el) 2(K−1) 1 − (el) 2 = (el) 2(1−K) − 1 1 − (el) 2
CRL is more sample-efficient than direct learning when:
M CRL &lt; M Direct
Since M CRL = M K • CEF and for a comparable direct learning approach, M Direct = M K • m where m represents the relative difficulty factor of direct learning, our condition becomes:
CEF &lt; m.
Thus for M CRL &lt; M Direct , substituting expressions it becomes:
M CRL = M K • CEF ≤ M K • (1 + (el) 2(1−K) − 1 1 − (el) 2 ) &lt; M K • m = M Direct . Thus M CRL &lt; M Direct ⇐⇒ (el) 2(1−K) − 1 1 − (el) 2 &lt; m − 1.
This completes the proof.</p>
<p>B Dataset Details</p>
<p>In this section we provide details of the datasets used for evaluation.We categorize the datasets into two categories, datasets that contain human annotated difficulties and others that do not.</p>
<p>B.1 Datasets with Human Annotated Difficulties</p>
<p>Blocksworld [4] is a dataset used to evaluate the planning capabilities of LLMs.Each task involves transitioning from an initial block configuration to a target configuration, which requires LLMs to generate a sequence of actions, or plan to achieve the goal.Tasks become more difficult as the required number of steps increases, since the model must reason over longer sequences and maintain correct intermediate states.</p>
<p>To study this, we group tasks into four in-distribution difficulty levels: Trivial with 1 step, Easy with 2 steps, Medium with 4 steps, and Hard with 6 steps.Additionally, we include an out-of-distribution (OOD) split with 8-step plans to test generalization beyond the training distribution.Trivial tasks are especially simple because the model only needs to predict one correct action out of four possible choices to complete the plan.This setup allows the LLM to first grasp fundamental planning mechanics, which can then be leveraged to learn more complex multi-step tasks.</p>
<p>Countdown [13] is a task where the model must reach a target value by combining given numbers using basic arithmetic operations.While the original dataset uses four numbers per instance, we extend it to create a range of difficulty levels based on the number of input numbers, mainly, Trivial (2), Easy (3), Medium (4), Hard (5), and OOD (6).As the number of inputs increases, the space of possible operation sequences grows rapidly, making it harder for the model to identify the correct combination and order of operations.In contrast, the trivial setting is extremely simple, requiring  just one operation between two numbers to reach the target, allowing the model to first learn basic arithmetic before scaling to more complex multi-step problems.</p>
<p>MATH [14] is a benchmark of 7,500 training and 5,000 test problems covering high-school level mathematics, with each problem labeled from Level 1 (easiest) to Level 5 (hardest).The dataset covers topics such as algebra, geometry, number theory, and probability, including step-by-step solutions.We create a difficulty-based setup using the existing labels, specifically, Trivial (Level 1), Easy (Level 2), Medium (Level 3), Hard (Level 4), and OOD (Level 5).As difficulty increases, problems require more complex reasoning, multi-step solutions, and deeper mathematical understanding.Trivial problems are typically short and rely on basic techniques, making them ideal for teaching foundational reasoning.We use Levels 1 through 4 for training and treat Level 5 as out-of-distribution to assess generalization to the most difficult problems.</p>
<p>We provide the summery of the difficulty splits for each dataset in Table 5.</p>
<p>B.2 Datasets without Human Annotated Difficulties</p>
<p>GSM8K [2] is a dataset of high-quality, linguistically diverse grade school math word problems designed to evaluate multi-step arithmetic reasoning.Each problem typically requires between two and eight steps involving basic arithmetic operations such as addition, subtraction, multiplication, and division.To assess performance across varying difficulty levels, we create a 4-way split into Trivial, Easy, Medium, and Hard, based on model error rates, as described in Section. 4 and illustrated in Figure .5</p>
<p>AQuA [46] is a dataset of algebraic word problems with multiple-choice answers and detailed rationales, designed to test arithmetic reasoning and symbolic manipulation.For our experiments, we randomly sample 5,000 problems for training.Similar to GSM8K, we use model error rates to define four difficulty levels, Trivial, Easy, Medium, and Hard (see Figure . 6).</p>
<p>Since both GSM8K and AQuA lack explicit difficulty annotations, we construct difficulty splits using model error rates and do not include an out-of-distribution (OOD) category for these datasets.</p>
<p>D Implementation Details</p>
<p>In this section, we provide implementation details for E2H Reasoner.We begin by describing the hardware setup, followed by training parameters, and finally the inference settings used to report the final results.</p>
<p>D.1 Hardware Details</p>
<p>For our experiments, we use up to three 80GB NVIDIA A100 GPUs, particularly for the 3B parameter models.One GPU is dedicated to vLLM for fast inference, while the remaining two are used for model training.On average our experiments require anywhere between 16-18 hours for training.</p>
<p>D.2 Training Details</p>
<p>We use GRPO as the policy optimization algorithm, combined with parameter-efficient fine-tuning using LoRA.All models are trained for up to 1600 GRPO steps, with hyperparameters listed in Table 6.For fair comparison, the same configuration is used in our SFT experiments.All training is conducted using the TRL library from Hugging Face.</p>
<p>D.3 Inference Details</p>
<p>For inference, we use a temperature of 0.0 in all experiments to ensure deterministic outputs and reproducibility.For pass@k evaluations, we enable sampling with a temperature of 0.7, top_p of 0.9, and top_k of 50, where top_k controls the number of candidate tokens considered at each decoding step.</p>
<p>E Comparisons with SFT</p>
<p>We compare vanilla supervised fine-tuning (SFT) with RL-based post-training methods (see Table 7).The results show that SFT performance varies significantly across tasks.For instance, in Blocksworld [4], where most problems involve fewer than four blocks, SFT performs well-the LLM  learns effectively due to the small gap between the training and test distributions.This aligns with prior work suggesting that SFT is most effective when the training data is high quality and closely aligned with the downstream task [24,25].In contrast, on tasks like Countdown, where the problem distribution is much broader, SFT performs notably worse than RL-based post-training.This gap is especially clear in the out-of-distribution (OOD) setting, where SFT fails to solve any Countdown OOD examples, unlike in Blocksworld, where its performance generalizes more successfully.We highlight these two tasks to illustrate how SFT performance can vary across distributions, in contrast to the more consistent behavior of RL-based post-training methods.</p>
<p>F Additional Results and Analysis of Scheduling</p>
<p>In this section, we expand on the results presented in the main paper by reporting all three parameter settings of Gaussian scheduling in Table 8.Similarly, for Qwen 2.5 1.5B Instruct we include the results for all Gaussian scheduling variants on GSM8K and AQuA in Table 9.Finally, we provide a</p>
<p>Figure 1 :
1
Figure 1: (a, b) Reinforcement learning (RL) based post-training is commonly believed to primarily improve</p>
<p>Figure 3 :
3
Figure 3: Illustration of cosine scheduling.</p>
<p>&lt;</p>
<p>l a t e x i t s h a 1 _ b a s e 6 4 = " F n I e K P U 1 E S D u D I u t m M 9 + s F i 3 p 3 0 = " &gt; A A A B + n i c b V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r s i M W X A x j K i e U C y h N n J b D J k Z n a Z m R X D m k + w 1 d 5 O b P 0 Z W 7 / E y W Y L T T x w 4 X D O v Z z L C W L O t H H d L 6 e w t r 6 x u V X c L u 3 s 7 u 0 f l A + P 2 j p K F K E t E v F I d Q O s K W e S t g w z n H Z j R b E I O O 0 E k + u 5 3 3 m g S r N I 3 p t p T H 2 B R 5 K F j G B j p b v H g T s o V 9 y q m w G t E i 8 n F c j R H J S / + 8 O I J I J K Q z j W u u e 5 s f F T r A w j n M 5 K / U T T G J M J H t G e p R I L q v 0 0 e 3 W G z q w</p>
<p>x 0 &lt;
0
l a t e x i t s h a 1 _ b a s e 6 4 = " E C r 5 M Q 2 E R / K a 8 O I 5 b J e i y f L k E p Q = " &gt; A A A B + n i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H a N Q Y 4 k X j x i l E c C G z I 7 D D B h d n Y z 0 2 s k K 5 / g V e / e j F d / x q t f 4 g B 7 U L C S T i p V 3 a l O B b E U B l 3 3 y 8 m t r W 9 s b u W 3 C z u 7 e / s H x c O j p o k S z X i D R T L S 7 Y A a L o X i D R Q o e T v W n I a B 5 K 1 g f D 3 z W w 9 c G x G p e 5 z E 3 A / p U I m B Y B S t d P f Y w 1 6 x 5 J b d O c g q 8 T J S g g z 1 X v G 7 2 4 9 Y E n K F T F J j O p 4 b o 5 9 S j Y J J P i 1 0 E 8 N j y s Z 0 y D u W K h p y 4 6 f z V 6 f k z C p 9 M o i 0 H Y V k r v 6 + S G l o z C Q M 7 G Z I c W S W v Z n 4 r x e E S 8 k 4 q P q p U H G C X L F F 8 C C R B C M y 6 4 H 0 h e Y M 5 c Q S y r S w v x M 2 o p o y t G 0 V b C n e c g W r p H l R 9 i r l y u 1 l q V b N 6 s n D C Z z C O X h w B T W 4 g T o 0 g M E Q n u E F X p 0 n 5 8 1 5 d z 4 W q z k n u z m G P 3 A + f w C 3 R J S K &lt; / l a t e x i t &gt; x t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z 1 6 z</p>
<p>25 Figure 4 :
254
Figure 4: Gaussian Sampler.(a) This figure represents the Gaussian sampling process.(bcd) These figures denote the sampling probabilities of different tasks changing along the training steps with different Gaussian sampler hyperparameters.</p>
<p>Figure 6 :
6
Figure 6: AQuA Difficulty distribution based on error rates, grouped into quartiles.</p>
<p>Table 8 :
8
100.0 100.0 98.8 44.1 17.6 95.0 89.9 73.3 46.5 24.3 E2H-C 100.0 0.0 0.0 0.0 0.0 100.0 55.3 0.0 0.0 0.0 Effect of scheduling strategies in LLM post-training.We compare balanced scheduling, traditional curriculum learning (CL), and our proposed E2H Reasoner variants, namely, E2H-G and E2H-C.CoT is reported as a reference.Blocksworld Countdown MATH Trivial Easy Med Hard OOD Trivial Easy Med Hard OOD Trivial Easy Med Hard OOD Qwen 2(0.25, 0.75) 100.0 100.0 96.4 53.3 23.2 98.5 90.8 71.0 43.5 19.4 90.4 81.0 71.5 58.2 37.2 E2H-G (0.5, 0.5) 100.0 100.0 89.2 32.9 10.6 99.6 89.3 73.4 41.7 9.4 91.1 78.2 70.5 56.6 35.0 E2H-G (0.75, 0.25) 100.0 100.0 42.9 3.9 2.0 100.0 87.8 52.3 16.7 6.5 90.0 79.3 70.6 57.1 35.9 E2H-C 100.0 100.0 94.1 52.0 22.5 100.0 90.4 69.7 35.7 12.6 90.4 80.0 73.4 58.9 38.1 LLaMA 3.2 3B Instruct CoT 24.0 0.0 1.2 1.0 0.0 37.1 4.6 0.3 0.0 0.0 65.9 44.6 35.2 24.1 13.6 Balanced 100.0 100.0 94.1 38.9 13.3 99.9 89.5 71.6 47.9 2.7 65.9 47.0 36.0 22.0 10.2 CL 100.0 0.0 0.0 0.0 0.0 17.2 36.0 22.7 11.2 4.1 74.1 54.1 43.9 28.0 12.5 E2H-G (0.25, 0.75) 100.0 100.0 98.8 44.1 17.6 95.0 89.9 73.3 46.5 24.3 78.7 58.4 46.4 32.3 14.5 E2H-G (0.5, 0.5) 100.0 100.0 88.1 26.3 9.6 94.4 86.7 65.1 27.9 2.3 66.1 45.0 36.2 24.4 11.4 E2H-G (0.75, 0.25) 100.0 97.8 75.0 21.1 4.0 98.9 89.4 48.6 0.7 0.0 63.8 48.2 35.6 24.0 8.6 E2H-C 100.0 100.0 15.5 0.0 0.0 96.7 64.0 25.9 15.8 6.4 84.6 69.6 63.0 47.6 28.6</p>
<p>Figure 2: Task Decomposition of Easy 2 Hard Reasoner (E2H).E2H first decomposes the overall task into levels of increasing difficulty, namely trivial, easy, and medium, to help the LLM acquire core skills.As training progresses, E2H schedules harder tasks accordingly.See Section 3.2 for scheduling details.</p>
<p>Table 1 :
1
Impact of task decomposition for LLM post-training.Trivial and easy examples help the model learn core principles that enable success on harder tasks.
BlocksworldCountdownMATHTrivial Easy Med Hard OOD Trivial Easy Med Hard OOD Trivial Easy Med Hard OODHard0.00.00.00.00.00.043.9 16.418.16.582.364.7 53.438.220.6Med + Hard2.00.00.00.00.012.947.8 33.119.28.887.168.9 58.142.521.0Easy + Med + Hard0.055.5 15.50.00.062.579.3 30.121.29.584.868.2 56.842.521.5Trivial + Easy + Med + Hard98.0100 83.321.12.696.164.9 28.818.19.287.272.0 61.646.325.7</p>
<p>Table 2 :
2
Effect of scheduling strategies in LLM post-training.We compare balanced scheduling, traditional curriculum learning (CL), and our proposed E2H Reasoner variants, namely, E2H-G and E2H-C.CoT is reported as a reference.
BlocksworldCountdownMATHTrivial Easy Med Hard OOD Trivial Easy Med Hard OOD Trivial Easy Med Hard OODCoT4.00.00.00.00.016.05.61.70.10.140.1 27.9 22.7 17.68.2Balanced98.0100 84.5 26.35.396.1 64.9 28.8 18.19.287.272 61.6 46.325.7CL46.0100 45.25.80.757.7 85.8 57.2 31.512.686.2 71.5 62.4 46.725.6E2H-G (0.25, 0.75)98.0 100.0 95.3 32.97.398.9 87.3 51.4 18.97.385.572 64.1 47.926.5E2H-G (0.5, 0.5)100100 34.5 10.50.795.2 84.2 48.1 28.114.285.3 71.7 62.5 48.727.6E2H-G (0.75, 0.25)98.093.3 17.92.00.095.7 56.0 28.8 17.110.286.0 72.4 62.0 46.726.3E2H-C100100 15.50.00.096.7 64.0 25.9 15.86.484.6 69.6 63.0 47.628.6</p>
<p>Table 4 :
4
Performance of E2H Reasoner on GSM8K and AQuA, which lack human difficulty labels.We use error rates to create difficulty-based splits of the data, which are then used for scheduling.Fig.5illustratesthese splits.
BlocksworldCountdownMATHTrivial Easy Med Hard OOD Trivial Easy Med Hard OOD Trivial Easy Med Hard OODCoT4.0 0.0 0.0 0.0 0.0 16.0 5.6 1.7 0.1 0.1 40.1 27.9 22.7 17.6 8.2GRPO (All)98.0 100 83.3 21.1 2.6 96.1 64.9 28.8 18.1 9.2 87.2 72.0 61.6 46.3 25.7GRPO (Hard)0.0 0.0 0.0 0.0 0.00.0 43.9 16.4 18.1 6.5 82.3 64.7 53.4 38.2 20.6Qwen 1.5B InstructGRPO (OOD)0.0 0.0 0.0 0.0 0.03.1 23.1 18.1 11.3 5.3 37.0 21.1 15.0 8.5 3.7CL46.0 100 45.2 5.8 0.7 57.7 85.8 57.2 31.5 12.6 86.2 71.5 62.4 46.7 25.6E2H-G98.0 100 95.3 32.9 7.3 95.2 84.1 48.1 28.1 14.2 85.3 71.7 62.5 48.7 27.6E2H-C100 100 15.5 0.0 0.0 96.7 64.0 25.9 15.8 6.4 84.6 69.6 63.0 47.6 28.6CoT1.0 6.7 7.1 1.0 0.0 24.0 15.9 4.4 0.5 0.0 68.0 55.8 48.8 33.4 20.0GRPO (All)100 100 88.1 38.3 14.6 99.6 89.3 73.4 41.7 9.4 90.1 78.5 71.9 57.4 36.8GRPO (Hard) 50.0 42.2 86.9 72.4 48.31.6 49.5 38.7 27.7 12.6 90.6 78.4 70.5 58.0 36.0Qwen 3B InstructGRPO (OOD)0.0 13.3 9.5 0.0 0.00.4 25.1 21.2 13.8 8.5 91.3 78.1 70.0 58.7 36.8CL100 100 77.3 42.1 20.5 39.0 91.0 72.9 38.7 12.5 90.6 78.9 72.5 57.2 36.6E2H-G100 100 96.4 53.3 23.2 98.5 90.8 71.0 43.5 19.4 90.4 81.0 71.5 58.2 37.2E2H-C100 100 94.1 52.0 22.5100 90.4 69.7 35.7 12.6 90.4 80.0 73.4 58.9 38.1CoT24.0 0.0 1.2 1.0 0.0 37.1 4.6 0.3 0.0 0.0 65.9 44.6 35.2 24.1 13.6GRPO (All)100 100 94.1 38.9 13.3 99.9 89.5 71.6 47.9 2.7 65.9 47.0 36.0 22.0 10.2GRPO (Hard)0.0 0.0 0.0 0.0 0.0 40.5 33.8 3.0 9.7 1.4 22.7 14.4 10.3 7.5 0.3LLaMa 3.2 3B InstructGRPO (OOD)0.0 0.0 0.0 0.0 0.08.0 0.3 0.0 0.0 0.0 63.6 39.0 31.3 19.0 7.6CL100 0.0 0.0 0.0 0.0 17.2 36.0 22.7 11.2 4.1 74.1 54.1 43.9 28.0 12.5E2H-G100 100 98.8 44.1 17.6 95.0 89.9 73.3 46.5 24.3 78.7 58.4 46.4 32.3 14.5E2H-C100 0.0 0.0 0.0 0.0100 55.3 0.0 0.0 0.0 74.8 60.6 48.3 34.3 15.8Figure 5: GSM8K Difficulty distribution based on errorrates, grouped into quartiles.Error Rate Distribution by Difficulty BucketQwen 1.5B Instruct GSM8K Trivial Easy Med Hard Avg Trivial Easy Med Hard Avg AQuA 90.2 87.3 76.5 38.1 67.7 70.8 51.3 20.8 2.6 40.9 GRPO 99.0 95.3 84.1 49.9 77.1 95.8 68.0 48.6 21.0 63.3 CoT CL 98.0 97.2 85.8 52.2 78.6 88.8 72.2 36.1 18.4 58.6Number of Questions200 300 400 500 600Trivial Easy Medium HardE2H-G 97.6 94.7 89.0 51.8 78.7 90.2 81.9 43.0 34.2 66.1 E2H-C 98.0 95.3 83.9 46.6 75.7 86.1 72.2 48.6 26.3 62.510000.00.20.4 Error Rate 0.60.81.0</p>
<p>Table 5 :
5
Difficulty splits for datasets with human-annotated difficulty levels.Each dataset is categorized based on task-specific properties, specifically, plan length for Blocksworld, number of operands for Countdown, and problem level for MATH.
DifficultyBlocksworld CountdownMATH(Plan Length) (Num. Operands) (Problem Level)Trivial (T)121Easy (E)232Medium (M)443Hard (H)654OOD865Number of Questions200 300 400 500Error Rate Distribution by Difficulty Bucket Trivial Easy Medium Hard10000.00.20.4 Error Rate 0.60.81.0</p>
<p>Table 6 :
6
Training hyperparameters used for GRPO post-training and LoRA adaptation.For our experiments, we use LLaMA 3.2 3B Instruct, Qwen 2.5 1.5B Instruct, and Qwen 2.5 3B Instruct.Qwen 2.5 1.5B Instruct is released under the Apache 2.0 License, Qwen 2.5 3B Instruct under the Qwen Research License, and LLaMA 3.2 3B Instruct under the Meta Community License.
Component ParameterValuelearning_rate1e-6lr_scheduler_typecosineper_device_train_batch_size2gradient_accumulation_steps4gradient_checkpointingtrueGRPOmax_steps1600bf16truetf32truenum_generations8beta0.001use_vllmtruevllm_gpu_memory_utilization0.2r32alpha64LoRAdropout0.1target_modulesq_proj, v_projtask_typeCAUSAL_LMC Model License Information</p>
<p>Table 7 :
7
Comparison of RL-based post-training and supervised fine-tuning (SFT).SFT performance variesnoticeably across tasks, highlighting its inconsistency in generalizing across domains.
ModelsMethodsBlocksworldCountdownTrivial Easy Med Hard OOD Trivial Easy Med Hard OODSFT100.0 97.8 88.1 55.3 16.5 97.4 41.8 14.2 4.6 0.0Qwen 2.5 / 1.5B InstructGRPO98.0 100.0 84.5 26.3 5.3 96.1 64.9 28.8 18.1 9.2
AcknowledgmentsThis work was supported in part by National Institutes of Health under grant U01AG070112 and ARPA-H under grant 1AY1AX000053.
Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, NeurIPS. 352022</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Chain of thoughtlessness? an analysis of cot in planning. Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati, NeurIPS2024</p>
<p>On the planning abilities of large language models-a critical investigation. Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati, NeurIPS. 362023</p>
<p>Deepseek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, NeurIPS. 352022</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Li, Wu, arXiv:2402.03300Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, Junxian He, arXiv:2503.188922025arXiv preprint</p>
<p>Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, Gao Huang, arXiv:2504.13837Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?. 2025arXiv preprint</p>
<p>Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D Goodman, arXiv:2503.013072025arXiv preprint</p>
<p>Llms still can't plan; can lrms? a preliminary evaluation of openai's o1 on planbench. Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati, arXiv:2409.133732024arXiv preprint</p>
<p>Stream of search (sos): Learning to search in language. Kanishk Gandhi, Denise , H J Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, Noah Goodman, CoLM. 2024</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, NeurIPS2021</p>
<p>On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Alekh Agarwal, Jason D Sham M Kakade, Gaurav Lee, Mahajan, Journal of Machine Learning Research. 22982021</p>
<p>Natural language reasoning, a survey. Fei Yu, Hongbo Zhang, Prayag Tiwari, Benyou Wang, ACM Computing Surveys. 56122024</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, ICLR2023</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, ICLR. 2023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, NeurIPS. 362023</p>
<p>Hongyi Ling, Shubham Parashar, Sambhav Khurana, Blake Olson, Anwesha Basu, Gaurangi Sinha, Zhengzhong Tu, James Caverlee, Shuiwang Ji, arXiv:2502.19295Complex LLM planning via automated heuristics discovery. 2025arXiv preprint</p>
<p>Shubham Parashar, Blake Olson, Sambhav Khurana, Eric Li, Hongyi Ling, James Caverlee, Shuiwang Ji, arXiv:2502.12521Inference-time computations for LLM reasoning and planning: A benchmark and insights. 2025arXiv preprint</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, ICLR2024</p>
<p>Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar, ICLR2025</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Sergey Quoc V Le, Yi Levine, Ma, arXiv:2501.17161Sft memorizes, rl generalizes: A comparative study of foundation model post-training. 2025arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, NeurIPS. 362023</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, ICML. 2009</p>
<p>Automated curriculum learning for neural networks. Alex Graves, Jacob Marc G Bellemare, Remi Menick, Koray Munos, Kavukcuoglu, ICML. 2017</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, Peter Stone, Journal of Machine Learning Research. 211812020</p>
<p>WISDOM: Progressive curriculum synthesis makes LLMs better mathematical reasoner. Chenhao Qiu, Qianglong Chen, Jintang Li, Caiyu Wang, Runsen Hua, Minghui Li, Shengshan Hu, Yechao Zhang, 2025</p>
<p>Online difficulty filtering for reasoning oriented reinforcement learning. Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, Jeongyeon Nam, Donghyun Kwak, arXiv:2504.033802025arXiv preprint</p>
<p>Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, Chong Luo, arXiv:2502.14768Logic-rl: Unleashing LLM reasoning with rule-based reinforcement learning. 2025arXiv preprint</p>
<p>Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, arXiv:2501.12599Kimi k1. 5: Scaling reinforcement learning with LLMs. 2025arXiv preprint</p>
<p>Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, David Lopez-Paz, arXiv:1907.02893Invariant risk minimization. 2019arXiv preprint</p>
<p>In search of lost domain generalization. Ishaan Gulrajani, David Lopez-Paz, ICLR. 2021</p>
<p>Correlated proxies: A new definition and improved mitigation for reward hacking. Cassidy Laidlaw, Shivam Singhal, Anca Dragan, ICLR2025</p>
<p>Gaussian mixture models. Encyclopedia of biometrics. Reynolds Douglas, 20097413</p>
<p>The EM algorithm and extensions. Geoffrey J Mclachlan, Thriyambakam Krishnan, 2008John Wiley &amp; Sons</p>
<p>An approximate policy iteration viewpoint of actor-critic algorithms. Zaiwei Chen, Siva Theja, Maguluri , arXiv:2208.032472022arXiv preprint</p>
<p>Approximate policy iteration schemes: A comparison. Bruno Scherrer, ICML. 2014</p>
<p>Error bounds for approximate policy iteration. Rémi Munos, ICML. 2003</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, ACL. 2017</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.1511520245 technical report. arXiv preprint</p>
<p>The llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv:2407.217832024arXiv preprint</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, ACL. 2017</p>
<p>Can cause overfitting to easy tasks. </p>
<p>May lead to forgetting earlier tasks. </p>
<p>E2H-C 1. Parameter-free and simple to apply. </p>
<p>Suitable for tasks with similar zero-shot performance across difficulty levels. </p>
<p>May overfit to easy tasks when rewards for hard tasks are sparse. </p>
<p>E2H-G 1. Effective across tasks and models. </p>
<p>Enables fine-grained control. </p>
<p>Requires tuning two hyperparameters. </p>
<p>This advancement holds significant societal implications. Enhanced reasoning in LLMs can improve decision-making processes in critical domains such as healthcare, education, and legal systems, where nuanced understanding is paramount. Moreover, by empowering smaller models, E2H Reasoner promotes broader accessibility to advanced AI capabilities. Although our tasks focus on reasoning, future extensions involving real-world interaction could pose risks. Societal, Impacts Our work introduced E2H Reasoner, a curriculum-based reinforcement learning (RL) approach designed to enhance the reasoning capabilities of small-scale large language models (LLMs). including potential misuse of language models</p>            </div>
        </div>

    </div>
</body>
</html>
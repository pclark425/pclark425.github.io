<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-990 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-990</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-990</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-ce18e93b5654065c7dbc56aaa93e39fc355ed7c4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ce18e93b5654065c7dbc56aaa93e39fc355ed7c4" target="_blank">Neural Granger Causality</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Pattern Analysis and Machine Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a class of nonlinear methods by applying structured multilayer perceptrons (MLPs) or recurrent neural networks (RNNs) combined with sparsity-inducing penalties on the weights to extract the Granger causal structure.</p>
                <p><strong>Paper Abstract:</strong> While most classical approaches to Granger causality detection assume linear dynamics, many interactions in real-world applications, like neuroscience and genomics, are inherently nonlinear. In these cases, using linear models may lead to inconsistent estimation of Granger causal interactions. We propose a class of nonlinear methods by applying structured multilayer perceptrons (MLPs) or recurrent neural networks (RNNs) combined with sparsity-inducing penalties on the weights. By encouraging specific sets of weights to be zero—in particular, through the use of convex group-lasso penalties—we can extract the Granger causal structure. To further contrast with traditional approaches, our framework naturally enables us to efficiently capture long-range dependencies between series either via our RNNs or through an automatic lag selection in the MLP. We show that our neural Granger causality methods outperform state-of-the-art nonlinear Granger causality methods on the DREAM3 challenge data. This data consists of nonlinear gene expression and regulation time courses with only a limited number of time points. The successes we show in this challenging dataset provide a powerful example of how deep learning can be useful in cases that go beyond prediction on large datasets. We likewise illustrate our methods in detecting nonlinear interactions in a human motion capture dataset.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e990.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e990.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>cMLP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Component-wise Multilayer Perceptron</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured MLP that models each output series with a separate network and applies group-structured sparsity penalties on input (first-layer) weights to identify Granger-causal inputs and relevant lags.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Component-wise MLP with structured group penalties (cMLP)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Each target series i is predicted by an independent MLP g_i whose first-layer weights are partitioned by input series and lags. Structured sparsity penalties (group lasso, sparse-group/group-sparse-group-lasso, and hierarchical group lasso) are applied to the columns/groups of the first-layer weight matrices so that zeroed groups indicate Granger non-causality. Optimization uses proximal gradient methods so groups can be set exactly to zero. The hierarchical penalty enforces nested sparsity across lags to automatically select maximal relevant lag per input.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated time-series and benchmark datasets (Lorenz-96, linear VAR simulations, DREAM3 gene-expression simulations, MoCap)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Synthetic and benchmark multivariate time-series data: Lorenz-96 (continuous chaotic climate model), simulated sparse VAR processes, DREAM3 (simulated gene expression with hidden/latent factors, many short replicates), and CMU MoCap human motion data. These are observational simulation environments (no active interventions); DREAM3 includes hidden/unobserved factors (latent confounders).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Variable selection via structured sparsity (group lasso, sparse-group lasso, hierarchical group lasso) applied to input-weight groups; proximal group soft-thresholding to downweight and zero irrelevant inputs and irrelevant lags.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant input variables / irrelevant lags, overfitting due to high model capacity, limited-data noise; implicitly addresses effects from uninformative series (distractors). The paper notes hidden/unobserved factors in DREAM3 but does not explicitly model latent confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detection by inspecting the L2/Frobenius norm of input-weight groups: groups (columns across lags) with norm exactly zero are taken as detected Granger-noncausal (i.e., deemed distractors).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Group soft-thresholding (proximal operator) applied during optimization: weights are shrunk towards zero and groups can be set exactly to zero; sparse-group variant combines whole-group and per-lag penalties to downweight specific lags.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>High AUROC in simulations when using structured penalties: e.g., Lorenz-96 (F=10, T=1000) cMLP AUROC 98.4±0.1; VAR(1) T=250 AUROC 91.6±0.4. Hierarchical penalty gives robust performance (Table 3: HIER AUROC ≈95.5±0.2 for K=5 in Lorenz-96). On DREAM3 the cMLP outperforms several prior methods on multiple datasets (AUPR and AUROC improvements reported in paper figures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Non-regularized or less-structured variants perform worse: GROUP (plain group lasso) and MIXED (sparse-group) degrade with larger K (e.g., GROUP AUROC drops to 80.5±0.5 at K=20 vs HIER ~95.2±0.3), indicating poorer robustness to over-specified lags and distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structured sparsity on input weights effectively detects and removes irrelevant series and irrelevant lags, improving Granger-causality recovery in limited-data, high-dimensional settings; hierarchical penalty is particularly robust to over-specified maximum lag K and maintains high AUROC across K, and proximal optimization yields exact-zero groups enabling clear detection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Granger Causality', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e990.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e990.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>cLSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Component-wise Long Short-Term Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A component-wise LSTM that models each output series with an independent LSTM and applies group-lasso penalties on input weight columns to select Granger-causal input series while leveraging recurrent memory for long-range dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Component-wise LSTM with group-lasso on input weights (cLSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For each target series i, an independent LSTM compresses the history into hidden/cell states; the collection of input-to-gate weight matrices' j-th column (for input series j) is penalized with a group-lasso penalty so that if all columns for input j are zero the series is declared non-causal. Training optimizes the penalized loss via proximal gradient descent with backpropagation-through-time (truncated for long sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same simulation/benchmark environments as cMLP (Lorenz-96, VAR, DREAM3, MoCap)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Observational simulation and real recorded multivariate time series. LSTM architecture allows implicit handling of long-range dependencies without explicit lag specification; DREAM3 replicates are short, so BPTT and batching considerations affect training.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Variable selection via group-lasso applied to input-weight columns across LSTM input/gate matrices; proximal group soft-thresholding zeros irrelevant input columns, removing influence of distractor series.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant input variables, noisy/limited-data signals; latent/unobserved factors are present in DREAM3 but not explicitly handled.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detection by zeroing of the L2 norm of the j-th column across input weight matrices W^1 (input gates, forget, output, cell): ||W^1_{:,j}||_2 = 0 indicates series j is non-causal.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Proximal group soft-thresholding during optimization (group lasso) shrinks and can zero entire input columns, effectively downweighting distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Strong performance in benchmark: e.g., Lorenz-96 F=10 T=1000 cLSTM AUROC 96.0±0.1 (Table 1); on DREAM3 cLSTM outperformed prior methods across all five datasets in AUROC and AUPR (figures in paper). cLSTM tended to excel when long-range dependencies and more data were available.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Unregularized LSTM baselines (LOO-LSTM) performed poorly; IMV-LSTM (attention-based) underperformed relative to cLSTM in these causal recovery tasks (IMV-LSTM AUROC often much lower, e.g., ~60s).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Penalizing LSTM input weights produces interpretable sparse input selection that removes distractors; cLSTM is especially effective for data with long memory (e.g., DREAM3) and achieves the best AUROC/AUPR among compared methods, showing recurrent architectures + input sparsity are effective for recovering causal structure in nonlinear, limited-data settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Granger Causality', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e990.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e990.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GROUP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Group Lasso penalty (neural net analogue)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A group-lasso regularizer applied to groups of first-layer input weights associated with an input series (across lags or across gate matrices) to encourage entire input groups to be zero, indicating non-causality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Group Lasso on input-weight groups</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Apply the Frobenius or L2 norm on groups (columns corresponding to one input across all first-layer parameters/lags) and add λ times this norm to the loss. Optimized with proximal group soft-thresholding which can shrink whole groups to zero, representing variable removal.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same time-series simulation/benchmark datasets used in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Observational synthetic and benchmark time series; used as a mechanism to detect and remove irrelevant inputs in cMLP and cLSTM.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Group-wise variable selection via penalization; entire input groups are downweighted/zeroed.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant variables / uninformative inputs, multicollinear redundant inputs insofar as they are not selected.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Group norm equals zero after proximal optimization indicates detection as distractor.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Group soft-thresholding (proximal operator) shrinks group weights; groups can be exactly zeroed.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Improves interpretability and sparsity but is sensitive to over-specified lags; in experiments GROUP AUROC falls as K increases (e.g., GROUP AUROC 88.1±0.8 at K=5, 80.5±0.5 at K=20 in Lorenz-96), showing decreased robustness vs hierarchical penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Group lasso provides a straightforward way to remove irrelevant inputs, but performance degrades when many irrelevant lags are included; more structured penalties (hierarchical) can be superior when lag selection is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Granger Causality', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e990.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e990.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MIXED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Group-sparse group lasso (sparse-group lasso variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A combined penalty that mixes a whole-group (across all lags) penalty with per-lag group penalties to induce sparsity across inputs and sparsity within input-lag groups.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Group-sparse group lasso (MIXED)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Penalty Ω = α||W_j'||_F + (1-α) Σ_k ||W_j'^{k}||_2, blending group-level and within-group (per-lag) sparsity. It encourages selection of a sparse set of input series and a sparse subset of lags per selected input.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic and benchmark time series (Lorenz-96, VAR, DREAM3)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Observational simulations and benchmarks where lags may be variably informative; used inside cMLP to select both series and their relevant lags.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Combined across-group and within-group sparsity to downweight irrelevant series and irrelevant lags.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant variables and irrelevant lags</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Zeroed per-lag and/or whole-group norms after proximal optimization indicate detected distractors or irrelevant lags.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Two-stage proximal (lag-specific group soft-thresholding followed by whole-group thresholding) reduces influence of spurious components.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Intermediate performance between GROUP and HIER; e.g., MIXED AUROC 90.1±0.5 (K=5) but degrades with larger K (85.4±0.3 at K=10), indicating partial robustness to irrelevant lags.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining group and within-group sparsity helps identify a sparse set of series and a sparse subset of lags, but is less robust than the hierarchical penalty when K is large.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Granger Causality', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e990.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e990.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HIER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical Group Lasso (lag-selection penalty)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A nested group penalty that imposes hierarchical sparsity over lags so that if a higher lag is zero then all longer lags are zero, enabling automatic selection of maximal lag per input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Hierarchical group lasso over lags</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Penalty Ω = Σ_{k=1}^K ||(W_j'^{k}, ..., W_j'^{K})||_F which enforces nested (contiguous) sparsity across lags; proximal mapping is applied iteratively from smallest nested group to largest. This selects both whether an input series is relevant and the maximal lag for which it is relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated and benchmark time series (Lorenz-96, VAR, DREAM3)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Observational time-series tasks where lag order is unknown; allows setting K large without overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Automatic lag selection and variable selection via hierarchical nested-group sparsity; discourages spurious long-lag connections.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant lags and irrelevant input variables (distractors introduced via over-specified lag sets)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Nested group norms zeroed by proximal operations indicate that longer lags (and potentially whole inputs) are irrelevant.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Iterative group soft-thresholding on nested groups reduces influence of spurious long-lag weights and can set them to zero.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Most robust of the considered penalties to over-specified K: e.g., HIER AUROC ≈95.5±0.2 (K=5) and remains ≈95.2±0.3 at K=20 in Lorenz-96, outperforming GROUP and MIXED which degrade as K grows.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Compared with plain GROUP (AUROC 88.1±0.8 at K=5 and 80.5±0.5 at K=20) highlights HIER's robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Hierarchical penalty enables reliable automatic lag selection and confers robustness against spurious long-lag signals and over-specified inputs, preserving high causal recovery performance even with large maximum lag K.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Granger Causality', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e990.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e990.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProximalOpt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proximal gradient optimization for structured sparsity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of proximal gradient descent (and iterative shrinkage/thresholding variants) to optimize nonconvex penalized neural network objectives such that structured group penalties yield exact zeros in parameter groups.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Proximal gradient descent (with group soft-thresholding / ISTA / GIST)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Iterative update W <- prox_{γλΩ}(W - γ∇L(W)) where prox implements group soft-thresholding (or nested/group-sparse proximal maps). Line-search variants (GIST) and standard ISTA are used; prox yields exact zeros for groups enabling interpretable selection. For RNNs LSTM training uses BPTT (truncated for long sequences) before prox step on input weights.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Optimization of cMLP and cLSTM on simulation and benchmark datasets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Deterministic optimization environment for penalized neural training; used to enforce sparsity and detect distractors by exact zeroing.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Exact group-wise thresholding (proximal operators) to zero-out groups corresponding to distractor inputs or lags.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant variables/weights due to overparameterization or limited data; reduces false positives from spurious correlations by penalization.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Groups reach exact zero by prox operator -> interpret as detected distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Soft-thresholding shrinks weights; repeated application can completely remove spurious groups.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Proximal methods produce exact-zero groups enabling clear variable selection. Optimizers gave similar AUROC in ablations (ISTA/GIST/Adam comparable), but proximal methods preferred because they produce exact zeros (e.g., ISTA AUROC ~98.0±0.2 for Lorenz F=10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Proximal optimization is essential to obtain exact-zero groups for interpretable causal selection; proximal ISTA/GIST comparable to Adam in AUROC but yields exact sparsity without ad-hoc thresholding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Granger Causality', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e990.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e990.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IMV-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interpretable Multivariable LSTM (attention-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An attention-based LSTM variant that produces attention weights over inputs to provide interpretability; used here as a baseline that aggregates attention to infer Granger relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>IMV-LSTM (attention aggregation baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train LSTMs with attention mechanisms that assign weights to inputs; infer importance/causal influence by aggregating mean attention values per input series and thresholding them to obtain edges.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated time-series (Lorenz-96, VAR) used as baseline comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Observational simulation datasets; attention is used post-hoc as a proxy for causal influence.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Uses attention weights as proxy for influence; thresholds on attention values determine selected inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Underperformed in experiments: IMV-LSTM AUROC much lower than cMLP/cLSTM (e.g., IMV-LSTM AUROC in Lorenz-96 often in 50s–60s to mid-70s depending on T and F).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Attention weights are a poor substitute for explicit sparsity-based variable selection when the goal is causal discovery; IMV-LSTM performed substantially worse in the authors' benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Granger Causality', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e990.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e990.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOO-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leave-One-Out LSTM baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that measures the increase in predictive loss when withholding each candidate input series from an LSTM, interpreting large loss increases as evidence of causal influence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Leave-One-Out (LOO) LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train full LSTM models with all inputs; for each candidate input j, train an LSTM without that input and measure change in loss for predicting target series i. Large increases imply importance; threshold the loss changes to detect edges.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated datasets (Lorenz-96, VAR) evaluated as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Observational simulations; intended as a model-agnostic importance/refutation check via ablation, but computationally expensive and sensitive to redundancies.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects influence by change in predictive loss when withholding an input.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Implicit: if withholding an input does not change loss, it is considered non-influential (but this can fail with redundant inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Performed poorly in experiments (AUROC near chance ~50 in many settings), likely due to overfitting of unregularized LSTMs and redundancy/collinearity among inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Withholding inputs is not a reliable refutation in high-dimensional time series because (i) unregularized models overfit and (ii) remaining inputs can preserve signals, so LOO-LSTM performed near chance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Granger Causality', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e990.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e990.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OKVAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Operator-valued Kernel-based VAR (OKVAR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art kernel (operator-valued kernel) regression method for nonlinear Granger causality detection used as a prior benchmark on DREAM3.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Operatorvalued kernel-based vector autoregressive models for network inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Operator-valued kernel VAR (OKVAR)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Kernel-based multivariate regression using operator-valued kernels to model multi-output nonlinear autoregressive relationships; used to infer network edges by learning nonlinear predictors and analyzing contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>DREAM3 gene-expression benchmark (simulated gene regulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Short replicate observational gene expression time courses with hidden factors; non-interventional benchmark dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Prior state-of-the-art for nonlinear Granger causality on DREAM3; the paper reports that cLSTM and cMLP outperform OKVAR on AUROC/AUPR in several datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Kernel-based nonlinear VAR methods are strong baselines, but structured sparse neural methods (cLSTM/cMLP) can achieve better recovery on DREAM3 according to the authors' comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Granger Causality', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e990.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e990.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAR-LASSO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear VAR with Lasso/group regularization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classical linear vector autoregressive model with L1 (lasso) or group penalties for sparse Granger network estimation; used as a baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LASSO / grouped VAR</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Linear VAR fit with L1 (lasso) or group penalties to induce sparsity across coefficient matrices; Granger causality is determined by nonzero coefficients across lags. Hierarchical/truncating penalties can also be used for lag selection.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic VAR simulations and DREAM3 benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Observational time-series; appropriate when relationships are approximately linear and lag-order selection is manageable.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>L1/group regularization remove irrelevant variables but assumes linearity; hierarchical/truncating lasso handle lag selection to some extent.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant variables, overfitting from high K, but does not handle nonlinear spurious signals or latent confounders explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Zero coefficients across all lags indicate Granger non-causality.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>L1 / group soft-thresholding shrink coefficients; tuning λ controls sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Reasonable baseline for linear dynamics; in paper cMLP/cLSTM outperform LASSO on nonlinear DREAM3 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Linear sparse VARs are useful baselines; however, they can be inconsistent if dynamics are nonlinear, motivating nonlinear sparse neural approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Granger Causality', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e990.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e990.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TransferEntropy/DI/Kernel-GC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-free measures: Transfer Entropy / Directed Information / Kernel-Granger causality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Model-free or kernel-based approaches for detecting directed dependencies (nonlinear causality) such as transfer entropy, directed information, and kernelized Granger causality methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Model-free nonlinear causality measures (transfer entropy, directed information, kernel-Granger)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Transfer entropy and directed information quantify information flow from the past of one series to another without parametric model assumptions; kernel-Granger uses kernel regression to capture nonlinear autoregressive relationships and infer edges. These approaches typically require large data and can suffer from high variance and curse of dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General multivariate time-series; referenced in discussion and related work</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Observational settings; model-free estimators are data-hungry and suffer in high-dimensional and limited-data contexts like DREAM3.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Can in principle capture nonlinear dependencies but are susceptible to high variance and confounding if observational data contain latent factors.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Paper notes these methods have high variance and require large amounts of data and suffer from curse of dimensionality; not used directly in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Model-free estimators can detect nonlinear dependencies but are not practical in high-dimensional, limited-data settings; motivates structured neural approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Granger Causality', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hierarchical vector autoregression <em>(Rating: 2)</em></li>
                <li>A sparse-group lasso <em>(Rating: 2)</em></li>
                <li>Proximal methods for hierarchical sparse coding <em>(Rating: 2)</em></li>
                <li>Operatorvalued kernel-based vector autoregressive models for network inference <em>(Rating: 2)</em></li>
                <li>Kernel-Granger causality and the analysis of dynamical networks <em>(Rating: 2)</em></li>
                <li>Transfer entropy—a model-free measure of effective connectivity for the neurosciences <em>(Rating: 1)</em></li>
                <li>An interpretable LSTM neural network for autoregressive exogenous model <em>(Rating: 1)</em></li>
                <li>Exploring interpretable LSTM neural networks over multi-variable data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-990",
    "paper_id": "paper-ce18e93b5654065c7dbc56aaa93e39fc355ed7c4",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "cMLP",
            "name_full": "Component-wise Multilayer Perceptron",
            "brief_description": "A structured MLP that models each output series with a separate network and applies group-structured sparsity penalties on input (first-layer) weights to identify Granger-causal inputs and relevant lags.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Component-wise MLP with structured group penalties (cMLP)",
            "method_description": "Each target series i is predicted by an independent MLP g_i whose first-layer weights are partitioned by input series and lags. Structured sparsity penalties (group lasso, sparse-group/group-sparse-group-lasso, and hierarchical group lasso) are applied to the columns/groups of the first-layer weight matrices so that zeroed groups indicate Granger non-causality. Optimization uses proximal gradient methods so groups can be set exactly to zero. The hierarchical penalty enforces nested sparsity across lags to automatically select maximal relevant lag per input.",
            "environment_name": "Simulated time-series and benchmark datasets (Lorenz-96, linear VAR simulations, DREAM3 gene-expression simulations, MoCap)",
            "environment_description": "Synthetic and benchmark multivariate time-series data: Lorenz-96 (continuous chaotic climate model), simulated sparse VAR processes, DREAM3 (simulated gene expression with hidden/latent factors, many short replicates), and CMU MoCap human motion data. These are observational simulation environments (no active interventions); DREAM3 includes hidden/unobserved factors (latent confounders).",
            "handles_distractors": true,
            "distractor_handling_technique": "Variable selection via structured sparsity (group lasso, sparse-group lasso, hierarchical group lasso) applied to input-weight groups; proximal group soft-thresholding to downweight and zero irrelevant inputs and irrelevant lags.",
            "spurious_signal_types": "Irrelevant input variables / irrelevant lags, overfitting due to high model capacity, limited-data noise; implicitly addresses effects from uninformative series (distractors). The paper notes hidden/unobserved factors in DREAM3 but does not explicitly model latent confounding.",
            "detection_method": "Detection by inspecting the L2/Frobenius norm of input-weight groups: groups (columns across lags) with norm exactly zero are taken as detected Granger-noncausal (i.e., deemed distractors).",
            "downweighting_method": "Group soft-thresholding (proximal operator) applied during optimization: weights are shrunk towards zero and groups can be set exactly to zero; sparse-group variant combines whole-group and per-lag penalties to downweight specific lags.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "High AUROC in simulations when using structured penalties: e.g., Lorenz-96 (F=10, T=1000) cMLP AUROC 98.4±0.1; VAR(1) T=250 AUROC 91.6±0.4. Hierarchical penalty gives robust performance (Table 3: HIER AUROC ≈95.5±0.2 for K=5 in Lorenz-96). On DREAM3 the cMLP outperforms several prior methods on multiple datasets (AUPR and AUROC improvements reported in paper figures).",
            "performance_without_robustness": "Non-regularized or less-structured variants perform worse: GROUP (plain group lasso) and MIXED (sparse-group) degrade with larger K (e.g., GROUP AUROC drops to 80.5±0.5 at K=20 vs HIER ~95.2±0.3), indicating poorer robustness to over-specified lags and distractors.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Structured sparsity on input weights effectively detects and removes irrelevant series and irrelevant lags, improving Granger-causality recovery in limited-data, high-dimensional settings; hierarchical penalty is particularly robust to over-specified maximum lag K and maintains high AUROC across K, and proximal optimization yields exact-zero groups enabling clear detection.",
            "uuid": "e990.0",
            "source_info": {
                "paper_title": "Neural Granger Causality",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "cLSTM",
            "name_full": "Component-wise Long Short-Term Memory",
            "brief_description": "A component-wise LSTM that models each output series with an independent LSTM and applies group-lasso penalties on input weight columns to select Granger-causal input series while leveraging recurrent memory for long-range dependencies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Component-wise LSTM with group-lasso on input weights (cLSTM)",
            "method_description": "For each target series i, an independent LSTM compresses the history into hidden/cell states; the collection of input-to-gate weight matrices' j-th column (for input series j) is penalized with a group-lasso penalty so that if all columns for input j are zero the series is declared non-causal. Training optimizes the penalized loss via proximal gradient descent with backpropagation-through-time (truncated for long sequences).",
            "environment_name": "Same simulation/benchmark environments as cMLP (Lorenz-96, VAR, DREAM3, MoCap)",
            "environment_description": "Observational simulation and real recorded multivariate time series. LSTM architecture allows implicit handling of long-range dependencies without explicit lag specification; DREAM3 replicates are short, so BPTT and batching considerations affect training.",
            "handles_distractors": true,
            "distractor_handling_technique": "Variable selection via group-lasso applied to input-weight columns across LSTM input/gate matrices; proximal group soft-thresholding zeros irrelevant input columns, removing influence of distractor series.",
            "spurious_signal_types": "Irrelevant input variables, noisy/limited-data signals; latent/unobserved factors are present in DREAM3 but not explicitly handled.",
            "detection_method": "Detection by zeroing of the L2 norm of the j-th column across input weight matrices W^1 (input gates, forget, output, cell): ||W^1_{:,j}||_2 = 0 indicates series j is non-causal.",
            "downweighting_method": "Proximal group soft-thresholding during optimization (group lasso) shrinks and can zero entire input columns, effectively downweighting distractors.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Strong performance in benchmark: e.g., Lorenz-96 F=10 T=1000 cLSTM AUROC 96.0±0.1 (Table 1); on DREAM3 cLSTM outperformed prior methods across all five datasets in AUROC and AUPR (figures in paper). cLSTM tended to excel when long-range dependencies and more data were available.",
            "performance_without_robustness": "Unregularized LSTM baselines (LOO-LSTM) performed poorly; IMV-LSTM (attention-based) underperformed relative to cLSTM in these causal recovery tasks (IMV-LSTM AUROC often much lower, e.g., ~60s).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Penalizing LSTM input weights produces interpretable sparse input selection that removes distractors; cLSTM is especially effective for data with long memory (e.g., DREAM3) and achieves the best AUROC/AUPR among compared methods, showing recurrent architectures + input sparsity are effective for recovering causal structure in nonlinear, limited-data settings.",
            "uuid": "e990.1",
            "source_info": {
                "paper_title": "Neural Granger Causality",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "GROUP",
            "name_full": "Group Lasso penalty (neural net analogue)",
            "brief_description": "A group-lasso regularizer applied to groups of first-layer input weights associated with an input series (across lags or across gate matrices) to encourage entire input groups to be zero, indicating non-causality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Group Lasso on input-weight groups",
            "method_description": "Apply the Frobenius or L2 norm on groups (columns corresponding to one input across all first-layer parameters/lags) and add λ times this norm to the loss. Optimized with proximal group soft-thresholding which can shrink whole groups to zero, representing variable removal.",
            "environment_name": "Same time-series simulation/benchmark datasets used in the paper",
            "environment_description": "Observational synthetic and benchmark time series; used as a mechanism to detect and remove irrelevant inputs in cMLP and cLSTM.",
            "handles_distractors": true,
            "distractor_handling_technique": "Group-wise variable selection via penalization; entire input groups are downweighted/zeroed.",
            "spurious_signal_types": "Irrelevant variables / uninformative inputs, multicollinear redundant inputs insofar as they are not selected.",
            "detection_method": "Group norm equals zero after proximal optimization indicates detection as distractor.",
            "downweighting_method": "Group soft-thresholding (proximal operator) shrinks group weights; groups can be exactly zeroed.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Improves interpretability and sparsity but is sensitive to over-specified lags; in experiments GROUP AUROC falls as K increases (e.g., GROUP AUROC 88.1±0.8 at K=5, 80.5±0.5 at K=20 in Lorenz-96), showing decreased robustness vs hierarchical penalty.",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Group lasso provides a straightforward way to remove irrelevant inputs, but performance degrades when many irrelevant lags are included; more structured penalties (hierarchical) can be superior when lag selection is needed.",
            "uuid": "e990.2",
            "source_info": {
                "paper_title": "Neural Granger Causality",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "MIXED",
            "name_full": "Group-sparse group lasso (sparse-group lasso variant)",
            "brief_description": "A combined penalty that mixes a whole-group (across all lags) penalty with per-lag group penalties to induce sparsity across inputs and sparsity within input-lag groups.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Group-sparse group lasso (MIXED)",
            "method_description": "Penalty Ω = α||W_j'||_F + (1-α) Σ_k ||W_j'^{k}||_2, blending group-level and within-group (per-lag) sparsity. It encourages selection of a sparse set of input series and a sparse subset of lags per selected input.",
            "environment_name": "Synthetic and benchmark time series (Lorenz-96, VAR, DREAM3)",
            "environment_description": "Observational simulations and benchmarks where lags may be variably informative; used inside cMLP to select both series and their relevant lags.",
            "handles_distractors": true,
            "distractor_handling_technique": "Combined across-group and within-group sparsity to downweight irrelevant series and irrelevant lags.",
            "spurious_signal_types": "Irrelevant variables and irrelevant lags",
            "detection_method": "Zeroed per-lag and/or whole-group norms after proximal optimization indicate detected distractors or irrelevant lags.",
            "downweighting_method": "Two-stage proximal (lag-specific group soft-thresholding followed by whole-group thresholding) reduces influence of spurious components.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Intermediate performance between GROUP and HIER; e.g., MIXED AUROC 90.1±0.5 (K=5) but degrades with larger K (85.4±0.3 at K=10), indicating partial robustness to irrelevant lags.",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Combining group and within-group sparsity helps identify a sparse set of series and a sparse subset of lags, but is less robust than the hierarchical penalty when K is large.",
            "uuid": "e990.3",
            "source_info": {
                "paper_title": "Neural Granger Causality",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "HIER",
            "name_full": "Hierarchical Group Lasso (lag-selection penalty)",
            "brief_description": "A nested group penalty that imposes hierarchical sparsity over lags so that if a higher lag is zero then all longer lags are zero, enabling automatic selection of maximal lag per input.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Hierarchical group lasso over lags",
            "method_description": "Penalty Ω = Σ_{k=1}^K ||(W_j'^{k}, ..., W_j'^{K})||_F which enforces nested (contiguous) sparsity across lags; proximal mapping is applied iteratively from smallest nested group to largest. This selects both whether an input series is relevant and the maximal lag for which it is relevant.",
            "environment_name": "Simulated and benchmark time series (Lorenz-96, VAR, DREAM3)",
            "environment_description": "Observational time-series tasks where lag order is unknown; allows setting K large without overfitting.",
            "handles_distractors": true,
            "distractor_handling_technique": "Automatic lag selection and variable selection via hierarchical nested-group sparsity; discourages spurious long-lag connections.",
            "spurious_signal_types": "Irrelevant lags and irrelevant input variables (distractors introduced via over-specified lag sets)",
            "detection_method": "Nested group norms zeroed by proximal operations indicate that longer lags (and potentially whole inputs) are irrelevant.",
            "downweighting_method": "Iterative group soft-thresholding on nested groups reduces influence of spurious long-lag weights and can set them to zero.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Most robust of the considered penalties to over-specified K: e.g., HIER AUROC ≈95.5±0.2 (K=5) and remains ≈95.2±0.3 at K=20 in Lorenz-96, outperforming GROUP and MIXED which degrade as K grows.",
            "performance_without_robustness": "Compared with plain GROUP (AUROC 88.1±0.8 at K=5 and 80.5±0.5 at K=20) highlights HIER's robustness.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Hierarchical penalty enables reliable automatic lag selection and confers robustness against spurious long-lag signals and over-specified inputs, preserving high causal recovery performance even with large maximum lag K.",
            "uuid": "e990.4",
            "source_info": {
                "paper_title": "Neural Granger Causality",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "ProximalOpt",
            "name_full": "Proximal gradient optimization for structured sparsity",
            "brief_description": "Use of proximal gradient descent (and iterative shrinkage/thresholding variants) to optimize nonconvex penalized neural network objectives such that structured group penalties yield exact zeros in parameter groups.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Proximal gradient descent (with group soft-thresholding / ISTA / GIST)",
            "method_description": "Iterative update W &lt;- prox_{γλΩ}(W - γ∇L(W)) where prox implements group soft-thresholding (or nested/group-sparse proximal maps). Line-search variants (GIST) and standard ISTA are used; prox yields exact zeros for groups enabling interpretable selection. For RNNs LSTM training uses BPTT (truncated for long sequences) before prox step on input weights.",
            "environment_name": "Optimization of cMLP and cLSTM on simulation and benchmark datasets",
            "environment_description": "Deterministic optimization environment for penalized neural training; used to enforce sparsity and detect distractors by exact zeroing.",
            "handles_distractors": true,
            "distractor_handling_technique": "Exact group-wise thresholding (proximal operators) to zero-out groups corresponding to distractor inputs or lags.",
            "spurious_signal_types": "Irrelevant variables/weights due to overparameterization or limited data; reduces false positives from spurious correlations by penalization.",
            "detection_method": "Groups reach exact zero by prox operator -&gt; interpret as detected distractors.",
            "downweighting_method": "Soft-thresholding shrinks weights; repeated application can completely remove spurious groups.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Proximal methods produce exact-zero groups enabling clear variable selection. Optimizers gave similar AUROC in ablations (ISTA/GIST/Adam comparable), but proximal methods preferred because they produce exact zeros (e.g., ISTA AUROC ~98.0±0.2 for Lorenz F=10).",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Proximal optimization is essential to obtain exact-zero groups for interpretable causal selection; proximal ISTA/GIST comparable to Adam in AUROC but yields exact sparsity without ad-hoc thresholding.",
            "uuid": "e990.5",
            "source_info": {
                "paper_title": "Neural Granger Causality",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "IMV-LSTM",
            "name_full": "Interpretable Multivariable LSTM (attention-based)",
            "brief_description": "An attention-based LSTM variant that produces attention weights over inputs to provide interpretability; used here as a baseline that aggregates attention to infer Granger relationships.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "IMV-LSTM (attention aggregation baseline)",
            "method_description": "Train LSTMs with attention mechanisms that assign weights to inputs; infer importance/causal influence by aggregating mean attention values per input series and thresholding them to obtain edges.",
            "environment_name": "Simulated time-series (Lorenz-96, VAR) used as baseline comparisons",
            "environment_description": "Observational simulation datasets; attention is used post-hoc as a proxy for causal influence.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": "Uses attention weights as proxy for influence; thresholds on attention values determine selected inputs.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Underperformed in experiments: IMV-LSTM AUROC much lower than cMLP/cLSTM (e.g., IMV-LSTM AUROC in Lorenz-96 often in 50s–60s to mid-70s depending on T and F).",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Attention weights are a poor substitute for explicit sparsity-based variable selection when the goal is causal discovery; IMV-LSTM performed substantially worse in the authors' benchmarks.",
            "uuid": "e990.6",
            "source_info": {
                "paper_title": "Neural Granger Causality",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "LOO-LSTM",
            "name_full": "Leave-One-Out LSTM baseline",
            "brief_description": "A baseline that measures the increase in predictive loss when withholding each candidate input series from an LSTM, interpreting large loss increases as evidence of causal influence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Leave-One-Out (LOO) LSTM",
            "method_description": "Train full LSTM models with all inputs; for each candidate input j, train an LSTM without that input and measure change in loss for predicting target series i. Large increases imply importance; threshold the loss changes to detect edges.",
            "environment_name": "Simulated datasets (Lorenz-96, VAR) evaluated as baseline",
            "environment_description": "Observational simulations; intended as a model-agnostic importance/refutation check via ablation, but computationally expensive and sensitive to redundancies.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": "Detects influence by change in predictive loss when withholding an input.",
            "downweighting_method": null,
            "refutation_method": "Implicit: if withholding an input does not change loss, it is considered non-influential (but this can fail with redundant inputs).",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Performed poorly in experiments (AUROC near chance ~50 in many settings), likely due to overfitting of unregularized LSTMs and redundancy/collinearity among inputs.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Withholding inputs is not a reliable refutation in high-dimensional time series because (i) unregularized models overfit and (ii) remaining inputs can preserve signals, so LOO-LSTM performed near chance.",
            "uuid": "e990.7",
            "source_info": {
                "paper_title": "Neural Granger Causality",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "OKVAR",
            "name_full": "Operator-valued Kernel-based VAR (OKVAR)",
            "brief_description": "A state-of-the-art kernel (operator-valued kernel) regression method for nonlinear Granger causality detection used as a prior benchmark on DREAM3.",
            "citation_title": "Operatorvalued kernel-based vector autoregressive models for network inference",
            "mention_or_use": "mention",
            "method_name": "Operator-valued kernel VAR (OKVAR)",
            "method_description": "Kernel-based multivariate regression using operator-valued kernels to model multi-output nonlinear autoregressive relationships; used to infer network edges by learning nonlinear predictors and analyzing contributions.",
            "environment_name": "DREAM3 gene-expression benchmark (simulated gene regulation)",
            "environment_description": "Short replicate observational gene expression time courses with hidden factors; non-interventional benchmark dataset.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Prior state-of-the-art for nonlinear Granger causality on DREAM3; the paper reports that cLSTM and cMLP outperform OKVAR on AUROC/AUPR in several datasets.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Kernel-based nonlinear VAR methods are strong baselines, but structured sparse neural methods (cLSTM/cMLP) can achieve better recovery on DREAM3 according to the authors' comparisons.",
            "uuid": "e990.8",
            "source_info": {
                "paper_title": "Neural Granger Causality",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "VAR-LASSO",
            "name_full": "Linear VAR with Lasso/group regularization",
            "brief_description": "Classical linear vector autoregressive model with L1 (lasso) or group penalties for sparse Granger network estimation; used as a baseline in comparisons.",
            "citation_title": "mention",
            "mention_or_use": "mention",
            "method_name": "LASSO / grouped VAR",
            "method_description": "Linear VAR fit with L1 (lasso) or group penalties to induce sparsity across coefficient matrices; Granger causality is determined by nonzero coefficients across lags. Hierarchical/truncating penalties can also be used for lag selection.",
            "environment_name": "Synthetic VAR simulations and DREAM3 benchmark",
            "environment_description": "Observational time-series; appropriate when relationships are approximately linear and lag-order selection is manageable.",
            "handles_distractors": false,
            "distractor_handling_technique": "L1/group regularization remove irrelevant variables but assumes linearity; hierarchical/truncating lasso handle lag selection to some extent.",
            "spurious_signal_types": "Irrelevant variables, overfitting from high K, but does not handle nonlinear spurious signals or latent confounders explicitly.",
            "detection_method": "Zero coefficients across all lags indicate Granger non-causality.",
            "downweighting_method": "L1 / group soft-thresholding shrink coefficients; tuning λ controls sparsity.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Reasonable baseline for linear dynamics; in paper cMLP/cLSTM outperform LASSO on nonlinear DREAM3 tasks.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Linear sparse VARs are useful baselines; however, they can be inconsistent if dynamics are nonlinear, motivating nonlinear sparse neural approaches.",
            "uuid": "e990.9",
            "source_info": {
                "paper_title": "Neural Granger Causality",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "TransferEntropy/DI/Kernel-GC",
            "name_full": "Model-free measures: Transfer Entropy / Directed Information / Kernel-Granger causality",
            "brief_description": "Model-free or kernel-based approaches for detecting directed dependencies (nonlinear causality) such as transfer entropy, directed information, and kernelized Granger causality methods.",
            "citation_title": "mention",
            "mention_or_use": "mention",
            "method_name": "Model-free nonlinear causality measures (transfer entropy, directed information, kernel-Granger)",
            "method_description": "Transfer entropy and directed information quantify information flow from the past of one series to another without parametric model assumptions; kernel-Granger uses kernel regression to capture nonlinear autoregressive relationships and infer edges. These approaches typically require large data and can suffer from high variance and curse of dimensionality.",
            "environment_name": "General multivariate time-series; referenced in discussion and related work",
            "environment_description": "Observational settings; model-free estimators are data-hungry and suffer in high-dimensional and limited-data contexts like DREAM3.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Can in principle capture nonlinear dependencies but are susceptible to high variance and confounding if observational data contain latent factors.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Paper notes these methods have high variance and require large amounts of data and suffer from curse of dimensionality; not used directly in experiments.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Model-free estimators can detect nonlinear dependencies but are not practical in high-dimensional, limited-data settings; motivates structured neural approaches.",
            "uuid": "e990.10",
            "source_info": {
                "paper_title": "Neural Granger Causality",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hierarchical vector autoregression",
            "rating": 2
        },
        {
            "paper_title": "A sparse-group lasso",
            "rating": 2
        },
        {
            "paper_title": "Proximal methods for hierarchical sparse coding",
            "rating": 2
        },
        {
            "paper_title": "Operatorvalued kernel-based vector autoregressive models for network inference",
            "rating": 2
        },
        {
            "paper_title": "Kernel-Granger causality and the analysis of dynamical networks",
            "rating": 2
        },
        {
            "paper_title": "Transfer entropy—a model-free measure of effective connectivity for the neurosciences",
            "rating": 1
        },
        {
            "paper_title": "An interpretable LSTM neural network for autoregressive exogenous model",
            "rating": 1
        },
        {
            "paper_title": "Exploring interpretable LSTM neural networks over multi-variable data",
            "rating": 1
        }
    ],
    "cost": 0.021213749999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Neural Granger Causality</h1>
<p>Alex Tank<em>, Ian Covert</em>, Nick Foti, Ali Shojaie, Emily B. Fox</p>
<h4>Abstract</h4>
<p>While most classical approaches to Granger causality detection assume linear dynamics, many interactions in real-world applications, like neuroscience and genomics, are inherently nonlinear. In these cases, using linear models may lead to inconsistent estimation of Granger causal interactions. We propose a class of nonlinear methods by applying structured multilayer perceptrons (MLPs) or recurrent neural networks (RNNs) combined with sparsity-inducing penalties on the weights. By encouraging specific sets of weights to be zero-in particular, through the use of convex group-lasso penalties-we can extract the Granger causal structure. To further contrast with traditional approaches, our framework naturally enables us to efficiently capture long-range dependencies between series either via our RNNs or through an automatic lag selection in the MLP. We show that our neural Granger causality methods outperform state-of-the-art nonlinear Granger causality methods on the DREAM3 challenge data. This data consists of nonlinear gene expression and regulation time courses with only a limited number of time points. The successes we show in this challenging dataset provide a powerful example of how deep learning can be useful in cases that go beyond prediction on large datasets. We likewise illustrate our methods in detecting nonlinear interactions in a human motion capture dataset.</p>
<p>Index Terms-time series, Granger causality, neural networks, structured sparsity, interpretability</p>
<h2>1 INTRODUCTION</h2>
<p>In many scientific applications of multivariate time series, it is important to go beyond prediction and forecasting and instead interpret the structure within time series. Typically, this structure provides information about the contemporaneous and lagged relationships within and between individual series and how these series interact. For example, in neuroscience it is important to determine how brain activation spreads through brain regions [1], [2], [3], [4]; in finance it is important to determine groups of stocks with low covariance to design low risk portfolios [5]; and, in biology, it is of great interest to infer gene regulatory networks from time series of gene expression levels [6], [7]. However, for a given statistical model or methodology, there is often a tradeoff between the interpretability of these structural relationships and expressivity of the model dynamics.</p>
<p>Among the many choices for understanding relationships between series, Granger causality [8], [9] is a commonly used framework for time series structure discovery that quantifies the extent to which the past of one time series aids in predicting the future evolution of another time series. When an entire system of time series is studied, networks of Granger causal interactions may be uncovered [10]. This is in contrast to other types of structure discovery, like coherence [11] or lagged correlation [11], which analyze strictly bivariate covariance relationships. That is, Granger causality metrics depend on the activity of the entire system of time series under study, making them more appropriate for understanding high-dimensional complex data streams. Methodology for estimating Granger causality may be separated into two classes, model-based and model-free.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Most classical model-based methods assume linear time series dynamics and use the popular vector autoregressive (VAR) model [7], [9]. In this case, the time lags of a series have a linear effect on the future of each other series, and the magnitude of the linear coefficients quantifies the Granger causal effect. Sparsity-inducing regularizers, like the Lasso [12] or group lasso [13], help scale linear Granger causality estimation in VAR models to the high-dimensional setting [7], [14].</p>
<p>In classical linear VAR methods, one must explicitly specify the maximum time lag to consider when assessing Granger causality. If the specified lag is too short, Granger causal connections occurring at longer time lags between series will be missed while overfitting may occur if the lag is too large. Lag selection penalties, like the hierarchical lasso [15] and truncating penalties [16], have been used to automatically select the relevant lags while protecting against overfitting. Furthermore, these penalties lead to a sparse network of Granger causal interactions, where only a few Granger causal connections exist for each series-a crucial property for scaling Granger causal estimation to the high-dimensional setting, where the number of time series and number of potentially relevant time lags all scale with the number of observations [17].</p>
<p>Model-based methods may fail in real world cases when the relationships between the past of one series and future of another falls outside of the model class [18], [19], [20]. This typically occurs when there are nonlinear dependencies between the past of one series and the future. Model-free methods, like transfer entropy [2] or directed information [21], are able to detect these nonlinear dependencies between past and future with minimal assumptions about the predictive relationships. However, these estimators have high variance and require large amounts of data for reliable estimation. These approaches also suffer from a curse of dimensionality [22] when the number of series grows, making them inappropriate in the high-dimensional setting.</p>
<p>Neural networks are capable of representing complex,</p>
<p>nonlinear, and non-additive interactions between inputs and outputs. Indeed, their time series variants, such as autoregressive multilayer perceptrons (MLPs) [23], [24], [25] and recurrent neural networks (RNNs) like long-short term memory networks (LSTMs) [26] have shown impressive performance in forecasting multivariate time series given their past [27], [28], [29]. While these methods have shown impressive predictive performance, they are essentially black box methods and provide little interpretability of the multivariate structural relationships in the series. A second drawback is that jointly modeling a large number of series leads to many network parameters. As a result, these methods require much more data to fit reliably and tend to perform poorly in high-dimensional settings.</p>
<p>We present a framework for structure learning in MLPs and RNNs that leads to interpretable nonlinear Granger causality discovery. The proposed framework harnesses the impressive flexibility and representational power of neural networks. It also sidesteps the black-box nature of many network architectures by introducing component-wise architectures that disentangle the effects of lagged inputs on individual output series. For interpretability and an ability to handle limited data in the high-dimensional setting, we place sparsity-inducing penalties on particular groupings of the weights that relate the histories of individual series to the output series of interest. We term these sparse component-wise models, e.g. cMLP and cLSTM, when applied to the MLP and LSTM, respectively. In particular, we select for Granger causality by adding group sparsity penalties [13] on the outgoing weights of the inputs.</p>
<p>As in linear methods, appropriate lag selection is crucial for Granger causality selection in nonlinear approachesespecially in highly parametrized models like neural networks. For the MLP, we introduce two more structured group penalties [15], [30] [31] that automatically detect both nonlinear Granger causality and also the lags of each inferred interaction. Our proposed cLSTM model, on the other hand, sidesteps the lag selection problem entirely because the recurrent architecture efficiently models long range dependencies [26]. When the true network of nonlinear interactions is sparse, both the cMLP and cLSTM approaches will select a subset of the time series that Grangercause the output series, no matter the lag of interaction. To our knowledge, these approaches represent the first set of nonlinear Granger causality methods applicable in high dimensions without requiring precise lag specification.</p>
<p>We first validate our approach and the associated penalties via simulations on both linear VAR and nonlinear Lorenz-96 data [32], showing that our nonparametric approach accurately selects the Granger causality graph in both linear and nonlinear settings. Second, we compare our cMLP and cLSTM models with existing Granger causality approaches [33], [34] on the difficult DREAM3 gene regulatory network recovery benchmark datasets [35] and find that our methods outperform a wide set of competitors across all five datasets. Finally, we use our cLSTM method to explore Granger causal interactions between body parts during natural motion with a highly nonlinear and complex dataset of human motion capture [36], [37]. Our implementation is available online: https://github.com/lancovert/ Neural-GC.</p>
<p>Traditionally, the success stories of neural networks have been on prediction tasks in large datasets. In contrast, here, our performance metrics relate to our ability to produce interpretable structures of interaction amongst the observed time series. Furthermore, these successes are achieved in limited data scenarios. Our ability to produce interpretable structures and train neural network models with limited data can be attributed to our use of structured sparsityinducing penalties and the regularization such penalties provide, respectively. We note that sparsity inducing penalties have been used for architecture selection in neural networks [38], [39]. However, the focus of the architecture selection was on improving predictive performance rather than on returning interpretable structures of interaction among observed quantities.</p>
<p>More generally, our proposed formulation shows how structured penalties common in regression [30], [31] may be generalized for structured sparsity and regularization in neural networks. This opens up new opportunities to use these tools in other neural network context, especially as applied to structure learning problems. In concurrent work, a similar notion of sparse-input neural networks were developed for high-dimensional regression and classification tasks for independent data [40].</p>
<h2>2 LINEAR GRANGER CAUSALITY</h2>
<p>Let $\mathbf{x}<em 1="1">{t} \in \mathbb{R}^{p}$ be a $p$-dimensional stationary time series and assume we have observed the process at $T$ time points, $\left(\mathbf{x}</em>}, \ldots, \mathbf{x<em t="t">{T}\right)$. Using a model-based approach, as is our focus, Granger causality in time series analysis is typically studied using the vector autoregressive model (VAR) [9]. In this model, the time series at time $t, \mathbf{x}</em>$, is assumed to be a linear combination of the past $K$ lags of the series</p>
<p>$$
\mathbf{x}<em k="1">{t}=\sum</em>}^{K} A^{(k)} \mathbf{x<em t="t">{t-k}+e</em>
$$</p>
<p>where $A^{(k)}$ is a $p \times p$ matrix that specifies how lag $k$ affects the future evolution of the series and $e_{t}$ is zero mean noise. In this model, time series $j$ does not Granger-cause time series $i$ if and only if for all $k, A_{i j}^{(k)}=0$. A Granger causal analysis in a VAR model thus reduces to determining which values in $A^{(k)}$ are zero over all lags. In higher dimensional settings, this may be determined by solving a group lasso regression problem [41]</p>
<p>$$
\begin{aligned}
\min <em t="K">{A^{(k)}, \ldots, A^{(K)}} &amp; \sum</em>}^{T}\left|\mathbf{x<em k="1">{t}-\sum</em>}^{K} A^{(k)} \mathbf{x<em 2="2">{t-k}\right|</em> \
&amp; +\lambda \sum_{i j}\left|\left(A_{i j}^{(1)}, \ldots, A_{i j}^{(K)}\right|_{2}\right.
\end{aligned}
$$}^{2</p>
<p>where $|\cdot|<em 2="2">{2}$ denotes the $L</em>\right|}$ norm. The group lasso penalty over all lags of each $(i, j)$ entry, $\left|\left(A_{i j}^{(1)}, \ldots, A_{i j}^{(K)<em i="i" j="j">{2}\right.$ jointly shrinks all $A</em>$ parameters to zero across all lags $k$ [13]. The hyper-parameter $\lambda&gt;0$ controls the level of group sparsity.}^{k</p>
<p>The group penalty in Equation (2) may be replaced with a structured hierarchical penalty [30], [42] that automatically</p>
<p>selects the lag of each Granger causal interaction [15]. Specifically, the hierarchical lag selection problem is given by</p>
<p>$$
\begin{aligned}
\min <em t="K">{A^{(1)}, \ldots, A^{(K)}} &amp; \sum</em>}^{T}\left|\mathbf{x<em k="1">{t}-\sum</em>}^{K} A^{(k)} \mathbf{x<em 2="2">{t-k}\right|</em> \
&amp; +\lambda \sum_{i j} \sum_{k=1}^{K}\left|\left(A_{i j}^{(k)}, \ldots, A_{i j}^{(K)}\right|_{2}\right.
\end{aligned}
$$}^{2</p>
<p>where $\lambda&gt;0$ now controls the lag order selected for each interaction. Specifically, at higher values of $\lambda$ there exists a $k$ for each $(i, j)$ pair such that the entire contiguous set of lags $\left(A_{i j}^{(k)}, \ldots, A_{i j}^{(K)}\right)$ is shrunk to zero. If $k=1$ for a particular $(i, j)$ pair, then all lags are equal to zero and series $i$ does not Granger-cause series $j$; thus, this penalty simultaneously selects for Granger non-causality and the lag of each Granger causal pair.</p>
<h2>3 MODELS FOR NEURAL GRANGER CAUSALITY</h2>
<h3>3.1 Adapting Neural Networks for Granger Causality</h3>
<p>A nonlinear autoregressive model (NAR) allows $\mathbf{x}_{t}$ to evolve according to more general nonlinear dynamics [25]</p>
<p>$$
\mathbf{x}<em 1="1" _t="&lt;t">{t}=g\left(x</em>
$$}, \ldots, x_{&lt;t p}\right)+e_{t</p>
<p>where $x_{&lt;t i}=\left(\ldots, x_{(t-2) i}, x_{(t-1) i}\right)$ denotes the past of series $i$ and we assume additive zero mean noise $e_{t}$.</p>
<p>In a forecasting setting, it is common to jointly model the full nonlinear functions $g$ using neural networks. Neural networks have a long history in NAR forecasting, using both traditional architectures [25], [43], [44] and more recent deep learning techniques [27], [29], [45]. These approaches either utilize an MLP where the inputs are $x_{&lt;t}=x_{(t-1):(t-K)}$, for some lag $K$, or a recurrent network, like an LSTM.</p>
<p>There are two problems with applying the standard neural network NAR model in the context of inferring Granger causality. The first is that these models act as black boxes that are difficult to interpret. Due to sharing of hidden layers, it is difficult to specify sufficient conditions on the weights that simultaneously allows series $j$ to Granger cause series $i$ but not Granger cause series $i^{\prime}$ for $i \neq i^{\prime}$. Second, a joint network over all $x_{t i}$ for all $i$ assumes that each time series depends on the same past lags of the other series. However, in practice, each $x_{t i}$ may depend on different past lags of the other series.</p>
<p>To tackle these challenges, we propose a structured neural network approach to modeling and estimation. First, instead of modeling $g$ jointly across all outputs $x_{t}$, as is standard in multivariate forecasting, we instead focus on each output component with a separate model:</p>
<p>$$
x_{t i}=g_{i}\left(x_{&lt;t 1}, \ldots, x_{&lt;t p}\right)+e_{t i}
$$</p>
<p>Here, $g_{i}$ is a function that specifies how the past $K$ lags are mapped to series $i$. In this context, Granger non-causality between two series $j$ and $i$ means that the function $g_{i}$ does not depend on $x_{&lt;t j}$, the past lags of series $j$. More formally,
Definition 1. Time series $j$ is Granger non-causal for time series $i$ if for all $\left(x_{&lt;t 1}, \ldots, x_{&lt;t p}\right)$ and all $x_{&lt;t j}^{\prime} \neq x_{&lt;t j}$,</p>
<p>$$
\begin{aligned}
&amp; g_{i}\left(x_{&lt;t 1}, \ldots, x_{&lt;t j}, \ldots, x_{&lt;t p}\right)= \
&amp; g_{i}\left(x_{&lt;t 1}, \ldots, x_{&lt;t j}^{\prime}, \ldots x_{&lt;t p}\right)
\end{aligned}
$$</p>
<p>that is, $g_{i}$ is invariant to $x_{&lt;t j}$.
In Section 3.2 and 3.3 we consider these component-wise models in the context of MLPs and LSTMs. We examine a set of sparsity inducing penalties as in Equations (2) and (3) that allow us to infer the invariances of Definition 1 that lead us to identify Granger non-causality.</p>
<h3>3.2 Sparse Input MLPs</h3>
<p>Our first approach is to model each output component $g_{i}$ with a separate MLP, so that we can easily disentangle the effects from inputs to outputs. We refer to this approach, displayed pictorially in Figure 1, as a component-wise MLP (cMLP). Let $g_{i}$ take the form of an MLP with $L-1$ layers and let the vector $h_{t}^{l} \in \mathbb{R}^{H}$ denote the values of the $m$ dimensional $l$ th hidden layer at time $t$. The parameters of the neural network are given by weights $\mathbf{W}$ and biases $\mathbf{b}$ at each layer, $\mathbf{W}=\left{W^{1}, \ldots, W^{L}\right}$ and $\mathbf{b}=\left{b^{1}, \ldots, b^{L}\right}$. To draw an analogy with the time series VAR model, we further decompose the weights at the first layer across time lags, $W^{1}=\left{W^{11}, \ldots, W^{1 K}\right}$. The dimensions of the parameters are given by $W^{1} \in \mathbb{R}^{H \times p K}, W^{l} \in \mathbb{R}^{H \times H}$ for $1&lt;l&lt;$ $L, W^{L} \in \mathbb{R}^{H}, b^{l} \in \mathbb{R}^{H}$ for $l&lt;L$ and $b^{L} \in \mathbb{R}$. Using this notation, the vector of first layer hidden values at time $t$ are given by</p>
<p>$$
h_{t}^{1}=\sigma\left(\sum_{k=1}^{K} W^{1 k} \mathbf{x}_{t-k}+b^{1}\right)
$$</p>
<p>where $\sigma$ is an activation function. Typical activation functions are either logistic or tanh functions. The vector of hidden units in subsequent layers is given by a similar form, also with $\sigma$ activation functions:</p>
<p>$$
h_{t}^{l}=\sigma\left(W^{l} h_{t}^{l-1}+b^{l}\right)
$$</p>
<p>After passing through the $L-1$ hidden layers, the time series output, $x_{t i}$, is given by a linear combination of the units in the final hidden layer</p>
<p>$$
x_{t i}=g_{i}\left(x_{&lt;t}\right)+e_{t i}=W^{L} h_{t}^{L-1}+b^{L}+e_{t i}
$$</p>
<p>where $W^{L}$ is the linear output decoder and $h_{t}^{L}$ is the final hidden output from the final $L-1$ th layer. The error term, $e_{t i}$, is modeled as mean zero Gaussian noise. We chose this linear output decoder since our primary motivation involves real-valued multivariate time series. However, other decoders like a logistic, softmax, or poisson likelihood with exponential link function [46], could be used to model nonlinear Granger causality in multivariate binary [47], categorical [48], or positive count time series [47].</p>
<h3>3.2.1 Penalized Selection of Granger Causality in the cMLP</h3>
<p>In Equation (5), if the $j$ th column of the first layer weight matrix, $W_{: j}^{i k}$, contains zeros for all $k$, then series $j$ does not Granger-cause series $i$. That is, $x_{(t-k) j}$ for all $k$ does not influence the hidden unit $h_{t}^{1}$ and thus the output $x_{t i}$. Following Definition 1, we see $g_{i}$ is invariant to $x_{&lt;t j}$. Thus, analogously to the VAR case, one may select for Granger</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. (left) Schematic for modeling Granger causality using cMLPs. If the outgoing weights for series j, shown in dark blue, are penalized to zero, then series j does not Granger-cause series i. (center) The group lasso penalty jointly penalizes the full set of outgoing weights while the hierarchical version penalizes the nested set of outgoing weights, penalizing higher lags more. (right) Schematic for modeling Granger causality using a cLSTM. If the dark blue outgoing weights to the hidden units from an input x<sub>(t−1)j</sub> are zero, then series j does not Granger-cause series i.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Example of group sparsity patterns of the first layer weights of a cMLP with four first layer hidden units and four input series with maximum lag k = 4. Differing sparsity patterns are shown for the three different structured penalties of group lasso (GROUP) from Equation (9), group sparse group lasso (MIXED) from Equation (10) and hierarchical lasso (HIER) from Equation (11).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Example of the group sparsity patterns in a sparse cLSTM model with a four dimensional hidden state and four input series. Due to the group lasso penalty on the columns of W, the W<sup>T</sup>, W<sup>in</sup>, W<sup>o</sup>, and W<sup>c</sup> matrices will share the same column sparsity pattern.</p>
<p>causality by applying a group penalty to the columns of the W<sup>1k</sup> matrices for each gi,</p>
<p>$$\min_{\mathbf{W}} \sum_{t=K}^{T} \left( x_{it} - g_{i} \left( x_{(t-1):(t-K)}\right) \right)^{2} + \lambda \sum_{j=1}^{F} \Omega\left( W_{j}^{\prime} \right). \tag{8}$$</p>
<p>where Ω is a penalty that shrinks the entire set of first layer weights for input series j, i.e., W<sup>1j</sup><sub>j</sub> = (W<sup>11j</sup><sub>j</sub>, ..., W<sup>1K</sup><sub>j</sub>), to zero. We consider three different penalties that, together, show how we recast structured regression penalties to the neural network case.</p>
<p>We first consider a group lasso penalty over the entire set of outgoing weights across all lags for time series j, W<sup>1j</sup><sub>j</sub>,</p>
<p>$$\Omega\left(W_{j}^{\prime}\right) = \left| W_{j}^{\prime} \right|_{F},\tag{9}$$</p>
<p>where || · ||<sub>F</sub> is the Froebenius matrix norm. This penalty shrinks all weights associated with lags for input series j equally. For large enough λ, the solutions to Equation (8) with the group penalty in Equation (9) will lead to many zero columns in each W<sup>1k</sup> matrix, implying only a small number of estimated Granger causal connections. This group penalty is the neural network analogue of the group lasso penalty across lags in Equation 2 for the VAR case.</p>
<p>To detect the lags where Granger causal effects exist, we propose a new penalty called a group sparse group lasso penalty. This penalty assumes that only a few lags of a series j are predictive of series i, and provides both sparsity across groups (a sparse set of Granger causal time series) and sparsity within groups (a subset of relevant lags)</p>
<p>$$\Omega\left(W_{j}^{\prime}\right) = \alpha \left| W_{j}^{\prime} \right|_{F} + (1 - \alpha) \sum_{k=1}^{K} \left| W_{j}^{\prime k} \right|_{2} \tag{10}$$</p>
<p>where α ∈ (0,1) controls the tradeoff in sparsity across and within groups. This penalty is related to, and is a generalization of, the sparse group lasso [49].</p>
<p>Finally, we may simultaneously select for both Granger causality and the lag order of the interaction by replacing the group lasso penalty in Equation (8) with a hierarchical group lasso penalty [15] in the MLP optimization problem,</p>
<p>$$\Omega\left(W_{j}^{\prime}\right) = \sum_{k=1}^{K} \left| \left( W_{j}^{\prime k}, \dots, W_{j}^{\prime K} \right) \right|_{F}. \tag{11}$$</p>
<p>The hierarchical penalty leads to solutions such that for each j there exists a lag k such that all W<sup>1k′</sup><sub>j</sub> = 0 for k′ &gt; k and all W<sup>1k′</sup><sub>j</sub> ≠ 0 for k′ ≤ k. Thus, this penalty effectively selects</p>
<p>the lag of each interaction. The hierarchical penalty also sets many columns of $W^{1 k}$ to be zero across all $k$, effectively selecting for Granger causality. In practice, the hierarchical penalty allows us to fix $K$ to a large value, ensuring that no Granger causal connections at higher lags are missed. Example sparsity patterns selected by the three penalties are shown in Figure 2.</p>
<p>While the primary motivation of our penalties is for efficient Granger causality selection, the lag selection penalties in Equations (10) and (11) are also of independent interest to nonlinear forecasting with neural networks. In this case, over-specifying the lag of a NAR model leads to poor generalization and overfitting [25]. One proposed technique in the literature is to first select the appropriate lags using forward orthogonal least squares [25]; our approach instead combines model fitting and lag selection into one procedure.</p>
<h3>3.3 Sparse Input RNNs</h3>
<p>Recurrent neural networks (RNNs) are particularly well suited for modeling time series, as they compress the past of a time series into a hidden state, aiming to capture complicated nonlinear dependencies at longer time lags than traditional time series models. As with MLPs, time series forecasting with RNNs typically proceeds by jointly modeling the entire evolution of the multivariate series using a single recurrent network.</p>
<p>As in the MLP case, it is difficult to disentangle how each series affects the evolution of another series when using an RNN. This problem is even more severe in complicated recurrent networks like LSTMs. To model Granger causality with RNNs, we follow the same strategy as with MLPs and model each $g_{i}$ function using a separate RNN, which we refer to as a component-wise RNN (cRNN). For simplicity, we assume a single-layer RNN, but our formulation may be easily generalized to accommodate more layers.</p>
<p>Consider an RNN for predicting a single component. Let $\mathbf{h}<em i="i" t="t">{t} \in \mathbb{R}^{H}$ represent the $H$-dimensional hidden state at time $t$, representing the historical context of the time series for predicting a component $x</em>$. The hidden state at time $t+1$ is updated recursively</p>
<p>$$
\mathbf{h}<em i="i">{t}=f</em>}\left(\mathbf{x<em t-1="t-1">{t}, \mathbf{h}</em>\right)
$$</p>
<p>where $f_{i}$ is some nonlinear function that depends on the particular recurrent architecture.</p>
<p>Due to their effectiveness at modeling complex time dependencies, we choose to model the recurrent function $f$ using an LSTM [26]. The LSTM model introduces a second hidden state variable $\mathbf{c}<em t="t">{t}$, referred to as the cell state, giving the full set of hidden parameter as $\left(\mathbf{c}</em>\right)$. The LSTM model updates its hidden states recursively as}, \mathbf{h}_{t</p>
<p>$$
\begin{aligned}
\mathbf{f}<em t="t">{t} &amp; =\sigma\left(W^{f} \mathbf{x}</em>}+U^{f} \mathbf{h<em t="t">{(t-1)}\right) \
\mathbf{i}</em>} &amp; =\sigma\left(W^{i n} \mathbf{x<em _t-1_="(t-1)">{t}+U^{i n} \mathbf{h}</em>\right) \
\mathbf{o}<em t="t">{t} &amp; =\sigma\left(W^{o} \mathbf{x}</em>}+U^{o} \mathbf{h<em t="t">{(t-1)}\right) \
\mathbf{c}</em>} &amp; =\mathbf{f<em t-1="t-1">{t} \odot \mathbf{c}</em>}+\mathbf{i<em t="t">{t} \odot \sigma\left(W^{c} \mathbf{x}</em>}+U^{c} \mathbf{h<em t="t">{t-1}\right) \
\mathbf{h}</em>} &amp; =\mathbf{o<em t="t">{t} \odot \sigma\left(\mathbf{c}</em>\right)
\end{aligned}
$$</p>
<p>where $\odot$ denotes element-wise multiplication and $\mathbf{i}<em t="t">{t}, \mathbf{f}</em>}$, and $\mathbf{o<em t="t">{t}$ represent input, forget and output gates, respectively, that control how each component of the state cell, $\mathbf{c}</em>$, is
updated and then transferred to the hidden state used for prediction, $\mathbf{h}<em t="t">{t}$. In particular, the forget gate, $\mathbf{f}</em>$, controls the amount that the current observation influences the new cell state. The additive form of the cell state update in the LSTM allows it to encode longrange dependencies, since cell states from far in the past may still influence the cell state at time $t$ if the forget gates remain close to one. In the context of Granger causality, this flexible architecture can represent long-range, nonlinear dependencies between time series. As in the cMLP, the output for series $i$ at time $t$ is given by a linear decoding of the hidden state}$, controls the amount that the past cell state influences the future cell state, and the input gate, $\mathbf{i}_{t</p>
<p>$$
x_{t i}=g_{i}\left(x_{&lt;t}\right)+e_{t i}=W^{2} \mathbf{h}<em i="i" t="t">{t}+e</em>
$$</p>
<p>where $W^{2}$ are the output weights. We let $\mathbf{W}=$ $\left(W^{1}, W^{2}, U^{1}\right)$ be the full set of parameters where $W^{1}=\left(\left(W^{f}\right)^{\top},\left(W^{i n}\right)^{\top},\left(W^{o}\right)^{\top},\left(W^{c}\right)^{\top}\right)^{\top}$ and $U^{1}=$ $\left(\left(U^{f}\right)^{\top},\left(U^{i n}\right)^{\top},\left(U^{o}\right)^{\top},\left(U^{c}\right)^{\top}\right)^{\top}$ represent the full set of first layer weights. As in the MLP case, other decoding schemes could be used in the case of categorical or count data.</p>
<h3>3.3.1 Granger Causality Selection in LSTMs</h3>
<p>In Equation (14) the set of input matrices $W^{1}$ controls how the past time series affect the forget gates, input gates, output gates, and cell updates, and, consequently, the update of the hidden representation. Like in the MLP case, for this component-wise LSTM model (cLSTM) a sufficient condition for Granger non-causality of an input series $j$ on an output $i$ is that all elements of the $j$ th column of $W^{1}$ are zero, $W_{: j}^{1}=0$. Thus, we may select series that Grangercause series $i$ using a group lasso penalty across columns of $W^{1}$ by</p>
<p>$$
\min <em t="2">{\mathbf{W}} \sum</em>
$$}^{T}\left(x_{i t}-g_{i}\left(x_{&lt;t}\right)\right)^{2}+\lambda \sum_{j=1}^{p}\left|W_{: j}^{1}\right|_{2</p>
<p>For a large enough $\lambda$, many columns of $W^{1}$ will be zero, leading to a sparse set of Granger causal connections. An example sparsity pattern in the LSTM parameters is shown in Figure 3.</p>
<h2>4 Optimizing the Penalized ObJeCTives</h2>
<h3>4.1 Optimizing the Penalized cMLP Objective</h3>
<p>We optimize the nonconvex objectives of Equation (8) using proximal gradient descent [50]. Proximal optimization is important in our context because it leads to exact zeros in the columns of the input matrices, a critical requirement for interpretating Granger non-causality in our framework. Additionally, a line search can be incorporated into the optimization algorithm to ensure convergence to a local minimum [51]. The algorithm updates the network weights $\mathbf{W}$ iteratively starting with $\mathbf{W}^{(0)}$ by</p>
<p>$$
\mathbf{W}^{(m+1)}=\operatorname{prox}_{\gamma^{(m)} \lambda \Omega}\left(\mathbf{W}^{(m)}-\gamma^{(m)} \nabla \mathcal{L}\left(\mathbf{W}^{(m)}\right)\right)
$$</p>
<p>where $\mathcal{L}=\sum_{t=K}^{T}\left(x_{t i}-g_{i}\left(x_{&lt;t}\right)\right)^{2}$ is the the neural network prediction loss and prox $_{\lambda \Omega}$ is the proximal operator with respect to the sparsity inducing penalty function $\Omega$. The</p>
<p>entries in $\mathbf{W}^{(0)}$ are initialized randomly from a standard normal distribution. The scalar $\gamma^{(m)}$ is the step size, which is either set to a fixed value or determined by line search [51]. While the objectives in Equation (8) are nonconvex, we find that no random restarts are required to accurately detect Granger causality connections.</p>
<p>Since the sparsity promoting group penalties are only applied to the input weights, the proximal step for weights at the higher levels is simply the identity function. The proximal step for the group lasso penalty on the input weights is given by a group soft-thresholding operation on the input weights [50],</p>
<p>$$
\begin{aligned}
\operatorname{prox}<em k="k">{\gamma^{(m)} M \Omega}\left(W</em> \lambda\right) \
&amp; =\left(1-\frac{\lambda \gamma^{(m)}}{\left|W_{: j}^{1}\right|}^{i}\right) &amp; =\operatorname{soft}\left(W_{: k}^{1}, \gamma^{(m)<em _="+">{F}}\right)</em>
\end{aligned}
$$} W_{: k}^{1</p>
<p>where $(x)_{+}=\max (0, x)$. For the group sparse group lasso, the proximal step on the input weights is given by groupsoft thresholding on the lag specific weights, followed by group soft thresholding on the entire resulting input weights for each series, see Algorithm 2. The proximal step on the input weights for the hierarchical penalty is given by iteratively applying the group soft-thresholding operation on each nested group in the penalty, from the smallest group to the largest group [42], and is shown in Algorithm 3.</p>
<p>Algorithm 1 Proximal gradient descent with line search algorithm for solving Equation (8). Proximal steps given in Equation (17) for the group lasso penalty, in Algorithm 2 for the group sparse group lasso penalty, and in Algorithm 3 for the hierarchical penalty.</p>
<div class="codehilite"><pre><span></span><code>Require: \(\lambda&gt;0\)
    \(m=0\), initialize \(\mathbf{W}^{(0)}\)
    while not converged do
        \(m=m+1\)
        determine \(\gamma\) by line search
        for \(j=1\) to \(p\) do
            \(W_{: j}^{1(m+1)}=\operatorname{prox}_{\gamma M \Omega}\left(W_{: j}^{1(m)}-\gamma \nabla_{W_{: j}^{1}} \mathcal{L}\left(\mathbf{W}^{(m)}\right)\right)\)
        end for
        for \(l=2\) to \(L\) do
            \(W^{l(m+1)}=W^{l(m)}-\gamma \nabla_{W^{l}} \mathcal{L}\left(\mathbf{W}^{(m)}\right)\)
        end for
    end while
    return \(\left(\mathbf{W}^{(m)}\right)\)
</code></pre></div>

<p>Algorithm 2 One pass algorithm to compute the proximal map for the group sparse group lasso penalty, for relevant lag selection in the cMLP model.</p>
<div class="codehilite"><pre><span></span><code>Require: \(\lambda&gt;0, \gamma&gt;0,\left(W_{: j}^{11}, \ldots, W_{: j}^{1 K}\right)\)
    for \(k=K\) to 1 do
        \(W_{: j}^{1 k}=\operatorname{soft}\left(W_{: j}^{1 k}, \gamma \lambda\right)\)
    end for
    \(\left(W_{: j}^{11}, \ldots, W_{: j}^{1 K}\right)=\operatorname{soft}\left(\left(W_{: j}^{11}, \ldots, W_{: j}^{1 K}\right), \gamma \lambda\right)\)
    return \(\left(W_{: j}^{11}, \ldots, W_{: j}^{1 K}\right)\)
</code></pre></div>

<p>Algorithm 3 One pass algorithm to compute the proximal map for the hierarchical group lasso penalty, for automatic lag selection in the cMLP model.</p>
<div class="codehilite"><pre><span></span><code>Require: \(\lambda&gt;0, \gamma&gt;0,\left(W_{: j}^{11}, \ldots, W_{: j}^{1 K}\right)\)
    for \(k=K\) to 1 do
        \(\left(W_{: j}^{1 k}, \ldots, W_{: j}^{1 K}\right)=\operatorname{soft}\left(\left(W_{: j}^{1 k}, \ldots, W_{: j}^{1 K}\right), \gamma \lambda\right)\)
    end for
    return \(\left(W_{: j}^{11}, \ldots, W_{: j}^{1 K}\right)\)
</code></pre></div>

<p>Since all datasets we study are relatively small, the gradients are with respect to the full data objective (i.e., all time points); for larger datasets, one could instead use proximal stochastic gradient descent [52].</p>
<h3>4.2 Optimizing the Penalized cLSTM Objective</h3>
<p>Similar to the cMLP, we optimize Equation (15) using proximal gradient descent. When the data consists of many replicates of short time series, like in the DREAM3 data in Section 7, we perform a full backpropagation through time (BPTT) to compute the gradients. However, for longer series we truncate the BPTT by unlinking the hidden sequences. In practice, we do this by splitting the dataset up into equal sized batches, and treating each batch as an independent realization. Under this approach, the gradients used to optimize Equation (15) are only approximations of the gradients of the full cLSTM model. This is very common practice in the training of of RNNs [53], [54], [55]. The full optimization algorithm for training is shown in Algorithm 4.</p>
<p>Algorithm 4 Proximal gradient descent with line search algorithm for solving Equation (15) for the cLSTM with group lasso penalty.</p>
<div class="codehilite"><pre><span></span><code>Require: \(\lambda&gt;0\)
    \(m=0\), initialize \(\mathbf{W}^{(0)}\)
    while not converged do
        \(m=m+1\)
        compute \(\nabla \mathcal{L}\left(\mathbf{W}^{(m)}\right)\) by BPTT (truncated for large \(T\) )
        determine \(\gamma\) by line search.
        for \(j=1\) to \(p\) do
            \(W_{: j}^{1(m+1)}=\operatorname{soft}\left(W_{: j}^{1(m)}-\gamma \nabla_{W_{: j}^{1}} \mathcal{L}\left(\mathbf{W}^{(m)}\right), \gamma \lambda\right)\)
        end for
        \(W^{2(m+1)}=W^{2(m)}-\gamma \nabla_{W^{2}} \mathcal{L}\left(\mathbf{W}^{(m)}\right)\)
        \(U^{1(m+1)}=U^{1(m)}-\gamma \nabla_{U^{1}} \mathcal{L}\left(\mathbf{W}^{(m)}\right)\)
    end while
    return \(\left(\mathbf{W}^{(m)}\right)\)
</code></pre></div>

<h2>5 Comparing CMLP and CLSTM Models for GRANGER CAUSALITY</h2>
<p>Both the cMLP and cLSTM frameworks model each component function $g_{i}$ using independent networks for each $i$. For the cMLP model, one needs to specify a maximum possible model lag $K$. However, our lag selection strategy (Equation 11) allows one to set $K$ to a large value and the weights for higher lags are automatically removed from the model. On the other hand, the cLSTM model requires</p>
<p>TABLE 1
Comparison of AUROC for Granger causality selection among different approaches, as a function of the forcing constant $F$ and the length of the time series $T$. Results are the mean across five initializations, with $95 \%$ confidence intervals.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">$F=10$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$F=40$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$T$</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: left;">cMLP</td>
<td style="text-align: center;">$\mathbf{8 6 . 6} \pm \mathbf{0 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 6 . 6} \pm \mathbf{0 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 8 . 4} \pm \mathbf{0 . 1}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 0} \pm \mathbf{0 . 5}$</td>
<td style="text-align: center;">$\mathbf{8 9 . 6} \pm \mathbf{0 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 5} \pm \mathbf{0 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">cLSTM</td>
<td style="text-align: center;">$81.3 \pm 0.9$</td>
<td style="text-align: center;">$93.4 \pm 0.7$</td>
<td style="text-align: center;">$96.0 \pm 0.1$</td>
<td style="text-align: center;">$75.1 \pm 0.9$</td>
<td style="text-align: center;">$87.8 \pm 0.4$</td>
<td style="text-align: center;">$94.4 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: left;">IMV-LSTM</td>
<td style="text-align: center;">$63.7 \pm 4.3$</td>
<td style="text-align: center;">$76.0 \pm 4.5$</td>
<td style="text-align: center;">$85.5 \pm 3.4$</td>
<td style="text-align: center;">$53.6 \pm 5.2$</td>
<td style="text-align: center;">$59.0 \pm 4.5$</td>
<td style="text-align: center;">$69.0 \pm 4.8$</td>
</tr>
<tr>
<td style="text-align: left;">LOO-LSTM</td>
<td style="text-align: center;">$47.9 \pm 3.2$</td>
<td style="text-align: center;">$49.4 \pm 1.8$</td>
<td style="text-align: center;">$50.1 \pm 1.0$</td>
<td style="text-align: center;">$50.1 \pm 3.3$</td>
<td style="text-align: center;">$49.1 \pm 3.2$</td>
<td style="text-align: center;">$51.1 \pm 3.7$</td>
</tr>
</tbody>
</table>
<p>no maximum lag specification, and instead automatically learns the memory of each interaction. As a consequence, the cMLP and cLSTM differ in the amount of data used for training, as noted by a comparison of the $t$ index in Equation (15) and Equation (11). For a length $T$ series, the cMLP and cLSTM models use $T-K$ and $T-1$ data points, respectively. While insignificant for large $T$, when the data consist of independent replicates of short series, as in the DREAM3 data in Section 7, the difference may be important. This ability to simultaneously model longer range dependencies while harnessing the full training set may explain the impressive performance of the cLSTM in the DREAM3 data in Section 7.</p>
<p>Finally, the zero outgoing weights in both the cMLP and cLSTM are a sufficient but not necessary condition to represent Granger non-causality. Indeed, series $i$ could be Granger non-causal of series $j$ through a complex configuration of weights that exactly cancel each other. However, because we wish to interpret the outgoing weights of the inputs as a measure of dependence, it is important that these weights reflect the true relationship between inputs and outputs. Our penalization schemes in both cMLP and cLSTM acts as a prior that biases the network to represent Granger non-causal relationships with zeros in the outgoing weights of the inputs, rather than through other configurations. Our simulation results in Section 6 validate this intuition.</p>
<h2>6 SIMULATION EXPERIMENTS</h2>
<h3>6.1 cMLP and cLSTM Simulation Comparison</h3>
<p>To compare and analyze the performance of our two approaches, the cMLP and cLSTM, we apply both methods to detecting Granger causality networks in simulated linear VAR data and simulated Lorenz-96 data [32], a nonlinear model of climate dynamics. Overall, the results show that our methods can accurately reconstruct the underlying Granger causality graph in both linear and nonlinear settings. We first describe the results from the Lorenz experiment and present the VAR results subsequently.</p>
<h3>6.1.1 Lorenz-96 Model</h3>
<p>The continuous dynamics in a $p$-dimensional Lorenz model are given by</p>
<p>$$
\frac{d x_{t i}}{d t}=\left(x_{t(i+1)}-x_{t(i-2)}\right) x_{t(i-1)}-x_{t i}+F
$$</p>
<p>where $x_{t(-1)}=x_{t(p-1)}, x_{t 0}=x_{t p}, x_{t(p+1)}=x_{t 1}$ and $F$ is a forcing constant that determines the level of nonlinearity and chaos in the series. Example series for two settings of $F$
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Example multivariate linear (VAR) and nonlinear (Lorenz, DREAM, and MoCap) series that we analyze using both cMLP and cLSTM models. Note as the forcing constant, $F$, in the Lorenz model increases, the data become more chaotic.
are displayed in Figure 4. We numerically simulate a $p=20$ Lorenz-96 model with a sampling rate of $\Delta_{t}=0.05$, which results in a multivariate, nonlinear time series with sparse Granger causal connections.</p>
<p>Using this simulation setup, we test our models' ability to recover the underlying causal structure. Average values of area under the ROC curve (AUROC) for recovery of the causal structure across five initialization seeds are shown in Table 1, and we obtain results under three different data set lengths, $T \in(250,500,1000)$, and two forcing constants, $F \in(10,40)$.</p>
<p>We compare results for the cMLP and cLSTM with two baseline methods that also rely on neural networks, the IMV-LSTM and leave-one-out LSTM (LOO-LSTM) approaches. The IMV-LSTM [56], [57] uses attention weights to provide greater interpretability than standard LSTMs, and it detects Granger causal relationships by aggregating its attention weights. LOO-LSTM detects Granger causal relationships through the increase in loss that results from</p>
<p>TABLE 2
Comparison of AUROC for Granger causality selection among different approaches, as a function of the VAR lag order and the length of the time series $T$. Results are the mean across five initializations, with $95 \%$ confidence intervals.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">VAR(1)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">VAR(2)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$T$</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: left;">cMLP</td>
<td style="text-align: center;">$\mathbf{9 1 . 6} \pm \mathbf{0 . 4}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 9} \pm \mathbf{0 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 8 . 4} \pm \mathbf{0 . 1}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 4} \pm \mathbf{0 . 2}$</td>
<td style="text-align: center;">$88.3 \pm 0.4$</td>
<td style="text-align: center;">$95.1 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: left;">cLSTM</td>
<td style="text-align: center;">$88.5 \pm 0.9$</td>
<td style="text-align: center;">$\mathbf{9 3 . 4} \pm \mathbf{1 . 9}$</td>
<td style="text-align: center;">$97.6 \pm 0.4$</td>
<td style="text-align: center;">$83.5 \pm 0.3$</td>
<td style="text-align: center;">$\mathbf{9 2 . 5} \pm \mathbf{0 . 9}$</td>
<td style="text-align: center;">$\mathbf{9 7 . 8} \pm \mathbf{0 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">IMV-LSTM</td>
<td style="text-align: center;">$53.7 \pm 7.9$</td>
<td style="text-align: center;">$63.2 \pm 8.0$</td>
<td style="text-align: center;">$60.4 \pm 8.3$</td>
<td style="text-align: center;">$53.5 \pm 3.9$</td>
<td style="text-align: center;">$54.3 \pm 3.6$</td>
<td style="text-align: center;">$55.0 \pm 3.4$</td>
</tr>
<tr>
<td style="text-align: left;">LOO-LSTM</td>
<td style="text-align: center;">$50.1 \pm 2.7$</td>
<td style="text-align: center;">$50.2 \pm 2.6$</td>
<td style="text-align: center;">$50.5 \pm 1.9$</td>
<td style="text-align: center;">$50.1 \pm 1.4$</td>
<td style="text-align: center;">$50.4 \pm 1.4$</td>
<td style="text-align: center;">$50.0 \pm 1.0$</td>
</tr>
</tbody>
</table>
<p>withholding each input time series (see Appendix).
We use $H=100$ hidden units for all four methods, as experiments show that performance does not improve with a different number of units. While more layers may prove beneficial, for all experiments we fix the number of hidden layers, $L$, to one and leave the effects of additional hidden layers to future work. For the cMLP, we use the hierarchical penalty with model lag of $K=5$; see Section 6.2 for a performance comparison of several possible penalties across model input lags.</p>
<p>For our methods, we compute AUROC values by sweeping $\lambda$ across a range of values; discarded edges (inferred Granger non-causality) for a particular $\lambda$ setting are those whose associated $L_{2}$ norm of the input weights of the neural network is equal to zero. Note that our proximal gradient algorithm sets many of these groups to be exactly zero. We compute AUROC values for the IMV-LSTM and LOO-LSTM by sweeping a range of thresholds for either the attention values or the increase in loss due to withholding time series.</p>
<p>As expected, the results indicate that the cMLP and cLSTM performance improves as the data set size $T$ increases. The cMLP outperforms the cLSTM both in the less chaotic regime of $F=10$ and the more chaotic regime of $F=40$, but the gap in their performance narrows as more data is used. Both methods outperform the IMV-LSTM and LOO-LSTM by a wide margin. Our models' $95 \%$ confidence intervals are also relatively narrow, at less than $1 \%$ AUROC for the cLSTM and cMLP, compared with 3-5\% for the IMVLSTM.</p>
<p>To understand the role of the number of hidden units in our methods, we perform an ablation study to test different values of $H$; the results show that both the cMLP and cLSTM are robust to smaller $H$ values, but that their performance benefits from $H=100$ hidden units (see Appendix). Additionally, we investigate the importance of the optimization algorithm; we found that Adam [58], proximal gradient descent [50] and proximal gradient descent with a line search [51] lead to similar results (see Appendix). However, because Adam requires a thresholding parameter and the line search is computationally costly, we use standard proximal gradient descent in the remainder of our experiments.</p>
<h3>6.1.2 VAR Model</h3>
<p>To analyze our methods' performance when the true underlying dynamics are linear, we simulate data from $p=20$ VAR(1) and VAR(2) models with randomly generated sparse transition matrices. To generate sparse dependencies for each time series $i$, we create self dependencies and randomly select three more dependencies among the other</p>
<p>TABLE 3
AUROC comparisons between different cMLP Granger causality selection penalties on simulated Lorenz-96 data as a function of the input model lag, $K$. Results are the mean across five initializations, with $95 \%$ confidence intervals.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Lag $K$</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">10</th>
<th style="text-align: center;">20</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GROUP</td>
<td style="text-align: center;">$88.1 \pm 0.8$</td>
<td style="text-align: center;">$82.5 \pm 0.3$</td>
<td style="text-align: center;">$80.5 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: left;">MIXED</td>
<td style="text-align: center;">$90.1 \pm 0.5$</td>
<td style="text-align: center;">$85.4 \pm 0.3$</td>
<td style="text-align: center;">$83.3 \pm 1.1$</td>
</tr>
<tr>
<td style="text-align: left;">HIER</td>
<td style="text-align: center;">$\mathbf{9 5 . 5} \pm \mathbf{0 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 4} \pm \mathbf{0 . 5}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 2} \pm \mathbf{0 . 3}$</td>
</tr>
</tbody>
</table>
<p>$p-1$ time series. Where series $i$ depends on series $j$, we set $A_{i j}^{k}=0.1$ for $k=1$ or $k=1,2$, and all other entries of $A$ are set to zero. Examining both VAR models allows us to see how well our methods detect Granger causality at longer time lags, even though no time lag is explicitly specified in our models. Our results are the average over five random initializations for a single dependency graph.</p>
<p>The AUROC results are displayed in Table 2 for the cLSTM, cMLP, IMV-LSTM, and LOO-LSTM approaches for three dataset lengths, $T \in(250,500,1000)$. The performance of the cLSTM and cMLP improves at larger $T$, and, as in the Lorenz-96 case, both models outperform the IMV-LSTM and LOO-LSTM by a wide margin. The cMLP remains more robust than the cLSTM with smaller amounts of data, but the cLSTM outperforms the cMLP on several occasions with $T=500$ or $T=1000$.</p>
<p>The IMV-LSTM consistently underperforms our methods with these datasets, likely because it is not explicitly designed for Granger causality discovery. Our finding that the IMV-LSTM performs poorly at this task is consistent with recent work suggesting that attention mechanisms are not indicative of feature importance [59]. The LOOLSTM approach consistently achieves poor performance, likely due to two factors: (i) unregularized LSTMs are prone to overfitting in the low-data regime, even when Granger causal time series are held out, and (ii) withholding a single time series will not impact the loss if the remaining time series have dependencies that retain its signal.</p>
<h3>6.2 Quantitative Analysis of the Hierarchical Penalty</h3>
<p>We next quantitatively compare the three possible structured penalties for Granger causality selection in the cMLP model. In Section 3.2 we introduced the full group lasso (GROUP) penalty over all lags (Equation 8), the group sparse group lasso (MIXED) (Equation 10) and the hierarchical (HIER) lag selection penalty (Equation 11). We compare these approaches across various choices of the cMLP model's maximum lag, $K \in(5,10,20)$. We use $H=10$</p>
<p>hidden units for data simulated from the nonlinear Lorenz model with $F=20$, $p=20$, and $T=750$. As in Section 6.1, we compute the mean AUROC over five random initializations and display the results in Table 3. Importantly, the hierarchical penalty outperforms both group and mixed penalties across all model input lags $K$. Furthermore, performance significantly declines as $K$ increases in both group and mixed settings while the performance of the hierarchical penalty stays roughly constant as $K$ increases. This result suggests that performance of the hierarchical penalty for nonlinear Granger causality selection is robust to the input lag, implying that precise lag specification is unnecessary. In practice, this allows one to set the model lag to a large value without worrying that nonlinear Granger causality detection will be compromised.</p>
<h3>6.3 Qualitative Analysis of the Hierarchical Penalty</h3>
<p>To qualitatively validate the performance of the hierarchical group lasso penalty for automatic lag selection, we apply our penalized cMLP framework to data generated from a sparse VAR model with longer interactions. Specifically, we generate data from a $p=10$, VAR(3) model as in Section 2. To generate sparse dependencies for each time series $i$, we create self dependencies and randomly select two more dependencies among the other $p-1$ time series. When series $i$ depends on series $j$, we set $A_{i j}^{k}=0.1$ for $k=1,2,3$. All other entries of $A$ are set to zero. This implies that the Granger causal connections that do exist are of true lag 3. We run the cMLP with the hierarchical group lasso penalty and a maximal lag order of $K=5$; for comparison, we also train a VAR model with a hierarchical penalty and maximal lag order $K=5$.</p>
<p>We visually display the selection results for one cMLP (i.e., one output series) and the VAR baseline across a variety of $\lambda$ settings in Figure 5. For the lower $\lambda=4.09 e-4$ setting, the cMLP both (i) overestimates the lag order for a few input series and (ii) allows some false positive Granger causal connections. For the higher $\lambda=7.94 e-4$, lag selection performs almost perfectly, in addition to correct estimation of the Granger causality graph. Higher $\lambda$ values lead to larger penalization on longer lags, resulting in weaker longlag connections. The VAR model, which is ideal for VAR data, does not perform noticeably better. While we show results for multiple $\lambda$ values for visualization, in practice one may use cross validation to select the appropriate $\lambda$.</p>
<h2>7 DREAM CHALLENGE</h2>
<p>We next apply our methods to estimate Granger causality networks from a realistically simulated time course gene expression data set. The data are from the DREAM3 challenge [35] and provide a difficult, nonlinear data set for rigorously comparing Granger causality detection methods [33], [34]. The data is simulated using continuous gene expression and regulation dynamics, with multiple hidden factors that are not observed. The challenge contains five different simulated data sets, each with different ground truth Granger causality graphs: two E. Coli (E.C.) data sets and three Yeast (Y.) data sets. Each data set contains $p=100$ different time series, each with 46 replicates sampled at 21</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Qualitative results of the cMLP automatic lag selection using a hierarchical group lasso penalty and maximal lag of $K=5$. The true data are from a VAR(3) model. The images display results for a single cMLP (one output series) and a VAR model using various penalty strengths $\lambda$. The rows of each image correspond to different input series while the columns correspond to the lag, with $k=1$ at the left and $k=5$ at the right. The magnitude of each entry is the $L_{2}$ norm of the associated input weights of the neural network after training. The true lag interactions are shown in the rightmost image. Brighter color represents larger magnitude. time points for a total of 966 time points. This represents a very limited data scenario relative to the dimensionality of the networks and complexity of the underlying dynamics of interaction. Three time series components from a single replicate of the E. Coli 1 data set are shown in Figure 4.</p>
<p>We apply both the cMLP and cLSTM to all five data sets. Due to the short length of the series replicates, we choose the maximum lag in the cMLP to be $K=2$ and use $H=10$ and $H=5$ hidden units for the cMLP and cLSTM, respectively. For our performance metric, we consider the DREAM3 challenge metrics of area under the ROC curve (AUROC) and area under the precision recall curve (AUPR). Both curves are computed by sweeping $\lambda$ over a range of values, as described in Section 6.</p>
<p>In Figure 6, we compare the AUROC and AUPR of our cMLP and cLSTM to previously published AUROC and AUPR results on the DREAM3 data [33]. These comparisons include both linear and nonlinear approaches: (i) a linear VAR model with a lasso penalty (LASSO) [7], (ii) a dynamic Bayesian network using first-order conditional dependencies (G1DBN) [34], and (iii) a state-of-the-art multioutput kernel regression method (OKVAR) [33]. The latter is the most mature of a sequence of nonlinear kernel Granger causality detection methods [60], [61]. In terms of AUROC, our cLSTM outperforms all methods across all five datasets. Furthermore, the cMLP method outperforms previous methods on two datasets, Y.1 and Y.3, ties G1DBN on Y.2, and slightly under performs OKVAR in E.C.1 and E.C.2. In terms of AUPR, both cLSTM and cMLP methods do much better than all previous approaches, with the cLSTM</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. (Top) AUROC and (bottom) AUPR (given in %) results for our proposed regularized cMLP and cLSTM models and the set of methods—OKVAR, LASSO, and G1DBN presented in [33]. These results are for the DREAM3 size-100 networks using the original DREAM3 data sets.</p>
<p>outperforming the cMLP in three datasets. The raw ROC curves for cMLP and cLSTM are displayed in Figure 7.</p>
<p>These results clearly demonstrate the importance of taking a nonlinear approach to Granger causality detection in a (simulated) real-world scenario. Among the nonlinear approaches, the neural network methods are extremely powerful. Furthermore, the cLSTM's ability to efficiently capture long memory (without relying on long-lag specifications) appears to be particularly useful. This result validates many findings in the literature where LSTMs outperform autoregressive MLPs. An interesting facet of these results, however, is that the impressive performance gains are achieved in a limited data scenario and on a task where the goal is recovery of interpretable structure. This is in contrast to the standard story of prediction on large datasets. To achieve these results, the regularization and induced sparsity of our penalties is critical.</p>
<h2>8 Dependencies in Human Motion Capture Data</h2>
<p>We next apply our methodology to detect complex, nonlinear dependencies in human motion capture (MoCap) recordings. In contrast to the DREAM3 challenge results, this analysis allows us to more easily visualize and interpret the learned network. Human motion has been previously modeled using both linear dynamical systems [62], switching linear dynamical systems [37], [63] and also nonlinear dynamical models using Gaussian processes [64]. While the focus of previous work has been on motion classification [62] and segmentation [37], our analysis delves into the potentially long-range, nonlinear dependencies between different regions of the body during natural motion behavior. We consider a data set from the CMU MoCap database</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7. ROC curves for the cMLP ( ) and cLSTM ( ) models on the five DREAM datasets.</p>
<p>[36] previously studied in [37]. The data set consists of <em>p</em> = 54 joint angle and body position recordings across two different subjects for a total of <em>T</em> = 2024 time points. In total, there are recordings from 24 unique regions because some regions, like the thorax, contain multiple angles of motion corresponding to the degrees of freedom of that part of the body.</p>
<p>We apply the cLSTM model with <em>H</em> = 8 hidden units to this data set. For computational speed ups, we break the original series into length 20 segments and fit the penalized cLSTM model from Equation (15) over a range of λ values. To develop a weighted graph for visualization, we let the edge weight <em>w<sub>i</sub></em> between components be the norm of the outgoing cLSTM weights from input series <em>j</em> to output component series <em>i</em>, standardized by the maximum such edge weight associated with the cLSTM for series <em>i</em>. Edges associated with more than one degree of freedom (angle directions for the same body part) are averaged together. Finally, to aid visualization, we further threshold edge weights of magnitude 0.01 and below.</p>
<p>The resulting estimated graphs are displayed in Figure 9 for multiple values of the regularization parameter, λ. While we present the results for multiple λ, one may use cross validation to select λ if one graph is required. To interpret the presented skeleton plots, it is useful to understand the full set of motion behaviors exhibited in this data set. These behaviors are depicted in Figure 8, and include instances of <em>jumping jacks</em>, <em>side twists</em>, <em>arm circles</em>, <em>knee raises</em>, <em>squats</em>, <em>punching</em>, various forms of <em>toe touches</em>, and <em>running in place</em>. Due to the extremely limited data for any individual behavior,</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8. (Top) Example time series from the MoCap data set paired with their particular motion behaviors. (Bottom) Skeleton visualizations of 12 possible exercise behavior types observed across all sequences analyzed in the main text.</p>
<p>havior, we chose to learn interactions from data aggregated over the entire collection of behaviors. In Figure 9, we see many intuitive learned interactions. For example, even in the more sparse graph (largest λ) we learn a directed edge from right knee to left knee and a separate edge from left knee to right. This makes sense as most human motion, including the motions in this dataset involving lower body movement, entail the right knee leading the left and then vice versa. We also see directed interactions leading down each arm, and between the hands and toes for toe touches.</p>
<h2>9 CONCLUSION</h2>
<p>We have presented a framework for nonlinear Granger causality selection using regularized neural network models of time series. To disentangle the effects of the past of an input series on the future of an output, we model each output series using a separate neural network. We then apply both the component multilayer perceptron (cMLP) and component long-short term memory (cLSTM) architectures, with associated sparsity promoting penalties on incoming weights to the network, and select for Granger causality. Overall, our results show that these methods outperform existing Granger causality approaches on the challenging DREAM3 data set and discover interpretable and insightful structure on a human MoCap data set.</p>
<p>Our work opens the door to multiple exciting avenues for future work. While we are the first to use a hierarchical lasso penalty in a neural network, it would be interesting to also explore other types of structured penalties, such as tree structured penalties [31].</p>
<p>Furthermore, although we have presented two relatively simple approaches, based off MLPs and LSTMs, our general framework of penalized input weights easily accommodates more powerful architectures. Exploring the effects of multiple hidden layers, powerful recurrent and convolutional</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 9. Nonlinear Granger causality graphs inferred from the human MoCap data set using the regularized cLSTM model. Results are displayed for a range of λ values. Each node corresponds to one location on the body.</p>
<p>architectures, like clockwork RNNs [65], and dilated causal convolutions [66], open up a wide range of research directions and the potential to detect long-range and complex dependencies. Further theoretical work on the identifiability of Granger non-causality in these more complex network models becomes even more important.</p>
<p>Finally, while we consider sparse input models, a different sparse output architecture would use a network, like an RNN, to learn hidden representations of each individual input series, and then model each output component as a sparse nonlinear combination across the hidden states of all time series, allowing a shared hidden representation across component tasks. A schematic of the proposed architecture that combines ideas from our cMLP and cLSTM models is shown in Figure 10.</p>
<h2>APPENDIX A MODEL ABLATIONS</h2>
<p>We ran two ablation studies to understand factors that influence our methods' performance. First, we tested the cMLP and cLSTM with different numbers of hidden units on the Lorenz-96 data. Table 4 shows the AUROC results from a single run for two datasets with forcing constants F ∈ (10, 40) and time series length T = 1000, using different numbers of hidden units, H ∈ (5, 10, 25, 50, 100). The results reveal that both models are robust to a small number of hidden units, but that their performance improves with</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 10. Proposed architecture for detecting nonlinear Granger causality that combines aspects of both the cLSTM and cMLP models. A separate hidden representation, h_{t1}, is learned for each series j using an RNN. At each time point, the hidden states are each fed into a sparse cMLP to predict the individual output for each series x_{t1}. Joint learning of the whole network with a group penalty on the input weights of the individual cMLPs would allow the network to share information about hidden features in each h_{t1} while also allowing interpretable structure learning between the hidden states of each series and each output.</p>
<p>larger values of H. These findings suggest that overparameterization can help with the nonconvex optimization objective, leading to solutions that achieve high predictive accuracy while minimizing the penalty from the sparsity-inducing regularizer.</p>
<p>Next, we tested three approaches for optimizing our penalized objectives (Equations 8 and 15). We compared standard gradient descent with Adam [58] to proximal gradient descent (ISTA) [50] and proximal gradient descent with a line search (GIST) [51] on the Lorenz-96 data with T = 1000 time points. Table 5 displays AUROC results across five initializations for two forcing constants F ∈ (10, 40), using the cMLP with H = 10 hidden units. The results show that the three methods lead to similar results for both F = 10 and F = 40, although we did not compare the optimizers in other scenarios, e.g., with lower T values or with the cLSTM.</p>
<p>Among these optimization approaches, Adam is fastest due to its adaptive learning rate, but it requires a parameter for thresholding the resulting weights (while the proximal methods lead to exact zeros). In contrast, GIST guarantees convergence to a local minimum and is less sensitive to the learning rate parameter, but it is also considerably slower than Adam and ISTA. We therefore use standard proximal gradient descent, or ISTA, in the remainder of our experiments, because it leads to exact zeros while being more efficient than GIST. In practice, this means running Algorithm 1 or Algorithm 4 using a fixed learning rate γ rather than determining it by a line search.</p>
<h2>APPENDIX B BASELINE METHODS</h2>
<p>The IMV-LSTM uses an attention mechanism to highlight the model's dependence on different parts of the input [57]. We train a separate IMV-LSTM model to predict each time series using all the time series as inputs, using the "IMV-Full" variant [57], and we use the attention weights from</p>
<p>TABLE 4 AUROC comparisons for the cMLP and cLSTM as a function of the number of hidden units H for simulated Lorenz-96 data. Results are calculated using a single run.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>cMLP</th>
<th></th>
<th>cLSTM</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>F</td>
<td>10</td>
<td>40</td>
<td>10</td>
<td>40</td>
</tr>
<tr>
<td>H = 5</td>
<td>96.5</td>
<td>91.0</td>
<td>91.9</td>
<td>86.9</td>
</tr>
<tr>
<td>H = 10</td>
<td>98.0</td>
<td>94.0</td>
<td>94.5</td>
<td>91.5</td>
</tr>
<tr>
<td>H = 25</td>
<td>98.4</td>
<td>94.3</td>
<td>95.6</td>
<td>92.3</td>
</tr>
<tr>
<td>H = 50</td>
<td>98.3</td>
<td>94.4</td>
<td>95.7</td>
<td>93.8</td>
</tr>
<tr>
<td>H = 100</td>
<td>98.5</td>
<td>94.5</td>
<td>95.7</td>
<td>95.2</td>
</tr>
</tbody>
</table>
<p>TABLE 5 AUROC comparisons between different optimization approaches for the cMLP with simulated Lorenz-96 data. Results are the mean across five initializations, with 95% confidence intervals.</p>
<table>
<thead>
<tr>
<th>F</th>
<th>10</th>
<th>40</th>
</tr>
</thead>
<tbody>
<tr>
<td>GISTA</td>
<td>98.0 ± 0.2</td>
<td>93.8 ± 0.3</td>
</tr>
<tr>
<td>ISTA</td>
<td>98.0 ± 0.2</td>
<td>94.1 ± 1.9</td>
</tr>
<tr>
<td>Adam</td>
<td>98.3 ± 0.1</td>
<td>95.1 ± 0.2</td>
</tr>
</tbody>
</table>
<p>the trained models to infer Granger causal relationships. Similar to the original work [56], we record the empirical mean of the attention values for each input time series for each model, and we construct a p × p matrix of these values for the separate IMV-LSTMs. We then sweep over a range of threshold values to determine the most influential inputs for each IMV-LSTM, and we trace out an ROC curve from which we calculate AUROC values.</p>
<p>The LOO-LSTM baseline is based on the idea that withholding a highly predictive input should result in a decrease in predictive accuracy, a direction that has been explored for providing model-agnostic notions of feature importance [67], [68]. We begin by training separate LSTM models to predict each time series using all time series as inputs. We then train separate LSTM models to predict each time series i using all inputs except time series j, and we record the increase in loss when the jth time series is withheld. Using the results, we construct a p × p matrix representing the differences in the loss, we sweep over a range of threshold values to determine the most influential inputs for each time series, and we trace out an ROC curve from which we calculate AUROC values.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>AT, IC, NF and EF acknowledge the support of ONR Grant N00014-15-1-2380, NSF CAREER Award IIS-1350133, and AFOSR Grant FA9550-16-1-0038. AT and AS acknowledge the support from NSF grants DMS-1161565 and DMS-1561814 and NIH grants R01GM114029 and R01GM133848.</p>
<h2>REFERENCES</h2>
<ul>
<li>[1] O. Sporns, <em>Networks of the Brain</em>. MIT Press, 2010.</li>
<li>[2] R. Vicente, M. Wibral, M. Lindner, and G. Pipa, "Transfer entropy—a model-free measure of effective connectivity for the neurosciences," <em>Journal of Computational Neuroscience</em>, vol. 30, no. 1, pp. 45–67, 2011.</li>
</ul>
<p>[3] P. A. Stokes and P. L. Purdon, "A study of problems encountered in Granger causality analysis from a neuroscience perspective," Proceedings of the National Academy of Sciences, vol. 114, no. 34, pp. E7063-E7072, 2017.
[4] A. Sheikhattar, S. Miran, J. Liu, J. B. Fritz, S. A. Shamma, P. O. Kanold, and B. Babadi, "Extracting neuronal functional network dynamics via adaptive Granger causality analysis," Proceedings of the National Academy of Sciences, vol. 115, no. 17, pp. E3869-E3878, 2018.
[5] W. F. Sharpe, G. J. Alexander, and J. W. Bailey, Investments. Prentice Hall, 1968.
[6] A. Fujita, P. Severino, J. R. Sato, and S. Miyano, "Granger causality in systems biology: modeling gene networks in time series microarray data using vector autoregressive models," in Brazilian Symposium on Bioinformatics. Springer, 2010, pp. 13-24.
[7] A. C. Lozano, N. Abe, Y. Liu, and S. Rosset, "Grouped graphical granger modeling methods for temporal causal modeling," in Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2009, pp. 577-586.
[8] C. W. Granger, "Investigating causal relations by econometric models and cross-spectral methods," Econometrica: Journal of the Econometric Society, pp. 424-438, 1969.
[9] H. Liitkepohl, New Introduction to Multiple Time Series Analysis. Springer Science \&amp; Business Media, 2005.
[10] S. Basu, A. Shojaie, and G. Michailidis, "Network Granger causality with inherent grouping structure," The Journal of Machine Learning Research, vol. 16, no. 1, pp. 417-453, 2015.
[11] K. Sameshima and L. A. Baccala, Methods in Brain Connectivity Inference Through Multivariate Time Series Analysis. CRC Press, 2016.
[12] R. Tibshirani, "Regression shrinkage and selection via the lasso," Journal of the Royal Statistical Society: Series B (Methodological), vol. 58, no. 1, pp. 267-288, 1996.
[13] M. Yuan and Y. Lin, "Model selection and estimation in regression with grouped variables," Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 68, no. 1, pp. 49-67, 2006.
[14] S. Basu, G. Michailidis et al., "Regularized estimation in sparse high-dimensional time series models," The Annals of Statistics, vol. 43, no. 4, pp. 1535-1567, 2015.
[15] W. B. Nicholson, J. Bien, and D. S. Matteson, "Hierarchical vector autoregression," arXiv preprint arXiv:1412.5250, 2014.
[16] A. Shojaie and G. Michailidis, "Discovering graphical Granger causality using the truncating lasso penalty," Bioinformatics, vol. 26, no. 18, pp. i517-i523, 2010.
[17] P. Bühlmann and S. Van De Geer, Statistics for High-Dimensional Data: Methods, Theory and Applications. Springer Science \&amp; Business Media, 2011.
[18] T. Teräsvirta, D. Tjøstheim, C. W. J. Granger et al., Modelling Nonlinear Economic Time Series. Oxford University Press Oxford, 2010.
[19] H. Tong, "Nonlinear time series analysis," International Encyclopedia of Statistical Science, pp. 955-958, 2011.
[20] B. Lusch, P. D. Maia, and J. N. Kutz, "Inferring connectivity in networked dynamical systems: Challenges using Granger causality," Physical Review E, vol. 94, no. 3, p. 032220, 2016.
[21] P.-O. Amblard and O. J. Michel, "On directed information theory and Granger causality graphs," Journal of Computational Neuroscience, vol. 30, no. 1, pp. 7-16, 2011.
[22] J. Runge, J. Heitzig, V. Petoukhov, and J. Kurths, "Escaping the curse of dimensionality in estimating multivariate transfer entropy," Physical Review Letters, vol. 108, no. 25, p. 258701, 2012.
[23] M. Raissi, P. Perdikaris, and G. E. Karniadakis, "Multistep neural networks for data-driven discovery of nonlinear dynamical systems," arXiv preprint arXiv:1801.01236, 2018.
[24] Ö. Koji, "River flow modeling using artificial neural networks," Journal of Hydrologic Engineering, vol. 9, no. 1, pp. 60-63, 2004.
[25] S. A. Billings, Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains. John Wiley \&amp; Sons, 2013.
[26] A. Graves, "Supervised sequence labelling," in Supervised Sequence Labelling with Recurrent Neural Networks. Springer, 2012, pp. 5-13.
[27] R. Yu, S. Zheng, A. Anandkumar, and Y. Yue, "Long-term forecasting using tensor-train RNNs," arXiv preprint arXiv:1711.00073, 2017.
[28] G. P. Zhang, "Time series forecasting using a hybrid arima and neural network model," Neurocomputing, vol. 50, pp. 159-175, 2003.
[29] Y. Li, R. Yu, C. Shahabi, and Y. Liu, "Graph convolutional recurrent neural network: Data-driven traffic forecasting," arXiv preprint arXiv:1707.01926, 2017.
[30] J. Huang, T. Zhang, and D. Metaxas, "Learning with structured sparsity," Journal of Machine Learning Research, vol. 12, no. Nov, pp. $3371-3412,2011$.
[31] S. Kim and E. P. Xing, "Tree-guided group lasso for multi-task regression with structured sparsity." in International Conference on Machine Learning, vol. 2. Citeseer, 2010, p. 1.
[32] A. Karimi and M. R. Paul, "Extensive chaos in the lorenz-96 model," Chaos: An Interdisciplinary Journal of Nonlinear Science, vol. 20, no. 4, p. 043105, 2010.
[33] N. Lim, F. D'Alché-Buc, C. Auliac, and G. Michailidis, "Operatorvalued kernel-based vector autoregressive models for network inference," Machine Learning, vol. 99, no. 3, pp. 489-513, 2015.
[34] S. Lèbre, "Inferring dynamic genetic networks with low order independencies," Statistical Applications in Genetics and Molecular Biology, vol. 8, no. 1, pp. 1-38, 2009.
[35] R. J. Prill, D. Marbach, J. Saez-Rodriguez, P. K. Sorger, L. G. Alexopoulos, X. Xue, N. D. Clarke, G. Altan-Bonnet, and G. Stolovitzky, "Towards a rigorous assessment of systems biology models: the dream3 challenges," PloS One, vol. 5, no. 2, p. e9202, 2010.
[36] CMU, "Carnegie mellon university motion capture database," 2009, data retrieved from CMU, /http:// mocap.cs.cmu.edu/.
[37] E. B. Fox, M. C. Hughes, E. B. Sudderth, M. I. Jordan et al., "Joint modeling of multiple time series via the beta process with application to motion capture segmentation," The Annals of Applied Statistics, vol. 8, no. 3, pp. 1281-1313, 2014.
[38] J. M. Alvarez and M. Salzmann, "Learning the number of neurons in deep networks," in Advances in Neural Information Processing Systems, 2016, pp. 2270-2278.
[39] C. Louizos, K. Ullrich, and M. Welling, "Bayesian compression for deep learning," in Advances in Neural Information Processing Systems, 2017, pp. 3288-3298.
[40] J. Feng and N. Simon, "Sparse-input neural networks for highdimensional nonparametric regression and classification," arXiv preprint arXiv:1711.07592, 2017.
[41] A. C. Lozano, N. Abe, Y. Liu, and S. Rosset, "Grouped graphical granger modeling for gene expression regulatory networks discovery," Bioinformatics, vol. 25, no. 12, pp. i110-i118, 2009.
[42] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach, "Proximal methods for hierarchical sparse coding," Journal of Machine Learning Research, vol. 12, no. Jul, pp. 2297-2334, 2011.
[43] S. R. Chu, R. Shoureshi, and M. Tenorio, "Neural networks for system identification," IEEE Control Systems Magazine, vol. 10, no. 3, pp. 31-35, 1990.
[44] S. Billings and S. Chen, "The determination of multivariable nonlinear models for dynamic systems using neural networks," 1996.
[45] Y. Tao, L. Ma, W. Zhang, J. Liu, W. Liu, and Q. Du, "Hierarchical attention-based recurrent highway networks for time series prediction," arXiv preprint arXiv:1806.00685, 2018.
[46] P. McCullagh and J. A. Nelder, Generalized Linear Models. CRC Press, 1989, vol. 37.
[47] E. C. Hall, G. Raskutti, and R. Willett, "Inference of highdimensional autoregressive generalized linear models," arXiv preprint arXiv:1605.02693, 2016.
[48] A. Tank, E. B. Fox, and A. Shojaie, "Granger causality networks for categorical time series," arXiv preprint arXiv:1706.02781, 2017.
[49] N. Simon, J. Friedman, T. Hastie, and R. Tibshirani, "A sparse-group lasso," Journal of Computational and Graphical Statistics, vol. 22, no. 2, pp. 231-245, 2013. [Online]. Available: https://doi.org/10.1080/10618600.2012.681250
[50] N. Parikh, S. Boyd et al., "Proximal algorithms," Foundations and Trends in Optimization, vol. 1, no. 3, pp. 127-239, 2014.
[51] P. Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye, "A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems," in International Conference on Machine Learning. PMLR, 2013, pp. 37-45.
[52] L. Xiao and T. Zhang, "A proximal stochastic gradient method with progressive variance reduction," SIAM Journal on Optimization, vol. 24, no. 4, pp. 2057-2075, 2014.
[53] R. J. Williams and D. Zipser, "Gradient-based learning algorithms for recurrent," Backpropagation: Theory, architectures, and applications, vol. 433, 1995.
[54] I. Sutskever, Training Recurrent Neural Networks. University of Toronto Toronto, Canada, 2013.</p>
<p>[55] P. J. Werbos et al., "Backpropagation through time: what it does and how to do it," Proceedings of the IEEE, vol. 78, no. 10, pp. 15501560, 1990.
[56] T. Guo, T. Lin, and Y. Lu, "An interpretable LSTM neural network for autoregressive exogenous model," arXiv preprint arXiv:1804.05251, 2018.
[57] T. Guo, T. Lin, and N. Antulov-Fantulin, "Exploring interpretable LSTM neural networks over multi-variable data," in International Conference on Machine Learning. PMLR, 2019, pp. 2494-2504.
[58] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," arXiv preprint arXiv:1412.6980, 2014.
[59] S. Wiegreffe and Y. Pinter, "Attention is not not explanation," arXiv preprint arXiv:1908.04626, 2019.
[60] V. Sindhwani, M. H. Quang, and A. C. Lozano, "Scalable matrix-valued kernel learning for high-dimensional nonlinear multivariate regression and Granger causality," arXiv preprint arXiv:1210.4792, 2012.
[61] D. Marinazzo, M. Pellicoro, and S. Stramaglia, "Kernel-Granger causality and the analysis of dynamical networks," Physical Review E, vol. 77, no. 5, p. 056215, 2008.
[62] E. Hsu, K. Pulli, and J. Popović, "Style translation for human motion," ACM Trans. Graph., vol. 24, no. 3, pp. 1082-1089, Jul. 2005. [Online]. Available: http://doi.acm.org/10.1145/1073204.1073315
[63] V. Pavlovic, J. M. Rehg, and J. MacCormick, "Learning switching linear models of human motion," in Advances in Neural Information Processing Systems, 2001, pp. 981-987.
[64] J. M. Wang, D. J. Fleet, and A. Hertzmann, "Gaussian process dynamical models for human motion," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, no. 2, pp. 283-298, 2007.
[65] J. Koutnik, K. Greff, F. Gomez, and J. Schmidhuber, "A clockwork rnn," arXiv preprint arXiv:1402.3511, 2014.
[66] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, "Wavenet: A generative model for raw audio," arXiv preprint arXiv:1609.03499, 2016.
[67] J. Lei, M. G'Sell, A. Rinaldo, R. J. Tibshirani, and L. Wasserman, "Distribution-free predictive inference for regression," Journal of the American Statistical Association, vol. 113, no. 523, pp. 1094-1111, 2018.
[68] G. Hooker and L. Mentch, "Please stop permuting features: An explanation and alternatives," arXiv preprint arXiv:1905.03151, 2019.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Denotes equal contribution.</li>
<li>Alex Tank was with the Department of Statistics, University of Washington, Seattle, WA, 98103. E-mail: alextank@uw.edu</li>
<li>Ian Covert, Nicholas Foti, and Emily Fox were with the Department of Computer Science, University of Washington, Seattle, WA, 98103.</li>
<li>Ali Shojaie was with the Department of Biostatistics, University of Washington, Seattle, WA, 98103</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
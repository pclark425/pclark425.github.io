<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9553 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9553</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9553</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-269922029</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.12063v2.pdf" target="_blank">CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct ~12K high-quality data to assess the strengths, weaknesses, and potential risks of various off-the-shelf LLMs. Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting. These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity. Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge. In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs. Our dataset is available at https://github.com/zt991211/CLAMBER</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9553",
    "paper_id": "paper-269922029",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004457249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models
1 Jun 2024</p>
<p>Tong Zhang scu.zhangtong@gmail.com 
College of Computer Science
Sichuan University
China</p>
<p>Engineering Research Center of Machine Learning and Industry Intelligence</p>
<p>Ministry of Education
China</p>
<p>♠ ♡⋆ 
Peixin Qin 
College of Computer Science
Sichuan University
China</p>
<p>Engineering Research Center of Machine Learning and Industry Intelligence</p>
<p>Ministry of Education
China</p>
<p>Yang Deng 
Chen Huang huangc.scu@gmail.com 
College of Computer Science
Sichuan University
China</p>
<p>Hongru Liang 
College of Computer Science
Sichuan University
China</p>
<p>Engineering Research Center of Machine Learning and Industry Intelligence</p>
<p>Ministry of Education
China</p>
<p>Junhong Liu 
Ant Group
National University of Singapore</p>
<p>Wenqiang Lei ♠♡ wenqianglei@scu.edu.cn 
College of Computer Science
Sichuan University
China</p>
<p>Engineering Research Center of Machine Learning and Industry Intelligence</p>
<p>Ministry of Education
China</p>
<p>Ant Group
National University of Singapore</p>
<p>Tat-Seng Chua 
CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models
1 Jun 2024EF0A5080C57E02809417AF4715833E6BarXiv:2405.12063v2[cs.CL]
Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction.To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy.Building upon the taxonomy, we construct ∼ 12K high-quality data to assess the strengths, weaknesses, and potential risks of various off-theshelf LLMs.Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting.These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity.Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge.In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs.Our dataset is available at https://github.com/zt991211/CLAMBER.</p>
<p>Introduction</p>
<p>Given well-defined user queries, large language models (LLMs) have demonstrated remarkable proficiency in facilitating the information search process (Pan et al., 2023;Kamalloo et al., 2023;Zhang and Choi, 2023;Huang et al., 2023).They provide more precise search results with the help of the inherent knowledge stored within LLMs.Nonetheless, as evidenced by previous studies (Kuhn et al., 2023;Deng et al., 2023a), the practical utility of LLMs is hindered by unclear and ambiguous user † Corresponding author.</p>
<p>⋆ Both authors contributed equally to this study.</p>
<p>queries in real-world scenarios.For instance, in a query like "what are the strategies for saving?", the term "saving" can have multiple interpretations, such as "saving money" or "saving from sins", depending on the user's actual need.This necessitates LLMs proactively identifying (i.e., determine if the query is ambiguous or not) and clarifying the ambiguities rather than providing potentially incorrect answers that may not align with the user's true needs, ultimately risking user trust and satisfaction (Liao et al., 2023).</p>
<p>Driven by this concern, recent works have explored LLMs' capacity to address ambiguous queries (Deng et al., 2023b;Kuhn et al., 2023).However, these investigations have been somewhat fragmented, lacking a comprehensive taxonomy, leading to incomplete and inconsistent handling of ambiguity distributions (Keyvan and Huang, 2022;Rahmani et al., 2023).As a notable example, they are often limited to contextual ambiguity, where the given context is insufficient for producing a definitive answer.In the era of LLMs, there should be more emphasis on the LLM-oriented ambiguity that may occur when inherent knowledge stored within LLMs have conflict understanding about the query.Consequently, it still remains unclear which ambiguities LLMs can effectively identify and clarify, along with the challenges that LLMs persistently encounter in this regard.</p>
<p>To this end, we introduce CLAMBER (Clarifying Ambiguous Query), a novel benchmark for comprehensively evaluating LLMs in identifying and clarifying various ambiguities using a well-organized taxonomy.Drawing inspiration from the input-process-output framework for evaluating collaborative systems (Pinsonneault and Kraemer, 1989), we establish a taxonomy that consolidates both input understanding and task completion perspectives into three primary dimensions, as illustrated in Table 1.These dimensions are further conceptualized into eight fine-grained categories to facilitate in-depth evaluation.Building upon this taxonomy, we construct ∼ 12K data for analyzing the pros and cons of LLMs when identifying and clarifying ambiguities.</p>
<p>With CLAMBER, we comprehensively evaluate strengths, weaknesses, and potential risks of various LLMs.Our findings indicate that Chat-GPT (OpenAI, 2022) outperforms other smallscale LLMs, especially excelling in identifying and clarifying ambiguities in multifaceted queries (Clarke et al., 2009), such as "What is the largest manufacturer in China?", which does not specify the type of "manufacturer".However, they still encounter numerous challenges: 1) current LLMs, despite leveraging chain-of-thought (CoT) and few-shot prompting, face challenges in identifying ambiguities.Our results suggest that CoT and few-shot prompting may lead to the overconfidence issue in small-scale LLMs, impacting ambiguity identification negatively.Even with a large number of shots and CoT support, LLMs only achieve a marginal improvement.Moreover, current LLMs struggle to leverage contextual cues to disambiguate pronouns, highlighting the inadequacy in deducing underlying ambiguities.2) Current LLMs fail to ask high-quality clarifying questions, due to the inability of knowing their knowledge gap.Despite LLMs recognize a query containing ambiguities, their lack of conflict resolution and inaccurate use of inherent knowledge results in uncertainty about which ambiguity to clarify.This prompts the need of developing effective methods for LLMs to resolve conflicts and accurately utilize their inherent knowledge.</p>
<p>In this paper, CLAMBER stands as a valuable resource to provide guidance and insight into evaluating LLMs and addressing ambiguous information needs for future improvements.In conclusion, our contributions are threefold:</p>
<p>• We introduce a taxonomy for categorizing various query ambiguities.This taxonomy combines three primary dimensions, detailed as eight categories for facilitating fine-grained evaluations.• We present a novel benchmark called CLAM-BER, tailored to the characteristics of LLMs.It contains ∼ 12K data featuring ambiguous user queries across diverse categories.• With CLAMBER, we evaluate the off-the-shelf LLMs in an inclusive manner.Our findings shed light on why current LLMs struggle to identify and clarify ambiguities.These insights will guide future research in this field.</p>
<p>Related Works</p>
<p>Our research is closely tied to the taxonomy and resolution of ambiguities in LLMs.We provide a literature review and highlight our differences.Ambiguity Taxonomy.As evidenced by a recent survey (Rahmani et al., 2023), there is a lack of a well-organized taxonomy for ambiguity in information retrieval.While previous research attempts to integrate ambiguity taxonomies, their taxonomies are fragmented and underdeveloped (Ginzburg, 1996;Song et al., 2007), failing to facilitate comprehensive evaluations.Recent taxonomies Min et al. (2020); Guo et al. (2021) are formulated based on a limited set of factual questions and lack precise definitions for each category.Moreover, existing taxonomies were established before the era of LLMs, disregarding the ambiguity specific to LLMs that may arise from conflicting interpretations of queries by the inherent knowledge stored within LLMs.This is evident when LLMs encounter unfamiliar entities (Yin et al., 2023) or potential inconsistencies within queries (Tamkin et al., 2022).For the first time, we introduce a well-organized taxonomy for categorizing various query ambiguities.Our taxonomy draws inspiration from the input-process-output view to evaluate collaborative systems.It combines three primary dimensions that capture potential ambiguities during input understanding and task completion of LLMs.Using this taxonomy, we construct ∼ 12K data for analyzing the pros and cons of LLMs in resolving different ambiguities.</p>
<p>Resolving ambiguity in LLMs.Recent efforts resort to CoT and few-shot prompting to enhance LLMs' capacity in identifying and clarifying ambiguous queries (Deng et al., 2023b;Kuhn et al., 2023;Cole et al., 2023).While these efforts have shown some improvements in performance, they are confined to tasks involving specific types of ambiguities, such as lexical ambiguity.In this paper, we incorporate CoT and few-shot prompting as baselines to evaluate their efficacy and inadequacy across a broader range of ambiguity types using CLAMBER.Other related works try to examine which of the two queries exhibits more ambiguity (Zhang and Choi, 2023), unable to determine if a</p>
<p>CLAMBER Benchmark</p>
<p>To evaluate LLMs in an inclusive manner, we present CLAMBER, which introduces a taxonomy encompassing three key dimensions (i.e., Epistemic Misalignment, Linguistic Ambiguity, Aleatoric Output) that capture potential ambiguities during input understanding and task completion.These three dimensions are further divided into eight specific categories.We delve into the taxonomy and data collection process in following sections.Each data comprises a user query, a binary ambiguity label, and a clarifying question for ambiguous queries.See details of data collection in Appendix C.</p>
<p>Epistemic Misalignment (EM)</p>
<p>Building upon the input understanding perspective, EM occurs when inherent knowledge stored within LLMs have conflict understanding about the query (Cole et al., 2023;Zhang and Choi, 2023).This ambiguity is a distinctive feature of LLMs, as they respond to queries using their inherent knowledge.</p>
<p>We categorize EM into two categories based on the source of conflicting:</p>
<p>• Unfamiliar.(Wang et al., 2023).To mitigate bias stemming by training data, CLAM-BER opts to utilize entirely new, fabricated knowledge that are unfamiliar to all LLMs.To achieve this, we resort to the ALCUNA dataset (Yin et al., 2023) as our data resource, which includes queries that contain new entities fabricated by modifying existing ones.We classify the queries containing new entities as ambiguous, while the rest are unambiguous.Subsequently, we instruct GPT-4 to generate a clarifying question for each ambiguous query, focusing on the ambiguity of new entities.</p>
<p>As for the Contradiction category, the contradiction in CLAMBER occurs when the query and the given examples fail to match within a single interpretation.To achieve this, we directly utilize the AmbiTask dataset (Tamkin et al., 2022) to provide ambiguous queries, which encodes contradiction among queries and provided examples.Additionally, we create clarifying questions for ambiguous queries by rule-based templates and manually transform ambiguous queries into unambiguous ones by resolving contradictions.</p>
<p>Linguistic Ambiguity (LA)</p>
<p>Building upon the input understanding perspective, LA arises when a word, phrase, or statement can be interpreted in multiple ways due to its imprecise or unclear meaning (Berry and Kamsties, 2004;Ortega-Martín et al., 2023).We categorize LA into the the lexical and semantic ambiguities1 , which encapsulate the main challenges in information retrieval (Coden et al., 2015;Xu et al., 2019).</p>
<p>• Lexical Ambiguity.It concerns individual terms with multiple meanings.For example, given a query "Tell me about the source of Nile", the term "source of Nile" can be interpreted in two meanings: the origin of the Nile river or the board game named "source of Nile".In this case, LLMs should ask for clarification: "Are you referring to the Nile river or the board game?" • Semantic Ambiguity.It involves the lack of context leading to more than one interpretation of a sentence (Ortega-Martín et al., 2023).For example, given a query "When did he land on the moon?", it is unclear who "he" may refer to without context.In this case, LLMs should ask for clarification: "Who is 'he' referring to?" Data Collection.Lexical Ambiguity pertains to individual terms with multiple meanings, often found in entity names and polysemy words (Keyvan and Huang, 2022).In this paper, we resort to the Am-bER (Chen et al., 2021) and AmbiPun dataset (Mittal et al., 2022), which contain ambiguous entity names and ambiguous polysemy words, respectively.We extract these terms along with their various meanings from the datasets and then create ambiguous queries, clarifying questions and unambiguous queries using GPT-4.As for Semantic Ambiguity, CLAMBER pay special focus investigating referent ambiguity following (Kuhn et al., 2022;Ortega-Martín et al., 2023).This type of ambiguity occurs in queries containing pronouns that lack contextual clues for clarification.Specifically, we employ the AmbiCoref dataset (Yuan et al., 2023), which consists of minimal pairs featuring ambiguous and unambiguous referents.In this regard, an ambiguous query can be achieved by reducing context sizes to a single sentence and creating sentences where the verbs involved limit the interpretation of their arguments.Additionally, we obtain the unambiguous queries by instructing GPT-4 and obtain clarifying questions by rulebased templates.</p>
<p>Aleatoric Output (AO)</p>
<p>Building upon the task completion perspective, AO occurs when the input is well-formed but the output contains potential confusion due to the lack of essential elements.It is prevalent across various types of queries in information retrieval, including faceted queries (Clarke et al., 2009), queries missing details (Trienes and Balog, 2019), board queries (Song et al., 2007) and under-specific queries (Aliannejadi et al., 2021).Previous studies have focused on specific aspects of this ambiguity, but there is a need for a more comprehensive understanding of this ambiguity in order to advance research.Inspired by (Zamani et al., 2020), we categorize AO into four specific categories based on the type of missing elements:</p>
<p>• Whom denotes the absence of personal details, such as expertise.Given a query "Suggest me some gifts for my mother", the response may vary due to missing the personal preferences of his mother.In this case, a clarifying question like: "What specific preferences does your mother have?" would be preferred.• Where pertains to the lack of spatial information, such as departure place.For example, given a query "Tell me how to reach New York", the response may vary due to missing the specific departure information.In this case, LLMs should ask for clarification "Where do you start from?" • When refers to the absence of temporal elements, such as specific dates.Given a query "How many goals did Argentina score in the World Cup?", the response may vary due to missing the specific World Cup year.This ambiguity requires LLMs to seek further details by asking clarifying questions "Which year of the World Cup are you referring to?" • What refers to the remaining types.For example, when a query is "Who played Thanos in Guardians of the Galaxy?", the response may vary due to missing the specific version of Guardians of the Galaxy.Clarifying question should arise: "Which version are you referring to: TV series, 2014 film, or Telltale Series?"Data Collection.We construct four categories of ambiguities by recognizing the specific missing elements in well-structured queries.To accomplish this, we resort to the the AmbigQA dataset (Min et al., 2020) and the Dolly-16K dataset (Conover et al., 2023)) containing factual and instrumental user search intent (Alexander et al., 2022).As for AmbigQA dataset, queries with multiple answers are deemed ambiguous, while those with a single answer are considered unambiguous.Ambiguous queries are manually categorized into the four categories.In the Dolly dataset, each query is automatically labeled as ambiguous or unambiguous by GPT-4, then manually verified and classified into the four categories if marked as ambiguous.</p>
<p>Due to the difficulty in crafting category-specific unambiguous queries, all four categories share the same set of unambiguous queries.</p>
<p>Validation and Revision</p>
<p>To ensure the quality of our dataset, we engage five linguistic experts for validation and revision.</p>
<p>Initially, each data is validated by four experts, and subsequently consolidated by the remaining expert.The validation process includes verifying the accuracy of ambiguity labels and assessing the effectiveness of clarifying questions.If there are discrepancies between the four experts' validation, the final expert examines their feedback and implements necessary data revisions.For further details on the validation and revision procedures, please refer to Appendix F. Finally, our data statistics are presented in Table 2.</p>
<p>Experimental Design</p>
<p>We consider two tasks to evaluate off-the-shelf LLMs, including identifying ambiguities (cf.Section 5) and asking clarifying questions (cf.Section 6).preserving the same number of data samples per category.There are 200 positive and negative examples for each category.Particularly, the negative examples of each category within Aleatoric Output is 800 since its uniform nature.</p>
<p>Usage of LLMs.As our set of LLMs, we evaluate Llama2-13B-Chat (i.e., Llama2-13B), Llama2-13B-Instruct (i.e., Llama2-13B-I), Vicuna-13B, Llama2-70B-Chat (i.e., Llama2-70B), and the GPT-3.5-Turbo-16k(i.e., ChatGPT).These LLMs are widely used in recent studies of information search (Deng et al., 2023b;Zhang and Choi, 2023).</p>
<p>Prompting Schemes.Following (Deng et al., 2023b), we devise four prompting schemes for evaluation: 1) Zero-shot w/o CoT, where the LLM is evaluated directly on the test dataset, 2) Zero-shot w/ CoT (Wei et al., 2022), where the LLM starts with ambiguity analysis before making predictions, 3) Few-shot w/o CoT (Dong et al., 2022), where the LLM is evaluated by providing examples, 4) Few-shot w/ CoT, where the LLM is evaluated by providing examples with their corresponding ambiguity analysis.In the few-shot setting, we provide two randomly selected examples, one is ambiguous and the other is unambiguous.Importantly, we carefully selected 3 prompts and test all LLMs on these prompts.We present the average performance across various prompts to guarantee the statistical significance of the experimental findings.Details on prompts are presented in Appendix A.</p>
<p>5 Task 1: Identifying Ambiguity</p>
<p>This section aims to evaluate the ability of LLMs to identify different categories of ambiguous user queries, focusing on both the overall performance (cf.Section 5.1) and performance specific to each category (cf.Section 5.2).Following (Hu et al., 2023;Deng et al., 2023b), we adopt the Accuracy and F1 score as metrics.Table 3: Overall ambiguity identification evaluation of LLMs with varying prompting schemes.ChatGPT emerges as the superior model, yet there is still considerable room for improvement, even enhanced by the CoT and Few-Shot.</p>
<p>Overall Evaluation</p>
<p>As shown in Table 3, our findings suggest that current LLMs, despite leveraging CoT and few-shot prompting, face challenges in identifying ambiguities.Our detailed observations are as follows.</p>
<p>Figure 1: Investigation on the identification accuracy when handling ambiguous (i.e, Acc@1) versus unambiguous queries (i.e, Acc@0).We report the results under Zero-shot w/o CoT setting.Small-scale LLMs tend to classify most queries as ambiguous.</p>
<p>In general, current LLMs are struggle to identify ambiguities.We observe that small-scale LLMs are unable to differentiate between ambiguous and unambiguous queries.In particular, they not only show significantly low performance but also demonstrate a substantial discrepancy between accuracy and F1 score.For instance, the accuracy of Llama2-70B with Zero-shot w/o CoT is 50.37, while its F1 score is notably lower at 34.27.This implies a notable variation in their performance when handling ambiguous versus unambiguous queries.As depicted in Figure 1, these models tend to classify most queries as ambiguous, even those that are actually unambiguous.Compared to small-scale LLMs, ChatGPT stands out as the superior model.However, it only reaches an accuracy of 54.25% and an F1 score of 52.77%.There remains large room for improvement.</p>
<p>CoT and few-shot prompting hold promise for enhancing ambiguity identification, but their ef- Table 5: Overconfidence evaluation on LLMs with and without few-shot prompting.Significant differences are marked in grey .</p>
<p>fectiveness is not guaranteed.They may lead to the overconfidence issue in small-scale LLMs, leading to negative outcomes.As shown in Table 3, the effectiveness of CoT and few-shot prompting doesn't consistently improve.To delve deeper, we follow Cole et al. (2023) and gauged LLMs' prediction confidence2 using Expected Calibration Error (ECE) and Area Under the Receiver Operating Characteristic curve (AUROC).ECE assesses the alignment of confidence scores with actual accuracy, while AUROC measures the ability of confidence scores to distinguish between correct and incorrect predictions.Our in-depth analysis, presented in Table 4 and Table 5, reveals that employing CoT and few-shot prompting leads small-   scale LLMs (e.g., Llama2-13B) to exhibit overconfidence and less accurate ambiguity prediction, contrary to our intended outcome.</p>
<p>Even bolstered by numerous shots and CoT support, LLMs still struggles to accurately identify query ambiguity.Figure 2 illustrates the performance of ChatGPT when enhanced with multiple shots.The results indicate that the improvement seen with few-shot prompting is minimal and often inferior to the zero-shot counterpart.A considerable number of shots (e.g., 12 shots) are required for few-shot prompting to outperform the zero-shot method.However, this also entails longer input lengths, risking exceeding the length limit for most small-scale LLMs in our study.Providing examples alone to ChatGPT could result in the learning of superficial patterns that contradict its inherent knowledge, thereby diminishing its performance.Furthermore, ChatGPT's difficulty in fully grasping correct reasoning with limited examples could be another contributing factor.</p>
<p>Fine-Grained Evaluation</p>
<p>This section analyzes the challenges LLMs faced in comprehending different ambiguities, offering insights to guide future enhancements.ChatGPT displays superior performance on Aleatoric Output compared to small-scale LLMs.Across all categories of Aleatoric Output, ChatGPT attains an average increase of 5% in accuracy and 8% in F1 score.This superior performance may stem from its vast world knowledge, enabling it to infer the absence of task-oriented elements in user queries.Additional results reveal ChatGPT performs exceptionally well in the "whom", while struggles more with the "when" and "where" categories.This suggests room for future improvement in handling queries lacking temporal and spatial elements.</p>
<p>The semantic category presents a significant challenge for all LLMs.As shown in 38) and a low F1 score (i.e., 28.17).We observe that 81.97% of errors are false negatives, indicating that ChatGPT often misidentifies queries with self-contradictions as unambiguous.This limitation could be attributed to its training approach (i.e., SFT and RLHF (Ouyang et al., 2022)), which compels ChatGPT to generate responses for all user queries, irrespective of potential contradictions.</p>
<p>6 Task 2: Asking Clarifying Questions</p>
<p>This section investigates the ability of LLMs to produce effective clarifying questions for resolving ambiguities.Overall, current LLMs fail to ask highquality clarifying questions, due to the inability of assessing their knowledge boundaries.Detailed observations are outlined below.</p>
<p>Overall Evaluation</p>
<p>We utilize BertScore for automated assessment, as lexical matching metrics can not adequately capture clarification abilities (Guo et al., 2021).Specifically, we compute the semantic similarity using BERT between the generated question and annotated clarifying questions.Additionally, we also conduct human evaluation3 to score whether the generated question is helpful in resolving query ambiguity (denoted as Help.4 ) ChatGPT demonstrating its superior capabilities in generating clarifying questions compared to small-scale LLMs.Table 7 showcases the effectiveness of clarifying questions produced by different LLMs.It is evident that ChatGPT demonstrates an average performance improvement of 10.29 compared to Vicuna-13B, the top-performing small-scale LLM.This indicates that ChatGPT excels in generating natural and useful clarifying questions (i.e., what to ask).</p>
<p>Fine-Grained Evaluation</p>
<p>We provide an in-depth error analysis to reveal the inadequacies in asking clarifying questions.Since ChatGPT + Few-shot w/ CoT stands as the most effective model, our analysis focus on it.Specifically, we randomly sampled 50 error clarifying questions (whose Help scores are 0) from each category, 400 in total.Inspired by (Deng et al., 2023b), we categorize these failure cases into four groups:</p>
<p>• Wrong Aspect.It refers the case when the generated question is aimed to clarify an incorrect aspect of the user's query.</p>
<p>• Under-specified.The generated question is too unspecific, making it difficult for the user to provide useful feedback.</p>
<p>• Over-specified.The generated question is an overly detailed one when the needed information is already evident in the user's original query.</p>
<p>• Generation error.ChatGPT doesn't generate the output as the required format, such as no clarification question.</p>
<p>As illustrated in Figure 3, inability of knowing their knowledge gap is the main reason for the inadequacies in asking effective clarifying questions.Specifically, when dealing with the Epistemic Misalignment and Linguistic Ambiguity, most errors are concentrated on Under-specified and Over-specified, while Wrong Aspect is evident in Aleatoric Output, with an average of 52.25% error rate.This indicates that ChatGPT can not fully comprehend semantic nuances and lack of conflict resolution despite their large parameters.Moreover, ChatGPT use their inherent knowledge inaccurately to clarify the missing elements of ambiguous queries.These findings imply that there exists a gap between inherent knowledge within LLMs and the ambiguities contained in user queries.</p>
<p>Conclusion</p>
<p>In this work, we introduce CLAMBER, a benchmark for evaluating LLMs in identifying and clarifying ambiguous user queries through a wellorganized taxonomy.CLAMBER comprises ∼ 12K high-quality data covering a wide range of ambiguity categories.With CLAMBER, we assess strengths, weaknesses, and potential risks of various off-the-shelf LLMs.Our results indicate that current LLMs still face difficulties in achieving optimal performance in ambiguity identification and clarification, limiting their practical utility in advanced information search applications.In this paper, CLAMBER acts as a foundation for enhancing the proactive capabilities of LLMs in addressing ambiguity.Moving forward, we plan to integrate more challenging and comprehensive datasets into our CLAMBER based on our taxonomy.</p>
<p>Limitations</p>
<p>In this section, we discuss the limitations of this work from the following perspectives: Sensitivity of Prompts.Similar to other studies on prompting LLMs (Amayuelas et al., 2023;Deng et al., 2023b), the evaluation results are likely to be sensitive to the prompts.While we employ three different prompts and report the average results, it is challenging to assert that they are the most suitable ones for our specific issue.Indeed, the sensitivity of prompts and their optimality present significant research areas within LLMs, warranting further exploration in future studies.Limited LLMs.We only use 5 Large Language Models (LLMs) in our CLAMBER benchmark due to computational constraints.If given additional resources and an improved experimental environment, it would be advantageous to evaluate the performance of other LLMs, such as PaLM540B, etc., in our CLAMBER benchmark.</p>
<p>Michael J. Q.Zhang and Eunsol Choi.2023.Clarify when necessary: Resolving ambiguity through interaction with lms.</p>
<p>A Prompt Design</p>
<p>Table 13 presents our four prompting schemes used for evaluation.In the case of few-shot prompting, we randomly choose two examples from our CLAMBER benchmark.The demonstration of chain-of-thoughts is written by human annotators, which represents their own ambiguity analysis.</p>
<p>B Implementation Details</p>
<p>The implementation of our LLMs is based on Pytorch and Transformers toolkit.In particular, for Llama2-13B-Chat5 and Llama2-70B-Chat6 , we adopt the official version in Huggingface.For Llama2-13B-instruct, We adopt the version7 that is fine-tuned on multiple instruction-following datasets.For Vicuna-13B, we choose the Vicuna-13B-delta-v1.5 version8 .In particular, we set the temperature to 0 for ChatGPT and 0.5 for other open-sourced LLMs.In addition, we set the maximum number of new tokens to 128.During inference, the decoding strategy of open-sourced LLMs is top-p sampling with a top-p of 0.8.For the F1 score, we use the weighted F1 score as our metric, given the balanced nature of our test set.Our aim is to ensure the model's accuracy without ambiguity, minimizing the need for excessive clarification.All of our experiments are conducted on two NVIDIA A100 GPUs.</p>
<p>C Details of Data Collection</p>
<p>In this section, we describe the detailed data collection process of each category, including the data processing and the prompts used by GPT-4.</p>
<p>C.1 ALCUNA Dataset</p>
<p>The ALCUNA dataset (Yin et al., 2023) creates new entities by altering existing entity attributes and relationships, resulting in artificial entities that are distinct from real-world entities.It contains numerous question-answer pairs designed as a benchmark to evaluate the capabilities of LLMs, especially in handling new knowledge.Specifically, we classify questions containing new entities in this dataset as ambiguous queries, and those involving existing entities as unambiguous queries.Furthermore, we randomly select 1500 ambiguous queries and employ GPT-4 to generate a clarifying question for each one, focusing on the ambiguity of the new entity.We provide the data examples and the prompt of generating clarifying question in Table 14.</p>
<p>C.2 AmbiTask Dataset</p>
<p>The AmbiTask Dataset (Tamkin et al., 2022) constructs multiple classification tasks, each accompanied by an instruction and two provided examples.</p>
<p>The two examples can lead to multiple explanations of the instruction, resulting in contradictions.</p>
<p>In particular, we select three classification tasks: "propn negation", "religious pronoun" and "subject location", totaling 1200 instances.we rephrase the ambiguous tasks using rule-based templates to enhance their clarity.These rephrased ambiguous tasks serve as our ambiguous queries.We generate clarifying questions based on the rule-based templates for these ambiguous queries.Additionally, we create unambiguous queries by manually modifying the instruction and make sure the two examples can lead to just one interpretation.We also rephrase these unambiguous queries to make them clear.The rule-based templates and data examples are provided in Table 16.</p>
<p>C.3 AmbER Dataset</p>
<p>The AmbER dataset (Chen et al., 2021) includes instances of entity ambiguity, where a single name can refer to multiple entities.Each ambiguous entity is annotated with its different meanings, and each meaning is associated with a factual question.Specifically, we have chosen the top-500 most frequent entities in the non-human category from AmbER as our data source.We feed the ambiguous entity and its questions related to each meaning into GPT-4, which then generates ambiguous queries along with corresponding clarifying questions.By providing these generated ambiguous queries and corresponding clarifying questions, we guide GPT-4 to produce a clear and unambiguous version.Further information about the prompts and data samples can be found in Table 17.</p>
<p>C.4 AmbiPun Dataset</p>
<p>The AmbiPun dataset (Mittal et al., 2022) comprises pun words that carry diverse meanings depending on the context.Each pun words is an-notated with its various meanings.We randomly select 500 instances as our data resource, following the same data collection process as the AmbER dataset.Please refer to Table 18 for the prompts and data examples.</p>
<p>C.5 AmbiCoref Dataset</p>
<p>The AmbiCoref dataset (Yuan et al., 2023) consists of minimal pairs featuring ambiguous and unambiguous referents.This dataset extends the scope of psycholinguistic research on how individuals perceive ambiguity in specific verb structures and their arguments.We incorporate the ambiguous and unambiguous referent of this dataset as corresponding queries into our benchmark.For those ambiguous queries, we use a template to generate a clarifying question.The templates and examples are in Table 15.</p>
<p>C.6 AmbigQA Dataset</p>
<p>The AmbigQA dataset (Min et al., 2020) consists of ambiguous factoid questions sourced from Natural Questions (Kwiatkowski et al., 2019).We classify the questions with multiple answers as ambiguous while those those with a single answer are considered unambiguous.Furthermore, we rely on the clarifying question annotations in (Lee et al., 2023), we use the key word in their annotations and further categorize each ambiguous question manually into four categories.We adopt their annotated clarifying questions directly.The data examples are presented in Table 19.</p>
<p>C.7 Dolly Dataset</p>
<p>The Dolly dataset (Conover et al., 2023) is commonly used for instructional fine-tuning purposes.</p>
<p>We specifically choose the instructions from the open-qa sub-category as they align with the task of information retrieval.Our approach involves instructing GPT-4 to differentiate between ambiguous and unambiguous queries, generating clarifying questions for the ambiguous ones, and then classifying them into our predefined categories.Please refer to Table 20 for examples of the prompts and data.</p>
<p>D Human Evaluation Details</p>
<p>To evaluate the effectiveness of clarifying questions produced by LLMs, we engage 3 annotators to conduct a human evaluation.Each annotator is tasked with evaluating each clarifying question alongside the corresponding ambiguous query and its associated category of ambiguity.The annotators are instructed to adhere to a specific protocol for evaluating the quality of clarifying questions: Initially, they are to verify if the clarifying questions generated by LLMs adhere to the correct format.Subsequently, they are to determine whether the clarifying questions effectively aid in resolving ambiguity within user queries.In cases where a clarifying question is considered unhelpful, the annotator will categorize the failure into one of four error types as detailed in Deng et al. (2023b): wrong aspect, under-specified, over-specified, or generation error.Overall, we assess totally 400 queries and measure the inter-annotator agreement.We achieve an inter-annotator reliability of Krippen-dorff's alpha of above 0.70 for all ambiguity categories in our taxonomy.In Table 12, we provide examples of generated clarifying questions for each error category.</p>
<p>E More Task Results</p>
<p>Table 9, 10, 11 present the results of all LLMs across different categories under three different settings: Zero-shot w/o CoT, Zero-shot w/ CoT, and Few-shot w/o CoT.We discover that while the exact values vary, the overall performance and analysis conclusions remain largely consistent with Sec 5.2.</p>
<p>F Human Validation and Revision</p>
<p>We initially engage 8 language experts via online platforms.Subsequently, they are assigned the task of reviewing 50 data samples according to provided instructions as part of a qualifying assessment.The 5 experts who successfully pass this assessment are then designated to validate and revise our dataset.</p>
<p>For each query, they are given the respective ambiguity label and a corresponding clarifying question if the query is ambiguous.They are required to adhere to a specific protocol for validating and re-vising our dataset: Firstly, they need to verify if the query is ambiguous and if the ambiguity label assigned is accurate.Secondly, if the query is deemed ambiguous, they should evaluate whether the clarifying question effectively resolves any ambiguity.</p>
<p>In instances of differing opinions during validation, discussions should be held to reach a consensus on the final data outcome.If significant disagreement persists even after discussion, the data will be discarded.We ensure the quality of our final data in two ways.The two authors of this paper acted as meta-reviewers, selecting 50 questions from each of the eight categories across the three dimensions in CLAMBER.The meta-reviewers assessed the correctness of ambiguity labels and the effectiveness of clarifying questions.For the 400 data samples, the average label accuracy was 92.4% and the average BLEU score was 73.2.Based on the results from the meta-reviewers, the data in CLAMBER is considered to be of high quality.</p>
<p>Figure 2 :
2
Figure 2: Performance of ChatGPT enhanced with multiple examples.We ensure a variety of categories in the examples and maintain an equal balance of ambiguous and unambiguous instances.</p>
<p>Figure 3 :
3
Figure 3: The statistics of error analysis.ChatGPT is unable to recognize their knowledge gap for the inadequacies in asking the effective clarifying questions.</p>
<p>Table 1 :
1
' if the sentence contains [category withhold] and 'Y' otherwise.The critic is in the restaurant.&gt;X.The butterfly is in the river.&gt;Y.The proposed taxonomy of ambiguous queries and examples.The clarifying questions of each example are provided in Table8.
DimensionCategoryExplanationExampleEpistemic MisalignmentUNFAMILIAR CONTRADICTIONQuery contains unfamiliar entities or facts Query contains self-contradictionsFind the price of Samsung Chromecast. Output 'XThe boar is in the theatre.&gt;?LinguisticLEXICALQuery contains terms with multiple meaningsTell me about the source of Nile.AmbiguitySEMANTICQuery lacks of context leading multiple interpretationsWhen did he land on the moon?Query output containsWHOconfusion due toSuggest me some gifts for my mother.missing personal elementsQuery output containsWHENconfusion due toHow many goals did Argentina score in the World Cup?Aleatoricmissing temporal elementsOutputQuery output containsWHEREconfusion due toTell me how to reach New York.missing spatial elementsQuery output containsWHATconfusion due toReal name of gwen stacy in spiderman?missing task-specific elements
query is ambiguous.</p>
<p>Table 2 :
2
CLAMBER Dataset Sources and Statistics.
CategorySourcesDistributionAmbig. Non-Ambig.ALLUnfamiliarALCUNA6845471231ContradictionAmbiTask6006001200LexicalAmbER,AmbiPun8159211,736SemanticAmbiCoref400400800WhatAmbigQA, Dolly1255Whom WhenAmbigQA, Dolly AmbigQA, Dolly762 7793884 in total 7167 in totalWhereAmbigQA, Dolly487
Each task utilizes different evaluation metrics, outlined in the corresponding sections.Test Dataset.Our experiments are conducted on sub-sample of 3600 instances randomly selected,</p>
<p>Table 4 :
4
Overconfidence evaluation on LLMs with and without CoT.Significant differences are marked in grey .
MetricModelZero-shot w/o CoT Zero-shot w/ CoT DifferenceVicuna-13B21.4719.81-1.66Llama2-13B-I22.4319.91-2.52ECE ↓Llama2-13B28.4845.14+16.66Llama2-70B48.2147.24-0.97ChatGPT29.7416.30-13.44Vicuna-13B49.7351.37+1.64Llama2-13B-I56.1856.40+0.22ROC ↑Llama2-13B57.0048.22-8.78Llama2-70B50.7456.33+5.59ChatGPT54.3557.35+3.00Vicuna-13B21.4725.66+4.19Llama2-13B-I22.4320.99-1.44Llama2-13B28.4844.10+15.62Llama2-70B48.2131.68-16.53ChatGPT29.7413.40-16.34Vicuna-13B49.7348.70-1.03Llama2-13B-I56.1856.56+0.38ROC ↑Llama2-13B57.0050.55-6.45Llama2-70B50.7443.84-6.9ChatGPT54.3551.57-2.78
Metric Model Zero-shot w/o CoT Few-shot w/o CoT Difference ECE ↓</p>
<p>27.97 27.72 29.5729.44 Llama2-13B 50.25 33.89 54.25 46.65 56.75 49.11 50.00 33.33 34.73 34.64 36.86 36.85 34.27 34.16 34.17 34.05 Llama2-70B 63.25 58.83 50.75 35.81 55.25 44.04 50.00 33.33 31.04 30.7731.37 31.07 31.37 31.07 31.47 31.16ChatGPT 38.00 28.17 60.00 59.67 58.75 58.06 50.75 49.32 65.40 50.54 68.77 57.48 65.00 45.66 63.10 45.24
Epistemic MisalignmentLinguistic AmbiguityAleatoric OutputMethodscontradictionunfamiliarlexicalsemanticwhatwhomwhenwhereAcc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1Vicuna-13B51.75 37</p>
<p>Table 6 :
6
The fine-grained ambiguity identification evaluation results under Few-shot w/o CoT setting.ChatGPT demonstrates excellent performance across all categories of Aleatoric Output, but it does not effectively address the semantic and contradiction categories.</p>
<p>Table 6
6, all
ChatGPT lags behind other small-scale LLMs on the contradiction category.As shown in Table6, ChatGPT only achieves limited accuracy (i.e.,</p>
<p>Table 7 :
7
Overall ambiguity clarification evaluation of LLMs with varying prompting schemes.ChatGPT emerges as the superior model to other open-sourced LLMs.We report BertScore (i.e., BS) and Help.</p>
<p>Table 8 :
8
The example clarifying questions associated with ambiguous queries in Table1.There are no discerning patterns according to the ambiguity category.</p>
<p>17.88 ChatGPT 39.50 30.10 50.75 36.59 53.50 49.23 54.50 44.75 49.70 46.56 49.95 46.90 52.44 50.15 49.35 46.10</p>
<p>Table 9 :
9
The fine-grained ambiguity identification evaluation results under Zero-shot w/o CoT setting..4152.25 44.79 56.50 51.54 52.25 44.56 37.23 37.00 36.1635.85 36.86 36.6138.16 38.01 Llama2-13B
Epistemic MisalignmentLinguistic AmbiguityAleatoric OutputMethodscontradictionunfamiliarlexicalsemanticwhatwhomwhenwhereAcc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1Vicuna-13B67.75 67</p>
<p>19.35 ChatGPT 42.48 38.97 55.25 55.24 74.00 72.79 54.00 43.26 65.70 53.44 64.35 50.61 64.00 49.93 63.30 48.42</p>
<p>Table 10 :
10
The fine-grained ambiguity identification evaluation results under Zero-shot w/ CoT setting.
Epistemic MisalignmentLinguistic AmbiguityAleatoric OutputMethodscontradictionunfamiliarlexicalsemanticwhatwhomwhenwhere
We omit the syntactic and pragmatic ambiguities as they are not commonly used in information retrieval.
Self-consistency confidence with 4 candidate answers are used to obtain the LLM's uncertainty(Xiong et al., 2023).
Refer to Appendix D for details on human evaluation.
It entails assigning a binary score (0 or 1) to each generated question.
https://huggingface.co/meta-llama/Llama-2-13b-chat-hf
https://huggingface.co/meta-llama/Llama-2-70b-chat-hf
https://huggingface.co/Expert68/Llama2_13b_instructed_ version2
https://huggingface.co/lmsys/vicuna-13b-v1.5
AcknowledgementsThis work was supported in part by the National Natural Science Foundation of China (No. 62272330 and No. 62206191); in part by the Natural Science Foundation of Sichuan (No. 2023NS-FSC0473), and in part by the Fundamental Research Funds for the Central Universities (No. 2023SCU12089 and No. YJ202219).Prompting SchemesZero-shot w/o CoT: Given a query, answer the question or ask a clarifying question.The response should start with "The answer is" or "The clarifying question is".Question: <Question>Zero-shot w/ CoT: Given a query first identify whether the question is ambiguous or not.If it is ambiguous, ask a clarifying question.If it is not ambiguous, answer the question.The response should start with the ambiguity analysis of the question and then follow by "Therefore, the question is not ambiguous.The answer is" or "Therefore, the question is ambiguous.The clarifying question is".Question: <Question>Few-shot w/o CoT: Given a query, answer the question or ask a clarifying question.The response should start with "The answer is" or "The clarifying question is".Question: Who played michael myers in rob zombie's movie?Output: The answer is Tyler Mane.Question: Give me some Mother's Day gift ideas.Output: The clarifying question is What are your mother's interests, hobbies, or something she has been wanting?Question: <Question>Few-shot w/ CoT: Given a query, answer the question or ask a clarifying question.The response should start with "The answer is" or "The clarifying question is".Question: Who played michael myers in rob zombie's movie?Output: In Rob Zombie's "Halloween" films, the role of Michael Myers was primarily played by Tyler Mane.Mane portrayed the iconic horror character in both Rob Zombie's "Halloween" (2007) and its sequel, "Halloween II" (2009).Therefore, the question is not ambiguous.The answer is Tyler Mane.Question: Give me some Mother's Day gift ideas.Output: there could be underlying ambiguities depending on the interests of the specific mother in question, the budget, and the giver's relationship to the mother.Without this context, a broad range of ideas could be considered appropriate.Therefore, the question is ambiguous.The clarifying question is: What are the interests or hobbies of the mother, and is there a particular budget range for the gift?Question: <Question>ALCUNA datasetThe prompt of generating clarifying questions:Given the user question: <question>.Note that the <entity> is a non-existent entity fabricated by existing entities.You need to generate a clarifying question about the <ENTITY> to better know its intended meaning.Your Generated Clarifying Question:Data Examples:An ambiguous example Query: What is the latitude of the habitat of inyidiidae?Clarifying Question: Can you please provide more information about "inyidiidae"?A unambiguous example Query: Is Mozambique a geographic distribution of Mantodea?AmbiCoref datasetThe template of clarifying questions: What does <PRONOUN> refer to? <A> or <B>?Data Examples: An ambiguous example Query: Matthew bought Joshua a pizza after he asked for more food.Who asked for more food?Clarifying Question: What does he refer to?Matthew or Joshua?A unambiguous example Query: Matthew made Joshua a square pizza before he submitted the order.Who submitted the order?AmbiTask datasetThe template of rephrasing ambiguous queries: The all possible word categories are either <category 1> or <category 2>.The following two examples share a specific word category.You need to first infer the specific word category from the examples.Please output "X" if the given sentence mentions the specific word category.Please output "Y" if the given sentence does not mention the word category.AmbER datasetThe prompt of generating ambiguous queries and clarifying questions:AmbiPun datasetThe prompt of generating ambiguous queries and clarifying questions: //1.Generate ambiguous queries Given a polysemy word <WORD>, it has two senses, including of <SENSE1> and <SENSE2>.You need to generate an information-seeking question based on the word <WORD>.You need to make the generated question be ambiguous due to the polysemy of word <WORD>.Note the question needs to contain the word <WORD>.Answering the generated requires a clarifying question to better understand the word <WORD>.generated question: //2.Generate clarifying question Given a question: <QUESTION> Note the polysemy word <WORD> has two senses, including of <SENSE1> and <SENSE2>.The given question has ambiguity due to the polysemy word <WORD>.You need to generate a clarifying question based on the word <WORD> to better clarify the ambiguity of the given question.clarifying question:The prompt of generating unambiguous queries: Given an ambiguous query and its clarifying question, you need to generate a unambiguous query based on them.FORMAT: {"unambiguous query": <STRING>} Data Examples: An ambiguous example Query: What is the meaning of Smart?Clarifying Question: Are you referring to the adjective 'smart' or a specific brand called 'Smart'?A unambiguous example Query: What are the common strategies for saving money?Dolly datasetThe prompt of generating clarifying questions and category classification:Give you an instruction, you first need to judge whether the instruction is ambiguous or not.If you think the instruction is ambiguous and falls into one of the following ambiguous types, you need to output its ambiguous type and the corresponding clarifying questions to help answer the ambiguous instruction.If you think the instruction is not ambiguous and does not miss any specific information, you need to rewrite it and make sure it falls into one of the following ambiguous types.Ambiguous types: 1. Missing personal information.For example, the instruction "Suggest me some good movies" misses the information of the user personal preference.2. Missing spatial information.For example, the instruction "How to reach a destination" misses the spatial information of the departure location.3. Missing temporal information.For example, the instruction "Make a restaurant reservation" misses the temporal information of the reservation time.4. Missing specific task-related information.For example, the instruction "convert string to int" misses the information of the programming language.You should output the ambiguous type, the ambiguous instruction and its corresponding clarifying questions for each instruction.FORMAT: {"ambiguous type": <STRING>, "ambiguous instruction": <STRING>, "clarifying question": <STRING>} Data Examples: An ambiguous example Query: Give me some Mother's Day gift ideas Clarifying Question: What are your mother's interests, hobbies, or something she has been wanting?A unambiguous example Query: Top scorer of uefa champions league of all time?
Orcas-i: Queries annotated with intent using weak supervision. Daria Alexander, Wojciech Kusa, Arjen P De Vries, 10.1145/3477495.3531737Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '22. the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '22New York, NY, USAAssociation for Computing Machinery2022</p>
<p>Building and evaluating open-domain dialogue corpora with clarifying questions. Mohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeffrey Dalton, Mikhail Burtsev, arXiv:2109.057942021arXiv preprint</p>
<p>Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models. Alfonso Amayuelas, Liangming Pan, Wenhu Chen, William Wang, 2023</p>
<p>Ambiguity in requirements specification. M Daniel, Erik Berry, Kamsties, Perspectives on software requirements. Springer2004</p>
<p>Evaluating entity disambiguation and the role of popularity in retrievalbased NLP. Anthony Chen, Pallavi Gudipati, Shayne Longpre, Xiao Ling, Sameer Singh, 10.18653/v1/2021.acl-long.345Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Overview of the trec 2009 web track. L A Charles, Nick Clarke, Ian Craswell, Soboroff, Trec. 20099</p>
<p>Did you mean a or b? supporting clarification dialog for entity disambiguation. Anni Coden, Daniel Gruhl, Neal Lewis, Pablo N Mendes, Sumprehswi@ eswc. 2015</p>
<p>Selectively answering ambiguous questions. Jeremy Cole, Michael Zhang, Daniel Gillick, Julian Eisenschlos, Bhuwan Dhingra, Jacob Eisenstein, 10.18653/v1/2023.emnlp-main.35Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Free dolly: Introducing the world's first truly open instructiontuned llm. Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, Reynold Xin, 2023</p>
<p>Rethinking conversational agents in the era of llms: Proactivity, non-collaborativity, and beyond. Yang Deng, Wenqiang Lei, Minlie Huang, Tat-Seng Chua, Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region. the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region2023a</p>
<p>Prompting and evaluating large language models for proactive dialogues: Clarification, target-guided, and noncollaboration. Yang Deng, Lizi Liao, Liang Chen, Hongru Wang, Wenqiang Lei, Tat-Seng Chua, 10.18653/v1/2023.findings-emnlp.711Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023b</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>Interrogatives: Questions, facts and dialogue. The handbook of contemporary semantic theory. Jonathan Ginzburg, 19965</p>
<p>Abg-coqa: Clarifying ambiguity in conversational question answering. Meiqi Guo, Mingda Zhang, Siva Reddy, Malihe Alikhani, 3rd Conference on Automated Knowledge Base Construction. 2021</p>
<p>Do large language models know about facts?. Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S Yu, Zhijiang Guo, 2023</p>
<p>Reduce human labor on evaluating conversational information retrieval system: A humanmachine collaboration approach. Chen Huang, Peixin Qin, Wenqiang Lei, Jiancheng Lv, 10.18653/v1/2023.emnlp-main.670Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2023Singapore</p>
<p>Hagrid: A humanllm collaborative dataset for generative informationseeking with attribution. Ehsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan Thakur, Jimmy Lin, 2023</p>
<p>How to approach ambiguous queries in conversational search: A survey of techniques, approaches, tools, and challenges. Kimiya Keyvan, Jimmy Xiangji Huang, arXiv:2212.07769ACM Computing Surveys. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar5562022. 2022arXiv preprintClam: Selective clarification for ambiguous questions with generative language models</p>
<p>Clam: Selective clarification for ambiguous questions with generative language models. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, 2023</p>
<p>Natural questions: a benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Transactions of the Association for Computational Linguistics. 72019</p>
<p>Asking clarification questions to handle ambiguity in open-domain qa. Dongryeol Lee, Segwang Kim, Minwoo Lee, Hwanhee Lee, Joonsuk Park, Sang-Woo Lee, Kyomin Jung, 2023</p>
<p>Proactive conversational agents in the post-chatgpt world. Lizi Liao, Grace Hui, Yang , Chirag Shah, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2023</p>
<p>AmbigQA: Answering ambiguous open-domain questions. Sewon Min, Julian Michael, Hannaneh Hajishirzi, Luke Zettlemoyer, 10.18653/v1/2020.emnlp-main.466Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Ambipun: Generating puns with ambiguous context. Anirudh Mittal, Yufei Tian, Nanyun Peng, 2022Association for Computational Linguistics</p>
<p>Introducing chatgpt. 2022OpenAI</p>
<p>Miguel Ortega-Martín, Óscar García-Sierra, Alfonso Ardoiz, Jorge Álvarez, Juan Carlos Armenteros, Adrián Alonso, arXiv:2302.06426Linguistic ambiguity analysis in chatgpt. 2023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Kwaiagents: Generalized information-seeking agent system with large language models. Haojie Pan, Zepeng Zhai, Hao Yuan, Yaojia Lv, Ruiji Fu, Ming Liu, Zhongyuan Wang, Bing Qin, arXiv:2312.048892023arXiv preprint</p>
<p>The impact of technological support on groups: An assessment of the empirical research. Alain Pinsonneault, Kenneth L Kraemer, Decision Support Systems. 521989</p>
<p>A survey on asking clarification questions datasets in conversational systems. A Hossein, Xi Rahmani, Yue Wang, Qiang Feng, Emine Zhang, Aldo Yilmaz, Lipani, 10.18653/v1/2023.acl-long.152Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada20231Association for Computational Linguistics</p>
<p>Identifying ambiguous queries in web search. Ruihua Song, Zhenxiao Luo, Ji-Rong Wen, Yong Yu, Hsiao-Wuen Hon, Proceedings of the 16th international conference on World Wide Web. the 16th international conference on World Wide Web2007</p>
<p>Alex Tamkin, Kunal Handa, Avash Shrestha, Noah Goodman, Task ambiguity in humans and language models. 2022</p>
<p>Identifying unclear questions in community question answering websites. Jan Trienes, Krisztian Balog, Advances in Information Retrieval: 41st European Conference on IR Research, ECIR 2019. Cologne, GermanySpringer2019. April 14-18, 2019Proceedings, Part I 41</p>
<p>Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, Yulia Tsvetkov, arXiv:2310.00935Resolving knowledge conflicts in large language models. 2023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, arXiv:2306.130632023arXiv preprint</p>
<p>Asking clarification questions in knowledgebased question answering. Jingjing Xu, Yuechen Wang, Duyu Tang, Nan Duan, Pengcheng Yang, Qi Zeng, Ming Zhou, Xu Sun, Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing. the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processingEMNLP-IJCNLP2019</p>
<p>Xunjian Yin, Baizhou Huang, Xiaojun Wan, arXiv:2310.14820Alcuna: Large language models meet new knowledge. 2023arXiv preprint</p>
<p>Ambicoref: Evaluating human and model sensitivity to ambiguous coreference. Yuewei Yuan, Chaitanya Malaviya, Mark Yatskar, 2023</p>
<p>Generating clarifying questions for information retrieval. Hamed Zamani, Susan Dumais, Nick Craswell, Paul Bennett, Gord Lueck, Proceedings of the web conference 2020. the web conference 20202020</p>
<p>Acc, Acc, 13B 50.00 33.33 49.50 38.37 51.50 38.48 50.50 35.68 22.46 20.42 22.58 20.49 22.58 20.49 22.18 20.18F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Vicuna. </p>
<p>Table 11: The fine-grained ambiguity identification evaluation results under Few-shot w/o CoT setting. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8756 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8756</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8756</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-267897795</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.14963v2.pdf" target="_blank">Mirror : A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback. Therefore, We propose Mirror , a M ult i ple-pe r spective self-r eflection method for kn o wledge-rich r easoning, to avoid getting stuck at a particular reflection iteration. Mirror enables LLMs to reflect from multiple-perspective clues, achieved through a heuristic interaction between a Navigator and a Reasoner. It guides agents toward diverse yet plausibly reliable reasoning trajectory without access to ground truth by encouraging (1) diversity of directions generated by Navigator and (2) agreement among strategically induced perturbations in responses generated by the Reasoner. The experiments on five reasoning datasets demonstrate that Mirror ’s superiority over several contemporary self-reflection approaches. Ad-ditionally, the ablation study studies clearly indicate that our strategies alleviate the afore-mentioned challenges. The code is released at https://github.com/hanqi-qi/Mirror.git.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8756.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8756.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mirror</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mirror: A Multiple-perspective Self-Reflection Method</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised multi-perspective self-reflection framework that uses a Navigator to generate question-adaptive directions and a Reasoner to produce answers; search is guided by a diversity+consistency reward via Monte-Carlo Tree Search to avoid reflection traps without access to ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 (175B) — used as a black-box LLM for prompting; highest-performing model in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Mirror (Multiple-perspective self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Navigator generates K question-oriented directions (diverse perspectives); Reasoner produces responses guided by those directions. A tree search (MCTS / UCT) explores direction-response trajectories and rewards both diversity of directions and inter-/intra-consistency among responses; final answer selected by consistency threshold or highest consistency score.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (STEM, Social, Humanity, Other) and FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>MMLU: multi-subject multiple-choice knowledge-rich QA; FEVER: fact verification (supports/refutes/not-enough-info).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>MMLU: STEM=0.76, Social=0.77, Humanity=0.71, Other=0.67; FEVER=0.64 (accuracy fractions as reported in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>CoT (1): STEM=0.63, Social=0.65, Humanity=0.53, Other=0.60; FEVER=0.58 (baseline Chain-of-Thought single sample)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered Navigator+Reasoner interaction; Monte-Carlo Tree Search (UCT) with an intrinsic reward combining diversity (encouraging different outcomes across iterations) and consistency (agreement among perturbation-guided responses). Intra-consistency (self-consistency) is fed back to Navigator prompts. Early-stop when inter-consistency exceeds threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative improvements vs baselines: Mirror yields large gains over CoT and Self-Consistency; for GPT-3.5 Mirror shows ~+0.09 absolute vs CoT(1) on averaged MMLU/FEVER domains and is reported as >15% relative improvement over unsupervised baselines. Ablations show diversity and question-oriented directions contribute to the gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires tree search (computational cost) and relies on frozen decoding / pretraining of the underlying LLM; smaller models obtain smaller gains and require lower intra-consistency thresholds. The paper notes Mirror is limited by the unaltered decoding process and pretraining and that excessive diversity could collect less relevant states (mitigated by filtering).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against CoT, Self-Consistency (5), Self-Correction (2), and Reflexion(w.GT). Mirror outperforms CoT and self-consistency baselines and is comparable to Reflexion(w.GT) for some settings (GPT-3.5 on STEM), while outperforming Self-Correction which sometimes degrades after iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations show (1) question-oriented (GenerativeDirect) directions outperform fixed generic directions (FixedDirect); (2) increasing action-space size (more directions) and the diversity reward increases probability that the correct answer is present in the search tree (ans-presence) and improves accuracy; (3) reward-based search + consistency assessment outperforms naive majority-vote on tree nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mirror : A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8756.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8756.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mirror</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mirror: A Multiple-perspective Self-Reflection Method</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same Mirror method evaluated with a smaller open model to measure effect of model scale on iterative self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama2-13B-chat (approx. 13B) — an open model used for prompting and evaluation; smaller than GPT-3.5 and shows lower self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Mirror (Multiple-perspective self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>See Mirror above; in practice the intra-consistency threshold T0 is reduced for smaller models (paper sets T0=0.5 for Llama/Vicuna vs 0.8 for GPT-3.5). Tree max depth used is 3 with up to 5 branches per layer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (STEM, Social, Humanity, Other) and FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>MMLU: STEM=0.57, Social=0.62, Humanity=0.58, Other=0.62; FEVER=0.54 (accuracy fractions as reported in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>CoT (1): STEM=0.42, Social=0.58, Humanity=0.42, Other=0.53; FEVER=0.40</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Navigator+Reasoner via prompts; MCTS (UCT) guided by diversity and consistency rewards; intra-consistency value included in prompts to Navigator. Early-stop on inter-consistency threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mirror shows notable absolute gains over CoT(1) and Self-Consistency(5) for Llama2-13B (e.g., STEM from 0.42 to 0.57). Paper reports improvements across most domains though gains are smaller than for GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller models are less consistent and thus need lower thresholds; the paper reports marginal improvements when ground-truth is accessible for smaller models and attributes this to smaller models' poorer adherence to directions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mirror outperforms CoT and Self-Consistency baselines for Llama2-13B. Reflexion(w.GT) (with ground truth) sometimes produces comparable results; Self-Correction often degrades performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Same qualitative ablations as for GPT-3.5: question-adaptive directions and diversity term improve performance; FixedDirect can degrade results for Llama13B.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mirror : A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8756.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8756.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mirror</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mirror: A Multiple-perspective Self-Reflection Method</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mirror evaluated with an instruction-tuned smaller model (Vicuna) to test generality across models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-v1.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vicuna-v1.5-7B — an instruction-tuned 7B model (referred to as Vicuna13B/Vicuna in places); smaller scale evaluation model.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Mirror (Multiple-perspective self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same Mirror method; intra-consistency threshold set lower for smaller models; diversity and consistency rewards used in MCTS tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (STEM, Social, Humanity, Other) and FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>MMLU: STEM=0.59, Social=0.64, Humanity=0.56, Other=0.65; FEVER=0.46 (accuracy fractions as reported in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>CoT (1): STEM=0.46, Social=0.57, Humanity=0.43, Other=0.57; FEVER=0.39</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Navigator+Reasoner prompts; MCTS with diversity+consistency intrinsic rewards; thresholds tuned lower for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mirror yields improvements over CoT(1) and Self-Consistency(5) for Vicuna (e.g., STEM from 0.46 to 0.59). Improvements are present but smaller than with larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller model's lower intra-consistency reduces the reliability of consistency-based stopping; less adherence to generated directions limits gains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mirror outperforms CoT and Self-Consistency baselines; compared to Reflexion(w.GT) Mirror sometimes underperforms on some domains for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Same ablation findings: generative question-specific directions and search diversity improve performance; FixedDirect can hurt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mirror : A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8756.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8756.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (ensemble over chain-of-thought samples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised method that samples multiple chain-of-thought outputs and selects the most consistent final answer (majority or frequency-based), improving reasoning robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 (175B) used with chain-of-thought prompting and multiple (n=5 or n=15) sampled outputs to compute answer consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (sample aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple chain-of-thought answers (e.g., 5 samples) and pick the most frequent final answer as the output (intra-consistency measure).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (STEM, Social, Humanity, Other) and FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Self-Consistency (5): STEM=0.67, Social=0.68, Humanity=0.58, Other=0.64; FEVER=0.61 (GPT-3.5, Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>CoT (1): STEM=0.63, Social=0.65, Humanity=0.53, Other=0.60; FEVER=0.58</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering to elicit chain-of-thought, sample multiple outputs (temperature, sampling), aggregation via majority/frequency to choose answer.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Self-Consistency(5) improves over CoT(1) across all reported domains for GPT-3.5 (e.g., overall absolute gains ~0.03–0.05), and self-consistency confidence correlates strongly with accuracy (R^2 ≈ 0.85 reported).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-consistency benefits plateau with more samples and may not fully solve self-assessment failures; majority voting across poorly-weighted nodes (in tree trajectories) can be hurt by low-quality nodes; self-consistency is used as a proxy but can still fail for knowledge-rich claims.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms CoT(1) but Mirror's multi-perspective tree-level inter-consistency and diversity reward further improve over simple self-consistency aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>The paper reports that self-consistency(15) (i.e., larger candidate pool) shows gains over self-consistency(5), but Mirror's reward-based selection outperforms naive majority voting on the generated tree.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mirror : A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8756.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8756.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Correction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction (LLM internal self-assessment and update)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-improvement approach where the LLM assesses its own previous answer and attempts to correct it based on internal evaluation (no ground truth).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 (175B) prompted to check its previous answer and update it if judged incorrect; experiments used 2 rounds in the paper (Self-Correct (2)).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Correction (internal critique then update)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt the model to assess its prior response (e.g., 'Please check your answer and find possible errors') and produce a revised answer if needed; relies on LLM's internal assessment without external verification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU and FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Self-Correct (2): GPT-3.5 reported accuracies STEM=0.63, Social=0.62, Humanity=0.55, Other=0.54; FEVER=0.55 (Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Initial CoT (1) baseline: STEM=0.63, Social=0.65, Humanity=0.53, Other=0.60; FEVER=0.58</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering (explicit negative feedback inserted) instructing the LLM to reconsider and produce a new answer; uses the LLM's own internal knowledge for assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical results show Self-Correction often does not improve and can degrade performance after iterations; paper reports performance degradation after two rounds across almost all datasets and models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLMs often fail to adhere to revision instructions or become stuck in reasoning loops; many incorrectly predicted samples remain unchanged despite explicit negative feedback (see Figure 2). Self-Correction relies on unreliable self-assessment without external signals.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Performs worse than Self-Consistency and Mirror in many settings; Mirror's direction-guided reflection and diversity/consistency rewards avoid the groundless answer-switching seen in Self-Correction.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Paper's analysis shows explicit prompting placement affects whether the model updates; strategies (NegPrefix) that place negative feedback nearer to generation point increase changed-sample rates but Self-Correction overall underperforms Mirror.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mirror : A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8756.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8756.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion (w.GT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (with ground truth guidance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-reflection method where the agent compares its answer with ground truth for evaluation and uses that signal to improve; used as an upper-bound reference in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 (175B), evaluated with Reflexion when the method has access to ground-truth feedback (denoted Reflexion(w.GT) in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion (ground-truth-assisted reflect-and-revise)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate reflection(s) and update answers using explicit comparisons between current outputs and ground-truth labels, effectively giving direct supervised feedback to the agent's verbal RL loop.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU and FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reflexion(w.GT) (5): GPT-3.5 reported accuracies STEM=0.79, Social=0.84, Humanity=0.78, Other=0.73; FEVER=0.72 (Table 2) — used as an upper-bound since ground truth is available.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>CoT (1) baseline: STEM=0.63, Social=0.65, Humanity=0.53, Other=0.60; FEVER=0.58</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompted iterative reflection where the agent receives direct feedback from ground truth and adjusts answers accordingly (external supervised feedback instead of unsupervised self-assessment).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Reflexion(w.GT) produces higher accuracy than unsupervised methods (CoT, Self-Consistency) and is comparable to Mirror in some GPT-3.5 settings; acts as an upper bound showing how much ground-truth feedback can help.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires access to ground truth during reflection, which is unrealistic in many unsupervised settings; not directly comparable as it leaks true labels into the reflection loop.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Reflexion(w.GT) often outperforms unsupervised methods but Mirror (unsupervised) achieves comparable results on some domains without using ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Not applicable in this paper (used as comparative upper-bound reference).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mirror : A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8756.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8756.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Negative-feedback update experiment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experiment measuring LLM willingness to revise after explicit negative feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical analysis showing that LLMs frequently fail to revise incorrect answers even after being told 'The answer is incorrect.'; reports per-model revision rates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, Llama, Vicuna (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various LLMs of different scales used to measure how often explicit negative feedback triggers a change in erroneous answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Explicit negative feedback prompting (Autostop / NeverStop experiment variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt strings such as 'The answer is incorrect.' or 'You failed in your previous trial; reconsider a new answer.' are inserted and the model is asked to reflect and possibly revise.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (multiple domains)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same knowledge-rich multiple-choice tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported percentage of incorrectly predicted examples that were successfully updated following explicit negative feedback: GPT-3.5: 65.6% updated; Llama: 51.79% updated; Vicuna: 74.09% updated (averaged across 5 iterations; Figure 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>N/A (this experiment measured update rates following targeted prompts vs baseline unchanged behavior); the paper also notes many incorrect predictions remain unchanged without any prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering with explicit negative-feedback statements placed at different positions (observed positional sensitivity); measured whether LLM changes answer in subsequent trial.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Shows that explicit negative feedback leads to only partial update rates and that there is large room for improving feedback quality and prompting strategies; positional strategies (NegPrefix) increase change rates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Large fraction of incorrect predictions remain unchanged even after explicit negative feedback, indicating LLMs can get 'stuck' in reasoning loops; positional sensitivity of prompts affects effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>This experiment motivated Mirror's use of question-oriented directions and diversity rewards because generic negative feedback is insufficient. NegPrefix (placing instruction nearer to generation point) showed the most improvement among the simple prompting variants tested.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Paper reports that moving negative-feedback instructions closer to the generation point (NegPrefix) increases the percentage of changed samples significantly relative to other placements (Table A2/A3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mirror : A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
                <li>Check your facts and try again: Improving large language models with external knowledge and automated feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8756",
    "paper_id": "paper-267897795",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Mirror",
            "name_full": "Mirror: A Multiple-perspective Self-Reflection Method",
            "brief_description": "An unsupervised multi-perspective self-reflection framework that uses a Navigator to generate question-adaptive directions and a Reasoner to produce answers; search is guided by a diversity+consistency reward via Monte-Carlo Tree Search to avoid reflection traps without access to ground truth.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "GPT-3.5 (175B) — used as a black-box LLM for prompting; highest-performing model in the paper's experiments.",
            "reflection_method_name": "Mirror (Multiple-perspective self-reflection)",
            "reflection_method_description": "Navigator generates K question-oriented directions (diverse perspectives); Reasoner produces responses guided by those directions. A tree search (MCTS / UCT) explores direction-response trajectories and rewards both diversity of directions and inter-/intra-consistency among responses; final answer selected by consistency threshold or highest consistency score.",
            "task_name": "MMLU (STEM, Social, Humanity, Other) and FEVER",
            "task_description": "MMLU: multi-subject multiple-choice knowledge-rich QA; FEVER: fact verification (supports/refutes/not-enough-info).",
            "performance_with_reflection": "MMLU: STEM=0.76, Social=0.77, Humanity=0.71, Other=0.67; FEVER=0.64 (accuracy fractions as reported in Table 2)",
            "performance_without_reflection": "CoT (1): STEM=0.63, Social=0.65, Humanity=0.53, Other=0.60; FEVER=0.58 (baseline Chain-of-Thought single sample)",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered Navigator+Reasoner interaction; Monte-Carlo Tree Search (UCT) with an intrinsic reward combining diversity (encouraging different outcomes across iterations) and consistency (agreement among perturbation-guided responses). Intra-consistency (self-consistency) is fed back to Navigator prompts. Early-stop when inter-consistency exceeds threshold.",
            "number_of_iterations": 3,
            "evidence_for_improvement": "Quantitative improvements vs baselines: Mirror yields large gains over CoT and Self-Consistency; for GPT-3.5 Mirror shows ~+0.09 absolute vs CoT(1) on averaged MMLU/FEVER domains and is reported as &gt;15% relative improvement over unsupervised baselines. Ablations show diversity and question-oriented directions contribute to the gains.",
            "limitations_or_failure_cases": "Requires tree search (computational cost) and relies on frozen decoding / pretraining of the underlying LLM; smaller models obtain smaller gains and require lower intra-consistency thresholds. The paper notes Mirror is limited by the unaltered decoding process and pretraining and that excessive diversity could collect less relevant states (mitigated by filtering).",
            "comparison_to_other_methods": "Compared against CoT, Self-Consistency (5), Self-Correction (2), and Reflexion(w.GT). Mirror outperforms CoT and self-consistency baselines and is comparable to Reflexion(w.GT) for some settings (GPT-3.5 on STEM), while outperforming Self-Correction which sometimes degrades after iterations.",
            "ablation_study_results": "Ablations show (1) question-oriented (GenerativeDirect) directions outperform fixed generic directions (FixedDirect); (2) increasing action-space size (more directions) and the diversity reward increases probability that the correct answer is present in the search tree (ans-presence) and improves accuracy; (3) reward-based search + consistency assessment outperforms naive majority-vote on tree nodes.",
            "uuid": "e8756.0",
            "source_info": {
                "paper_title": "Mirror : A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mirror",
            "name_full": "Mirror: A Multiple-perspective Self-Reflection Method",
            "brief_description": "Same Mirror method evaluated with a smaller open model to measure effect of model scale on iterative self-reflection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-13B-Chat",
            "model_description": "Llama2-13B-chat (approx. 13B) — an open model used for prompting and evaluation; smaller than GPT-3.5 and shows lower self-consistency.",
            "reflection_method_name": "Mirror (Multiple-perspective self-reflection)",
            "reflection_method_description": "See Mirror above; in practice the intra-consistency threshold T0 is reduced for smaller models (paper sets T0=0.5 for Llama/Vicuna vs 0.8 for GPT-3.5). Tree max depth used is 3 with up to 5 branches per layer.",
            "task_name": "MMLU (STEM, Social, Humanity, Other) and FEVER",
            "task_description": "Same as above.",
            "performance_with_reflection": "MMLU: STEM=0.57, Social=0.62, Humanity=0.58, Other=0.62; FEVER=0.54 (accuracy fractions as reported in Table 2)",
            "performance_without_reflection": "CoT (1): STEM=0.42, Social=0.58, Humanity=0.42, Other=0.53; FEVER=0.40",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Navigator+Reasoner via prompts; MCTS (UCT) guided by diversity and consistency rewards; intra-consistency value included in prompts to Navigator. Early-stop on inter-consistency threshold.",
            "number_of_iterations": 3,
            "evidence_for_improvement": "Mirror shows notable absolute gains over CoT(1) and Self-Consistency(5) for Llama2-13B (e.g., STEM from 0.42 to 0.57). Paper reports improvements across most domains though gains are smaller than for GPT-3.5.",
            "limitations_or_failure_cases": "Smaller models are less consistent and thus need lower thresholds; the paper reports marginal improvements when ground-truth is accessible for smaller models and attributes this to smaller models' poorer adherence to directions.",
            "comparison_to_other_methods": "Mirror outperforms CoT and Self-Consistency baselines for Llama2-13B. Reflexion(w.GT) (with ground truth) sometimes produces comparable results; Self-Correction often degrades performance.",
            "ablation_study_results": "Same qualitative ablations as for GPT-3.5: question-adaptive directions and diversity term improve performance; FixedDirect can degrade results for Llama13B.",
            "uuid": "e8756.1",
            "source_info": {
                "paper_title": "Mirror : A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mirror",
            "name_full": "Mirror: A Multiple-perspective Self-Reflection Method",
            "brief_description": "Mirror evaluated with an instruction-tuned smaller model (Vicuna) to test generality across models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-v1.5-7B",
            "model_description": "Vicuna-v1.5-7B — an instruction-tuned 7B model (referred to as Vicuna13B/Vicuna in places); smaller scale evaluation model.",
            "reflection_method_name": "Mirror (Multiple-perspective self-reflection)",
            "reflection_method_description": "Same Mirror method; intra-consistency threshold set lower for smaller models; diversity and consistency rewards used in MCTS tree search.",
            "task_name": "MMLU (STEM, Social, Humanity, Other) and FEVER",
            "task_description": "Same as above.",
            "performance_with_reflection": "MMLU: STEM=0.59, Social=0.64, Humanity=0.56, Other=0.65; FEVER=0.46 (accuracy fractions as reported in Table 2)",
            "performance_without_reflection": "CoT (1): STEM=0.46, Social=0.57, Humanity=0.43, Other=0.57; FEVER=0.39",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Navigator+Reasoner prompts; MCTS with diversity+consistency intrinsic rewards; thresholds tuned lower for smaller models.",
            "number_of_iterations": 3,
            "evidence_for_improvement": "Mirror yields improvements over CoT(1) and Self-Consistency(5) for Vicuna (e.g., STEM from 0.46 to 0.59). Improvements are present but smaller than with larger models.",
            "limitations_or_failure_cases": "Smaller model's lower intra-consistency reduces the reliability of consistency-based stopping; less adherence to generated directions limits gains.",
            "comparison_to_other_methods": "Mirror outperforms CoT and Self-Consistency baselines; compared to Reflexion(w.GT) Mirror sometimes underperforms on some domains for smaller models.",
            "ablation_study_results": "Same ablation findings: generative question-specific directions and search diversity improve performance; FixedDirect can hurt performance.",
            "uuid": "e8756.2",
            "source_info": {
                "paper_title": "Mirror : A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (ensemble over chain-of-thought samples)",
            "brief_description": "An unsupervised method that samples multiple chain-of-thought outputs and selects the most consistent final answer (majority or frequency-based), improving reasoning robustness.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "GPT-3.5 (175B) used with chain-of-thought prompting and multiple (n=5 or n=15) sampled outputs to compute answer consistency.",
            "reflection_method_name": "Self-Consistency (sample aggregation)",
            "reflection_method_description": "Generate multiple chain-of-thought answers (e.g., 5 samples) and pick the most frequent final answer as the output (intra-consistency measure).",
            "task_name": "MMLU (STEM, Social, Humanity, Other) and FEVER",
            "task_description": "Same benchmarks as above.",
            "performance_with_reflection": "Self-Consistency (5): STEM=0.67, Social=0.68, Humanity=0.58, Other=0.64; FEVER=0.61 (GPT-3.5, Table 2)",
            "performance_without_reflection": "CoT (1): STEM=0.63, Social=0.65, Humanity=0.53, Other=0.60; FEVER=0.58",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering to elicit chain-of-thought, sample multiple outputs (temperature, sampling), aggregation via majority/frequency to choose answer.",
            "number_of_iterations": 5,
            "evidence_for_improvement": "Self-Consistency(5) improves over CoT(1) across all reported domains for GPT-3.5 (e.g., overall absolute gains ~0.03–0.05), and self-consistency confidence correlates strongly with accuracy (R^2 ≈ 0.85 reported).",
            "limitations_or_failure_cases": "Self-consistency benefits plateau with more samples and may not fully solve self-assessment failures; majority voting across poorly-weighted nodes (in tree trajectories) can be hurt by low-quality nodes; self-consistency is used as a proxy but can still fail for knowledge-rich claims.",
            "comparison_to_other_methods": "Outperforms CoT(1) but Mirror's multi-perspective tree-level inter-consistency and diversity reward further improve over simple self-consistency aggregation.",
            "ablation_study_results": "The paper reports that self-consistency(15) (i.e., larger candidate pool) shows gains over self-consistency(5), but Mirror's reward-based selection outperforms naive majority voting on the generated tree.",
            "uuid": "e8756.3",
            "source_info": {
                "paper_title": "Mirror : A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Self-Correction",
            "name_full": "Self-Correction (LLM internal self-assessment and update)",
            "brief_description": "A self-improvement approach where the LLM assesses its own previous answer and attempts to correct it based on internal evaluation (no ground truth).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "GPT-3.5 (175B) prompted to check its previous answer and update it if judged incorrect; experiments used 2 rounds in the paper (Self-Correct (2)).",
            "reflection_method_name": "Self-Correction (internal critique then update)",
            "reflection_method_description": "Prompt the model to assess its prior response (e.g., 'Please check your answer and find possible errors') and produce a revised answer if needed; relies on LLM's internal assessment without external verification.",
            "task_name": "MMLU and FEVER",
            "task_description": "Same benchmarks.",
            "performance_with_reflection": "Self-Correct (2): GPT-3.5 reported accuracies STEM=0.63, Social=0.62, Humanity=0.55, Other=0.54; FEVER=0.55 (Table 2)",
            "performance_without_reflection": "Initial CoT (1) baseline: STEM=0.63, Social=0.65, Humanity=0.53, Other=0.60; FEVER=0.58",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering (explicit negative feedback inserted) instructing the LLM to reconsider and produce a new answer; uses the LLM's own internal knowledge for assessment.",
            "number_of_iterations": 2,
            "evidence_for_improvement": "Empirical results show Self-Correction often does not improve and can degrade performance after iterations; paper reports performance degradation after two rounds across almost all datasets and models.",
            "limitations_or_failure_cases": "LLMs often fail to adhere to revision instructions or become stuck in reasoning loops; many incorrectly predicted samples remain unchanged despite explicit negative feedback (see Figure 2). Self-Correction relies on unreliable self-assessment without external signals.",
            "comparison_to_other_methods": "Performs worse than Self-Consistency and Mirror in many settings; Mirror's direction-guided reflection and diversity/consistency rewards avoid the groundless answer-switching seen in Self-Correction.",
            "ablation_study_results": "Paper's analysis shows explicit prompting placement affects whether the model updates; strategies (NegPrefix) that place negative feedback nearer to generation point increase changed-sample rates but Self-Correction overall underperforms Mirror.",
            "uuid": "e8756.4",
            "source_info": {
                "paper_title": "Mirror : A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Reflexion (w.GT)",
            "name_full": "Reflexion (with ground truth guidance)",
            "brief_description": "A self-reflection method where the agent compares its answer with ground truth for evaluation and uses that signal to improve; used as an upper-bound reference in experiments.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "GPT-3.5 (175B), evaluated with Reflexion when the method has access to ground-truth feedback (denoted Reflexion(w.GT) in experiments).",
            "reflection_method_name": "Reflexion (ground-truth-assisted reflect-and-revise)",
            "reflection_method_description": "Generate reflection(s) and update answers using explicit comparisons between current outputs and ground-truth labels, effectively giving direct supervised feedback to the agent's verbal RL loop.",
            "task_name": "MMLU and FEVER",
            "task_description": "Same benchmarks.",
            "performance_with_reflection": "Reflexion(w.GT) (5): GPT-3.5 reported accuracies STEM=0.79, Social=0.84, Humanity=0.78, Other=0.73; FEVER=0.72 (Table 2) — used as an upper-bound since ground truth is available.",
            "performance_without_reflection": "CoT (1) baseline: STEM=0.63, Social=0.65, Humanity=0.53, Other=0.60; FEVER=0.58",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompted iterative reflection where the agent receives direct feedback from ground truth and adjusts answers accordingly (external supervised feedback instead of unsupervised self-assessment).",
            "number_of_iterations": 5,
            "evidence_for_improvement": "Reflexion(w.GT) produces higher accuracy than unsupervised methods (CoT, Self-Consistency) and is comparable to Mirror in some GPT-3.5 settings; acts as an upper bound showing how much ground-truth feedback can help.",
            "limitations_or_failure_cases": "Requires access to ground truth during reflection, which is unrealistic in many unsupervised settings; not directly comparable as it leaks true labels into the reflection loop.",
            "comparison_to_other_methods": "Reflexion(w.GT) often outperforms unsupervised methods but Mirror (unsupervised) achieves comparable results on some domains without using ground truth.",
            "ablation_study_results": "Not applicable in this paper (used as comparative upper-bound reference).",
            "uuid": "e8756.5",
            "source_info": {
                "paper_title": "Mirror : A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Negative-feedback update experiment",
            "name_full": "Experiment measuring LLM willingness to revise after explicit negative feedback",
            "brief_description": "Empirical analysis showing that LLMs frequently fail to revise incorrect answers even after being told 'The answer is incorrect.'; reports per-model revision rates.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5, Llama, Vicuna (aggregate)",
            "model_description": "Various LLMs of different scales used to measure how often explicit negative feedback triggers a change in erroneous answers.",
            "reflection_method_name": "Explicit negative feedback prompting (Autostop / NeverStop experiment variants)",
            "reflection_method_description": "Prompt strings such as 'The answer is incorrect.' or 'You failed in your previous trial; reconsider a new answer.' are inserted and the model is asked to reflect and possibly revise.",
            "task_name": "MMLU (multiple domains)",
            "task_description": "Same knowledge-rich multiple-choice tasks.",
            "performance_with_reflection": "Reported percentage of incorrectly predicted examples that were successfully updated following explicit negative feedback: GPT-3.5: 65.6% updated; Llama: 51.79% updated; Vicuna: 74.09% updated (averaged across 5 iterations; Figure 2).",
            "performance_without_reflection": "N/A (this experiment measured update rates following targeted prompts vs baseline unchanged behavior); the paper also notes many incorrect predictions remain unchanged without any prompt.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering with explicit negative-feedback statements placed at different positions (observed positional sensitivity); measured whether LLM changes answer in subsequent trial.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Shows that explicit negative feedback leads to only partial update rates and that there is large room for improving feedback quality and prompting strategies; positional strategies (NegPrefix) increase change rates.",
            "limitations_or_failure_cases": "Large fraction of incorrect predictions remain unchanged even after explicit negative feedback, indicating LLMs can get 'stuck' in reasoning loops; positional sensitivity of prompts affects effectiveness.",
            "comparison_to_other_methods": "This experiment motivated Mirror's use of question-oriented directions and diversity rewards because generic negative feedback is insufficient. NegPrefix (placing instruction nearer to generation point) showed the most improvement among the simple prompting variants tested.",
            "ablation_study_results": "Paper reports that moving negative-feedback instructions closer to the generation point (NegPrefix) increases the percentage of changed samples significantly relative to other placements (Table A2/A3).",
            "uuid": "e8756.6",
            "source_info": {
                "paper_title": "Mirror : A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
            "rating": 1,
            "sanitized_title": "check_your_facts_and_try_again_improving_large_language_models_with_external_knowledge_and_automated_feedback"
        }
    ],
    "cost": 0.018235,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning
24 Jun 2024</p>
<p>Hanqi Yan 
King's College London</p>
<p>Qinglin Zhu 
King's College London</p>
<p>Xinyu Wang 
King's College London</p>
<p>University of Warwick</p>
<p>Lin Gui 
King's College London</p>
<p>Yulan He yulan.he@kcl.ac.ukxinyu.wang.11@warwick.ac.uk 
King's College London</p>
<p>University of Warwick</p>
<p>The Alan Turing Institute</p>
<p>Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning
24 Jun 202422198BF5DACC87D2E2539E264899E1A9arXiv:2402.14963v2[cs.CL]
While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources.In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback.Therefore, We propose Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, to avoid getting stuck at a particular reflection iteration.Mirror enables LLMs to reflect from multipleperspective clues, achieved through a heuristic interaction between a Navigator and a Reasoner.It guides agents toward diverse yet plausibly reliable reasoning trajectory without access to ground truth by encouraging (1) diversity of directions generated by Navigator and (2) agreement among strategically induced perturbations in responses generated by the Reasoner.The experiments on five reasoning datasets demonstrate that Mirror's superiority over several contemporary self-reflection approaches.Additionally, the ablation study studies clearly indicate that our strategies alleviate the aforementioned challenges.The code is released at https://github.com/hanqi-qi/Mirror.git.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have become an important and flexible building block in a variety of tasks.They can be further improved by iterative correction in many tasks (Madaan et al., 2023;Gou et al., 2023a;Shinn et al., 2023;Pan et al., 2023), such as code generation, arithmetic problem solving and reasoning.During iterative refinement, the critic module, which assesses the current response and generates valuable feedback, is crucial to drive performance improvement.Some research shows that LLMs have selfassessment abilities (Manakul et al., 2023;Madaan et al., 2023).For example, LLMs can reject its own prediction and generate a response 'I don't know' when they are not confident about their predictions (Kadavath et al., 2022).Empirical observations demonstrate LLMs' competence in various reasoning tasks, leading to the utilization of advanced LLMs to evaluate the predictions made by other models (Hao et al., 2023;Zhou et al., 2023;Liu et al., 2023b).However, recent studies suggest that relying directly on LLMs' judgements is not trustworthy and can lead to failures in knowledgerich iterative reasoning (Huang et al., 2023).To guide LLMs through a reasoning loop, existing solutions either incorporate external resources to verify LLMs' outputs (Peng et al., 2023;Yao et al., 2023b), or train a critic module on labelled assessment datasets (Gou et al., 2023a;Zelikman et al., 2022).Furthermore, self-consistency is considered a robust unsupervised method to identify confident and reliable LLM outputs.</p>
<p>In self-refinement, the quality of generated feedback also plays a pivotal role.The Self-Refine method (Madaan et al., 2023) introduced taskspecific metrics for multifaceted feedback generation, requiring LLMs to evaluate their outputs across various aspects, such as fluency, engagement, and relevance for the dialogue generation task.This process often heavily relies on human expertise, and generating effective feedback for reasoning tasks can be even more difficult as it is obscure to define the essential attributes for different problems.Providing overly general feedback fails to guide LLMs toward generating better outputs in subsequent iterations.</p>
<p>The inefficiency of self-assessment and feedback generation capabilities largely hinders the performance of iterative refinements.On one hand, as depicted in Figure 1, it is evident that in the absence of a ground truth reference, LLMs fail to consistently improve their predictions, indicating their limitations in self-assessment 1 .On the other hand, even when ground truth labels are available, LLMs often fail to adhere to instructions for revising their incorrect predictions, as shown in Figure 2.Each bar represents the number (averaged over 5 iterations) of revised (blue) and unchanged samples (grey) among the incorrectly predicted samples.It is undesirable to see that a large number of incorrect predictions stay unchanged, suggesting that LLMs can become trapped in a reasoning loop.</p>
<p>To address the aforementioned limitations and generate high-quality feedback without relying on human experts, we propose a novel framework, refer to as Mirror (Multiple-perspective selfreflection method for knowledge-rich reasoning).Mirror enables LLMs to reflect from multipleperspective clues and this is achieved in a heuristic manner between a Navigator and a Reasoner, resembling a typical human tutoring process.For example, when tackling a complex scientific problem, the Navigator generates clues of key elements and rationales behind posing the question, which are crucial in focusing the response on the essential aspects.This information, tailored to the question, serve as instructions for prompting the Reasoner to adjust their predictions accordingly and avoid getting stuck at a particular stage.</p>
<p>To initiate the unsupervised self-reflection properly and avoid being trapped in one reasoning loop, Mirror integrates an intrinsically motivated planning algorithm to search for the optimal reasoning trajectory.Inspired by the findings in §3.1 and §3.2, we propose to reward both the diversity of generated directions and the agreement among strategically induced perturbations in responses.Notably differing from existing tree-based planning methods for reasoning (Hao et al., 2023 et al., 2023), Mirror avoids deteriorated searching space by encouraging diverse generative outcomes from LLMs at each reflection step, and enhances the self-assessment ability by considering the agreements among multiple-perspective perturbations strategically induced in responses.We evaluate the performance of Mirror on two categories of reasoning tasks: MMLU (Hendrycks et al., 2021), a knowledge-rich question-answering dataset, and FEVER (Thorne et al., 2018), a factchecking dataset.Mirror achieves a significant average improvement of over 15% compared to recent popular unsupervised self-refinement methods.The empirical observations demonstrate that the proposed diversity-based reward and answer assessment strategy serve as reliable sources for performance enhancement.</p>
<p>Related Work</p>
<p>Self-Reflection LLMs.Extensive research (Honovich et al., 2022;Xie et al., 2023b) has been conducted to enhance LLMs through the concept of self-reflection, where LLMs learn from automatically generated feedback to understand and reflect on their own outputs.This feedback can stem from various sources: the LLM itself (Madaan et al., 2023;Shinn et al., 2023), a separately trained critic module (Gou et al., 2023b;Peng et al., 2023) or external sources (Yao et al., 2023b), such as Wikipedia or an Internet Browser.Gou et al. (2023b); Peng et al. (2023) argued that evaluators trained on task-oriented feedback offer superior performance.For example, Refiner (Paul et al., 2023) took context and hypotheses as input to generate templates-based feedback for various error types.Recent studies (Peng et al., 2023;Shinn et al., 2023;Hao et al., 2023) have fully utilized the in-context learning capability of LLMs, prompting them to generate high-quality feedback based on their pre-vious generation or potential templates.Madaan et al. (2023) proposed multiple task-oriented metrics and prompted LLMs to evaluate their own outputs based on these criteria.Similarly, Peng et al. (2023); Glaese et al. (2022) adopted external tools to predict multi-facet human preference scores.Our solution aligns with this trend by aiming to provide informative and customized instructions tailored to the specific task and query.Moreover, it seeks to achieve this without relying on human intervention or external tools, thereby rendering self-refinement more feasible in practice.</p>
<p>Reasoning models augmented with tree search.Recently, tree-based reasoning has attracted significant attention, such as Tree-of-Thought (ToT) (Yao et al., 2023a), Grace (Khalifa et al., 2023), and SelfEval-Decoding (Xie et al., 2023b).At each reasoning step, ToT adopts breadth-first search and depth-first search, while the latter two methods select the top-k scoring candidates during the decoding process.Moreover, Monte-Carlo Tree Search (MCTS) is one of the popular search algorithms (Swiechowski et al., 2023), which strikes a balance between exploitation and exploration.Some existing approaches establish a reinforcement learning framework to maximize reward through learning optimal actions/states (Du et al., 2023a;Parthasarathy et al., 2023;Zhu et al., 2023).Other studies fully utilize the capability of LLMs for interaction and feedback generation.For instance, RAP (Hao et al., 2023) leveraged step-wise rewards from interactions with the world model to decompose and solve the problem step-by -step, rather than a iterative manner.LATS (Zhou et al., 2023) was the first work in leveraging MCTS for self-reflection.However, their feedback contains information from comparisons with ground truth, which is not applicable in our case.Instead, our approach, Mirror has no access to gold labels, and we incorporate a novel diversity reward to avoid the inefficient search in the reflection iteration.</p>
<p>Lost in the Reasoning Loop</p>
<p>Given the observed challenges in enhancing LLMs' self-improvement without ground truth labels, particularly in knowledge-rich reasoning tasks, our initial experiment aims to address these challenges by breaking them down into two sub-questions.</p>
<p>Q1:</p>
<p>To what extent can LLMs assess the correctness of a statement?This investigation involves enhancing their capabilities through supervised train-ing.The primary goal is to discern if there are viable solutions to enhance the verification ability of LLMs on knowledge-rich statements.Q2: How well can LLMs generate high-quality feedback to guide their own subsequent response update?It is especially challenging when the feedback generation models are not trained on highquality data, relying solely on the in-context learning capability of LLMs.</p>
<p>LLMs in Knowledge Grounding</p>
<p>We experiment with the multiple-choice dataset, MMLU (Hendrycks et al., 2021), covering 57 subjects across STEM, Humanity, Social and other domains.To evaluate the ability of LLMs in assessing the knowledge-rich statements, we construct the positive and negative statements by substituting the question with the correct choice and a randomly selected choice from the other three incorrect choices, respectively.Table 1 presents the assessment accuracy of assessing.There are three categories of methods: in-context learning, finetuned on statements, and classification based on intermediate activations from LLMs.</p>
<p>As illustrated in the first group results in Table A1, an increase in accuracy is observed as the size of Llama-2-13B-chat increases.Notably, GPT-3.5 with 175B parameters consistently achieves the best results across the three domains, although the improvement is not directly proportional to the parameter size.We then apply advanced prompting techniques, i.e., UniLangCheck (Zhang et al., 2023) on the best-performing method, GPT-3.5.Our analysis reveals that the improvements are predominantly driven by self-consistency, while UniLangCheck does not consistently contribute to improvement in grounding.For UniLangCheck, we firstly prompt LLMs to generate a fact about the key elements in a question before making the final assessment.It can be partially explained by the accumulation error, i.e., the inaccurate facts generated by LLMs before reaching the final conclusion can affect the outcome.We also calculate the correlation between accuracy and self-consistency, represented by the probability of generating a single answer through multiple repeated prompting.The average correlation R2 for questions in the MMLU datasets across three LLMs is about 0.85, indicating that self-consistency can be relied upon as a proxy for assessment 2 .We also evaluate the performance of some supervised methods (denoted with ⋆ in Table 1).TRUE (Honovich et al., 2022) involves fine-tuning a T5 (Raffel et al., 2020) model on a collection of natural language inference (NLI) datasets for fact-checking.We further fine-tune its classifier head on our training set.ActivationRegress (Marks and Tegmark, 2023) trains classifiers using activations extracted from Llama2-13B 12-layer encodings as inputs.ContrastSearch (Burns et al., 2023) is trained using contrastive and consistency loss while having no access to the factual labels.This is achieved by constructing data pairs that include both a positive-labeled and negative-labeled statements, irrespective of the true factual labels.It is surprising that both TRUE and ActivationRegress are inferior than the unsupervised ContrastSearch.</p>
<p>LLMs in Feedback Generation</p>
<p>Evaluating the quality of generated feedback poses a significant challenge, particularly when such feedback is utilized across diverse tasks (Madaan et al., 2023).Drawing inspiration from the pivotal role of feedback in the self-improvement, we propose to leverage the performance of LLMs in subsequent iterations for evaluation.Specifically, LLMs can access to ground truth, enabling them to evaluate the correctness of their current responses.This information is then integrated into feedback generation.Consequently, we assess the quality of feedback by examining the percentage of examples that are incorrectly answered, along with the percentage of instances where responses in the next round are revised for the same incorrectly answered examples.This comparison sheds light on the effectiveness of instructions in guiding LLMs to rectify their erroneous responses.Firstly, we follow the consistency evaluation results are shown in Table A1.</p>
<p>settings in (Shinn et al., 2023) to incorporate the assessment results in the feedback: "Observation: The answer is incorrect."is inserted after presenting the question and previous attempt, and the LLMs are required to generate refection and response to this question again.From the results in Figure 2, it is consistently observed across different model scales that LLMs struggle to update their predictions despite receiving explicit negative feedback.The average percentage of successfully updated examples for GPT-3.5, Llama, and Vicuna are 65.6%, 51.79% and 74.09%, respectively, indicating an ample room for improvement.</p>
<p>Motivated by the following two observations: (1) LLMs are particularly susceptible to context influence at the beginning or near the end (Liu et al., 2023a), (2) In-Context Learning is highly sensitive to stylistic and emotional words in demonstrations (Min et al., 2022;Li et al., 2023), we develop three prompting strategies for feedback generation.An incorrectly predicted example with different prompting strategies is shown in Figure A2.The results in Table A2 and Table A3 suggest that based on correct question assessment, enhancing the exploration capability within a diverse answer space could lead to higher accuracy in answering knowledge-rich questions.</p>
<p>The above empirical findings regarding the two research questions provide valuable insights for our proposed model, named Mirror.Distinguishing itself from existing self-improvement methods, Mirror makes two significant contributions: (1) it features a Navigator module for generating multiple question-adaptive directions, with diversity constraints implemented to prevent invalid reflections.(2) it relies on the consistency of the inherent multiple perspectives for boosted self-assessment.</p>
<p>The Framework of Mirror</p>
<p>In this section, we introduce our unsupervised selfreflection framework, Mirror, depicted in Figure 3.The reward R consists of Diversity and Consistency terms.Diversity is applied to prevent reflection from becoming stuck and to facilitate intraconsistency involved in the stop criteria for selfassessment.The Consistency reward also influences direction generation.</p>
<p>Problem Setup</p>
<p>Given a question, the Reasoner is to arrive at the final answer through interacting with a Navigator.We consider a Markov Decision Process (MDP) defined by a tuple (S, A, P, π, γ, R), where the s t ∈ S and a t ∈ A denote the state and action, respectively in the t-th reflection iteration.In the context of multiple-choice question, a t is the direction generated by the Navigator, and s t is the response generated by the Reasoner, including the answer to the question and the rationale behind.R(s, a) is the reward function.Therefore, we have state transition distribution P(s t |s t−1 , a t−1 ) and action generation distribution π(a t |s t−1 , a t−1 , q, p 0 , R), where p 0 is the prompt for the Navigator to generate direction a t .It is nontrivial to obtain frequent rewards that incentivize self-refinement progress without access to the ground truth.Therefore, we turn to an intrinsically motivated planning algorithm, i.e., Monte-Carlo Tree Search (MCTS) (Kocsis and Szepesvári, 2006;Browne et al., 2012;Swiechowski et al., 2023) to efficiently explore the environment augmenting rewards with auxiliary objectives (Mu et al., 2022;Du et al., 2023b).</p>
<p>Comparing to existing work search-based reasoning methods based on frozen LLMs (Hao et al., 2023;Zhou et al., 2023), we highlight two notable contributions addressing the vulnerabilities of LLMs as discussed in §3: (1) Step-wise Multipleperspective self-assessment: unlike approaches that rely on ground truth or majority-voting based on several complete generated trajectories, our framework utilizes multiple-perspective consistency as stop criteria at each step t. (2) Novel Reward Mechanism: a novel diversity mechanism is designed to avoid the null space encountered in traditional random search settings.Our method is detailed in Algorithm 1 in the Appendix.</p>
<p>Multiple-perspective Assessment</p>
<p>Motivated by the empirical results in § 3.1 regarding knowledge-grounding, we propose to employ an advanced consistency-based method as a surrogate for factual correctness when external resources are unavailable.This method considers both intra-and inter-consistency of the generated responses.Specially, we employ the Navigator for K question-oriented direction generation, a ∼ π(a t |s t−1 , a t−1 , q, p 0 , R).These K directions are intended to provide diverse perspectives for problem-solving, with the agreement among guided responses representing interconsistency.Meanwhile, the confidence in selfconsistency (Wang et al., 2023) serves as the measure of intra-consistency.</p>
<p>To integrate consistency considerations into the assessment per reflection iteration, we use intraconsistency to determine whether the Reasoner should accept its initial response.If the intraconsistency for our initial answer surpasses a threshold T 0 , we consider it as the final result; otherwise, we integrate the inter-consistency as an indicator for stopping criteria in subsequent reflection iterations.We derive the final answer when the inter-consistency exceeds T 0 or when reach the predefined maximum iterations, selecting the final answer with the highest consistency score 3 .This inter-consistency also becomes part of reward R consistency for the current state and contribute to the direction generation.Besides, the intra-consistency value is transformed into verbal form, becoming part of the prompt p 0 given for Navigator to generate direction.This is inspired by our observation that higher intra-consistency implies a higher likelihood of correctness, so we offer this additional information to assist in feedback generation.This is similar to (Xu et al., 2023), where ICL performance benefits from accessing to the prediction of a supervised fine-tuned smaller model.Our assessment method is different from majority vote, which treats every node with the same weight when aggregating the final result.The comparison results are shown in Table 4.</p>
<p>Diverse and Valid Search Space</p>
<p>Obtaining a meaningful and diverse action space is challenging due to the absence of a dense and well-defined reward function in the planning algorithm.One of the predominant reasons is that different action sequences can lead to similar outcomes (Baranes and Oudeyer, 2013).In our context, considering the limitation of LLMs in following instructions, the Reasoner may ignore the differences among multiple directions and generate identical responses merely based on the question.Therefore, some intrinsically motivated reinforcement learning algorithms choose to explore outcomes rather than actions (Oudeyer and Kaplan, 2007;Ladosz et al., 2022).MCTS addresses the limitation of sparse rewards by visiting novel states or transitions through random exploration (Du et al., 2023b).The most popular algorithm in the MCTS family, Upper Confidence Bound for Trees (UCT) (Kocsis and Szepesvári, 2006) is treated as the choice of child node, UCT
= R j + 2C p 2 In N (n) N (n j )
, where R j is the average reward for child node j, while the second term encourages sampling from nodes whose children are less visited.N (n) is the number of times current node (parent) has been visited in previous iterations, and N (n j ) is times of the child node has been visited.The C p &gt; 0 is a constant to control balance between exploitation (first term) and exploration (second term).In our case, we specifically promote diversity between the parent and child node, i.e., the response in previous attempt s t−1 and the current attempt s t .For multiple-choice questions in MMLU, we assess if the predicted choices are the same across two reflection iterations.The discrepancy in responses indicates the alleviation of null direction space and the avoidance of being stuck, especially given the relatively low consistency with the response from the previous iteration.The relationship between task performance and the diversity of responses in the generated tree, as illustrated in Figure 5, confirms our motivation for diversity enhancement.</p>
<p>However, maximizing diversity of outcomes may not always be enough, as less relevant states might be collected (Du et al., 2023b).Therefore, to ensure valid search space, we filter out states whose associated responses are not in the correct form, such as failing to provide a final choice, or refusing to answer questions for moral considerations.For search efficiency, our proposed stopping criteria is to terminate the search process once its interconsistency surpasses a threshold, thereby avoiding unnecessary search and expansion costs.</p>
<p>Can Mirror Steer LLMs in Iterative</p>
<p>Improvements?</p>
<p>We evaluate our proposed Mirror on MMLU and FEVER (Thorne et al., 2018).FEVER is a fact-checking dataset featuring three labels for knowledge-rich statements, i.e., supports, refutes and not enough info.4</p>
<p>Experimental Setup and Results</p>
<p>Comparison methods.The evaluation models are GPT-3.5,Llama2-13B-Chat (Touvron et al., 2023), and Vicuna-v1.5-7B(Zheng et al., 2023) 5 .We equip the LLMs with different reasoning mechanisms, including Chain-of-Thought (CoT) (Wei et al., 2022), Self-consistency (Wang et al., 2023), Self-Correction (Huang et al., 2023) and Reflexion(w.GT) (Shinn et al., 2023).We implement CoT by prompting LLMs to first generate step-by-step thoughts and then generate answers based on those thoughts.We repeat this process for five times, resulting in Self-Consistency (5) .The remaining two methods are self-improvement techniques where LLMs are first prompted to generate reflections, followed by updating their current response accordingly if applicable.Self-Correction relies on LLM's internal knowledge for answer assessment, while Reflexion compares the current answer with the ground truth for evaluation.Answer: B</p>
<p>Self-improvement</p>
<p>Please check your answer and find possible errors.If everything is good, stick to your answer, otherwise, update to a new answer.It seems there may be a discrepancy between your rationale and your selection.Please consider revising your answer to align with the correct interpretation of the score.</p>
<p>Self-improvement</p>
<p>Question</p>
<p>Initial Answer</p>
<p>Self-improvement</p>
<p>Encourage the student to consider the specific indicators that would lead to each level of depression.Encourage them to think about the impact of a score of 50 on the Beck Depression Inventory-II and how it matches up with the levels of depression described in the choices given.</p>
<p>It seems there may be a discrepancy between your rationale and your selection.Please consider revising your answer to align with the correct interpretation of the score.Consider revising your answer accordingly.Results.The results are shown in Table 2.By comparing CoT with Self-Correction, we observe the performance degradation after two rounds of self-Correction across almost all datasets and models.This observation aligns with our findings in §3.1 and in (Huang et al., 2023).Equipped with self-consistency (5) , significant performance improvements are evident across all settings.Mirror considers additional inter-consistency, achieves the most notable improvements, with a relative increase of more than 15% across the three models.Figure 4 illustrates the reasoning process of Self-correction and Mirror.Both methods fail to answer correctly in the first trial.With question-oriented direction, the Reasoner better identify errors in the initial response, such as, the error in score direction and inconsistency between rationales and selection.The consistency-based criteria built in the tree further improves the fact assessment.During backpropagation, node s</p>
<p>Self</p>
<p>(1)</p>
<p>1 receives a higher reward, leading to the leftmost reasoning path (details of direction a
(1) 1 , a (1) 2 , a(2)
2 and corresponding responses are shown in the text frame).By contrast, Self-correction seems to engage in groundless inference by switching answers without explicit clues.Even comparing Mirror with Relexion(w.GT), we find comparable results for GPT-3.5 on the STEM dataset, for Llama on all datasets except for STEM and for Vicuna on STEM and Humanity.From the perspective of the model, the average improvements over baselines for GPT-3.5 are particularly prominent, partly explained by its better ability to adhere to provided directions.This can also explain the marginal improvements even ground truth are accessible to the smaller models.</p>
<p>Analysis</p>
<p>We discuss the effects of key strategies in Mirror.</p>
<p>Question-Oriented Direction.Motivated by the findings in § 3.2 that LLMs struggle to effectively reflect on themselves with generic feedback, Mirror is equipped with a Navigator for generating question-oriented directions.To study the effects of these directions (results in Table 3), we adopt our Navigator for direction generation for CoT settings, in which the direction (Gener-ativeDirect) is introduced before the LLM generates its thought on the previous trial.We then replace all adaptive directions with a single generic direction (FixedDirect) which reads: Read the question and choices carefully and diagnose the previous response by locating the incorrect clues and update the response if applicable.Comparing with CoT, the inclusion of GenerativeDirect boosts the performance across all settings with significant improvements.Conversely, FixedDirect sometimes results in performance degradation for Llama13B.The impact of FixedDirect is similar to advanced instruction intended to provide general direction for the task, whereas GenerativeDirect offers questionspecific advice to accurately summarize clues for solution.Referencing to the example in Figure A3, Mirror (bottom) firstly prompts the Navigator for direction generation (highlighted in red), which captures the key elements, such as "the characteristics of a connected and undirected graph".The Reasoner then follows this direction to explain the key concepts of this graph, laying a solid foundation for reaching the correct conclusion.Without such direction, the Reasoner may overlook or misinterpret knowledge about this graph, leading to errors in the conclusion.Diversity of the Search Space.We demonstrate the impact of multiple-perspective directions, aiming at guiding the Reasoner out of reflection traps.To this end, we compute the percentage of generated trajectories containing the correct answers (ans _ presence) and the according task performances (acc) across various action space sizes, i.e., the number of generated directions.The results in Figure 5 indicate that lager search space enhanced by the R diversity can increase the probability of reaching the correct answer.where the ground truth is included in the tree (ans-presence), with different sizes of search space (Num).Results for GPT-3.5 and Llama13B are in Figure A4a and A4b.</p>
<p>We provide a case to show the effects of diversity in directions (highlighted difference are in bold).The quantitative results of semantic similarity between multiple directions are shown in Table A5.</p>
<p>Case study</p>
<p>Question: The price elasticity of demand for a product is greater if -Answer without Direction: C. the number of substitute products is limited.</p>
<p>-Answer with Direction: Direction 1: Pay close attention to nuances among options and prioritize selections that resonate with economic principles and the factors influencing consumer behavior.</p>
<p>Answer 1: Given the advice, the most suitable choice would be: A. the proportion of the good of the consumer's budget is high.This aligns with economic principles of demand elasticity.Direction 2: Prioritize options that align with factors impacting consumer responsiveness to price changes, such as budget allocation and substitute availability, while considering the time frame for consumer reactions.</p>
<p>Answer 2: Given the advice, the most suitable choice would be: A. the proportion of the good of the consumer's budget is high.This choice suggests that when a significant portion of a consumer's budget is allocated to a particular product, they are likely to be more sensitive to price changes.</p>
<p>Performance of Answer Assessment Criteria.</p>
<p>As discussed in Section 3.1, LLMs struggle to assess the correctness of knowledge-rich statements, a capability that can be consistently enhanced through self-consistency.We further reform the majority-voting assessment process by considering the inter-consistency built in the hierarchical decision-making tree.To study the effects of our answer assessment criteria described in §4.2, we compare them with two other voting methods, i.e., self-consistency and majority vote within our gener-ated tree-trajectories.We average the results from Table 2 for CoT and Self-consistency (5) across four domains in MMLU and denote them as CoT (1)  and CoT (5) , respectively.We also compare with CoT (15) because our generated trees have at most 3 layers and 5 branches in each layer, so there are 15 candidate nodes.For Majority (tree) , we select the final answer through majority-voting among all intermediate nodes in our generated tree-trajectories.The results of different final answer assessments are presented in Table 4.The performance improvements are observed on self-consistency (15)  over self-consistency (5) , although the improvements percentage isn't pronounced as that seen in the comparison between self-consistency (5) over CoT.We observe a performance increase after applying majority-voting in the CoT settings, while this simple strategy doesn't yield improvements in the generated tree.This is because undesirable responses may be generated during the node expanding phase, and majority voting treats all nodes equally.In contrast, our reward-based search tends to focus on reliable nodes with higher confidence in each reflection step, thereby avoiding search cost on less desirable nodes.</p>
<p>Models</p>
<p>Ans. Assessment MMLU FEVER GPT-3.5:CoT (1) 0.60 0.58 CoT (5)  0.64 0.61 CoT (15)  0.67 0.62 +Majority (tree)  0.69 0.59 +Reward Search (tree)  0.73 0.64</p>
<p>Llama13B:</p>
<p>CoT (1) 0.49 0.40 CoT (5)  0.53 0.46 CoT (15)  0.55 0.48 +Majority (tree)  0.58 0.50 +Reward Search (tree)  0.60 0.54 Vicuna13B:</p>
<p>CoT (1) 0.51 0.39 CoT (5)  0.56 0.43 CoT (15)  0.58 0.43 +Majority (tree)  0.59 0.43 +Reward Search (tree)  0.60 0.46 Search Efficiency Analysis.Mirror is a tree-like searching algorithm that benefits from iterative reasoning process, which enhances the reasoning ability at the computation cost.To mitigate the search cost, we (i) incorporate the Monte-Carlo tree search for its selective search and expansion.(ii) introduce early-stop criteria to encourage a shallow and avoid multiple playouts.We summarise the tree-depth in Table 5.The results below show that our resulting tree, with a maximum depth of 3, is heavily unbalanced and shallow.</p>
<p>STEM Social Hum Others FEVER Depth=2 0.17 0.12 0.28 0.13 0.04 Depth=3 0.00 0.09 0.20 0.04 0.00</p>
<p>Table 5: Depth of the search tree based on GPT-3.5 across different datasets.</p>
<p>Conclusion</p>
<p>In this paper, we present a multiple-perspective reflection method, called Mirror, for knowledgeenriched reasoning.To tackle the limitations of LLMs in fact assessment and the generation of high-quality feedback, Mirror is equipped with a directional Navigator, enabling the Reasoner to identify multiple key clues in problem-solving.Furthermore, the consistency among responses generated under different directions enhances the validity of answer assessment, particularly when ground truth is not accessible.Experiments conducted demonstrate Mirror's superiority over several contemporary CoT-based and self-consistency-based reasoning approaches without access to ground truth.Moreover, the ablation study results clearly show that our strategies effectively alleviate the aforementioned challenges.</p>
<p>Limitations</p>
<p>In this study, our primary focus is to identify optimal reasoning trajectories based on generated outputs and frozen states.However, the ability to assess facts and generate reflections may be limited by the unaltered decoding process and pre-training.</p>
<p>To fully leverage the potential of LLMs in complex reasoning, it is beneficial to explore two directions:</p>
<p>(1) Strategically guiding fine-grained generation, such as token-level generation during the decoding phase within the expansive generation space.(2) Fine-tuning LLMs through access to limited taskoriented data to enhance their responses to more complex problems.</p>
<p>A More Experimental Details for Initial Study</p>
<p>A.1 Experiment for Figure 1 The prompt used in Autostop is "You were either successful or unsuccessful in your previous trial.Stick to your previous answer if it is correct, otherwise consider a new answer".The prompt used for NeverStop is "You failed in your previous trial and reconsider a new answer".The motivation behind Autostop is that we totally rely on the LLM's internal knowledge to check the correctness of its own outputs.However, LLM fails in this setting as the performance is even worse than initial stage.For NeverStop, we hope to identify that some correctly answered samples will be kept unchanged even the negative feedback provided.However, we didn't find a pattern between the changed and unchanged predicted samples.</p>
<p>A.2 Implementation for Knowledge Grounding and Results</p>
<p>Dataset We evaluate LLMs' knowledge grounding ability on knowledge-rich multiple-choice dataset, MMLU.It consists of four domains: STEM, Social, Humanity and Other, totaling 56 subjects.All methods are evaluated on 50 randomly selected samples for each subject (excluding those in the Other domain), and the remaining samples are used as the training set where applicable.</p>
<p>Models and Baselines</p>
<p>In addition to Llama2-13B, Llama2-70B, and GPT-3.5 for prompting, we also leverage unified language checking, Uni-LangCheck (Zhang et al., 2023), for statement assessment.UniLangCheck aims to check if language input is factual and fair via prompting LLMs to generate groundings for fact-checking.Therefore, we firstly prompt LLMs to generate a fact about the key element in the question before proceeding to the final assessment.We repeatedly prompt the LLMs for 5 times and use the majority-voted answer as the result for Self-Consistency (Wang et al., 2023).TRUE (Honovich et al., 2022) is the T5-11B (Raffel et al., 2020) model fine-tuned on a collection of natural language inference (NLI) datasets to check factual correctness, and has been used by previous works within similar contexts (Gao et al., 2023a,b).We further fine-tune its classifier head on our training set, which is annotated as factually correct or not, before evaluation.Both Contrastive Consistent Search (ContrastSearch ) (Burns et al., 2023) and ActivationRegress (Marks and Tegmark, 2023) train classifiers whose inputs are activations extracted from Llama2-13B 12-layer encodings6 .</p>
<p>ActivationRegress trains a logistic classifier on the activations with factual labels as supervision.Con-trastSearch, instead, operates without factual labels.For a statement s i , we firstly construct a datapair x + and x − by annotating True and False to this statement ,regardless of its factual correctness.Then, we derive the probabilities by mapping x to a number between 0 and 1, i.e., p + = p θ (ϕ(x + i )) and p − = p θ (ϕ(x − i )).The mapping function p θ is updated such that the probabilities are both confident (p
+ i ≈ 1 − p − i ) and consistent (p + i ̸ ≈ p − i )
. Prompt Settings The basic prompt for knowledge grounding is shown in Figure A1a.This is used for Llama2, GPT-3.5 and Self-Consistency.The advanced prompt inspired by UniLangCheck is illustrated in Figure A1b.For each subject, we randomly select 50 samples and extract their question and choice to build a statement for knowledge checking.The correctness of this statement is deemed True if the selected choice is exactly the correct one, otherwise it is labeled False.</p>
<p>Correlation between Self-consistency Confidence and Accuracy For the self-consistency(5) baseline, we calculate the R 2 for confidence (the frequency of the current answer among all generated answers, totaling 5) and the accuracy.The results are shown in Table A1.We observe a high correlation between the two variables, which inspires our design of multiple-consistency for answer assessment.</p>
<p>A.3 Implementation for Direction Generation</p>
<p>Based on the observation that existing feedback has limited effects to guide LLMs to update their</p>
<p>C.1 Implementation Details</p>
<p>Hyper-parameter settings.In order to encourage diverse direction generation, we set the generation temperature as 0.8 for all the models, and we set do _ Sample = True for llama and vicuna to avoid greedy search.For the threshold T 0 in selfassessment to deriving the final answer, we set 0.8 for GPT35, and 0.5 for llama and Vicuna according to the results on limited validation data.These results reveal that larger language models are more consistent in their multiple outputs, which is more difficult for the smaller models.Hence, we adopt a relatively lower threshold for smaller models.This observation can be partially explained by the tendency of larger LMs to rely on their parametric memory (Xie et al., 2023a).</p>
<p>Prompt Settings.We provide 5 demonstrations along with instruction when prompting LLMs.We show the prompts/instructions provided to LLMs in direction generation and response generation process.(a) p 0 in direction generation in π(a t |s t , p 0 , R).The guidance in the upper is for initial response, the bottom one is for reflection in the subsequent iterations.(b) Prompt for response generation given previous response and direction.P(s t |s t−1 , a t−1 ; q).</p>
<p>Prompt for Direction Generation (MMLU)</p>
<p>As a tutor, your focus is on guiding the student to navigate multiple-choice question-answering problems strategically.Encourage them to dissect the question, identifying key elements and nuances within each choice.Emphasize the importance of understanding subtle differences that could distinguish correct from incorrect options.</p>
<p>As a tutor, your are supposed to meticulously evaluate the student's approach to multiple-choice problems.Question, Choices and the student's previous thought and answer are given, check if the facts mentioned in the thought is correct and if there might be a more appropriate option than the one chosen.If the student's reasoning thought is accurate and the proposed answer is the most appropriate, encourage them to adhere to their initial trial.Otherwise, guide the student to revisit specific details, explore alternative choice.</p>
<p>Prompt for Direction Generation (FEVER)</p>
<p>As a tutor, your focus is on guiding the student to navigate fact-checking problems strategically.Encourage them to dissect the claim, identifying key elements and associate facts.Emphasize the correct relation between important elements that could distinguish SUPPORTS from REFUTES options.Also, lacking of enough information will lead to NOT ENOUGH INFO.</p>
<p>As a tutor, your are supposed to meticulously evaluate the student's approach to fact verification task.Claim and the student's previous thought and answer are given, check if the relations mentioned in the Thought is correct and if there might be a more appropriate answer.If the student's reasoning thought is accurate and the proposed answer is the most appropriate, encourage them to adhere to their initial trial.Otherwise, guide the student to revisit specific details, explore alternative answer.</p>
<p>Prompt for Response Generation (MMLU)</p>
<p>You are an expert in multiple-choice question answering.Each problem will provide you with a question and answer choices.Read the question and all the choices carefully, along with the provided advice, and solve the problem by having a thought.Thought can reason about the current situation.Finish[answer] returns the answer and finishes the task.</p>
<p>You're an advanced reasoning agent capable of selfreflection and continuous improvement.Your objective is to tackle multiple-choice question answering problems.Each problem will provide you with a question, answer choices, your previous line of reasoning, and the detailed analyses from an experienced tutor.</p>
<p>In a succinct review, assess the accuracy of your earlier answer based on your expertise and the advice, subsequently arrive at the definitive response.</p>
<p>Prompt for Response Generation (FEVER) You're an advanced reasoning agent capable of selfreflection in fact verification task.Claim and the your previous response and answer are given, along with the advice.In a succinct review, assess the accuracy of your earlier answer based on your expertise and the advice, subsequently arrive at the definitive response.</p>
<p>Computational budget.The total running costs for using GPT-3.5 in our experiments are approximately $500.In addition, the running time for Llama2 and Vicuna in our experiments totalled 320 hours, utilising one 40G A100 graphics cards.</p>
<p>C.2 Additional Results</p>
<p>We provide additional results as complementary to our main results.</p>
<p>Results on GSM8K dataset.To further verify the effectiveness of Mirror in various reasoning tasks, we include the math problem, i.e., GSM8k (Cobbe et al., 2021).The performance superiority is evident when comparing with the best-performing unsupervised baseline, i.e., selfconsistency .</p>
<p>COT w. self-cons (5)  Effects of question-oriented direction.To save computational resources, we randomly select 20 samples from each of the four domain datasets in MMLU and from FEVER.We show an example of generated direction in the CoT settings.</p>
<p>Question: For a connected, undirected graph , which of the following must be true?I.</p>
<p>degree is even.II.</p>
<p>. III. G has at least one vertex with degree 1. Thought: I.A connected, undirected graph G = (V, E) is a type of graph in which there is a path between every pair of vertices.For such a graph, the sum of the degrees of all vertices is always even.This is because the sum of the degrees of all vertices is equal to twice the number of edges Therefore, statement I must be true.</p>
<p>Action: Finish[D. I and II only]</p>
<p>Model Output Thought: I. degree even.This statement is not necessarily true.In an undirected graph, the sum of the degrees of all vertices can be odd.For example, consider a graph with three vertices, each with degree 1.The sum of the degrees would be 3, which is odd.</p>
<p>... The diversity of Search Space One of our motivations is to broaden the diversity of actions available for more effective exploration.Consequently, we compute the upper bound results for our generated tree, indicating the presence of the correct answer in the tree signifies a correctly answered sample.Results are shown in Figure A4.</p>
<p>Action:</p>
<p>To analyse the effects of different LLMs quantitatively, we calculate the average pairwise semantic similarity between multiple directions for one question, then 1-similarity to obtain the diversity measurement shown below.The pretrained model, all-MiniLM-L6-v27 is used for sentence-pair similarity calculation.The results is consistent with the intuition that sophisticated LLMs incline to less diverse instruction although such diverse directions are already capable to improve task performances.</p>
<p>D Ethics Statement</p>
<p>We utilized two publicly available datasets: Massive Multitask Language Understanding (MMLU) and FEVER (Fact Extraction and Verification).MMLU is a multiple-choice question-answering dataset covering 57 subjects across STEM, social sciences, humanities, and more.Notably, some subjects, such as moral disputes and moral scenarios, contain statements that may raise ethical concerns.Here, LLMs could be misused or misinterpret the information.We strongly recommend thorough consideration of safety implications before applying such techniques in real-world scenarios.For the FEVER dataset, positive claims (facts) are extracted from Wikipedia, and negative claims are generated by contrasting these facts and subsequently verified without knowledge of their original source sentences.However, due to Wikipedia's editable nature, the extracted facts may not always be entirely accurate.Consequently, we advise against solely relying on our work as the truth source for any fact-checking task to avoid potential confusion and bias.</p>
<p>Figure 1 :
1
Figure 1: Without ground truth for validating LLM-generated outputs, LLMs struggle to consistently improve their own outputs due to their incapability of self-assessment.Autostop and Neverstop provide different generic feedback without leaking the correctness of the current response.</p>
<p>Figure 2 :Figure 3 :
23
Figure 2: The average number (across all iterations) of changed and unchanged samples among those predicted incorrectly.Large percentage of unchanged samples indicate the limited capability for efficient reflection.</p>
<p>My previous response was partly correct.A score of 50 on the Beck Depression Inventory-II does suggest a significant level of depression.Therefore, the correct answer is D. Answer: D</p>
<p>Figure 5 :
5
Figure 5: The Accuracy (acc) and the percentage of samples</p>
<p>A. I only B. II only C. III only D. I and II Oversight: Consider first identifying any possible misconceptions about the concepts of connected, undirected graphs and their properties.Next, analyze each statement in the question and evaluate its relevance to the definition and characteristics of connected, undirected graphs.</p>
<p>Figure A3 :
A3
Figure A3: With question-oriented direction, the Reasoner answers questions with explicit clues.</p>
<p>FigureA4: The task performance, Accuracy (acc) and the percentage of samples where the ground truth is included in the tree (ans-presence), with different size of search space (Num).</p>
<p>I think a score of 50 on the Beck Depression Inventory-II suggests moderate depression.It's important to note that this score indicates a significant level of depression that may require professional attention.So, the correct choice is C. moderate depression.Upon reviewing the scoring criteria for the Beck Depression Inventory-II, I have revised my understanding.A score of 50 on the inventory actually suggests severe depression, indicating a high level of depression that may require immediate professional attention.Therefore, the correct answer is D. severe depression.Thank you for directing me to ensure a precise understanding of the scoring criteria.A score of 50 on the Beck Depression Inventory-II suggests: A. no or minimal depression.Scores on the BDI-II range from 0 to 63, with higher scores indicating more severe depressive symptoms.A score of 50 falls within the range considered to indicate no or minimal depression.So, the correct choice is: A. Upon reviewing the previous answer, I realized that I made an error in my initial response.I apologize for any confusion.Let me correct the answer: A score of 50 on the Beck Depression Inventory-II suggests: B. mild depression.
Initial AnswerQuestion: A score of 50 on theBeck Depression Inventory-IIsuggests:?Thought:A. no or minimal depression.B. mild depression.C. moderate depression. D. severe depression.Answer: AAnswer: DThought: Answer: D
Thought:Thought:</p>
<p>Table 2 :
2
Performances
MethodsSTEM Social Hum Others FEVERRelexion(w.GT) (5)0.790.840.780.730.72GPT-3.5 (CoT)0.630.650.530.600.58Self-Consistency (5)0.670.680.580.640.61Self-Correct (2)0.630.620.550.540.55Mirror0.760.770.710.670.64Relexion(w.GT) (5)0.640.630.600.640.59Llama13B(CoT)0.420.580.420.530.40Self-Consistency (5)0.450.600.490.570.46Self-Correct (2)0.420.520.530.450.36Mirror0.570.620.580.620.54Relexion(w.GT) (5)0.620.680.590.690.59Vicuna13B (CoT)0.460.570.430.570.39Self-Consistency (5)0.500.620.530.600.43Self-Correct (2)0.430.490.420.490.38Mirror0.590.640.560.650.46
-Correction Mirror Figure 4: Reasoning process of self-correction and Mirror.Text in red are generated directions.Our diversity is characterised in (i) generating directions tailored to questions (ii) encouraging exploration in multiple plausible reasoning paths.The final answer is derived through an agreement among multiple trajectories. of different reasoning methods, withan upper-bound represented by results obtained when ground truth is provided, denoted as Relexion(w.GT).The superscripts denote the number of reasoning iterations.</p>
<p>Table 3 :
3
Performances of using generic fixed direction and generative direction on top of CoT.</p>
<p>Table 4 :
4
Results of different answer assessment methods.</p>
<p>Table A4 :
A4
Based on GPT-3.5, performances of different reasoning methods.
Stem Social Humanity Other FeverGPT-3.5 0.665 0.6730.6740.648 0.632LLama 0.689 0.6910.6880.669 0.661Vicuna 0.694 0.6900.6920.679 0.673w. self-cons (15) Mirror0.720.750.770.80</p>
<p>Table A5 :
A5
Diversity among different directions generated by Mirror.</p>
<p>Experiment details are shown in Appendix A.2, self-
The threshold T0 for different models are datasets are set according to the validation performance, details in C.1.
We also include the evaluation results on GSM8K(Cobbe et al., 2021) in TableA4.
We denote them as Llama13B and Vicuna13B for simplicity. Experiment details can be found in Appendix C.
The original dimensions of Llama2-30B is 5024. We apply PCA to reduce this dimensionality to obtain 50dimensional activations as classifier input.
https://huggingface.co/sentence-transformers/ all-MiniLM-L6-v2
AcknowledgementsThis work was supported in part by the UK Engineering and Physical Sciences Research Council (EPSRC) through a Turing AI Fellowship (grant no.EP/V020579/1, EP/V020579/2) and a New Horizons grant (grant no.EP/X019063/1).As an expert in knowledge grounding, you'll be assessing statements that consist of a question followed by a proposed answer.The question forms the initial part of the statement, and the answer follows it.Utilize your thoughtful analysis to determine the correctness of each statement.Conclude the assessment with a "Finish[answer]" that returns either True or False, marking the completion of the task.Here are some examples: {examples} (END OF EXAMPLES) Statement: {question:q, answer: a} Thought: thought Action: Finish<a href="a">answer</a> Basic prompt for knowledge grounding.Text in gray is extracted from datasets, in red shadow is generated by LLMs.In your capacity as a specialist in knowledge grounding, your task is streamlined into a comprehensible two-step process.Firstly, assume the role of a question architect, delineating the essential "key elements/knowledge" integral to formulating a sound question.Subsequently, based on these identified key knowledge elements, proffer your response.The ensuing step involves a meticulous comparison of your proposed answer with the provided solution to ascertain accuracy.Conclude this evaluative process with a succinct 'Finish[answer]current incorrect response, we propose several simple strategies to enhance the effectiveness of generated feedback in the self-improvement process.These strategies are mainly inspired by the following two observations: (1) LLMs are more susceptible to context influence at the beginning or near the end(Liu et al., 2023a)(2) ICL is highly sensitive to the stylish and emotional words in demonstrations(Min et al., 2022;Li et al., 2023).We summarize the different strategies in the diagram shown in FigureA2.We show relative percentage of changed samples those incorrectly predicted ones before and after applying the NegReflect in TableA2.The percentages have been greatly improved with the instruction which has been inserted closer to the end of prompt.To verify whether this change could lead to task performance, we display the detailed performances over three LLMs after applying different instructions in TableA3.It is clear that NegPrefix demonstrates the most significant improvements across all the datasets and models.In contrast, NewAnswer has the same sentences Neg-Prefix as but its position is far away from the generating point for LLMs.This can be explained that position of instruction is important in ICL.And the performance of NewAnswer is slightly better than baseline, it can be partly explained that the NewAnswer explicitly show the negative attitude towards and guide the model to generate a different answer.Among the three models, the average promotion on GPT3.5 is the most negligible.This can be explained that larger model are more confident with its internal knowledge and less vulnerable to given noisy text.ModelPrompts ChangeGPT35B Mirror algorithmWe introduce the pipeline of the proposed Mirror in Algorithm 1 involves iteratively conducting a UCT-SEARCH until predefined iteration constraint is reached, and the best action a(BESTCHILD(v 0 , 0)) leading to the best child of the root node v 0 returns.Previous TrialYou have attempted to answer the following question before and failed.Below is the last unsuccessful trial you attempted to answer..BaselineThe answer is incorrect .ObservationThe Thought in last trial is not factually correct and I will reconsider and propose a different answer.NewAnswerThe Thought in last trial is not factually correct and I will reconsider and propose a different answer.Node in the tree is v and its associated state is s(v), representing the response generated by Reasoner.NegPrefixBaselineThe action is a(v), reward is R and N (•) is the times of the node having been visited.r(v) is the reward for the terminate state at each iteration.The overall process consists of three steps: (1) SEARCHPOLICY to obtain the terminal node v l .through which expands the tree until fully expanded.Specially, we randomly add one or more nodes to the root node according to the possible actions.In our case, we generate multiple responses to the given question and previous attempts/response.When the current node is fully expanded, we apply the UTC algorithm to select the best child node.(2) SIMULATION the reward r for v l through SIMULATIONPOLICY.This phrase is to simulate the future rewards of the current node through multiple interactions.For simplicity, we follow the similar process as expansion and return the reward r for selected action-state pair.(3) BACKPROPA-GATE the simulation results to the selected nodes to accelerate SEARCHPOLICY in next iteration.Algorithm is described in Algorithm 1.C Experiments for MirrorWe will introduce the implementation details and provide complementary results experimented on Mirror in this section.
Active learning of inverse models with intrinsically motivated goal exploration in robots. Adrien Baranes, Pierre-Yves Oudeyer, 10.1016/J.ROBOT.2012.05.008Robotics Auton. Syst. 6112013</p>
<p>Spyridon Samothrakis, and Simon Colton. 2012. A survey of monte carlo tree search methods. Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, 10.1109/TCIAIG.2012.2186810IEEE Transactions on Computational Intelligence and AI in Games. 41</p>
<p>Discovering latent knowledge in language models without supervision. Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt, ICLR 2023The Eleventh International Conference on Learning Representations. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR, abs/2110.141682021</p>
<p>Learning universal policies via text-guided video generation. Yilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B Tenenbaum, Dale Schuurmans, Pieter Abbeel, 10.48550/ARXIV.2302.00111CoRR, abs/2302.001112023a</p>
<p>Guiding pretraining in reinforcement learning with large language models. Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, Jacob Andreas, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USAPMLR2023b. July 2023Proceedings of Machine Learning Research</p>
<p>RARR: Researching and revising what language models say, using language models. Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, Kelvin Guu, 10.18653/v1/2023.acl-long.910Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a1</p>
<p>Enabling large language models to generate text with citations. Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen, Empirical Methods in Natural Language Processing (EMNLP). 2023b</p>
<p>Improving alignment of dialogue agents via targeted human judgements. Amelia Glaese, Nathan Mcaleese, Maja Trkebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, A See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Sovna Mokr'a, Nicholas Fernando, Boxi Wu, Rachel Foley, ArXiv, abs/2209.14375Gabriel, William S. Isaac, John F. J. Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving2022</p>
<p>CRITIC: large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, 10.48550/ARXIV.2305.11738CoRR, abs/2305.117382023a</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, ArXiv, abs/2305.117382023b</p>
<p>Reasoning with language model is planning with world model. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, 10.48550/ARXIV.2305.14992CoRR, abs/2305.149922023</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>TRUE: Re-evaluating factual consistency evaluation. Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, Yossi Matias, 10.18653/v1/2022.naacl-main.287Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United States2022Association for Computational Linguistics</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, 10.48550/ARXIV.2310.01798CoRR, abs/2310.017982023</p>
<p>Language models (mostly) know what they know. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova Dassarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, 10.48550/ARXIV.2207.05221CoRR, abs/2207.05221Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan2022</p>
<p>Grace: Discriminator-guided chain-of-thought reasoning. Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Lu Ho Hin Lee, Wang, Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Bandit based monte-carlo planning. Levente Kocsis, Csaba Szepesvári, European conference on machine learning. Springer2006</p>
<p>Exploration in deep reinforcement learning: A survey. Pawel Ladosz, Lilian Weng, Minwoo Kim, Hyondong Oh, Information Fusion. 852022</p>
<p>Cheng Li, Jindong Wang, Kaijie Zhu, Yixuan Zhang, Wenxin Hou, Jianxun Lian, Xing Xie, arXiv:2307.11760Emotionprompt: Leveraging psychology for large language models enhancement via emotional stimulus. 2023arXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, 10.48550/ARXIV.2307.03172CoRR, abs/2307.031722023a</p>
<p>Training socially aligned language models in simulated human society. Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, Soroush Vosoughi, arXiv:2305.169602023barXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Prasad Bodhisattwa, Shashank Majumder, Amir Gupta, Peter Yazdanbakhsh, Clark, 10.48550/ARXIV.2303.17651CoRR, abs/2303.176512023</p>
<p>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark J F Gales, 10.48550/ARXIV.2303.08896CoRR, abs/2303.088962023</p>
<p>The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. Samuel Marks, Max Tegmark, ArXiv, abs/2310.068242023</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, 10.18653/V1/2022.EMNLP-MAIN.759Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 20222022</p>
<p>Improving intrinsic exploration with language abstractions. Jesse Mu, Victor Zhong, Roberta Raileanu, Minqi Jiang, Noah Goodman, Tim Rocktäschel, Edward Grefenstette, Advances in Neural Information Processing Systems. 202235</p>
<p>What is intrinsic motivation? a typology of computational approaches. Pierre-Yves Oudeyer, Frederic Kaplan, Frontiers in neurorobotics. 162007</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. Liangming Pan, Wenda Michael Stephen Saxon, Deepak Xu, Xinyi Nathani, William Wang, Wang Yang, ArXiv, abs/2308.031882023</p>
<p>C-MCTS: safe planning with monte carlo tree search. Dinesh Parthasarathy, Georgios D Kontes, Axel Plinge, Christopher Mutschler, 10.48550/ARXIV.2305.16209CoRR, abs/2305.162092023</p>
<p>Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, ArXiv, abs/2304.01904Refiner: Reasoning feedback on intermediate representations. 2023</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, Jianfeng Gao, 10.48550/ARXIV.2302.12813CoRR, abs/2302.128132023</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211402020</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>Monte carlo tree search: a review of recent modifications and applications. Maciej Swiechowski, Konrad Godlewski, 10.1007/S10462-022-10228-YArtif. Intell. Rev. 5632023Bartosz Sawicki, and Jacek Mandziuk</p>
<p>FEVER: a large-scale dataset for fact extraction and VERification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, NAACL-HLT. 2018</p>
<p>Hugo Touvron, Louis Martin, Kevin R Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M Bikel, Lukas Blecher, Cantón Cristian, Moya Ferrer, Guillem Chen, David Cucurull, Jude Esiobu, Jeremy Fernandes, Wenyin Fu, Brian Fu, Cynthia Fuller, Vedanuj Gao, Naman Goswami, Anthony S Goyal, Saghar Hartshorn, Rui Hosseini, Hakan Hou, Marcin Inan, Viktor Kardas, Madian Kerkez, Isabel M Khabsa, A V Kloumann, Punit Korenev, Marie-Anne Singh Koura, Thibaut Lachaux, Jenya Lavril, Diana Lee, Yinghai Liskovich, Yuning Lu, Xavier Mao, Todor Martinet, Pushkar Mihaylov, Igor Mishra, Yixin Molybog, Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, R Smith, Xia Subramanian, Binh Tan, Ross Tang, Adina Taylor, Jian Williams, Puxin Xiang Kuan, Zhengxu Xu, Iliyan Yan, Zarov, ArXiv, abs/2307.09288Yuchen Zhang, Angela Fan. Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien Rodriguezand Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts. Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su, 2023a</p>
<p>Junxian He, and Qizhe Xie. 2023b. Self-evaluation guided beam search for reasoning. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Mingsung Kan, </p>
<p>Small models are valuable plug-ins for large language models. Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Julian J Chenguang, Mcauley, 10.48550/ARXIV.2305.08848CoRR, abs/2305.088482023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, ArXiv, abs/2305.106012023a</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, R Karthik, Yuan Narasimhan, Cao, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023b. May 1-5, 2023OpenReview.net</p>
<p>STar: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. 2022</p>
<p>Interpretable unified language checking. Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen M Meng, James R Glass, ArXiv, abs/2304.037282023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Haotong Zhang, Joseph Gonzalez, Ion Stoica, ArXiv, abs/2306.056852023</p>
<p>Language agent tree search unifies reasoning acting and planning in language models. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang ; Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, Yujiu Yang, 10.18653/v1/2023.acl-long.245CoRR, abs/2310.04406Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. 20231Solving math word problems via cooperative reasoning induced language models</p>            </div>
        </div>

    </div>
</body>
</html>
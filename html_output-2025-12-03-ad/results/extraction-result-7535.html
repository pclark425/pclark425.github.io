<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7535 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7535</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7535</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-1196cd4aa938a34105755feb47ce1610b58ea5de</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1196cd4aa938a34105755feb47ce1610b58ea5de" target="_blank">BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> A novel generative model, BindGPT, which uses a conceptually simple but powerful approach to create 3D molecules within the protein's binding site, and shows how such a simple conceptual approach combined with pre-training and scaling can perform on par or better than the current best-specialized diffusion models, language models, and graph neural networks while being two orders of magnitude cheaper to sample.</p>
                <p><strong>Paper Abstract:</strong> Generating novel active molecules for a given protein is an extremely challenging task for generative models that requires an understanding of the complex physical interactions between the molecule and its environment. This paper presents a novel generative model, BindGPT, which uses a conceptually simple but powerful approach to create 3D molecules within the protein's binding site. Our model produces molecular graphs and conformations jointly, eliminating the need for an extra graph reconstruction step. We pre-train BindGPT on a large-scale dataset and fine-tune it with reinforcement learning using scores from external simulation software. We demonstrate how a single pre-trained language model can serve at the same time as a 3D molecular generative model, a conformer generator conditioned on the molecular graph, and a pocket-conditioned 3D molecule generator. Notably, the model does not make any representational equivariance assumptions about the domain of generation. We show how such a simple conceptual approach combined with pre-training and scaling can perform on par or better than the current best-specialized diffusion models, language models, and graph neural networks while being two orders of magnitude cheaper to sample.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7535.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7535.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BindGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only language model (GPT-style) that represents 3D molecular structures as text (SMILES + XYZ) and performs joint generation of molecular graphs and conformations, conformer generation conditioned on a molecular graph, and pocket-conditioned 3D molecule generation; pretrained on large 3D datasets and fine-tuned with supervised and reinforcement learning using docking scores as external rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BindGPT (GPT-NeoX-based decoder-only LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>108M parameters (15 layers, 12 heads, hidden size 768); authors also evaluated 11M, 58M, 304M in scaling experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>base LLM pretrained on domain 3D molecular/pocket data, then supervised fine-tuned and RL-fine-tuned (domain fine-tuned + RL with external oracle); can be used with external tool assistance (RDKit) for scoring/postprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / Structure-based drug design (computational molecular design, protein-ligand binding)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation/generation of 3D small-molecule structures and conformers; pocket-conditioned generation of 3D ligand poses for given protein pockets; optimization/search of ligand designs for binding affinity (via RL using docking scores). The model generates SMILES followed by coordinate tokens (XYZ) to simulate molecular 3D structures.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Prompting by prepending context tokens: <POCKET> followed by pocket text (CA coordinates only) for pocket-conditioned generation; <LIGAND> followed by SMILES (optionally with <XYZ> coordinates) for conformation generation. Also conditioning on numerical binding energy values (as text) during finetuning (RFT). Zero-shot and conditional sampling from the pretrained/fine-tuned model; RL finetuning uses sampled molecules as responses to pocket prompts and external docking as reward. Data augmentations used as part of training prompting: SMILES randomization (many SMILES per molecule) and random 3D rotations of pocket+ligand (same rotation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Generative validity (% valid molecules), druglikeness metrics (SA score, QED, Lipinski counts), distributional metrics (Jensen-Shannon divergence on bond lengths/angles/dihedrals, ring counts, bond type frequencies), RMSD (atom-wise after alignment), RMSD-coverage (CDF P(RMSD < x)), docking binding energy (QVINA/Vina score, lower is better), time per 1000 valid molecules (s).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Generative molecule task (pretrained): Validity 98.58%, SA 0.77, QED 0.59, Lipinski 4.86, RMSD 0.89, time 13s per 1000 valid molecules (Table 1). GEOM-DRUGS finetuned distributional metrics: QED 0.616, SA 0.826, Lipinski 4.896; low JS divergences for bond lengths 0.029, bond angles 0.075, dihedrals 0.098; time for 1000 valid molecules 200s (Table 2). Conformer generation (Platinum, zero-shot) matched Torsional Diffusion when assisted by RDKit (figure summary). Pocket-conditioned generation (Table 3): BindGPT-FT Vina -5.44 ±2.09; BindGPT-RFT (context reward conditioning) Vina -7.24 ±1.68; BindGPT-RL (RL finetuned) Vina -8.60 ±1.90 with SA 0.84 ±0.05, QED 0.43 ±0.17, Lipinski 4.81 ±0.52.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared baselines include: XYZ-Transformer (pretrained) Valid 12.87% (no explicit H) and 17.86% (H), SA 0.21 (no H) / 0.54 (H), QED 0.30 / 0.37, Time 165s / 394s (Table 1). EDM and MolDiff (diffusion baselines) on GEOM-DRUGS: EDM QED 0.558, SA 0.568, MolDiff QED 0.668, SA 0.874; BindGPT QED 0.616, SA 0.826; JS metrics: EDM and MolDiff had higher JS divergences on many structural metrics whereas BindGPT reported best JS on bond lengths/angles/dihedrals and many bond-type metrics (Table 2). Pocket-conditioned baselines: Pocket2Mol Vina -7.15 ±4.89, SA 0.75 ±0.12, QED 0.57 ±0.15; TargetDiff Vina -7.80 ±3.61, SA 0.58 ±0.12 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Large-scale pretraining on specialized 3D datasets improves downstream generative and pocket-conditioned performance', 'Model size: good scaling up to ~300M parameters, overfitting observed at ~300M (they chose 108M as best given data)', 'Representation choice: coupling SMILES with XYZ (same atom ordering) greatly improves validity and removes need for external graph reconstruction (avoids instability of XYZ-only approaches)', 'Explicit hydrogens increases difficulty (larger sequences) but BindGPT can model explicit H with good validity; other models struggle', 'Data augmentations: SMILES randomization and random 3D rotations improve generalization and reduce overfitting', 'Batching and optimization: very large token batch sizes (1.6M tokens) important for stable pretraining', 'Fine-tuning strategy: supervised finetuning on CrossDocked and RL finetuning with docking rewards (REINFORCE) significantly improves pocket-binding search', 'Use of external tools (RDKit) as assistance/scoring improves conformer matching in some evaluations', 'KL penalty (distillation-style) during RL stabilizes training and helps keep output distribution diverse', 'Token weighting during SFT (higher weight on XYZ tokens) affects learning of coordinates']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Pretraining: 108M parameter decoder-only GPT-NeoX architecture, rotary position embeddings; pretrained on Uni-Mol dataset (208M conformations, 12M molecules, 3.2M pockets) for 1 epoch; large-batch training with 1.6M tokens per step, AdamW optimizer, lr max 1e-3 (warmup 2000 steps), mixed bfloat16, FlashAttention2, DeepSpeed ZeRO-3, training on 8 A6000 GPUs ~36 hours for pretraining reported. Supervised finetuning: CrossDocked expansion (~27M pocket-ligand pairs) for 1 epoch, lr 5e-4, token loss weighting (SMILES weight 1, XYZ tokens weight 5, pocket tokens weight 0). RL finetuning: distributed REINFORCE, 8 GPU workers, local batch 16, flat lr 1.4e-5, KL weight alpha=0.05, gradient clipping norm 1.0; docking oracle: QVINA/AutoDock Vina for reward evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Overfitting when scaling model beyond ~300M parameters given available pretraining data; high stochasticity of 3D coordinate tokens leads to high perplexity; XYZ-only representations require external graph reconstruction (RDKit/OpenBabel) and are unstable—small positional noise can break bonds (causing low validity); some baselines (Uni-Mol) failed to generalize zero-shot to the Platinum dataset while BindGPT required RDKit assistance to fully match top conformer baselines in some settings; RL approaches can change druglikeness metrics (e.g., QED decreased in RL model compared to FT), and RL training can be unstable with certain algorithms (PPO/REINVENT) requiring REINFORCE and KL penalty for stability; generating explicit hydrogens increases sequence length and difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7535.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7535.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XYZ-Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XYZ-transformer (as described by Flam-Shepherd & Aspuru-Guzik, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-style language model approach that represents molecular and protein spatial structures in coordinate formats (XYZ, CIF, PDB) and autoregressively generates atom-wise descriptions of 3D structures as text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XYZ-Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder LM (GPT-style) as described in cited work (not used experimentally in this paper beyond comparison/mention); authors note it was pretrained on a smaller scale</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / materials / structural biology (3D molecular and protein structures)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based generation of 3D molecular and protein structures in standard coordinate file formats (XYZ, CIF, PDB); atom-wise autoregressive description of structures.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Textual encoding of coordinates (XYZ-style) and token-level generation; specific prompting strategies not detailed in this paper beyond reference to using XYZ as a base format.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>In this paper XYZ-Transformer is reported against BindGPT on validity, SA, QED, Lipinski, and time to generate 1k molecules (Table 1); specific evaluation in original work not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Reported in this paper for pretrained comparison: Validity 12.87% (no explicit H) and 17.86% (with explicit H), SA 0.21 / 0.54, QED 0.30 / 0.37, Lipinski 4.79 / 4.82, time 165s / 394s for 1000 molecules (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>BindGPT significantly outperformed XYZ-Transformer on validity and many druglikeness metrics in the authors' pretraining comparison (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['XYZ-only coordinate representation lacks connectivity information and relies on external tools for bond reconstruction, which makes validity sensitive to coordinate noise', 'Scale of pretraining: XYZ-Transformer was noted as one of the only other models pretrained at scale in this domain but had much lower validity in these comparisons']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Not applicable in this paper beyond the comparison table; XYZ-Transformer was pretrained in its own work (cited) and reused as a baseline comparison here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Low molecular validity when using XYZ-only generation due to graph reconstruction failures; sensitive to small coordinate perturbations that change bonds or disconnect graphs; poorer performance when explicit hydrogens are included.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7535.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7535.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Uni-Mol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uni-Mol: A universal 3D molecular representation learning framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A coordinate-level encoder language-model-like framework (modified BERT) pretrained on large 3D structure datasets to perform tasks such as conformation generation and property prediction; requires initial coordinates (e.g., from RDKit) for conformation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Uni-mol: A universal 3d molecular representation learning framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Uni-Mol (modified BERT-style encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder LM (BERT-like) pretrained on 3D molecular data; not a decoder/generative LM in the same sense as BindGPT but used for conformer tasks</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / molecular representation learning / conformer generation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Coordinate-to-coordinate mapping for conformation generation given a molecular graph (requires initial coordinates for generation/encoder), and other 3D tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Coordinate-level input encoding; the model uses initialized coordinates (e.g., from RDKit) as an input to generate conformations — not framed as prompted textual generation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Compared on conformer generation (Platinum dataset) using RMSD-coverage and other conformer metrics; Uni-Mol reportedly failed to generalize zero-shot to the Platinum dataset in the authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Not quantified with specific numbers in this paper beyond relative statement that Uni-Mol failed to generalize to Platinum zero-shot while BindGPT performed better (Figure 4(b) discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Torsional Diffusion (specialized SE(3)-equivariant diffusion) was the best baseline for conformer generation; Uni-Mol did not match performance zero-shot in the authors' test.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Need for initial coordinate initialization (RDKit) for Uni-Mol to generate conformations', 'Structural diversity in test data (Platinum) caused generalization failures for Uni-Mol in zero-shot setting']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Uni-Mol evaluated in this paper’s comparisons is pretrained and finetuned on same GEOM-DRUGS data as BindGPT for fair comparison; specifics are from the cited Uni-Mol work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Fails to generalize zero-shot to structurally diverse datasets (Platinum) unless assisted by initialization tools (RDKit); requires coordinate initialization which is an external dependency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7535.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7535.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lingo3DMol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generation of 3d molecules in pockets via a language model (Lingo3DMol)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently published language-model-based approach that generates 3D molecules conditioned on protein pockets using language-model paradigms (cited as concurrent related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generation of 3d molecules in pockets via a language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Lingo3DMol (as presented in Feng et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>language model for 3D molecule-in-pocket generation (details not provided in this paper beyond citation)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / structure-based drug design (pocket-conditioned 3D molecule generation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based generation of 3D molecules placed in protein pockets (pocket-conditioned 3D molecule generation).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Not detailed in this paper beyond that it is a language-model-based generation method for pocket-conditioned tasks; cited as a concurrent work.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Not specified in this paper; likely uses docking/binding and structural metrics (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Mentioned as a concurrent LM-based approach for pocket-conditioned generation; direct factors influencing accuracy not described in this paper']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Not detailed here; refer to the cited Nature Machine Intelligence 2024 paper for specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files <em>(Rating: 2)</em></li>
                <li>Uni-mol: A universal 3d molecular representation learning framework <em>(Rating: 2)</em></li>
                <li>Generation of 3d molecules in pockets via a language model <em>(Rating: 2)</em></li>
                <li>Torsional diffusion for molecular conformer generation <em>(Rating: 1)</em></li>
                <li>Equivariant diffusion for molecule generation in 3D <em>(Rating: 1)</em></li>
                <li>MolDiff: Addressing the atom-bond inconsistency problem in 3D molecule diffusion generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7535",
    "paper_id": "paper-1196cd4aa938a34105755feb47ce1610b58ea5de",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "BindGPT",
            "name_full": "BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning",
            "brief_description": "A decoder-only language model (GPT-style) that represents 3D molecular structures as text (SMILES + XYZ) and performs joint generation of molecular graphs and conformations, conformer generation conditioned on a molecular graph, and pocket-conditioned 3D molecule generation; pretrained on large 3D datasets and fine-tuned with supervised and reinforcement learning using docking scores as external rewards.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BindGPT (GPT-NeoX-based decoder-only LM)",
            "model_size": "108M parameters (15 layers, 12 heads, hidden size 768); authors also evaluated 11M, 58M, 304M in scaling experiments",
            "model_type": "base LLM pretrained on domain 3D molecular/pocket data, then supervised fine-tuned and RL-fine-tuned (domain fine-tuned + RL with external oracle); can be used with external tool assistance (RDKit) for scoring/postprocessing",
            "scientific_domain": "Chemistry / Structure-based drug design (computational molecular design, protein-ligand binding)",
            "simulation_task_description": "Text-based simulation/generation of 3D small-molecule structures and conformers; pocket-conditioned generation of 3D ligand poses for given protein pockets; optimization/search of ligand designs for binding affinity (via RL using docking scores). The model generates SMILES followed by coordinate tokens (XYZ) to simulate molecular 3D structures.",
            "prompting_strategy": "Prompting by prepending context tokens: &lt;POCKET&gt; followed by pocket text (CA coordinates only) for pocket-conditioned generation; &lt;LIGAND&gt; followed by SMILES (optionally with &lt;XYZ&gt; coordinates) for conformation generation. Also conditioning on numerical binding energy values (as text) during finetuning (RFT). Zero-shot and conditional sampling from the pretrained/fine-tuned model; RL finetuning uses sampled molecules as responses to pocket prompts and external docking as reward. Data augmentations used as part of training prompting: SMILES randomization (many SMILES per molecule) and random 3D rotations of pocket+ligand (same rotation).",
            "evaluation_metric": "Generative validity (% valid molecules), druglikeness metrics (SA score, QED, Lipinski counts), distributional metrics (Jensen-Shannon divergence on bond lengths/angles/dihedrals, ring counts, bond type frequencies), RMSD (atom-wise after alignment), RMSD-coverage (CDF P(RMSD &lt; x)), docking binding energy (QVINA/Vina score, lower is better), time per 1000 valid molecules (s).",
            "reported_accuracy": "Generative molecule task (pretrained): Validity 98.58%, SA 0.77, QED 0.59, Lipinski 4.86, RMSD 0.89, time 13s per 1000 valid molecules (Table 1). GEOM-DRUGS finetuned distributional metrics: QED 0.616, SA 0.826, Lipinski 4.896; low JS divergences for bond lengths 0.029, bond angles 0.075, dihedrals 0.098; time for 1000 valid molecules 200s (Table 2). Conformer generation (Platinum, zero-shot) matched Torsional Diffusion when assisted by RDKit (figure summary). Pocket-conditioned generation (Table 3): BindGPT-FT Vina -5.44 ±2.09; BindGPT-RFT (context reward conditioning) Vina -7.24 ±1.68; BindGPT-RL (RL finetuned) Vina -8.60 ±1.90 with SA 0.84 ±0.05, QED 0.43 ±0.17, Lipinski 4.81 ±0.52.",
            "baseline_accuracy": "Compared baselines include: XYZ-Transformer (pretrained) Valid 12.87% (no explicit H) and 17.86% (H), SA 0.21 (no H) / 0.54 (H), QED 0.30 / 0.37, Time 165s / 394s (Table 1). EDM and MolDiff (diffusion baselines) on GEOM-DRUGS: EDM QED 0.558, SA 0.568, MolDiff QED 0.668, SA 0.874; BindGPT QED 0.616, SA 0.826; JS metrics: EDM and MolDiff had higher JS divergences on many structural metrics whereas BindGPT reported best JS on bond lengths/angles/dihedrals and many bond-type metrics (Table 2). Pocket-conditioned baselines: Pocket2Mol Vina -7.15 ±4.89, SA 0.75 ±0.12, QED 0.57 ±0.15; TargetDiff Vina -7.80 ±3.61, SA 0.58 ±0.12 (Table 3).",
            "factors_reported": [
                "Large-scale pretraining on specialized 3D datasets improves downstream generative and pocket-conditioned performance",
                "Model size: good scaling up to ~300M parameters, overfitting observed at ~300M (they chose 108M as best given data)",
                "Representation choice: coupling SMILES with XYZ (same atom ordering) greatly improves validity and removes need for external graph reconstruction (avoids instability of XYZ-only approaches)",
                "Explicit hydrogens increases difficulty (larger sequences) but BindGPT can model explicit H with good validity; other models struggle",
                "Data augmentations: SMILES randomization and random 3D rotations improve generalization and reduce overfitting",
                "Batching and optimization: very large token batch sizes (1.6M tokens) important for stable pretraining",
                "Fine-tuning strategy: supervised finetuning on CrossDocked and RL finetuning with docking rewards (REINFORCE) significantly improves pocket-binding search",
                "Use of external tools (RDKit) as assistance/scoring improves conformer matching in some evaluations",
                "KL penalty (distillation-style) during RL stabilizes training and helps keep output distribution diverse",
                "Token weighting during SFT (higher weight on XYZ tokens) affects learning of coordinates"
            ],
            "experimental_conditions": "Pretraining: 108M parameter decoder-only GPT-NeoX architecture, rotary position embeddings; pretrained on Uni-Mol dataset (208M conformations, 12M molecules, 3.2M pockets) for 1 epoch; large-batch training with 1.6M tokens per step, AdamW optimizer, lr max 1e-3 (warmup 2000 steps), mixed bfloat16, FlashAttention2, DeepSpeed ZeRO-3, training on 8 A6000 GPUs ~36 hours for pretraining reported. Supervised finetuning: CrossDocked expansion (~27M pocket-ligand pairs) for 1 epoch, lr 5e-4, token loss weighting (SMILES weight 1, XYZ tokens weight 5, pocket tokens weight 0). RL finetuning: distributed REINFORCE, 8 GPU workers, local batch 16, flat lr 1.4e-5, KL weight alpha=0.05, gradient clipping norm 1.0; docking oracle: QVINA/AutoDock Vina for reward evaluation.",
            "limitations_or_failure_modes": "Overfitting when scaling model beyond ~300M parameters given available pretraining data; high stochasticity of 3D coordinate tokens leads to high perplexity; XYZ-only representations require external graph reconstruction (RDKit/OpenBabel) and are unstable—small positional noise can break bonds (causing low validity); some baselines (Uni-Mol) failed to generalize zero-shot to the Platinum dataset while BindGPT required RDKit assistance to fully match top conformer baselines in some settings; RL approaches can change druglikeness metrics (e.g., QED decreased in RL model compared to FT), and RL training can be unstable with certain algorithms (PPO/REINVENT) requiring REINFORCE and KL penalty for stability; generating explicit hydrogens increases sequence length and difficulty.",
            "uuid": "e7535.0",
            "source_info": {
                "paper_title": "BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "XYZ-Transformer",
            "name_full": "XYZ-transformer (as described by Flam-Shepherd & Aspuru-Guzik, 2023)",
            "brief_description": "A GPT-style language model approach that represents molecular and protein spatial structures in coordinate formats (XYZ, CIF, PDB) and autoregressively generates atom-wise descriptions of 3D structures as text.",
            "citation_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files",
            "mention_or_use": "mention",
            "model_name": "XYZ-Transformer",
            "model_size": null,
            "model_type": "decoder LM (GPT-style) as described in cited work (not used experimentally in this paper beyond comparison/mention); authors note it was pretrained on a smaller scale",
            "scientific_domain": "Chemistry / materials / structural biology (3D molecular and protein structures)",
            "simulation_task_description": "Text-based generation of 3D molecular and protein structures in standard coordinate file formats (XYZ, CIF, PDB); atom-wise autoregressive description of structures.",
            "prompting_strategy": "Textual encoding of coordinates (XYZ-style) and token-level generation; specific prompting strategies not detailed in this paper beyond reference to using XYZ as a base format.",
            "evaluation_metric": "In this paper XYZ-Transformer is reported against BindGPT on validity, SA, QED, Lipinski, and time to generate 1k molecules (Table 1); specific evaluation in original work not detailed here.",
            "reported_accuracy": "Reported in this paper for pretrained comparison: Validity 12.87% (no explicit H) and 17.86% (with explicit H), SA 0.21 / 0.54, QED 0.30 / 0.37, Lipinski 4.79 / 4.82, time 165s / 394s for 1000 molecules (Table 1).",
            "baseline_accuracy": "BindGPT significantly outperformed XYZ-Transformer on validity and many druglikeness metrics in the authors' pretraining comparison (Table 1).",
            "factors_reported": [
                "XYZ-only coordinate representation lacks connectivity information and relies on external tools for bond reconstruction, which makes validity sensitive to coordinate noise",
                "Scale of pretraining: XYZ-Transformer was noted as one of the only other models pretrained at scale in this domain but had much lower validity in these comparisons"
            ],
            "experimental_conditions": "Not applicable in this paper beyond the comparison table; XYZ-Transformer was pretrained in its own work (cited) and reused as a baseline comparison here.",
            "limitations_or_failure_modes": "Low molecular validity when using XYZ-only generation due to graph reconstruction failures; sensitive to small coordinate perturbations that change bonds or disconnect graphs; poorer performance when explicit hydrogens are included.",
            "uuid": "e7535.1",
            "source_info": {
                "paper_title": "BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Uni-Mol",
            "name_full": "Uni-Mol: A universal 3D molecular representation learning framework",
            "brief_description": "A coordinate-level encoder language-model-like framework (modified BERT) pretrained on large 3D structure datasets to perform tasks such as conformation generation and property prediction; requires initial coordinates (e.g., from RDKit) for conformation generation.",
            "citation_title": "Uni-mol: A universal 3d molecular representation learning framework",
            "mention_or_use": "mention",
            "model_name": "Uni-Mol (modified BERT-style encoder)",
            "model_size": null,
            "model_type": "encoder LM (BERT-like) pretrained on 3D molecular data; not a decoder/generative LM in the same sense as BindGPT but used for conformer tasks",
            "scientific_domain": "Chemistry / molecular representation learning / conformer generation",
            "simulation_task_description": "Coordinate-to-coordinate mapping for conformation generation given a molecular graph (requires initial coordinates for generation/encoder), and other 3D tasks.",
            "prompting_strategy": "Coordinate-level input encoding; the model uses initialized coordinates (e.g., from RDKit) as an input to generate conformations — not framed as prompted textual generation in this paper.",
            "evaluation_metric": "Compared on conformer generation (Platinum dataset) using RMSD-coverage and other conformer metrics; Uni-Mol reportedly failed to generalize zero-shot to the Platinum dataset in the authors' experiments.",
            "reported_accuracy": "Not quantified with specific numbers in this paper beyond relative statement that Uni-Mol failed to generalize to Platinum zero-shot while BindGPT performed better (Figure 4(b) discussion).",
            "baseline_accuracy": "Torsional Diffusion (specialized SE(3)-equivariant diffusion) was the best baseline for conformer generation; Uni-Mol did not match performance zero-shot in the authors' test.",
            "factors_reported": [
                "Need for initial coordinate initialization (RDKit) for Uni-Mol to generate conformations",
                "Structural diversity in test data (Platinum) caused generalization failures for Uni-Mol in zero-shot setting"
            ],
            "experimental_conditions": "Uni-Mol evaluated in this paper’s comparisons is pretrained and finetuned on same GEOM-DRUGS data as BindGPT for fair comparison; specifics are from the cited Uni-Mol work.",
            "limitations_or_failure_modes": "Fails to generalize zero-shot to structurally diverse datasets (Platinum) unless assisted by initialization tools (RDKit); requires coordinate initialization which is an external dependency.",
            "uuid": "e7535.2",
            "source_info": {
                "paper_title": "BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Lingo3DMol",
            "name_full": "Generation of 3d molecules in pockets via a language model (Lingo3DMol)",
            "brief_description": "A recently published language-model-based approach that generates 3D molecules conditioned on protein pockets using language-model paradigms (cited as concurrent related work).",
            "citation_title": "Generation of 3d molecules in pockets via a language model",
            "mention_or_use": "mention",
            "model_name": "Lingo3DMol (as presented in Feng et al., 2024)",
            "model_size": null,
            "model_type": "language model for 3D molecule-in-pocket generation (details not provided in this paper beyond citation)",
            "scientific_domain": "Chemistry / structure-based drug design (pocket-conditioned 3D molecule generation)",
            "simulation_task_description": "Text-based generation of 3D molecules placed in protein pockets (pocket-conditioned 3D molecule generation).",
            "prompting_strategy": "Not detailed in this paper beyond that it is a language-model-based generation method for pocket-conditioned tasks; cited as a concurrent work.",
            "evaluation_metric": "Not specified in this paper; likely uses docking/binding and structural metrics (citation only).",
            "reported_accuracy": null,
            "baseline_accuracy": null,
            "factors_reported": [
                "Mentioned as a concurrent LM-based approach for pocket-conditioned generation; direct factors influencing accuracy not described in this paper"
            ],
            "experimental_conditions": "Not detailed here; refer to the cited Nature Machine Intelligence 2024 paper for specifics.",
            "limitations_or_failure_modes": null,
            "uuid": "e7535.3",
            "source_info": {
                "paper_title": "BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files",
            "rating": 2
        },
        {
            "paper_title": "Uni-mol: A universal 3d molecular representation learning framework",
            "rating": 2
        },
        {
            "paper_title": "Generation of 3d molecules in pockets via a language model",
            "rating": 2
        },
        {
            "paper_title": "Torsional diffusion for molecular conformer generation",
            "rating": 1
        },
        {
            "paper_title": "Equivariant diffusion for molecule generation in 3D",
            "rating": 1
        },
        {
            "paper_title": "MolDiff: Addressing the atom-bond inconsistency problem in 3D molecule diffusion generation",
            "rating": 1
        }
    ],
    "cost": 0.015238999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning</h1>
<p>Artem Zholus ${ }^{1,3,4}$, Maksim Kuznetsov ${ }^{1}$, Roman Schutski ${ }^{2}$, Rim Shayakhmetov ${ }^{1}$, Daniil Polykovskiy ${ }^{1}$, Sarath Chandar ${ }^{3,4,5}$, and Alex Zhavoronkov ${ }^{2}$<br>${ }^{1}$ Insilico Medicine Canada Inc.<br>${ }^{2}$ Insilico Medicine AI Limited<br>${ }^{3}$ Mila - Quebec AI Institute<br>${ }^{4}$ Polytechnique Montréal<br>${ }^{5}$ CIFAR AI Chair</p>
<h4>Abstract</h4>
<p>Generating novel active molecules for a given protein is an extremely challenging task for generative models that requires an understanding of the complex physical interactions between the molecule and its environment. In this paper, we present a novel generative model, BindGPT which uses a conceptually simple but powerful approach to create 3D molecules within the protein's binding site. Our model produces molecular graphs and conformations jointly, eliminating the need for an extra graph reconstruction step. We pretrain BindGPT on a large-scale dataset and fine-tune it with reinforcement learning using scores from external simulation software. We demonstrate how a single pretrained language model can serve at the same time as a 3D molecular generative model, conformer generator conditioned on the molecular graph, and a pocket-conditioned 3D molecule generator. Notably, the model does not make any representational equivariance assumptions about the domain of generation. We show how such simple conceptual approach combined with pretraining and scaling can perform on par or better than the current best specialized diffusion models, language models, and graph neural networks while being two orders of magnitude cheaper to sample.</p>
<h2>1 Introduction</h2>
<p>The landscape of drug discovery presents immense challenges and risks, demanding substantial investments of time and resources to design, test, and deliver new medicines to the market. Within this context, Computer-Aided Drug Design (CADD) (Yu \&amp; MacKerell, 2017) stands as a pivotal methodology, harnessing software screenings and physical simulations to facilitate a more efficient exploration of the vast space of drug-like molecules, estimated to be around $10^{60}$ in size (Polishchuk et al., 2015; Gómez-Bombarelli et al., 2018). Deep learning advancements have revolutionized this exploration by leveraging neural generative models trained on extensive compound datasets. Notably, the textual representation of molecular structures using SMILES (Weininger, 1988) and SELFIES Krenn et al. (2020) has enabled the utilization of Language Models for the generation of novel, drug-like molecular compounds (Segler et al., 2018; Bagal et al., 2022).
Recent research has demonstrated the capability of deep generative models to generate novel molecular compounds directly in 3D, with the flexibility to incorporate protein pocket and ligand subfragment conditions. Among these, diffusion models such as EDM (Hoogeboom et al., 2022) and DiffDock (Corso et al., 2023) initiate the generation process with an arbitrary spatial distribution of atoms and progressively refine their positions to yield physically</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The pipeline of our 3D molecule pretraining-finetuning paradigm. During pretraining the model is trained on a mix of molecules and pockets in isolation. But at finetuning, we simply concatenate the pocket text representation and the molecule text representation for each pocket-ligand pair.
viable molecular structures. Meanwhile, autoregressive models like Pocket2Mol (Peng et al., 2022) sequentially predict the type and location of each successive atom, building upon the existing molecular framework. Additionally, work by Flam-Shepherd \&amp; Aspuru-Guzik (2023) has highlighted the proficiency of language models in handling spatial representations of molecular and protein structures through formats like XYZ, CIF, and PDB. However, it's noteworthy that most spatial molecular generators focus exclusively on atom types and locations. They depend on supplementary tools, such as OpenBabel (O'Boyle et al., 2011), for the critical task of bond reconstruction. This reliance can introduce vulnerabilities, as the precision required for atom placement means that minor positional adjustments can significantly alter reconstructed molecular bonds or even make the molecular graph disconnected.</p>
<p>In this work, we introduce a novel framework that applies language modeling to the domain of 3D molecular data represented by textual tokens. This entirely data-driven approach, devoid of any inductive biases at both the model and representation levels, capitalizes on the established GPT paradigm, integrating cutting-edge techniques to enhance the scalability of model training and inference. By adopting the language model pretraining paradigm, our framework showcases the ability to foster a powerful causal language model adept at navigating the complex space of 3D molecules. This proficiency is demonstrated through successful applications in downstream tasks, including learning the distribution of 3D molecules, generating 3D conformations and the generation of molecules with targeted binding affinity to specific proteins.
Our main contributions are the following:</p>
<ul>
<li>We introduce BindGPT, a Language Model for handling spatial molecular structures in text format. It uses structural SMILES and spatial XYZ formats to describe molecular graphs and atom locations, eliminating the dependency on external software for graph reconstruction.</li>
<li>We propose scalable pretraining-finetuning method for drug discovery in 3D that covers several 3D molecular generation tasks in a single paradigm.</li>
<li>We show how BindGPT can create accurate and realistic 3D molecular structures both zero-shot and after finetuning, with the option to include molecular graphs or protein pocket descriptions as prompts. The method offers comparable generation quality to leading approaches with the speedup of up to 100x.</li>
<li>Finally, we demonstrate the effectiveness of the Reinforcement Learning framework to finetune BindGPT with an external feedback from docking software. We show that the resulting model can find structures with high binding scores for any given protein as a result for the RL finetuning.</li>
</ul>
<h1>2 Background</h1>
<p>Molecule Generation Small drug-like molecules can be represented as 2D or 3D graphs with node and edge attributes. However, one of the most popular molecular representation in the machine learning community is SMILES (Weininger, 1988), which can be seen as a compressed textual encoding of the Depth-First-Search applied to the molecular graph. It's simplicity and expressivity made it work very well with language models - even a simple LSTM (Hochreiter \&amp; Schmidhuber, 1997) model can outperform graph-neural networks for the molecule generation task (Flam-Shepherd et al., 2022). In addition, SELFIES (Krenn et al., 2022), is a modification of SMILES which is a robust string representation such that every SELFIES token sequence is a valid molecule and vice versa.
The biological function of small molecules arises through their binding to specific protein pockets. The spatial structure of the protein pocket is an essential domain knowledge to increase the efficiency of molecular generation in drug design tasks. With the increase of molecular structure datasets sizes (Francoeur et al., 2020; Hu et al., 2005) a plethora of pocket-conditioned generators emerged (Peng et al., 2022; Luo et al., 2021; Lin et al., 2022; Corso et al., 2023). The challenge with pocket-conditioned molecular generation arises from a relatively small size of existing 3D binding poses datasets, which motivated a heavy use of specialized architectures, like SE(3) equivariant neural networks (Hoogeboom et al., 2022).
Molecular Generative Models in 3D. Apart from Language Models, there exist other types of generative models that approach molecule generation, including 3D-aware ones. The first group of works uses Diffusion Models, which employ the denoising diffusion process (Ho et al., 2020; Song et al., 2021) to learn to recover the data from noise. The second group of works relies on Graph Neural Networks (GNNs) to autoregressively build 2D or 3D molecular graphs. These approaches can be combined as GNNs can serve as efficient backbones for diffusion process once they are node-equivariant (Niu et al., 2020) (to generate 2D graphs) or SE(3)-equivariant (Peng et al., 2023b) (to generate 3D graphs). SBDD (Luo et al., 2021) model uses autoregressive graph generation for pocket-conditioned molecule generation. TargetDiff (Schneuing et al., 2023) generalizes this model to use a diffusionGNN for the same task. Pocket2Mol (Peng et al., 2022) uses an informed autoregressive sampling mechanism for efficient pocket-conditioned molecule generation. Another batch of works use the aforementioned methods for the unconditional molecule generation. EDM (Hoogeboom et al., 2022) proposes an E(3) equivariant diffusion model for molecule generation. MolDiff (Peng et al., 2023b) is a diffusion model that addresses the inconsistency problem between generated atoms and bonds connecting them.
Language Models for Drug Discovery. The language models show outstanding results in drug discovery domain. The molecular structures can be easily represented in a textual formats like SMILES (Segler et al., 2018) or SELFIES (Flam-Shepherd et al., 2022), enabling the effective training of well-known language model architectures on large datasets of chemical entities. Recent studies reveals the potential of applying language models to address various challenges in drug discovery. For instance, LigGPT (Bagal et al., 2022) leverages the GPT (Radford et al., 2018) architecture to generate molecular structures given the conditions of molecular descriptors. MoLFormer (Katharopoulos et al., 2020) incorporates billion-size chemical entities database to perform large-scale pretrain and further finetune to predict molecular properties. BARTSmiles (Chilingaryan et al., 2022) is developed atop of BART (Lewis et al., 2019) architecture, training a meaningful chemical representation and refining it for tasks such as chemical property prediction, chemical reaction prediction, and retrosynthesis.
However, the challenge of 3D molecule generation has received limited attention in language model approach. Three notable studies in this area include the XYZ-transformer (FlamShepherd \&amp; Aspuru-Guzik, 2023), Uni-Mol (Zhou et al., 2023), and Lingo3DMol (Feng et al., 2024). The XYZ-transformer leverages GPT for generating atom-wise description of molecular and protein structures. Uni-Mol modifies BERT for large-scale pretraining on a large 3D structures dataset. In particular, Uni-Mol formulates molecular tasks as coordinates-to-coordinates mapping given molecular graph. To the best of our knowledge, we propose the first approach that applies the modern decoder-only language modeling paradigm to the 3D drug discovery problem with several downstream applications.</p>
<h1>3 Method</h1>
<p>The key idea of our method is utilizing an autoregressive token generation model, influenced by GPT-based models, to solve several 3D small molecule generation tasks in one simple yet flexible paradigm. The main principle in our approach is to formulate several 3D molecular design task as prompted generation of text. To achieve that, we layout the tokens of a condition before the tokens of the object to generate. For instance, a prompt can be the protein pocket for the pocket-conditioned generation task or the 2D molecular structure for the conformation generation task.</p>
<h3>3.1 Architecture and text format</h3>
<p>In our work we follow the decoderonly paradigm in Language Models and use the GPT-NeoX ${ }^{\top}$ architecture (Black et al., 2022) that utilizes rotary position embeddings (Su et al., 2021). This technique allows for the length generalization, which is required since the sequence lengths may vary significantly between the pretraining, and fine-tuning stages.
Similarly to Flam-Shepherd \&amp; AspuruGuzik (2023), we use the XYZ representation as a base format to describe the
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Data layout during the pretraining. Arrows show the tokens sequence order. Nodes such as <POCKET> show special tokens. Training is done on a mixture of pocket and ligand datasets. Concrete examples are shown in Appendix A.
spatial atom allocation. The idea of XYZ format is to represent the atom type and its 3D coordinates within every line on text. The main drawback of this format is the lack of charge and connectivity information. One should use external software like RDKit (Landrum et al., 2024) or OpenBabel (O'Boyle et al., 2011) to reconstruct the molecular graph. It introduces an instability since even small noise in atom positions can drastically change the reconstructed graph or even break it down (Peng et al., 2023a). To alleviate that, we propose to couple the XYZ format with the SMILES format. The latter can efficiently represent the molecular structure, while the former allows describing atom positions. To align these two formats we enforce to have the same atom ordering in both. We also remove the atom symbol from the XYZ representation as it already was shown in SMILES. For proteins, there is no need to describe their connectivity, therefore we simply write atom names grouped by aminoacids.
A schematic example of the two kind of the model input is shown in Figure 2 and the very detailed example of concrete input sequences (including its tokenization) is shown in Figure 6 in Appendix A. In particular, the sequence starts with the <LIGAND> token followed by a SMILES string, tokenized at the character level. Next, there goes the <XYZ> special token marking the end of SMILES and the beginning of the coordinate part of the string. The tokenization strategy uses 6 tokens per 3D position: we use one token for the integer part and one token for the fractional part of the number. When working with protein pockets, we use a similar strategy. Specifically, the sequence begins with the <POCKET> token followed by the sequence of atoms where each atom is a separate token. Since pockets can be hundreds of atoms large, we follow the AlphaFold's (Jumper et al., 2021) approach and retain only the 3D coordinates of the Alpha-carbon atoms in the corresponding aminoacids. An example of the final representation of pockets is shown in Figure 8.</p>
<h3>3.2 Pretraining</h3>
<p>In this work, we aim to leverage insights accumulated by the NLP community in the paradigm of Large Language Models: pretraining-finetuning, prompting, scaling, finetuning with Reinforcement Learning, tool use, etc.(Kaplan et al., 2020; Hoffmann et al., 2022;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Radford et al., 2019). Since our model covers only a specialized domain of molecular tasks, it does not require trillion-scale diverse datasets for good performance as NLP tasks do. Thus, we use a large-scale but specialized dataset of 3D molecules and protein pockets. During pretraining, we use the model with 108M parameters consisting of 15 layers, 12 heads, and a hidden size of 768 . We found this size of the model to be enough for the tasks we care about - generating molecules in 3D (See Appendix C for a justification of the size). Every sequence in the training batch is either a ligand sequence of tokens or a pocket sequence of tokens following the scheme described earlier. Since the dataset has much fewer pockets than ligands, for one epoch of training on ligands, we do 5 epochs of training on proteins, that is, around $8 \%$ of all tokens seen by the model are pocket tokens. To speedup and stabilize pretraining, we use large batch training (Keskar et al., 2017) with 1.6M tokens per one training step. We found this many tokens per batch to be important for stable training in this task even with smaller learning rates. The detailed description of the training implementation is provided in Appendix G. Despite the wide use of transformers in drug discovery, the majority of current works in this space do not use recent advancements of efficient Language Models pretraining: neither technical ones, such as Flash-attention (Dao, 2023) or DeepSpeed (Rasley et al., 2020), nor the algorithimic ones, such as learning rate scaling. Our work aims to fill this gap by demonstrating the effectiveness of the pretraining for 3D drug discovery.</p>
<h1>3.3 Finetuning</h1>
<h3>3.3.1 Supervised finetuning</h3>
<p>As a result of the pretraining, BindGPT gains an understanding of a broad chemical space. This comprehensive understanding enables us to efficiently narrow it down through the supervised fine-tuning on a specialized dataset. During the supervised fine-tuning phase, we continue model training on CrossDocked 2020(Francoeur et al., 2020), which is a high-quality dataset containing aligned pocket-ligand pairs. Most of the prior methods subsample less than $1 \%$ of the best pocket-ligand pairs and they don't benefit from it's diversity and scale. To obtain a bigger version of CrossDocked, we extract all intermediate ligand poses (with respect to the docking process), including the lower quality ones. Despite quite large size, CrossDocked was created by docking 14 k unique molecules into 3 k pockets (Francoeur et al., 2020). This is why we observed an dramatic overfitting when training on the $1 \%$ version of CrossDocked and even on the full one. To alleviate that, we resort to two standard augmentation techniques used in drug discovery. First, we employ SMILES randomization (Bjerrum, 2017), which can heavily randomize one molecule by yielding 100-1000 different SMILES strings (all corresponding to that molecule). Second, we randomly rotate the 3D coordinates of the protein pocket and of the ligand (with the same rotation matrix). This way our model learns to understand structural and spatial properties of molecular binding beyond just token sequences.
Since the pretrained BindGPT is trained on both ligands (starting from the <LIGAND> token) and pockets (starting from the <POCKET> token), the information about the structure of both is learned by the model. In our finetuning setup, we represent each pocket-ligand pair as a</p>
<p>string starting with the pocket string representation followed by the string representation of the ligand (See Section 3.1 for their description). Therefore, having learned them separately during pretraining, the finetuning exploits the independent knowledge of both pockets and ligands to learn a conditional dependency between them. In addition to that, since our version of CrossDocked contains both high and low score conformations, we test another version of context where we condition on the pocket and binding energy score obtained from the CrossDocked dataset (which is originally computed through the docking software (Trott \&amp; Olson, 2010; Eberhardt et al., 2021)). This way we can perform a variant of contrastive learning by learning the structure of good and bad examples. During evaluation of the model, we can sample molecules conditioned on some desired value of the binding affinity. The input layout for both versions is shown in Figure 3.</p>
<h1>3.3.2 Reinforcement Learning</h1>
<p>Despite the ubiquitous use of Reinforcement Learning (RL) for language models in Drug Discovery (see Section 2 and Appendix G), we did not find it been used within the pretraining paradigm of modern LLMs (Hoffmann et al., 2022; Kaplan et al., 2020; Ouyang et al., 2022). Our main motivation to use RL after the pretraining/finetuning stages is to use the knowledge distilled into the model from massive amounts of less structured data. We believe, this is the first work performing reinforcement learning on molecules that utilizes knowledge from pretraining and supervised finetuning. Despite there are dozens of works doing RL with LMs on molecules, none of them do that within the LLM paradigm and none of them consider target-conditioned RL problem. In our opinion, the latter is primarily due to pocket-conditioned generation is not possible without large-scale pretraining as we show in the experimental section.
We apply the REINFORCE algorithm (Williams, 1992) for further model finetuning. It allows using the feedback (called reward) from an external oracle to train model to generate even better structures compared to the ones it generates after the SFT stage. The resulting RL-finetuned model can generalize model and produce high affinity molecules even for the new pockets. In our procedure, on each training step we generate 3D structure of ligands for a batch of random protein pockets. Then we compute the reward using an external docking software that estimates the binding energy between the pocket and the generated ligand. The final step involves updating the language model with the batch of prompts (pockets), responses (ligands), and rewards (binding energies). We initially tested PPO (Schulman et al., 2017) and REINVENT (Oliwecrona et al., 2017), but found REINFORCE to be more stable for our project, which aligns with another recent finding in the field of RL applied to language models in NLP (Ahmadian et al., 2024). Also, it's important to mention, that we apply the KL-penalty between the model's initialized and current state to stabilize the procedure. Further details such as hyperparameters can be found in the Appendix B.</p>
<h2>4 Results</h2>
<p>In this section, we describe our experimental results. We start with a brief data description followed by the description of the three 3D molecular generative tasks: the 3D generative modeling of molecules and conformation generation given molecular graph (in Section 4.1) and then pocket-conditioned generation (in Section 4.2).
For pretraining, we use a large 3D molecular dataset proposed by the authors of the Uni-Mol model (Zhou et al., 2023). The dataset contains 208 M conformations for 12 M molecules and 3.2 M spatial structures of protein pockets. For finetuning in the pocket-conditioned generation task, we use the aforementioned CrossDocked dataset which contains aligned pocket-molecule pairs. Our filtration of the dataset has around 27 M pocket-ligand pairs covering a cross product of 14 k molecules with 3 k pockets (not all of the pairs are present, and for some of them, there is more than one pose, each with different score). We also hold out a set of 100 pockets from the training data for evaluating the model performance. For the tasks of 3D molecule and 3D conformer generation (Section 4.1), to make comparisons with baselines more fair, we also finetune the model on the GEOM-DRUGS (Axelrod \&amp; GómezBombarelli, 2022) dataset, with drug-like molecules having high-quality 3D molecular</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: (left) Sampled conformations for reference molecules from the Platinum dataset (See Appendix E for a higher resolution image) (right) RMSD Coverage metric calculated on the Platinum dataset for the 3D conformation generation task.
conformations. This dataset contains 27 M conformations for 300 k molecules, and it serves as a standard benchmark for the machine learning-based 3D molecular generators. Finally, we use the Platinum (Friedrich et al., 2017) dataset as a hold-out evaluation dataset to test our model and baselines on zero-shot conformer generation. Platinum dataset contains the best-in-class experimentally validated conformations for testing conformer generation software.</p>
<h1>4.1 Generative Modeling of 3D molecules</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">method</th>
<th style="text-align: center;">Valid $(\uparrow)$</th>
<th style="text-align: center;">SA $(\uparrow)$</th>
<th style="text-align: center;">QED $(\uparrow)$</th>
<th style="text-align: center;">Lipinski $(\uparrow)$</th>
<th style="text-align: center;">RMSD $(\downarrow)$</th>
<th style="text-align: center;">Time,s $(\downarrow)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">XYZ-TF</td>
<td style="text-align: center;">$12.87 \%$</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">4.79</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">165</td>
</tr>
<tr>
<td style="text-align: left;">BindGPT (Ours)</td>
<td style="text-align: center;">$\mathbf{9 8 . 5 8 \%}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 9}$</td>
<td style="text-align: center;">$\mathbf{4 . 8 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 9}$</td>
<td style="text-align: center;">$\mathbf{1 3}$</td>
</tr>
<tr>
<td style="text-align: left;">XYZ-TF $(\mathrm{H})$</td>
<td style="text-align: center;">$17.86 \%$</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">4.82</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">394</td>
</tr>
<tr>
<td style="text-align: left;">BindGPT $(\mathrm{H})$ (Ours)</td>
<td style="text-align: center;">$\mathbf{7 7 . 3 3 \%}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 1}$</td>
<td style="text-align: center;">$\mathbf{4 . 9 1}$</td>
<td style="text-align: center;">$\mathbf{3 . 4 4}$</td>
<td style="text-align: center;">$\mathbf{1 5 6}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Generative metrics for the molecule generation task after the pretraining. (H) is explicit hydrogens are generated with molecules. For XYZ-TF, the RMSD calculation algorithm failed to converge.</p>
<p>Metrics. We provide the validity $(\uparrow)$ of generated molecules and druglikeness metrics - SA $(\uparrow)$, QED $(\uparrow)$, and Lipinski $(\uparrow)$ that are agnostic to 3D but measure how likely the molecule to be a drug. Also, we adopt a range of distribution metrics that were used for the MolDiff method (Peng et al., 2023a). Those metrics measure the discrepancy between true and modelled molecular distributions by computing the Jensen-Shannon divergences on the set of molecular properties and features distributions. We compute RMSD (Root-Mean-Squared-Distance) $(\downarrow)$ - which measures the quality of 3D structures by aligning the generated one with the one from RDkit (i.e., we regenerate conformer via RDkit) and computing the atomwise distance. Finally, we measure the time needed to generate 1 K valid 3D molecules on one GPU. Note that this choice of metrics is standard for this task (see Peng et al. (2023a) for a more detailed description of them). For the 3D conformation generation given molecule task, we compute the RMSD-coverage $(\uparrow)$ metric. This is a standard performance metric for 3D conformer generation models (see e.g. Jing et al. (2022)). It is represented by the cumulative distribution function of RMSD between generated and reference conformers. The metric is a function of the threshold $x: P(\operatorname{RMSD}&lt;x)$. An ideal model should have as high metric value as possible for as low thresholds as possible.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Group</th>
<th style="text-align: center;">Metrics</th>
<th style="text-align: center;">EDM</th>
<th style="text-align: center;">MolDiff</th>
<th style="text-align: center;">BindGPT <br> (Ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Druglikeness</td>
<td style="text-align: center;">QED $(\uparrow)$</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">$\mathbf{0 . 6 6 8}$</td>
<td style="text-align: center;">0.616</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SA $(\uparrow)$</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 4}$</td>
<td style="text-align: center;">0.826</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lipinski $(\uparrow)$</td>
<td style="text-align: center;">4.923</td>
<td style="text-align: center;">$\mathbf{4 . 9 8 6}$</td>
<td style="text-align: center;">4.896</td>
</tr>
<tr>
<td style="text-align: center;">3D structures</td>
<td style="text-align: center;">JS. bond lengths $(\downarrow)$</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">$\mathbf{0 . 0 2 9}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">JS. bond angles $(\downarrow)$</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.155</td>
<td style="text-align: center;">$\mathbf{0 . 0 7 5}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">JS. dihedral angles $(\downarrow)$</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.162</td>
<td style="text-align: center;">$\mathbf{0 . 0 9 8}$</td>
</tr>
<tr>
<td style="text-align: center;">Bonds</td>
<td style="text-align: center;">JS. num. bonds per atoms $(\downarrow)$</td>
<td style="text-align: center;">0.139</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 5}$</td>
<td style="text-align: center;">0.160</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">JS. freq. bond types $(\downarrow)$</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">$\mathbf{0 . 0 4 5}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">JS. freq. bond pairs $(\downarrow)$</td>
<td style="text-align: center;">0.396</td>
<td style="text-align: center;">0.136</td>
<td style="text-align: center;">$\mathbf{0 . 0 4 3}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">JS. freq. bond triplets $(\downarrow)$</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">$\mathbf{0 . 0 4 2}$</td>
</tr>
<tr>
<td style="text-align: center;">Rings</td>
<td style="text-align: center;">JS. num. rings $(\downarrow)$</td>
<td style="text-align: center;">0.106</td>
<td style="text-align: center;">$\mathbf{0 . 0 6 2}$</td>
<td style="text-align: center;">0.094</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">JS. num. n-sized rings $(\downarrow)$</td>
<td style="text-align: center;">0.107</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">$\mathbf{0 . 0 2 3}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Num. Intersecting rings $(\uparrow)$</td>
<td style="text-align: center;">3.667</td>
<td style="text-align: center;">8.000</td>
<td style="text-align: center;">$\mathbf{9 . 0 0 0}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Time for 1000 valid molecules, s $(\downarrow)$</td>
<td style="text-align: center;">$1.4 \times 10^{6}$</td>
<td style="text-align: center;">7500</td>
<td style="text-align: center;">$\mathbf{2 0 0}$</td>
</tr>
</tbody>
</table>
<p>Table 2: The qualities of the generated 3D molecules after finetuning on GEOM-DRUGS.
Baselines. For the molecule generation task, we consider the current best 3D generative models. EDM (Hoogeboom et al., 2022) and MolDiff (Peng et al., 2023a) are task-specialized diffusion models for 3D molecule generation. XYZ-Transformer (Flam-Shepherd \&amp; AspuruGuzik, 2023) is another 3D molecular transformer that was proposed for small-scale data. Note that XYZ-TF is the only model capable of large scale pretraining besides our model, so we pretrain only XYZ-TF and BindGPT on the Uni-Mol data. We also do the GEOM-DRUGS evaluation, where we report MolDiff and EDM trained on the full dataset and for BindGPT finetuned on the same version of it. For conformer generation, we compare BindGPT with the current state-of-the-art methods, Torsional Diffusion (Jing et al., 2022) and the Uni-Mol model (Zhou et al., 2023). The former is a specialized SE(3)-equivariant diffusion model capable of conformation generation only. The latter is a modified BERT (Devlin et al., 2019). As a coordinate-level encoder LM, the Uni-Mol model needs input coordinates to generate a conformation, which is why this model uses RDKit as a tool for initializing coordinates.</p>
<p>Results. The molecular generative modeling results are shown in Tables 2 and 1. First, the pretrained BindGPT model consistently outperforms the XYZ-TF baseline both without and with explicit hydrogens. The latter is a much more challenging task and almost no baseline methods can do that (except EDM, which is not scalable) since reconstructing hydrogens can be done on a post-processing step but explicit modeling of them makes the molecule size several times larger. BindGPT is the first model capable of modeling hydrogen explicitly at such large scale. Also, XYZ-TF has a very low validity rate due to the need of graph reconstruction. Next, for the methods trained on the GEOM-DRUGS dataset, BindGPT (being finetuned on this data) shows state-of-the-art performance scores for nearly all distributional evaluation metrics. Even though BindGPT does not outperform MolDiff in Druglikeness, that could be explained by a smaller vocabulary of the (Peng et al., 2023a), containing only frequent atoms. For the conformation generation task, the current best baseline is Torsional Diffusion (TD) (Jing et al., 2022). We use the Platinum dataset to compare TD trained on GEOM-DRUGS with Uni-Mol-BERT and BindGPT, both of which are pretrained and finetuned on the same data. Figure 4(b) shows the results for zero-shot evaluations on Platinum. Surprisingly, Uni-Mol fails to generalize to this new dataset (even assisted by RDKit), which we think is because of its structural diversity. BindGPT, in contrast, is capable of matching the performance of TD when assisted by the RDKit tool and having a small gap when not. All the above results demonstrate the generalisability of our model - none of the baselines is able to solve this wide range of task at this level of quality.</p>
<h1>4.2 Pocket-conditioned Molecule Generation</h1>
<p>Metrics. The main metrics for this task include the measure of ligand-pocket affinity; and druglikeness of the ligand. The first one is represented via the binding energy $(\downarrow)$ computed by the QVINA (Alhossary et al., 2015) docking software, while the second one comprises the</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Examples of binding poses and Vina scores ( $\downarrow$ ) for 2gns and 4d7o pockets.
aforementioned druglikeness metrics ( $\mathrm{SA}(\uparrow)$, QED $(\uparrow)$, and Lipinsky $(\uparrow)$ ). For each baseline we report the time required to generate 100 valid molecules for one pocket.
Baselines. Apart from the BindGPT model, we include the baselines such as 3D diffusion model (TargetDiff (Guan et al., 2023)) and autoregressive Graph Neural Network (Pocket2Mol (Peng et al., 2022)). Note that none of the baselines perform large-scale pretraining. Instead, they resort to heavy inductive biases to efficiently learn from small-scale data.</p>
<p>Results. Performance of our approach is summarized in Table 3. We depict the performance of three version of BindGPT. First, BindGPT-FT is a model finetuned on the complete CrossDocked data (the data layout as described in Figure 3, top), i.e. both good and bad binding pairs. This model serves as an initialization for the reinforcement learning model. Second, BindGPT-RFT is the model finetuned on CrossDocked with the reward in the context. To get higher affinity molecules from that model, we condition the model on random binding energy values within $[-12,-10]$, which are the best scores observed by the model (in around $0.1 \%$ of examples). Finally, the BindGPT-RL model is trained with RL (see Section 3.3.2 and Appendix B.3) from the BindGPT-FT initialization. Our main conclusion is that the RL finetuned model can learn to search the space of binding molecules much more efficiently and significantly outperforms all the previous best baselines in terms of the binding energy.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Vina score $(\downarrow)$</th>
<th style="text-align: center;">SA $(\uparrow)$</th>
<th style="text-align: center;">QED $(\uparrow)$</th>
<th style="text-align: center;">Lipinski $(\uparrow)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pocket2Mol</td>
<td style="text-align: center;">$-7.15 \pm 4.89$</td>
<td style="text-align: center;">$0.75 \pm 0.12$</td>
<td style="text-align: center;">$\mathbf{0 . 5 7} \pm \mathbf{0 . 1 5}$</td>
<td style="text-align: center;">$\mathbf{4 . 8 8} \pm \mathbf{0 . 3 7}$</td>
</tr>
<tr>
<td style="text-align: left;">TargetDiff</td>
<td style="text-align: center;">$-7.80 \pm 3.61$</td>
<td style="text-align: center;">$0.58 \pm 0.12$</td>
<td style="text-align: center;">$0.48 \pm 0.19$</td>
<td style="text-align: center;">$4.51 \pm 0.85$</td>
</tr>
<tr>
<td style="text-align: left;">BindGPT-FT (Ours)</td>
<td style="text-align: center;">$-5.44 \pm 2.09$</td>
<td style="text-align: center;">$0.78 \pm 0.10$</td>
<td style="text-align: center;">$0.50 \pm 0.17$</td>
<td style="text-align: center;">$4.72 \pm 0.70$</td>
</tr>
<tr>
<td style="text-align: left;">BindGPT-RFT (Ours)</td>
<td style="text-align: center;">$-7.24 \pm 1.68$</td>
<td style="text-align: center;">$0.74 \pm 0.11$</td>
<td style="text-align: center;">$0.48 \pm 0.22$</td>
<td style="text-align: center;">$4.32 \pm 1.25$</td>
</tr>
<tr>
<td style="text-align: left;">BindGPT-RL (Ours)</td>
<td style="text-align: center;">$\mathbf{- 8 . 6 0} \pm \mathbf{1 . 9 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 4} \pm \mathbf{0 . 0 5}$</td>
<td style="text-align: center;">$0.43 \pm 0.17$</td>
<td style="text-align: center;">$4.81 \pm 0.52$</td>
</tr>
</tbody>
</table>
<p>Table 3: Generative metrics for the pocket-conditioned generation task.</p>
<h1>5 Discussion and Conclusion</h1>
<p>In this work, we presented BindGPT, a scalable framework for training capable language models that can generate 3D molecules as text. Through a series of studies on a range of different 3D molecular generative tasks, we demonstrate the generality of our approach as it can solve each of them by matching or surpassing the baselines. Notably, our method does not have any inductive biases about the generative domain acting as a general and data-driven approach. Unlike all the baselines which have strong inductive biases, our method solves each downstream task without any such assumptions. The task of a particular</p>
<p>interest in our work is the pocket-based molecule generation where our model outperforms all the baselines with a large margin. We show that the large-scale pretraining paradigm can be efficiently transfered from NLP to the 3D drug discovery.</p>
<h1>Acknowledgments</h1>
<p>Sarath Chandar is supported by the Canada CIFAR AI Chairs program, the Canada Research Chair in Lifelong Machine Learning, and the NSERC Discovery Grant.</p>
<h2>References</h2>
<p>Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms, 2024.</p>
<p>Amr Alhossary, Stephanus Daniel Handoko, Yuguang Mu, and Chee-Keong Kwoh. Fast, accurate, and reliable molecular docking with QuickVina 2. Bioinformatics, 31(13):22142216, 02 2015. ISSN 1367-4803. doi: 10.1093/bioinformatics/btv082. URL https://doi . org/10.1093/bioinformatics/btv082.</p>
<p>Simon Axelrod and Rafael Gómez-Bombarelli. Geom, energy-annotated molecular conformations for property prediction and molecular generation. Scientific Data, 9(1):185, Apr 2022. ISSN 2052-4463. doi: 10.1038/s41597-022-01288-4. URL https://doi.org/10.1038/ s41597-022-01288-4.</p>
<p>Viraj Bagal, Rishal Aggarwal, P. K. Vinod, and U. Deva Priyakumar. Molgpt: Molecular generation using a transformer-decoder model. Journal of Chemical Information and Modeling, 62(9):2064-2076, May 2022. ISSN 1549-9596. doi: 10.1021/acs.jcim.1c00600. URL https://doi.org/10.1021/acs.jcim.1c00600.</p>
<p>Ahmet Bakan, Lidio M. Meireles, and Ivet Bahar. ProDy: Protein Dynamics Inferred from Theory and Experiments. Bioinformatics, 27(11):1575-1577, 04 2011. ISSN 1367-4803. doi: 10.1093/bioinformatics/btr168. URL https://doi.org/10.1093/bioinformatics/btr168.</p>
<p>Esben Jannik Bjerrum. SMILES enumeration as data augmentation for neural network modeling of molecules. CoRR, abs/1703.07076, 2017. URL http://arxiv.org/abs/1703. 07076.</p>
<p>Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive language model. In Proceedings of the ACL Workshop on Challenges \&amp; Perspectives in Creating Large Language Models, 2022. URL https://arxiv.org/abs/2204.06745.</p>
<p>Gayane Chilingaryan, Hovhannes Tamoyan, Ani Tevosyan, Nelly Babayan, Lusine Khondkaryan, Karen Hambardzumyan, Zaven Navoyan, Hrant Khachatrian, and Armen Aghajanyan. Bartsmiles: Generative masked language models for molecular representations, 2022.</p>
<p>Gabriele Corso, Hannes Stärk, Bowen Jing, Regina Barzilay, and Tommi S. Jaakkola. DiffDock: Diffusion steps, twists, and turns for molecular docking. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=kkF8_K-mBb5.</p>
<p>Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.</p>
<p>Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology. org/N19-1423.</p>
<p>Jerome Eberhardt, Diogo Santos-Martins, Andreas F. Tillack, and Stefano Forli. Autodock vina 1.2.0: New docking methods, expanded force field, and python bindings. Journal of Chemical Information and Modeling, 61(8):3891-3898, Aug 2021. ISSN 1549-9596. doi: 10.1021/acs.jcim.1c00203. URL https://doi.org/10.1021/acs.jcim.1c00203.</p>
<p>Wei Feng, Lvwei Wang, Zaiyun Lin, Yanhao Zhu, Han Wang, Jianqiang Dong, Rong Bai, Huting Wang, Jielong Zhou, Wei Peng, Bo Huang, and Wenbiao Zhou. Generation of 3d molecules in pockets via a language model. Nature Machine Intelligence, Jan 2024. ISSN 2522-5839. doi: 10.1038/s42256-023-00775-6. URL https://doi.org/10.1038/ s42256-023-00775-6.</p>
<p>Daniel Flam-Shepherd and Alán Aspuru-Guzik. Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files, 2023.</p>
<p>Daniel Flam-Shepherd, Kevin Zhu, and Alán Aspuru-Guzik. Language models can learn complex molecular distributions. Nature Communications, 13(1), June 2022. ISSN 2041-1723. doi: 10.1038/s41467-022-30839-x. URL http://dx.doi.org/10.1038/ s41467-022-30839-x.</p>
<p>Paul G. Francoeur, Tomohide Masuda, Jocelyn Sunseri, Andrew Jia, Richard B. Iovanisci, Ian Snyder, and David R. Koes. Three-dimensional convolutional neural networks and a cross-docked data set for structure-based drug design. Journal of Chemical Information and Modeling, 60(9):4200-4215, Sep 2020. ISSN 1549-9596. doi: 10.1021/acs.jcim.0c00411. URL https://doi.org/10.1021/acs.jcim.0c00411.</p>
<p>Nils-Ole Friedrich, Agnes Meyder, Christina de Bruyn Kops, Kai Sommer, Florian Flachsenberg, Matthias Rarey, and Johannes Kirchmair. High-quality dataset of protein-bound ligand conformations and its application to benchmarking conformer ensemble generators. Journal of Chemical Information and Modeling, 57(3):529-539, Mar 2017. ISSN 1549-9596. doi: 10.1021/acs.jcim.6b00613. URL https://doi.org/10.1021/acs.jcim.6b00613.</p>
<p>Rafael Gómez-Bombarelli, Jennifer N. Wei, David Duvenaud, José Miguel HernándezLobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams, and Alán Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS Central Science, 4(2):268-276, Feb 2018. ISSN 2374-7943. doi: 10.1021/acscentsci.7b00572. URL https://doi.org/10.1021/acscentsci.7b00572.</p>
<p>Jiaqi Guan, Wesley Wei Qian, Xingang Peng, Yufeng Su, Jian Peng, and Jianzhu Ma. 3D equivariant diffusion for target-aware molecule generation and affinity prediction. In The Eleventh International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=kJqXEPXMsE0.</p>
<p>Thomas A. Halgren. Merck molecular force field. i. basis, form, scope, parameterization, and performance of mmff94. J. Comput. Chem., 17(5-6):490-519, 1996. URL http://dblp. uni-trier.de/db/journals/jcc/jcc17.html#Halgren96.</p>
<p>Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 6840-6851. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9 (8):1735-1780, 1997.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022.</p>
<p>Emiel Hoogeboom, Víctor Garcia Satorras, Clément Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3D. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 8867-8887. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/ v162/hoogeboom22a.html.</p>
<p>Liegi Hu, Mark L Benson, Richard D Smith, Michael G Lerner, and Heather A Carlson. Binding moad (mother of all databases). Proteins: Structure, Function, and Bioinformatics, 60(3):333-340, 2005.</p>
<p>Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi S. Jaakkola. Torsional diffusion for molecular conformer generation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=w6fj2r62r_H.</p>
<p>John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583-589, 2021.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.
A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the International Conference on Machine Learning (ICML), 2020.</p>
<p>Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=H1oyRiYgg.</p>
<p>Mario Krenn, Florian Häse, AkshatKumar Nigam, Pascal Friederich, and Alan AspuruGuzik. Self-referencing embedded strings (SELFIES): A 100\% robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024, oct 2020. doi: 10.1088/2632-2153/aba947. URL https://dx.doi.org/10.1088/2632-2153/aba947.</p>
<p>Mario Krenn, Qianxiang Ai, Senja Barthel, Nessa Carson, Angelo Frei, Nathan C. Frey, Pascal Friederich, Théophile Gaudin, Alberto Alexander Gayle, Kevin Maik Jablonka, Rafael F. Lameiro, Dominik Lemm, Alston Lo, Seyed Mohamad Moosavi, José Manuel NápolesDuarte, AkshatKumar Nigam, Robert Pollice, Kohulan Rajan, Ulrich Schatzschneider, Philippe Schwaller, Marta Skreta, Berend Smit, Felix Strieth-Kalthoff, Chong Sun, Gary Tom, Guido Falk von Rudorff, Andrew Wang, Andrew D. White, Adamo Young, Rose Yu, and Alán Aspuru-Guzik. Selfies and the future of molecular string representations. Patterns, 3(10):100588, October 2022. ISSN 2666-3899. doi: 10.1016/j.patter.2022.100588. URL http://dx.doi.org/10.1016/j.patter.2022.100588.</p>
<p>Greg Landrum, Paolo Tosco, Brian Kelley, Ric, David Cosgrove, sriniker, gedeck, Riccardo Vianello, NadineSchneider, Eisuke Kawashima, Gareth Jones, Dan N, Andrew</p>
<p>Dalke, Brian Cole, Matt Swain, Samo Turk, AlexanderSavelyev, Alain Vaucher, Maciej Wójcikowski, Ichiru Take, Vincent F. Scalfani, Daniel Probst, Kazuya Ujihara, guillaume godin, Axel Pahl, Rachel Walker, Juuso Lehtivarjo, Francois Berenger, jasondbiggs, and strets123. rdkit/rdkit: 2023.09.4 (q3 2023) release, January 2024. URL https://doi.org/10.5281/zenodo. 10460537.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019.</p>
<p>Haitao Lin, Yufei Huang, Meng Liu, Xuanjing Li, Shuiwang Ji, and Stan Z. Li. DiffBP: Generative Diffusion of 3D Molecules for Target Protein Binding, December 2022.</p>
<p>Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.
Shitong Luo, Jiaqi Guan, Jianzhu Ma, and Jian Peng. A 3D generative model for structurebased drug design. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 62296239. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper.files/ paper/2021/file/314450613369e0ee72d0da7f6fee773c-Paper.pdf.</p>
<p>Andrew T. McNutt, Paul Francoeur, Rishal Aggarwal, Tomohide Masuda, Rocco Meli, Matthew Ragoza, Jocelyn Sunseri, and David Ryan Koes. Gnina 1.0: molecular docking with deep learning. Journal of Cheminformatics, 13(1):43, Jun 2021. ISSN 1758-2946. doi: 10.1186/s13321-021-00522-2. URL https://doi.org/10.1186/s13321-021-00522-2.</p>
<p>Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation invariant graph generation via score-based generative modeling, 2020.</p>
<p>Noel M. O'Boyle, Michael Banck, Craig A. James, Chris Morley, Tim Vandermeersch, and Geoffrey R. Hutchison. Open Babel: An open chemical toolbox. Journal of Cheminformatics, 3(1):33, Oct 2011. ISSN 1758-2946. doi: 10.1186/1758-2946-3-33. URL https://doi.org/ 10.1186/1758-2946-3-33.</p>
<p>Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de novo design through deep reinforcement learning, 2017.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019.</p>
<p>Xingang Peng, Shitong Luo, Jiaqi Guan, Qi Xie, Jian Peng, and Jianzhu Ma. Pocket2mol: Efficient molecular sampling based on 3d protein pockets. In International Conference on Machine Learning, 2022.</p>
<p>Xingang Peng, Jiaqi Guan, Qiang Liu, and Jianzhu Ma. MolDiff: Addressing the atom-bond inconsistency problem in 3D molecule diffusion generation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 27611-27629. PMLR, 23-29 Jul 2023a. URL https://proceedings.mlr.press/v202/peng23b.html.</p>
<p>Xingang Peng, Jiaqi Guan, Qiang Liu, and Jianzhu Ma. MolDiff: Addressing the atom-bond inconsistency problem in 3D molecule diffusion generation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett</p>
<p>(eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 27611-27629. PMLR, 23-29 Jul 2023b. URL https://proceedings.mlr.press/v202/peng23b.html.</p>
<p>P G Polishchuk, T I Madzhidov, and A Varnek. Estimation of the size of drug-like chemical space based on GDB-17 data. J Comput Aided Mol Des, 27(8):675-679, August 2013.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.</p>
<p>Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models, 2020.</p>
<p>Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, KDD '20, pp. 3505-3506, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3406703. URL https://doi.org/10.1145/3394486.3406703.</p>
<p>Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao Du, Tom Blundell, Pietro Lió, Carla Gomes, Max Welling, Michael Bronstein, and Bruno Correia. Structure-based drug design with equivariant diffusion models, 2023.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017.</p>
<p>Marwin H. S. Segler, Thierry Kogej, Christian Tyrchan, and Mark P. Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS Central Science, 4(1):120-131, Jan 2018. ISSN 2374-7943. doi: 10.1021/acscentsci.7b00512. URL https://doi.org/10.1021/acscentsci.7b00512.</p>
<p>Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations, 2021.</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. CoRR, abs/2104.09864, 2021. URL https://arxiv.org/abs/2104.09864.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.</p>
<p>Oleg Trott and Arthur J. Olson. AutoDock Vina: Improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Journal of Computational Chemistry, 31(2):455-461, 2010. doi: https://doi.org/10.1002/jcc. 21334. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc. 21334.</p>
<p>Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. Trl: Transformer reinforcement learning. https: //github.com/huggingface/tr1, 2020.</p>
<p>David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of Chemical Information and Computer Sciences, 28 (1):31-36, Feb 1988. ISSN 0095-2338. doi: 10.1021/ci00057a005. URL https://doi.org/10. 1021/ci00057a005.
R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229-256, 1992.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp.38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/2020.emnlp-demos.6.</p>
<p>Wenbo Yu and Alexander D. MacKerell. Computer-Aided Drug Design Methods, pp. 85106. Springer New York, New York, NY, 2017. ISBN 978-1-4939-6634-9. doi: 10.1007/ 978-1-4939-6634-9_5. URL https://doi.org/10.1007/978-1-4939-6634-9_5.</p>
<p>Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-mol: A universal 3d molecular representation learning framework. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=6K2RM6wVqKu.</p>
<h1>A Tokenization and Data Representation</h1>
<div class="codehilite"><pre><span></span><code><span class="o">&lt;</span><span class="n">LIGAND</span><span class="o">&gt;[</span><span class="n">H</span><span class="o">]</span><span class="n">c1c</span><span class="p">(</span><span class="n">F</span><span class="p">)</span><span class="n">c</span><span class="p">(</span><span class="o">[</span><span class="n">H</span><span class="o">]</span><span class="p">)</span><span class="n">c2c</span><span class="p">(</span><span class="n">C</span><span class="p">(</span><span class="n">F</span><span class="p">)(</span><span class="n">F</span><span class="p">)</span><span class="n">F</span><span class="p">)</span><span class="n">c</span><span class="p">(</span><span class="o">[</span><span class="n">H</span><span class="o">]</span><span class="p">)</span><span class="n">c</span><span class="p">(</span><span class="n">C</span><span class="nv">@N</span><span class="p">)</span><span class="n">nc2c1</span><span class="o">[</span><span class="n">H</span><span class="o">]</span>
<span class="o">&lt;</span><span class="n">XYZ</span><span class="o">&gt;</span>
<span class="mi">21</span>
<span class="w">    </span><span class="mf">2.775</span><span class="o">-</span><span class="mf">0.640</span><span class="w"> </span><span class="mf">2.950</span>
<span class="w">    </span><span class="mf">2.078</span><span class="o">-</span><span class="mf">0.379</span><span class="w"> </span><span class="mf">2.160</span>
<span class="w">    </span><span class="mf">2.123</span><span class="w"> </span><span class="mf">0.876</span><span class="w"> </span><span class="mf">1.583</span>
<span class="w">    </span><span class="mf">3.028</span><span class="w"> </span><span class="mf">1.767</span><span class="w"> </span><span class="mf">2.004</span>
<span class="w">    </span><span class="mf">1.236</span><span class="w"> </span><span class="mf">1.224</span><span class="w"> </span><span class="mf">0.571</span>
<span class="w">    </span><span class="mf">1.321</span><span class="w"> </span><span class="mf">2.226</span><span class="w"> </span><span class="mf">0.157</span>
<span class="w">    </span><span class="mf">0.270</span><span class="w"> </span><span class="mf">0.300</span><span class="w"> </span><span class="mf">0.109</span>
<span class="o">-</span><span class="mf">0.667</span><span class="w"> </span><span class="mf">0.577</span><span class="w"> </span><span class="o">-</span><span class="mf">0.917</span>
<span class="o">-</span><span class="mf">0.709</span><span class="w"> </span><span class="mf">1.913</span><span class="w"> </span><span class="o">-</span><span class="mf">1.616</span>
<span class="o">-</span><span class="mf">0.953</span><span class="w"> </span><span class="mf">2.943</span><span class="w"> </span><span class="o">-</span><span class="mf">0.761</span>
<span class="w">    </span><span class="mf">0.457</span><span class="w"> </span><span class="mf">2.211</span><span class="w"> </span><span class="o">-</span><span class="mf">2.248</span>
<span class="o">-</span><span class="mf">1.677</span><span class="w"> </span><span class="mf">1.990</span><span class="w"> </span><span class="o">-</span><span class="mf">2.572</span>
<span class="o">-</span><span class="mf">1.580</span><span class="o">-</span><span class="mf">0.419</span><span class="w"> </span><span class="o">-</span><span class="mf">1.293</span>
<span class="o">-</span><span class="mf">2.313</span><span class="w"> </span><span class="o">-</span><span class="mf">0.241</span><span class="w"> </span><span class="o">-</span><span class="mf">2.077</span>
<span class="o">-</span><span class="mf">1.534</span><span class="w"> </span><span class="o">-</span><span class="mf">1.641</span><span class="w"> </span><span class="o">-</span><span class="mf">0.648</span>
<span class="o">-</span><span class="mf">2.449</span><span class="w"> </span><span class="o">-</span><span class="mf">2.694</span><span class="w"> </span><span class="o">-</span><span class="mf">0.998</span>
<span class="o">-</span><span class="mf">3.207</span><span class="w"> </span><span class="o">-</span><span class="mf">3.516</span><span class="w"> </span><span class="o">-</span><span class="mf">1.313</span>
<span class="o">-</span><span class="mf">0.650</span><span class="w"> </span><span class="o">-</span><span class="mf">1.928</span><span class="w"> </span><span class="mf">0.332</span>
<span class="w">    </span><span class="mf">0.227</span><span class="w"> </span><span class="o">-</span><span class="mf">0.985</span><span class="w"> </span><span class="mf">0.701</span>
<span class="w">    </span><span class="mf">1.129</span><span class="w"> </span><span class="o">-</span><span class="mf">1.298</span><span class="w"> </span><span class="mf">1.711</span>
<span class="w">    </span><span class="mf">1.093</span><span class="w"> </span><span class="o">-</span><span class="mf">2.287</span><span class="w"> </span><span class="mf">2.164</span>
</code></pre></div>

<p>Figure 6: An example of a 3D molecule encoded as text that the model trains to predict. The sequence starts with a special token indicating the beginning of a small molecule: <LIGAND> followed by a SMILES string, tokenized character-level. Next follows the <XYZ> special token marking the end of SMILES and the beginning of the coordinate part of the output. The coordinate part of the output starts with the number of atoms in the molecule followed by a series of 3D coordinates for each atom. Unlike the standard XYZ format, our representation doesn't include atom types before each coordinate due to the preceding SMILES string, which already provides atom sequence and connectivity. In our case, the order of 3D coordinates correspond to the order of atoms as they appear in the SMILES string. Each coordinate triplet is encoded with six tokens: one token specifies the integer part with a sign and another one defines a fractional part of the floating point number.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: The visualization of the tokenization scheme for molecules in our language model. We use the same text as in Figure 6. Text is colored according to how tokenization is performed. Note that untokenized symbols (e.g. white spaces and new line separators) are not passed to the model since they don't carry any useful information. Also note that the coloring is made only for visualization purposes.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: An example of the protein pocket encoded as text that the model trains to predict during pretraining. Sequence starts with a special token <POCKET> marking the beginning of the protein pocket sequence. After that, we write a sequence of heavy atoms ignoring the edge structure of the pocket part of the protein molecule. After that, we write 3D coordinates of Alpha-Carbon atoms, each of which appears only once per aminoacid. The tokenization of the 2D part of the pocket is character level (except for CA) and for the 3D part it is two tokens per real number like in the ligand tokenization.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"><POCKET>NCACOCCCCCCONCACOCCCCNCACOCCCNCACOCCNCCNNCACOCCCCNCACOCNCAC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">OCCCNCNNNCACOCCCNCACOCCCONNCACOCCCONNCACOCCOONCACOCCCCCCCNCACOCCSC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$&lt;$ XYZ&gt;</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$-4.991$</td>
<td style="text-align: center;">4.794</td>
<td style="text-align: center;">6.134</td>
</tr>
<tr>
<td style="text-align: center;">3.067</td>
<td style="text-align: center;">2.185</td>
<td style="text-align: center;">$-5.773$</td>
</tr>
<tr>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">$-1.334$</td>
<td style="text-align: center;">3.936</td>
</tr>
<tr>
<td style="text-align: center;">7.077</td>
<td style="text-align: center;">1.335</td>
<td style="text-align: center;">2.009</td>
</tr>
<tr>
<td style="text-align: center;">1.134</td>
<td style="text-align: center;">$-7.460$</td>
<td style="text-align: center;">$-5.195$</td>
</tr>
<tr>
<td style="text-align: center;">2.384</td>
<td style="text-align: center;">$-5.084$</td>
<td style="text-align: center;">0.318</td>
</tr>
<tr>
<td style="text-align: center;">$-5.272$</td>
<td style="text-align: center;">$-7.393$</td>
<td style="text-align: center;">$-5.431$</td>
</tr>
<tr>
<td style="text-align: center;">7.391</td>
<td style="text-align: center;">$-1.954$</td>
<td style="text-align: center;">0.092</td>
</tr>
<tr>
<td style="text-align: center;">$-0.887$</td>
<td style="text-align: center;">$-4.613$</td>
<td style="text-align: center;">2.238</td>
</tr>
<tr>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">5.700</td>
<td style="text-align: center;">$-2.473$</td>
</tr>
<tr>
<td style="text-align: center;">$-3.573$</td>
<td style="text-align: center;">8.085</td>
<td style="text-align: center;">4.613</td>
</tr>
<tr>
<td style="text-align: center;">3.905</td>
<td style="text-align: center;">$-1.313$</td>
<td style="text-align: center;">$-4.534$</td>
</tr>
<tr>
<td style="text-align: center;">$-10.845$</td>
<td style="text-align: center;">7.057</td>
<td style="text-align: center;">4.070</td>
</tr>
</tbody>
</table>
<p>Figure 9: The visualization of the tokenization for pockets in our language model. We use the same example as in Figure 8.</p>
<h1>B Technical Description of the Training Pipeline</h1>
<h2>B. 1 Pretraining</h2>
<p>To achieve efficient pretraining, we use large batch training (Keskar et al., 2017) with 1.6 M tokens per batch. We set microbatch size to the maximal that fits to the GPU memory and we do gradient accumulation to get large enough batch size (to eventually have 1.6 M tokens per batch). Since training sequences have variable length (which comes from the fact that molecules have different sizes), only a part of tokens contribute to the loss so we make sure we have at least 1.6 M such "enabled" tokens. We use learning rate warmup of 2000 steps, followed by cosine annealing of the learning rate. The maximal learning rate during pretraining is $10^{-3}$ regardless of the model size. We found this many tokens per batch to be important for stable training in this task even with smaller learning rates, especially for models larger than 100 M parameters. We use AdamW optimizer (Loshchilov \&amp; Hutter, 2019) with a weight decay factor of $10^{-2}$. We use gradient clipping with the maximal grad norm of 1.0. The pretraining takes around 55 k optimization steps over 36 hours on one compute node with 8 A6000 GPUs. We employ Flash-Attention2 Dao (2023) and DeepSpeed optimization accelerator. To use more performant tensor cores, we train with mixed precision where computation is done within the bfloat16 datatype. As the distributed optimizer, we use DeepSpeed ZeRO Stage-3 optimizer (Rajbhandari et al., 2020). We train the model for 1 epoch only. The amount of tokens in the dataset is 42 B for the version without explicit hydrogens and 90B tokens for the version with explicit hydrogens. The total size of the Uni-Mol pretraining dataset is around 150GB.</p>
<h2>B. 2 Supervised Finetuning</h2>
<p>We use the public CrossDocked version v1.3 and for each molecule (except the ones optimized by the Gnina model (McNutt et al., 2021) as it yields too many bad intermediate samples) we take its "minimized" and "docked" formats and extract all intermediate molecules from their files. For each such molecule we cut the pocket with the ProDy (Bakan et al., 2011) tool. As a result of this process we obtain around 27 M pocket-ligand pairs. The size of the CrossDocked that we use is around 50GB.</p>
<p>We use the same recipe for finetuning as for the pretraining with a few changes in hyperparameters. In particular, we use maximal learning rate of $5 \times 10^{-4}$ and only 100 warump steps. The learning rate schedule, weight decay, optimizer, maximal gradient norm are the same as for the pretraining. The only substantial difference from the pretraining stage is the weighted loss which we use for the CrossDocked finetuning. Specifically, we weight tokens that correspond to different parts of the output, differently. For example, the SMILES tokens have the weight of 1 while tokens that correspond to the XYZ coordinates placed after SMILES have the weight of 5 . The tokens corresponding to the pocket have the weight of 0 since they are used as the context only and we don't intend to generate them. As it was described in the Section 3.3.1, we do SMILES randomization (see Bjerrum (2017) for implementation details) and rotate pocket it's ligand randomly - first we sample a random 3D rotation vector, we convert it to a rotation matrix and apply it to the coordinates of both. Also, we enforce the origin of their coordinates to be the same, namely, the coordinate center of the ligand (i.e. we guarantee that the model will generate coordinates around the origin). We train the model on the CrossDocked dataset for 1 epoch. As it was mentioned in Section 3.3.1, we extract the full version of the CrossDocked data.</p>
<p>For the finetuning on the GEOM-DRUGS dataset, we use the same hyperparameters as in the SFT stage for CrossDocked with only two differences. First, we weight the loss for all tokens with the same weight of 1 . Second, we don't rotate 3D coordinates of the molecule but only do SMILES randomization.</p>
<h1>B. 3 Reinforcement Learning Funetuning description</h1>
<p>The last stage of our pipeline is Reinforcement Learning. We use a distributed Reinforcement Learning algorithm based on the TRL (von Werra et al., 2020) training loop. That is, we launch multiple GPU-workers, where each repeatedly samples experiences, computes rewards, computes the update for the policy (i.e. the transformer language model), synchronizes them, and then performs the gradient update. We use 8 gpu workers, each with the local batch size of 16. At every step, we sample a batch of pockets, sample molecules for them, then we compute the rewards via docking tool and perform only one gradient update. We found this to be crucial for our task as otherwise the training might diverge. Even algorithms that are believed to be more powerful, such as PPO (Schulman et al., 2017), experience instabilities when the policy lag is bigger. Our surrogate loss for Reinforcement Learning has the following form:</p>
<p>$$
\begin{gathered}
L(\theta)=\mathbb{E}<em _theta="\theta">{s \sim \mathcal{D}, a \sim p</em> L(\theta, s, a) \
L(\theta, s, a)=-R(s, a) \frac{1}{|a|} \log p_{\theta}(a \mid s)+\alpha \mathrm{KL}\left(p_{\theta_{0}}(\cdot \mid s) | p_{\theta}(\cdot \mid s)\right)
\end{gathered}
$$}(a \mid s)</p>
<p>Here $s$ is the tokenized representation of the pocket and $a$ is the tokenized representation of the generated molecule. $p_{\theta}$ is the current version of the language model being finetuned while $p_{\theta_{0}}$ is the result of the SFT stage. $\log p_{\theta}(a \mid s)$ is the sum of generated token logprobabilities. $\mathcal{D}$ is the dataset of prompts (i.e. the pockets-only subset of CrossDocked). $R(s, a)$ is the vina score computed for the corresponding pocket-molecule pair. Finally, we compute the distillation style KL since we want to keep the output distribution of the RL model wide. The KL weight $\alpha$ in our experiments is $\alpha=0.05$. We use a flat learning rate of $1.4 \times 10^{-5}$ and no weight decay. Like before, we clip the gradient norm at 1.0. In our surrogate loss function, the loglikelihood of the token sequence is averaged (instead of being summed). We found this crucial for training stability.</p>
<h2>C Evaluating Pretraining at Different Scales</h2>
<p>As described in the Section 3.2, we pretrain the model on 208M 3D conformations of molecules. By experiment with different models size, we observed that the model scales well up until the size of 300M parameters, where its perplexity shows overfitting. We, therefore, stick to the 100M model in our later experiments as we found it yielding the best results. Figure 11 shows the hold-out test set perplexity on ZINC-250k ${ }^{2}$ for sizes 11M, 58M, 108M, 304M. Note that high value of perplexity is dictated by the highly stochastic nature of 3D molecule coordinates. We believe the model quality can be improved further by increasing the amount of the data for pretraining and the current 108 M model obtains the best performance given the pretraining dataset.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 11: Test perplexity on the hold-out Zinc-250k dataset with 3D conformations from RDKit. BindGPT is the first model capable of this type of evaluation.</p>
<h2>D RL finetuning Training Curves</h2>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 12: Ligand-pocket affinity objectives averaged over several runs for RL finetuning stage: (top left) Vina Score (top right) Ligand connectivity (bottom) Synthetic Accessibility</p>
<h1>E More Samples From the BindGPT Model</h1>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 13: 3D conformations generated by BindGPT with explicit hydrogens for a fixed molecule graph. No assistance tools are used. Also, no manual cherry-picking is used here.</p>
<h2>F Augmenting BindGPT with an External Tool for Assisted Generation</h2>
<p>For the unconditional molecule generation tasks and the conformation generation tasks, we enhance the 3D generative abilities of the BindGPT model though the use of the RDKit tool. However, we use it only as a scoring mechanism while our model still acts as the</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ The same procedure was conducted for collecting 3D conformations for ZINC-250k as for the original pretraining data.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
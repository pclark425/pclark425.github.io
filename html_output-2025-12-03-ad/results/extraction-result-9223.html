<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9223 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9223</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9223</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-236609688</p>
                <p><strong>Paper Title:</strong> <a href="https://journals.flvc.org/FLAIRS/article/download/128551/130033" target="_blank">Detecting Anomalies in Sequences of Short Text Using Iterative Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Business managers using Intelligent Virtual Assistants (IVAs) to enhance their company’s customer service need ways to accurately and efﬁciently detect anomalies in conversations between the IVA and customers, vital for customer retention and satisfaction. Unfortunately, anomaly detection is a challenging problem because of the subjective nature of what is deﬁned as anomalous. Detecting anomalies in sequences of short texts, common in chat settings, is even more difﬁcult because independently generated texts are similar only at a semantic level, resulting in an abundance of false positives. In addition, literature for detecting anomalies in time ordered sequences of short text is shallow considering the abundance of such data sets in online settings. We introduce a technique for detecting anomalies in sequences of short textual data by adaptively and iteratively learning low perplexity language models. Our al-gorithm deﬁnes a short textual item as anomalous when its cross-entropy exceeds the upper conﬁdence interval of a trained additive regression model. We demonstrate successful case studies and bridge the gap between theory and practice by ﬁnding anomalies in sequences of real conversations with virtual chat agents. Empirical evaluation shows that our method achieves, on average, 31% higher max F1 scores than the baseline method of non-negative matrix factorization across three large human-annotated sequences of short texts.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9223.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9223.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ILM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive nonparametric n-gram language-modeling pipeline that computes per-item cross-entropy on time-ordered short texts, retrains (slides) its training window when model perplexity exceeds a threshold, and flags anomalies by feeding cross-entropy time series to a forecasting model and using its upper confidence bound.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Iterative Language Model (ILM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Nonparametric n-gram count-based language model (bigram by default)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Time-ordered sequences of short text items (per-user chat turns); irregularly sampled cross-entropy time series</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>Customer-IVA (intelligent virtual assistant) conversational text for a large international airline</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Textual outliers in sequences of short text (e.g., missed intent, spelling/new vocabulary, preprocessing errors, multiple intents)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train an n-gram LM on a sliding training window (size x, e.g., 2000 items). For each new text item compute cross-entropy H(p,LM) normalized by first c words (c=30). Monitor LM perplexity; when perplexity > thresh_perplex (experimented in range 2–3) retrain LM on the next window. Feed the time series of cross-entropies into Facebook Prophet (additive regression/time-series decomposition) and mark a text item anomalous if its cross-entropy exceeds the Prophet model's upper confidence interval.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>TONMF (Text Outliers using Non-Negative Matrix Factorization); references to RPCA and SVD as other baselines in literature</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision/Recall/F1 (max F1 reported per intent); runtime (items/sec)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported max F1 per intent: Gift Certificates ILM F1 = 0.49 (vs TONMF 0.15); First Class Upgrades ILM F1 = 0.15 (vs TONMF 0.05); Find Companion Fare Discount Code ILM F1 = 0.49 (vs TONMF 0.014). Overall ILM achieves on average 31% higher max F1 than TONMF. Throughput: slowest ILM run processed >6 user turns/sec (suitable vs IVA 4.6 turns/sec).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>ILM outperforms TONMF across nearly all parameter configurations and intents (on average +31% max F1), and is faster in several settings; TONMF performed particularly poorly on short text and favored detecting code/number-type anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance sensitive to choice of parameters (training window x, thresh_perplex, n-gram order, normalization c); shortness and high variability of some intents (e.g., First Class Upgrades) limit detection (ILM F1 low for that intent). Nonparametric n-gram approach may be less powerful than large parametric LMs in data-rich settings but chosen for adaptivity and low-data regimes. Higher thresh_perplex reduces updates (faster) but may miss concept drift; lowest ILM runs still require retraining cost.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Combining adaptive sliding-window n-gram language models with cross-entropy scoring and a time-series forecasting model (Prophet) to detect anomalies in irregularly sampled sequences of short texts is effective and unsupervised; normalizing cross-entropy by a fixed number of initial words (c=30) stabilized results; ILM is practical for real-time deployment on IVA traffic.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9223.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9223.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TONMF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text Outliers using Non-Negative Matrix Factorization (TONMF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An NMF-based outlier detection method that models a term-document matrix as a low-rank component plus a sparse outlier matrix; document outlier scores are L2 norms of outlier-component columns and can be used for anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Outlier detection for text data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TONMF (NMF-based outlier detection)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Non-negative Matrix Factorization with an explicit outlier matrix (block coordinate descent optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Term-document matrix built from short texts (words × user turns)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>Customer-IVA conversational text (same intents as ILM experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Outlier documents / user turns that cannot be well-represented by low-rank topic structure (e.g., unique codes, rare terms)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Construct term-document matrix A; decompose A = L0 + Z0 where L0 = W0 C0 is low-rank representation and Z0 is sparse outlier matrix. Compute outlier score per document as L2 norm of corresponding column in Z0. Feed outlier scores to Prophet for time-series anomaly detection similarly to ILM.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against ILM in this paper; TONMF has been reported to outperform RPCA and SVD in prior work</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision/Recall/F1 (max F1), runtime</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported max F1 per intent: Gift Certificates TONMF F1 = 0.15 (vs ILM 0.49); First Class Upgrades TONMF F1 = 0.05 (vs ILM 0.15); Find Companion Fare Discount Code TONMF F1 = 0.014 (vs ILM 0.49). TONMF often slower as r (topics) increases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Worse than ILM across intents; TONMF tends to favor detecting anomalies that are unique codes/numbers and underperforms on very short texts because it expects longer documents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Designed for longer documents (e.g., newspaper articles); poor suitability for very short texts leads to many missed anomalies and a bias toward detecting code/number anomalies; optimization and runtime can be heavy as topic count r increases.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>TONMF's outlier predictions were dominated by code/number occurrences (e.g., discount codes), comprising a much higher fraction of predicted anomalies than their actual prevalence in the data, indicating a systematic bias when applied to short chat turns.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9223.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9223.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prophet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Facebook Prophet (additive time series forecasting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An additive regression/time-series decomposition forecasting model used to model irregularly sampled cross-entropy series and provide confidence intervals; anomalous text items are detected when their cross-entropy exceeds the model's upper confidence bound.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Forecasting at scale</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Facebook Prophet</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Additive regression / time-series decomposition (piecewise linear/logistic trend, seasonal components via Fourier series)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Irregularly sampled time series (sequence of cross-entropy scores over time)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>Cross-entropy time series derived from customer-IVA conversational text</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Time-local anomalies in the cross-entropy series corresponding to surprising textual items</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train Prophet on the cross-entropy time series; use the model's estimated upper confidence interval (e.g., 95% CI) to label items whose cross-entropy exceeds the bound as anomalies. Prophet handles missing time steps and irregular sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used to generate confidence intervals for anomaly detection; downstream evaluation metrics are Precision/Recall/F1 when combined with ILM or TONMF</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported as standalone accuracy; used as the detection thresholding mechanism for ILM and TONMF pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Chosen for ease-of-use and handling of irregular sampling; other time-series anomaly detectors could be used but were not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Assumes additive decomposition and normally distributed errors (MAP estimation); detection quality depends on quality/stationarity of cross-entropy time series and chosen confidence level.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Using a forecasting model that tolerates irregular sampling (Prophet) to model cross-entropy sequences is practical for chat data where messages arrive irregularly, enabling anomaly detection via model confidence intervals rather than fixed thresholds.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9223.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9223.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cross-Entropy n-gram Scoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-Entropy of text items under a recent n-gram language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scoring technique computing the negative average log-probability of an item's n-grams under a language model (H(p,LM)), where higher values indicate more surprising (potentially anomalous) items relative to recent data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>No country for old members: User lifecycle and linguistic change in online communities</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>n-gram cross-entropy scoring</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Nonparametric n-gram language model scoring (bigram used in examples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Short text (represented as sequence of n-grams); produces time series of cross-entropy scores</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>IVA conversational short text (customer chat turns)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Surprising/rare textual items relative to recent linguistic state (outliers)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For each text item p, compute H(p,LM) = - (1/N) sum_j log P_LM(p_j) where p_j are n-grams; normalize by first c words (c=30) for stability. High cross-entropy indicates surprise; these scores are used as input to a time-series anomaly detector (Prophet).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as primary scoring fed into Prophet; final evaluation measured by Precision/Recall/F1 of detected anomalies after thresholding the time-series model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Cross-entropy based ILM produced substantially higher F1 than TONMF on the evaluated intents (see ILM results).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Cross-entropy scoring (with iterative retraining) outperformed matrix-factorization-based outlier scoring (TONMF) on sequences of short text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Choice of n-gram order limited by shortness of text (higher n-grams left for future work); normalization scheme (first c words) is heuristic; cross-entropy depends on LM quality and retraining policy; nonparametric n-gram models can be less powerful than parametric deep LMs but adapt quicker to distributional shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Normalizing cross-entropy by a fixed number of initial words (c=30) stabilized detection; using LM perplexity as an adaptive trigger to retrain the LM allows the method to handle concept drift without continuous manual annotation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>No country for old members: User lifecycle and linguistic change in online communities <em>(Rating: 2)</em></li>
                <li>Outlier detection for text data <em>(Rating: 2)</em></li>
                <li>Forecasting at scale <em>(Rating: 2)</em></li>
                <li>Exploring the limits of language modeling <em>(Rating: 1)</em></li>
                <li>Anomaly detection for short texts: Identifying whether your chatbot should switch from goal-oriented conversation to chit-chatting <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9223",
    "paper_id": "paper-236609688",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "ILM",
            "name_full": "Iterative Language Model",
            "brief_description": "An adaptive nonparametric n-gram language-modeling pipeline that computes per-item cross-entropy on time-ordered short texts, retrains (slides) its training window when model perplexity exceeds a threshold, and flags anomalies by feeding cross-entropy time series to a forecasting model and using its upper confidence bound.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Iterative Language Model (ILM)",
            "model_type": "Nonparametric n-gram count-based language model (bigram by default)",
            "model_size": null,
            "data_type": "Time-ordered sequences of short text items (per-user chat turns); irregularly sampled cross-entropy time series",
            "data_domain": "Customer-IVA (intelligent virtual assistant) conversational text for a large international airline",
            "anomaly_type": "Textual outliers in sequences of short text (e.g., missed intent, spelling/new vocabulary, preprocessing errors, multiple intents)",
            "method_description": "Train an n-gram LM on a sliding training window (size x, e.g., 2000 items). For each new text item compute cross-entropy H(p,LM) normalized by first c words (c=30). Monitor LM perplexity; when perplexity &gt; thresh_perplex (experimented in range 2–3) retrain LM on the next window. Feed the time series of cross-entropies into Facebook Prophet (additive regression/time-series decomposition) and mark a text item anomalous if its cross-entropy exceeds the Prophet model's upper confidence interval.",
            "baseline_methods": "TONMF (Text Outliers using Non-Negative Matrix Factorization); references to RPCA and SVD as other baselines in literature",
            "performance_metrics": "Precision/Recall/F1 (max F1 reported per intent); runtime (items/sec)",
            "performance_results": "Reported max F1 per intent: Gift Certificates ILM F1 = 0.49 (vs TONMF 0.15); First Class Upgrades ILM F1 = 0.15 (vs TONMF 0.05); Find Companion Fare Discount Code ILM F1 = 0.49 (vs TONMF 0.014). Overall ILM achieves on average 31% higher max F1 than TONMF. Throughput: slowest ILM run processed &gt;6 user turns/sec (suitable vs IVA 4.6 turns/sec).",
            "comparison_to_baseline": "ILM outperforms TONMF across nearly all parameter configurations and intents (on average +31% max F1), and is faster in several settings; TONMF performed particularly poorly on short text and favored detecting code/number-type anomalies.",
            "limitations_or_failure_cases": "Performance sensitive to choice of parameters (training window x, thresh_perplex, n-gram order, normalization c); shortness and high variability of some intents (e.g., First Class Upgrades) limit detection (ILM F1 low for that intent). Nonparametric n-gram approach may be less powerful than large parametric LMs in data-rich settings but chosen for adaptivity and low-data regimes. Higher thresh_perplex reduces updates (faster) but may miss concept drift; lowest ILM runs still require retraining cost.",
            "unique_insights": "Combining adaptive sliding-window n-gram language models with cross-entropy scoring and a time-series forecasting model (Prophet) to detect anomalies in irregularly sampled sequences of short texts is effective and unsupervised; normalizing cross-entropy by a fixed number of initial words (c=30) stabilized results; ILM is practical for real-time deployment on IVA traffic.",
            "uuid": "e9223.0"
        },
        {
            "name_short": "TONMF",
            "name_full": "Text Outliers using Non-Negative Matrix Factorization (TONMF)",
            "brief_description": "An NMF-based outlier detection method that models a term-document matrix as a low-rank component plus a sparse outlier matrix; document outlier scores are L2 norms of outlier-component columns and can be used for anomaly detection.",
            "citation_title": "Outlier detection for text data",
            "mention_or_use": "use",
            "model_name": "TONMF (NMF-based outlier detection)",
            "model_type": "Non-negative Matrix Factorization with an explicit outlier matrix (block coordinate descent optimization)",
            "model_size": null,
            "data_type": "Term-document matrix built from short texts (words × user turns)",
            "data_domain": "Customer-IVA conversational text (same intents as ILM experiments)",
            "anomaly_type": "Outlier documents / user turns that cannot be well-represented by low-rank topic structure (e.g., unique codes, rare terms)",
            "method_description": "Construct term-document matrix A; decompose A = L0 + Z0 where L0 = W0 C0 is low-rank representation and Z0 is sparse outlier matrix. Compute outlier score per document as L2 norm of corresponding column in Z0. Feed outlier scores to Prophet for time-series anomaly detection similarly to ILM.",
            "baseline_methods": "Compared against ILM in this paper; TONMF has been reported to outperform RPCA and SVD in prior work",
            "performance_metrics": "Precision/Recall/F1 (max F1), runtime",
            "performance_results": "Reported max F1 per intent: Gift Certificates TONMF F1 = 0.15 (vs ILM 0.49); First Class Upgrades TONMF F1 = 0.05 (vs ILM 0.15); Find Companion Fare Discount Code TONMF F1 = 0.014 (vs ILM 0.49). TONMF often slower as r (topics) increases.",
            "comparison_to_baseline": "Worse than ILM across intents; TONMF tends to favor detecting anomalies that are unique codes/numbers and underperforms on very short texts because it expects longer documents.",
            "limitations_or_failure_cases": "Designed for longer documents (e.g., newspaper articles); poor suitability for very short texts leads to many missed anomalies and a bias toward detecting code/number anomalies; optimization and runtime can be heavy as topic count r increases.",
            "unique_insights": "TONMF's outlier predictions were dominated by code/number occurrences (e.g., discount codes), comprising a much higher fraction of predicted anomalies than their actual prevalence in the data, indicating a systematic bias when applied to short chat turns.",
            "uuid": "e9223.1"
        },
        {
            "name_short": "Prophet",
            "name_full": "Facebook Prophet (additive time series forecasting)",
            "brief_description": "An additive regression/time-series decomposition forecasting model used to model irregularly sampled cross-entropy series and provide confidence intervals; anomalous text items are detected when their cross-entropy exceeds the model's upper confidence bound.",
            "citation_title": "Forecasting at scale",
            "mention_or_use": "use",
            "model_name": "Facebook Prophet",
            "model_type": "Additive regression / time-series decomposition (piecewise linear/logistic trend, seasonal components via Fourier series)",
            "model_size": null,
            "data_type": "Irregularly sampled time series (sequence of cross-entropy scores over time)",
            "data_domain": "Cross-entropy time series derived from customer-IVA conversational text",
            "anomaly_type": "Time-local anomalies in the cross-entropy series corresponding to surprising textual items",
            "method_description": "Train Prophet on the cross-entropy time series; use the model's estimated upper confidence interval (e.g., 95% CI) to label items whose cross-entropy exceeds the bound as anomalies. Prophet handles missing time steps and irregular sampling.",
            "baseline_methods": null,
            "performance_metrics": "Used to generate confidence intervals for anomaly detection; downstream evaluation metrics are Precision/Recall/F1 when combined with ILM or TONMF",
            "performance_results": "Not reported as standalone accuracy; used as the detection thresholding mechanism for ILM and TONMF pipelines.",
            "comparison_to_baseline": "Chosen for ease-of-use and handling of irregular sampling; other time-series anomaly detectors could be used but were not evaluated here.",
            "limitations_or_failure_cases": "Assumes additive decomposition and normally distributed errors (MAP estimation); detection quality depends on quality/stationarity of cross-entropy time series and chosen confidence level.",
            "unique_insights": "Using a forecasting model that tolerates irregular sampling (Prophet) to model cross-entropy sequences is practical for chat data where messages arrive irregularly, enabling anomaly detection via model confidence intervals rather than fixed thresholds.",
            "uuid": "e9223.2"
        },
        {
            "name_short": "Cross-Entropy n-gram Scoring",
            "name_full": "Cross-Entropy of text items under a recent n-gram language model",
            "brief_description": "A scoring technique computing the negative average log-probability of an item's n-grams under a language model (H(p,LM)), where higher values indicate more surprising (potentially anomalous) items relative to recent data.",
            "citation_title": "No country for old members: User lifecycle and linguistic change in online communities",
            "mention_or_use": "use",
            "model_name": "n-gram cross-entropy scoring",
            "model_type": "Nonparametric n-gram language model scoring (bigram used in examples)",
            "model_size": null,
            "data_type": "Short text (represented as sequence of n-grams); produces time series of cross-entropy scores",
            "data_domain": "IVA conversational short text (customer chat turns)",
            "anomaly_type": "Surprising/rare textual items relative to recent linguistic state (outliers)",
            "method_description": "For each text item p, compute H(p,LM) = - (1/N) sum_j log P_LM(p_j) where p_j are n-grams; normalize by first c words (c=30) for stability. High cross-entropy indicates surprise; these scores are used as input to a time-series anomaly detector (Prophet).",
            "baseline_methods": null,
            "performance_metrics": "Used as primary scoring fed into Prophet; final evaluation measured by Precision/Recall/F1 of detected anomalies after thresholding the time-series model.",
            "performance_results": "Cross-entropy based ILM produced substantially higher F1 than TONMF on the evaluated intents (see ILM results).",
            "comparison_to_baseline": "Cross-entropy scoring (with iterative retraining) outperformed matrix-factorization-based outlier scoring (TONMF) on sequences of short text.",
            "limitations_or_failure_cases": "Choice of n-gram order limited by shortness of text (higher n-grams left for future work); normalization scheme (first c words) is heuristic; cross-entropy depends on LM quality and retraining policy; nonparametric n-gram models can be less powerful than parametric deep LMs but adapt quicker to distributional shifts.",
            "unique_insights": "Normalizing cross-entropy by a fixed number of initial words (c=30) stabilized detection; using LM perplexity as an adaptive trigger to retrain the LM allows the method to handle concept drift without continuous manual annotation.",
            "uuid": "e9223.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "No country for old members: User lifecycle and linguistic change in online communities",
            "rating": 2,
            "sanitized_title": "no_country_for_old_members_user_lifecycle_and_linguistic_change_in_online_communities"
        },
        {
            "paper_title": "Outlier detection for text data",
            "rating": 2,
            "sanitized_title": "outlier_detection_for_text_data"
        },
        {
            "paper_title": "Forecasting at scale",
            "rating": 2,
            "sanitized_title": "forecasting_at_scale"
        },
        {
            "paper_title": "Exploring the limits of language modeling",
            "rating": 1,
            "sanitized_title": "exploring_the_limits_of_language_modeling"
        },
        {
            "paper_title": "Anomaly detection for short texts: Identifying whether your chatbot should switch from goal-oriented conversation to chit-chatting",
            "rating": 1,
            "sanitized_title": "anomaly_detection_for_short_texts_identifying_whether_your_chatbot_should_switch_from_goaloriented_conversation_to_chitchatting"
        }
    ],
    "cost": 0.010633749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Detecting Anomalies in Sequences of Short Text Using Iterative Language Models</p>
<p>Cynthia Freeman cynthia.freeman@verint.com 
Verint Intelligent Self-Service</p>
<p>University of New
Mexico</p>
<p>Ian Beaver ian.beaver@verint.com 
Verint Intelligent Self-Service</p>
<p>Abdullah Mueen mueen@cs.unm.edu 
University of New
Mexico</p>
<p>Detecting Anomalies in Sequences of Short Text Using Iterative Language Models</p>
<p>Business managers using Intelligent Virtual Assistants (IVAs) to enhance their company's customer service need ways to accurately and efficiently detect anomalies in conversations between the IVA and customers, vital for customer retention and satisfaction. Unfortunately, anomaly detection is a challenging problem because of the subjective nature of what is defined as anomalous. Detecting anomalies in sequences of short texts, common in chat settings, is even more difficult because independently generated texts are similar only at a semantic level, resulting in an abundance of false positives. In addition, literature for detecting anomalies in time ordered sequences of short text is shallow considering the abundance of such data sets in online settings. We introduce a technique for detecting anomalies in sequences of short textual data by adaptively and iteratively learning low perplexity language models. Our algorithm defines a short textual item as anomalous when its cross-entropy exceeds the upper confidence interval of a trained additive regression model. We demonstrate successful case studies and bridge the gap between theory and practice by finding anomalies in sequences of real conversations with virtual chat agents. Empirical evaluation shows that our method achieves, on average, 31% higher max F1 scores than the baseline method of non-negative matrix factorization across three large human-annotated sequences of short texts.</p>
<p>Introduction</p>
<p>We work for a company that designs and builds domainspecific Intelligent Virtual Assistants (IVAs) on behalf of other companies and organizations, typically for customer service automation. Many companies deploy IVAs for problem resolution and cutting costs in call centers.</p>
<p>A business manager using an IVA to enhance his company's customer service can analyze interactions between customers and IVAs to identify interactions leading to customer complaints like in Figure 1. This interaction indicates the business manager may need to include options to reprint a gift certificate on the company's website and have the IVA direct customers to it, or if such an option already exists, the IVA is unaware of the option and should be updated and the website should make the printing option more conspicuous.</p>
<p>Copyright © 2021by the authors. All rights reserved. CUSTOMER : Reprint a gift certificate? IVA : Gift Certificates can be purchased at XXXXX.com. The increasing adaptation of IVAs creates a problem; there is a corresponding increase in the number of humancomputer interactions to be reviewed for quality assurance. Therefore, discovering a means to expedite review and analysis of these interactions is critical. This requires efficient detection of anomalies in conversations. Conversational turns tend to be short (45 characters per user turn on average in our data) and are ordered by time.</p>
<p>Detecting anomalies under such conditions is difficult. Textual anomaly detection, even without such constraints, is already a notoriously difficult problem for a multitude of reasons: 1) What is defined as anomalous may differ based on application. The textual item lmao doc martens are just emo timbs would probably be anomalous to an IVA answering questions about airline travel but not so on Twitter. 2) What is anomalous today may not be anomalous tomorrow which is especially true for applications such as IVAs. Introduction of a new type of promotion such as a credit card offer may create textual items that are found to be anomalous initially. However, they must be considered normal soon after introduction. 3) It is unrealistic to assume that anomaly detection systems will have access to thousands of tagged data sets. For chat text, annotated data sets for anomalies are even more limited; we have the NPS Chat Corpus (Forsythand and Martell 2019) which is only tagged for speech and dialogue acts, the Twitter Triple Corpus (Sordoni et al. 2015) and Ubuntu Dialogue Corpus (Lowe et al. 2015) where both are not annotated for anomalies, and UseNet (Shaoul and C. 2013) which is also not annotated and specifically omits documents with less than 500 words. 4) Non-anomalous data occurs in much larger quantities than anomalous data. This can present a problem for a machine learning classifier approach to anomaly detection as the classes are not represented equally. Thus, an accuracy measure might present excellent results, but the accuracy is only reflecting the unequal class distribution in the data (the accuracy paradox).</p>
<p>We address these difficulties in detecting outliers in sequences of short textual data by using cross-entropies from iterative language models.</p>
<p>As we work for an IVA company, we have access to massive quantities of chat data. For our experiments, we selected a large international airlines IVA which interacts with users on the airline's website and mobile application, providing travel advice such as flight status information, baggage and security rules, and even helps with the booking process. This particular assistant was selected as it is a very active IVA with a diverse user base. On average, it responds to 4.6 user inputs per second and engages in 115.5 unique conversations per minute with a global user base.</p>
<p>The iterative language model updates in an adaptive manner, based on the perplexity of the language model. We compare the iterative language models to a non-negative matrix factorization method built for textual anomaly detection that has been reported to outperform many other commonly used baselines such as robust principal component analysis (RPCA) and singular value decomposition (SVD). Our iterative language model achieves, on average, 31% higher max F1 scores on a large human to IVA conversational data set and is also unsupervised.</p>
<p>Related Work</p>
<p>Existing studies cannot address the problem of how to detect outliers in sequences of short text. Of the anomaly detection methods that are designed for a textual domain:</p>
<ol>
<li>
<p>Existing methods often assume that the pieces of text are large (Guthrie 2008) containing at least 1000 words or are full-length newspaper articles as in (Zhuang et al. 2017). Short text does not have enough content or words which hinders the application of conventional machine learning and text mining algorithms (Chen, Jin, and Shen 2011).</p>
</li>
<li>
<p>Existing methods are often built for very specific tasks such as authorship identification (Guthrie, Guthrie, and Wilks 2008) or detecting outlier sections in legal documents (Aktolga, Ros, and Assogba 2011). For example, in (Aktolga, Ros, and Assogba 2011), outliers in legal documents are detected by exploiting bill structure, specifically Sections, the smallest units of a bill. However, detecting anomalies in sequences of short text is more general and includes not just IVA conversations but also social domains like Twitter and Facebook (Bakarov, Yadrintsev, and Sochenkov 2018;Nedelchev, Usbeck, and Lehmann 2020).</p>
</li>
<li>
<p>Existing methods are typically not built for anomaly detection in time ordered text. For example, the work in (Jain et al. 2019) involves time ordered text, but the goal is to detect malicious chatbots instead of anomalous texts from real users.</p>
</li>
<li>
<p>Existing methods do not take into account the dynamic nature of data such as chat or tweets that make it difficult to keep models up to date. In (Xia, GAO, and others 2005), support vector machines (SVMs) trained on chat data annotated for anomalies need to be frequently updated or performance suffers. Frequent periodic retraining of the SVM is not feasible as this requires constant annotation of chat corpora.</p>
</li>
</ol>
<p>We address these problems by identifying outliers via language model cross-entropies, inspired from the work in (Danescu-Niculescu-Mizil et al. 2013) where crossentropies are used to predict how long a user will stay active in an Internet community.</p>
<p>Language models can be parametric or nonparametric. Parametric approaches include deep learning techniques but require large quantities of data and often cannot adapt to rapid changes in the distribution of the data in an online setting. Nonparametric approaches include count-based techniques. Although they tend to perform worse compared to parametric approaches, nonparametric approaches can efficiently incorporate new information and require significantly less data (Jozefowicz et al. 2016). Given our data's sparse and dynamic nature, we restrict ourselves to nonparametric approaches (more specifically, statistics on N-grams). By incorporating nonparametric techniques to account for short text and updating the language model on a sliding window when its perplexity is too high, we can also account for our data's dynamic nature.</p>
<p>Methods</p>
<p>We begin by giving some background on language models and how we use cross-entropies to detect anomalies in sequences of short text. We then introduce our iterative language models 1 and discuss our baseline.</p>
<p>Language Models</p>
<p>A language model is a probability distribution over sequences of symbols pertaining to a language (Jurafsky 2000), and the perplexity is used to evaluate the quality of a language model where the lower the value, the better. For bigrams, the perplexity of the sequence W = w 1 w 2 ...w N is: (
N i=1
1 P (wi|wi−1) ) 1/N . Our iterative language model "slides" (retrains) when the perplexity reaches a threshold.</p>
<p>Cross-Entropy</p>
<p>The cross-entropies of textual items (Danescu-Niculescu-Mizil et al. 2013) are used to determine anomalies. The cross-entropy of a textual item p given a language model LM is: H(p, LM ) = − 1 N j log(P LM (p j )) where p j = n-grams making up p, N = number of n-grams making up p, and P LM (p j ) = probability of n-gram p j under the LM. Higher n-grams are left for future work due to the shortness of data. The higher H(p, LM ) is, the more surprising the item p is given the recent, past linguistic state. In other words, a low H(p, LM ) means that p reflects what is commonly seen in the past.</p>
<p>Iterative Language Models</p>
<p>How the iterative language model (ILM) is updated and used to detect anomalies is highlighted in Algorithm 1. If a language model is trained on texts from dates or indices a to b, we represent this language model as LM a:b .</p>
<p>The model takes as input a list of textual items. In our IVA application, every textual item is a user turn in conversations with an IVA. This list is sorted by time. The ILM takes as parameters the following: 1) thresh perplex = the threshold for perplexity to retrain the language model, 2) x = training window size, and 3) n = the n-gram to use for crossentropy calculation. The LM is trained on the first x many textual items, creating LM 0:x . For every textual item i onwards from x, the cross-entropy and perplexity are determined using the language model and chosen n-gram. If the perplexity &gt; thresh perplex , we slide the language model forward by retraining on new textual items.</p>
<p>We input the cross-entropies to Facebook Prophet (Taylor and Letham 2018), an additive regression model involving a special time series decomposition method with a piecewise linear or logistic growth curve trend, a yearly seasonal component modeled using Fourier series or a weekly seasonal component, an optionally user-provided list of holidays, and an error term that is assumed to be normally distributed. Parameters are estimated using MAP. Prophet formulates the forecasting problem as a curve-fitting exercise, similar to generalized additive modeling. Thus, Prophet can innately handle time series with missing time steps. For our iterative language model, we have a time series of crossentropies, and we do not necessarily have cross-entropies for every time step; this is dependent on when a customer chats with an intelligent virtual assistant, but Facebook Prophet can deal with these irregularly sampled time series. We input the cross-entropies to Facebook Prophet and train a forecasting model, using the confidence interval of the model to detect anomalies. Prophet was chosen due to its ease of use, requiring little expert knowledge, and its open availability, but other viable options include any time series anomaly detection method that can incorporate irregular sampling.</p>
<p>Baseline: TONMF</p>
<p>There are no existing studies that can address the problem of how to detect outliers in sequences of short text. However, we provide the closest baseline possible that we could find for this task. We use as a baseline a non-negative matrix factorization method adjusted for detecting outliers in text called Text Outliers using Non-Negative Matrix Factorization (TONMF) developed in (Kannan et al. 2017).</p>
<p>Non-negative matrix factorization (NMF) approximates a non-negative matrix X ∈ R p×n to a lower rank approximation r ≤ rank(X). A non-negative basis matrix W ∈ R p×r and coordinate matrix C ∈ R r×n are determined that minimizes ||X − W C|| 2 F where F is the Frobenius norm. In (Kannan et al. 2017), the authors model the outliers along with the low rank space of the input matrix. Suppose A is the term-document matrix. In our application, every row represents a word, and every column represents a user turn in conversations with an IVA. A ∈ R m×n is represented as 
for i in range(x+1, length(textualItems)) do p ← textualItems[i]; pj ← determineNgrams(p,n); N ← length(pj); CES.append(− 1 N j log(LM (pj))) if ( N k=1 1 P (w k |w k−1 ) ) 1/N &gt; thresh perplex then LM ← trainLM(textualItems[i-x:i], n); allCES.append(CES); CES ← [];
return allCES a sum: A = L 0 + Z 0 where L 0 = W 0 C 0 and W 0 ∈ R m×r and C 0 ∈ R r×n . In other words, every document in A is represented as a linear combination of r topics. In situations where a document cannot be well-represented by L 0 , it is placed in the outlier matrix Z 0 as a non-zero entry.</p>
<p>Outlier scores for documents are calculated by the L2 norm of columns in Z 0 . We feed these outlier scores to Prophet (Taylor and Letham 2018) as like with the iterative language model. W 0 , C 0 , and Z 0 are determined via block coordinate descent for computational simplicity.</p>
<p>Empirical Evaluation</p>
<p>We begin with a description of the large, real world data sets used and how they were annotated. We then proceed with a description of how data was preprocessed and parameters chosen and conclude with results comparing the performance of the iterative language model and TONMF.</p>
<p>Data Set and Annotation</p>
<p>The airlines IVA that we selected for our experiments can recognize 1,230 unique user intentions, or interpretations of a user input that allows one to formulate the best response. For example, if the customer asks about upgrading his flight due to health issues, the IVA determines that the customer's intent is about First Class Upgrades and responds accordingly. The intentions are used as a class label within the IVA. Once the IVA determines the user intention, the response associated with that intention is returned.</p>
<p>We selected three intents of varying popularity levels to monitor for our experiment: Find Companion Fare Discount Code, First Class Upgrades, and Gift Certificates. Find Companion Fare and First Class Upgrades are in the top ten most frequently hit intents; in one year of logs First Class Upgrades was hit 58,187 times and Find Companion Fare Discount Code was hit 56,389 times. We also wanted to experiment with an intent that was only moderately popular, so we included Gift Certificates which has 8,378 textual items.</p>
<p>Unlike newspaper articles or movie reviews, customer text in IVA conversations tends to be much shorter, presenting a significant challenge for tagging. For every intent, the user text was fed to a language model. IVA responses are excluded because the response is usually identical for the same intent. However, the IVA response was provided for annotators. A graduate student fully tagged the Gift Certificates intent data set for anomalies. However, for Find Companion Fare Discount Code and First Class Upgrades, only 3,000 user inputs of each intent were annotated due to the enormity of these two data sets. The annotator was instructed to mark any of the following as anomalous:</p>
<ol>
<li>Missed Intent (not due to preprocessing): Sometimes the IVA will incorrectly classify a user's intent in the conversation (such as in Figure 1). As another example: in the Gift Certificates intent, the user may ask about Amazon gift cards, but the IVA incorrectly assumes this involves gift cards that can only be used for airline miles. For the Find Companion Fare Discount Code intent, the customer may ask about where to apply the code when purchasing a ticket online, but the IVA directs the user to an account link to see existing discount codes instead. For the First Class Upgrades intent, the IVA provides options on how to buy such upgrades instead of helping the customer determine if upgrades are even available in the first place or are already bought out for a particular flight.</li>
</ol>
<p>Spelling Mistakes and New Vocabulary:</p>
<p>A spelling mistake may be infrequent, and, therefore, be marked as an anomaly such as: buyiing gift certificates. Novel terminology assigned to an existing intent may mean that a new product or service has been released that needs to be added to the IVA's intent library.</p>
<ol>
<li>Preprocessing Errors: A preprocessing step done by the IVA may cause an error in intent classification. For example, the user may state that he is looking for gift certificates because they are missing in his account. However, the preprocessing step converts looking for gift certificates to search for gift certificates which brings up a menu of gift certificates one can buy instead of pre-bought gift certificates under one's account.</li>
</ol>
<p>Multiple Intents:</p>
<p>The user may ask something that has multiple possible intents. For example, the user may ask for a gift certificate for miles because of a death in the family. Acceptable intents would include Gift Certificates as well as Bereavement Fare.</p>
<p>Most of these categories require attention from the business manager to improve IVA performance and prevent issues from reaching a larger set of customers over a longer duration of time. For example, assuming the intent under review was defined for answering questions around Gift Cer-tificates that can only be used for flights, a business user may see the following anomalous scenarios that need attention:</p>
<p>• For category 1: Missed Intent, if customers ask about using Amazon gift cards and the IVA incorrectly returns the Gift Certificates response which states they may be used for purchasing flights, this is indicative of user confusion on how other types of gift cards can be applied. If other gift cards cannot be used for purchasing flights, the requirements must be made more explicit on the airlines website or there needs to be a new intent generated in the IVA for these type of redemption questions.</p>
<p>• For category 2: Spelling Mistakes and New Vocabulary, identifying novel spelling mistakes are helpful for our IVA developers in building our word to vocabulary term mappings. As a preprocessing step in the IVA, words are stemmed and then mapped to specific vocabulary terms. For example, words such as baggage or carry-on are mapped to a Luggage vocabulary label. We have a standalone tool to build up this vocabulary by loading a set of user inputs and then exposing all of the words that are unknown to the IVA. Content creators can then quickly associate misspellings to which vocabulary labels they belong to and then export these changes for inclusion in the next IVA version. Also, the identification of unexpected words in a given intent may mean the context around the words intended to map to the intent may have changed. For example, before 2013, the word pixel in a device help desk IVA would have been associated with a measurement of screen resolution. But within that year, there would begin to be occurrences of the bigram Google pixel in conversations for device support that would be flagged as anomalous during the first several occurrences. This would alert IVA designers that they need to differentiate between questions about screen resolution and a specific smart phone device in the IVA intents.</p>
<p>• For category 3: Preprocessing Errors, converting looking for gift certificates to search for gift certificates also indicates a problem in our word to vocabulary term mappings in our IVA preprocessing step. Normalizing looking to search may be too great an assumption, and we must consider the possibility of the word looking to apply to multiple situations such as a missing gift certificate in an account.</p>
<p>• For category 4: Multiple Intents, we can identify cases where the IVA can be more personable. If a customer asks for gift certificates because of a death in the family, the IVA can still direct the user to the Gift Certificates intent but also use apologetic language.</p>
<p>As our experimental data is IVA to human conversations, one might ask why we do not just consider sentiment analysis, escalation detection (e.g. Can I talk to a human?), or intent misclassification for anomaly detection. Simply performing sentiment analysis or escalation detection on the text, although possibly helpful, is not enough. For our IVA to human conversational data, once customers determine that their concerns are not being addressed, they very frequently just leave the chat instead of spewing expletives or expressing frustration. Anomalies in IVA conversations also encompass more than just multi-label classification errors as shown in the above four anomaly categories. Note, however, there still needs to be a way to detect any missed intent classification. This is not as simple as just looking at classifier confidence. Intent classification can be done via a unique combination of machine learning classifiers, regular expressions, and conversation flow decision trees. We wanted a generic enough methodology that only requires customer text which is independent of the implementation details of the IVA.</p>
<p>Application</p>
<p>We use the adaptive update language model, set x = 2000, and experiment with various thresh perplex (where 2 &lt; thresh perplex &lt; 3). Every intent data set has its own language model. In calculating cross-entropies, we normalize by just using the first c words of p. This form of normaliza-tion is employed in (Danescu-Niculescu-Mizil et al. 2013) as there is no consensus on how to normalize entropy measures. We use c = 30 where results are stable across multiple choices of c.</p>
<p>We also perform TONMF on every 2, 000 many textual items (to make it comparable to the ILM) and test various parameter configurations. Every intent data set has its own application of TONMF. In our implementation of TONMF, we restricted β = 1 as the creators of TONMF have stated that the algorithm is not overly sensitive to choice of β. We experiment with r =10 to 45 topics per intent. As for α, it balances the importance given to outliers against the matrix sparsity criterion during regularization. The larger α is, the more important the outlier portion of the regularization. However, for lower ranks of r, the increase in the value of α does not make any predictions. This is because, beyond a particular limit, the weights given to the outlier criterion do not supersede the optimization's main objective of extracting low-rank patterns from the data. Figure 2 shows the run time and F1 scores for every parameter configuration for the iterative language model versus the baseline TONMF, using a confidence interval of .95. For nearly all parameter configurations, the iterative language model (ILM) has higher F1 scores than the baseline (TONMF) across all time. In addition, the ILM can perform faster than TONMF in several cases.</p>
<p>Results</p>
<p>For Gift Certificates, the highest F1 score for the ILM was .49 whereas the baseline can only reach .15. For First Class Upgrades, ILM achieves .15, and TONMF can only reach .05. For Find Companion Fare Discount code, ILM hits .49 whereas the baseline can only hit .014.</p>
<p>First Class Upgrades was the most difficult data set for both the ILM and TONMF. This is because this intent was quickly discovered by the annotator to encompass too many user questions that the IVA was not customized to address. The IVA response to a user question hitting the First Class Upgrades intent is: You can use your miles to upgrade in advance, request a Paid Upgrade during check-in or at the departure gate, or if you're an XXXXX member, you can upgrade for free. However, this does not cover questions regarding if a first class upgrade for a particular flight is available or bought out, first class upgrade code usage, or how to buy coach seats when only first class seats are available. Yet all of these questions are classified by the IVA under the First Class Upgrades intent. In addition, midway through the year, a new premium upgrade was promoted by the airlines company, but the IVA was never updated to reflect this change. Thus, all questions involving these premium upgrades was directed to First Class Upgrades, confusing customers. There was a lot more variety in the kinds of questions customers asked in the First Class Upgrades intent compared to the other two intents, making it difficult for ILM and TONMF to establish a textual norm.</p>
<p>The four types of anomalies were not distinguished during annotation for the sake of time and effort on the anno-tator's part (especially as the IVA data is proprietary, and, thus, we cannot utilize annotation crowdsourcing tools like Mechanical Turk), but a deeper analysis on the categories would be valuable and is left for future work. For example, we determined that TONMF favors detecting anomalies belonging in the category of unique codes and numbers (e.g. a customer asks if a Discount Code XYZ is valid or if their ticket number is available for an upgrade). In fact, for Find Companion Fare Discount Code, textual items containing codes comprised over 85% of TONMFs predictions whereas textual items containing codes only comprise 40% of Find Companion Fare Discount Code. Such predictions make up 20% of predictions made by the ILM. Similarly, for First Class Upgrades, predictions containing only codes make up 8% of the data set, 20-30% of predictions made by TONMF, and 1-6% of predictions made by the ILM. TONMF generally performs worse; this is most likely due to the fact that TONMF expects larger bodies of text. In (Kannan et al. 2017), TONMF was only tested on Reuters newspaper articles, RCV20, and Wiki People. It would be beneficial to include such analysis on the other anomaly categories.</p>
<p>Accuracy-speed trade-off is universal. For the iterative language model, the higher thresh perplex is, the fewer slides the language model makes (fewer updates), and, in turn, the number of times the language model is retrained is decreased. Thus, the time required to determine crossentropies for the entire data set goes down. However, even the slowest of the ILM runs was able to process over 6 user turns per second, which is fast enough to deploy in realtime alongside this airline IVA which answers 4.6 turns/sec. For TONMF, an increase in r (number of topics) increases the time needed to solve the optimization problem. Nonnegative matrix factorization is a NP-hard problem; thus, the authors make use of block coordinate descent for computational efficiency.</p>
<p>Significance and Impact</p>
<p>Detecting outliers in sequences of short text is a difficult problem because text is typically sparse and high dimensional. However, this detection is vital for IVAs in use by business managers who seek ways to improve the customer experience. Existing techniques assume that the text samples either have no ordering or are long; however, this is not always the case especially in the domains of chat, Facebook comments, or Twitter. The shortage of publicly available, annotated resources compounds our problems, and even if annotated data is available, the dynamic nature of conversational data necessitates constant retraining of models. In this paper, we demonstrate our technique for detecting outliers in sequences of short text using cross-entropies from iterative language models. We take advantage of our company's massive repository of chat data sets to address the lack of publicly available, annotated data. We compare the iterative language model to TONMF as a baseline and achieve, on average, 31% higher max F1 scores on real human to IVA conversations.</p>
<p>Our ultimate goal is to determine how to improve our IVAs given the outliers and the categories they belong to. By deploying our textual anomaly detection system alongside the IVA, we can record anomalies as they happen in real-time for downstream health monitoring of live production IVAs and help identify how the IVA can be improved.</p>
<p>Figure 1 :
1An anonymized anomalous conversation between a customer and a live airlines IVA. Printing, not purchasing, gift certificates is the customer's intent.</p>
<p>Figure 2 :
2Run time (in seconds) versus F1 scores for the intents Gift Certificates, First Class Upgrades, Find Companion Fare Discount Code for the iterative language model (ILM, blue dots) and baseline (TONMF, orange stars) where every point represents a parameter configuration. The confidence interval was .95.
Implementations available on https://anon-share. s3-us-west-2.amazonaws.com/ilm_2020.zip</p>
<p>Detecting outlier sections in us congressional legislation. E Aktolga, I Ros, Y Assogba, Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. the 34th international ACM SIGIR conference on Research and development in Information RetrievalAktolga, E.; Ros, I.; and Assogba, Y. 2011. Detecting outlier sec- tions in us congressional legislation. In Proceedings of the 34th in- ternational ACM SIGIR conference on Research and development in Information Retrieval, 235-244.</p>
<p>Anomaly detection for short texts: Identifying whether your chatbot should switch from goal-oriented conversation to chit-chatting. A Bakarov, V Yadrintsev, I Sochenkov, International Conference on Digital Transformation and Global Society. SpringerBakarov, A.; Yadrintsev, V.; and Sochenkov, I. 2018. Anomaly detection for short texts: Identifying whether your chatbot should switch from goal-oriented conversation to chit-chatting. In Interna- tional Conference on Digital Transformation and Global Society, 289-298. Springer.</p>
<p>Short text classification improved by learning multi-granularity topics. M Chen, X Jin, D Shen, Twenty-Second International Joint Conference on Artificial Intelligence. Chen, M.; Jin, X.; and Shen, D. 2011. Short text classification improved by learning multi-granularity topics. In Twenty-Second International Joint Conference on Artificial Intelligence.</p>
<p>No country for old members: User lifecycle and linguistic change in online communities. C Danescu-Niculescu-Mizil, R West, D Jurafsky, J Leskovec, C Potts, Proceedings of the 22nd international conference on World Wide Web. the 22nd international conference on World Wide WebDanescu-Niculescu-Mizil, C.; West, R.; Jurafsky, D.; Leskovec, J.; and Potts, C. 2013. No country for old members: User lifecycle and linguistic change in online communities. In Proceedings of the 22nd international conference on World Wide Web, 307-318.</p>
<p>E N Forsythand, C H Martell, The nps chat corpus. Forsythand, E. N., and Martell, C. H. 2019. The nps chat corpus.</p>
<p>An unsupervised approach for the detection of outliers in corpora. D Guthrie, L Guthrie, Y Wilks, Guthrie, D.; Guthrie, L.; and Wilks, Y. 2008. An unsupervised approach for the detection of outliers in corpora. Statistics 3409- 3413.</p>
<p>Unsupervised detection of anomalous text. D Guthrie, Ph.D. Dissertation, CiteseerGuthrie, D. 2008. Unsupervised detection of anomalous text. Ph.D. Dissertation, Citeseer.</p>
<p>Characterizing and detecting livestreaming chatbots. S Jain, D Niranjan, H Lamba, N Shah, P Kumaraguru, Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and MiningJain, S.; Niranjan, D.; Lamba, H.; Shah, N.; and Kumaraguru, P. 2019. Characterizing and detecting livestreaming chatbots. In Pro- ceedings of the 2019 IEEE/ACM International Conference on Ad- vances in Social Networks Analysis and Mining, 683-690.</p>
<p>Exploring the limits of language modeling. R Jozefowicz, O Vinyals, M Schuster, N Shazeer, Y Wu, arXiv:1602.02410arXiv preprintJozefowicz, R.; Vinyals, O.; Schuster, M.; Shazeer, N.; and Wu, Y. 2016. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410.</p>
<p>Speech &amp; language processing. D Jurafsky, Pearson Education IndiaJurafsky, D. 2000. Speech &amp; language processing. Pearson Edu- cation India.</p>
<p>Outlier detection for text data. R Kannan, H Woo, C C Aggarwal, H Park, Proceedings of the 2017 siam international conference on data mining. the 2017 siam international conference on data miningSIAMKannan, R.; Woo, H.; Aggarwal, C. C.; and Park, H. 2017. Outlier detection for text data. In Proceedings of the 2017 siam interna- tional conference on data mining, 489-497. SIAM.</p>
<p>The ubuntu dialogue corpus: A large dataset for research in unstructured multiturn dialogue systems. R Lowe, N Pow, I Serban, J Pineau, R Nedelchev, R Usbeck, J Lehmann, arXiv:1506.08909Proceedings of The 12th Language Resources and Evaluation Conference. The 12th Language Resources and Evaluation ConferencearXiv preprintTreating dialogue quality evaluation as an anomaly detection problemLowe, R.; Pow, N.; Serban, I.; and Pineau, J. 2015. The ubuntu dialogue corpus: A large dataset for research in unstructured multi- turn dialogue systems. arXiv preprint arXiv:1506.08909. Nedelchev, R.; Usbeck, R.; and Lehmann, J. 2020. Treating dia- logue quality evaluation as an anomaly detection problem. In Pro- ceedings of The 12th Language Resources and Evaluation Confer- ence, 508-512.</p>
<p>C Shaoul, C , W , A reduced redundancy usenet corpus. Shaoul, C., and C., W. 2013. A reduced redundancy usenet corpus.</p>
<p>A neural network approach to context-sensitive generation of conversational responses. A Sordoni, M Galley, M Auli, C Brockett, Y Ji, M Mitchell, J.-Y Nie, J Gao, B Dolan, arXiv:1506.06714Sordoni, A.; Galley, M.; Auli, M.; Brockett, C.; Ji, Y.; Mitchell, M.; Nie, J.-Y.; Gao, J.; and Dolan, B. 2015. A neural network ap- proach to context-sensitive generation of conversational responses. arXiv:1506.06714.</p>
<p>Forecasting at scale. S J Taylor, B Letham, The American Statistician. 721Taylor, S. J., and Letham, B. 2018. Forecasting at scale. The American Statistician 72(1):37-45.</p>
<p>Nil is not nothing: Recognition of chinese network informal language expressions. Y Xia, W Gao, Xia, Y.; GAO, W.; et al. 2005. Nil is not nothing: Recognition of chinese network informal language expressions.</p>
<p>Identifying semantically deviating outlier documents. H Zhuang, C Wang, F Tao, L Kaplan, J Han, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingZhuang, H.; Wang, C.; Tao, F.; Kaplan, L.; and Han, J. 2017. Iden- tifying semantically deviating outlier documents. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2748-2757.</p>            </div>
        </div>

    </div>
</body>
</html>
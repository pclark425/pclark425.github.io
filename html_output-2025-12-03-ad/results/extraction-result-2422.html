<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2422 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2422</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2422</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-269605978</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.02449v1.pdf" target="_blank">Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design</a></p>
                <p><strong>Paper Abstract:</strong> Experimental design techniques such as active search and Bayesian optimization are widely used in the natural sciences for data collection and discovery. However, existing techniques tend to favor exploitation over exploration of the search space, which causes them to get stuck in local optima. This ``collapse"problem prevents experimental design algorithms from yielding diverse high-quality data. In this paper, we extend the Vendi scores -- a family of interpretable similarity-based diversity metrics -- to account for quality. We then leverage these quality-weighted Vendi scores to tackle experimental design problems across various applications, including drug discovery, materials discovery, and reinforcement learning. We found that quality-weighted Vendi scores allow us to construct policies for experimental design that flexibly balance quality and diversity, and ultimately assemble rich and diverse sets of high-performing data points. Our algorithms led to a 70%-170% increase in the number of effective discoveries compared to baselines.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2422.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2422.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>qVS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quality-weighted Vendi Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A similarity-based set utility that multiplies the Vendi Score (entropy of normalized kernel eigenvalues measuring diversity) by the average quality score of items, yielding an interpretable metric that jointly rewards diversity and item quality with a tunable sensitivity parameter q.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Quality-weighted Vendi Score (qVS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>qVS = (average per-item quality) * VS_q, where VS_q is the order-q Vendi Score (exp of Rényi-type entropy of normalized eigenvalues of the similarity/kernel matrix). The kernel k(x,x') encodes similarity; s(x) (or model predictive scores p(x)) supply per-item qualities. The order q >= 0 controls sensitivity to eigenvalue non-uniformity (trade-off between treating items as effectively distinct vs requiring strictly unique items). qVS is used both as an objective to evaluate candidate batches/sets and as a utility to score marginal gains when selecting experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General experimental design: active search, Bayesian optimization, molecular/materials discovery, reinforcement learning trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experiments by selecting subsets that maximize qVS (either directly by continuous optimization when gradients are available, or approximately via a sequential greedy heuristic for discrete pools). For Bayesian/active search, per-item predictive scores (e.g., p(y=1|x) or surrogate objective values f(x) or UCB samples) are used as the quality s(x) inside qVS; batches are chosen to maximize expected qVS of returned positives or solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Implicitly measured as number of expensive oracle evaluations (fixed batch sizes / fixed total number of queries); computational overhead also includes cost of evaluating kernel matrices and their eigen-decompositions (O(n^3) in batch size n for naive eigen-decomposition) and cost of surrogate modeling and candidate scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected increase in qVS (or expected increase in VS of positives) is used as the decision utility; surrogate predictive probabilities or posterior samples (Thompson sampling / UCB) are used to form expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Balanced via the qVS objective: quality term (average s(x)) encourages exploitation (high-quality candidates) while the VS_q term rewards diverse sets (exploration). The order q modulates the relative emphasis on diversity vs quality (lower q favors counting more items, higher q enforces stronger diversity). In sequential decision-making the algorithm greedily selects items maximizing expected marginal increase of qVS.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit: diversity is quantified by the Vendi Score computed from the similarity/kernel matrix of the candidate set (entropy of normalized eigenvalues). Diversity promotion is directly baked into the utility (VS_q) rather than via hard distance constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of expensive oracle queries (batch size and total iterations); discrete pool selection or continuous domain with fixed batch sizes/trust-region budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handles budget by sequential greedy selection until the desired batch size is reached, or by gradient-based optimization with multi-start when continuous and gradients available; uses trust-region TuRBO-style local sampling + Thompson sampling to produce candidates in expensive continuous problems.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Metrics operationalized include Vendi Score of discovered positives (effective number of distinct high-quality discoveries), maximum objective value found (best f(x)), effective discovery count (EDC) thresholded by objective, and domain-specific quantities (e.g., MOF storage capacity, energy penalty).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics include Vendi Scores (for various q orders) of discovered positives, best objective values found (e.g., MOF storage capacity; Table 4 reports best values across q settings), effective discovery counts, and domain-specific penalties (energy penalty for MOF release). The paper reports a 70%–170% increase in number of effective discoveries compared to baselines (abstract).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to diversity-blind active search/BayesOpt (count-based AS, TuRBO/UCB BayesOpt), ROBOT (rank-ordered diverse BayesOpt), random search, Expected Coverage Improvement (ECI), SELECT, and DPP-based approaches mentioned in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>qVS-based policies (qVS-AS and qVS-BayesOpt with q≈1) achieved the highest Vendi Scores in most active search problems and were competitive or superior to ROBOT and TuRBO in BayesOpt tasks; in some tasks (lunar lander, MOF search) encouraging diversity improved optimization progress compared to traditional BayesOpt. The paper states a 70%–170% increase in effective discoveries vs baselines; specific best-MOF storage values and energy-penalty improvements are shown in experimental tables/figures.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Empirical gains reported as increases in effective discoveries (70%–170%) and improved best-objective outcomes in several tasks; no explicit runtime/FLOP reductions reported—efficiency described in terms of experimental (oracle) budget utilization and improved discovery per query.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Yes — the paper analyzes the trade-off via the order q: lower q (e.g., q=0) recovers count-based search favoring exploitation; higher q increases priority on diversity. They show q that matches evaluation metric tends to perform best; varying q produces smooth changes in behavior. They note that diversity can improve optimization progress in rugged search spaces (avoiding local optima), and recommend tuning q or potentially adapting it dynamically.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key findings: (1) Using qVS provides an interpretable, continuous mechanism to balance diversity and quality without hard distance constraints (removing need for tuned τ used by constraint-based methods); (2) greedy sequential selection provides scalable approximate maximization in discrete pools; (3) combining qVS with local BayesOpt strategies (TuRBO + Thompson sampling) yields practical diverse BayesOpt; (4) tuning q is important—q≈1 often a good default, but task-dependent tuning can improve results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2422.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2422.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>qVS-AS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quality-weighted Vendi Score Active Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active search algorithm that selects batches to maximize expected qVS of discovered positives, using model predictive probabilities as per-item quality and a greedy sequential batch construction to manage combinatorial complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>qVS-AS (diverse active search)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Given a pool of unlabeled candidates and a probabilistic model producing p(y=1|x), qVS-AS scores candidate additions by expected marginal increase in qVS of the positive subset (using p(x) as s(x)). For discrete pools, it constructs batches greedily: at each step, compute marginal qVS gain for each remaining candidate (using current predictive probabilities and kernel similarities) and pick the maximizer until batch size n is reached. For sequential (single-query) operation, the one-step Bayesian-optimal decision is to maximize expected increase in VS of positives.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Rare-item discovery tasks: molecular photoswitches, materials (bulk metallic glasses), product recommendation simulations (FashionMNIST), and other experimental discovery settings with expensive labels.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates oracle queries by scoring candidates by expected marginal increase in qVS (i.e., expected increase in effective number of high-quality discoveries) and greedily selecting the highest-scoring candidates until budgeted batch size is reached.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Primarily counts of oracle queries (batch size / number of iterations), plus computational cost of evaluating p(y|x), computing kernel matrices and eigenvalues for candidate sets; combinatorial cost avoided via greedy heuristic and limited batch sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected increase in qVS of positives (using predictive probabilities to compute expectation) serves as the utility; this acts as the information-aware decision criterion in a Bayesian decision framework.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration via VS term (diversity of positives), exploitation via average predictive probability term s(x); the order q modulates sensitivity. For q=0, reduces to count-based active search (pure exploitation for hits); larger q enforces exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit qVS objective using kernel-based Vendi Score computed on predicted positives, encouraging selection of positives spread in feature/kernel space.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch sizes and total number of queries (oracle calls).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Greedy marginal-gain selection until budgeted batch size; uses predictive model to prioritize likely positives while maintaining diversity via qVS.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Vendi Score of discovered positives (effective distinct high-quality discoveries), hit rate and precision-at-k in predictive benchmarking, and domain-specific success definitions (e.g., photoswitch properties).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Vendi Scores across q orders, maximum pairwise distance and determinant-of-kernel metrics for spread; reported consistent improvements in VS over baselines across the photoswitch, materials, and FashionMNIST tasks (numerical tables in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Expected Coverage Improvement (ECI), SELECT, traditional active search (q=0/count), and diversity-blind baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>qVS-AS achieved highest VS in most experiments; when not best it was close to best tuned baseline. Demonstrated better spread (max pairwise distance, kernel determinant) and higher effective discovery counts.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported increases in effective diverse discoveries per fixed number of oracle queries (part of the 70%–170% overall claim); no explicit wall-clock savings reported.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Empirical analysis across q ∈ {0,0.1,0.5,1,2,∞} showing smooth behavioral change; matching q to evaluation metric yields best performance; higher q pushes toward stronger diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Greedy expected-qVS marginal gain selection is an effective approximate allocation strategy for constrained-budget active search, balancing between checking high-probability positives and diversifying discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2422.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2422.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>qVS-BayesOpt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quality-weighted Vendi Score Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BayesOpt extension that returns a set of M high-quality, diverse solutions by maximizing qVS over candidate sets, integrating trust-region local search (TuRBO) and surrogate posterior sampling (Thompson sampling) to produce candidate proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>qVS-BayesOpt (diverse Bayesian optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each BayesOpt iteration, surrogate models (GPs) or their local trust-region variants (TuRBO) produce candidate evaluations via Thompson sampling or UCB; these surrogate values are treated as per-item quality s(x)=f_sample(x) or UCB(x) and qVS (Eq.11) is maximized over a set size M to select the batch of query points. The algorithm replaces hard distance constraints (as in ROBOT) with the qVS objective to balance solution quality and diversity, and uses trust-region sampling to handle high-dimensional continuous spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Global optimization problems in materials and control: rover path planning, lunar lander control policies (reinforcement learning), metal-organic framework (MOF) property optimization, and general black-box optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates queries by selecting sets that maximize qVS of candidate solutions (quality-weighted Vendi Score with quality derived from surrogate samples or UCB scores), using TuRBO to generate local candidate pools and greedy maximization to choose final queries; seeks multiple diverse high-quality proposals per iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Oracle evaluation budget (number of black-box function evaluations) is primary cost; additional computational costs include GP fitting and local candidate generation (TuRBO), and eigen-decomposition of kernel matrices for qVS evaluation. For discrete structured data (MOFs), continuous embedding is not required, removing VAE training cost but requiring kernel evaluations across candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Utility is expected qVS or qVS computed on surrogate draws; not mutual-information based but uses Bayesian surrogate samples to assess expected utility increase.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Quality term (surrogate objective or UCB) pushes toward exploitation of high-value regions; VS term rewards diversity across solutions to encourage exploration. Trust-region sampling (TuRBO) provides local exploitation while multiple trust regions and qVS selection maintain global diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit: qVS computed using a kernel derived from distance function (k_δ) replaces hard minimum-distance constraints; the order q controls diversity sensitivity. This removes the need for a user-specified distance threshold τ used by rank-ordered approaches like ROBOT.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of objective evaluations per iteration and overall (oracle calls); also fixed number M of solutions to return per iteration in rank-ordered scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Generates candidate sets within M trust regions, scores candidates via qVS on surrogate draws, and selects top-scoring batch constrained by the available evaluation budget; uses greedy selection to manage combinatorial explosion.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best observed objective values (max f(x)), Vendi Score of 'good' solutions (EDC above threshold), and domain-specific metrics (e.g., storage capacity in MOFs, energy penalty for toxin release).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Plots of best objective value found across repeats, tables of best MOF storage capacity across q values, and EDC/Vendi Score of 'good' solutions; reports qVS-BayesOpt (q=1) competitive with or outperforming ROBOT and TuRBO in some tasks, and improved EDC and energy-penalty metrics in MOF experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with TuRBO (diversity-blind TuRBO BayesOpt), ROBOT (rank-ordered diverse BayesOpt with distance constraints), UCB-based BayesOpt (for discrete MOF case), and random search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>qVS-BayesOpt is competitive with ROBOT in continuous problems and outperformed both TuRBO and ROBOT in some discrete/structured tasks (MOF, lunar lander), where encouraging diversity helped escape local optima; in MOF search, qVS-BayesOpt improved effective discovery count and lowered average energy penalty vs baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Improved discovery per oracle call and improved best-objective attainment under the same evaluation budget; quantitative claim in paper: overall 70%–170% more effective discoveries vs baselines (abstract), with task-specific numeric tables showing best-objective improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper analyzes effect of order q on BayesOpt performance (Table 4 shows best MOF storage across q values) and notes that improper q can harm performance (some q values worse than q=0), suggesting q tuning or dynamic adaptation as an important design choice.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Principle: replacing hard distance constraints with qVS yields an automatic quality-vs-diversity trade-off that avoids manual τ tuning; combining qVS with local BayesOpt sampling (TuRBO + Thompson) yields practical allocations of limited oracle budget to diverse high-quality candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2422.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2422.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROBOT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROBOT (Rank-Ordered Bayesian Optimization with Trust Regions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rank-ordered BayesOpt approach (Maus et al., 2023) that maintains multiple high-quality solutions enforced to be at least a fixed distance τ from one another, implemented via trust-region local optimizers (extension of TuRBO).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discovering Many Diverse Solutions with Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ROBOT (rank-ordered BayesOpt)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ROBOT generates M solutions x*_1,...,x*_M by sequentially optimizing the surrogate objective under hard pairwise distance constraints δ(x*_i, x*_j) ≥ τ for j<i, typically using local TuRBO-style trust-region optimization (often with Thompson sampling inside trust regions). The hard constraint τ enforces diversity explicitly by geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Diverse solution discovery in Bayesian optimization tasks (materials design, control, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates evaluations by centering trust regions on current solutions and performing local optimization, producing M constrained solutions per allocation step that are mutually distant by τ.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Oracle evaluations (function evaluations) and internal cost of multiple local optimizers; constraints introduce additional search cost to enforce τ.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Standard BayesOpt surrogate utilities (e.g., Thompson samples or acquisition functions) subject to distance constraints; not formulated as expected information gain explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Hard geometric constraints (τ) force exploration across distinct modes; local trust regions enable exploitation within neighborhoods.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit hard-distance constraints between solutions (minimum pairwise distance τ).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed oracle evaluation budget and number of trust regions/local optimizers.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Allocates budget across trust regions and constrained optimizations; requirement to tune τ can affect feasibility and budget usage.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>High objective value solutions and diversity enforced by distance constraints; evaluated by max objective and diversity of returned set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported by the paper as baseline comparisons; in this paper ROBOT is used as a baseline and is competitive in some continuous tasks but cannot be applied to structured discrete MOF search without continuous embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against qVS-BayesOpt and TuRBO in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>qVS-BayesOpt was competitive with or outperformed ROBOT in certain tasks; ROBOT requires a tuned τ and may be preferred when constraints and geometry are well understood.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>ROBOT provides a structured way to produce diverse candidates but requires tuning τ which can be costly; paper argues qVS can avoid this hyperparameter.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper contrasts ROBOT's hard-constraint diversity (τ) vs qVS's soft, quality-weighted diversity and discusses trade-offs in hyperparameter specification and applicability to structured/discrete domains.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>If problem geometry is known and τ is meaningful, ROBOT is appropriate; otherwise qVS offers a more flexible automatic balance between diversity and quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2422.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2422.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TuRBO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trust-region Bayesian Optimization (TuRBO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalable local Bayesian optimization approach that runs several local trust-region optimizers in parallel and uses surrogate sampling (e.g., Thompson sampling) within trust regions to tackle high-dimensional black-box optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scalable Global Optimization via Local Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TuRBO (trust-region BayesOpt)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>TuRBO maintains multiple local Gaussian-process-based trust regions that adaptively expand or shrink depending on local performance; within each trust region, candidates are proposed (often with Thompson sampling) to exploit local structure while parallel regions enable global exploration. In qVS-BayesOpt TuRBO is used to produce candidate pools whose surrogate samples serve as quality inputs for qVS selection.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>High-dimensional continuous black-box optimization tasks (e.g., control tasks, path planning, materials optimization with continuous features).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates evaluation budget across multiple trust regions; local sampling focuses budget into promising neighborhoods while multiple regions cover space.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of objective evaluations and cost of maintaining multiple local GPs; trust-region machinery aims to improve sample-efficiency (discover more per evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not explicitly mutual-information-based; uses surrogate posterior samples and acquisition mechanics (e.g., Thompson sampling) to balance exploration and exploitation in local neighborhoods.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Parallel local optimizers (exploit within regions) combined with multiple trust regions (explore globally); trust-region size adaptation mediates exploration/exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via multiple trust regions; not primarily a diversity-enforcing objective though can be combined with diversity objectives (e.g., ROBOT or qVS selection).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of function evaluations; parallel allocation across trust regions.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Adaptive allocation of evaluations by expanding/shrinking trust regions based on recent improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best objective values found within evaluation budget; sample efficiency (objective vs number of evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>TuRBO is used as a baseline; qVS-BayesOpt extends TuRBO by selecting diverse candidates from TuRBO-generated pools.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a baseline diversity-blind optimizer in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>qVS-BayesOpt (built on TuRBO) showed improved diversity and often competitive or better optimization performance versus TuRBO alone in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>TuRBO improves scalability to higher dimensions; integrating qVS can improve effectiveness per evaluation by diversifying candidate selection.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>TuRBO controls local/global trade-offs via trust region dynamics; qVS adds an explicit set-level diversity-quality trade-off to candidate selection from TuRBO.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Combining local trust-region sampling with a set-level diversity utility (qVS) yields better coverage of distinct high-quality regions than pure TuRBO exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2422.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2422.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Determinantal Point Process-based Diversity Methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DPPs are probabilistic models that naturally encode repulsion/negative correlation (diversity) and can incorporate per-item quality via diagonal scaling of a kernel matrix, often used to diversify batches in Bayesian optimization and sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Determinantal Point Processes for Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DPP-based diversity selection</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DPPs model set probabilities proportional to determinant of a quality-weighted kernel (volume spanned by feature vectors); they encourage selecting diverse subsets and can incorporate per-item quality by scaling kernel entries by item qualities. They are commonly used to diversify batch proposals in BayesOpt and active learning.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Batch selection for BayesOpt, active learning, and general diversified sampling tasks in ML and experimental design.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Selects batches to maximize DPP likelihood (or sample from DPP) to trade off quality and diversity; often used as an acquisition augmentation to standard BO acquisition functions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost of sampling or MAP selection from a DPP (matrix determinants/eigen-decompositions, scaling roughly O(n^3) in batch size); oracle evaluation budget remains primary external cost.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Typically not formulated as information gain; objective is set-probability under DPP (volume/likelihood), sometimes combined with acquisition functions that encode expected improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Quality weighting (diagonal scaling) biases selection toward high-quality items (exploitation) while determinant/repulsion enforces diversity (exploration); however DPP likelihoods can go to zero with duplicates and behave differently than qVS.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via negative correlations encoded by determinants of the kernel matrix—sets with orthogonal/less-similar feature vectors get higher probability.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Used alongside fixed evaluation budgets; batch size constraints also apply.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Often performing MAP selection or approximate sampling to produce a batch under computational limits; combined with acquisition heuristics to control budget use.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not intrinsic—evaluated by downstream objectives (e.g., improvement in best-found value, diversity of discovered set).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Referenced as a common baseline/approach for inducing diversity in BayesOpt; the paper notes DPPs can incorporate quality via kernel scaling but that similar scaling does not behave desirably for the Vendi Score.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned and compared conceptually to qVS; in literature used as a baseline for diversified batch BO (e.g., Wang et al., Nava et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Paper argues qVS yields interpretable effective-count semantics and avoids DPP degeneracy with duplicates; empirical BO comparisons in literature show DPP-based batch methods are useful, but qVS provides an alternative set-utility perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>DPPs can yield more diverse batches improving downstream discovery per oracle call; computational cost of kernel determinants and sampling is a consideration.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>DPPs balance quality (via scaling) and diversity (via determinant), but behaviors differ from qVS because qVS uses entropy-based effective count vs DPP volume interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>qVS proposed as an alternative when DPP scaling of kernels does not yield monotonic dependence on per-item quality in entropy-based set metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2422.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2422.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ECI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Coverage Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active search method (Malkomes et al., 2021) that encourages diversity by maximizing coverage increase, implemented by summing volumes of hyperspheres drawn around discovered targets as a coverage metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Beyond the Pareto Efficient Frontier: Constraint Active Search for Multiobjective Experimental Design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Expected Coverage Improvement (ECI)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ECI evaluates candidate additions based on the expected increase in coverage, approximated as the summed volumes of balls/hyperspheres around predicted targets; points close to existing discoveries contribute less incremental coverage and are thus de-prioritized.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active search for diverse discovery in experimental design, including materials and molecular searches.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Selects queries with high expected increase in coverage (volume) using predictive probabilities, trading off hit-rate and spatial coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Oracle queries (number of experiments) and cost of approximate volume calculations over the feature space; overhead scales with number of candidates and dimensionality for coverage computations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected coverage increase (a geometric proxy for information about distinct regions); not mutual-information based.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploitation via predictive score of being a target; exploration via coverage term that penalizes candidates near already-discovered targets.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit coverage objective that reduces marginal utility for candidates close to existing positives.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed query budget and batch sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selects batches based on marginal expected coverage increase until budgeted queries are allocated.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Coverage of discovered positives and raw hit counts; experiments in paper used ECI as a baseline and tuned hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as a baseline in active search experiments; qVS-AS often outperformed or was competitive with ECI across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in AS experiments (photoswitch, materials, FashionMNIST) against qVS-AS and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>qVS-AS typically achieved higher Vendi Scores; ECI performance depends on hyperparameter tuning and geometry of search space.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>ECI improves spatial coverage per query compared to naive exploitative search but requires hyperparameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>ECI embodies a geometric diversity vs hit-rate tradeoff; paper contrasts coverage-based approaches with qVS's kernel-eigenvalue entropy-based diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Coverage objectives can be effective but require defining radius/volume parameters that may be difficult in high dimensions; qVS provides an alternative that avoids geometric hyperparameter specification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2422.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2422.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELECT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELECT (diversity-aware active selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method (Vanchinathan et al., 2015) that extends selection strategies with an explicit priority for diversity, used as an active-search baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discovering Valuable Items from Massive Data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SELECT (diversity-prioritized selection)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SELECT augments selection/acquisition strategies with a diversity priority (implemented via features or heuristics) to prefer items that increase coverage or spread among discovered positives, serving as a baseline diversity-aware AS algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active search and discovery in large data pools (recommendation, molecular/material discovery).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Ranks candidates by a combination of predicted utility and diversity heuristic; selects top-scoring items under fixed budget.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Oracle query count and computational cost of diversity heuristics over the candidate pool.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Heuristic combination of predictive score and diversity term; not an explicit probabilistic expected-information metric.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit heuristic balancing predicted utility and a diversity priority.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Heuristic diversity term (coverage or distance-based) included in scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of queries/batch sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selects candidates under budget using combined score until budget exhausted; hyperparameters control the diversity/utility balance.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Hit rate and diversity metrics of discovered items; used as baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in AS experiments; qVS-AS often outperformed SELECT when hyperparameters of baselines were tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a comparative baseline in active search experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>qVS-AS outperformed SELECT on Vendi Score metrics in multiple datasets in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>SELECT improves diversity of discoveries compared to purely exploitative search, but may need tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>SELECT requires tuning to balance exploration/exploitation; qVS offers an interpretable continuous alternative using eigenvalue-entropy-based diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Discovering Many Diverse Solutions with Bayesian Optimization <em>(Rating: 2)</em></li>
                <li>Scalable Global Optimization via Local Bayesian Optimization <em>(Rating: 2)</em></li>
                <li>Beyond the Pareto Efficient Frontier: Constraint Active Search for Multiobjective Experimental Design <em>(Rating: 2)</em></li>
                <li>Bayesian Optimal Active Search and Surveying <em>(Rating: 2)</em></li>
                <li>Determinantal Point Processes for Machine Learning <em>(Rating: 2)</em></li>
                <li>The Vendi Score: A Diversity Evaluation Metric for Machine Learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2422",
    "paper_id": "paper-269605978",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "qVS",
            "name_full": "Quality-weighted Vendi Score",
            "brief_description": "A similarity-based set utility that multiplies the Vendi Score (entropy of normalized kernel eigenvalues measuring diversity) by the average quality score of items, yielding an interpretable metric that jointly rewards diversity and item quality with a tunable sensitivity parameter q.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Quality-weighted Vendi Score (qVS)",
            "system_description": "qVS = (average per-item quality) * VS_q, where VS_q is the order-q Vendi Score (exp of Rényi-type entropy of normalized eigenvalues of the similarity/kernel matrix). The kernel k(x,x') encodes similarity; s(x) (or model predictive scores p(x)) supply per-item qualities. The order q &gt;= 0 controls sensitivity to eigenvalue non-uniformity (trade-off between treating items as effectively distinct vs requiring strictly unique items). qVS is used both as an objective to evaluate candidate batches/sets and as a utility to score marginal gains when selecting experiments.",
            "application_domain": "General experimental design: active search, Bayesian optimization, molecular/materials discovery, reinforcement learning trajectories",
            "resource_allocation_strategy": "Allocate experiments by selecting subsets that maximize qVS (either directly by continuous optimization when gradients are available, or approximately via a sequential greedy heuristic for discrete pools). For Bayesian/active search, per-item predictive scores (e.g., p(y=1|x) or surrogate objective values f(x) or UCB samples) are used as the quality s(x) inside qVS; batches are chosen to maximize expected qVS of returned positives or solutions.",
            "computational_cost_metric": "Implicitly measured as number of expensive oracle evaluations (fixed batch sizes / fixed total number of queries); computational overhead also includes cost of evaluating kernel matrices and their eigen-decompositions (O(n^3) in batch size n for naive eigen-decomposition) and cost of surrogate modeling and candidate scoring.",
            "information_gain_metric": "Expected increase in qVS (or expected increase in VS of positives) is used as the decision utility; surrogate predictive probabilities or posterior samples (Thompson sampling / UCB) are used to form expectations.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Balanced via the qVS objective: quality term (average s(x)) encourages exploitation (high-quality candidates) while the VS_q term rewards diverse sets (exploration). The order q modulates the relative emphasis on diversity vs quality (lower q favors counting more items, higher q enforces stronger diversity). In sequential decision-making the algorithm greedily selects items maximizing expected marginal increase of qVS.",
            "diversity_mechanism": "Explicit: diversity is quantified by the Vendi Score computed from the similarity/kernel matrix of the candidate set (entropy of normalized eigenvalues). Diversity promotion is directly baked into the utility (VS_q) rather than via hard distance constraints.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of expensive oracle queries (batch size and total iterations); discrete pool selection or continuous domain with fixed batch sizes/trust-region budgets.",
            "budget_constraint_handling": "Handles budget by sequential greedy selection until the desired batch size is reached, or by gradient-based optimization with multi-start when continuous and gradients available; uses trust-region TuRBO-style local sampling + Thompson sampling to produce candidates in expensive continuous problems.",
            "breakthrough_discovery_metric": "Metrics operationalized include Vendi Score of discovered positives (effective number of distinct high-quality discoveries), maximum objective value found (best f(x)), effective discovery count (EDC) thresholded by objective, and domain-specific quantities (e.g., MOF storage capacity, energy penalty).",
            "performance_metrics": "Reported metrics include Vendi Scores (for various q orders) of discovered positives, best objective values found (e.g., MOF storage capacity; Table 4 reports best values across q settings), effective discovery counts, and domain-specific penalties (energy penalty for MOF release). The paper reports a 70%–170% increase in number of effective discoveries compared to baselines (abstract).",
            "comparison_baseline": "Compared to diversity-blind active search/BayesOpt (count-based AS, TuRBO/UCB BayesOpt), ROBOT (rank-ordered diverse BayesOpt), random search, Expected Coverage Improvement (ECI), SELECT, and DPP-based approaches mentioned in related work.",
            "performance_vs_baseline": "qVS-based policies (qVS-AS and qVS-BayesOpt with q≈1) achieved the highest Vendi Scores in most active search problems and were competitive or superior to ROBOT and TuRBO in BayesOpt tasks; in some tasks (lunar lander, MOF search) encouraging diversity improved optimization progress compared to traditional BayesOpt. The paper states a 70%–170% increase in effective discoveries vs baselines; specific best-MOF storage values and energy-penalty improvements are shown in experimental tables/figures.",
            "efficiency_gain": "Empirical gains reported as increases in effective discoveries (70%–170%) and improved best-objective outcomes in several tasks; no explicit runtime/FLOP reductions reported—efficiency described in terms of experimental (oracle) budget utilization and improved discovery per query.",
            "tradeoff_analysis": "Yes — the paper analyzes the trade-off via the order q: lower q (e.g., q=0) recovers count-based search favoring exploitation; higher q increases priority on diversity. They show q that matches evaluation metric tends to perform best; varying q produces smooth changes in behavior. They note that diversity can improve optimization progress in rugged search spaces (avoiding local optima), and recommend tuning q or potentially adapting it dynamically.",
            "optimal_allocation_findings": "Key findings: (1) Using qVS provides an interpretable, continuous mechanism to balance diversity and quality without hard distance constraints (removing need for tuned τ used by constraint-based methods); (2) greedy sequential selection provides scalable approximate maximization in discrete pools; (3) combining qVS with local BayesOpt strategies (TuRBO + Thompson sampling) yields practical diverse BayesOpt; (4) tuning q is important—q≈1 often a good default, but task-dependent tuning can improve results.",
            "uuid": "e2422.0",
            "source_info": {
                "paper_title": "Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "qVS-AS",
            "name_full": "Quality-weighted Vendi Score Active Search",
            "brief_description": "An active search algorithm that selects batches to maximize expected qVS of discovered positives, using model predictive probabilities as per-item quality and a greedy sequential batch construction to manage combinatorial complexity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "qVS-AS (diverse active search)",
            "system_description": "Given a pool of unlabeled candidates and a probabilistic model producing p(y=1|x), qVS-AS scores candidate additions by expected marginal increase in qVS of the positive subset (using p(x) as s(x)). For discrete pools, it constructs batches greedily: at each step, compute marginal qVS gain for each remaining candidate (using current predictive probabilities and kernel similarities) and pick the maximizer until batch size n is reached. For sequential (single-query) operation, the one-step Bayesian-optimal decision is to maximize expected increase in VS of positives.",
            "application_domain": "Rare-item discovery tasks: molecular photoswitches, materials (bulk metallic glasses), product recommendation simulations (FashionMNIST), and other experimental discovery settings with expensive labels.",
            "resource_allocation_strategy": "Allocates oracle queries by scoring candidates by expected marginal increase in qVS (i.e., expected increase in effective number of high-quality discoveries) and greedily selecting the highest-scoring candidates until budgeted batch size is reached.",
            "computational_cost_metric": "Primarily counts of oracle queries (batch size / number of iterations), plus computational cost of evaluating p(y|x), computing kernel matrices and eigenvalues for candidate sets; combinatorial cost avoided via greedy heuristic and limited batch sizes.",
            "information_gain_metric": "Expected increase in qVS of positives (using predictive probabilities to compute expectation) serves as the utility; this acts as the information-aware decision criterion in a Bayesian decision framework.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration via VS term (diversity of positives), exploitation via average predictive probability term s(x); the order q modulates sensitivity. For q=0, reduces to count-based active search (pure exploitation for hits); larger q enforces exploration.",
            "diversity_mechanism": "Explicit qVS objective using kernel-based Vendi Score computed on predicted positives, encouraging selection of positives spread in feature/kernel space.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed batch sizes and total number of queries (oracle calls).",
            "budget_constraint_handling": "Greedy marginal-gain selection until budgeted batch size; uses predictive model to prioritize likely positives while maintaining diversity via qVS.",
            "breakthrough_discovery_metric": "Vendi Score of discovered positives (effective distinct high-quality discoveries), hit rate and precision-at-k in predictive benchmarking, and domain-specific success definitions (e.g., photoswitch properties).",
            "performance_metrics": "Vendi Scores across q orders, maximum pairwise distance and determinant-of-kernel metrics for spread; reported consistent improvements in VS over baselines across the photoswitch, materials, and FashionMNIST tasks (numerical tables in paper).",
            "comparison_baseline": "Compared against Expected Coverage Improvement (ECI), SELECT, traditional active search (q=0/count), and diversity-blind baselines.",
            "performance_vs_baseline": "qVS-AS achieved highest VS in most experiments; when not best it was close to best tuned baseline. Demonstrated better spread (max pairwise distance, kernel determinant) and higher effective discovery counts.",
            "efficiency_gain": "Reported increases in effective diverse discoveries per fixed number of oracle queries (part of the 70%–170% overall claim); no explicit wall-clock savings reported.",
            "tradeoff_analysis": "Empirical analysis across q ∈ {0,0.1,0.5,1,2,∞} showing smooth behavioral change; matching q to evaluation metric yields best performance; higher q pushes toward stronger diversity.",
            "optimal_allocation_findings": "Greedy expected-qVS marginal gain selection is an effective approximate allocation strategy for constrained-budget active search, balancing between checking high-probability positives and diversifying discoveries.",
            "uuid": "e2422.1",
            "source_info": {
                "paper_title": "Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "qVS-BayesOpt",
            "name_full": "Quality-weighted Vendi Score Bayesian Optimization",
            "brief_description": "A BayesOpt extension that returns a set of M high-quality, diverse solutions by maximizing qVS over candidate sets, integrating trust-region local search (TuRBO) and surrogate posterior sampling (Thompson sampling) to produce candidate proposals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "qVS-BayesOpt (diverse Bayesian optimization)",
            "system_description": "At each BayesOpt iteration, surrogate models (GPs) or their local trust-region variants (TuRBO) produce candidate evaluations via Thompson sampling or UCB; these surrogate values are treated as per-item quality s(x)=f_sample(x) or UCB(x) and qVS (Eq.11) is maximized over a set size M to select the batch of query points. The algorithm replaces hard distance constraints (as in ROBOT) with the qVS objective to balance solution quality and diversity, and uses trust-region sampling to handle high-dimensional continuous spaces.",
            "application_domain": "Global optimization problems in materials and control: rover path planning, lunar lander control policies (reinforcement learning), metal-organic framework (MOF) property optimization, and general black-box optimization.",
            "resource_allocation_strategy": "Allocates queries by selecting sets that maximize qVS of candidate solutions (quality-weighted Vendi Score with quality derived from surrogate samples or UCB scores), using TuRBO to generate local candidate pools and greedy maximization to choose final queries; seeks multiple diverse high-quality proposals per iteration.",
            "computational_cost_metric": "Oracle evaluation budget (number of black-box function evaluations) is primary cost; additional computational costs include GP fitting and local candidate generation (TuRBO), and eigen-decomposition of kernel matrices for qVS evaluation. For discrete structured data (MOFs), continuous embedding is not required, removing VAE training cost but requiring kernel evaluations across candidates.",
            "information_gain_metric": "Utility is expected qVS or qVS computed on surrogate draws; not mutual-information based but uses Bayesian surrogate samples to assess expected utility increase.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Quality term (surrogate objective or UCB) pushes toward exploitation of high-value regions; VS term rewards diversity across solutions to encourage exploration. Trust-region sampling (TuRBO) provides local exploitation while multiple trust regions and qVS selection maintain global diversity.",
            "diversity_mechanism": "Explicit: qVS computed using a kernel derived from distance function (k_δ) replaces hard minimum-distance constraints; the order q controls diversity sensitivity. This removes the need for a user-specified distance threshold τ used by rank-ordered approaches like ROBOT.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of objective evaluations per iteration and overall (oracle calls); also fixed number M of solutions to return per iteration in rank-ordered scenarios.",
            "budget_constraint_handling": "Generates candidate sets within M trust regions, scores candidates via qVS on surrogate draws, and selects top-scoring batch constrained by the available evaluation budget; uses greedy selection to manage combinatorial explosion.",
            "breakthrough_discovery_metric": "Best observed objective values (max f(x)), Vendi Score of 'good' solutions (EDC above threshold), and domain-specific metrics (e.g., storage capacity in MOFs, energy penalty for toxin release).",
            "performance_metrics": "Plots of best objective value found across repeats, tables of best MOF storage capacity across q values, and EDC/Vendi Score of 'good' solutions; reports qVS-BayesOpt (q=1) competitive with or outperforming ROBOT and TuRBO in some tasks, and improved EDC and energy-penalty metrics in MOF experiments.",
            "comparison_baseline": "Compared with TuRBO (diversity-blind TuRBO BayesOpt), ROBOT (rank-ordered diverse BayesOpt with distance constraints), UCB-based BayesOpt (for discrete MOF case), and random search.",
            "performance_vs_baseline": "qVS-BayesOpt is competitive with ROBOT in continuous problems and outperformed both TuRBO and ROBOT in some discrete/structured tasks (MOF, lunar lander), where encouraging diversity helped escape local optima; in MOF search, qVS-BayesOpt improved effective discovery count and lowered average energy penalty vs baselines.",
            "efficiency_gain": "Improved discovery per oracle call and improved best-objective attainment under the same evaluation budget; quantitative claim in paper: overall 70%–170% more effective discoveries vs baselines (abstract), with task-specific numeric tables showing best-objective improvements.",
            "tradeoff_analysis": "Paper analyzes effect of order q on BayesOpt performance (Table 4 shows best MOF storage across q values) and notes that improper q can harm performance (some q values worse than q=0), suggesting q tuning or dynamic adaptation as an important design choice.",
            "optimal_allocation_findings": "Principle: replacing hard distance constraints with qVS yields an automatic quality-vs-diversity trade-off that avoids manual τ tuning; combining qVS with local BayesOpt sampling (TuRBO + Thompson) yields practical allocations of limited oracle budget to diverse high-quality candidates.",
            "uuid": "e2422.2",
            "source_info": {
                "paper_title": "Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ROBOT",
            "name_full": "ROBOT (Rank-Ordered Bayesian Optimization with Trust Regions)",
            "brief_description": "A rank-ordered BayesOpt approach (Maus et al., 2023) that maintains multiple high-quality solutions enforced to be at least a fixed distance τ from one another, implemented via trust-region local optimizers (extension of TuRBO).",
            "citation_title": "Discovering Many Diverse Solutions with Bayesian Optimization",
            "mention_or_use": "use",
            "system_name": "ROBOT (rank-ordered BayesOpt)",
            "system_description": "ROBOT generates M solutions x*_1,...,x*_M by sequentially optimizing the surrogate objective under hard pairwise distance constraints δ(x*_i, x*_j) ≥ τ for j&lt;i, typically using local TuRBO-style trust-region optimization (often with Thompson sampling inside trust regions). The hard constraint τ enforces diversity explicitly by geometry.",
            "application_domain": "Diverse solution discovery in Bayesian optimization tasks (materials design, control, etc.).",
            "resource_allocation_strategy": "Allocates evaluations by centering trust regions on current solutions and performing local optimization, producing M constrained solutions per allocation step that are mutually distant by τ.",
            "computational_cost_metric": "Oracle evaluations (function evaluations) and internal cost of multiple local optimizers; constraints introduce additional search cost to enforce τ.",
            "information_gain_metric": "Standard BayesOpt surrogate utilities (e.g., Thompson samples or acquisition functions) subject to distance constraints; not formulated as expected information gain explicitly.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Hard geometric constraints (τ) force exploration across distinct modes; local trust regions enable exploitation within neighborhoods.",
            "diversity_mechanism": "Explicit hard-distance constraints between solutions (minimum pairwise distance τ).",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed oracle evaluation budget and number of trust regions/local optimizers.",
            "budget_constraint_handling": "Allocates budget across trust regions and constrained optimizations; requirement to tune τ can affect feasibility and budget usage.",
            "breakthrough_discovery_metric": "High objective value solutions and diversity enforced by distance constraints; evaluated by max objective and diversity of returned set.",
            "performance_metrics": "Reported by the paper as baseline comparisons; in this paper ROBOT is used as a baseline and is competitive in some continuous tasks but cannot be applied to structured discrete MOF search without continuous embedding.",
            "comparison_baseline": "Compared against qVS-BayesOpt and TuRBO in the paper's experiments.",
            "performance_vs_baseline": "qVS-BayesOpt was competitive with or outperformed ROBOT in certain tasks; ROBOT requires a tuned τ and may be preferred when constraints and geometry are well understood.",
            "efficiency_gain": "ROBOT provides a structured way to produce diverse candidates but requires tuning τ which can be costly; paper argues qVS can avoid this hyperparameter.",
            "tradeoff_analysis": "Paper contrasts ROBOT's hard-constraint diversity (τ) vs qVS's soft, quality-weighted diversity and discusses trade-offs in hyperparameter specification and applicability to structured/discrete domains.",
            "optimal_allocation_findings": "If problem geometry is known and τ is meaningful, ROBOT is appropriate; otherwise qVS offers a more flexible automatic balance between diversity and quality.",
            "uuid": "e2422.3",
            "source_info": {
                "paper_title": "Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "TuRBO",
            "name_full": "Trust-region Bayesian Optimization (TuRBO)",
            "brief_description": "A scalable local Bayesian optimization approach that runs several local trust-region optimizers in parallel and uses surrogate sampling (e.g., Thompson sampling) within trust regions to tackle high-dimensional black-box optimization.",
            "citation_title": "Scalable Global Optimization via Local Bayesian Optimization",
            "mention_or_use": "use",
            "system_name": "TuRBO (trust-region BayesOpt)",
            "system_description": "TuRBO maintains multiple local Gaussian-process-based trust regions that adaptively expand or shrink depending on local performance; within each trust region, candidates are proposed (often with Thompson sampling) to exploit local structure while parallel regions enable global exploration. In qVS-BayesOpt TuRBO is used to produce candidate pools whose surrogate samples serve as quality inputs for qVS selection.",
            "application_domain": "High-dimensional continuous black-box optimization tasks (e.g., control tasks, path planning, materials optimization with continuous features).",
            "resource_allocation_strategy": "Allocates evaluation budget across multiple trust regions; local sampling focuses budget into promising neighborhoods while multiple regions cover space.",
            "computational_cost_metric": "Number of objective evaluations and cost of maintaining multiple local GPs; trust-region machinery aims to improve sample-efficiency (discover more per evaluation).",
            "information_gain_metric": "Not explicitly mutual-information-based; uses surrogate posterior samples and acquisition mechanics (e.g., Thompson sampling) to balance exploration and exploitation in local neighborhoods.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Parallel local optimizers (exploit within regions) combined with multiple trust regions (explore globally); trust-region size adaptation mediates exploration/exploitation.",
            "diversity_mechanism": "Implicit via multiple trust regions; not primarily a diversity-enforcing objective though can be combined with diversity objectives (e.g., ROBOT or qVS selection).",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed number of function evaluations; parallel allocation across trust regions.",
            "budget_constraint_handling": "Adaptive allocation of evaluations by expanding/shrinking trust regions based on recent improvements.",
            "breakthrough_discovery_metric": "Best objective values found within evaluation budget; sample efficiency (objective vs number of evaluations).",
            "performance_metrics": "TuRBO is used as a baseline; qVS-BayesOpt extends TuRBO by selecting diverse candidates from TuRBO-generated pools.",
            "comparison_baseline": "Used as a baseline diversity-blind optimizer in comparisons.",
            "performance_vs_baseline": "qVS-BayesOpt (built on TuRBO) showed improved diversity and often competitive or better optimization performance versus TuRBO alone in experiments.",
            "efficiency_gain": "TuRBO improves scalability to higher dimensions; integrating qVS can improve effectiveness per evaluation by diversifying candidate selection.",
            "tradeoff_analysis": "TuRBO controls local/global trade-offs via trust region dynamics; qVS adds an explicit set-level diversity-quality trade-off to candidate selection from TuRBO.",
            "optimal_allocation_findings": "Combining local trust-region sampling with a set-level diversity utility (qVS) yields better coverage of distinct high-quality regions than pure TuRBO exploitation.",
            "uuid": "e2422.4",
            "source_info": {
                "paper_title": "Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DPP",
            "name_full": "Determinantal Point Process-based Diversity Methods",
            "brief_description": "DPPs are probabilistic models that naturally encode repulsion/negative correlation (diversity) and can incorporate per-item quality via diagonal scaling of a kernel matrix, often used to diversify batches in Bayesian optimization and sampling.",
            "citation_title": "Determinantal Point Processes for Machine Learning",
            "mention_or_use": "use",
            "system_name": "DPP-based diversity selection",
            "system_description": "DPPs model set probabilities proportional to determinant of a quality-weighted kernel (volume spanned by feature vectors); they encourage selecting diverse subsets and can incorporate per-item quality by scaling kernel entries by item qualities. They are commonly used to diversify batch proposals in BayesOpt and active learning.",
            "application_domain": "Batch selection for BayesOpt, active learning, and general diversified sampling tasks in ML and experimental design.",
            "resource_allocation_strategy": "Selects batches to maximize DPP likelihood (or sample from DPP) to trade off quality and diversity; often used as an acquisition augmentation to standard BO acquisition functions.",
            "computational_cost_metric": "Cost of sampling or MAP selection from a DPP (matrix determinants/eigen-decompositions, scaling roughly O(n^3) in batch size); oracle evaluation budget remains primary external cost.",
            "information_gain_metric": "Typically not formulated as information gain; objective is set-probability under DPP (volume/likelihood), sometimes combined with acquisition functions that encode expected improvement.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Quality weighting (diagonal scaling) biases selection toward high-quality items (exploitation) while determinant/repulsion enforces diversity (exploration); however DPP likelihoods can go to zero with duplicates and behave differently than qVS.",
            "diversity_mechanism": "Implicit via negative correlations encoded by determinants of the kernel matrix—sets with orthogonal/less-similar feature vectors get higher probability.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Used alongside fixed evaluation budgets; batch size constraints also apply.",
            "budget_constraint_handling": "Often performing MAP selection or approximate sampling to produce a batch under computational limits; combined with acquisition heuristics to control budget use.",
            "breakthrough_discovery_metric": "Not intrinsic—evaluated by downstream objectives (e.g., improvement in best-found value, diversity of discovered set).",
            "performance_metrics": "Referenced as a common baseline/approach for inducing diversity in BayesOpt; the paper notes DPPs can incorporate quality via kernel scaling but that similar scaling does not behave desirably for the Vendi Score.",
            "comparison_baseline": "Mentioned and compared conceptually to qVS; in literature used as a baseline for diversified batch BO (e.g., Wang et al., Nava et al.).",
            "performance_vs_baseline": "Paper argues qVS yields interpretable effective-count semantics and avoids DPP degeneracy with duplicates; empirical BO comparisons in literature show DPP-based batch methods are useful, but qVS provides an alternative set-utility perspective.",
            "efficiency_gain": "DPPs can yield more diverse batches improving downstream discovery per oracle call; computational cost of kernel determinants and sampling is a consideration.",
            "tradeoff_analysis": "DPPs balance quality (via scaling) and diversity (via determinant), but behaviors differ from qVS because qVS uses entropy-based effective count vs DPP volume interpretation.",
            "optimal_allocation_findings": "qVS proposed as an alternative when DPP scaling of kernels does not yield monotonic dependence on per-item quality in entropy-based set metrics.",
            "uuid": "e2422.5",
            "source_info": {
                "paper_title": "Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ECI",
            "name_full": "Expected Coverage Improvement",
            "brief_description": "An active search method (Malkomes et al., 2021) that encourages diversity by maximizing coverage increase, implemented by summing volumes of hyperspheres drawn around discovered targets as a coverage metric.",
            "citation_title": "Beyond the Pareto Efficient Frontier: Constraint Active Search for Multiobjective Experimental Design",
            "mention_or_use": "use",
            "system_name": "Expected Coverage Improvement (ECI)",
            "system_description": "ECI evaluates candidate additions based on the expected increase in coverage, approximated as the summed volumes of balls/hyperspheres around predicted targets; points close to existing discoveries contribute less incremental coverage and are thus de-prioritized.",
            "application_domain": "Active search for diverse discovery in experimental design, including materials and molecular searches.",
            "resource_allocation_strategy": "Selects queries with high expected increase in coverage (volume) using predictive probabilities, trading off hit-rate and spatial coverage.",
            "computational_cost_metric": "Oracle queries (number of experiments) and cost of approximate volume calculations over the feature space; overhead scales with number of candidates and dimensionality for coverage computations.",
            "information_gain_metric": "Expected coverage increase (a geometric proxy for information about distinct regions); not mutual-information based.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploitation via predictive score of being a target; exploration via coverage term that penalizes candidates near already-discovered targets.",
            "diversity_mechanism": "Explicit coverage objective that reduces marginal utility for candidates close to existing positives.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed query budget and batch sizes.",
            "budget_constraint_handling": "Selects batches based on marginal expected coverage increase until budgeted queries are allocated.",
            "breakthrough_discovery_metric": "Coverage of discovered positives and raw hit counts; experiments in paper used ECI as a baseline and tuned hyperparameters.",
            "performance_metrics": "Used as a baseline in active search experiments; qVS-AS often outperformed or was competitive with ECI across tasks.",
            "comparison_baseline": "Compared in AS experiments (photoswitch, materials, FashionMNIST) against qVS-AS and other baselines.",
            "performance_vs_baseline": "qVS-AS typically achieved higher Vendi Scores; ECI performance depends on hyperparameter tuning and geometry of search space.",
            "efficiency_gain": "ECI improves spatial coverage per query compared to naive exploitative search but requires hyperparameter tuning.",
            "tradeoff_analysis": "ECI embodies a geometric diversity vs hit-rate tradeoff; paper contrasts coverage-based approaches with qVS's kernel-eigenvalue entropy-based diversity.",
            "optimal_allocation_findings": "Coverage objectives can be effective but require defining radius/volume parameters that may be difficult in high dimensions; qVS provides an alternative that avoids geometric hyperparameter specification.",
            "uuid": "e2422.6",
            "source_info": {
                "paper_title": "Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "SELECT",
            "name_full": "SELECT (diversity-aware active selection)",
            "brief_description": "A method (Vanchinathan et al., 2015) that extends selection strategies with an explicit priority for diversity, used as an active-search baseline in the paper.",
            "citation_title": "Discovering Valuable Items from Massive Data",
            "mention_or_use": "use",
            "system_name": "SELECT (diversity-prioritized selection)",
            "system_description": "SELECT augments selection/acquisition strategies with a diversity priority (implemented via features or heuristics) to prefer items that increase coverage or spread among discovered positives, serving as a baseline diversity-aware AS algorithm.",
            "application_domain": "Active search and discovery in large data pools (recommendation, molecular/material discovery).",
            "resource_allocation_strategy": "Ranks candidates by a combination of predicted utility and diversity heuristic; selects top-scoring items under fixed budget.",
            "computational_cost_metric": "Oracle query count and computational cost of diversity heuristics over the candidate pool.",
            "information_gain_metric": "Heuristic combination of predictive score and diversity term; not an explicit probabilistic expected-information metric.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Explicit heuristic balancing predicted utility and a diversity priority.",
            "diversity_mechanism": "Heuristic diversity term (coverage or distance-based) included in scoring.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of queries/batch sizes.",
            "budget_constraint_handling": "Selects candidates under budget using combined score until budget exhausted; hyperparameters control the diversity/utility balance.",
            "breakthrough_discovery_metric": "Hit rate and diversity metrics of discovered items; used as baseline in experiments.",
            "performance_metrics": "Reported in AS experiments; qVS-AS often outperformed SELECT when hyperparameters of baselines were tuned.",
            "comparison_baseline": "Used as a comparative baseline in active search experiments.",
            "performance_vs_baseline": "qVS-AS outperformed SELECT on Vendi Score metrics in multiple datasets in the paper.",
            "efficiency_gain": "SELECT improves diversity of discoveries compared to purely exploitative search, but may need tuning.",
            "tradeoff_analysis": "SELECT requires tuning to balance exploration/exploitation; qVS offers an interpretable continuous alternative using eigenvalue-entropy-based diversity.",
            "uuid": "e2422.7",
            "source_info": {
                "paper_title": "Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Discovering Many Diverse Solutions with Bayesian Optimization",
            "rating": 2,
            "sanitized_title": "discovering_many_diverse_solutions_with_bayesian_optimization"
        },
        {
            "paper_title": "Scalable Global Optimization via Local Bayesian Optimization",
            "rating": 2,
            "sanitized_title": "scalable_global_optimization_via_local_bayesian_optimization"
        },
        {
            "paper_title": "Beyond the Pareto Efficient Frontier: Constraint Active Search for Multiobjective Experimental Design",
            "rating": 2,
            "sanitized_title": "beyond_the_pareto_efficient_frontier_constraint_active_search_for_multiobjective_experimental_design"
        },
        {
            "paper_title": "Bayesian Optimal Active Search and Surveying",
            "rating": 2,
            "sanitized_title": "bayesian_optimal_active_search_and_surveying"
        },
        {
            "paper_title": "Determinantal Point Processes for Machine Learning",
            "rating": 2,
            "sanitized_title": "determinantal_point_processes_for_machine_learning"
        },
        {
            "paper_title": "The Vendi Score: A Diversity Evaluation Metric for Machine Learning",
            "rating": 2,
            "sanitized_title": "the_vendi_score_a_diversity_evaluation_metric_for_machine_learning"
        }
    ],
    "cost": 0.02210525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Quality-Weighted Vendi Scores and Their Application to Diverse Experimental Design
3 May 2024</p>
<p>Quan Nguyen 
Department of Computer Science &amp; Engineering
Washington University in St. Louis</p>
<p>Adji Bousso Dieng 
Department of Computer Science
Princeton University</p>
<p>Quality-Weighted Vendi Scores and Their Application to Diverse Experimental Design
3 May 2024A68DCAD30299979342A77EFFC981AFB1arXiv:2405.02449v1[stat.ML]Vendi ScoringActive LearningActive SearchBayesian OptimizationReinforcement LearningBiologyMaterials ScienceMachine Learning
Experimental design techniques such as active search and Bayesian optimization are widely used in the natural sciences for data collection and discovery.However, existing techniques tend to favor exploitation over exploration of the search space, which causes them to get stuck in local optima.This collapse problem prevents experimental design algorithms from yielding diverse high-quality data.In this paper, we extend the Vendi scores-a family of interpretable similarity-based diversity metrics-to account for quality.We then leverage these quality-weighted Vendi scores to tackle experimental design problems across various applications, including drug discovery, materials discovery, and reinforcement learning.We found that quality-weighted Vendi scores allow us to construct policies for experimental design that flexibly balance quality and diversity, and ultimately assemble rich and diverse sets of high-performing data points.Our algorithms led to a 70%-170% increase in the number of effective discoveries compared to baselines. 1</p>
<p>Introduction</p>
<p>Many real-world tasks can be framed as expensive discovery problems, where one explores large databases in search of rare, valuable items.For instance, a scientist aiming to find a drug for a disease may need to iterate over millions of molecules to discover those that bind to specific biological targets.These search problems also often involve an expensive labeling process: the scientist will need to perform costly, time-consuming experiments to test for a molecule's binding activity to study its characteristics.This high cost in experimentation rules out exhaustive search and motivates the need for more sophisticated search strategies.VS qVS q = 0 q = 0.1 The quality-weighted Vendi Score balances between the quality of the selected data points and their diversity; this balance is smoothly controlled by the order q.
q = 1 q = 5 q = ∞
Powerful active search (AS) and Bayesian optimization (BayesOpt) techniques have been developed over the years (Garnett et al., 2012;Jiang et al., 2017;Nguyen et al., 2021;Nguyen and Garnett, 2023;Eriksson et al., 2019) to tackle the problems mentioned above.While these advances have led to a more flexible experimental design framework, there has been a recent surge in interest in modifying existing algorithms to not only perform effective search/optimization but also induce more diversity during experimentation.For example, Malkomes et al. (2021) proposed an AS method in which data points sufficiently close to discoveries already made are removed from the pool, effectively encouraging a more diverse search.Maus et al. (2023), on the other hand, formulated a BayesOpt problem where the goal is to maintain and optimize multiple solutions that are constrained to be different from one another.These works leverage local penalization to induce diversity.</p>
<p>Even though local penalization is a natural way to encourage diversity, it requires defining constraints to control the algorithms' behavior.However, setting these constraints can be challenging for complex, high-dimensional spaces.In this work, we present an alternative approach to local penalization to enforce diversity in experimental design.More specifically, we extend the Vendi scores (VS) (Friedman and Dieng, 2023;Pasarkar and Dieng, 2023) to account for the quality of the items in a given input set.We call these new scores quality-weighted Vendi scores.</p>
<p>These quality-weighted Vendi scores offer us a mathematically convenient way to evaluate quality and diversity and yield a unified framework for diverse experimental design.We applied the quality-weighted Vendi scores to both AS and BayesOpt across a wide variety of tasks involving drug discovery, materials discovery, and reinforcement learning.For all these tasks, we compared against strong existing baselines for experimental design, as well as analyzed the performance of our algorithms for various quality-diversity trade-offs.In all our experiments, we found that experimental design algorithms leveraging quality-weighted Vendi scores tend to outperform their counterparts in terms of both the diversity and the quality of the data points they yield.</p>
<p>The Quality-Weighted Vendi Score</p>
<p>We first provide background on the Vendi score (VS) as a metric of diversity of a given set and introduce our extension to the VS that incorporates quality scores of individual members.We then examine computationally efficient ways to optimize this quality-weighted VS, which will allow us to tackle a wide range of experimental design tasks in Section 3.</p>
<p>The Vendi Score</p>
<p>Consider a finite set of data points X = {x i } n i=1 in some domain .Friedman and Dieng (2023) introduced the Vendi Score (VS) to characterize the diversity of a collection of items such as X .VS is defined as the exponential of the Shannon entropy of the normalized eigenvalues of the kernel similarity matrix corresponding to X .Specifically, given a positive semidefinite similarity function k : × → , where k(x, x) = 1 for all x ∈ , denote by K ∈ n×n the kernel matrix corresponding to the set X = {x i } n i=1 where each entry K i, j = k(x i , x j ).Further denote the eigenvalues of K as λ 1 , λ 2 , . . ., λ n .The VS is defined as:
VS(X ; k) = exp − n i=1 λ i log λ i ,(1)
where λ 1 , λ 2 , . . ., λ n are the normalized eigenvalues of K such that λ i = λ i / n i=1 λ i , and 0 log 0 is defined to be 0. As K is a positive semidefinite matrix, the eigenvalues λ 1 , λ 2 , . . ., λ n are non-negative and the normalized eigenvalues λ 1 , λ 2 , . . ., λ n sum to 1.The VS is then valid and can be viewed as the Shannon entropy of these normalized eigenvalues.Friedman and Dieng (2023) explored the features of the VS as a diversity metric, and demonstrated that VS(X ; k) can be interpreted as the effective number of unique samples of X .In the extreme case where all items in X are unique, K is the identity matrix and VS(X ; K) = n.At the other end of the spectrum, if all items are identical, K is the all-one matrix and VS(X ; K) = 1.Overall, the VS offers us a mathematically principled yet convenient way to quantify the diversity of the items in a set X .Unlike the determinantal point process (DPP) likelihoods (Kulesza and Taskar, 2012), another tool commonly used in diversity-related machine learning tasks, VS does not reduce to 0 when there are duplicates in the input set X .Compared to Hill numbers (Hill, 1973), the VS is not restricted to the assumption that different data points (species) are completely dissimilar to one another and thus allows us to effectively account for similarity between pairs of items.</p>
<p>Accounting for Quality in the Vendi Score</p>
<p>The VS captures diversity but treats all items in X the same when it comes to their quality.In many situations, however, we may reasonably want our diversity metric to further express preference for items that exhibit desirable characteristics by incorporating "quality scores", upweighting or downweighting the final output based on the quality of individual items.In other words, given a score function s :</p>
<p>→ that quantifies the quality of a given item x ∈ , we are interested in an extension to the VS that increases not only with more diverse but also with higher-quality items.</p>
<p>One may be tempted to mimic the DPPs' ability to naturally incorporate "quality scores" into their likelihoods, whereby the input kernel matrix is modified so that each entry K i, j is multiplied with the two corresponding quality scores s(x i ) and s(x j ).Unfortunately, applying the same modification here for the VS does not lead to desirable results.The mismatch in behavior stems from the inherent mathematical difference between the two operations.The DPP likelihood can be interpreted as the volume spanned by the quality-weighted feature vectors of individual items x i whose outer products yield the kernel matrix.It therefore increases with higher values of s(x i ).The VS, on the other hand, computes the Shannon entropy of the normalized eigenvalues of the kernel matrix, which does not behave monotonically with respect to the values of the entries in K.</p>
<p>Our chosen solution to extend the VS to account for quality is simple-multiply the VS as defined in Eq. 1 by the average quality score of individual items:
qVS(X ; k, s) = n i=1 s(x i )/n VS(X ; k).
(2)</p>
<p>This quality-weighted VS, qVS for short, possesses multiple desiderata partially inherited from the VS.First, the output is maximized when all items are maximally diverse (the covariances K i, j = 0) and achieve the highest quality score.Conversely, the qVS is minimized when all items are identical and yield the lowest quality score.</p>
<p>Fixing the quality scores s(x i ), more diverse items (as measured by the VS) yield higher qVS values.Fixing the diversity score measured by the VS, higher-quality items yield higher qVS values.The intuitive interpretation of the VS as the effective number of unique samples carries over as well.The qVS can be interpreted as the effective number of high-quality samples in the input set.Overall, the qVS allows us to express our preference for diverse sets of high-quality data points.</p>
<p>Controlling the Quality-Diversity Trade-off</p>
<p>The balance between diversity and quality for a given set of items is explicitly achieved by maximizing the qVS in Eq. 2. However, in many scenarios, we may reasonably seek to control this balance to favor more diverse or higher-quality items, depending on our goal.Inspired by the Rényi entropy, Pasarkar and Dieng (2023) proposed a generalization of the VS by introducing an extra hyperparameter q ≥ 0, defining the VS of order q as:
VS q (X ; k) = exp 1 1 − q log n i=1 (λ i ) q , (3)
where λ 1 , λ 2 , . . ., λ n are, again, the normalized eigenvalues of the kernel matrix K corresponding to the input set X .Further, when q ∈ {0, 1, ∞}, the VS q is defined as the limit of Eq. 3 as q approaches the target order.We briefly note that when q = 1, we recover the traditional VS that is the Shannon entropy of the normalized eigenvalues of the similarity matrix.</p>
<p>The order q smoothly controls the sensitivity to the non-uniformity of these eigenvalues, and thus the evaluation of the diversity of X .A smaller value of q leads to a VS that is more sensitive to X in that the VS increases faster than that of a larger q when we add items to X .In the extreme case, VS 0 is simply the count function, which increases by 1 every time a new data point is added to X , ignoring the diversity of the set.For a larger q, it takes a completely unique data point (with zero covariances with other items) to lead to an increase of 1 in the VS.We use this more general VS in Eq. 2 to effectively balance quality and diversity.</p>
<p>Optimizing the Quality-Weighted Vendi Score</p>
<p>With the qVS in hand, we can evaluate the value of a given input set X , assessing its diversity and the quality of its members.A question naturally arises: given the domain , how can we identify a subset of a particular size that maximizes the qVS?That is, we want to find:
X * = arg max X ⊂ ,|X |=n qVS(X ; k, s). (4)
As the VS in Eq. 1 is differentiable, if the domain is a compact space, the qVS can be efficiently maximized using an off-the-shelf gradient-based optimizer with multi-start, assuming that we also have access to the gradients of the kernel k as well as the score function s.</p>
<p>The panels of Fig. 1 show an example in 2 dimensions within = [−1, 1] 2 .Here, we use an isotropic Gaussian kernel: k(x 1 , x 2 ) = exp −∥x 1 − x 2 ∥ 2 /2 , and a Gaussian score function centered at the origin: s(x ) = exp −∥x ∥ 2 /2 , shown as the heat maps.We run a gradient-based optimizer with multi-start to find a batch X * of size n = 10 maximizing the VS (top row) and the qVS (bottom row) of different orders q and show the members of the optimal batches as red x's.We notice interesting distinctions between the panels.The data points at the top maximizing the VS are well spread out across the domain, maximizing pure diversity.Further, as q increases, groups of close-by data points are discouraged, and the points are pushed towards the boundary.The data points maximizing the qVS at the bottom, on the other hand, favor points that yield high values for the score function while still achieving excellent diversity among the items.This figure showcases the qVS' ability to account for both quality and diversity, with the order q serving to flexibly control the balance between the two, with priority for diversity growing as q increases.</p>
<p>If we can only evaluate k or s in a black-box manner, or if the domain is a discrete set of items, optimization of the qVS becomes more challenging.In fact, given a discrete search space , exact maximization of Eq. 4 is a combinatorial problem, as we need to iterate over all possible subsets X of size n in search for the optimal X * .We instead opt for approximate maximization of the qVS using the sequential greedy heuristic (Nemhauser et al., 1978;Krause and Guestrin, 2007), which has found application in maximizing functions with diminishing returns, including DPPs' likelihoods.More specifically, we sequentially build the approximately optimal batch X * from the empty set, finding the item x that yields the largest increase in the qVS at each iteration:
x * = arg max x∈ \X * qVS X * ∪ {x}; k, s − qVS X * ; k, s .
(5)</p>
<p>We repeat this greedy search until the running batch X * reaches the desirable size, at which point we return X * as the set that approximately maximizes the qVS.While we do not prove any theoretical guarantee in terms of optimization performance of the greedy strategy on the qVS, we empirically observe that the procedure works well in our experiments and is efficient enough to run at scale.</p>
<p>Designing Diverse Experiments with the Vendi Score</p>
<p>We are tasked with sequentially querying an expensive-to-evaluate oracle to obtain observations of a system of interest.At each iteration of this procedure, we train a probabilistic model on the data collected so far and use the predictions of this model to decide which data point to label next.This process is repeated for a pre-specified number of iterations.The goal is to design effective ways to collect more data to maximize a metric of interest at the end of the procedure.</p>
<p>Diverse Active Search</p>
<p>Given a large but finite pool of unlabeled data , we seek to identify data points belonging to a rare, valuable class of interest.We label these valuable data points, referred to as positives, with y = 1 and use y = 0 for the other points, referred to as negatives.The label of a data point is not known a priori, but can be determined by querying an expensive oracle.Traditional AS targets achieving the highest "hit rate", that is, maximizing the number of positives in the collected data, (x, y)∈ y, where is the collection of data we have chosen to label at the end of the search.</p>
<p>Given this formulation, active search (AS) strategies tend to become too exploitative, making many observations within regions in the search space known to yield a high hit rate.As Nguyen and Garnett (2023) argued, in settings such as scientific discovery, there are diminishing returns in making additional discoveries in a frequently observed region: "a discovery in a novel region of the design space may offer more marginal insight than the 100th discovery in an already densely labeled region."We thus aim at an alternative AS setting that rewards diverse discoveries.As the VS has been established as a principled diversity metric, we propose to directly use it to measure our search performance when diversity is of interest and modify the AS objective to be maximized to be:
VS q + ; k , (6)
where the operator + gives the subset of positives within a set:
+ ≜ {x | (x, y) ∈ , y = 1}. (7)
Algorithm 1 qVS-AS for diverse active search 1: inputs observations , query batch size n 2: returns query batch X of size n maximizing 9 3: X ← ▷ sequentially built from the empty set 4: for i ← 1, . . ., n do 5:
for x ∈ \ ( ∪ X ) do 6: α(x) = qVS X + ( ) ∪ X ∪ {x}; k, p
▷ candidate scored by the qVS 7:</p>
<p>end for 8:</p>
<p>X ← X ∪ {arg max x∈ ( ∪X ) α(x)} ▷ add candidate yielding largest qVS 9: end for Our goal in this diversity-aware search is to collect a set of diverse positives.Interestingly, setting q = 0 recovers the base version of AS, where our utility function is the count function.We thus view our formulation as a generalization of traditional AS.</p>
<p>How should we design our queries to the oracle, sequentially selecting among the unlabeled data, so as to maximize the objective defined above?Assuming access to a probabilistic predictive model that outputs, Pr ( y = 1 | x, ), the probability that an unlabeled item x has a positive label given the data observed so far, we can derive the one-step Bayesian optimal decision, the data point x * that maximizes the expected increase of the objective in Eq. 6:
x * = arg max x∈ \ VS q ( ∪ {x}) + ; k − VS q + ; k ,(8)
where the expectation is taken with respect to the label of each unlabeled candidate x ∈ \ .</p>
<p>In a purely sequential regime where queries are made one after another, Bayesian decision theory guides us to select x * to greedily maximize our VS objective.In batch settings where multiple queries are made simultaneously to maximize experimental throughput-which are common in the real world-one may be tempted to simply extend the search criterion in Eq. 8 from a single candidate x ∈ \ to a batch of queries X ⊂ \ , seeking to maximize the expected increase of the VS of the positives in ∪ X .This is a daunting task.First, for any candidate batch X of size b, we need to iterate over 2 b possible label combinations of this batch to compute the expected VS after making these queries.Second, similar to the task of finding a batch to optimize the VS or the qVS, finding a batch to optimize the expected VS means searching over a combinatorial space, requiring exponential computational effort.</p>
<p>The difficulties above motivate us to find a computationally tractable alternative criterion to select the next queries at each iteration of a batch AS problem.An ideal batch of queries should balance between high probability for the candidates to have positive labels (after all, positives are the target of our search) and diversity among both those queries and the already observed positives.As its goal is precisely offering an evaluation function that balances between some metric of quality and diversity, we turn to the qVS for this task.Specifically, we set the scoring function s(x) to be exactly the probability of being a positive p(x), and use the qVS of the positives within ∪ X as our diverse search criterion.In other words, we seek to find:
X * = arg max X ⊂ \ qVS + ∪ X ; k, p = arg max X ⊂ \ x∈X + ( )∪X Pr ( y = 1 | x, ) /n VS + ∪ X ; k ,(9)
where n = | + ∪ X |.Our last step is to identify the batch X that maximizes this qVS metric, and we appeal to the methods described in 2.4 for this task.We show the pseudocode for our algorithm in Algorithm 1.</p>
<p>Diverse Bayesian Optimization</p>
<p>Bayesian optimization (BayesOpt) (Garnett, 2022) is a framework for optimizing black-box functions.Given a domain , which can be either discrete or continuous, BayesOpt sets out to find the global optimum of an objective function f of interest:
x * = arg max x∈ f (x).
Unlike active search (AS) where we work with binary labels, BayesOpt deals with real-valued labels y that are the outputs of the objective function f .Further, while AS focuses on finding many valuable data points within a search space, BayesOpt targets the singular, most valuable data point, tackling a different yet also relevant class of discovery problems commonly encountered in experimental design.</p>
<p>Similar to our discussion on AS, the pure-optimization formulation of BayesOpt often leads to overly exploitative strategies.Even if a BayesOpt algorithm can effectively identify the global optimum of the objective function, Maus et al. (2023) argued this "all-or-nothing" goal of finding a single best solution to a problem is undesirable in many scenarios.For example, when trying to discover metal-organic frameworks (MOFs) with high capacity for storage of toxic gasses, scientists may apply BayesOpt to identify a hypothetical MOF that, when simulated by a computer program, possesses a high storage capacity.However, this high-performing MOF may turn out to be infeasible to synthesize in practice due to having unrealistic physical attributes.This leads to wasted resources and efforts.Having a diverse set of MOF candidates would give the scientist higher chances at finding a synthesizable MOF that meats the target criteria.Maus et al. (2023) proposed a modified formulation that aims to find many diverse solutions.Their framework involves maintaining a set of possible solutions that are of high quality and diverse, where diversity is enforced by constraining the solutions to be at least a pre-specified distance away from one another.Formally, denote by δ a distance function for an objective function f of interest, they seek a sequence of M solutions {x * 1 , x * 2 , . . ., x * M } such that: X m = TuRBO m ( ) 5: end for 6: X ← ∪ M m=1 X m ▷ merge all candidates 7: X ← ▷ sequentially built from the empty set 8: for i ← 1, . . ., n do 9:
x * 1 = arg max x∈ f (x), x * i = arg max x∈ f (x) subject to δ(x * i , x * j ) ≥ τ, ∀ j &lt; i, (10)for x ∈ X \ X do 10: α(x) = qVS X ∪ {x}; k, f
▷ candidate scored by the qVS 11:</p>
<p>end for 12:</p>
<p>X ← X ∪ {arg max x∈X \X α(x)} ▷ add candidate yielding largest qVS 13: end for where τ is a user-specified distance threshold that controls the diversity of the resulting solutions.Maus et al. (2023) dubbed this formulation rank-ordered BayesOpt, as the solutions x * 1 , x * 2 , . . ., x * M are ranked in that each subsequent solution is constrained to be far away from those that precede it.The authors further proposed extending the trust region-based BayesOpt algorithm TuRBO (Eriksson et al., 2019) to this setting.TuRBO tackles high-dimensional problems via local optimization and consists of a set of local optimizers.Each optimizer maintains a trust region around a promising region, and the size of the region expands or shrinks based on optimization performance.(Local optimization is often accomplished with Thompson sampling (Russo et al., 2018) within each trust region.)This strategy is particularly amenable to the rank-ordered formulation above, as one could center a trust region around each member of the solution set x * i , and iteratively refine that member via local optimization while obeying the diversity constraints.The resulting algorithm, called ROBOT, was shown to be able to identify diverse and high-quality solutions in several tasks.</p>
<p>As mentioned, the goal of rank-ordered BayesOpt is to collect diverse data points that yield high objective values.We propose to also use our qVS for this task and seek:
X * = arg max X ⊂ qVS q (X ; k δ , f ) = arg max X ⊂ x∈X f (x)/M VS q (X ; k δ ),(11)
where M = |X | is the number of solutions we wish to return to the user, and k δ is the similarity function derived from the distance function δ (by, for example, inversing or subtracting from a maximum distance).This formulation removes the hyperparameter τ in 10 that constraints the solutions to be at least some distance away.We view this as a desirable feature in many instances, for example if the task of setting τ is not straightforward, which is often the case in high dimensions where reasoning about distances becomes challenging.By relying on the qVS to automatically balance diversity and quality among our solutions, we avoid this hyperparameter that might be difficult to tune.Furthermore, our algorithm can also take advantage of the trust region-based strategy of TuRBO.Specifically, at each iteration of the BayesOpt loop, we use Thompson sampling to generate samples f of the Gaussian process fitted on the objective function f within a local region of the domain, and use these samples f in lieu of the actual f (x) values to maximize the criterion in Eq. 11.The selected data points that maximize the criterion are chosen as our queries at the current iteration.We give the pseudocode for our algorithm in Algorithm 2.</p>
<p>Lastly, we acknowledge two potential concerns.First, if the geometry of the domain of the problem in question is well understood, and the user is confident the constraints in the formulation of ROBOT in Eq. 10 are desirable, then that method should indeed be preferred to ours, as ROBOT specifically adheres to the constraints provided to it.Second, while the qVS and VS metrics do have a hyperparameter of their own-the order q-which controls the sensitivity to the diversity of the items, we argue that the basic form with q = 1 serves as a good starting point, and observe good performance of q = 1 in our experiments.Empirically, a user may tune q during a validation step prior to running actual experiments by observing the induced behavior on a toy example and adjusting q to match their preference.</p>
<p>Related Work</p>
<p>The quality-weighted diversity metric described in this paper, qVS, directly extends the Vendi score (Friedman and Dieng, 2023), which has found use in a wide range of applications (Pasarkar et al., 2023;Berns et al., 2023;Wu et al., 2023;Liu et al., 2024).The qVS is an alternative to the commonly used likelihoods of determinantal point processes (DPP), which can also account for the quality of a set of items but do not have a natural interpretation suitable for evaluating diversity and quality.</p>
<p>We use the qVS to tackle two specific experimental design problems, active search (AS) and Bayesian optimization (BayesOpt), which commonly model discovery tasks in science and engineering.While Garnett et al. (2012) originally formulated AS as maximizing the raw number of discovered targets, multiple subsequent works have extended the AS framework to settings where diversity is of concern.Vanchinathan et al. (2015) considered an Upper Confidence Bound-style algorithm (Auer, 2002) with an extra priority for diversity.Malkomes et al. (2021) proposed maximizing a coverage objective, defined as the sum of the volumes of hyperspheres drawn around the targets discovered.This coverage metric encourages the queries to be far away from one another.Nguyen and Garnett (2023) considered a multiclass setting and opted for a metric that rewards diversity in the labels.</p>
<p>In BayesOpt, diversity has been artificially induced to aid optimization, commonly via a DPP (Wang et al., 2018;Nava et al., 2022).Maus et al. (2023), on the other hand, directly targeted discovering diverse, high-quality solutions.As discussed in 3, they extended the state-of-the-art TuRBO algorithm by constraining the current solutions to be some distance away from one another.The resulting BayesOpt method ROBOT, as well as methods for AS mentioned earlier, crucially depend on accurately specifying the constraints to achieve the desired diversity.However, these distance constraints can be challenging to specify and enforce accurately, especially in high dimensions or with structured data.The qVS, in addition to being interpretable, offers a more flexible approach to diverse experimental design: instead of employing hard constraints on how far apart the solutions should be, we rely on the qVS to balance quality and diversity.The qVS, as we will show in 3, can be flexibly applied to many settings, including settings with non-continuous data for which TuRBO and ROBOT aren't applicable.</p>
<p>Experiments</p>
<p>We now present results from our numerical experiments, comparing active search (AS) and Bayesian optimization (BayesOpt) performance of our methods against a wide range of baselines.In each experimental setting, we average results across 10 repeats with different initial data, chosen uniformly at random from the search space (these sets of 10 different initial data sets are shared across the methods).</p>
<p>Diverse Active Search</p>
<p>We first discuss AS, where our goal is to collect a diverse set of positive points in a binary setting.We study the performance of our method, which we call qVS-AS, under different orders q ∈ {0, 0.1, 0.5, 1, 2, ∞}, both in the search behavior as defined in 9 and in the evaluation metric.Again note that q = 0 gives us the traditional AS setting (Garnett et al., 2012), which counts the raw number of positives discovered and does not account for diversity.We also consider relevant active learning/search algorithms discussed in 4: Expected Coverage Improvement (ECI) (Malkomes et al., 2021) and SELECT (Vanchinathan et al., 2015).These methods come with their own hyperparameters to tune, and we only report the results obtained from the highest-performing hyperparameters.</p>
<p>We consider the molecular discovery problem studied by Mukadum et al. (2021), where our target is photoswitches (molecules that change their properties upon irradiation) in chemical databases that exhibit both desirable light absorbance and long half-lives.Roughly 36% of the molecules in the search space are targets.We also include a materials discovery application where we search for alloys that can form valuable bulk metallic glasses with higher toughness and better wear resistance than crystalline alloys.This data set comprises 106 810 alloys from the materials literature, approximately 4% of which exhibit glass-forming ability (Kawazoe et al., 1997;Ward et al., 2016).Finally, following Nguyen and Garnett (2023), we use the FashionMNIST data set (Xiao et al., 2017) of 70,000 images of articles of clothing to simulate a product recommendation problem.Here, we assume that a user is looking for, unbeknownst to the recommendation engine, t-shirts and tops (members of class 0, one-tenth of the data set) while shopping online, and the goal is to assemble a diverse set of products belonging to this unknown class.</p>
<p>To first ensure the quality of our predictive model, we perform the following benchmarking experiments.In each experiment, we train the model on 100 random points from a data set.We then pick out the test points that yield the highest posterior probabilities Pr ( y = 1 | x, ) and record the proportion of this set are positives.For each data set, we repeat this experiment 10 times and record the average precision-at-k, which ranges consistently from 80% to 100%, indicating Table 1: Average Vendi Scores under different orders q across 10 repeated experiments of the molecular discovery problem with the photoswitch data; the best performance in each column is highlighted in bold (including ties).A star ( * ) superscript indicates that the reported result is chosen from the best hyperparameter in that setting.</p>
<p>method Vendi Score max.pairwise dist.kernel matrix det.q = 0 q = 0.1 q = 0.5 Table 2: Average Vendi Scores under different orders q across 10 repeated experiments of the materials discovery problem with the bulk metal glass data; the best performance in each column is highlighted in bold (including ties).A star ( * ) superscript indicates that the reported result is chosen from the best hyperparameter in that setting.
q = 1 q = 2 q = ∞
method Vendi Score max.pairwise dist.kernel matrix det.q = 0 q = 0.1 q = 0.5 Table 3: Average Vendi Scores under different orders q across 10 repeated experiments of the product recommendation problem with the FashionMNIST data; the best performance in each column is highlighted in bold (including ties).A star ( * ) superscript indicates that the reported result is chosen from the best hyperparameter in that setting.
q = 1 q = 2 q = ∞
method Vendi Score max.pairwise dist.kernel matrix det.q = 0 q = 0.1 q = 0.5  that our model produces high-quality predictions and recovers pure sets of rare positives.
q = 1 q = 2 q = ∞
We use the VS of the collected positives in Eq. 6 as our performance metric and show in the first portion of Tables 1, 2, and 3 the VS (of different orders q) achieved by each method, averaged across the 10 repeats.We see that our method qVS-AS performs well across the problems, achieving the highest VS in most cases; when it is not the best, it is typically a close second behind a method with tuned hyperparameters.Inspecting the performance of different realizations of qVS-AS under varying values of the order q, we observe a reasonable trend: qVS-AS with the order q matching that of the evaluation metric tends to perform the best; further, there is a smooth change in performance as we move across the different values of q, showcasing the ability of this hyperparameter to smoothly control our algorithm's behavior.</p>
<p>To further study the diversity of the data collected by each method, we consider two metrics that quantify the spread of the discovered targets: the maximum distance between any pair of positives discovered and the determinant of the kernel matrix of the collected positives (i.e., the squared volume spanned by the feature vectors of the selected data points with positive labels).The last two columns in Tables 1,  2, and 3 show these results, where our method can again be observed to achieve consistently good performance.</p>
<p>Finally, to visually illustrate our method's ability to assemble diverse data, we show in Fig. 2 the locations of the queries made by diversity-blind AS and our method, end for 8:</p>
<p>X ← X ∪ {arg max x∈ ( ∪X ) α(x)} ▷ add candidate yielding largest qVS 9: end for within the two-dimensional embedding of the bulk metal glass data set computed by performing PCA on the features.We see that the overly exploitative diversity-blind search simply focuses on a small portion of the search space, while our method qVS-AS (with q = 1) is able to thorough explore the different regions of positives.We also show the VS of the collected positives of the two policies (interpreted as the effective discovery count), where our policy clearly outperforms diversity-blind search.</p>
<p>Diverse Bayesian Optimization</p>
<p>We now present results from BayesOpt tasks, as formulated in Section 3. To study the performance of our method, qVS-BayesOpt, we include as baselines (1) TuRBO (Eriksson et al., 2019), the diversity-blind algorithm upon which qVS-BayesOpt is based, (2) ROBOT (Maus et al., 2023), which also tackles diverse BayesOpt, and (3) a random search algorithm that uniformly samples its queries from the search space at random.ROBOT has a hyperparameter τ that controls the quality-diversity trade-off in its search, which we set to the values used in the original investigation by Maus et al. (2023).We test these methods on three optimization tasks: (1) the rover path-finding task involves optimizing the path of a mars rover while avoiding obstacles; (2) the lunar lander task from reinforcement learning where we aim to optimize the control policy for an autonomous vehicle to safely land on a given terrain; (3) the metal-organic framework (MOF) storage capacity optimization task as explored by Liu et al. (2024), where we aim to identify the MOFs that have the highest storage capacity for ammonia.Figure 4: Trajectories identified by various search methods in the rover path finding problem.Our method finds a diverse set of paths, whose diversity can be controlled using the order q of the qVS.</p>
<p>While the first two tasks are formulated as continuous optimization problems (60and 12-dimensional, respectively), the third involves a discrete search space of structured data (a database of 1000 MOFs).Typically, to deal with structured data such as molecules in BayesOpt, one may train a deep learning model such as a variational autoencoder (VAE) (Kingma and Welling, 2013) to obtain a continuous embedding of the candidates one searches over (see Gómez-Bombarelli et al. (2018) for an example in drug discovery).From there, one can apply BayesOpt algorithms such as TuRBO to that continuous embedding.However, unlike drug-like molecules which have enjoyed enduring interest from the machine learning community, MOFs are relatively unexplored materials to which, to our knowledge, there does not exist any consistently suitable VAE that can be applied.We instead reuse the MOF-specific kernel function proposed by Liu et al. (2024), which operates on any given pair among the 1000 candidate MOFs.Without a continuous embedding, the trust region-based algorithm TuRBO, and thus its extension to diverse BayesOpt, ROBOT, cannot be applied to this MOF search task.Instead, we employ a simple Upper Confidence Bound (UCB) algorithm (Auer, 2002) as our baseline of traditional BayesOpt.To realize our algorithm with the qVS, we directly use the UCB score as our metric of quality in Eq. 11 which we use as our criterion for finding the next Results from these experiments are reported in Fig. 3, where we show the highest objective value achieved across the 10 repeats.We see that our method qVS-BayesOpt (q = 1) performs well across the experiments and remains competitive against the state-of-the-art ROBOT in the two continuous optimization problems.Surprisingly, in the lunar lander and MOF search tasks, encouraging more diversity in our search not only does not result in any slowdown in optimization progress compared to traditional BayesOpt, but actually leads to improved performance.We hypothesize this is because the search spaces in these two problems consist of many local optima in which exploitative BayesOpt algorithms could become trapped.To highlight our method's ability to identify diverse solutions, we first show in Fig. 4 the rover paths optimized by TuRBO, which targets pure optimization, in a representative run.(Here, the number of solutions to be returned to the user M = 3.) We see that these paths are effectively identical to and overlap one another.On the other hand, the other panels show the set of 3 solutions optimized by qVS-BayesOpt from the run using the same initial data as TuRBO above, which exhibit best storage value q = 0 20.47 (1.16) q = 0.1 22.07 (0.81) q = 0.5 20.07 (0.61) q = 1 21.24 (0.59) q = 2 20.23 (0.56) q = ∞ 20.23 (0.56)</p>
<p>Table 4: Average storage capacity values (higher is better) and standard errors of the best MOFs found by our algorithm under different orders q.Here, q = 0 corresponds to regular, diversity-blind Bayesian optimization.</p>
<p>Figure 6: Energy penalty to release stored toxins (lower is better) of the best MOFs found by different algorithms.Our qVS-BayesOpt (q = 1) outperforms regular BayesOpt and random search.</p>
<p>varying degrees of diversity corresponding to q ∈ {0.5, 1, 1.5}.</p>
<p>We further seek to visualize the search behaviors of our method by first using the multidimensional scaling technique (Cox and Cox, 2001) to compute a 2-dimensional embedding from the kernel matrix of the candidates within the database.This embedding is shown in the first panel of Fig. 5, where the scatter points' colors and opacity levels are set based on the corresponding MOFs' storage capacity levels (the objective value to be maximized).We then mark the MOFs that are selected by each method in the remaining panels and observe a number of distinct trends: compared to random search, regular BayesOpt focuses on the lower-left region where the storage capacity is high; qVS-BayesOpt, on the other hand, further inspects the lower-right portion, which also contains high-capacity MOFs but in fewer numbers.It is exactly this diverse sampling strategy that we hoped to achieve with the qVS.We further compute a metric similar to the effective discovery count (EDC) in Section 5.1 whereby a MOF is classified as "good" if it yields a storage capacity (the objective value to be maximized) of at least 15, and the VS of these good MOFs collected by each policy is reported.We see that our qVS method outperforms both baselines; interestingly, regular BayesOpt yields a lower EDC than even random search-another indication of the failure mode of its overly exploitative strategy.To study the effect of the order q on the algorithm's performance, we include in Table 4 the best objective value found under q ∈ {0, 0.1, 0.5, 1, 2, ∞}.We see that q = 0.1 yields the best performance, while some values of q lead to even worse performance than regular BayesOpt (when q = 0).This behavior indicates the importance of setting q appropriately; an interesting future direction could be to dynamically set q based on search progress.</p>
<p>To conclude our analysis, we include another relevant metric in the MOF search application, which is the percentage energy penalty incurred when the stored toxins are eventually released for disposal, which ranges from 0 to 1 and a lower value indicates higher energy efficiency.Here, Fig. 6 shows the average penalty across the optimization runs discussed above by the three algorithms, where we once again observe that (1) regular BayesOpt could fail to compete against even random search and (2) qVS-BayesOpt outperforms the two baselines.</p>
<p>Conclusion</p>
<p>We extended the Vendi scores to account for quality.We used these new qualityweighted Vendi scores, or qVS, to propose a unified framework for experimental design tasks to make diverse discoveries in discrete and continuous spaces.To optimize qVS, we proposed the sequential greedy strategy, widely used to optimize functions with diminishing returns.Our extensive experiments on scientific discovery problems show that the algorithms resulting from our framework can collect diverse, high-quality data, effectively balancing exploitation and exploration.</p>
<p>Figure 1 :
1
Figure 1: Batches of 10 data points maximizing various VS and qVS functions, obtained with multi-start gradient-based optimization.The scoring function is a Gaussian function centered at the middle point, as illustrated by the heat maps.The quality-weighted Vendi Score balances between the quality of the selected data points and their diversity; this balance is smoothly controlled by the order q.</p>
<p>Figure 2 :
2
Figure2: Data points collected by diversity-blind search and our diversity-aware policy in the materials discovery problem with bulk metal glasses.Our method appropriately balances between exploring the search space and focusing on regions containing positive data, and discovers more effective positives as a result.</p>
<p>Figure 3 :
3
Figure 3: Average optimization performance and standard errors across 10 repeated experiments.Our method (shown in red) performs competitively across the different settings.</p>
<p>(a) TuRBO (paths overlapping) (b) qVS-BayesOpt with q = 0.5 (c) qVS-BayesOpt with q = 1 (d) qVS-BayesOpt with q = 1.5</p>
<p>Figure 5 :
5
Figure 5: Data points collected by each search strategy in the MOF search problem across the repeats, illustrated as red x's.(All data points in the search space are visualized in the first panel.)Our method focuses on specific, high-performing regions compared to the random search, while exploring the space more evenly compared to regular Bayesian optimization.</p>
<p>Algorithm 2 qVS-BayesOpt with TuRBO for diverse Bayesian optimization 1: inputs observations , number of trust regions M , query batch size n 2: returns query batch X of size n maximizing 11 3: for m ← 1, . . ., M do ▷ generate candidates in each trust region</p>
<p>4:</p>
<p>71 (2.50) 25.58 (0.17) 1.00 (0.00)
random search7.20 (0.86)7.18 (0.85)7.09 (0.82)6.99 (0.79)6.82 (0.74)5.42 (0.41)13.16 (0.62)0.88 (0.08)ECI  <em>25.00 (0.00)24.89 (0.00)24.46 (0.00)23.92 (0.00)22.85 (0.00)12.85 (0.00)15.35 (0.52)0.99 (0.00)SELECT  </em>143.90 (2.85) 130.63 (4.47)99.59 (5.66)69.57 (6.12)36.19 (5.13)9.67 (1.46)12.42 (0.60)0.00 (0.00)q = 0186.00 (3.04) 146.35 (4.20)69.49 (5.14)26.42 (3.86)8.66 (1.45)3.31 (0.36)6.45 (0.54)0.00 (0.00)q = 0.1 174.30 (3.59) 162.82 (4.05) 120.07 (5.23)75.92 (5.11)31.67 (3.07)8.13 (0.75)10.49 (0.36)0.00 (0.00)qVS-ASq = 0.5 160.40 (3.07) 156.68 (3.08) 141.15 (3.13) 120.74 (3.20) q = 1 153.70 (1.24) 151.93 (1.23) 144.48 (1.19) 134.45 (1.17) 113.21 (1.24) 83.15 (3.14)20.94 (1.19) 35.32 (1.28)15.08 (0.39) 18.58 (0.70)0.00 (0.00) 0.31 (0.05)q = 2137.60 (1.72) 136.82 (1.70) 133.59 (1.65) 129.32 (1.57) 120.19 (1.38)50.55 (0.65)22.34 (0.35)0.96 (0.00)q = ∞65.40 (4.40)65.34 (4.39)65.11 (4.33)64.82 (4.26)64.25 (4.13)52.
Code can be found at https://github.com/vertaix/Quality-Weighted-Vendi-Score.
AcknowledgementsAdji Bousso Dieng is supported by the National Science Foundation, Office of Advanced Cyberinfrastructure (OAC): #2118201 and by Schmidt Sciences via the AI2050 Early Career Fellowship.DedicationThis paper is dedicated to Kwame Nkrumah.
Using Confidence Bounds for Exploitation-Exploration Trade-offs. P Auer, Journal of Machine Learning Research. 32002. Nov</p>
<p>Towards Mode Balancing of Generative Models via Diversity Weights. S Berns, S Colton, C Guckelsberger, arXiv:2304.11961[cs.LG]2023arXiv preprint</p>
<p>Multidimensional Scaling. T F Cox, M A Cox, 2001Chapman and Hall</p>
<p>Scalable Global Optimization via Local Bayesian Optimization. D Eriksson, M Pearce, J Gardner, R D Turner, M Poloczek, Advances in Neural Information Processing Systems. 201932</p>
<p>The Vendi Score: A Diversity Evaluation Metric for Machine Learning. D Friedman, A B Dieng, Transactions on Machine Learning Research. 2023</p>
<p>Bayesian Optimization. R Garnett, 2022Cambridge University Press</p>
<p>Bayesian Optimal Active Search and Surveying. R Garnett, Y Krishnamurthy, X Xiong, J Schneider, R Mann, Proceedings of the 29th International Conference on Machine Learning. the 29th International Conference on Machine Learning2012</p>
<p>Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules. R Gómez-Bombarelli, J N Wei, D Duvenaud, J M Hernández-Lobato, B Sánchez-Lengeling, D Sheberla, J Aguilera-Iparraguirre, T D Hirzel, R P Adams, A Aspuru-Guzik, ACS Central Science. 422018</p>
<p>Diversity and Evenness: A Unifying Notation and Its Consequences. M O Hill, Ecology. 5421973</p>
<p>Efficient Nonmyopic Active Search. S Jiang, G Malkomes, G Converse, A Shofner, B Moseley, R Garnett, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning2017</p>
<p>Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys, volume 37A of Condensed Matters. Y Kawazoe, J.-Z Yu, A.-P Tsai, T Masumoto, 1997Springer-Verlag</p>
<p>D P Kingma, M Welling, arXiv:1312.6114Auto-Encoding Variational Bayes. 2013arXiv preprintstat.ML</p>
<p>Near-optimal Observation Selection using Submodular Functions. A Krause, C Guestrin, Proceedings of the 21st AAAI Conference on Artificial Intelligence. the 21st AAAI Conference on Artificial Intelligence2007</p>
<p>A Kulesza, B Taskar, Determinantal Point Processes for Machine Learning. Foundations and Trends® in Machine Learning. 20125</p>
<p>Diversitydriven, efficient exploration of a mof design space to optimize mof properties: application to nh3 adsorption. T.-W Liu, Q Nguyen, A B Dieng, D Gomez-Gualdron, 2024ChemRxiv preprint</p>
<p>Beyond the Pareto Efficient Frontier: Constraint Active Search for Multiobjective Experimental Design. G Malkomes, B Cheng, E H Lee, M Mccourt, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine Learning2021</p>
<p>Discovering Many Diverse Solutions with Bayesian Optimization. N Maus, K Wu, D Eriksson, J Gardner, Proceedings of the 26th International Conference on Artificial Intelligence and Statistics. the 26th International Conference on Artificial Intelligence and Statistics2023</p>
<p>Efficient Discovery of Visible Light-Activated Azoarene Photoswitches with Long Half-Lives Using Active Search. F Mukadum, Q Nguyen, D M Adrion, G Appleby, R Chen, H Dang, R Chang, R Garnett, S A Lopez, Journal of Chemical Information and Modeling. 61112021</p>
<p>Diversified Sampling for Batched Bayesian Optimization with Determinantal Point Processes. E Nava, M Mutny, A Krause, Proceedings of the 25th International Conference on Artificial Intelligence and Statistics. the 25th International Conference on Artificial Intelligence and Statistics2022</p>
<p>An Analysis of Approximations for Maximizing Submodular Set Functions-I. G L Nemhauser, L A Wolsey, M L Fisher, Mathematical Programming. 1411978</p>
<p>Nonmyopic Multiclass Active Search with Diminishing Returns for Diverse Discovery. Q Nguyen, R Garnett, Proceedings of the 26th International Conference on Artificial Intelligence and Statistics. the 26th International Conference on Artificial Intelligence and Statistics2023</p>
<p>Nonmyopic Multifidelity Acitve Search. Q Nguyen, A Modiri, R Garnett, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine Learning2021</p>
<p>A Pasarkar, A B Dieng, arXiv:2310.12952[cs.LG]Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning. 2023arXiv preprint</p>
<p>Vendi Sampling For Molecular Simulations: Diversity As A Force For Faster Convergence And Better Exploration. A P Pasarkar, G M Bencomo, S Olsson, A B Dieng, The Journal of Chemical Physics. 141592023</p>
<p>D J Russo, B Van Roy, A Kazerouni, I Osband, Z Wen, A Tutorial on Thompson Sampling. Foundations and Trends® in Machine Learning. 201811</p>
<p>Discovering Valuable Items from Massive Data. H P Vanchinathan, A Marfurt, C.-A Robelin, D Kossmann, A Krause, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 21th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2015</p>
<p>Active model learning and diverse action sampling for task and motion planning. Z Wang, C R Garrett, L P Kaelbling, T Lozano-Pérez, IEEE/RSJ International Conference on Intelligent Robots and Systems. 2018</p>
<p>A general-purpose machine learning framework for predicting properties of inorganic materials. L Ward, A Agrawal, A Choudhary, C Wolverton, npj Computational Materials. 212016</p>
<p>Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning. S Wu, K Lu, B Xu, J Lin, Q Su, C Zhou, arXiv:2311.08182[cs.CL]2023arXiv preprint</p>
<p>Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. H Xiao, K Rasul, R Vollgraf, arXiv:1708.07747[cs.LG]2017arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9039 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9039</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9039</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-273125542</p>
                <p><strong>Paper Title:</strong> Embers of autoregression show how large language models are shaped by the problem they are trained to solve</p>
                <p><strong>Paper Abstract:</strong> Significance ChatGPT and other large language models (LLMs) have attained unprecedented performance in AI. These systems are likely to influence a diverse range of fields, such as education, intellectual property law, and cognitive science, but they remain poorly understood. Here, we draw upon ideas in cognitive science to show that one productive way to understand these systems is by analyzing the goal that they were trained to accomplish. This perspective reveals some surprising limitations of LLMs, including difficulty on seemingly simple tasks such as counting words or reversing a list. Our empirical results have practical implications for when language models can safely be used, and the approach that we introduce provides a broadly useful perspective for reasoning about AI.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9039.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9039.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_rot13_output_prob</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 decoding rot-13: sensitivity to output probability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation showing that GPT-4's accuracy decoding a deterministic rot-13 shift cipher varies strongly with the corpus-probability of the correct (decoded) sentence: higher-probability targets are decoded far more accurately than lower-probability ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4, a large instruction-tuned transformer language model trained primarily by next-word prediction over large Internet text corpora; evaluated via API (version gpt-4-0613) with temperature 0.0.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Shift cipher decoding (rot-13) — output probability manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Deterministic character-level shift-cipher decoding (rot-13). Example sentences were categorized as high-, medium-, or low-probability word sequences (probabilities estimated with GPT-2) to test whether target output probability affects decoding accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Decoding accuracy substantially higher for high-probability outputs; example summary in abstract: 51% accuracy when the output is high-probability vs 13% when the output is low-probability (reported for 'simple cipher' / rot-like tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Not compared to a human baseline in this paper; model accuracy shows large dependence on target output probability (worse on low-probability outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Models accessed via API at temperature 0.0; stimuli for sentence-based tasks drawn/constructed from Global Voices and manipulated by RoBERTa; probabilities estimated using GPT-2; basic prompting (single-query) used; sample sizes for shift-cipher conditions typically 100 items.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No human performance collected on the same stimuli, so direct human-model comparisons are not reported here; authors note that more advanced prompting (chain-of-thought) or augmentations (code-execution) can increase absolute performance but the relative sensitivity to output probability remains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embers of autoregression show how large language models are shaped by the problem they are trained to solve', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9039.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9039.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_reversal_output_prob</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 reversing sequences: sensitivity to output probability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4's accuracy reversing a sequence of words is much higher when the correct reversed sentence is a high-probability string than when it is low-probability, despite the task being deterministic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See above (GPT-4 description).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Reversal of word sequences — output probability manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Deterministic task: reverse a sequence of words; test items selected so that the correct reversed output has high vs low corpus probability.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>97% accuracy when the answer is a high-probability sentence vs 53% accuracy when the output is low-probability (reported in Table 1 example).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline reported for these exact stimuli; authors emphasize the counterintuitive dependence on output probability for a deterministic task.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Basic prompting; targets categorized by corpus probability estimated with GPT-2; logistic regressions used to test significance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Absolute performance can be improved by other prompting techniques, but the paper's claim concerns the relative difference across high- vs low-probability outputs; no same-task human data reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embers of autoregression show how large language models are shaped by the problem they are trained to solve', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9039.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9039.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_rot13_input_prob</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 encoding rot-13: sensitivity to input probability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When asked to encode sentences with a rot-13 cipher, GPT-4's accuracy depends on the corpus-probability of the input sentence (less robust when inputs are low-probability).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See above (GPT-4 description).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Shift cipher encoding (rot-13) — input probability manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Deterministic encoding of input sentences into rot-13; inputs were categorized as high- or low-probability word sequences to test input-probability sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported example: 21% accuracy when the input is a high-probability sentence vs 11% accuracy when the input is low-probability (Table 1 example).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline reported; demonstrates input-probability can affect performance though authors predict input-probability effects are generally weaker/more conditional than output-probability effects.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Basic prompting; stimuli construction: high-probability sentences from Global Voices, medium via RoBERTa substitutions, low by shuffling; sample sizes per condition ≈100.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Input-probability effects were less consistent across models (GPT-3.5 showed no significant input-prob effect in some tasks); authors note LLMs can generalize to novel inputs so input-prob effects are less pervasive than output-prob effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embers of autoregression show how large language models are shaped by the problem they are trained to solve', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9039.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9039.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_PigLatin_task_freq</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Pig Latin encoding/decoding: sensitivity to task frequency (common vs rare variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 (and other models) perform better on common, attested variants of Pig Latin and related tasks than on artificially created rare variants, showing sensitivity to task frequency in pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See above (GPT-4 description).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Pig Latin encoding/decoding (common vs. rare variants)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Encoding English into Pig Latin (common) vs a made-up 'Boar Etruscan' variant (rare/unattested) and decoding both; assesses sensitivity to how frequently a task/variant appears in corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Table 1 example: GPT-4 gets 42% accuracy on the most common Pig Latin variant vs 23% accuracy on a rare variant (example reported in Table 1). Across more detailed tests, GPT-3.5, GPT-4, and Claude 3 performed significantly better on Pig Latin encoding than on the rare Boar Etruscan (P < 0.01).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline reported; authors conclude models do better on common task variants encountered in training than on rare/unseen variants.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Variant frequencies estimated via corpus analyses (Pile, C4). Encoding/decoding prompts given with an example to avoid ambiguity; sample sizes: 100 per condition for most tasks, 1000 for acronym encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Some models had floor effects (very low accuracy) on rare variants, making comparisons noisy; decoding may be easier than encoding for unfamiliar endings, so variant frequency sometimes affects encoding more strongly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embers of autoregression show how large language models are shaped by the problem they are trained to solve', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9039.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9039.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs_shift_level_spike</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs' shift-cipher performance across shift levels (spike at rot-13)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Across evaluated LLMs (GPT-4, GPT-3.5, Claude 3, Llama 3, Gemini 1.0), decoding accuracy as a function of shift amount shows a pronounced spike at rot-13, the most common shift in corpora; GPT-4 achieves >0.50 on the three most frequent shifts and <0.03 on most others.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple: GPT-4 (gpt-4-0613), GPT-3.5 (gpt-3.5-turbo-0613), Claude 3 (claude-3-opus-20240229), Llama 3 (llama-3-70b-chat-hf), Gemini 1.0 (gemini-1.0-pro-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Five LLMs accessed via API (versions listed in Methods) evaluated with temperature 0.0; Llama 3 noted as 70B parameter model in the paper; other exact parameter counts not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Llama 3: 70B; others: not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Shift cipher decoding across shift levels (1–25)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Decode sentences shifted by n letters for n in 1..25; measures performance as a function of task-variant frequency (how often particular shift amounts appear in the training corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Most models (except Gemini 1.0) show a performance spike at shift=13; GPT-4 scores ≥0.50 on the three highest-frequency shifts (rot-1, rot-3, rot-13) and <0.03 accuracy for most other shifts; Gemini 1.0 performed at ≈0% across shift conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline; authors use these results to argue LLMs rely on task frequency observed during training and thus do not generalize uniformly across task variants.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Corpus frequency of each shift measured in C4 corpus; decoding prompts simple; significance tested comparing rot-13 vs rot-12 (P < 0.01 for GPT-3.5, GPT-4, Claude 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Some models (Claude 3) showed better generalization to rare shifts than others; absence of human comparison; some floor/ceiling effects for smaller models limited analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embers of autoregression show how large language models are shaped by the problem they are trained to solve', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9039.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9039.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Counting_number_frequency_effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs counting lists: accuracy patterns track number frequency in corpora</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When counting words in a list, LLM accuracy is not monotonic with list length but shows spikes at numbers that are more frequent in natural text (multiples of ten); correlations show accuracy relates more strongly to corpus frequency than numerical magnitude for several models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (reported examples: GPT-4, Claude 3; others evaluated too)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See above descriptions for models; all evaluated at temperature 0.0 via API with basic prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Llama 3: 70B; other sizes not specified</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Counting words in a list (1–100) — frequency vs magnitude analysis</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Count the number of words in lists of varying length; analyze whether accuracy correlates more with corpus frequency of the numeral string vs the numeric magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Models often show accuracy spikes for frequently occurring numbers (multiples of 10). Spearman correlations reported: GPT-4 accuracy correlation with number frequency = 0.84 vs with magnitude = 0.70; Claude 3 correlation with frequency = 0.85 vs magnitude = 0.66.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline reported; authors interpret the results as evidence that corpus frequency of numerals influences LLM performance on counting tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Counting inputs varied from length 1 to 100; 30 samples per number; logistic regressions and Spearman correlations used for analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No human comparison; authors note patterns driven by frequency in training data and that prompting/augmentation can change absolute performance but frequency effects persist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embers of autoregression show how large language models are shaped by the problem they are trained to solve', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9039.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9039.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_article_swapping_output_prob</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 article-swapping task: output probability strongly affects accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>In an article-swapping task (swap each article with preceding word), GPT-4's accuracy ranged from near-zero on low-probability outputs to very high on high-probability outputs, illustrating sensitivity to target-output probability even for deterministic transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See above (GPT-4 description).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Article swapping — output probability manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Deterministic transformation swapping each article (a/an/the) with the preceding word; outputs categorized by corpus-probability of the full output sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Example reported: GPT-4 article-swapping accuracy ranged from 0.02 on low-probability examples to 0.83 on high-probability examples (reported in main text / Fig. 4 discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct human baseline reported; authors highlight dramatic performance differences conditioned on output probability.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompt included a single example to disambiguate task; stimuli creation methods as in Materials and Methods; significance tested via logistic regression.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Effect shows deterministic tasks can still be influenced by LLM priors; advanced prompting can raise accuracy but relative effects remain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embers of autoregression show how large language models are shaped by the problem they are trained to solve', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9039.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9039.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Acronyms_first_vs_second_letter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Acronym formation: first-letter (common) vs second-letter (rare) across models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>All five evaluated LLMs perform much better at forming acronyms from the first letters of words (common in corpora) than forming acronyms from second letters (rare), supporting task-frequency sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple: GPT-4, GPT-3.5, Claude 3, Llama 3, Gemini 1.0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Five LLMs accessed via API; basic prompting; Llama 3 specified as 70B parameters, others not parameterized in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Llama 3: 70B; others: not specified</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Acronym formation (first-letter common vs second-letter rare)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Given lists of words, produce an acronym formed by either the first letter of each word (common task) or the second letter (rare task).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative: performance much higher on first-letter acronyms than on second-letter acronyms for all five models; effect statistically significant (P < 10^-4). Exact accuracy values per model not provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline reported; authors use results to support sensitivity to task frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Acronym inputs lists of 7 words; sample size 1000 per condition; basic prompting without heavy prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Exact per-model accuracies not enumerated in main text (reported in figures/SI); some models have very low absolute accuracy on rare variants (floor effects).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embers of autoregression show how large language models are shaped by the problem they are trained to solve', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9039.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9039.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sorting_alpha_vs_reverse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sorting task: alphabetical (common) vs reverse-alphabetical (rare) across models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>All five LLMs perform better on sorting lists alphabetically (a frequent pattern in corpora) than on reverse-alphabetical sorting, consistent with sensitivity to task frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple: GPT-4, GPT-3.5, Claude 3, Llama 3, Gemini 1.0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See descriptions above; evaluated at temperature 0.0 via API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Llama 3: 70B; others not specified</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sorting words: alphabetical order (common) vs reverse alphabetical (rare)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Given lists of words, sort into either alphabetical order (common in corpora) or reverse alphabetical order (rare).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>All five models perform better on alphabetical than reverse-alphabetical sorting; reported significance P < 10^-5 for all models. Exact accuracy numbers not provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline reported; results interpreted as evidence models better at variants more frequently illustrated in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Sorting inputs length varied 10–20; sample sizes 100 per condition; C4 corpus used to estimate relative frequency (alphabetical estimated 150x more common than reverse).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors caution that advanced prompting or model augmentations can change absolute performance; their claims concern relative differences across task variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embers of autoregression show how large language models are shaped by the problem they are trained to solve', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9039.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9039.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>General_evaluation_summary</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Summary: five LLMs evaluated on cognitive-style tasks emphasizing probability effects</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper evaluates GPT-4, GPT-3.5, Claude 3, Llama 3 (70B), and Gemini 1.0 on 11 tasks (e.g., shift ciphers, Pig Latin, acronyms, counting, sorting, linear functions) to test hypotheses that LLM performance is sensitive to output probability, input probability, and task frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5, Claude 3, Llama 3 (70B), Gemini 1.0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models are large transformer-based LLMs trained primarily via next-word prediction on Internet text, often followed by instruction tuning; evaluated via respective APIs at temperature 0.0.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Only Llama 3 explicitly listed as 70B in paper; other models' parameter counts not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Battery of 11 tasks (see Table 2): article swapping, reversal, counting, acronyms, linear function application, multiplication, sorting, keyboard cipher, shift cipher, Pig Latin, birthdays</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A set of deterministic and quasi-deterministic tasks drawn from manipulations often used in cognitive evaluations (character and word manipulations, arithmetic/transformations, factual recall) chosen to create low-probability conditions and reveal LLM dependencies on corpus-level statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Varied by model and task; main findings: strong sensitivity to output probability (many tasks show large accuracy differences between high- and low-probability targets), task-frequency sensitivity (models do better on common variants like rot-13, Pig Latin, alphabetical sorting, Celsius→Fahrenheit linear function), and weaker/more conditional input-probability effects (notable for birthday retrieval task). Specific examples: GPT-4 reversal 97% vs 53% (high vs low output prob); GPT-4 rot-13 input prob 21% vs 11%; GPT-4 cipher example 51% vs 13% (high vs low output prob).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct human baselines collected on the same stimuli in this paper; authors discuss that without matched human data they cannot make strong claims about qualitative human-model differences, though they hypothesize humans would be less sensitive to these corpus-probability effects.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Basic prompting used (single-shot or zero-shot with an example where needed); temperature 0.0; some chain-of-thought / step-by-step prompting and few-shot experiments explored in SI and found to raise absolute performance but preserve relative trends; sample sizes: often 100 per condition, counting had 30 per number, acronym 1000 per condition.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No matched human experiments on same stimuli; some models had floor/ceiling effects; exact model sizes for many models not reported; results specific to basic prompting and the particular API/model versions tested; authors note more advanced techniques or tool-augmented models (code execution) can change outcomes but do not invalidate qualitative effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embers of autoregression show how large language models are shaped by the problem they are trained to solve', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Using cognitive psychology to understand GPT-3 <em>(Rating: 2)</em></li>
                <li>Emergent analogical reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with GPT-4 <em>(Rating: 2)</em></li>
                <li>Reasoning or reciting? Exploring the capabilities and limitations of language models through counterfactual tasks <em>(Rating: 2)</em></li>
                <li>Large language models struggle to learn long-tail knowledge <em>(Rating: 1)</em></li>
                <li>Comparing humans, GPT-4, and GPT-4v on abstraction and reasoning tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9039",
    "paper_id": "paper-273125542",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "GPT-4_rot13_output_prob",
            "name_full": "GPT-4 decoding rot-13: sensitivity to output probability",
            "brief_description": "Evaluation showing that GPT-4's accuracy decoding a deterministic rot-13 shift cipher varies strongly with the corpus-probability of the correct (decoded) sentence: higher-probability targets are decoded far more accurately than lower-probability ones.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0613)",
            "model_description": "OpenAI's GPT-4, a large instruction-tuned transformer language model trained primarily by next-word prediction over large Internet text corpora; evaluated via API (version gpt-4-0613) with temperature 0.0.",
            "model_size": null,
            "test_battery_name": "Shift cipher decoding (rot-13) — output probability manipulation",
            "test_description": "Deterministic character-level shift-cipher decoding (rot-13). Example sentences were categorized as high-, medium-, or low-probability word sequences (probabilities estimated with GPT-2) to test whether target output probability affects decoding accuracy.",
            "llm_performance": "Decoding accuracy substantially higher for high-probability outputs; example summary in abstract: 51% accuracy when the output is high-probability vs 13% when the output is low-probability (reported for 'simple cipher' / rot-like tasks).",
            "human_baseline_performance": null,
            "performance_comparison": "Not compared to a human baseline in this paper; model accuracy shows large dependence on target output probability (worse on low-probability outputs).",
            "experimental_details": "Models accessed via API at temperature 0.0; stimuli for sentence-based tasks drawn/constructed from Global Voices and manipulated by RoBERTa; probabilities estimated using GPT-2; basic prompting (single-query) used; sample sizes for shift-cipher conditions typically 100 items.",
            "limitations_or_caveats": "No human performance collected on the same stimuli, so direct human-model comparisons are not reported here; authors note that more advanced prompting (chain-of-thought) or augmentations (code-execution) can increase absolute performance but the relative sensitivity to output probability remains.",
            "uuid": "e9039.0",
            "source_info": {
                "paper_title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4_reversal_output_prob",
            "name_full": "GPT-4 reversing sequences: sensitivity to output probability",
            "brief_description": "GPT-4's accuracy reversing a sequence of words is much higher when the correct reversed sentence is a high-probability string than when it is low-probability, despite the task being deterministic.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0613)",
            "model_description": "See above (GPT-4 description).",
            "model_size": null,
            "test_battery_name": "Reversal of word sequences — output probability manipulation",
            "test_description": "Deterministic task: reverse a sequence of words; test items selected so that the correct reversed output has high vs low corpus probability.",
            "llm_performance": "97% accuracy when the answer is a high-probability sentence vs 53% accuracy when the output is low-probability (reported in Table 1 example).",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline reported for these exact stimuli; authors emphasize the counterintuitive dependence on output probability for a deterministic task.",
            "experimental_details": "Basic prompting; targets categorized by corpus probability estimated with GPT-2; logistic regressions used to test significance.",
            "limitations_or_caveats": "Absolute performance can be improved by other prompting techniques, but the paper's claim concerns the relative difference across high- vs low-probability outputs; no same-task human data reported.",
            "uuid": "e9039.1",
            "source_info": {
                "paper_title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4_rot13_input_prob",
            "name_full": "GPT-4 encoding rot-13: sensitivity to input probability",
            "brief_description": "When asked to encode sentences with a rot-13 cipher, GPT-4's accuracy depends on the corpus-probability of the input sentence (less robust when inputs are low-probability).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0613)",
            "model_description": "See above (GPT-4 description).",
            "model_size": null,
            "test_battery_name": "Shift cipher encoding (rot-13) — input probability manipulation",
            "test_description": "Deterministic encoding of input sentences into rot-13; inputs were categorized as high- or low-probability word sequences to test input-probability sensitivity.",
            "llm_performance": "Reported example: 21% accuracy when the input is a high-probability sentence vs 11% accuracy when the input is low-probability (Table 1 example).",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline reported; demonstrates input-probability can affect performance though authors predict input-probability effects are generally weaker/more conditional than output-probability effects.",
            "experimental_details": "Basic prompting; stimuli construction: high-probability sentences from Global Voices, medium via RoBERTa substitutions, low by shuffling; sample sizes per condition ≈100.",
            "limitations_or_caveats": "Input-probability effects were less consistent across models (GPT-3.5 showed no significant input-prob effect in some tasks); authors note LLMs can generalize to novel inputs so input-prob effects are less pervasive than output-prob effects.",
            "uuid": "e9039.2",
            "source_info": {
                "paper_title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4_PigLatin_task_freq",
            "name_full": "GPT-4 Pig Latin encoding/decoding: sensitivity to task frequency (common vs rare variants)",
            "brief_description": "GPT-4 (and other models) perform better on common, attested variants of Pig Latin and related tasks than on artificially created rare variants, showing sensitivity to task frequency in pretraining data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0613)",
            "model_description": "See above (GPT-4 description).",
            "model_size": null,
            "test_battery_name": "Pig Latin encoding/decoding (common vs. rare variants)",
            "test_description": "Encoding English into Pig Latin (common) vs a made-up 'Boar Etruscan' variant (rare/unattested) and decoding both; assesses sensitivity to how frequently a task/variant appears in corpora.",
            "llm_performance": "Table 1 example: GPT-4 gets 42% accuracy on the most common Pig Latin variant vs 23% accuracy on a rare variant (example reported in Table 1). Across more detailed tests, GPT-3.5, GPT-4, and Claude 3 performed significantly better on Pig Latin encoding than on the rare Boar Etruscan (P &lt; 0.01).",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline reported; authors conclude models do better on common task variants encountered in training than on rare/unseen variants.",
            "experimental_details": "Variant frequencies estimated via corpus analyses (Pile, C4). Encoding/decoding prompts given with an example to avoid ambiguity; sample sizes: 100 per condition for most tasks, 1000 for acronym encoding.",
            "limitations_or_caveats": "Some models had floor effects (very low accuracy) on rare variants, making comparisons noisy; decoding may be easier than encoding for unfamiliar endings, so variant frequency sometimes affects encoding more strongly.",
            "uuid": "e9039.3",
            "source_info": {
                "paper_title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLMs_shift_level_spike",
            "name_full": "LLMs' shift-cipher performance across shift levels (spike at rot-13)",
            "brief_description": "Across evaluated LLMs (GPT-4, GPT-3.5, Claude 3, Llama 3, Gemini 1.0), decoding accuracy as a function of shift amount shows a pronounced spike at rot-13, the most common shift in corpora; GPT-4 achieves &gt;0.50 on the three most frequent shifts and &lt;0.03 on most others.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple: GPT-4 (gpt-4-0613), GPT-3.5 (gpt-3.5-turbo-0613), Claude 3 (claude-3-opus-20240229), Llama 3 (llama-3-70b-chat-hf), Gemini 1.0 (gemini-1.0-pro-001)",
            "model_description": "Five LLMs accessed via API (versions listed in Methods) evaluated with temperature 0.0; Llama 3 noted as 70B parameter model in the paper; other exact parameter counts not specified in this paper.",
            "model_size": "Llama 3: 70B; others: not specified in paper",
            "test_battery_name": "Shift cipher decoding across shift levels (1–25)",
            "test_description": "Decode sentences shifted by n letters for n in 1..25; measures performance as a function of task-variant frequency (how often particular shift amounts appear in the training corpora).",
            "llm_performance": "Most models (except Gemini 1.0) show a performance spike at shift=13; GPT-4 scores ≥0.50 on the three highest-frequency shifts (rot-1, rot-3, rot-13) and &lt;0.03 accuracy for most other shifts; Gemini 1.0 performed at ≈0% across shift conditions.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline; authors use these results to argue LLMs rely on task frequency observed during training and thus do not generalize uniformly across task variants.",
            "experimental_details": "Corpus frequency of each shift measured in C4 corpus; decoding prompts simple; significance tested comparing rot-13 vs rot-12 (P &lt; 0.01 for GPT-3.5, GPT-4, Claude 3).",
            "limitations_or_caveats": "Some models (Claude 3) showed better generalization to rare shifts than others; absence of human comparison; some floor/ceiling effects for smaller models limited analysis.",
            "uuid": "e9039.4",
            "source_info": {
                "paper_title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Counting_number_frequency_effects",
            "name_full": "LLMs counting lists: accuracy patterns track number frequency in corpora",
            "brief_description": "When counting words in a list, LLM accuracy is not monotonic with list length but shows spikes at numbers that are more frequent in natural text (multiples of ten); correlations show accuracy relates more strongly to corpus frequency than numerical magnitude for several models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (reported examples: GPT-4, Claude 3; others evaluated too)",
            "model_description": "See above descriptions for models; all evaluated at temperature 0.0 via API with basic prompting.",
            "model_size": "Llama 3: 70B; other sizes not specified",
            "test_battery_name": "Counting words in a list (1–100) — frequency vs magnitude analysis",
            "test_description": "Count the number of words in lists of varying length; analyze whether accuracy correlates more with corpus frequency of the numeral string vs the numeric magnitude.",
            "llm_performance": "Models often show accuracy spikes for frequently occurring numbers (multiples of 10). Spearman correlations reported: GPT-4 accuracy correlation with number frequency = 0.84 vs with magnitude = 0.70; Claude 3 correlation with frequency = 0.85 vs magnitude = 0.66.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline reported; authors interpret the results as evidence that corpus frequency of numerals influences LLM performance on counting tasks.",
            "experimental_details": "Counting inputs varied from length 1 to 100; 30 samples per number; logistic regressions and Spearman correlations used for analysis.",
            "limitations_or_caveats": "No human comparison; authors note patterns driven by frequency in training data and that prompting/augmentation can change absolute performance but frequency effects persist.",
            "uuid": "e9039.5",
            "source_info": {
                "paper_title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4_article_swapping_output_prob",
            "name_full": "GPT-4 article-swapping task: output probability strongly affects accuracy",
            "brief_description": "In an article-swapping task (swap each article with preceding word), GPT-4's accuracy ranged from near-zero on low-probability outputs to very high on high-probability outputs, illustrating sensitivity to target-output probability even for deterministic transformations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0613)",
            "model_description": "See above (GPT-4 description).",
            "model_size": null,
            "test_battery_name": "Article swapping — output probability manipulation",
            "test_description": "Deterministic transformation swapping each article (a/an/the) with the preceding word; outputs categorized by corpus-probability of the full output sentence.",
            "llm_performance": "Example reported: GPT-4 article-swapping accuracy ranged from 0.02 on low-probability examples to 0.83 on high-probability examples (reported in main text / Fig. 4 discussion).",
            "human_baseline_performance": null,
            "performance_comparison": "No direct human baseline reported; authors highlight dramatic performance differences conditioned on output probability.",
            "experimental_details": "Prompt included a single example to disambiguate task; stimuli creation methods as in Materials and Methods; significance tested via logistic regression.",
            "limitations_or_caveats": "Effect shows deterministic tasks can still be influenced by LLM priors; advanced prompting can raise accuracy but relative effects remain.",
            "uuid": "e9039.6",
            "source_info": {
                "paper_title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Acronyms_first_vs_second_letter",
            "name_full": "Acronym formation: first-letter (common) vs second-letter (rare) across models",
            "brief_description": "All five evaluated LLMs perform much better at forming acronyms from the first letters of words (common in corpora) than forming acronyms from second letters (rare), supporting task-frequency sensitivity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple: GPT-4, GPT-3.5, Claude 3, Llama 3, Gemini 1.0",
            "model_description": "Five LLMs accessed via API; basic prompting; Llama 3 specified as 70B parameters, others not parameterized in paper.",
            "model_size": "Llama 3: 70B; others: not specified",
            "test_battery_name": "Acronym formation (first-letter common vs second-letter rare)",
            "test_description": "Given lists of words, produce an acronym formed by either the first letter of each word (common task) or the second letter (rare task).",
            "llm_performance": "Qualitative: performance much higher on first-letter acronyms than on second-letter acronyms for all five models; effect statistically significant (P &lt; 10^-4). Exact accuracy values per model not provided in main text.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline reported; authors use results to support sensitivity to task frequency.",
            "experimental_details": "Acronym inputs lists of 7 words; sample size 1000 per condition; basic prompting without heavy prompt engineering.",
            "limitations_or_caveats": "Exact per-model accuracies not enumerated in main text (reported in figures/SI); some models have very low absolute accuracy on rare variants (floor effects).",
            "uuid": "e9039.7",
            "source_info": {
                "paper_title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Sorting_alpha_vs_reverse",
            "name_full": "Sorting task: alphabetical (common) vs reverse-alphabetical (rare) across models",
            "brief_description": "All five LLMs perform better on sorting lists alphabetically (a frequent pattern in corpora) than on reverse-alphabetical sorting, consistent with sensitivity to task frequency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple: GPT-4, GPT-3.5, Claude 3, Llama 3, Gemini 1.0",
            "model_description": "See descriptions above; evaluated at temperature 0.0 via API.",
            "model_size": "Llama 3: 70B; others not specified",
            "test_battery_name": "Sorting words: alphabetical order (common) vs reverse alphabetical (rare)",
            "test_description": "Given lists of words, sort into either alphabetical order (common in corpora) or reverse alphabetical order (rare).",
            "llm_performance": "All five models perform better on alphabetical than reverse-alphabetical sorting; reported significance P &lt; 10^-5 for all models. Exact accuracy numbers not provided in main text.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline reported; results interpreted as evidence models better at variants more frequently illustrated in training data.",
            "experimental_details": "Sorting inputs length varied 10–20; sample sizes 100 per condition; C4 corpus used to estimate relative frequency (alphabetical estimated 150x more common than reverse).",
            "limitations_or_caveats": "Authors caution that advanced prompting or model augmentations can change absolute performance; their claims concern relative differences across task variants.",
            "uuid": "e9039.8",
            "source_info": {
                "paper_title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "General_evaluation_summary",
            "name_full": "Summary: five LLMs evaluated on cognitive-style tasks emphasizing probability effects",
            "brief_description": "Paper evaluates GPT-4, GPT-3.5, Claude 3, Llama 3 (70B), and Gemini 1.0 on 11 tasks (e.g., shift ciphers, Pig Latin, acronyms, counting, sorting, linear functions) to test hypotheses that LLM performance is sensitive to output probability, input probability, and task frequency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5, Claude 3, Llama 3 (70B), Gemini 1.0",
            "model_description": "Models are large transformer-based LLMs trained primarily via next-word prediction on Internet text, often followed by instruction tuning; evaluated via respective APIs at temperature 0.0.",
            "model_size": "Only Llama 3 explicitly listed as 70B in paper; other models' parameter counts not specified here.",
            "test_battery_name": "Battery of 11 tasks (see Table 2): article swapping, reversal, counting, acronyms, linear function application, multiplication, sorting, keyboard cipher, shift cipher, Pig Latin, birthdays",
            "test_description": "A set of deterministic and quasi-deterministic tasks drawn from manipulations often used in cognitive evaluations (character and word manipulations, arithmetic/transformations, factual recall) chosen to create low-probability conditions and reveal LLM dependencies on corpus-level statistics.",
            "llm_performance": "Varied by model and task; main findings: strong sensitivity to output probability (many tasks show large accuracy differences between high- and low-probability targets), task-frequency sensitivity (models do better on common variants like rot-13, Pig Latin, alphabetical sorting, Celsius→Fahrenheit linear function), and weaker/more conditional input-probability effects (notable for birthday retrieval task). Specific examples: GPT-4 reversal 97% vs 53% (high vs low output prob); GPT-4 rot-13 input prob 21% vs 11%; GPT-4 cipher example 51% vs 13% (high vs low output prob).",
            "human_baseline_performance": null,
            "performance_comparison": "No direct human baselines collected on the same stimuli in this paper; authors discuss that without matched human data they cannot make strong claims about qualitative human-model differences, though they hypothesize humans would be less sensitive to these corpus-probability effects.",
            "experimental_details": "Basic prompting used (single-shot or zero-shot with an example where needed); temperature 0.0; some chain-of-thought / step-by-step prompting and few-shot experiments explored in SI and found to raise absolute performance but preserve relative trends; sample sizes: often 100 per condition, counting had 30 per number, acronym 1000 per condition.",
            "limitations_or_caveats": "No matched human experiments on same stimuli; some models had floor/ceiling effects; exact model sizes for many models not reported; results specific to basic prompting and the particular API/model versions tested; authors note more advanced techniques or tool-augmented models (code execution) can change outcomes but do not invalidate qualitative effects.",
            "uuid": "e9039.9",
            "source_info": {
                "paper_title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Using cognitive psychology to understand GPT-3",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Emergent analogical reasoning in large language models",
            "rating": 2,
            "sanitized_title": "emergent_analogical_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with GPT-4",
            "rating": 2,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Reasoning or reciting? Exploring the capabilities and limitations of language models through counterfactual tasks",
            "rating": 2,
            "sanitized_title": "reasoning_or_reciting_exploring_the_capabilities_and_limitations_of_language_models_through_counterfactual_tasks"
        },
        {
            "paper_title": "Large language models struggle to learn long-tail knowledge",
            "rating": 1,
            "sanitized_title": "large_language_models_struggle_to_learn_longtail_knowledge"
        },
        {
            "paper_title": "Comparing humans, GPT-4, and GPT-4v on abstraction and reasoning tasks",
            "rating": 1,
            "sanitized_title": "comparing_humans_gpt4_and_gpt4v_on_abstraction_and_reasoning_tasks"
        }
    ],
    "cost": 0.0186795,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Embers of autoregression show how large language models are shaped by the problem they are trained to solve
October 4, 2024</p>
<p>R Thomas Mccoy 
Shunyu Id 0000-0001-9383-4079
Yao 
Dan Friedman 
Mathew D Hardy 
Thomas L Griffiths 0009-0001-2495-1893</p>
<p>Indiana University Bloomington,
Bloomington, IN</p>
<p>Department of Linguistics, Yale University,
New Haven, CT 06520. New Haven, CT 06520. San Francisco, CA 94110.</p>
<p>Wu Tsai Institute, Yale University,</p>
<p>4 Present address: OpenAI,</p>
<p>Embers of autoregression show how large language models are shaped by the problem they are trained to solve
October 4, 20248977BF887A775A4B0B101AEA908180E410.1073/pnas.2322420121/-/DCSupplementalreceived December 19, 2023; accepted August 9, 2024
The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations.We argue that to develop a holistic understanding of these systems, we must consider the problem that they were trained to solve: next-word prediction over Internet text.By recognizing the pressures that this task exerts, we can make predictions about the strategies that LLMs will adopt, allowing us to reason about when they will succeed or fail.Using this approach-which we call the teleological approach-we identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input.To test our predictions, we evaluate five LLMs (GPT-3.5,GPT-4, Claude 3, Llama 3, and Gemini 1.0) on 11 tasks, and we find robust evidence that LLMs are influenced by probability in the hypothesized ways.Many of the experiments reveal surprising failure modes.For instance, GPT-4's accuracy at decoding a simple cipher is 51% when the output is a high-probability sentence but only 13% when it is low-probability, even though this task is a deterministic one for which probability should not matter.These results show that AI practitioners should be careful about using LLMs in low-probability situations.More broadly, we conclude that we should not evaluate LLMs as if they are humans but should instead treat them as a distinct type of system-one that has been shaped by its own particular set of pressures.Competing interest statement: D.F. was previously affiliated with Google, which created one model that we evaluated (Gemini 1.0).S.Y. is affiliated with OpenAI, which created two models that we evaluated (GPT-3.5 and GPT-4).</p>
<p>cognitive science | artificial intelligence | large language models Large language models (LLMs), such as ChatGPT (1), Claude (2), and Llama (3), receive a piece of text as input and generate additional text as output.Virtually any task can be framed in the form of linguistic queries, so LLMs could be applied to virtually any task-from summarizing text to generating computer code.This flexibility is exciting: it led one recent paper to argue that LLMs display "sparks of artificial general intelligence" (4).However, it also hinders us from understanding LLMs holistically.Since we can only run a finite number of tests, how can we understand a system whose potential scope is infinite?Answering this question requires some method for deciding which tests will be most informative about the general strengths and weaknesses of LLMs.</p>
<p>One popular way to select evaluations is to use a human-centric approach: test for the properties that are viewed as most important for characterizing human cognition.For instance, GPT-4 was evaluated on the SAT and other real-world exams (1), and many LLMs have been evaluated on tests from cognitive psychology (5), such as tests of analogical reasoning (6).Other analyses have compared the internal representations of LLMs to representations from linguistics (7) and neuroscience (8).Human-inspired analyses are valuable because they allow AI to benefit from the wealth of nuanced tests that have been developed in cognitive science.However, human-centric approaches also have an important limitation: LLMs are not humans, so the tests that might be most informative about them may differ from the tests that are most informative about humans (9).In particular, a human-centric approach runs the risk of highlighting the strengths of these models-their overlap with human abilities-without revealing their idiosyncratic weaknesses (Fig. 1).How can we approach the problem of understanding a new type of intelligence, evaluating it on its own terms?</p>
<p>We argue for another approach that has been very productive in cognitive science: understanding intelligent systems by understanding the sorts of problems that they developed to solve (10)(11)(12)(13).This approach-which we refer to as the teleological approach-is complementary to the perspectives discussed above because it focuses on the system's goals and environment, rather than its representations and processing mechanisms.The crucial question to ask, then, is what problem(s) do LLMs need to solve, and how do these pressures influence them?We focus on perhaps the most salient</p>
<p>Significance</p>
<p>ChatGPT and other large language models (LLMs) have attained unprecedented performance in AI.These systems are likely to influence a diverse range of fields, such as education, intellectual property law, and cognitive science, but they remain poorly understood.Here, we draw upon ideas in cognitive science to show that one productive way to understand these systems is by analyzing the goal that they were trained to accomplish.This perspective reveals some surprising limitations of LLMs, including difficulty on seemingly simple tasks such as counting words or reversing a list.Our empirical results have practical implications for when language models can safely be used, and the approach that we introduce provides a broadly useful perspective for reasoning about AI.Fig. 1.Humans and LLMs have some shared properties and some properties that differ.If we analyze LLMs using tests designed for humans, we risk identifying only the shared properties, missing the properties that are unique to LLMs (the dotted region of the diagram).We argue that to identify the properties in the dotted region, we must approach LLMs on their own terms by considering the problem that they were trained to solve: next-word prediction over Internet text.</p>
<p>pressure that defines any machine learning system, namely the task that it was trained to perform.For the LLMs that have been the focus of recent attention in AI, this task is autoregressionnext-word prediction (14,15)-performed over Internet text.(Many LLMs are further optimized for additional objectives, but we focus on next-word prediction because it comprises the bulk of training; see below for discussion.)We argue that a full understanding of LLMs should be strongly informed by the fact that next-word prediction is their primary training objective.We therefore hypothesize that, even when they are asked to perform tasks that seem very different from next-word prediction-e.g., solving math problems-their performance on such tasks is highly influenced by the autoregressive substrate from which all of their abilities emerge.</p>
<p>A mismatch between the problem that a system developed to solve and the task that it is given can have significant consequences.Just as the human drive to obtain sweet and fatty foods can be maladaptive in a world where those foods are easily available, the autoregressive tendencies of LLMs can cause problems when they are given a task that is not nextword prediction.In this paper, we exhibit what might seem like surprising failures of LLMs on tasks that are straightforward for humans to perform, such as using simple ciphers, constructing acronyms, calculating linear functions, and counting; see Fig. 2.These failures can be understood in terms of a conflict between next-word prediction and the task at hand.Being able to reason about when LLMs will fail at a task is critical as these systems become more widely deployed.</p>
<p>Based on an analysis of the problem that LLMs are trained to solve (statistical next-word prediction), we make three predictions about how LLMs will be influenced by their origin in this taskthe embers of autoregression that appear in these systems even as they might show sparks of artificial general intelligence.These predictions are listed in Table 1.For example, we predict that, even when the task is a deterministic one that does not depend on probability, LLMs will achieve higher accuracy when the answer is high-probability than when it is low-probability.In the next section, we discuss these hypotheses in more detail.</p>
<p>We then test these hypotheses through an extensive set of experiments.Since our goal is to identify sources of difficulty for LLMs, our experiments need to take place in settings where LLMs make at least some errors.To create such settings, we use an adversarial strategy that follows straightforwardly from our hypotheses: we use tasks that push LLMs into low-probability situations where we expect they will perform poorly.For instance, one task we use is decoding simple ciphers, since text written in a cipher is a low-probability subspace of the broader space of possible types of text.Table 2 gives the full list of tasks that we use.As long as models indeed have a nonnegligible error rate on these tasks, we can investigate which factors cause this error rate to increase or decrease-focusing on the factors that we have hypothesized will have an effect.</p>
<p>Using this strategy, we find robust evidence for the effects described in Table 1: as task probability or example probability varies, LLM accuracy can indeed vary substantially in the ways that we have hypothesized.Overall, our findings illustrate how we can understand LLMs more clearly if we recognize the pressures that have shaped them.</p>
<p>Background: LLMs</p>
<p>LLMs are trained to take in the start of a piece of text and then predict what word will come next.* The LLM's prediction takes the form of a probability distribution, specifying for each word in the vocabulary the LLM's predicted probability that this word will be the next one.For example, given the input I wrote a __, an LLM should assign a high probability to letter or book but a low probability to waffle or the.In almost all contexts, there are multiple possibilities for what could appear next, so this task is probabilistic rather than deterministic.</p>
<p>The way that an LLM comes to perform next-word prediction is by learning from data.The LLM is defined by a large number of numerical parameters, which govern how it maps an input to a predicted probability distribution.These parameters are initialized randomly and are updated based on data: The LLM is shown many passages of text, and for each position in each passage, it predicts what words are likely to appear in that position.The LLM is then shown the word that actually did appear, and its parameters are adjusted such that, if it were shown the same input again, it would assign a higher probability to the correct word than it previously did.As the training proceeds, the LLM's text-predicting abilities become stronger.A trained LLM can be used to generate text (such as a chatbot response) by giving it an input and then sampling a word from the probability distribution that it generates.To produce text longer than one word, the process is iterated, using the LLM's previously produced word(s) as part of the input when predicting each successive word.</p>
<p>LLM training data are typically scraped from the Internet, resulting in a diverse range of types of text.A powerful consequence of this diversity is that learning to perform nextword prediction gives the LLM experience with a wide range of tasks, including many that might seem very different from next-word prediction (15).For example, an LLM can learn to summarize through experience with predicting what comes after In summary, and it can learn arithmetic via inputs like 17 + 34 = __.In this way, a single LLM can learn to perform many tasks that historically would have required multiple specialized systems.Therefore, next-word prediction is a major factor contributing to the power of LLMs-but, as we will see below, it also leads to some important shortcomings.</p>
<p>A Teleological Approach to Understanding LLMs</p>
<p>To understand an information-processing system such as an LLM, the approach we argue for is to characterize the problem Fig. 2. GPT-4 struggles on some seemingly simple tasks such as counting, article swapping, shift ciphers, and linear functions.In the counting and article swapping examples, GPT-4 fails in the cases where the correct output is a low-probability piece of text (for the counting example, we refer to 29 as low-probability because it occurs much less frequently in natural corpora than 30 does).In the shift cipher and linear function examples, GPT-4 performs well on the task variants that are common in Internet text but poorly on the variants that are rare (note that the shift cipher with a shift of 13 is over 100 times more common in Internet text than the shift cipher with a shift of 12; and the linear function f (x) = (9/5)x + 32 is common because it is the Celsius-to-Fahrenheit conversion, while the other linear function has no special significance).The GPT-4 predictions were obtained using gpt-4-0613 on the OpenAI API; other model versions (e.g., the online chat interface) may give different predictions.</p>
<p>that the system solves and to then use this characterization as a source of hypotheses about the system's capacities and biases.We refer to this as the teleological approach because it focuses on the system's goal (telos in Greek) (e.g., ref. 16).Teleological explanation is a common strategy in making sense of biological systems, manifest in computational-level (10) and rational (12) analysis in cognitive science and adaptationist explanations in evolutionary biology (17,18) and evolutionary psychology (12).Just as teleological explanations can be incomplete or misleading in these settings (19,20), we do not anticipate that all properties of LLMs can be understood via their goals.However, we believe that this is a useful lens through which to study these systems and generate predictions about their behavior.In the rest of this section, we describe more clearly the goals that LLMs are trained to accomplish.We then use this analysis to generate hypotheses about LLM behavior.</p>
<p>What Problem Do LLMs Solve?Anderson (12) argued that the problem solved by a cognitive system can be characterized by three factors: the system's goal, the environment in which the system pursues this goal, and the computational limitations that constrain the system.For a machine learning system, those factors correspond to the following aspects of a model: Note that many recent LLMs are not solely trained on nextword prediction but also go through a training phase based on instruction tuning (22), which aims to align model behavior with human preferences.In this paper, we only analyze next-word prediction, leaving instruction tuning for future work.We start with next-word prediction because, during LLM training, the next-word prediction phase is much longer than the instruction tuning phase.We therefore conjecture that next-word prediction plays a greater role in shaping LLMs than instruction tuning does, making next-word prediction a reasonable initial focus for the goal of understanding LLMs.Note also that some LLMs (e.g., some recent versions of GPT-4) are augmented with components that go beyond pure text, such as image-processing abilities or code-execution modules.In this paper, we only consider LLMs that process text sequences, though the teleological perspective could be applied to augmented LLMs in future work; see the Discussion.</p>
<p>Importantly, the tasks for which LLMs are used often differ from the problem they were trained to solve.For example, even though they were trained for next-word prediction, LLMs are sometimes asked to develop Python code.Why does this discrepancy matter?When a system is adapted for one purpose but co-opted for a different one, the original purpose may influence the system's nature in ways that would not make sense if only the new purpose were considered.As an example from biology, the basic mammalian body plan evolved for a life on four legs, but humans recently evolved to instead walk on two legs.Biologists have argued that the friction between our quadrupedal roots and our bipedal lifestyle contributes to several musculoskeletal ailments that are common in humans (23).For instance, having our spines be perpendicular to the ground rather than parallel with it causes strains that make humans susceptible to lower back pain (24).We anticipate that the goals, training distribution, and Table 1.Effects on the performance of language models that are attributable to the fact that they are statistical next-word prediction systems</p>
<p>Property Description Example</p>
<p>Sensitivity to output probability Even when the task is deterministic, LLMs achieve higher accuracy when the correct answer is high-probability text than when it is low-probability text.</p>
<p>When asked to reverse a sequence of words, GPT-4 gets 97% accuracy when the answer is a high-probability sentence yet 53% accuracy when the output is low probability.</p>
<p>Sensitivity to input probability</p>
<p>Even when the task is deterministic, LLMs sometimes achieve higher accuracy when the input text is high-probability than when it is low-probability, but input probability is less influential than output probability.</p>
<p>When asked to encode sentences in a simple cipher (rot-13), GPT-4 gets 21% accuracy when the input is a high-probability sentence yet 11% accuracy when the input is low-probability.</p>
<p>Sensitivity to task frequency</p>
<p>Even when there is no difference in the complexity of the tasks, LLMs perform better on tasks that are frequent than ones that are rare.</p>
<p>When asked to translate English sentences into Pig Latin, GPT-4 gets 42% accuracy when using the most common variant of Pig Latin but only 23% accuracy when using a rare variant.</p>
<p>model architecture used in LLMs create opportunities for such mismatches.</p>
<p>Hypothesized Embers of Autoregression.Do LLMs face anything analogous to humans' back pain-any quirks attributable to a mismatch between what they "adapted" to do and what they are used for?To answer this question, we must consider the types of solutions that would arise from the pressures that shape LLMs.First, the fact that LLMs are neural networks makes them statistical systems.The other two factors-the task of nextword prediction and the training distribution of Internet textdictate which statistics they are sensitive to: the statistics of word sequences in Internet text.Building on this analysis, we make three predictions (listed in Table 1) about ways in which LLM performance is influenced by frequency and probability.A crucial aspect of these predictions is the first clause of each one.For instance, it is clear that probability is important for next-word prediction, but we are predicting that probability will influence LLMs even in tasks that are not inherently probabilistic, such as reversing a list.In later sections, we develop these hypotheses in more detail and provide extensive evidence supporting them.</p>
<p>What We Are Not Arguing.We are arguing that certain types of tasks and examples will be harder for LLMs than others.We do not claim that LLMs are incapable of handling these tasks and examples.That is, our predictions are of the form "for LLMs, X is harder than Y" rather than "LLMs can't do X."This distinction is important for explaining how we set up our experiments.For most experiments, we test LLMs using basic prompting (i.e., simply asking them to provide an answer to a query), even though LLM accuracy can be substantially increased via more sophisticated inference techniques such as chain-of-thought prompting (25,26) or the tree-of-thoughts framework (27).Our decision to use basic prompting would be problematic if we were claiming that LLMs cannot perform our tasks: such claims would be true only if LLMs fail under all prompting approaches, so investigations of such claims should use the strongest possible prompting approach.However, since this is not the type of claim we are making, it is not necessary to use the strongest available prompting paradigms.Instead, because our claims are comparative ("for LLMs, X is harder than Y"), what is important is ensuring that the two conditions (X and Y) are evaluated in the same way as each other.Since any prompting approach could work for this purpose, we chose the approach that is the fastest and most straightforward to run, namely basic prompting-though see Discussion for experiments with other prompting techniques.</p>
<p>In addition, we are not claiming that researchers have ignored the autoregressive origins of LLMs.Indeed, in the "Sparks of artificial general intelligence" paper (4), Section 8 is titled "Limitations of autoregressive architecture highlighted by GPT-4," and it considers failures of planning in arithmetic and text generation that result from only predicting the next word in a sequence.What we are claiming is that this aspect of LLMs has been neglected in constructing effective evaluations of their capacities.As highlighted in Fig. 1, much of the literature evaluating LLMs has started with tasks that are viewed as important indicators of human abilities and then assessed how well LLMs can do them.By instead starting with tasks that we anticipate will be challenging for systems that are focused on next-word prediction, we get a more balanced view of what kinds of tasks are easy or hard for these systems.</p>
<p>Motivating Our Predictions: A Bayesian Analysis</p>
<p>In Table 1, we listed several hypotheses about factors that cause difficulty for LLMs.Here, we show how we arrived at these hypotheses by analyzing the problem that LLMs need to solve.To make this discussion more concrete, we will provide brief examples from one particular domain, namely shift ciphers (defined below).Later in the paper, we will further test our predictions with an additional ten tasks.</p>
<p>Running Example: Shift Ciphers.In a shift cipher, a message is encoded by shifting each letter forward in the alphabet a certain number of positions.For example, with a shift of 1, How are you?becomes Ipx bsf zpv? because the letter after h is i, the letter after o is p, etc.One particularly prominent case is rot-13 (short for "rotate by 13"), the cipher with a shift of 13 positions.Rot-13 is popular online as a spoiler-free way to share information.For example, in some puzzle-solving forums, members write hints in rot-13 so that those who want to solve the puzzle without help will not read the hint inadvertently.</p>
<p>A Bayesian Analysis.LLMs are statistical systems.We therefore expect that their predictions will be influenced by probability, even in deterministic situations where probability should be irrelevant.To motivate this hypothesis more formally, we first frame the LLM's task as finding the most probable output given some input, where the input is the start of a word sequence, and the output is the sequence's continuation.In other words, the LLM should find the output that maximizes P(output|input) (the probability of the output given the input).By Bayes' rule, this problem is equivalent to maximizing P(input|output)P(output).Therefore, as long as there are multiple outputs for which P(input|output) is nonzero, the LLM's predictions will be influenced by the unconditional probability of the output, P(output): among the candidates that yield a nonzero value for When decoding text written in the rot-13 cipher, both models score better when the answer is high-probability than when it is low-probability.Top Middle: Effect of input probability.As predicted, input probability does not show as strong an effect as output probability.Top Right: Effect of task probability.Both models score better on a frequently occurring shift cipher (rot-13) than on a rare one (rot-12).All error intervals show one SE.Bottom: Examples for output and input probability; see Fig. 2 for task probability examples.</p>
<p>P(input|output), LLMs will be biased toward selecting ones with a high P(output).</p>
<p>The argument made so far only covers nondeterministic situations, when there are multiple outputs that yield a nonzero P(input|output).Now consider deterministic settings, where there is only one output that could go with a given input; rot-13 is one such setting.In principle, P(output) should not matter in such cases because the likelihood, P(input|output), fully determines the output on its own: this value is nonzero for the correct output and zero for all others.In practice, however, an LLM's likelihood estimates are unlikely to be perfect, meaning that there will in fact be multiple candidate outputs that yield a nonzero likelihood.With multiple output candidates in consideration, P(output) will affect the model's predictions in practice, even if in theory it should be irrelevant.This analysis points toward several hypotheses about LLM behavior, which we discuss in the next few paragraphs.</p>
<p>Sensitivity to Output Probability.The most straightforward consequence of our analysis is the prediction that LLMs will be biased toward producing outputs that are high-probability word sequences, meaning that they will perform better when the correct output is indeed high-probability than when it is not.The results in Fig. 3 (Top Left) support this prediction: for both GPT-3.5 and GPT-4, accuracy increased as the log probability of the correct output was increased, supporting the hypothesis that these systems are influenced by the unconditional probability of potential outputs.</p>
<p>Sensitivity to Input Probability and Task Probability.Under our analysis, the factor that causes LLMs to favor high-probability outputs is noise in the LLM's estimate of the likelihood.We therefore hypothesize that LLMs will be particularly error-prone in situations where their likelihood estimates are particularly noisy.We identify two such situations: when task probability is low and when input probability is low.</p>
<p>Task probability is the probability that a task will be illustrated in a random sample of text.This probability determines how many examples of the task will appear during an LLM's training.When the task probability is low, the LLM's likelihood estimates will be noisy because it will not have had enough experience with the task to produce precise estimates of task-relevant statistics, leading to poorer overall performance.</p>
<p>This prediction is borne out for shift ciphers.As mentioned above, rot-13 is a commonly used shift cipher.In contrast, rot-12-which uses a shift of 12 rather than 13-is no more complex than rot-13 but is used rarely.We found that GPT-3.5 and GPT-4 performed much better at decoding messages written in rot-13 than rot-12 (Fig. 3, Top Right), supporting the conclusion that LLMs are sensitive to task probability.</p>
<p>Another factor that we expect will reduce the quality of likelihood estimates is the probability of individual examples: a trained LLM will have had less experience with low-probability strings than high-probability ones.Therefore, the information that the model captures may be less robust for low-probability strings than high-probability strings.Information learned during pretraining is important for both processing the input and producing the output.We have already hypothesized that LLMs will be sensitive to output probability; this argument adds the additional hypothesis that they will be sensitive to input probability.Fig. 3 (Top Middle) shows that GPT-4 achieves higher accuracy when the input is high-probability than when it is low-probability, illustrating that LLMs can indeed be sensitive to input probability.GPT-3.5 shows no significant effect of input probability.In the next subsection, we describe why LLMs may show less sensitivity to input probability than output probability or task probability.</p>
<p>We have identified these effects as consequences of autoregression, but autoregression is not the only objective that we would expect to produce these effects.Sensitivity to word sequence probability would likely arise in any system that models text distributions, and sensitivity to task probability would likely arise in any system that uses statistical learning.We focus on autoregression because it is central to current LLMs, but future work could use the perspective to investigate whether other objectives also yield these effects.</p>
<p>Strength of Hypothesized Effects.We have hypothesized that LLMs will be sensitive to the probability of both the input and the target output.Though these effects sound similar, we have predicted them for two different reasons.Input probability was predicted to matter only when the task depends on information about the input that models mainly learn through experience with that specific input (i.e., when the model's ability to estimate the likelihood, P(input|output), does not generalize to novel inputs).We expect that such situations only arise occasionally, such that a model's dependence on input probability may not manifest itself routinely.In contrast, output probability was predicted to matter not only in the same situations where input probability matters but also in an additional context: whenever the model has any uncertainty about what the output is (in which case the model will use the prior probability of the output to help resolve its uncertainty).We expect that this situation is common since neural networks rarely produce probabilities that are precisely equal to one.We therefore expect that output probability will be more broadly influential than input probability.</p>
<p>Another relationship between our hypotheses is that task probability and input probability were hypothesized to matter for similar reasons: both influence the training data in ways that will affect the level of noise in the LLM's learned likelihood estimates.However, we predict that task probability will lead to such effects more reliably than input probability: we expect that LLMs will often be able to generalize to novel inputs (because neural networks usually perform well on new examples that are similar to ones they have seen), such that low input probability will not always hamper LLM performance.In contrast, novel tasks require a higher-order type of generalization that is more challenging for neural networks, so we predict that task probability will routinely have an effect.</p>
<p>In sum, we have predicted that output probability and task probability will be more broadly influential than input probability.An initial piece of evidence for this hypothesis is the fact that, in Fig. 3, output probability and task probability have a substantial effect, whereas input probability shows only a minor effect and only in one model.</p>
<p>Overview of Experiments</p>
<p>In the next several sections, we test the hypotheses developed above by analyzing LLM performance on a wider range of tasks.We evaluated five models: GPT-4 (1), GPT-3.5 (1), Claude 3 (2), Llama 3 (3), and Gemini 1.0 (28).GPT-4 is the state-of-theart in many areas and is the focus of the "sparks of AGI" paper (4) that our work connects to.GPT-3.5 is similar to GPT-4 but smaller, enabling us to investigate the effects of model scale.The remaining models are other prominent LLMs.We accessed all models through APIs and used a temperature of 0.0; see Materials and Methods for more details.We also ran preliminary tests with the smaller-scale models OLMo (29) and Llama 2 (3), but their scores were too low to yield meaningful trends, with accuracies of 0% on most tasks.</p>
<p>The tasks that we tested these models on are described in Table 2.We chose these tasks for two reasons.First, as described in the introduction, we selected tasks that push models into lowprobability situations so that models will have a nonnegligible error rate-a requirement for our goal of observing what causes the error rate to increase or decrease.Second, most of these tasks can be solved with a simple, deterministic algorithm that is invariant to various changes to the task.For example, consider a shift cipher that moves each letter n positions forward in the alphabet.The most straightforward way to decode this cipher is to shift each letter back n positions-an algorithm that succeeds regardless of the value of n and the identity of the words being processed.Thus, if a person demonstrated that they could decipher one message written in rot-13, we might assume that they knew this algorithm and could therefore perform equally well on any other shift cipher or on any other inputs.However, we have hypothesized that LLMs are not invariant to such properties: we expect their performance to vary based on which task variant is used and which inputs are used.The tasks described in the table allow us to test these predictions.Following the "sparks of AGI" paper (4), we do not tune models on these tasks but rather describe the task in the prompt; see Materials and Methods.Many of our tasks involve character-level manipulations of words.This factor might seem unfair to LLMs because they operate over subword tokens (30) rather than characters.However, we gave all models a spelling test (SI Appendix, section L) and found that they robustly encode the spelling of their tokens, making it reasonable to test them on tasks that involve character-level manipulations; see also refs.31 and 32.</p>
<p>Sensitivity to Output Probability</p>
<p>We have hypothesized that LLMs will perform better when the correct answer is a high-probability string than when it is a low-probability string, even in deterministic situations where the answer could be determined without considering probability.Here, we test this hypothesis by evaluating models on sets of examples that vary in the probability of their outputs (probabilities were estimated using GPT-2 (15); see SI Appendix, section J).For example, for shift ciphers, we evaluate accuracy on three sets of examples corresponding to messages that, when are high-, medium-, or low-probability sequences of words; see (1) below for examples:</p>
<p>(1) a. High-probability: Are they now building a bridge of their own?b.Medium-probability: Are dogs yet climbing a jungle of their own?c.Low-probability: Are their jungle of dogs a yet climbing own?Fig. 4 shows the results.Output probability had a statistically significant effect on accuracy (P &lt; 0.05) in all cases except Gemini 1.0 decoding shift ciphers (its accuracy was 0% across conditions) and Claude 3 forming acronyms (P = 0.38).Many cases showed large effect sizes; e.g., GPT-4's article-swapping accuracy ranged from 0.02 on the set of low-probability examples to 0.83 for high-probability examples.</p>
<p>Analysis of Shift Cipher Errors.As a more targeted analysis of output probability effects, we gave models messages encoded with a shift of 13 where the correct answer was created by manually changing one word in a high-probability sentence to a new word that was still grammatical but was now low-probability.In such cases, models often "regularized" the output by producing the high-probability sentence that was similar to the correct answer, rather than the correct answer; indeed, GPT-3.5 and GPT-4 produced the regularized version more often than the correct one (see SI Appendix, Fig. S23, for quantitative results), as in the following GPT-4 response:</p>
<p>(2) a. Correct output: Because of this, their names were chanted for security reasons.b.GPT-4 output: Because of this, their names were changed for security reasons.</p>
<p>This tendency to push the output toward a higher-probability sentence further supports the conclusion that LLMs are biased toward high-probability outputs.</p>
<p>Counting.As a final investigation of output probability, we evaluated models on counting how many words are in a list.Fig. 5 plots LLM accuracy as a function of the number being counted to.LLMs often show spikes in accuracy for the numbers that are most frequent in corpora, namely multiples of 10.Further, for all models except Llama 3, accuracy has a higher Spearman correlation with the frequency of the number being counted to than with its magnitude (e.g., the correlation coefficients have an</p>
<p>5.</p>
<p>Model performance on counting a list of words, as a function of the length of the list.The intervals around the lines show one SE.</p>
<p>absolute value of 0.84 vs. 0.70 for GPT-4, and 0.85 vs. 0.66 for Claude 3), providing additional evidence for the importance of frequency.</p>
<p>Summary of Output Probability Effects.</p>
<p>We have shown that the performance of the LLMs we tested is heavily influenced by the probability of the target output, even though the tasks being investigated were not inherently probabilistic.These results support our hypothesis that LLMs are sensitive to the probability of the sequences they must produce.</p>
<p>Sensitivity to Input Probability</p>
<p>We have hypothesized that LLMs will sometimes perform worse when the input is low probability than when it is high probability.However, we have also hypothesized that the influence of input probability will be less pervasive than the influence of output probability: we predicted that LLMs would use output probability whenever they have some uncertainty about the output-a condition that we expect holds frequently-whereas we predicted that input probability would only matter when an LLM's ability to process an input is highly dependent on prior experience with that specific input-a condition that we expect to hold less often under the view that, for many tasks, neural networks are not restricted to handling only the inputs they have seen but can also generalize across inputs.</p>
<p>Consistent with these predictions, we found that, for most of the tasks we investigated, input probability had little or no effect on accuracy (Fig. 6); see SI Appendix, section E.4 for additional discussion of the asymmetry between input probability and output probability.There was, however, one task-the birthday task-where input probability had a large effect (P &lt; 10 −15 for all models).In this task, a model is given the name of a public figure and is asked to return that person's birthday.For this task, the answer cannot be deduced from the input alone, so the only way it can be produced is if the model has encountered it during training.This task therefore has the properties that we have hypothesized would lead to input probability sensitivity; the fact that models indeed display this sensitivity therefore supports our analysis.</p>
<p>Additional situations where input probability has a large effect can be found in prior work studying LLM performance on arithmetic (33) and factual recall (34,35).Overall, we conclude that, under the right conditions, LLM performance can indeed be meaningfully affected by input probability.</p>
<p>Sensitivity to Task Probability</p>
<p>We have hypothesized that LLMs will perform better on tasks that are frequently illustrated in Internet text than tasks that occur more rarely-even when the rare task is no more complex than the common one.In this section, we test this hypothesis by evaluating models on common and rare variants of several tasks (we use corpus analyses to estimate task frequency; see SI Appendix, section K).Fig. 9 summarizes the results.</p>
<p>Shift Ciphers.There are 25 possible shift ciphers, corresponding to the 25 unique shifts that can be applied within the alphabet.These different shifts are not all used with the same frequency.</p>
<p>Based on an analysis of the C4 corpus (36), we found that the three most common shift levels are rot-1, rot-3, and rot-13 (Fig. 7, Top).Rot-1 is common because it is the smallest shift, so it is a natural choice for illustrating the concept of a shift cipher.Rot-3 is common because Julius Caesar famously used this cipher (37), creating a precedent that has influenced many others to adopt a shift of 3 as well (e.g., refs.38 and 39).Finally, as mentioned above, rot-13 is common because there is a convention of using it in online forums.</p>
<p>If LLMs are indeed sensitive to task probability, we would expect them to perform better on shifts of 1, 3, and 13 than on other shifts.To test this prediction, we evaluated the models on decoding text written with each shift from 1 to 25.The results are striking (Fig. 7, Bottom): all models except Gemini 1.0 show a spike in accuracy at 13, which is by far the most common shift.GPT-4 scores 0.50 or above on the three highest-frequency shifts, but its accuracy is less than 0.03 for all other shifts.Claude 3 differs from the other models in that it performs well on some rare shifts, suggesting that it has a more generalizable decoding 7. Analysis of shift ciphers with shift levels varying from 1 to 25. Top: Number of occurrences of each shift level in the C4 corpus.Bottom: Accuracy on decoding shift ciphers with various shifts.Most models display a spike in accuracy at a shift of 13, which the top plot shows to be the most common shift level.</p>
<p>strategy than other models, but it still also shows a spike in performance at 13.These results therefore follow the prediction that LLMs would perform better on commonly used shifts than rarely used ones.</p>
<p>To test the significance of these task effects, we compared rot-13 to rot-12 (as examples of a common and rare variant, respectively).We only considered GPT-3.5, GPT-4, and Claude 3, as the other models scored near 0% on all shift ciphers.All three of these models had a statistically significantly better performance on decoding rot-13 than rot-12 (P &lt; 0.01).We also evaluated these models on encoding; in this case, GPT-4 and Claude 3 still performed significantly better on rot-13 than rot-12 (P &lt; 0.01), but for GPT-3.5, there was no significant difference, likely due to floor effects (GPT-3.5'sencoding accuracy was close to 0.0 for both shifts; see Fig. 9).</p>
<p>Analysis of Shift Cipher Errors.</p>
<p>For shift levels with a low accuracy (e.g., shifts other than 1, 3, or 13 for GPT-4), the incorrect answers that models produce are often recognizable sayings or quotations.For instance, in one rot-10 case, GPT-4 erroneously produced a quote from Shakespeare (3-b).Clearly something is rot-10 in the state of Denmark:</p>
<p>(3)</p>
<p>a. Correct answer: As a doctor of humanities, he was a university professor, founded a university and a newspaper, and won awards in journalism and literature.b.GPT-4 output: To be or not to be, that is the question, whether tis nobler in the mind to suffer the slings and arrows of outrageous fortune.</p>
<p>This tendency to produce famous quotations in contexts where accuracy is low is consistent with our Bayesian analysis: when an LLM is highly uncertain about the task, we expect that its likelihood estimates would be close to uniform, so that its predictions would effectively be sampled from its prior-a process that would be likely to produce high-probability sentences such as famous quotations.</p>
<p>Pig Latin.Pig Latin is a secret "language" based on English.To convert an English sentence into Pig Latin, the first consonant cluster of each word is moved to the end of the word and then -ay is added to the end of the word.We compared Pig Latin to a system that we made up called Boar Etruscan that instead uses -uv as the letter pair that is added at the end.For instance, the English word main would become ainmay in Pig Latin or ainmuv in Boar Etruscan.We tested LLMs on both encoding sentences into these fake languages and decoding sentences from these fake languages.The results are in Fig. 9, in which Pig Latin is the "common" variant and Boar Etruscan is the "rare" one.For encoding, GPT-3.5, GPT-4, and Claude 3 performed much better on Pig Latin than Boar Etruscan, a difference that was statistically significant (P &lt; 0.01); the other models showed no significant difference, likely due to floor effects (their accuracy was near 0% for both variants).For decoding, all five models scored better on Pig Latin than Boar Etruscan, but the difference was significant only for GPT-4 and Llama 3 (P &lt; 0.05).</p>
<p>In addition to this binary comparison of attested vs. unattested, Pig Latin also provides an opportunity for a finer-grained comparison because there are several variants of Pig Latin that have varying levels of commonness (40).All the major variants handle consonant-initial words the same (by moving the initial consonant cluster to the end and adding -ay, such as turning frog to ogfray), but they differ for vowel-initial words: all the variants add some fixed syllable at the end of vowel-initial words, but the identity of that syllable varies.We identified all mentions of Pig Latin in the Pile dataset (41), as a proxy for the training data of GPT models and found 68 cases where the added syllable was -way, 46 where it was -ay, 26 where it was -yay, and 8 where it was -hay; we found similar proportions in the C4 dataset (36).We tested LLMs on these four variants, plus a fifth one where the added syllable was -say (which has 0 occurrences in C4 and the Pile).As an example of how these variants differ for vowel-initial words, the word and could become andway, anday, andyay, andhay, or andsay, depending on the variant used.</p>
<p>We found that (particularly for encoding) model performance patterns with the corpus frequency of the Pig Latin variant (Fig. 8).The effect of variant was significant for encoding (P &lt; 0.01) for all models except Llama 3 (P = 0.19), but for decoding, it was significant only for Claude 3 (P &lt; 0.01) and Gemini 1.0 (P &lt; 0.01).Variant frequency might matter less for decoding than encoding because models may find it easier to remove an unfamiliar ending (what must be done when decoding a rare variant) than to produce an unfamiliar ending (what must be done when encoding a rare variant).This finding is striking because all five of these variants are very similar: they treat most words identically because they differ only for vowelinitial words; and even in these cases, they differ from each other only in a single letter.Nonetheless, these differences are enough to yield markedly differing performance along the lines we predicted.</p>
<p>Acronyms.People frequently join together the first letter of each word in a sequence of words, but it is rare to join the second letter of each word.We therefore use first-letter acronym formation as a common task (e.g., producing RESPOND from revolve edifice scrappy panicky outlast negated drizzle) and second-letter acronym formation as a rare task (e.g., producing RESPOND from prequel leaping ascetic splurge policed invader edifice).For all five models, performance was much higher on first-letter acronyms than second-letter ones (Fig. 9), and the effect was statistically significant (P &lt; 10 −4 ).Linear Functions.We compared two tasks where models had to apply a linear function to a number.As a common function, we used f (x) = (9/5)x + 32; this function occurs frequently in natural text because it is the function that converts temperatures from Celsius to Fahrenheit, so text written for multinational audiences often includes nearby pairs of numbers illustrating both x and f (x).As a rare function, we used f (x) = (7/5)x + 31, which has no special significance and thus does not show up often in natural text.GPT-3.5, GPT-4, and Claude 3 scored  Comparing performance on common vs. rare versions of tasks.For shift ciphers, the common variant that we illustrate is rot-13, and the rare variant is rot-12.For Pig Latin, the common variant is Pig Latin, and the rare variant is Boar Etruscan (see text).For acronyms, the common variant is acronyms formed from the first letters of words, while the rare variant is acronyms formed from the second letters of words.The common linear function is f (x) = (9/5)x +32 (which is the Celsius-to-Fahrenheit conversion), and the rare linear function is f (x) = (7/5)x + 31.The common version of sorting is sorting in alphabetical order, while the rare variant uses reverse alphabetical order.Error bars show one SE.</p>
<p>reasonably well for the common function yet poorly for the rarer one (Fig. 9), a difference that was statistically significant for all three models (P &lt; 0.01).Llama 3 performed slightly better on the common function than the rare one, but this difference was not significant (P = 0.22), and Gemini 1.0 performed poorly on both functions.</p>
<p>Sorting.We gave models a list of words and told them to sort it into either alphabetical or reverse alphabetical order.Based on an analysis of the C4 corpus, we estimate that alphabetical order is 150 times more common than reverse alphabetical order in Internet text.Paralleling this frequency difference, all five LLMs perform better on alphabetical order than reverse alphabetical order (Fig. 9; P &lt; 10 −5 for all models).</p>
<p>Discussion</p>
<p>Our experiments highlight two scenarios where AI practitioners should be careful about using LLMs.First, we have shown that LLMs perform worse on rare tasks than on common ones, so we should be cautious about applying them to tasks that are rare in pretraining data.Second, we have shown that LLMs perform worse on examples with low-probability answers than ones with high-probability answers, so we should be careful about using LLMs in situations where they might need to produce lowprobability text.Overcoming these limitations is an important target for future work in AI.SI Appendix, section C also describes several other properties of LLMs that relate to the problem they were trained to solve, including sensitivity to wording, difficulty on tasks that depend on meaning (43)(44)(45)(46), and limited compositionality and systematicity (47)(48)(49)(50)(51).</p>
<p>Our experiments were inspired by a teleological analysisan analysis aimed at understanding a system by understanding the problems that it was trained to solve.Our results therefore demonstrate the usefulness of the teleological approach as a way to illuminate important properties of a model.</p>
<p>Understanding a New Type of Intelligence.A range of proposals have been made regarding how to think about LLMs (e.g., refs.[52][53][54].To understand the benefits of the teleological approach, it is helpful to compare it to a more prevalent approach that we call desideratum-focused evaluation, in which the evaluator tests for properties that they wish for models to have, such as the ability to perform certain tasks (e.g., refs.55 and 56) or the avoidance of a particular type of error (e.g., refs.57 and 58).Crucially, this approach is model-agnostic: what we want from a model is not driven by that model's nature.Therefore, the desideratum-driven approach runs the risk of missing modelspecific properties that one would not think to check for based on a generic characterization of the desired behavior.For example, desideratum-focused tests for rot-13 would be likely to miss models' sensitivity to answer probability because probability is not part of how humans usually discuss this task.In contrast, the teleological approach starts with an analysis of the model and is therefore well-suited for capturing the ways in which a model's behavior is influenced by its nature.</p>
<p>An additional benefit of the teleological approach is that it reveals unifying principles behind why systems do what they do (10)(11)(12)(13).Such explanatory principles are useful because they enable us to make general predictions about the sorts of scenarios that models are likely to handle well or poorly.</p>
<p>To be clear, we are not arguing against testing for desideratadoing so answers important questions about whether models are doing what we want.Rather, we are arguing that such testing should be informed by a teleological analysis because this analysis can reveal nonobvious ways that models are likely to deviate from the desired behavior.</p>
<p>Comparing Models to Humans.We have argued that, to understand LLMs, we should approach them on their own terms rather than evaluating them in the same ways that we test humans.But what if your goal is to compare LLMs to humans?Even in that case, we argue that the teleological perspective is important; much as it should inform testing for desiderata, it should also inform comparisons to humans.Specifically, we argue that humanlikeness can be better assessed by the two-step process in (5) than the direct process in (4): (4) Suboptimal approach for investigating human-likeness:</p>
<p>Ask "is this model like a human?" (5) Better approach for investigating human-likeness: a. First, use teleologically motivated experiments to characterize the model in its own terms.b.Then, ask "in what ways is this characterization like a human and unlike a human?"</p>
<p>The direct approach in (4) is suboptimal because the natural way to pursue this path is to evaluate models by using tests developed for humans.Such an evaluation may overestimate similarities to humans because, for many tasks, there is only one way to be correct yet many ways to be incorrect.Thus, to the extent that models are accurate, they will likely be accurate in human-like ways; and to the extent that they make mistakes, a test that is designed for humans may only highlight the sorts of mistakes that humans are liable to make, omitting potential failure modes that are unique to models.The teleological approach in (5) mitigates this risk by evaluating models in a way that is less biased by our view of human cognition.</p>
<p>We have focused on the first step of (5).Without a fair comparison (59-63) that evaluates humans in the same settings, we cannot make strong claims about whether the properties listed in Table 1 make LLMs qualitatively unlike humans.Prior work has shown that some of these properties are present in humans to some extent; e.g., in at least some cases, humans perform better on a task they were trained on than on a similar but new task (64,65), and a human's ease of processing a sentence is influenced by that sentence's probability (66-68)-mirroring the effects of task probability and example probability that we have observed in LLMs.Nonetheless, we suspect that humans are less sensitive to these factors than LLMs, in part because of the diversity of the tasks that humans must perform; that is, although next-word prediction likely plays a role in human cognition (8,69,70), humans are also faced with many other tasks.In particular, we expect that humans are better than LLMs at using abstract algorithms that can be equally well applied across task variants (e.g., across different shifts in a shift cipher); humans can use such strategies by leveraging working memory and explicit reasoning, mechanisms that may be less available to LLMs than to humans (71).If this view is accurate, humans would not show such stark differences across tasks as those we have found for LLMs.</p>
<p>Evaluating Models Fairly: Prompts and Scaling.All of our experiments have used basic prompting, in which models are simply given a query.For many of our tasks, it is likely that performance could be improved by more advanced prompting techniques; e.g., Wei et al.'s chain-of-thought approach (26) substantially improved performance on last-letter concatenation, which is similar to our acronym task.However, it would not invalidate our conclusions if there are conditions in which LLM performance is greater than what we have observed.We do not claim to be highlighting fundamental incapabilities of LLMs but rather are claiming that some tasks and examples are harder for LLMs than others.Therefore, the existence of any setting that yields the predicted performance differences supports our claims, even if other settings exist where LLM performance is at ceiling for both conditions being compared.</p>
<p>As a first step toward investigating other prompting techniques, we used chain-of-thought prompting and step-by-step prompting to evaluate GPT-4 on shift ciphers.We found that these methods can indeed substantially increase performance, but the basic trends that we have identified (sensitivity to task frequency and output probability) still hold, just with higher overall levels of accuracy; see SI Appendix, section D.</p>
<p>A similar conclusion applies to increasing model size.Across almost all tasks, we have observed that GPT-4 substantially outperforms GPT-3.5, showing that increased model scale can provide large benefits on the tasks we have studied.However, GPT-4 still displayed the same qualitative trends as GPT-3.5 (sensitivity to task probability and example probability).Therefore, like prompting techniques, it appears that scaling may improve overall performance but may not fully overcome the ways in which models are influenced by their nature.</p>
<p>We also investigated the effects of few-shot learning, a popular technique for improving LLM performance in which the LLM is given several examples of correct input-output pairs before it is given the input that it must translate to an output.We found that LLMs that underwent few-shot learning continued to be influenced by task frequency and example probability (SI Appendix, section B).We also found that LLM performance was influenced by the nature of the examples that were provided: in some cases, LLMs benefitted more from examples that were similar to the one it had to process than examples that were not.These results support the teleological perspective by suggesting that LLMs are influenced by pressures that arise at all stages of their training: we observed effects attributable to the initial training stage of next-word prediction as well as effects attributable to the task-specific examples used in few-shot learning.In this paper, we have focused on one stage of trainingthe next-word prediction stage-but future work could further investigate other stages such as instruction tuning.</p>
<p>One approach that is likely to perform well on our tasks is to enable LLMs to execute computer programs, since computer code is well suited for handling structured tasks such as those we have focused on (72).Indeed, a recent version of GPT-4 has been augmented with the ability to execute code, and we have anecdotally found that when it utilizes this feature, it can perform much better on our tasks.The fact that LLMs augmented with code execution can score well on our tasks does not invalidate the claims of our paper, because we do not claim that our tasks are impossible for AI.Rather, our point is that we can understand AI systems by reasoning about their nature.The fact that a code-executing system can handle our tasks welland the fact that augmenting with code execution seems to be necessary-supports our main argument: on our tasks, we expect a next-word prediction system to perform poorly, while we expect Python code to perform well.</p>
<p>Our results have focused on ways in which the teleological perspective illuminates shortcomings of LLMs, but being critical of LLMs is not our goal.Instead, our goal is to promote a perspective that accurately captures the properties of LLMs, both positive properties and negative ones.Though in many cases this perspective has led us to point out model weaknesses, in some ways, it makes LLMs more impressive than they would otherwise seem.Once we fully recognize that LLMs are statistical next-word prediction systems, it becomes remarkable that they can perform rot-13 or acronym formation at all, even if they do not perform these tasks perfectly.</p>
<p>Conclusion.</p>
<p>Recent paper titles have made many statements about what language models are:</p>
<p>• "Language Models are Unsupervised Multitask Learners" (15) • "Language Models are Few-Shot Learners" (73) • "Language Models are General-Purpose Interfaces" (74) • "Language Models are Multilingual Chain-of-Thought Reasoners" (75) • "Language Models are Open Knowledge Graphs" (76) We should absolutely recognize these advanced properties.Nonetheless, we should also remember a simpler fact: Language models are...language models!That is, they are statistical next-word prediction systems.This fact has some important consequences: For instance, as we have shown, language models have greater difficulty with infrequent tasks than frequent ones, even when comparing two tasks that seem equally complex to a human; and they have greater difficulty on examples with lowprobability answers than high-probability ones, even when the task is deterministic.Both of these properties-as well as the others discussed above-can be attributed to the way that LLM training focuses on the statistics of word sequences.In sum, to understand what language models are, we must understand what we have trained them to be.</p>
<p>Materials and Methods</p>
<p>Models.We used the most recent time-stamped model versions that were availablewhenwerantheexperiments:gpt-3.5-turbo-0613,gpt-4-0613,claude-3-opus-20240229,gemini-1.0-pro-001,and llama-3-70b-chat-hf.We accessed them through the OpenAI API for the GPT models, the Claude API for Claude 3, the Gemini API for Gemini 1.0, and the together.aiAPI for Llama 3. We used a temperature of 0.0; see SI Appendix, section H.7 for discussion.</p>
<p>Stimuli.For sentence-based tasks, the high-probability stimuli were sentences from Global Voices, a news service that shares its content under a permissive license that allows sharing and modification.Medium-probability sentences were created by taking the high-probability sentences and using RoBERTa (77) to replace some of the words with others that had a low probability in that context (but were still grammatical).Low-probability sentences were created by shuffling the words of the medium-probability examples, except that the first and last words were left in place.</p>
<p>For tasks based on words, the words were drawn from the CMU Pronouncing Dictionary (http://www.speech.cs.cmu.edu/cgi-bin/cmudict).The acronym inputs were lists of 7 words, the sorting input lists varied in length from 10 to 20, and the counting input lists varied from length 1 to 100.For linear functions, inputs were sampled uniformly from the integers from 0 to 999.For multiplication, each input number was sampled uniformly from the integers from 100 to 999.For the birthday task, public figures and their birthdays were obtained from the WikiBio dataset (78).</p>
<p>The inputs to models included a prompt describing the task to be performed, with an example provided for tasks where an example would help to avoid ambiguity (namely, article swapping, reversal, the keyboard cipher, shift ciphers, and Pig Latin).The sample size was 1,000 for each condition in the acronym task, 30 for each number in the counting task, and 100 for each condition in all other tasks.All stimuli are available on the project GitHub (see below).</p>
<p>Statistical Tests.To determine the statistical significance of differences in task variants or example probability, we used logistic regressions where the response variable was 1 if the model produced the correct answer or 0 otherwise.The predictors generally included input probability, output probability, input length, and output length, as well as task variant for conditions where we compared task variants; the predictors other than the one of interest were included to check whether the factors of interest had a significant effect even when potential confounding factors were taken into account.Some predictors were excluded when they were not relevant.See SI Appendix, section I for details of all significance tests.Data, Materials, and Software Availability.All of our materials are publicly available on GitHub (https://github.com/tommccoy1/embers-ofautoregression)(79) with a time-stamped release on Zenodo (https://zenodo.org/records/13763259) (80).</p>
<p>Fig. 3 .
3
Fig. 3.When processing shift ciphers, LLMs are highly sensitive to probability, even though shift ciphers are deterministic.Top Left: Effect of output probability.When decoding text written in the rot-13 cipher, both models score better when the answer is high-probability than when it is low-probability.Top Middle: Effect of input probability.As predicted, input probability does not show as strong an effect as output probability.Top Right: Effect of task probability.Both models score better on a frequently occurring shift cipher (rot-13) than on a rare one (rot-12).All error intervals show one SE.Bottom: Examples for output and input probability; see Fig.2for task probability examples.</p>
<p>Fig.</p>
<p>Fig. The effect of output probability on model accuracy across five tasks.The intervals around the lines show one SE.</p>
<p>Fig.</p>
<p>Fig. The effect of input probability on model accuracy across five tasks.The intervals around the lines show one SE.</p>
<p>Fig. 8 .
8
Fig. 8. Accuracy on Pig Latin variants when encoding (Left) or decoding (Right), as a function of the variant's frequency.The shaded intervals show one SE.</p>
<p>Fig. 9.Comparing performance on common vs. rare versions of tasks.For shift ciphers, the common variant that we illustrate is rot-13, and the rare variant is rot-12.For Pig Latin, the common variant is Pig Latin, and the rare variant is Boar Etruscan (see text).For acronyms, the common variant is acronyms formed from the first letters of words, while the rare variant is acronyms formed from the second letters of words.The common linear function is f (x) = (9/5)x +32 (which is the Celsius-to-Fahrenheit conversion), and the rare linear function is f (x) = (7/5)x + 31.The common version of sorting is sorting in alphabetical order, while the rare variant uses reverse alphabetical order.Error bars show one SE.</p>
<p>Table 2 . The tasks that we used to evaluate LLMs
2
To keep this table manageably sized, some of the examples are not from the datasets that we used to evaluate LLMs but are instead shorter examples of the tasks that those datasets target.
TaskDescriptionExampleArticle swappingSwap each article (a, an,In box the I saw key a. →or the) with theIn the box I saw a key.preceding word.ReversalReverse a sequence ofeveryone! morning,words.Good → Goodmorning, everyone!CountingCount the words orlively news exhibit steepletters in a list.→ 4AcronymsJoin the first letters of theview inch show into traywords in a list.→ VISITLinear functionApply the function328 → 622.4f (x) = (9/5)x + 32.MultiplicationMultiply two three-digit351 times 373 → 130923numbers.SortingSort a list of words ininto, trek, game, magic →alphabetical order.game, into, magic, trekKeyboard cipherReplace each letter withHello world! → Jraapthe one to the right of iteptaf!on a keyboard.Shift cipherDecode by shifting eachFgnl urer! → Stay here!letter 13 positionsbackward in thealphabet.Pig LatinMove the first consonantfrogs aren't noisy. →cluster of each word toogsfray aren'taythe end and add −ay.oisynay.BirthdaysReturn the birth date of aJeremy Lin → August 23,provided public figure.1988</p>
<p>We show that LLMs are sensitive to task probability, a factor that encompasses the default-vs.-counterfactualsituations that Wu et al. study but also includes other cases where there is no default, such as linear functions.As part of our more direct focus on probability, we used corpus analyses to measure task frequency for several of our experiments, a type of analysis that was not used in Wu et al.'s work.Overall, because our work and Wu et al.'s use nonoverlapping sets of tasks and draw compatible conclusions, we view these two papers as mutually reinforcing.
Summary of Task Probability Effects. Across seven pairs of tasks,models performed substantially better on common task variantsthan rare ones. Contemporaneous work by Wu et al. (42) drewsimilar conclusions: they found that LLMs performed better onthe default version of a task (e.g., executing Python code underthe true assumption that Python uses 0-based indexing) than ona counterfactual version of the task (e.g., falsely assuming thatPython uses 1-based indexing). We view our work as highlightinga general phenomenon of which Wu et al.'s conclusion is a specialcase:
of 12 https://doi.org/10.1073/pnas.2322420121 pnas.org
PNAS 2024 Vol. 121 No. 41 e2322420121 https://doi.org/10.1073/pnas.2322420121
of 12 https://doi.org/10.1073/pnas.2322420121 pnas.org
of 12 https://doi.org/10.1073/pnas.2322420121 pnas.org
of 12 https://doi.org/10.1073/pnas.2322420121 pnas.org
of 12 https://doi.org/10.1073/pnas.2322420121 pnas.org
of 12 https://doi.org/10.1073/pnas.2322420121 pnas.org
ACKNOWLEDGMENTS.WethankRobertFrank,TaniaLombrozo,DakotaMcCoy, Benjamin Riley, Paul Smolensky, and Cameron Turner for helpful discussion.Any errors are our own.R.T.M. was supported by the NSF Social, Behavioral, and Economic Sciences Postdoctoral Research Fellowship under Grant No. 2204152, S.Y. was supported by the Harold W. Dodds Fellowship from Princeton, and M.D.H. was supported by the US Department of Defense through a National Defense Science and Engineering Graduate Fellowship.
arXivGPT-4 technical report. 2023. August 202428OpenAIPreprint</p>
<p>Introducing the next generation of Claude. Anthropic, Anthropic. 2024. 28 August 2024Preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, 2023. August 202428</p>
<p>S Bubeck, Sparks of artificial general intelligence: Early experiments with GPT-4. 2023. August 202428</p>
<p>Using cognitive psychology to understand GPT-3. M Binz, E Schulz, Proc. Natl. Acad. Sci. U.S.A. 120e22185231202023</p>
<p>Emergent analogical reasoning in large language models. T Webb, K J Holyoak, H Lu, Nat. Hum. Behav. 72023</p>
<p>Emergent linguistic structure in artificial neural networks trained by self-supervision. C D Manning, K Clark, J Hewitt, U Khandelwal, O Levy, Proc. Natl. Acad. Sci. U.S.A. 1172020</p>
<p>The neural architecture of language: Integrative modeling converges on predictive processing. M Schrimpf, Proc. Natl. Acad. Sci. U.S.A. 118e21056461182021</p>
<p>How do we know how smart AI systems are?. M Mitchell, Science. 38159572023</p>
<p>. D Marr, ( W H Vision, Freeman, 1982</p>
<p>Toward a universal law of generalization for psychological science. R N Shepard, Science. 2371987</p>
<p>J R Anderson, The Adaptive Character of Thought. Erlbaum1990</p>
<p>Understanding human intelligence through human limitations. T L Griffiths, TiCS. 242020</p>
<p>Distributed representations, simple recurrent networks, and grammatical structure. J L Elman, Mach. Learn. 71991</p>
<p>Language models are unsupervised multitask learners. A Radford, 2019. 28 August 2024</p>
<p>Functional explanation and the function of explanation. T Lombrozo, S Carey, Cognition. 992006</p>
<p>Three kinds of adaptationism. P Godfrey-Smith, Adaptationism and Optimality, S. H. Orzack, E. Sober2001Cambridge University Press122</p>
<p>S J Shettleworth, Cognition, Evolution, and Behavior. Oxford University Press2010</p>
<p>The spandrels of San Marco and the Panglossian paradigm: A critique of the adaptationist programme. S Gould, R Lewontin, Proc. R. Soc. London Ser. B. 2051979</p>
<p>Mechanistic versus functional understanding. T Lombrozo, D Wilkenfeld ; Theology, S R Grimm, Varieties of Understanding: New Perspectives from Philosophy, Psychology, and. New York, NYOxford University Press2019</p>
<p>Attention is all you need. A Vaswani, NeurIPS. 302017</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, NeurIPS. 352022</p>
<p>The perils of being bipedal. B Latimer, Ann. Biomed. Eng. 332005</p>
<p>The scars of human evolution. W M Krogman, Sci. Am. 1851951</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, NeurIPS. 352022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, NeurIPS. 352022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, NeurIPS. 362023</p>
<p>Gemini Team, Google , Gemini: A family of highly capable multimodal models. 2023. August 202428</p>
<p>D Groeneveld, Accelerating the science of language models. OLMo2024. August 202428</p>
<p>Neural machine translation of rare words with subword units. R Sennrich, B Haddow, A Birch, Proceedings of ACL. ACL2016</p>
<p>Models in a spelling bee: Language models implicitly learn the character composition of tokens. I Itzhak, O Levy, Proceedings of NAACL. NAACL2022</p>
<p>What do tokens know about their characters and how do they know it. A Kaushal, K Mahowald, Proceedings of NAACL. NAACL2022</p>
<p>Impact of pretraining term frequencies on few-shot numerical reasoning. Y Razeghi, R L Logan, I V , M Gardner, S Singh, Findings of EMNLP. 2022</p>
<p>Large language models struggle to learn long-tail knowledge. N Kandpal, H Deng, A Roberts, E Wallace, C Raffel, ICML (PMLR, 2023). </p>
<p>Pythia: A suite for analyzing large language models across training and scaling. S Biderman, ICML (PMLR, 2023). 202</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, J. Mach. Learn. Res. 212020</p>
<p>The Twelve Caesars. ; R Suetonius, Worthington, 1883Alexander Thomson121Translator</p>
<p>The Codebreakers. D Kahn, 1967Scribner</p>
<p>The Code Book. S Singh, 1999Doubleday</p>
<p>Underdetermination in language games: Survey &amp; analysis of Pig Latin dialects. B Vaux, A I Nevins, 77th Annual Meeting of the LSA. Atlanta, GA2003</p>
<p>The Pile: An 800GB dataset of diverse text for language modeling. L Gao, 2020. August 202428</p>
<p>Reasoning or reciting? Exploring the capabilities and limitations of language models through counterfactual tasks. Z Wu, Proceedings of NAACL. NAACL2024</p>
<p>Dissociating language and thought in large language models. K Mahowald, TiCS. 282024</p>
<p>Entity tracking in language models. N Kim, S Schuster, Proceedings of ACL. ACL2023</p>
<p>The debate over understanding in AI's large language models. M Mitchell, D C Krakauer, Proc. Natl. Acad. Sci. U.S.A. 120e22159071202023</p>
<p>COMPS: Conceptual minimal pair sentences for testing robust property knowledge and its inheritance in pre-trained language models. K Misra, J Rayz, A Ettinger, Proceedings of EACL. EACL2023</p>
<p>Connectionism and cognitive architecture: A critical analysis. J A Fodor, Z W Pylyshyn, Cognition. 281988</p>
<p>Rethinking eliminative connectionism. G F Marcus, Cognit. Psychol. 371998</p>
<p>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. B M Lake, M Baroni, in ICML. 2018</p>
<p>Faith and fate: Limits of transformers on compositionality. N Dziri, NeurIPS. 362023</p>
<p>Comparing humans, GPT-4, and GPT-4v on abstraction and reasoning tasks. M Mitchell, A B Palmarini, A K Moskvichev, AAAI 2024 LLM-CP Workshop. 2023</p>
<p>AI and the everything in the whole wide world benchmark. I D Raji, E M Bender, A Paullada, E Denton, A Hanna, NeurIPS Datasets and Benchmarks Track. 2021</p>
<p>Talking about large language models. M Shanahan, ACM Commun. 672024</p>
<p>Generative models as a complex systems science. A Holtzman, P West, L Zettlemoyer, 2023. August 202428</p>
<p>BIG-bench authors, Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 05/2023. 2023</p>
<p>LMentry: A language model benchmark of elementary language tasks. A Efrat, O Honovich, O Levy, Findings of ACL. 2023</p>
<p>Beyond accuracy: Behavioral testing of NLP models with CheckList. M T Ribeiro, T Wu, C Guestrin, S Singh, Proceedings of ACL. ACL2020</p>
<p>Holistic Evaluation of Language Models. P Liang, 2023TMLR</p>
<p>Are We Smart Enough to Know How Smart Animals Are?. F De Waal, 2016Norton &amp; Co</p>
<p>Performance vs. competence in human-machine comparisons. C Firestone, Proc. Natl. Acad. Sci. U.S.A. 1172020</p>
<p>Can you hear me now? Sensitive comparisons of human and machine perception. M A Lepori, C Firestone, Cognit. Sci. 46e131912022</p>
<p>Can language models handle recursively nested grammatical structures? A case study on comparing models and humans. A K Lampinen, arXiv2022. August 202428Preprint</p>
<p>Language models show human-like content effects on reasoning tasks. I Dasgupta, 2022. August 202428</p>
<p>Mechanization in problem solving: The effect of Einstellung. A S Luchins, Psychol. Monogr. 54951942</p>
<p>Rational simplification and rigidity in human planning. M K Ho, J D Cohen, T L Griffiths, Psychol. Sci. 342023</p>
<p>Contextual effects on word perception and eye movements during reading. S F Ehrlich, K Rayner, J. Verb. Learn. Verb. Behav. 201981</p>
<p>The effect of word predictability on reading time is logarithmic. N J Smith, R Levy, Cognition. 1282013</p>
<p>Rational integration of noisy evidence and prior semantic expectations in sentence interpretation. E Gibson, L Bergen, S T Piantadosi, Proc. Natl. Acad. Sci. U.S.A. 1102013</p>
<p>A probabilistic Earley parser as a psycholinguistic model. J Hale, Proceedings of NAACL. NAACL2001</p>
<p>Expectation-based syntactic comprehension. R Levy, Cognition. 1062008</p>
<p>Probing the psychology of AI models. R Shiffrin, M Mitchell, Proc. Natl. Acad. Sci. U.S.A. 120e23009631202023</p>
<p>From word models to world models: Translating from natural language to the probabilistic language of thought. L Wong, arXiv2023. August 202428Preprint</p>
<p>Language models are few-shot learners. T Brown, NeurIPS. 332020</p>
<p>Language models are general-purpose interfaces. Y Hao, 2022. August 202428</p>
<p>Language models are multilingual chain-of-thought reasoners. F Shi, ICLR. 2023</p>
<p>Language models are open knowledge graphs. C Wang, X Liu, D Song, 2020. August 202428</p>
<p>Y Liu, RoBERTa: A robustly optimized BERT pretraining approach. 2019. August 202428</p>
<p>Neural text generation from structured data with application to the biography domain. R Lebret, D Grangier, M Auli, Proceedings of EMNLP. EMNLP2016</p>
<p>Embers of autoregression. R T Mccoy, 14 September 2024</p>
<p>Embers of autoregression. R T Mccoy, 14 September 2024</p>            </div>
        </div>

    </div>
</body>
</html>
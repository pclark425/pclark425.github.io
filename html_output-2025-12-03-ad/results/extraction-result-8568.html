<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8568 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8568</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8568</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-273234137</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.07432v1.pdf" target="_blank">Can Transformers Reason Logically? A Study in SAT Solving</a></p>
                <p><strong>Paper Abstract:</strong> We theoretically and empirically study the logical reasoning capabilities of LLMs in the context of the Boolean satisfiability (SAT) problem. First, we construct a decoder-only Transformer that can solve SAT using backtracking and deduction via Chain-of-Thought (CoT). We prove its correctness by showing trace equivalence to the well-known DPLL SAT-solving algorithm. Second, to support the implementation of this abstract construction, we design a compiler $\texttt{PARAT}$ that takes as input a procedural specification and outputs a transformer model implementing this specification. Third, rather than $\textit{programming}$ a transformer to reason, we evaluate empirically whether it can be $\textit{trained}$ to do so by learning directly from algorithmic traces ("reasoning paths") of the DPLL algorithm.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8568.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8568.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PARAT-compiled Transformer SAT-solver</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PARAT-compiled Decoder-only Transformer SAT-solver (provable construction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only Transformer model produced by the PARAT compiler that implements a DPLL-like SAT solver via Chain-of-Thought (CoT) generation, performing parallel clause deduction and backtracking and provably deciding 3-SAT up to a fixed number of variables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PARAT-compiled Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A decoder-only Transformer whose weights are produced by the PARAT compiler from a procedural specification of a DPLL-like SAT solving algorithm; uses CoT generation to perform trial-and-error decision, unit propagation, and backtracking; implements parallel clause processing via attention and MLP layers and approximates 'averaging hard attention' with scaled softmax (exactness parameter β).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈5,011,862 parameters (for p=20 compiled instance: 7 layers, 5 heads, 502 embedding dims)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>3-SAT (DIMACS 3-CNF SAT decision; DIMACS(p,c))</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Decide satisfiability (SAT vs UNSAT) of 3-CNF boolean formulas encoded in DIMACS format (up to p variables and c clauses); a strict symbolic logical reasoning / decision problem (NP-complete).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Theoretical construction + weight compilation via PARAT to implement DPLL-style algorithm as Chain-of-Thought: make decisions (assumptions), apply parallel deduction/unit propagation via attention, detect conflicts, and perform backtracking; uses a MEAN/COPY attention pattern approximated by softmax with scaling β to emulate hard attention.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Perfect accuracy on the paper's evaluation datasets for compiled model (evaluated up to p=20 variables and c≤88 clauses); empirical CoT lengths bounded in datasets by approx. 8·p·2^{0.08p} (much lower than worst-case bound p·2^{p+1}).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to learned/fine-tuned Transformer LLMs in the paper: the compiled model achieves perfect, provable correctness up to the compiled p (here p=20) while trained models only generalize within training-length regimes and fail to length-generalize; ablations over soft-attention exactness β show accuracy dependence on β (higher β needed for longer inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Construction requires model size and parameter values that depend on the maximum number of variables p (not length-general); worst-case CoT length is exponential in p (p·2^{p+1}); sensitive to approximation error from soft attention (requires sufficiently large exactness β; paper shows accuracy degrades at low β); not intended to be competitive with production SAT solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Demonstrates decoder-only Transformers can implement strict symbolic logical reasoning (3-SAT decision) via CoT and backtracking and that such behavior can be compiled into concrete weights; shows parallel clause deduction is achievable via attention; highlights a trade-off: provable correctness up to fixed p vs poor length-generalization and exponential worst-case CoT steps; suggests value in integrating compiled reasoning components into LLMs to improve reliable logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Transformers Reason Logically? A Study in SAT Solving', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8568.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8568.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Trained CoT Transformers (LLaMA-like 70M/140M)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trained decoder-only Transformer LLMs (LLaMA-like 70M and 140M) fine-tuned on DPLL Chain-of-Thought traces</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small LLaMA-style decoder Transformers trained on supervised Chain-of-Thought traces produced by a custom DPLL SAT solver; evaluated on SAT vs UNSAT prediction and CoT generation for 3-SAT instances sampled from multiple distributions and variable-count ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-like Transformer variants (70M and 140M architectures used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer architectures (LLaMA-like) used in experiments: 70M configuration: 6 layers, 512 embedding dims, 8 heads, 512 attention hidden dims, 2048 MLP hidden dims; 140M configuration: 12 layers, 768 embedding dims, 12 heads, 768 attention hidden dims, 3072 MLP hidden dims; both used 850 context size and were trained with Adam (5 epochs) on CoT traces generated by a custom SAT solver.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70M and 140M parameter configurations (as described above)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>3-SAT SAT/UNSAT prediction with Chain-of-Thought supervision</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary decision (SAT vs UNSAT) for DIMACS-encoded 3-SAT formulas; models are trained to generate the CoT trace (decisions, unit propagations, backtracks) and final SAT/UNSAT token; datasets included marginal (paired) / random / skewed sampling across variable ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised fine-tuning on algorithmic Chain-of-Thought traces: models are trained to autoregressively produce DPLL-style reasoning traces (assumptions, unit propagation, backtrack tokens) and the final decision; training datasets sampled over variable ranges p∈[6,10] and p∈[11,15] with balanced SAT/UNSAT examples and 5×10^5 samples per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Near-perfect SAT vs UNSAT prediction accuracy when tested on the same number of variables as seen in training (Table 1 reports accuracies generally in the 98.7%–100% range across marginal/random/skewed datasets and both p ranges).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms previously-reported single-pass Transformer behavior (cited Zhang et al. 2023) which fails to generalize across distributions; within this paper, models trained with CoT generalize across input distributions when variable counts are held equal; however, compared to the PARAT-compiled model, trained models lack provable guarantees and fail to generalize in input length/variable-count.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Severe failure to length-generalize: performance degrades drastically when evaluated on formulas with numbers of variables outside the training range (models trained on p∈[6,10] or [11,15] perform poorly on larger/smaller p); learned attention patterns are too 'soft' to generalize to larger problems, and training does not produce an algorithm that is correct for all input lengths; other resource limits (context size, model size) constrain generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Training on Chain-of-Thought DPLL traces enables models to learn general reasoning procedures that generalize across distributions at the same input length, yielding high in-regime accuracy; however, such learning does not yield length-general algorithms—logical reasoning generalization is limited by model architecture/size and the nature of attention (softness), motivating use of compiled/provable reasoning modules or different architectures for length-general logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Transformers Reason Logically? A Study in SAT Solving', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Towards revealing the mystery behind chain of thought: A theoretical perspective <em>(Rating: 2)</em></li>
                <li>Tracr: Compiled Transformers as a Laboratory for Interpretability <em>(Rating: 2)</em></li>
                <li>On the paradox of learning to reason from data <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Looped transformers as programmable computers <em>(Rating: 2)</em></li>
                <li>The expressive power of transformers with chain of thought <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8568",
    "paper_id": "paper-273234137",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "PARAT-compiled Transformer SAT-solver",
            "name_full": "PARAT-compiled Decoder-only Transformer SAT-solver (provable construction)",
            "brief_description": "A decoder-only Transformer model produced by the PARAT compiler that implements a DPLL-like SAT solver via Chain-of-Thought (CoT) generation, performing parallel clause deduction and backtracking and provably deciding 3-SAT up to a fixed number of variables.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PARAT-compiled Transformer",
            "model_description": "A decoder-only Transformer whose weights are produced by the PARAT compiler from a procedural specification of a DPLL-like SAT solving algorithm; uses CoT generation to perform trial-and-error decision, unit propagation, and backtracking; implements parallel clause processing via attention and MLP layers and approximates 'averaging hard attention' with scaled softmax (exactness parameter β).",
            "model_size": "≈5,011,862 parameters (for p=20 compiled instance: 7 layers, 5 heads, 502 embedding dims)",
            "reasoning_task_name": "3-SAT (DIMACS 3-CNF SAT decision; DIMACS(p,c))",
            "reasoning_task_description": "Decide satisfiability (SAT vs UNSAT) of 3-CNF boolean formulas encoded in DIMACS format (up to p variables and c clauses); a strict symbolic logical reasoning / decision problem (NP-complete).",
            "method_or_approach": "Theoretical construction + weight compilation via PARAT to implement DPLL-style algorithm as Chain-of-Thought: make decisions (assumptions), apply parallel deduction/unit propagation via attention, detect conflicts, and perform backtracking; uses a MEAN/COPY attention pattern approximated by softmax with scaling β to emulate hard attention.",
            "performance": "Perfect accuracy on the paper's evaluation datasets for compiled model (evaluated up to p=20 variables and c≤88 clauses); empirical CoT lengths bounded in datasets by approx. 8·p·2^{0.08p} (much lower than worst-case bound p·2^{p+1}).",
            "baseline_comparison": "Compared to learned/fine-tuned Transformer LLMs in the paper: the compiled model achieves perfect, provable correctness up to the compiled p (here p=20) while trained models only generalize within training-length regimes and fail to length-generalize; ablations over soft-attention exactness β show accuracy dependence on β (higher β needed for longer inputs).",
            "limitations_or_failures": "Construction requires model size and parameter values that depend on the maximum number of variables p (not length-general); worst-case CoT length is exponential in p (p·2^{p+1}); sensitive to approximation error from soft attention (requires sufficiently large exactness β; paper shows accuracy degrades at low β); not intended to be competitive with production SAT solvers.",
            "insights_or_conclusions": "Demonstrates decoder-only Transformers can implement strict symbolic logical reasoning (3-SAT decision) via CoT and backtracking and that such behavior can be compiled into concrete weights; shows parallel clause deduction is achievable via attention; highlights a trade-off: provable correctness up to fixed p vs poor length-generalization and exponential worst-case CoT steps; suggests value in integrating compiled reasoning components into LLMs to improve reliable logical reasoning.",
            "uuid": "e8568.0",
            "source_info": {
                "paper_title": "Can Transformers Reason Logically? A Study in SAT Solving",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Trained CoT Transformers (LLaMA-like 70M/140M)",
            "name_full": "Trained decoder-only Transformer LLMs (LLaMA-like 70M and 140M) fine-tuned on DPLL Chain-of-Thought traces",
            "brief_description": "Small LLaMA-style decoder Transformers trained on supervised Chain-of-Thought traces produced by a custom DPLL SAT solver; evaluated on SAT vs UNSAT prediction and CoT generation for 3-SAT instances sampled from multiple distributions and variable-count ranges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-like Transformer variants (70M and 140M architectures used in experiments)",
            "model_description": "Decoder-only Transformer architectures (LLaMA-like) used in experiments: 70M configuration: 6 layers, 512 embedding dims, 8 heads, 512 attention hidden dims, 2048 MLP hidden dims; 140M configuration: 12 layers, 768 embedding dims, 12 heads, 768 attention hidden dims, 3072 MLP hidden dims; both used 850 context size and were trained with Adam (5 epochs) on CoT traces generated by a custom SAT solver.",
            "model_size": "70M and 140M parameter configurations (as described above)",
            "reasoning_task_name": "3-SAT SAT/UNSAT prediction with Chain-of-Thought supervision",
            "reasoning_task_description": "Binary decision (SAT vs UNSAT) for DIMACS-encoded 3-SAT formulas; models are trained to generate the CoT trace (decisions, unit propagations, backtracks) and final SAT/UNSAT token; datasets included marginal (paired) / random / skewed sampling across variable ranges.",
            "method_or_approach": "Supervised fine-tuning on algorithmic Chain-of-Thought traces: models are trained to autoregressively produce DPLL-style reasoning traces (assumptions, unit propagation, backtrack tokens) and the final decision; training datasets sampled over variable ranges p∈[6,10] and p∈[11,15] with balanced SAT/UNSAT examples and 5×10^5 samples per dataset.",
            "performance": "Near-perfect SAT vs UNSAT prediction accuracy when tested on the same number of variables as seen in training (Table 1 reports accuracies generally in the 98.7%–100% range across marginal/random/skewed datasets and both p ranges).",
            "baseline_comparison": "Outperforms previously-reported single-pass Transformer behavior (cited Zhang et al. 2023) which fails to generalize across distributions; within this paper, models trained with CoT generalize across input distributions when variable counts are held equal; however, compared to the PARAT-compiled model, trained models lack provable guarantees and fail to generalize in input length/variable-count.",
            "limitations_or_failures": "Severe failure to length-generalize: performance degrades drastically when evaluated on formulas with numbers of variables outside the training range (models trained on p∈[6,10] or [11,15] perform poorly on larger/smaller p); learned attention patterns are too 'soft' to generalize to larger problems, and training does not produce an algorithm that is correct for all input lengths; other resource limits (context size, model size) constrain generalization.",
            "insights_or_conclusions": "Training on Chain-of-Thought DPLL traces enables models to learn general reasoning procedures that generalize across distributions at the same input length, yielding high in-regime accuracy; however, such learning does not yield length-general algorithms—logical reasoning generalization is limited by model architecture/size and the nature of attention (softness), motivating use of compiled/provable reasoning modules or different architectures for length-general logical reasoning.",
            "uuid": "e8568.1",
            "source_info": {
                "paper_title": "Can Transformers Reason Logically? A Study in SAT Solving",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Towards revealing the mystery behind chain of thought: A theoretical perspective",
            "rating": 2,
            "sanitized_title": "towards_revealing_the_mystery_behind_chain_of_thought_a_theoretical_perspective"
        },
        {
            "paper_title": "Tracr: Compiled Transformers as a Laboratory for Interpretability",
            "rating": 2,
            "sanitized_title": "tracr_compiled_transformers_as_a_laboratory_for_interpretability"
        },
        {
            "paper_title": "On the paradox of learning to reason from data",
            "rating": 2,
            "sanitized_title": "on_the_paradox_of_learning_to_reason_from_data"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Looped transformers as programmable computers",
            "rating": 2,
            "sanitized_title": "looped_transformers_as_programmable_computers"
        },
        {
            "paper_title": "The expressive power of transformers with chain of thought",
            "rating": 1,
            "sanitized_title": "the_expressive_power_of_transformers_with_chain_of_thought"
        }
    ],
    "cost": 0.013617250000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CAN TRANSFORMERS REASON LOGICALLY? A STUDY IN SAT SOLVING
9 Oct 2024</p>
<p>Leyan Pan leyanpan@gatech.edu 
Georgia Institute of Technology
30332AtlantaGAUSA</p>
<p>Vijay Ganesh vganesh@gatech.edu 
Georgia Institute of Technology
30332AtlantaGAUSA</p>
<p>Jacob Abernethy 
Georgia Institute of Technology
30332AtlantaGAUSA</p>
<p>Google Research</p>
<p>Chris Esposo cesposo3@gatech.edu 
Georgia Institute of Technology
30332AtlantaGAUSA</p>
<p>Wenke Lee 
Georgia Institute of Technology
30332AtlantaGAUSA</p>
<p>CAN TRANSFORMERS REASON LOGICALLY? A STUDY IN SAT SOLVING
9 Oct 2024792644CA67E18509EC667EBF6467DA2DarXiv:2410.07432v1[cs.LG]
We theoretically and empirically study the logical reasoning capabilities of LLMs in the context of the Boolean satisfiability (SAT) problem.First, we construct a decoder-only Transformer that can solve SAT using backtracking and deduction via Chain-of-Thought (CoT).We prove its correctness by showing trace equivalence to the well-known DPLL SAT-solving algorithm.Second, to support the implementation of this abstract construction, we design a compiler PARAT that takes as input a procedural specification and outputs a transformer model implementing this specification.Third, rather than programming a transformer to reason, we evaluate empirically whether it can be trained to do so by learning directly from algorithmic traces ("reasoning paths") of the DPLL algorithm.</p>
<p>INTRODUCTION</p>
<p>Transformer-based Language Models (LLMs, Vaswani et al. (2017)) have demonstrated remarkable success in a wide range of tasks framed in natural language, especially when using prompting techniques such as Chain-of-Thought (CoT, Wei et al. (2022)).On the other hand, even the most advanced LLMs face challenges in reliable multi-step reasoning, frequently hallucinating towards nonsensical conclusions (Kambhampati et al. (2024)).Evaluating progress on logical deduction in language models remains an ongoing challenge as researchers have continued to disagree on even a reasonable definition of what constitutes "reasoning."</p>
<p>This paper focuses on the question of LLM reasoning capability in what we believe is the simplest and most mathematically precise setting: the Boolean satisfiability problem (SAT, Cook (1971)).SAT problems provide an excellent starting point for studying the reasoning ability of LLMs given that (a) natural language often encodes Boolean logic, and (b) we already have many useful algorithms that implement logical deduction to solve SAT problems Biere et al. (2009).Notably, notwithstanding the NP-completeness of SAT, humans implicitly solve simple boolean satisfaction problems in their daily lives; scheduling a multi-person meeting across time zones, for example.</p>
<p>In this work we aim to rigorously investigate Transformers' multi-step reasoning and backtracking capability in solving formal logical reasoning problems, and we demonstrate through a theoretical construction that decoder-only Transformers can reliably decide SAT instances.Theorem 1.1 (Informal version of Theorem 4.5).For any p, c ∈ N + , there exist a decoder-only Transformer with O(p 2 ) parameters that can decide all 3-SAT instances of at most p variables and c clauses using Chain-of-Thought reasoning.</p>
<p>To investigate the properties of our construction empirically, we design a compiler that converts computational graphs of abstract sequence operations used in our construction into Transformer model weights.We implemented the construction in PyTorch and empirically validated its correctness on random 3-SAT instances.We also investigated its empirical properties such as the number of generated CoT tokens.</p>
<p>Additionally, we perform training experiments to demonstrate that Transformers can effectively learn from deductive reasoning and the backtracking process of the DPLL algorithm encoded as
(¬x2 ∨ ¬x4 ∨ ¬x1) ∧ (x3 ∨ x4 ∨ ¬x1) ∧ (¬x1 ∨ ¬x3 ∨ ¬x2) ∧ (x1 ∨ ¬x2 ∨ ¬x4) ∧ (¬x4 ∨ x2 ∨ x1) ∧ (x1 ∨ ¬x2 ∨ x4)
Transformer Chain-of-Thought from Theorem 4.5 Chain-of-Thought.We show that Transformers equipped with CoT can generalize between SAT instances generated from different distributions within the same number of variables p.However, LLMs trained on SAT instances with CoT still struggle to solve instances with unseen number of variables, demonstrating challenges in learning length-generalizable reasoning and opportunities to incorporate compiled reasoning components in Transformer LLMs to improve reasoning capabilities.</p>
<p>Contributions We prove by theoretical construction that decoder-only Transformers can solve 3-SAT, a fundamental NP-Complete logical reasoning problem, by performing logical deduction and backtracking using Chain-of-Thought (CoT).We show that Transformers can perform logical deduction on all conditions (clauses) in parallel instead of checking each condition sequentially.Nevertheless, the construction requires exponentially many CoT steps in the worst case, although it is much faster on most typical examples.</p>
<p>We design PARAT, a compiler of high-level sequence operations written in Numpy-like syntax into Transformer model weights, to empirically validate and analyze theoretical constructions of Transformer algorithms.</p>
<p>We empirically demonstrate that the compiled SAT-solver model can solve SAT formulas up to 20 propositions and 88 clauses with perfect accuracy.Note that our goal is not to compete with modern state-of-the-art SAT solvers.Rather, we answer a fundamental question about whether LLMs can perform propositional reasoning.Finally, our training experiments suggest that Chain-of-Thought allows Transformer-LLMs to achieve out-of-distribution generalization for the same input lengths.</p>
<p>RELATED WORK</p>
<p>Theoretical Expressiveness of Transformers and Chain-of-Thought (CoT): Owing to the empirical success of Transformer-based models, many researchers have investigated the capabilities of the Transformer architecture from a theoretical perspective.This line of research focuses on what types of computation can Transformer models simulate by providing theoretical constructions of Transformer models with idealized assumptions.The seminal work of Liu et al. (2023) showed that Transformers can simulate automata using a single pass over only a logarithmic number of layers w.r.t. the number of states.Yao et al. (2021) demonstrated that transformers can perform parentheses matching of at most k types of parentheses and D appearance of each (Dyck k,D ) with D + 1 layers.</p>
<p>However, the computation power of one pass of the Transformer model is fundamentally limited (Merrill &amp; Sabharwal (2023)), and the success of Chain-of-Thought (CoT) reasoning (Wei et al. (2022)) has sparked more recent research on how CoT can improve upon the expressiveness of Transformer models.Pérez et al. (2019) proved that Transformers can emulate the execution of single-tape Turing machines if each output vector is appended to the input vector sequence at the next iteration.Giannou et al. (2023) showed that Transformers can recurrently simulate arbitrary programs written in a one-instruction-set language if the output vector at every position of the Transformer is passed as input to the model at the next iteration.Li et al. (2024) proved that Transformers can simulate arbitrary boolean circuits using CoT by representing the circuit in the positional encoding and is commonly perceived to have shown that Transformers with CoT can "solve all problems".In particular, transformers can decide all problems in P/poly ⊇ P with polynomial steps of CoT.Merrill &amp; Sabharwal (2024) showed that Transformers with averaging hard attention can decide all regular languages with a linear number of CoT tokens and decide all problems in P with a polynomial number of CoT tokens.Feng et al. (2023) shows that Transformer CoT can perform integer arithmetic, solve linear equations, and perform dynamic programming for the longest increasing subsequence and edit distance problems.These seminal works profoundly advanced our understanding of the capabilities of Transformer models from a theoretical perspective.</p>
<p>How our work differs from the above-mentioned results: Many of the above papers are focused on problems in P or P/poly, while 3-SAT is an NP-complete problem.It is widely believed that P is a strict subset of NP, and it is not known whether NP is a subset of P/poly.In other words, our results are not comparable to these earlier results.Step, our construction performs deductive reasoning over the full input in parallel while any single-tape TM must process each input token sequentially.Furthermore, the CoT produced by our theoretical construction abstractly represents the human reasoning process of trial and error, as demonstrated in Figure 1.</p>
<p>Compilation of Transformer Weights.Further, prior work on the theoretical construction of Transformer models rarely provide practical implementations.Notably, Giannou et al. (2023) provide an implementation of their construction and demonstrate its execution on several programs.However, the model is initialized "manually" using prolonged sequences of array assignments, limiting its extensibility to other theoretical frameworks.</p>
<p>More recently, Lindner et al. (2023) released Tracr, which compiles RASP (Weiss et al. (2021)) programs into decoder-only Transformer models.The "Restricted Access Sequence Processing Language" (RASP, Weiss et al. (2021)) is a human-readable representation of a subset of operations that Transformers can perform via self-attention and MLP layers.In our preliminary attempt to implement a SAT solver model with Tracr, we identified several implementation inconveniences and limitations of Tracr when scaling to more complex algorithms, which motivated the development of our compiler.In particular: (1) Every "variable" (termed sop in Lindner et al. (2023)) in Tracr must be either a one-hot categorical encoding or a single numerical value.This constraint makes representing more complex vector structures highly inconvenient.Furthermore, each select operation (i.e., self-attention) accepts only a single sop as the query and key vectors, whereas our theoretical construction often requires incorporating multiple sops as queries and keys.</p>
<p>(2) Tracr represents position indices and many other discrete sops with a one-hot encoding, allocating a residual stream dimension for each possible value of the sop.In particular, compiling models with a context length of n requires O(n) additional embedding dimensions for each SOp that represents a position index.</p>
<p>(3) For each binary operation between one-hot encoded sops (such as position indices), Tracr creates an MLP layer that first creates a lookup table of all possible value combinations of the input sops.This results in an MLP layer of O(n 3 ) parameters.</p>
<p>PRELIMINARIES</p>
<p>The Boolean satisfiability problem (SAT) is the problem of determining whether there exists an assignment A of the variables in a Boolean formula F such that F is true under A. In this paper we only consider 3-SAT instances in conjunctive normal form (CNF), where groups of at most 3 variables and their negations (literals) can be joined by OR operators into clauses, and these clauses can then be joined by AND operators.In our implementations we use the well-known DIMACS encoding for CNF formulae whereby each literal is converted to a positive or negative integer corresponding to its index, and clauses are separated by a 0 .</p>
<p>Token Embedding and Positional Encoding</p>
<p>Each input token s i is converted into a continuous vector representation Embed(s i ) ∈ R d using a fixed embedding map Embed(•).To incorporate positional information, a positional encoding vector p i ∈ R d is added to each token embedding.The initial input to the first Transformer block is
x (0) ← (Embed(s 1 ) + p 1 , Embed(s 2 ) + p 2 , . . . , Embed(s n ) + p n ) ∈ R n×d .
Transformer Blocks.For l = 1, . . ., L, each block l of the transformer processes an embedded sequence x (l−1) ∈ R n×d to produce another embedded sequence x (l) ∈ R n×d .Each block consists of a multi-head self-attention (MHA) mechanism and a position-wise feed-forward network (MLP).</p>
<p>We have a set of parameter tensors that includes MLP parameters O ∈ R (Hd h )×demb .We will collectively refer to all such parameters at layer l as Γ (l) , whereas the self-attention parameters for attention head h at layer l will be referred to as Γ (l,h) .We can now process the embedded sequence x (l−1) to obtain x (l) in two stages: h (l) ← x (l−1) + MHA x (l−1) ; Γ (l) , and x (l) ← h (l) + MLP h (l) ; Γ (l) ,
W (l) 1 ∈ R demb×d * mlp , b (l) 1 ∈ R d * mlp , W (l) 2 ∈ R dmlp×d , and b (l) 2 ∈ R d , self-attention parameters W (l,h) Q , W (l,h) K , W (l,h) V ∈ R d×d
where
MHA x; Γ (l) := Concat Attention(x; Γ (l,1) ), . . . , Attention(x; Γ (l,H) ) W (l) O Attention(x; Γ (l,h) ) := softmax d −1/2 h xW (l,h) Q (W (l,h) K x) ⊤ + M xW (l,h) V MLP h; Γ (l) := σ hW (l) 1 + b (l) 1 W (l) 2 + b (l) 2 .
The n × n matrix M is used as a "mask" to ensure self-attention is only backward looking, so we set M [i, j] = ∞ for i ≥ j and M [i, j] = 0 otherwise.Finally, we use the ReGLU(•) : R 2dmlp → R dmlp activation function σ(•) at each position.Tiven input u ∈ R n×2dmlp , for each position i we split u i into two halves u i,1 , u i,2 ∈ R d and, using ⊗ denotes element-wise multiplication, we define
σ ReGLU (u i ) = u i,1 ⊗ ReLU (u i,2 ) .
(1)</p>
<p>Output Layer</p>
<p>After the final Transformer block, the output representations are projected onto the vocabulary space, and the output vector at the final position is converted to a probability distribution over the vocabulary via the softmax function:
Algorithm 1: Greedy Decoding Input: Model M : V * → ∆(V),end o = x (L) W out + b out ∈ R n×V , p n+1 = softmax(λo n ) ∈ ∆(V)(2)
where
W out ∈ R d×V , b out ∈ R V , V
is the size of the vocabulary, o n ∈ R V is the output vector at the last position, and λ ∈ R is the temperature.</p>
<p>Autoregressive Decoding and Chain-of-Thought During generation, the Transformer model is repeatedly invoked to generate the next token and appended to the input tokens.When using the greedy seconding strategy, the highest probability token is always selected and the generation process is described in Algorithm 1.</p>
<p>In probabilistic decoding, the next token is instead selected based on the probability distribution s t+1 ∼ p t+1 (v) using temperature λ in Equation (2).In this paper, we refer to the full generated sequence of tokens as the Chain-of-Thought, and the number of chain-of-thought tokens in Algorithm 1 is t − n.</p>
<p>TRANSFORMERS AND SAT: LOGICAL DEDUCTION AND BACKTRACKING</p>
<p>This section presents and explains our main results on Transformers' capability in deductive reasoning and backtracking with CoT.To rigorously state our results, we first formally define decision problems, decision procedures, and what it means for a model to "solve" a decision problem using CoT: Definition 4.1 (Decision Problem).Let V be a vocabulary, Σ ⊆ V be an alphabet, L ⊆ Σ * be a set of valid input strings.We say that a mapping f : L → {0, 1} is a decision problem defined on L. Definition 4.2 (Decision Procedure).We say that an algorithm A is a decision procedure for the decision problem f , if given any input string x from L, A outputs 1 if f (x) = 1, and 0 otherwise.Definition 4.3 (Autoregressive Decision Procedure).For any map M : V * → ∆(V), which we refer to as an auto-regressive next-token prediction model, and E = {E 0 , E 1 } ⊂ V, define decision procedure A M,E as follows: For any input s 1:n , run Algorithm 1 with stop tokens E. A M,E outputs 0 if s 1:t ends with E 0 and A M,E output 1 otherwise.We say M autoregressively decides decision problem f if there is some E ⊂ V for which A M,E decides f .Remarks on Theorem 4.5</p>
<p>• The upper bound on the CoT length p • 2 p+1 is a worst-case upper bound which assumes that the model is unable to make any logical deductions have to try all 2 p assignments.However, this upper bound is never reached in practice, and in Figure 3 we show that the number of CoT tokens is no greater than 8p • 2 0.08p for most formulas.If the number of backtracking steps is bounded by T then the CoT is no longer than (2p + 1)(T + 1)</p>
<p>• The worst-case CoT length is independent of the number of clauses c, which is due to the parallel deduction over all clauses within the Transformer construction.Otherwise, sequentially processing each clause would take at least c • 2 O(p) number of steps.</p>
<p>• Positional encodings are not included in the number of parameters.The positional encoding at position i is the numerical value i at a particular dimension.</p>
<p>• Each parameter can be represented with O(p + log c) bits</p>
<p>We show our full proof via trace equivalence with DPLL in Appendix C. The construction uses adapted versions of lemmas from Feng et al. (2023) as basic building blocks.</p>
<p>Here we describe our design of the algorithm and CoT process for SAT-Solving and a high-level description on the internal mechanisms of the Transformer model.</p>
<p>Logical Deduction and Backtracking in CoT.In Figure 1, we illustrate the Chain-of-Thought of our model when solving SAT instances.The model performs SAT-solving through trial-anderror reasoning by iteratively exploring different solution paths while "learning" lessons from failed attempts, inspired by the "abstract DPLL" algorithm (Nieuwenhuis et al. (2005)).Specifically, at each step, the model: (1) attempts a solving direction by assigning a truth value to a variable; (2) deduces new variable assignments based on the clauses and current assignments; and (3) backtracks and negates the last assumed assignment if a conflict arises.The chain of thought keeps track of the current "assignment trace", which includes the order of current assignments and whether each assignment is an assumption or a deduction.This iterative process continues until a satisfying assignment is found, which mirrors the human problem-solving process of trying different possible solutions before reaching an answer.</p>
<p>Parallel Processing of Constraints.In our construction, we demonstrate that Transformers can process all clauses in parallel when checking for satisfaction, conflicts, and finding possible deductions.</p>
<p>Essentially, each clause in the input formula as well as the current state can be "summarized" as a single binary vector, and the query and key transformations ensure that the dot product between the current state (assignment) and each clause provides information on whether deduction is possible between them.The attention operation averages over clauses where deductions are possible and the following MLP layer filters out the result of deductions.</p>
<p>COMPILER FOR COMPLEX TRANSFORMER ALGORITHMS</p>
<p>In the previous section, we presented a theoretical construction of a Transformer capable of solving SAT instances through backtracking and parallel deduction.However, relying solely on theorems and proofs can make it challenging to gain practical insights and verify correctness.To address this, we introduce ParametricTransformer (PARAT), which provides a framework for converting theoretical constructions of Transformers into practical models to facilitate empirical analysis and validation.</p>
<p>SUPPORTED FEATURES AND OPERATIONS</p>
<p>Our compiler is designed to provide an intuitive syntax resembling standard numerical array manipulation, akin to NumPy, while supporting a diverse and extensible set of abstract operations.PARAT is capable of implementing</p>
<p>• NumPy-like Array Syntax for indexing, arithmetic, and comparison.</p>
<p>• Multi-Level Abstraction to enable low-level customization.</p>
<p>• Multi-stage Evaluation Mechanisms to facilitate debugging and error localization</p>
<p>• High Extensibility through structured class inheritance, promoting the addition of new features and operations.</p>
<p>Each intermediate "variable" is an instance of the SOp base class (name adapted from Lindner et al. ( 2023)), and each instance sop of SOp is assigned a dimension d sop ∈ N + and can be viewed as an abstract representation of an R n×dsop array where n is the number of tokens in the input to the Transformer model.A PARAT "program" is basically a sequence of array operations over SOps.</p>
<p>Throughout this section, we refer to the indices along the first dimension of an SOp as "position" and refer to indices along the second dimension as "dimension".</p>
<p>The "inputs" to a program are arbitrary positional encoding and token embedding SOps, represented by the base class names PosEncSOp and TokEmbSOp respectively.For example, the OneHotTokEmb class represents the one-hot embedding of tokens and Indices represents the numerical value of the index of each position.</p>
<p>The rest of the program performs various operations that compute new SOps based on existing ones.We provide implementations of basic building block operations including (but not limited to) the following:</p>
<p>• Mean(q, k, v) Represents the "Averaging Hard Attention" operation.At each position i, this operation identifies all target positions j with the maximal value of q ⊤ i k j for j ≤ i and computes the average of the corresponding v j values.</p>
<p>• sop[idx, :] Performs indexing using a one-dimensional index array idx, producing an SOp out such that out We present our full code implementing our construction for Theorem 4.5 using PARAT in Appendix D.
[i, j] = sop[idx[i], j] for i ∈ [n]</p>
<p>THE COMPILATION PROCESS</p>
<p>PARAT takes in a SOp that contains the computational graph of the algorithm and outputs a PyTorch (Paszke et al. (2017)) model.The compilation process follows stages similar to those of Tracr:</p>
<ol>
<li>Computational Graph Construction: When a user writes sop operations, each operation automatically creates a dependency tree of all operations required for computing the resulting sop value.2. Reduction to Base Operations: Each sop operation is reduced to one of 5 base classes:</li>
</ol>
<p>SelfAttention for operation that requires information from other token positions, GLUMLP for non-linear local operations, Linear for linear local operations, PosEncSOp for positional encodings, or TokEmbSOp for token embeddings.Sequential Linear operations are reduced to a single operation through matrix multiplication and dependency merging.</p>
<p>Allocation of Layers and Residual Stream:</p>
<p>The computational graph is topologically sorted such that each sop appears later than its dependencies.This sorting is then used to assign SelfAttention and GLUMLP sops to Transformer layer numbers that comply with dependency constraints.Furthermore, each non-Linear sop is also allocated a portion of the residual stream equal to their d sop size.4. Model Construction and Weight Assignment: A PyTorch model is initialized based on the number of required layers, hidden size, and embedding size inferred from the previous steps.The computed weights for each sop are assigned to different model components based on their types.Notably, each SelfAttention sop corresponds to an attention head, and each GLUMLP sops corresponds to part of a MLP layer with ReGLU activation.</p>
<p>Soft vs Hard Attention</p>
<p>The reduction of Mean to SelfAttention induces inevitable numerical errors due to Mean representing averaging a strict subset of previous positions while SelfAttention computes a weighted average over all previous positions via softmax.This error also affects other operations based on Mean such as position indexing.We control this error via an "exactness" parameter β that scales the attention logits, and Lemma C.6 shows that the error decreases exponentially w.r.t.β.</p>
<p>Multi-Stage Evaluation</p>
<p>To facilitate debugging, PARAT allows 3 types of evaluations for every sop at different stages of compilation.</p>
<p>• sop.abstract eval(tokens) evaluates sop on a sequence of input tokens without any numerical errors.This can be used to validate the correctness of the algorithm implementation as sop operations.• sop.concrete eval(tokens) evaluates sop on an input sequence after reducing to the base classes at step 2 of the compilation process.This helps localize errors stemming from incorrect reduction of high-level operations to base classes.• Model evaluation This corresponds to evaluating the Pytorch model after the full compilation process.</p>
<p>ANALYSIS OF THE COMPILED SAT-SOLVING MODEL</p>
<p>With PARAT, we successfully compiled our theoretical construction in Theorem 4.5 using the code in Appendix D. For p = 20 number of variables, the resulting Transformer has 7 layers, 5 attention heads, 502 embedding dimensions, and 5011862 parameters.With a concrete implementation of our theoretical construction in PyTorch, we empirically investigate 3 questions (1) Does the compiled model correctly decide SAT instances?(2) How many steps does the model take to solve actual 3-SAT instances?(3) How does error induced by soft attention affect reasoning accuracy?These questions reveal further insights that are not available by observing the theoretical constructions alone and demonstrate the additional values provided by PARAT.</p>
<p>Evaluation Datasets We evaluate our models on randomly sampled DIMACS encoding of 3-SAT formulas.We focus on SAT formulas with exactly 3 literals in each clause, with the number of clauses c between 4.1p and 4.4p, where p is the number of variables.</p>
<p>It is well-known that the satisfiability of such random 3-SAT formulas highly depends on the clause/variable ratio, where a formula is very likely satisfiable if c/p ≪ 4.26 and unsatisfiable if c/p ≫ 4.26 (Crawford &amp; Auton (1996)).This potentially allows a model to obtain high accuracy just by observing the statistical properties such as the c/p ratio.To address this, we constrain this ratio for all formulas to be near the critical ratio 4.26.Furthermore, our "marginal" datasets contain pairs of SAT vs UNSAT formulas that differ from each other by only a single literal.This means that the SAT and UNSAT formulas in the dataset have almost no statistical difference in terms of c/p ratio, variable distribution, etc., ruling out the possibility of obtaining SAT vs UNSAT information solely via statistical properties.</p>
<p>We also use 3 different sampling methods to generate formulas of different solving difficulties to evaluate our model:</p>
<p>• Marginal: Composed of pairs of formulas that differ by only one token.</p>
<p>• Random: Formulas are not paired by differing tokens and each clause is randomly generated.</p>
<p>• Skewed: Formulas where polarity and variable sampling are not uniform; For each literal, one polarity is preferred over the other.Some literals are also preferred over others.</p>
<p>We generate the above 3 datasets for each variable number 4 ≤ p ≤ 20, resulting in 51 total datasets of 2000 samples each.Each sample with p variables contains 16.4p to 17.6p input tokens, which is at least 320 for p = 20.</p>
<p>Model Unless otherwise stated, the model we experiment with is compiled from the code in D using PARAT with max number of variables p = 20, max number of clauses c = 88, and exactness parameter β = 20.The model uses greedy decoding during generation.</p>
<p>Accuracy Our compiled model achieves perfect accuracy on all evaluation datasets described above.This provides empirical justification for our theoretical construction for Theorem 4.5 as well as PARAT.This result is included in Figure 2 to compare with trained models.</p>
<p>How many steps?</p>
<p>We perform experiments to measure the empirical Chain-of-Thought length required for solving SAT formulas of different sizes.For all formulas we evaluated, the maximum CoT length is bounded by 8p•2 0.08p , which is significantly less than the theoretical bound of p•2 (p+1) .This indicates that the model can use deduction to reduce the search space significantly.The figure illustrating the results is in Appendix Figure 3.</p>
<p>Effect of Soft Attention</p>
<p>In our previous evaluations, we used a sufficiently large "exactness" value β to ensure that the error from MEAN based operations does not affect the final output of greedy sampling.The use of "Averaging Hard Attention" is prevalent in previous works on theoretical construction.However, how exactly does soft-attention affect the final reasoning output?</p>
<p>In Figure 4 we present the SAT/UNSAT prediction accuracy for models under 8 different "mean exactness" β values on our "marginal" datasets ranging from 2.5 to 20.Recall that β controls how the well soft attention approximates "hard" attention in each self-attention layer.Our results demonstrate that longer inputs generally require larger β values to achieve high accuracy.This may explain why Transformers fail to learn generalizable algorithmic procedures, as the attention learned on smaller formulas may be too "soft" to generalize to larger inputs.</p>
<p>6 CAN TRANSFORMER LEARN SAT SOLVING FROM DATA?</p>
<p>Our previous sections showed that Transformer and weights exist for solving SAT instances using CoT with backtracking and deduction.However, it is unclear to what extent Transformers can learn such formal reasoning procedures by training on SAT formulas.Previously, Zhang et al. (2023) showed that when using a single pass of a Transformer model (without CoT), Transformers fail to generalize to logical puzzles sampled from different distributions even when they have the same number of propositions.</p>
<p>This section provides proof-of-concept evidence that training on the Chain-of-Thought procedure with deduction and backtracking described in Figure 1 can facilitate Out-of-Distribution generalization within the same number of variables.</p>
<p>Datasets In Section 5.3 we introduced 3 different distributions over random 3-SAT formulas of varying difficulties.For training data, we use the same sampling methods, but instead of having a separate dataset for each variable number p, we pick 2 ranges p ∈ [6, 10] and p ∈ [11,15], where for each sample a random p value is picked uniformly random from the range.Each formula with p variables contains 16.4p to 17.6p tokens.This results in 2 × 3 training datasets, each containing 5 × 10 5 training samples1 , with balanced SAT vs UNSAT samples.For each formula, we generate the corresponding chain of thought in the same format as Figure 1 using a custom SAT Solver.The evaluation data is exactly the same as Section 5.3.[11,15] for both train and test datasets).</p>
<p>Model and Training</p>
<p>As shown in Table 1, our trained models achieve near-perfect SAT vs UNSAT prediction accuracy when tested on the same number of variables as the training data, even when on formulas sampled from different distributions.Recall that the "marginal" dataset has SAT vs UNSAT samples differing by a single token (out of at least 16p tokens in the input formula), which minimizes statistical evidence that can be used for SAT/UNSAT prediction.Our experiments suggest that the LLM have very likely learned general reasoning procedures using CoT that can be applied to all formulas with the same number of variables as the data they are trained on.</p>
<p>LIMITATIONS IN LENGTH GENERALIZATION</p>
<p>The second experiment evaluates the model's ability to generalize to formulas with a different number of variables than seen during training.We use the model trained on the Marginal dataset from section 6.1 and evaluate datasets with 4-20 variables, generated using the three methods described, with 2,000 samples each.For this experiment, we evaluate the binary SAT vs UNSAT prediction accuracy.</p>
<p>Results</p>
<p>In Figure 2, our results indicate that performance degrades drastically beyond the training regime when the number of variables increases.This shows that the model is unable to learn a general SAT-solving algorithm that works for all inputs of arbitrary lengths, which corroborates our theoretical result where the size of the Transformer for SAT-solving depends on the number of variables.This further demonstrates the value of having a compiled Transformer that provably works well on all inputs up to p variables for any given p.</p>
<p>C PROOFS C.1 NOTATION DETAILS</p>
<p>3-SAT SAT problems where the Boolean formula is expressed in conjunctive normal form (CNF) with three literals per clause will be referred to as 3-SAT.A formula in CNF is a conjunction (i.e."AND") of clauses, a clause is a disjunction (i.e."OR") of several literals, and each literal is either a variable or its negation.In the case of 3-SAT, each clause contains at most three literals.An example 3-SAT formula with 4 variables and 6 clauses is:
(x 1 ∨ ¬x 2 ) ∧ (¬x 1 ∨ x 2 ∨ ¬x 3 ) ∧ (x 2 ∨ x 4 ∨ ¬x 1 )∧ (x 1 ∨ ¬x 3 ∨ x 4 ) ∧ (¬x 2 ∨ ¬x 3 ∨ ¬x 4 ) ∧ (¬x 4 ∨ ¬x 1 )
The 3-SAT problem refers to determining if any assignment of truth values to the variables allows the formula ϕ to evaluate as true.It is well-known that 3-SAT is NP-hard and is widely believed to be unsolvable in polynomial time.</p>
<p>DIMACS Encoding</p>
<p>The DIMACS format is a standardized encoding scheme for representing Boolean formulas in conjunctive normal form (CNF) for SAT problems.Each clause in the formula is represented as a sequence of integers followed by a terminating "0" (i.e."0" represents ∧ symbols and parentheses).Positive integers correspond to variables, while negative integers represent the negations of variables.For instance, if a clause includes the literals x 1 , ¬x 2 , and x 3 , it would be represented as "1 -2 3 0" in the DIMACS format.</p>
<p>For the 3-SAT example in the previous paragraph, the corresponding DIMACS representation would be:
1 -2 0 -1 2 -3 0 2 4 -1 0 1 -3 4 0 -2 -3 -4 0 -4 -1 0 C.2 USEFUL LEMMAS FOR TRANSFORMERS
In this section, we present adapted versions of several lemmas from Feng et al. (2023).Specifically, an MLP with ReGLU can exactly simulate ReLU, linear operations, and multiplication without error.</p>
<p>For Self-attention lemmas, we directly adapt from Feng et al. (2023).</p>
<p>C.2.1 LEMMAS FOR MLP WITH REGLU ACTIVATION</p>
<p>This section shows several lemmas showing the capabilities of the self-attention operation and MLP layers to approximate high-level vector operations.These high-level operations are later used as building blocks for the Transformer SAT-solver.Specifically, with appropriate weight configurations, a 2-layer MLP with ReGLU activation f (x) = W 2 [(W 1 x + b) ⊗ relu(V x + c)] can approximate the following vector operations for arbitrary input x:</p>
<p>• Simulate a 2-layer MLP with ReLU activation:
W 2 ReLU(W ′ 1 x + b ′ 1 ) + b ′ 2
• Simulate any linear operation W x • Simulate element-wise multiplication: x 1 ⊗ x 2 Lemma C.1 (Simulating a 2-Layer ReLU MLP with ReGLU Activation).A 2-layer MLP with ReGLU activation function can simulate any 2-layer MLP with ReLU activation function.</p>
<p>Proof.Let the ReLU MLP be defined as:
g(x) = W ′ 2 ReLU(W ′ 1 x + b ′ 1 ) + b ′ 2 .
Set the weights and biases of the ReGLU MLP as follows:
W 1 = 0, b 1 = 1, V = W ′ 1 , b 2 = b ′ 1 , W 2 = W ′ 2 , b = b ′ 2 .
Then, the ReGLU MLP computes:
f (x) = W ′ 2 [(0 • x + 1) ⊗ ReLU(W ′ 1 x + b ′ 1 )] + b ′ 2 .
Simplifying:
f (x) = W ′ 2 [1 ⊗ ReLU(W ′ 1 x + b ′ 1 )] + b ′ 2 = W ′ 2 ReLU(W ′ 1 x + b ′ 1 ) + b ′ 2 = g(x).
Thus, the ReGLU MLP computes the same function as the ReLU MLP.Proof.To compute a linear function using the ReGLU MLP, we can set the activation to act as a scalar multiplier of one.Set the weights and biases as:
W 1 = W , b 1 = b, V = 0, b 2 = 1, W 2 = I, b = 0.
Here, I is the identity matrix.</p>
<p>Since V x + b 2 = b 2 = 1, we have:
ReLU(V x + b 2 ) = ReLU(1) = 1.
Then, the ReGLU MLP computes:
f (x) = I [(W x + b) ⊗ 1] = W x + b.
Thus, any linear operation can be represented by appropriately setting W 1 , b 1 , and W 2 .</p>
<p>Lemma C.3 (Element-wise Multiplication via ReGLU MLP).</p>
<p>A 2-layer MLP with ReGLU activation can compute the element-wise multiplication of two input vectors x 1 and x 2 , that is,
f (x) = x 1 ⊗ x 2 ,
where x = [x 1 ; x 2 ] denotes the concatenation of x 1 and x 2 .</p>
<p>Proof.
Let x = [x 1 ; x 2 ] ∈ R 2n , where x 1 , x 2 ∈ R n .
Set the weights and biases:
W 1 = I n I n , b 1 = 0 2n , V = I n −I n , b 2 = 0 2n , W 2 = [I n −I n ] , b = 0 n .
Compute:
W 1 x + b 1 = x 1 x 1 , V x + b 2 = x 2 −x 2 , ReLU(V x + b 2 ) = ReLU(x 2 ) ReLU(−x 2 ) .
The element-wise product:
(W 1 x + b 1 ) ⊗ ReLU(V x + b 2 ) = x 1 ⊗ ReLU(x 2 ) x 1 ⊗ ReLU(−x 2 ) .
Compute the output:
f (x) = W 2 [(W 1 x + b 1 ) ⊗ ReLU(V x + b 2 )] + b = x 1 ⊗ ReLU(x 2 ) − x 1 ⊗ ReLU(−x 2 ) = x 1 ⊗ (ReLU(x 2 ) − ReLU(−x 2 )) = x 1 ⊗ x 2 .
Thus, the ReGLU MLP computes f (x) = x 1 ⊗ x 2 without restrictions on x 2 .</p>
<p>C.2.2 CAPABILITIES OF THE SELF-ATTENTION LAYER</p>
<p>In this subsection, we provide 2 core lemmas on the capabilities of the self-attention layer from Feng et al. (2023).</p>
<p>Let n ∈ N be an integer and let x 1 , x 2 , • • • , x n be a sequence of vectors where
x i = ( xi , r i , 1) ∈ [−M, M ] d+2 , xi ∈ R d , r i ∈ R, and M is a large constant. Let K, Q, V ∈ R d ′ ×(d+2)
be any matrices with ∥V ∥ ∞ ≤ 1, and let 0 &lt; ρ, δ &lt; M be any real numbers.Denote q i = Qx i , k j = Kx j , v j = V x j , and define the matching set
S i = {j ≤ i : |q i • k j | ≤ ρ}.
Equipped with these notations, we define two basic operations as follows:</p>
<p>• COPY: The output is a sequence of vectors Feng et al. (2023)] The matrices Q, K, V and scalars ρ, δ satisfy that for all considered sequences x 1 , x 2 , • • • , x n , the following hold:
u 1 , • • • , u n with u i = v pos(i) , where pos(i) = argmax j∈Si r j . • MEAN: The output is a sequence of vectors u 1 , • • • , u n with u i = mean j∈Si v j . Assumption C.4. [Assumption C.6 from• For any i, j ∈ [n], either |q i • k j | ≤ ρ or q i • k j ≤ −δ.
• For any i, j ∈ [n], either i = j or |r i − r j | ≥ δ.</p>
<p>Assumption C.4 says that there are sufficient gaps between the attended position (e.g., pos(i)) and other positions.The two lemmas below show that the attention layer with casual mask can implement both COPY operation and MEAN operation efficiently.</p>
<p>Lemma C.5 (Lemma C.7 from Feng et al. (2023)).Assume Assumption C.4 holds with ρ ≤ δ 2 8M .For any ϵ &gt; 0, there exists an attention layer with embedding size O(d) and one causal attention head that can approximate the COPY operation defined above.Formally, for any considered sequence of vectors x 1 , x 2 , . . ., x n , denote the corresponding attention output as o 1 , o 2 , . . ., o n .Then, we have Feng et al. (2023)).Assume Assumption C.4 holds with ρ ≤ δϵ 16M ln( 4M n ϵ ) .For any 0 &lt; ϵ ≤ M , there exists an attention layer with embedding size O(d) and one causal attention head that can approximate the MEAN operation defined above.Formally, for any considered sequence of vectors x 1 , x 2 , . . ., x n , denote the attention output as o 1 , o 2 , . . ., o n .Then, we have ∥o i − u i ∥ ∞ ≤ ϵ for all i ∈ [n] with S i ̸ = ∅.Moreover, the ℓ ∞ norm of attention parameters is bounded by O(poly(M, 1/δ, log(n), log(1/ϵ))).
∥o i − u i ∥ ∞ ≤ ϵ for all i ∈ [n] with S i ̸ = ∅. Moreover, the ℓ ∞ norm of attention parameters is bounded by O(poly(M, 1/δ, log(n), log(1/ϵ))). Lemma C.6 (Lemma C.8 from</p>
<p>C.3 THEORETICAL CONSTRUCTION</p>
<p>Preprint Note: We're in the process of reformatting the construction and proof for better organization</p>
<p>Notations</p>
<p>• p denotes the number of variables • t i denotes the token at position i • T vars denotes the set of tokens that denote variables and their negations.i.e. '1', '2', . . ., 'n', '-1', '-2', . . ., '-n'</p>
<p>• b denotes boolean variables</p>
<p>Proof.We first describe the encoding format of the formulas and the solution trace format before going into the details of model construction.</p>
<p>Input Format.We consider 3-CNF-SAT formulas in the DIMACS representation, with an initial [BOS] token and an ending [SEP] token.Each variable x i for i ∈ [n] has 2 associated tokens: i and -i (e.g., 1 and -1), where the positive token indicates that the i-th variable appears in the clause while the negative token indicates that the negation of the i-th variable appears in the clause.Clauses are separated using the 0 token.For example, the formula
(¬x 2 ∨ ¬x 4 ∨ ¬x 1 ) ∧ (x 3 ∨ x 4 ∨ ¬x 1 ) ∧ (¬x 1 ∨ ¬x 3 ∨ ¬x 2 ) ∧(x 1 ∨ ¬x 2 ∨ ¬x 4 ) ∧ (¬x 4 ∨ x 2 ∨ x 1 ) ∧ (x 1 ∨ ¬x 2 ∨ x 4 )
would be represented as:
[BOS] -2 -4 -1 0 3 4 -1 0 -1 -3 -2 0 1 -2 -4 0 -4 2 1 0 1 -2 4 0 [SEP]
Solution Trace Format.The trace keeps track of the order of the assignments made and whether each assignment is a decision (assumption) or a unit propagation (deduction).Literals with a preceding D token are decision literals while other literals are from unit propagation.When the model encounters a conflict between the current assignment and the formula, it performs a backtrack operation denoted by [BT] and performs another attempt with the last decision literal negated.In particular, compared to Figure 1, we used D to abbreviate Assume and use [BT] to abbreviate Backtrack</p>
<p>As an example, the solution trace for the above SAT formula would be:
[SEP] D 2 D 1 -4 3 [BT] D 2 D -1 -4 [BT] -2 D 3 D 4 -1 SAT
Embedding Layer.Our token set consists of one token for each variable and its negation, the separator token 0, and a special token D to denote where decisions are made.The positional encoding occupies a single dimension and contains the numerical value of the position of the token in the string.(i.e.there exists a dimension pos such that the position embedding of position i is i • e pos )</p>
<p>Layer 1.The first layer prepares for finding the nearest separator token and D token.Let i denote the position index of tokens:</p>
<ol>
<li>Compute i sep where i sep = i if the corresponding token t i ∈ {'0', '[SEP]', '[BT]'} and i sep = 0 otherwise 2. Similarly, compute i D where i D = i if the corresponding token t i = D and i sep = 0 otherwise.</li>
</ol>
<p>Compute
(i − 1) 2 , i 2 for index equality comparison
The first 2 operations can both be computed using a single MLP layer that multiplies between i from the positional encoding using Lemma C.3.Similarly, the 3rd operation is a multiplication operation that can be performed with Lemma C.3.</p>
<p>Layer 2. This layer uses 2 heads to perform the following tasks:</p>
<ol>
<li>Copy the index and type of the last separator token and stores
p sep i ′ = max{j : j ≤ i, t j ∈ {'0', '[SEP]', '[BT]'}} b 0 = (t j = '0') b [SEP] = (t j = '[SEP]') b [BT] = (t j = '[BT]') for j = p sep i ′ 2. (Backtrack) Compute the position of the nearest D token p D i = max{j : j ≤ i, t j = 'D'} 3. Compute (p sep i ′ ) 2 for index operation
Task 1 can be achieved via the COPY operation from Lemma C.5 with
q i = 1, k i = i sep , v j = (j, I[t j = '0'], I[t j = '[SEP]'], I[t j = '[UP]'], I[t j = '[BackTrack]']).
Task 2 is highly similar to task 1 and can be achieved using COPY with
q i = 1, k i = i D , v j = (j)
Task 3 is a multiplication operation that can be performed using Lemma C.3.</li>
</ol>
<p>Layer 3 This layer uses 1 head to copy the several values from the previous token to the current token.Specifically, this layer computes:</p>
<p>1.The position of the previous separator token, not including the current position: Task 1 and 2 is done by copying p sep i ′ and I[t i = 'D'] from the previous token.Specifially, we use the COPY operation from Lemma C.5 with q i = ((i − 1) 2 , i − 1, 1) and k j = (−1, 2j, −j 2 ) which determines i − 1 = j via −((i − 1) − j) 2 = 0 and v j = (p sep i ′ , I[t i = 'D']).Task 4 is a local multiplication operation that can be implemented via Lemma C.3.
p sep i = max{j : j &lt; i, t j ∈ {'0', '[SEP]','[UP]', '[BackTrack]'}}
Layer 4. This layer uses 2 heads to perform the following tasks:</p>
<ol>
<li>
<p>Compute the sum of all variable token embeddings after the previous separator to encode a vector representation of assignments and clauses at their following separator token.
− i if b [SEP] is true, i.e. p − i = p sep− i + d sep i + 4 • b [SEP]
. The additional 4 is the number of variables per clause + 1 to ensure that we don't consider the last clause as an assignment.</p>
</li>
<li>
<p>(Backtrack) Compute the position of the nearest D token to the last separator token
p D− i = p D p sep i ′ 4. Compute b exceed = (p − i &gt; p D− i + 1
), this denotes whether we're beyond the last decision of the previous state.</p>
</li>
</ol>
<p>Compare (p D-</p>
<p>i ≤ p − i ) for b BT finished at the next layer.
6. Compare if p D- i = p − i for the b backtrack operator. 7. Compute b ′ copy = (p − i &lt; p sep i ′ − 1)
Task 1 is achieved using a MEAN operation with q i = ((p sep i ) 2 , p sep i , 1), k j = ((−1, 2p sep j , −(p sep j ) 2 ), v j = e id(tj ) for t j ∈ T vars .This attention operations results in ri i−p sep i</p>
<p>The MLP layer then uses Lemma C.3 to multiply the mean result by i − p sep i to obtain the r i .</p>
<p>Task 2 is achieved using the COPY operation with q i = ((
p sep i ) 2 , p sep i , 1), k j = (−1, 2j, −j 2 ) and v j = p sep i ′ .
The MLP layer then performs the addition operation the computes p − i by Lemma C.2 Similarly, Task 3 is achieved using the COPY operation with q i = ((
p sep i ) 2 , p sep i , 1), k j = (−1, 2j, −j 2 ) and v j = p D i .
Layer 5.The third layer uses 5 heads to perform the following tasks:</p>
<ol>
<li>Determine whether the current assignment r i satisfies the formula.b sat 2. Determine whether the current assignment r i results in a contradiction with a clause of the formula b cont 3. Find clauses with at least 2 False literals and sum up the unassigned literals in these clauses.This would result in all the variables that can be currently determined via unit propagation.To describe the operations performed in this layer, we interpret the r i vectors computed in the previous layer as a 2n-dimensional binary encoding of the clause/assignment preceding token i.The value at dimension 2j − 1 is 1 if the clause/assignment contains variable j (1-indexed) in positive polarity and the value at dimension 2j is one iff the clause/assignment contains variable j is in negative polarity.For example, the clause 1 -2 4 is represented as the binary vector r = [1, 0, 0, 1, 0, 0, 1, 0] when the number of variables is n = 4.The r representation for each clause is at the '0' separator following the clause in the input format.</li>
</ol>
<p>We now define the linear transformation T [v true , v f alse , v none ] ∈ R 2n×2n where v true , v f alse , v none ∈ {(0, 0), (0, 1), (1, 0), (1, 1)}.The transformation takes every pair of values in r corresponding to each variable, determines whether the variable is true, false, or noneexistent in the clause/assignment represented by r, and replaces each pair with the corresponding v true , v f alse , v none value.</p>
<p>For example, when r = [1, 0, 0, 1, 0, 0, 1, 0], applying T [(1, 1), (1, 0), (0, 1)] will result in [1, 1, 1, 0, 0, 1, 1, 1].Also, T [(1, 0), (0, 1), (0, 0)] is equivalent to the identity operation.Intuitively, the transformation changes 2-element binary vectors representing true, false, and non-existence within the clause/assignment.The transformation is used to construct query and key matrices to satisfy the desired properties of the assignment-clause dot product.</p>
<p>Parallel Deduction over Clauses Task 1 (checking satisfiability) is achieved via an MEAN Lemma C.6 with q i = (r i , 1) and k j = M (−r j , c</p>
<p>(1) j ) and v j = 1[t j = '[BOS]'], where c</p>
<p>(1)
j =    0 t j = '0', −0.5 t j = '[BOS]' −M otherwise
and M is a sufficiently large constant to approximate hard-max with the softmax operation.</p>
<p>Correctness: Consider the case where r i denotes the binary encoding of the current assignment and r j denotes the binary encoding of a clause at a '0' separator position.Then r i • r j denotes the number of common literals in the assignment and the clause, i.e. how many literals in the clause are True according to the assignment r i .Therefore, the clause is satisfied by the assignment ending at position i as long as r i • r j ≥ 1.Since we only consider the r j values at the '0' separators as the binary encoding of the clause, all these positions have c</p>
<p>(1) j = 0. Therefore, q i • k j = −M r i • r j , which is 0 for non-satisfied clauses and &lt; −M for satisfied clauses.Also notice that since c</p>
<p>(1) j = −0.5 for j = 1 (i.e. the first [BOS] token), so
q i • k 1 = − M 2 .
If the formula is satisfied, all clauses must be satisfied, and each clause must have attention score q i • k j &lt; −M while the [BOS] token has attention score − M 2 .For sufficiently large M , we can view the softmax operation as selecting the value vector of the largest attention score item, which is
[BOS]. Since v 1 = 1[t 1 = '[BOS]' = 1,
the result of the attention head will be 1.Conversely, if at least one clause is not satisfied, then q i • k j = 0 for that particular clause.As such, the [BOS] token will not be selected and the result of the attention operation will be 0.</p>
<p>Similarly, task 2 (Detecting Conflict) is also achieved via MEAN(Lemma C.6) with q i = (T [(1, 0), (0, 1), (1, 1)]r i , 1), k j = M (−r j , c</p>
<p>(1)
j ), v j = 1 − 1[t j = '[BOS]'],
where the definition of M and c</p>
<p>(1) j is the same as Task 1.For task 3 (unit propagation), apply MEAN (Lemma C.6) with q i = (T [(0, 1), (1, 0), (0, 0)]r i , 1), k j = M (r j , c Correctness: Here we show that, if the assignment at position i does not make the formula unsatisfied, then the resulting vector is approximately a binary encoding of all literals that can be unit-propagated.Consider again the case where r i denotes the binary encoding of the current assignment and r j denotes the binary encoding of a clause at a '0' separator position.Here q i • k j denotes M times the number of false literals in clause j according to the current assignment i.Since each clause has three variables, if a clause has three false assignments, then the formula is unsatisfied by the assignment and thus requires no further unit propagation.Therefore, we consider the case where each clause has at most 2 opposing assignments.</p>
<p>If there are no clauses with 2 opposing assignments, then all clause attention logits q i • k j will be at most M , while the attention logit to the [BOS] token will be c</p>
<p>(2) 1 = 1.5M .Since M is a large number, most attention weights will be assigned to [BOS] after the softmax operation and result in a zero embedding vector.</p>
<p>If at least one clause has 2 opposing assignments, all these clauses will have attention logits q i • k ≈ 2M .Therefore, the attention value will be evenly distributed on all clauses with 2 opposing assignments.The resulting attention output o [U P ] will be the average of embedding of all clauses with 2 opposing assignments, multiplied by c since v j = c • r j .Since there are at most c clauses, the number of attended clauses is at most c, and the divisor when computing the average is at most c.Therefore, the resulting o [U P ] will be a embedding vector where every literal that appeared in at least one clause with 2 false literals have their corresponding position assigned to a ≥ 1 value.</p>
<p>Layer 6 This layer does the remaining boolean operators required for the output.In particular,
• b unsat = b no decision ∧ b cont • b [BT] = b cont ∧ ¬(t i = [BT])
• Compute a vector that is equal to b backtrack • e BT , which is equal to e BT if b backtrack is True and 0 otherwise.This is to allow the operation at the output layer for backtracking Note that ∧ can be implemented as a single ReLU operation for tasks 1 and 2 that can be implemented with Lemma C.1, and task 3 is a multiplication operation implemented with Lemma C.3</p>
<p>Layer 7 This layer performs a single operation with the MLP layer: Compute b copy • e copy , which gates whether e copy should be predicted based on b copy .This enables condition 5 at the output layer.</p>
<p>Output Projection The final layer is responsible for producing the output of the model based on the computed output of the pervious layers.We constructed prioritized conditional outputs, where the model outputs the token according to the first satisfied conditional in the order below: 7. output D if the current token is not D
1. If b sat output SAT 2. If b cont ∧ b no decision output UNSAT 3. If b cont ∧ ¬(t i = [BackTrack]) output '[BackTrack]'4</p>
<p>output a unassigned variable</p>
<p>For the output layer, we use l [TOKEN] to denote the output logit of [TOKEN].Since the final output of the model is the token with the highest logit, we can implement output priority by assigning outputs of higher priority rules with higher logits than lower priority rules.Specifically, we compute the output logits vector using the output layer linear transformation as:
2 7 • b sat • e SAT + 2 6 • b cont • e [BackTrack] + 2 5 • b unsat • e UNSAT +2 4 •b backtrack •e BT +2 3 •b copy •e copy +2 2 •e UnitPropVar +2 1 •(1−1[t i = 'D ′ ])•e D +2 0 •T [(0, 0), (0, 0), (1, 1)]r i
Proposition C.7.There exists a transformer with 7 layers, 5 heads, O(p) embedding dimension, and O(p 2 ) weights that, on all inputs s ∈ DIMACS(p, c), predicts the same token as the output as the above operations.Furthermore, let l ctx = 4c + p • 2 p be the worst-case maximum context length required to complete SAT-solving, then all weights are within poly(l ctx ) and can be represented within O(p + log c) bits.</p>
<p>We only argue from a high level why this is true due to the complexity of the construction.In the above construction, we demonstrate how each operation can be approximated by a Self-attention or MLP layer.We can set the embedding dimension to the sum of dimensions of all the intermediate values and allocate for every intermediate values a range of dimensions that's equal to the dimension of the variables.All dimensions are initialized to 0 in the positional encoding of the transformer except for the dimensions assigned to the positional index i.Similarly, only the dimensions assigned to the one-hot token representation are initialized in the token embeddings.At each layer, the self-attention heads and MLP layers extract the variable values from the residual stream and perform the operations assigned to them at each layer.
• If t = D, set isDecision ← True. • Else if t = [BT],
remove literals from M up to and including the last decision literal (i.e., perform backtracking).• Else if t = i or -i for some i ∈ [n]:</p>
<p>-Let l be the literal corresponding to
x i = T if t = i, or x i = F if t = -i.
-If l is already assigned in M with a conflicting value, set M S (R) = fail.</p>
<p>-Else, append l to M , annotated as a decision literal if isDecision = True, or as a unit propagation otherwise.-Reset isDecision ← False.</p>
<p>• Else, set M S (R) = error.We now present the inductive lemma: Lemma C.15 (Inductive Lemma).For any p, c ∈ N + , for any input F DIMACS ∈ DIMACS(p, c) of length n, let F be the boolean formula in CNF form encoded in F DIMACS .Let A be the model described in section C.3 with parameters p, c.Let (s 1:n , s 1:n+1 , . . . ) be the trace of s when running the Greedy Decoding Algorithm 1 with model A and input prompt s 1:n = F DIMACS .For every i ∈ N + , if f S (s 1:n+i ) = S and S / ∈ {SAT, UNSAT, error}, then there exist j ∈ N + and S ′ ∈ S such that S =⇒ S ′ and f S (s 1:n+i+j ) = S ′ .</p>
<p>We now show trace equivalence between the model A and some instantiating of the abstract DPLL with a specific heuristic: Definition C.16.For any heuristic h : S → L where L is the set of literals, let DPLL h denote an instantiation of the abstract DPLL algorithm that selects h(S) as the decision literal when performing Decide and only performs the BackTrack operation for Backjump.h(S) is a valid heuristic if DPLL h always abides by the Decide transition.Lemma C.17. (Trace Simulation) There exists a valid heuristic h : S → L for which the Transformer model A is trace equivalent to DPLL h on all inputs in DIMACS(p, c) Proof.We aim to show that there exists a valid heuristic h : S → L such that the Transformer model A is trace equivalent to DPLL h on all inputs in DIMACS(p, c).</p>
<p>Define the heuristic h as follows: For any state S ∈ S, let h(S) be the literal that the Transformer model A selects as its next decision literal when in state S.</p>
<p>Formally, given that the model A outputs tokens corresponding to decisions, unit propagations, backtracks, etc., and that these tokens can be mapped to transitions in the abstract DPLL system via the mapping M S (as per the Translating CoT to Abstract DPLL State definition), we set:
h(S) =
the decision literal chosen by A in state S, if A performs a Decide transition, undefined, otherwise.</p>
<p>This heuristic is valid because A always abides by the Decide transition rules, ensuring h(S) selects a literal that occurs in F and is undefined in M , satisfying the conditions of a valid heuristic.</p>
<p>Define a mapping ϕ : Σ A → Σ B , where Σ A is the set of possible states of model A, and Σ B is the set of possible states of DPLL h , such that for any state S in the execution trace of A, ϕ(S) = S.</p>
<p>That is, we identify the states of A with the corresponding states in DPLL h by mapping the sequence of assignments and the formula F directly.</p>
<p>Proof of Trace Equivalence:</p>
<p>We proceed by induction on the number of steps in the execution trace.</p>
<p>Base Case (i = 0):</p>
<p>At the beginning, both algorithms start from the initial state with no assignments:</p>
<p>For A : S A 0 = ∅ ∥ F, and For DPLL h : S B 0 = ∅ ∥ F.</p>
<p>Clearly, ϕ(S A 0 ) = S B 0 .</p>
<p>Inductive Step:</p>
<p>Assume that after k steps, the states correspond via ϕ: ϕ(S A k ) = S B k .We need to show that after the next transition, the states still correspond, i.e., ϕ(S A k+1 ) = S B k+1 .Suppose the model A applies a UnitPropagate operation, transitioning from state S A k to S A k+1 by adding a literal l deduced via unit propagation.</p>
<p>Since unit propagation is deterministic and depends solely on the current assignment M and formula F , DPLL h will also apply the same UnitPropagate operation, transitioning from S B k to S B k+1 by adding the same literal l.</p>
<p>Thus, ϕ(S A k+1 ) = S B k+1 .Suppose the model A applies a Decide operation, transitioning from S A k to S A k+1 by adding a decision literal l = h(S A k ).By the definition of the heuristic h, DPLL h also selects l as the decision literal in state S B k .Both algorithms make the same decision and transition to the same next state.</p>
<p>Therefore, ϕ(S A k+1 ) = S B k+1 .Suppose the model A applies a Backjump operation, backtracking to a previous state and assigning a new literal.</p>
<p>Since DPLL h performs only the BackTrack operation for Backjump (as per the definition), and A simulates this operation, both algorithms backtrack in the same manner and update their assignments accordingly.</p>
<p>Thus, ϕ(S A k+1 ) = S B k+1 .If the model A reaches a terminal state indicating SAT or UNSAT, then so does DPLL h , since their sequences of transitions have been identical up to this point.In all cases, the next state of model A corresponds to the next state of DPLL h under the mapping ϕ.Therefore, by induction, the execution traces of A and DPLL h are such that for all i, ϕ(S A i ) = S B i .</p>
<p>Since the heuristic h selects the same decision literals as the model A, and A always abides by the Decide transition (as per its design), h is a valid heuristic according to the definition provided.# The nearest 'D' token, which denotes the next token is a decision</p>
<p>D CODE FOR THEORETICAL CONSTUCTION</p>
<p>Figure 1 :
1
Figure 1: Visualization of the Chain-of-Thought (CoT) process used by our model to solve the SAT formula described in Theorem 4.5.The model autonomously performs trial-and-error reasoning, making multiple attempts and backtracking upon encountering conflicts.Here, T represents True and F represents False.Tokens in typewriter font denote the CoT generated by the model.</p>
<p>h for every h = 1, . . ., H, and multi-head projection matrix W (l)</p>
<p>Definition 4.4 (3-SAT p,c ).Let DIMACS(p, c) denote the set of valid DIMACS encodings of 3-SAT instances with at most p variables and c clauses with a prepended [BOS] token and an appended [SEP] token.Define 3-SAT p,c : DIMACS(p, c) → {0, 1} as the problem of deciding whether the 3-SAT formula encoded in the input in DIMACS(p, c) encoding is satisfiable.With the above definition, we're ready to present a formal statement of our theoretical construction of a Transformer model that performs SAT Solving: Theorem 4.5 (Decoder-only Transformers can solve SAT).For any p, c ∈ N + , there exists a Transformer model M : V * → ∆(V) that autoregressively decides 3-SAT p,c in no more than p • 2 p+1 CoT iterations.M requires L = 7 layers, H = 5 heads, d emb = O(p), and O(p 2 ) parameters.</p>
<p>and j ∈ [d sop ].This mirrors NumPy's array indexing semantics.• sop[:, start:end] Extracts a slice of dimensions from sop, where start, end ∈ [d sop ], resulting in a new SOp of dimension end − start.This operation is analogous to NumPy slicing.• Element-wise operations such as sop1 + sop2, sop1 -sop2, sop1 * sop2, logical operations (&amp; for AND, | for OR), and comparison operations (≥, ≤, &gt;, &lt;), following standard broadcasting rules.As an illustrative example, the following function returns a one-dimensional SOp representing the position index of the closest token within a set of target tokens: def nearest_token_id(tok_emb: OneHotTokEmb, vocab: List[str], targets: List[str], indices: Indices=indices): # Get the token ids of the target tokens target_tok_ids = [vocab.index(target)for target in targets] # Get whether the current token is one of the target tokens # by summing the one-hot embedding target_token_embs = Concat([tok_emb[:, target_tok_id] for target_tok_id in target_tok_ids]) in_targets = target_token_embs.sum(axis=1)# Filter the indices to only include the target tokens filtered_index = indices * in_targets return filtered_index.max()</p>
<p>Figure 2: Result of the Length generalization experiments, showing SAT/UNSAT prediction accuracy of Transformer-LLM trained on the marginal, random, and skewed dataset on the marginal dataset over 4-20 variables.Left: model trained on 6-10 variables.Right: model trained on 11-15 variables.</p>
<p>Figure 3 :
3
Figure 3: Chain-of-Thought Lengths generated by the compiled SAT-Solver Model vs the number of boolean variables in sampled SAT formulas, y-axis in log scale.Solid lines denote the maximum CoT length for each dataset while opaque, dashed lines denote the average CoT length.The empirical maximum CoT length in our datasets is bounded by 8p • 2 0.08p.</p>
<p>Figure 4 :
4
Figure 4: The impact of soft attention in Transformer layers on the SAT/UNSAT prediction accuracy.β is a scaling factor that allows the soft attention operation to better simulate hard attention at the cost of larger model parameter values in attention layers.The model achieves perfect accuracy on all "marginal" datasets starting at β = 17.5, and for lower β values, accuracy is negatively correlated with the number of variables in the datasets.</p>
<p>Lemma C. 2 (
2
Simulating Linear Operations with ReGLU MLP).A 2-layer MLP with ReGLU activation can compute any linear operation f (x) = W x + b.</p>
<p>2.</p>
<p>Dermine if the previous token is D: b decision = (t i−1 = 'D') i.e., whether the current token is a decision variable 3. (Induction) Compute the offset of the current token to the previous separator token d sep i = i − p sep i ′ 4. Compute (p sep i ) 2 , for equality comparison at the next layer.</p>
<p>Induction) Compute the position of the second-to-last separator p sep− i = max{j : j &lt; p sep i , t j ∈ {'0', '[SEP]','[UP]', '[BackTrack] ′ }} = p sep p sep i ′ and the corresponding current position in the previous state p − i = p sep− i + d sep i .As a special case for the first state, we also add 4 to p</p>
<p>b f inal = b exceed ∧ b decision 5. Compare b no decision = (p D i ≤ p sep i ), which denotes whether the current state contains no decision variables 6. Compute b BT finished = (p D- i ≤ p − i ) ∧ b [BackTrack] 7. Compare p − i with p D− i − 1 by storing p − i ≤ p D− i − 1 and p − i ≥ p D− i − 1 (to check for equality at the next layer) 8. Compare b backtrack = (p − i = p D− i − 1)</p>
<p>Let the attention result be o U P .The MLP layer then computes e U P = ReLU (o U P )−ReLU (o U P − 1) − T [(1, 1), (1, 1), (0, 0)]r i via Lemma C.1.</p>
<p>. (BackTrack) If b backtrack , output the negation of the token from position p D− i + 1 5. (Induction) If b copy , copy token from position p − i + 1 as output (e copy ) 6. output a unit propagation variable, if any.</p>
<p>6.</p>
<p>Return the state M ∥ F . 7. If any of the above steps fail, set M S (R) = fail.</p>
<h1></h1>
<p>vocab.index(target) for target in targets] target_tokens = Concat([tok_emb[:, target_tok_id]      for target_tok_id in target_tok_ids]) in_targets = Linear(target_tokens, np.ones((1, len(targets)))) filtered_index = (indices * in_targets) new_v = [] for v_i in v: if isinstance(v_i, SOp): new_v.append(v_i)elif v_i == 'target' or v_i == 'targets': new_v.append(target_tokens).zeros((2<em> num_vars, 2 * num_vars)) true_vec_off = (true_vec[0] -none_vec[0], true_vec[1] -none_vec[1]) false_vec_off = (false_vec[0] -none_vec[0], false_vec[1] -none_vec[1]) for i inrange(num_vars): true_id = i false_id = num_vars + i mat[true_id, true_id] = true_vec_off[0] mat[true_id, false_id] = false_vec_off[0] mat[false_id, true_id] = true_vec_off[1] mat[false_id, false_id] = false_vec_off[1] bias = np.zeros(2</em> num_vars) bias[:num_vars] += none_vec[0] bias[num_vars:] = none_vec[1] return Linear([encodings, ones], np.hstack([mat.T, bias.reshape((-1,1))])) def dpll(num_vars, num_clauses, context_len, mean_exactness=20, nonsep_penalty=20, return_logs=False) -&gt; Tuple[ SOp, List, Dict[str, SOp]]: vocab: List = ([str(i) for i in range(1, num_vars + 1)] + [str(-i) for i in range(1, num_vars + 1)] + ['0', '[SEP]', '[BT]', '[BOS]', 'D', 'SAT', 'UNSAT']) idx: Dict[str, int] = {token: idx for idx, token in enumerate(vocab)} sop_logs: Dict[str, SOp] = {} sops.config["mean_exactness"Thenearest (including self) separator token and whether # the previous separator token is '0', '[SEP]', '[UP]', '[BT]' p_i_sep_p, b_0, b_SEP, b_BackTrack = ( nearest_sep[:, 0].named("p_i_sep_p"), nearest_sep[:, 1].named("b_0"), nearest_sep[:, 2].named("b_SEP"), nearest_sep[:, 3].named("b_BackTrack"))</p>
<p>, s 2 , . . ., s n ) ∈ V n , where V is a vocabulary, a Transformer model M : V * → ∆(V) maps s to a distribution p n+1 ∈ ∆(V) by composing a sequence of parameterized intermediate operations.These begin with a token embedding layer, following by L transformer blocks (layers), each block consisting of H attention heads, with embedding dimension d emb , head dimension d h , and MLP hidden dimension d mlp .Let us now describe each of these maps in detail.
3.1 AUTOREGRESSIVE DECODER-ONLY TRANSFORMER ARCHITECTUREThe Transformer architecture Vaswani et al. (2017) is a foundational model in deep learning forsequence modeling tasks. In our work, we focus on the autoregressive decoder-only Transformer,which generates sequences by predicting the next token based on previously generated tokens. Itis a relatively complex architecture, and here we only give a precise but quite concise description,and we refer the reader Vaswani et al. (2017) among many others for additional details. Given aninput sequence of tokens s = (s 1</p>
<p>prompt s 1:n = (s 1 , s 2 , . . ., s n ), stop tokens E ⊆ V, t ← n while t ← t + 1 do
p t ← M (s 1:t−1 ) ;// Obtain model outputs t ← arg max v∈V p t (v) ;// Select most probable tokenif s t ∈ E return s 1:t</p>
<p>Table 1 :
1Marginal RandomSkewed Marginal Random SkewedMarginal 99.88%99.99%99.99%98.66%99.70% 99.57%Random99.96% 100.00% 100.00%99.11%99.75% 99.55%Skewed99.96% 100.00% 99.99%99.41%99.74% 99.48%
Average accuracies (%) of SAT/UNSAT prediction for models trained and tested on different datasets in the training regime for number of variables p ∈ [6, 10] and p ∈ [11, 15].Columns denote train datasets, and rows denote test datasets.Each accuracy is computed over 10000 total samples.p ∈ [6, 10] p ∈ [11, 15] Our first set of experiments evaluates the model's performance on SAT formulas sampled from different distributions from training, but the number of variables in formulas remains the same (p ∈ [6, 10] and p ∈</p>
<p>The number of training samples is negligible compared to the total number of possible formulas. Note that the number of clauses is at least 4p, each clause contains 3 literals and each literal has at least p choices. This results in p 12p possibilities, which is &gt; 10 56 for p = 6
ACKNOWLEDGEMENTSThis work is supported by NSF awards IIS-2229876 and CCF-2403391.The authors thank Hassan Naveed for his exploratory work on using RASP for SAT-solver implementation, William Armstrong for suggesting the abstract DPLL paper and contributing code, and Jintong Jiang for providing suggestions on the paper's presentation and writing.The only intermediate values whose dimensions are dependent on p are the vectors for one-hot encodings and storing binary encodings of clauses and assignments.They all have size 2p.Therefore, the number of total allocated embedding sizes is also O(p).Furthermore, shows that all parameter values are polynomial with respect to the context length and the inverse of approximation errors.Note that we need only guarantee the final error is less than 1 to prevent affecting the output token.Furthermore, we can choose all parameter values so that they are multiples of 0.5.As such, all parameters are within poly(l ctx ) and can be represented by O(log(l ctx )) = O(p + log c)C.4 CORRECTNESSNote: This section assumes prior knowledge in propositional logic and SAT solving, including an understanding of the DPLL algorithm.For a brief explanation of the notations in this section, please refer to(Nieuwenhuis et al. (2005)).For more general knowledge, please refer to(Biere et al. (2009)).We prove that the above model autoregressive solves 3-SAT p,c by showing that it uses the CoT to simulate the "Abstract DPLL Procedure".C.4.1 ABSTRACT DPLLIn this section, we provide a description of abstract DPLL.Since the focus of this paper is not to show the correctness of the DPLL algorithm but rather how our model's CoT is equivalent to it, we only present the main results fromNieuwenhuis et al. (2005)and refer readers to the original work for proof of the theorems.Let M be an ordered trace of variable assignments with information on whether each assignment is an decision literal (i.e.assumption) or an unit propagation (i.e., deduction).For example, the ordered trace 3 d 1 2 4 d 5 denotes the following sequence of operations:Let F denote the a SAT formula in CNF format (which includes 3-SAT), C denote a clause (e.g., x 1 ∨ ¬x 2 ∨ x 3 ), l denote a single literal (e.g., ¬x 2 ), and l d denote a decision literal.Let M |= F denote that the assignment in M satisfies the formula F .Definition C.8 (State in the DPLL Transition System).A state S ∈ S in the DPLL transition system is either:• The special states SAT, UNSAT, indicating that the formula satisfiable or unsatisfiable• A pair M ∥ F , where:andrepresenting variable assignments, where • denotes concatenation.Annotations indicate whether a literal is a decision literal (denoted by l d ) or derived through unit propagation.We denote the empty sequence of literals by ∅, unit sequences by their only literal, and the concatenation of two sequences by simple juxtaposition.While M is a sequence, it can also be viewed as a set of variable assignments by ignoring annotations and order.Definition C.9 (Adapted from Definition 1 ofNieuwenhuis et al. (2005)).The Basic DPLL system consists of the following transition rules S =⇒ S:UnitPropagate :Decide :Backjump :There is some clauseFail :Explanation of the Backjump Operation:The Backjump operation allows the DPLL algorithm to backtrack to a previous decision and learn a new literal.In particular, F |= C ∨ l ′ means that, for some clause C, every assignment that satisfies F must either satisfy C (i.e., contain the negation of each literal in C) or contain l ′ as an assignment.However, if M |= ¬C, which means that M conflicts with C and thus contains the negation of each literal in C, then if we want some assignment containing M to still satisfy F , then the assignment must also include the literal l ′ as an assignment to ensure that it satisfies C ∨ l ′ , a requirement for satisfying F .In our construction, we only consider the narrower set of BackTrack operations that find the last decision and negate it: Lemma C.10. [Corrollary of Lemma 6 fromNieuwenhuis et al. (2005)is always a valid Backjump operation in Definition C.9. Definition C.11 (Run of the DPLL Algorithm).A run of the DPLL algorithm on formula F is a sequence of states• S 0 is the initial state ∅ ∥ F• For each i = 0, 1, . . ., n − 1, the transition S i =⇒ S i+1 is valid according to the transition rules of the DPLL system in Definition C.9 (e.g., UnitPropagate, Decide, Backjump, or Fail);• S n is a final state that is either SAT or UNSAT Note that the above definition is simply the expansion of ∅ ∥ F =⇒ !S T .The following theorem states that the DPLL procedure always decides the satisfiability of CNF formulas: Lemma C.12. [Theorem 5 and Theorem 9 Combined fromNieuwenhuis et al. (2005)] The Basic DPLL system provides a decision procedure for the satisfiability of CNF formulas F .Specifically:3. There exist no infinite sequences of the formTRACE EQUIVALENCE AND INDUCTIVE PROOFWe demonstrate that our Transformer in Theorem 4.5 solves SAT by showing that the CoT produced by the Transformer is "trace equivalent" to an abstract DPLL algorithm with some heuristic.We first provide definition of "trace equivalence": Definition C.13 (Trace Equivalence of Algorithms).Let A and B be two algorithms.Let Σ A and Σ B be the sets of possible states of A and B, respectively.We say that algorithms A and B are trace equivalent if there exists a bijective mapping ϕ : Σ A → Σ B , independent of the input, such that for every input s, the traces produced by A and B satisfy the following:If the execution of A on input s produces the traceand the execution of B on the same input s produces the traceThat is, the sequences of states of A and B are in one-to-one correspondence via the fixed mapping ϕ, and corresponding states are related by this mapping for every input s.We first show how to convert a chain of thought of the model into a state in the abstract DPLL algorithm.Consider the following model input and Chain-of-Thought trace:Recall that [BT] denotes backtracking and D denotes that the next token is a decision literal.Note that the prompt input ends at [SEP] and the rest is the Chain-of-Though produced by the model.We want to convert this trace to a state S = M ∥F such that F is the CNF formula in the DIAMCS encoding in the prompt input and M is the "assignment trace" at the last attempt (i.e., after the last [BT] token.).As such, M correspond to the D 2 D -1 -4 portion of the trace and thus M = 2 d 1d 4 as described in Appendix C.4.1.We formalize this process as follows: Definition C.14 (Translating CoT to Abstract DPLL State).For any number of variables p ∈ N + , let V be the set of tokens:Define a mapping f S : V * → S ∪ {error} that converts a sequence of tokens R ∈ V * into an abstract DPLL state as follows:1.If R ends with SAT or UNSAT, then set M S (R) to SAT or UNSAT accordingly.
Handbook of Satisfiability. Armin Biere, Marijn Heule, Frontiers in Artificial Intelligence and Applications. Hans van Maaren, and Toby Walsh1852009IOS Press</p>
<p>The complexity of theorem-proving procedures. Stephen A Cook, 10.1145/800157.805047Proceedings of the 3rd Annual ACM Symposium on Theory of Computing. Michael A Harrison, Ranan B Banerji, Jeffrey D Ullman, the 3rd Annual ACM Symposium on Theory of ComputingShaker Heights, Ohio, USAACMMay 3-5, 1971. 1971</p>
<p>Experimental results on the crossover point in random 3-sat. James M Crawford, Larry D Auton, org/10.1016/0004-3702(95)00046-1Frontiers in Problem Solving: Phase Transitions and Complexity. 0004-37028111996Artificial Intelligence</p>
<p>Towards revealing the mystery behind chain of thought: A theoretical perspective. Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, Liwei Wang, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, NeurIPS; New Orleans, LA, USA2023. 2023. December 10 -16, 2023, 2023</p>
<p>Looped transformers as programmable computers. Angeliki Giannou, Shashank Rajput, Jy-Yong Sohn, Kangwook Lee, Jason D Lee, Dimitris Papailiopoulos, Proceedings of the 40th International Conference on Machine Learning. Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, Jonathan Scarlett, the 40th International Conference on Machine LearningPMLRJul 2023202</p>
<p>Position: LLMs can't plan, but can help planning in LLM-modulo frameworks. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Paul Saldyt, Anil B Murthy, Forty-first International Conference on Machine Learning. 2024</p>
<p>Chain of thought empowers transformers to solve inherently serial problems. Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Tracr: Compiled Transformers as a Laboratory for Interpretability. David Lindner, János Kramár, Sebastian Farquhar, Matthew Rahtz, Thomas Mcgrath, Vladimir Mikulik, 2023</p>
<p>Transformers learn shortcuts to automata. Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>The parallelism tradeoff: Limitations of log-precision transformers. William Merrill, Ashish Sabharwal, 10.1162/tacla00562Transactions of the Association for Computational Linguistics. 112023</p>
<p>The expressive power of transformers with chain of thought. William Merrill, Ashish Sabharwal, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Logic for Programming, Artificial Intelligence, and Reasoning. Robert Nieuwenhuis, Albert Oliveras, Cesare Tinelli, Franz Baader and Andrei Voronkov2005SpringerBerlin, Heidelberg; Berlin HeidelbergAbstract dpll and abstract dpll modulo theories</p>
<p>Automatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, 2017</p>
<p>On the turing completeness of modern neural network architectures. Jorge Pérez, Javier Marinković, Pablo Barceló, International Conference on Learning Representations. 2019</p>
<p>Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, ArXiv, abs/2302.139712023</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. Isabelle Guyon, Samy Ulrike Von Luxburg, Hanna M Bengio, Rob Wallach, S V N Fergus, Roman Vishwanathan, Garnett, Long Beach, CA, USADecember 4-9, 2017. 2017</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Thinking like transformers. Gail Weiss, Yoav Goldberg, Eran Yahav, Proceedings of the 38th International Conference on Machine Learning. Marina Meila, Tong Zhang, the 38th International Conference on Machine LearningPMLR18-24 Jul 2021139Proceedings of Machine Learning Research</p>
<p>Self-attention networks can process bounded hierarchical languages. Shunyu Yao, Binghui Peng, Christos Papadimitriou, Karthik Narasimhan, Association for Computational Linguistics (ACL). 2021</p>
<p>On the paradox of learning to reason from data. Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van Den, Broeck, 10.24963/ijcai.2023/375Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI '23. the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI '232023</p>
<p>For the 70M model, we use models with 6 layers, 512 embedding dimensions, 8 heads, 512 attention hidden dimensions, and 2048 MLP hidden dimensions. For the 140M model, we use 12 layers, 768 embedding dimensions, 12 heads, 768 attention hidden dimensions, and 3072 MLP hidden dimensions. Both models have 850 context size. We trained for 5 epochs on both datasets using the Adam optimizer with a scheduled cosine learning rate decaying from 6 × 10 −4 to 6 × 10 −5 with β 1 = 0.9 and β 2 = 0.95. B ADDITIONAL EXPERIMENT RESULTS In Figure 3 we provide results on the number of Chain-of-Thought tokens required to solve randomly generated SAT instances. A Training Details We Use, Llama Touvron, 2023models in the HuggingFace library. In Figure 4 we provide results on how the SAT/UNSAT prediction accuracy is affected by numerical errors introduced by softmax</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6552 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6552</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6552</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-269613802</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.04086v1.pdf" target="_blank">Optimizing Language Model's Reasoning Abilities with Weak Supervision</a></p>
                <p><strong>Paper Abstract:</strong> While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations poses scalability challenges, particularly as models and data requirements grow. To mitigate this, we explore the potential of enhancing LLMs' reasoning abilities with minimal human supervision. In this work, we introduce self-reinforcement, which begins with Supervised Fine-Tuning (SFT) of the model using a small collection of annotated questions. Then it iteratively improves LLMs by learning from the differences in responses from the SFT and unfinetuned models on unlabeled questions. Our approach provides an efficient approach without relying heavily on extensive human-annotated explanations. However, current reasoning benchmarks typically only include golden-reference answers or rationales. Therefore, we present \textsc{PuzzleBen}, a weakly supervised benchmark that comprises 25,147 complex questions, answers, and human-generated rationales across various domains, such as brainteasers, puzzles, riddles, parajumbles, and critical reasoning tasks. A unique aspect of our dataset is the inclusion of 10,000 unannotated questions, enabling us to explore utilizing fewer supersized data to boost LLMs' inference capabilities. Our experiments underscore the significance of \textsc{PuzzleBen}, as well as the effectiveness of our methodology as a promising direction in future endeavors. Our dataset and code will be published soon on \texttt{Anonymity Link}.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6552.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6552.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pretrained generative transformer language model evaluated in the paper on PUZZLEBEN's reasoning tasks using standard prompting and chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pretrained transformer-based LLM (proprietary), evaluated with instruction prompting and chain-of-thought style prompting in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>PUZZLEBEN (Puzzles, Riddles, Parajumble, Critical Reasoning, Brainteasers subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>general puzzles / reasoning tasks (paper does not specify explicit spatial puzzle games)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PUZZLEBEN</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>standard prompting; zero-shot chain-of-thought (CoT); k-shot ICL reported elsewhere in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>chain-of-thought style prompting (CoT) and standard prompting (no external symbolic search or solver reported)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Natural-language questions and human-written rationales from PUZZLEBEN; no special grid or spatial encoding reported</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (task-wise classification/solve rate on PUZZLEBEN test subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per-table results reported in the paper: GPT-4 evaluated with standard prompting and zero-shot CoT and attains substantially higher accuracy than smaller models; the paper reports up to ~81% on some subtasks with CoT (exact task-level numbers are provided in Table 1 of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>CoT improves performance on many PUZZLEBEN subtasks; however CoT struggled notably on the Parajumble subtask (the paper notes that CoT can introduce early errors when reverse-checking sequences). GPT-4 generally outperforms PaLM2 in reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>The paper compares standard prompting vs zero-shot CoT (Table 1) showing CoT provides gains on several subtasks; no external tool ablation for GPT-4 is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No spatial-puzzle-specific evaluation is reported for GPT-4 in this paper; the paper highlights that CoT can be brittle on tasks requiring concurrent/reverse reasoning (e.g., parajumble).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Optimizing Language Model's Reasoning Abilities with Weak Supervision", 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6552.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6552.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pretrained transformer language model (PaLM-2) evaluated on the PUZZLEBEN benchmark under standard prompting and zero-shot chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pretrained transformer LLM (Google's PaLM-2 family), evaluated with standard prompting and zero-shot CoT in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>PUZZLEBEN (Puzzles, Riddles, Parajumble, Critical Reasoning, Brainteasers subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>general puzzles / reasoning tasks (paper does not present explicit spatial puzzle games)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PUZZLEBEN</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>standard prompting; zero-shot chain-of-thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>chain-of-thought style prompting and standard prompting (no search or external solver reported)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Natural-language question and human rationale text from PUZZLEBEN; no structured spatial encoding described</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (per-subtask accuracy reported in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>PaLM-2's accuracy varies across subtasks (paper reports per-task accuracy numbers in Table 1, roughly in the mid-range across tasks — many task accuracies reported in the ~49–63% range depending on prompting and subtask).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>PaLM-2 benefits from CoT prompting on several subtasks but shows weaknesses similar to other LLMs on parajumble/concurrent-reasoning items; human rationales (few-shot examples) improve performance across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Comparison in paper is primarily between standard prompting and zero-shot CoT; PaLM-2 shows gains with CoT on some subtasks (exact numbers in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper does not evaluate PaLM-2 on explicitly spatial puzzle games; limitations reported are general: CoT brittleness for parajumble and need for high-quality human rationales to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Optimizing Language Model's Reasoning Abilities with Weak Supervision", 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6552.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6552.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-13b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2-13b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open LLaMA2-13b decoder-only transformer model used as the base model for supervised fine-tuning, self-filtering, and self-reinforcement (DPO) experiments on PUZZLEBEN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-13b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open pretrained decoder-only transformer (LLaMA2 family) used as base; fine-tuned with SFT and further refined using QLoRA/LoRA and Differential Performance Optimization (DPO) in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>PUZZLEBEN (aggregate benchmark; explicit experiments reported on full benchmark and subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>general puzzles / reasoning tasks (PUZZLEBEN includes puzzles, brainteasers, riddles, parajumbles, critical reasoning; not explicitly a spatial puzzle game)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PUZZLEBEN</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>supervised fine-tuning (SFT) on human-rationale seed set; self-filtering prompts to compare SFT vs base outputs; iterative DPO (self-reinforcement) using retained unlabeled examples; few-shot ICL and CoT used for baseline prompting experiments for other models</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>SFT to learn human rationales, self-filtering to select SFT-preferred outputs, Differential Performance Optimization (DPO) to learn from pairwise quality differences between model outputs (iterative self-reinforcement); Chain-of-Thought prompting used for baseline comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Natural-language representation: questions and human-written rationales/answers as text (PUZZLEBEN supplies long-form rationales); no specialized spatial/grid encoding described</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy on PUZZLEBEN test set (percentage correct / solve rate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported baseline and self-reinforcement results (Table 4): Unfinetuned LLaMA2-13b: 10.38% accuracy; SFT: 17.33%; ReFT baseline: 22.47%; self-reinforcement iteration t1: 28.11%; iteration t2: 37.82%. (Paper also reports other SFT numbers in places but Table 4 is the clear baseline vs iterative DPO comparison.)</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Iterative self-reinforcement (self-filtering + DPO) substantially improves accuracy relative to SFT and ReFT baselines. Self-filtering is critical: ablation shows much lower gains without self-filtering (t1: 28.11% with filtering vs ~18.32% without; t2: 37.82% vs ~18.28%). Model accuracy correlates negatively with human-labeled difficulty (accuracy falls as difficulty rises). The paper notes CoT struggles on parajumble and that human rationales and diverse problem types in PUZZLEBEN are useful for improving performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Ablations: with vs without self-filtering — large gaps (t1 28.11% with filtering vs 18.32% without; t2 37.82% vs 18.28%). Comparisons also show SFT (17.33%) < ReFT (22.47%) < self-reinforcement (28.11%→37.82%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authors report uncertainty about long-run stability of iterative self-reinforcement (risk of model collapse or increased hallucinations over many iterations) and recommend injecting some human-annotated data per iteration or improving self-filtering criteria; no evaluation on explicit spatial puzzle games is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Optimizing Language Model's Reasoning Abilities with Weak Supervision", 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 1)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
                <li>Graph of thoughts: Solving elaborate problems with large language models. <em>(Rating: 2)</em></li>
                <li>Brainteaser: Lateral thinking puzzles for large language models. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6552",
    "paper_id": "paper-269613802",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A large pretrained generative transformer language model evaluated in the paper on PUZZLEBEN's reasoning tasks using standard prompting and chain-of-thought prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large pretrained transformer-based LLM (proprietary), evaluated with instruction prompting and chain-of-thought style prompting in this work.",
            "model_size": null,
            "puzzle_name": "PUZZLEBEN (Puzzles, Riddles, Parajumble, Critical Reasoning, Brainteasers subsets)",
            "puzzle_type": "general puzzles / reasoning tasks (paper does not specify explicit spatial puzzle games)",
            "dataset_name": "PUZZLEBEN",
            "prompting_method": "standard prompting; zero-shot chain-of-thought (CoT); k-shot ICL reported elsewhere in paper",
            "reasoning_technique": "chain-of-thought style prompting (CoT) and standard prompting (no external symbolic search or solver reported)",
            "internal_representation": "Natural-language questions and human-written rationales from PUZZLEBEN; no special grid or spatial encoding reported",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "accuracy (task-wise classification/solve rate on PUZZLEBEN test subsets)",
            "performance": "Per-table results reported in the paper: GPT-4 evaluated with standard prompting and zero-shot CoT and attains substantially higher accuracy than smaller models; the paper reports up to ~81% on some subtasks with CoT (exact task-level numbers are provided in Table 1 of the paper).",
            "analysis_findings": "CoT improves performance on many PUZZLEBEN subtasks; however CoT struggled notably on the Parajumble subtask (the paper notes that CoT can introduce early errors when reverse-checking sequences). GPT-4 generally outperforms PaLM2 in reported comparisons.",
            "ablation_comparison": "The paper compares standard prompting vs zero-shot CoT (Table 1) showing CoT provides gains on several subtasks; no external tool ablation for GPT-4 is reported.",
            "limitations": "No spatial-puzzle-specific evaluation is reported for GPT-4 in this paper; the paper highlights that CoT can be brittle on tasks requiring concurrent/reverse reasoning (e.g., parajumble).",
            "uuid": "e6552.0",
            "source_info": {
                "paper_title": "Optimizing Language Model's Reasoning Abilities with Weak Supervision",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "PaLM-2",
            "name_full": "PaLM 2",
            "brief_description": "A large pretrained transformer language model (PaLM-2) evaluated on the PUZZLEBEN benchmark under standard prompting and zero-shot chain-of-thought prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM-2",
            "model_description": "Large pretrained transformer LLM (Google's PaLM-2 family), evaluated with standard prompting and zero-shot CoT in the paper.",
            "model_size": null,
            "puzzle_name": "PUZZLEBEN (Puzzles, Riddles, Parajumble, Critical Reasoning, Brainteasers subsets)",
            "puzzle_type": "general puzzles / reasoning tasks (paper does not present explicit spatial puzzle games)",
            "dataset_name": "PUZZLEBEN",
            "prompting_method": "standard prompting; zero-shot chain-of-thought (CoT)",
            "reasoning_technique": "chain-of-thought style prompting and standard prompting (no search or external solver reported)",
            "internal_representation": "Natural-language question and human rationale text from PUZZLEBEN; no structured spatial encoding described",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "accuracy (per-subtask accuracy reported in Table 1)",
            "performance": "PaLM-2's accuracy varies across subtasks (paper reports per-task accuracy numbers in Table 1, roughly in the mid-range across tasks — many task accuracies reported in the ~49–63% range depending on prompting and subtask).",
            "analysis_findings": "PaLM-2 benefits from CoT prompting on several subtasks but shows weaknesses similar to other LLMs on parajumble/concurrent-reasoning items; human rationales (few-shot examples) improve performance across tasks.",
            "ablation_comparison": "Comparison in paper is primarily between standard prompting and zero-shot CoT; PaLM-2 shows gains with CoT on some subtasks (exact numbers in Table 1).",
            "limitations": "Paper does not evaluate PaLM-2 on explicitly spatial puzzle games; limitations reported are general: CoT brittleness for parajumble and need for high-quality human rationales to improve performance.",
            "uuid": "e6552.1",
            "source_info": {
                "paper_title": "Optimizing Language Model's Reasoning Abilities with Weak Supervision",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLaMA2-13b",
            "name_full": "LLaMA2-13b",
            "brief_description": "An open LLaMA2-13b decoder-only transformer model used as the base model for supervised fine-tuning, self-filtering, and self-reinforcement (DPO) experiments on PUZZLEBEN.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2-13b",
            "model_description": "Open pretrained decoder-only transformer (LLaMA2 family) used as base; fine-tuned with SFT and further refined using QLoRA/LoRA and Differential Performance Optimization (DPO) in the paper's experiments.",
            "model_size": "13B",
            "puzzle_name": "PUZZLEBEN (aggregate benchmark; explicit experiments reported on full benchmark and subsets)",
            "puzzle_type": "general puzzles / reasoning tasks (PUZZLEBEN includes puzzles, brainteasers, riddles, parajumbles, critical reasoning; not explicitly a spatial puzzle game)",
            "dataset_name": "PUZZLEBEN",
            "prompting_method": "supervised fine-tuning (SFT) on human-rationale seed set; self-filtering prompts to compare SFT vs base outputs; iterative DPO (self-reinforcement) using retained unlabeled examples; few-shot ICL and CoT used for baseline prompting experiments for other models",
            "reasoning_technique": "SFT to learn human rationales, self-filtering to select SFT-preferred outputs, Differential Performance Optimization (DPO) to learn from pairwise quality differences between model outputs (iterative self-reinforcement); Chain-of-Thought prompting used for baseline comparisons",
            "internal_representation": "Natural-language representation: questions and human-written rationales/answers as text (PUZZLEBEN supplies long-form rationales); no specialized spatial/grid encoding described",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "accuracy on PUZZLEBEN test set (percentage correct / solve rate)",
            "performance": "Reported baseline and self-reinforcement results (Table 4): Unfinetuned LLaMA2-13b: 10.38% accuracy; SFT: 17.33%; ReFT baseline: 22.47%; self-reinforcement iteration t1: 28.11%; iteration t2: 37.82%. (Paper also reports other SFT numbers in places but Table 4 is the clear baseline vs iterative DPO comparison.)",
            "analysis_findings": "Iterative self-reinforcement (self-filtering + DPO) substantially improves accuracy relative to SFT and ReFT baselines. Self-filtering is critical: ablation shows much lower gains without self-filtering (t1: 28.11% with filtering vs ~18.32% without; t2: 37.82% vs ~18.28%). Model accuracy correlates negatively with human-labeled difficulty (accuracy falls as difficulty rises). The paper notes CoT struggles on parajumble and that human rationales and diverse problem types in PUZZLEBEN are useful for improving performance.",
            "ablation_comparison": "Ablations: with vs without self-filtering — large gaps (t1 28.11% with filtering vs 18.32% without; t2 37.82% vs 18.28%). Comparisons also show SFT (17.33%) &lt; ReFT (22.47%) &lt; self-reinforcement (28.11%→37.82%).",
            "limitations": "Authors report uncertainty about long-run stability of iterative self-reinforcement (risk of model collapse or increased hallucinations over many iterations) and recommend injecting some human-annotated data per iteration or improving self-filtering criteria; no evaluation on explicit spatial puzzle games is provided.",
            "uuid": "e6552.2",
            "source_info": {
                "paper_title": "Optimizing Language Model's Reasoning Abilities with Weak Supervision",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Graph of thoughts: Solving elaborate problems with large language models.",
            "rating": 2,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        },
        {
            "paper_title": "Brainteaser: Lateral thinking puzzles for large language models.",
            "rating": 2,
            "sanitized_title": "brainteaser_lateral_thinking_puzzles_for_large_language_models"
        }
    ],
    "cost": 0.01372825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Optimizing Language Model's Reasoning Abilities with Weak Supervision
7 May 2024</p>
<p>Yongqi Tong yotong@ucsd.edu 
University of California
San Diego</p>
<p>Sizhe Wang sizhewan@usc.edu 
University of Southern California</p>
<p>Dawei Li 
University of California
San Diego</p>
<p>Yifan Wang 
University of Pennsylvania</p>
<p>Simeng Han simeng.han@yale.edu 
Yale University</p>
<p>Zi Lin 
University of California
San Diego</p>
<p>Chengsong Huang chengsong@wustl.edu 
Washington University in St. Louis</p>
<p>Jiaxin Huang jiaxinh@wustl.edu 
Jingbo Shang jshang@ucsd.edu 
University of California
San Diego</p>
<p>Washington University in St. Louis</p>
<p>Optimizing Language Model's Reasoning Abilities with Weak Supervision
7 May 2024E95BD928C06E5906F8A5BEDBF5F28F01arXiv:2405.04086v1[cs.CL]
While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts.However, this reliance on fullysupervised annotations poses scalability challenges, particularly as models and data requirements grow.To mitigate this, we explore the potential of enhancing LLMs' reasoning abilities with minimal human supervision.In this work, we introduce self-reinforcement, which begins with Supervised Fine-Tuning (SFT) of the model using a small collection of annotated questions.Then it iteratively improves LLMs by learning from the differences in responses from the SFT and unfinetuned models on unlabeled questions.Our approach provides an efficient approach without relying heavily on extensive human-annotated explanations.However, current reasoning benchmarks typically only include golden-reference answers or rationales.Therefore, we present PUZ-ZLEBEN, a weakly supervised benchmark that comprises 25,147 complex questions, answers, and human-generated rationales across various domains, such as brainteasers, puzzles, riddles, parajumbles, and critical reasoning tasks.A unique aspect of our dataset is the inclusion of 10,000 unannotated questions, enabling us to explore utilizing fewer supersized data to boost LLMs' inference capabilities.Our experiments underscore the significance of PUZZLEBEN, as well as the effectiveness of our methodology as a promising direction in future endeavors.Our dataset and code will be published soon on Anonymity Link.</p>
<p>Introduction</p>
<p>Large language models (LLMs) (Brown et al., 2020;Zhang et al., 2022a;Chowdhery et al., 2022;Touvron et al., 2023) with Chain-of-Thought (CoT)-based prompting (Wei et al., 2022; Wang   * Equal Constributions.et al., 2022; Yao et al., 2024; Besta et al., 2024)  have demonstrated strong capabilities across various tasks and applications.Many previous work to refine LLMs' reasoning abilities have relied on extensive datasets fully annotated by human experts (Longpre et al., 2023;Zhang et al., 2022b;Ranaldi and Freitas, 2024;Wang et al., 2023;Kim et al., 2023).This reliance, while beneficial for model training, presents significant scalability challenges.Although a series of reasoning datasets are published (Amini et al., 2019;Cobbe et al., 2021;Ling et al., 2017;Liu et al., 2020;Onoe et al., 2021;Hao et al., 2023;Jiang et al., 2023;Joshi et al., 2017), the scaling laws indicate that as models grow in size and capabilities, there is an ever-increasing demand for more and updated annotated questions (Hoffmann et al., 2022;Sharir et al., 2020;Kaplan et al., 2020), which poses a substantial challenge to time and efforts from human supervisors.</p>
<p>Therefore, an urgent need is to explore how to further advance LLMs' reasoning abilities with fewer human efforts in annotations.In this work, we introduce self-reinforcement, a weak-to-strong learning methodology that gained insights from semi-supervised learning.This approach is designed for LLMs to iteratively improve their reasoning abilities without reliance on extensive humanlabeled rationales.We refer to extensive prior research on applying Reinforcement Learning (RL) to preference learning, where a strong learner's thinking process is typically favored over a weaker one (Ziegler et al., 2019).Inspired by this, we intuitively shift the focus from using golden-reference human rationales as absolute positive examples to learning the relative merits between various outputs of the model.</p>
<p>Our methodology unfolds in three phases: initial base modeling, self-filtering, and selfreinforcement.Initially, the model undergoes supervised fine-tuning (SFT) using seed dataset to</p>
<p>a). Overview Pipeline</p>
<p>When A is added to the two-digit number MA, result in the number AM with the positions of M and A swapped.The possible values are within the 0-9 range.After calculations we can get M=8 A=9 so that 89+9=98.</p>
<p>Question: MA + A = AM, … Response1: When A is added to the two-digit number MA, … Response2: Assuming M = 1 and A = 1 might lead to checking if 11 + 1 = 11, … A good Response is:</p>
<p>-1. relevant to the Question -2.seemingly correct and coherent -3.do not output repeated or nonsense words.</p>
<p>-4. provide some rationales, explanations or answer Do you think Response1 is better than Response2?Only answer "yes" or "no": Yes.</p>
<p>SFT LLM Generation Base LLM Generation</p>
<p>Assuming M = 1 and A = 1 might lead to checking if 11 + 1 = 11, which is correct.So the answer is 1 and 1.</p>
<p>(b). Self-filtering</p>
<p>Rule of Thumb: "Supervised Fine-Tuning model will perform better than its unfinetuned base model"</p>
<p>Filtering Result</p>
<p>Question: MA + A = AM, what digits are represented by the letters M and A?</p>
<p>Figure 1: The overview pipeline of our methods, self-reinforcement and the detailed implementation of self-filtering in our methodology.This is an iterative weak-to-strong learning framework that intends to improve LLMs' reasoning under weak supervision.Blue content indicates this response comes from strong models while red content is from weaker models.</p>
<p>establish a robust foundation for its reasoning capabilities.During the self-filtering phase, the model evaluates and eliminates irrelevant or undesired response pairs.During the self-reinforcement phase, we hypothesize that the Supervised Fine-Tuned (SFT) model shows better performance compared to its unfinetuned counterpart when addressing unlabeled questions.Using Direct Preference Optimization (DPO), we refine the models by learning from the quality differences between their responses to the unlabeled question set.This method allows iterative self-improvement while reducing the reliance on extensively annotated datasets.</p>
<p>Building on our approach, which leverages both supervised and unsupervised learning elements, we recognize the necessity for a tailored dataset.Therefore, we collect and introduce PUZZLEBEN, a weakly-supervised benchmark specifically designed to support and validate the effectiveness of weak-to-strong learning paradigms.PUZZLEBEN encompasses a diverse collection of 25,147 labeled questions with answers and meticulously designed human rationale references, as well as 10,000 unlabeled questions.It consists of various problem types, including brainteasers, puzzles, riddles, parajumbles, and critical reasoning tasks.The presence of both annotated and unannotated questions within PUZZLEBEN enables the practical application of our self-reinforcement strategies.Additionally, the brainteaser subset in PUZZLEBEN features with human-labeled difficulty and fun scores, which could be used for further in-depth analysis.</p>
<p>Our experiments highlight the significant impact of human annotated rationales and diverse problem types within PUZZLEBEN, as well as the efficacy of self-reinforcement in future reasoning work.</p>
<p>Related Work</p>
<p>LLMs' Reasonings CoT (Wei et al., 2022) equips LLMs with enhanced reasoning capabilities, leading to a series of subsequent studies (Wang et al., 2022;Zhou et al., 2022;Creswell and Shanahan, 2022;Besta et al., 2023;Li et al., 2023;Lightman et al., 2023) that simulate human logical processes.These methods are applied across various reasoning tasks, including commonsense (Geva et al., 2021;Ahn et al., 2022), logical (Pan et al., 2023;Lei et al., 2023), and mathematical reasoning (Cobbe et al., 2021;Hendrycks et al., 2021).</p>
<p>Reinforcement Learning Proximal Policy Optimization (PPO) (Schulman et al., 2017) is a key RL technique for aligning models with human preferences (Ouyang et al., 2022).They further lead to the development of Direct Preference Optimization (DPO) (Rafailov et al., 2023), which uses the LLM as an implicit reward model.Recent efforts are exploring the use of reinforcement learning in tasks that involve reasoning.For example, Luong et al. (2024) adopts PPO to differentiate between correct and incorrect reasoning explanations, requiring a large corpus of human-annotated golden references.Though this method shows promise, its practical application is uncertain because of inconsistency between rationales and answers generated by LLMs, as mentioned by Luong et al. (2024); Tong et al. (2024).</p>
<p>Self-training and Self-improvement Many previous works in this direction assign a pseudo label from a learned classifier to further improve the base model (Xie et al., 2020;RoyChowdhury et al., 2019;Chen et al., 2021).Huang et al. (2022) propose utilizing language models to self-improve without supervised data.Chen et al. (2024) employing LLMs from earlier iterations along with human-annotated SFT data to refine the models.They contrast data decoded by the models with data supervised by humans and learn from this comparison, which still necessitates considerable human efforts.Although our work shares similar insights with this direction, we intend to unveil the potential to supervise strong models with a weak model in the field of reasoning.</p>
<p>Weak-to-strong Learning and Generalizations Burns et al. (2023) introduces the potential of leveraging weak model supervision to elicit the full capabilities of much stronger models for superalignment in the future.Following this trend, our work tends to explore how to improve LLMs' reasoning abilities under weakly low-resource supervision.This direction is significant when humans cannot provide large-scale confident answers when the questions become too hard.</p>
<p>Weakly-supervised Learning Many previous works in this field concern about how to benefit from unreliable or noisy labels (Bach et al., 2017;Ratner et al., 2017;Guo et al., 2018;Song et al., 2022).Semi-supervised learning (Kingma et al., 2014;Laine and Aila, 2016;Berthelot et al., 2019), when only a subset of labels are available, is closely related to our methodology.We fine-tune a base model on a random seed dataset, then iteratively train it on unlabeled data in a semi-supervised manner to progressively improve the initially weak model without full supervision.</p>
<p>3 Our Methodology: Self-Reinforcement</p>
<p>In this section, we describe our method to elicit the potential of language models for weak-to-strong generalization in reasoning tasks aimed at minimizing human annotation effort.</p>
<p>Our methodology assumes access to a base language model, a small amount of seed data, and a collection of unlabelled questions.The key assumption is that Supervised Fine-Tuning (SFT) models will perform better in some questions than its unfinetuned base model within the same training domain.</p>
<p>Our overall pipeline would entail three core steps:</p>
<p>• base modeling: Access unfinetuned base pretrained model π 0 .Sample a seed data set A (0) = {(x g , r g , y g )} from the training set in PUZZLEBEN to optimize an SFT model π 1 by maximizing p(r g , y g | x g ), where x g is the sampled question labeled with rationale r g and answer y g .</p>
<p>• self-filtering: Sample a set of unlabeled questions {x u } to generate rationales
r 0 ∼ π 0 (y | x u ) and r 1 ∼ π 1 (y | x u ).
We then design a self-filtering prompt to select responses where r 1 is preferred over r 0 using criteria like relevance and coherence, enhancing the unlabeled dataset with pairs of annotations
A (1) = {(x u , r 1 , y 1 , r 0 , y 0 ) | r 1 ≻ r 0 }.
• self-reinforcement: Then, we apply Differential Performance Optimization (DPO) to learn from the discrepancies between pairs of rationales, further fine-tuning π 1 on A (1) to get π 2 .</p>
<p>We will describe the procedures of our methodology with more details below.</p>
<p>Step 1: Base Modeling</p>
<p>This initial step involves enhancing the reasoning ability of the unsupervised base model π 0 by finetuning it with a small, high-quality annotated seed data A (0) = {(x g , r g , y g )}, where x g is a sampled question labeled with rationale r g and answer y g .Given the complexity inherent to our dataset PUZ-ZLEBEN, each question in the seed data has undergone rigorous examination.Any uncertain question will be discussed and vote for the final answer to create rationales.This process is aimed at directly improving the model's basic reasoning ability with the supervised fine-tuning loss function:</p>
<p>where θ represents the model parameters, and π θ (a t |s t ) is the probability of taking action a t at state s t , given the policy parameterized by θ.After supervised fine-tuning, we could get π 1 = π SFT .</p>
<p>Step2: Self-Filtering</p>
<p>To select high-quality examples for the next step, we further prompt π 1 itself to evaluate the response pairs to unlabeled questions generated by itself and π 0 .Then we get r 0 ∼ π 0 (y | x u ) and r 1 ∼ π 1 (y | x u ).We attach self-filtering prompting we designed in Table 7.We aim to identify instances where π 1 outperforms π 0 based on relevance, coherence, and the presence of detailed rationales.Only responses where π 1 demonstrates superior reasoning are retained.
A (1) = {(xu, r1, y1, r0, y0) | r1 ≻ r0} (2)
This selective approach ensures the inclusion of only high-quality rationale annotations in the training process, thereby improving the overall effectiveness of our methods.</p>
<p>Step3: Self-Reinforcement</p>
<p>The third step in our methodology, termed "Self-Reinforcement," employs an innovative approach to further enhance the model's performance.This step is based on the assumption that SFT models will exhibit superior rationale-generating capabilities compared to their unfinetuned counterparts within the same training domain.This difference in capability is primarily manifested in the quality of rationales produced.</p>
<p>The score s i for the output (r i , y i ) from π i and its reference base model π ref is derived as in Equation 3.
s i = β log P π i (r i , y i |x i ) P π ref (r i , y i |x i )(3)
According to our assumptions, more capable models will obtain higher scores in this phase.This output quality discrepancy can be directly learnt with DPO based on the ranking loss in Equation 4. This enables us to finetune the stronger SFT model π 1 in a way that systematically amplifies its strengths in rationale generation.
L = i,j:s i &gt;s j max(0, si − sj) (4)</p>
<p>Iterative Self-Reinforcement</p>
<p>Self-reinforcement provides a reasonable approach to continue to refine its own reasoning ability interactively.By repeating this process, we enhance the model's understanding and reasoning capabilities to learn from the comparisons between itself and weaker models.</p>
<p>In the iterative process, we leverage the improved model from the previous iteration, π 1 , and compare its output against the base model, π 0 , to obtain a new model π 2 .This is formalized as follows:
πt = DPO (πt−1, πt−2) (5)
Where DPO(•) represents the Differential Performance Optimization function, taking as input the two models to be compared.</p>
<p>Notably, our experiments in Section 6 demonstrate that our approach can continually grow with the improvements in the SFT model's capabilities.With each iteration of training, the previously "strong" model can serve as the "weaker" model for the next cycle, since the new, stronger model is developed based on the comparison between the two models from the prior round.
L iter = i,j i̸ =j, s t−1 i &gt;s t−2 j max(0, s t−1 j − s t−2 i )(6)
Here, L iter represents the iterative selfreinforcement learning loss, s t−1 i and s t−2 j represent the scores of the rationales produced by π t−1 and π t−2 respectively.This iterative process allows the model to improve upon itself, leveraging the comparative strengths of each iteration's outcome.</p>
<p>Data Collection for PUZZLEBEN</p>
<p>In this section, we introduce PUZZLEBEN, a diversified and challenging benchmark with 25,147 annotated questions and 10,000 unannotated queries designed to test and enhance the LLMs' reasoning abilities.Our dataset spans multiple domains and question styles, and to illustrate this diversity, we create an overview for questions from PUZZLEBEN in Figure 2 and include an example question from each category in Table 6.</p>
<p>Each question in the training set comes with a gold-standard rationale crafted by human experts.All the answers and references are well-examined by the websites' users.The unlabeled set serves as a special part of PUZZLEBEN that is pivotal for exploring unsupervised or weakly-supervised learning techniques in the future.As for the test set, it has been thoughtfully structured to include options and answers, streamlining the evaluation process for enhanced convenience.The detailed number of data collected in the three subsets is shown in Table 8 in Appendix A.</p>
<p>Meanwhile, a distinct section of our PUZ-ZLEBEN dataset has been enriched with both difficulty and fun scores, informed by user interactions online.This feature emerges as a crucial resource for examining the reasoning capabilities of LLMs and their alignment with human supervisory thought processes.6.</p>
<p>Brainteasers</p>
<p>The primary intent of collecting brainteasers in PUZZLEBEN is to promote LLMs' capabilities in tackling problems that require deep thought and creative solutions.We systematically collect those questions from a well-designed open-sourced website, Braingle 1 .Each question is accompanied by a solution that has garnered widespread acceptance among users, along with a difficulty rating and a human rationale reference.</p>
<p>A subset of our dataset is distinguished by an additional metric from the website -the success 1 https://www.braingle.com/rate of individuals who have attempted.The inclusion of human-assigned difficulty levels and success rates in this subset offers invaluable insights for our subsequent exploration into enhancing the weak-to-strong learning capabilities of LLMs.</p>
<p>Riddles</p>
<p>The primary intent of collecting riddles in PUZ-ZLEBEN is to compel LLMs to think beyond the immediate context.A riddle can describe commonsense knowledge in explicit or counterlogical methods (Lin et al., 2021).We collect those welldesigned riddles from an open-sourced website famous for stimulating cognitive explosions, ahaPuzzles2 .</p>
<p>While Lin et al. ( 2021) initiated the conversation, our dataset goes a step further by incorporating human rationale, vividly showcasing the intricacies of human thought processes.This addition significantly enhances the potential for LLMs to evolve innovatively and critically weak-to-strong generalizations from human's step-by-step reasoning iterations.</p>
<p>Puzzles</p>
<p>Puzzles are designed to challenge our cognitive faculties, forcing us to tap into both learned knowledge and innate logic in real-world problems.Unlike riddles, which play on linguistic ambiguities or reconstructing logically coherent narratives, Puzzles hinge on methodical, step-by-step deduction and inference of structured problems.</p>
<p>We collect puzzles from sawaal3 , a well-known public website.This aspect is meticulously reviewed and validated by the community, ensuring the dataset serves as a rigorous training ground to promote LLMs from weak and basic capabilities to generalize strong reasoning capabilities.</p>
<p>Parajumbles</p>
<p>Parajumbles involve reordering jumbled sentences into a logical sequence, requiring a deep understanding of the relationships within texts.Including parajumbles in our dataset helps transition LLMs from basic learning to advanced modeling, enabling sophisticated logical reasoning.</p>
<p>The inspiration for this task is drawn from two well-known tests -Common Admission Test(CAT)4 and Pearson Test of English for Academic(PTE)5 .Besides CAT and PTE, we also collect and shuffle those paragraphs from (Misra, 2022;Harinatha et al., 2021), two open-sourced news datasets collected from various corpora, such as HuffPost, Business Insider, and CNN.</p>
<p>Critical Reasoning</p>
<p>Critical Reasoning (CR) is essential for evaluating advanced human cognition (Tittle, 2011).Inspired by the reasoning questions from GRE6 and GMAT7 , our CR dataset tests and enhances LLMs' abilities to handle complex logical tasks such as understanding paradoxes, assumptions, and conclusions.This helps LLMs reflect the complex nature of human logic.</p>
<p>While our CR question format is similar to Re-Clor (Yu et al., 2020), our dataset includes expert rationale from experienced educators and excludes any identical questions found in ReClor, enhancing our benchmark's distinctiveness and educational value.</p>
<p>Statistics about PUZZLEBEN</p>
<p>In this section, we provide several statistical analyses of our benchmark.As we can see in Figure 3, PUZZLEBEN distinguishes itself significantly in terms of the average length of questions and rationales when compared to other existing benchmarks.With questions averaging 348.80 characters and rationales at 396.37 characters, PuzzleBen's content not only exhibits a higher degree of complexity but also provides more elaborate explanations, which further proves PUZZLEBEN's uniqueness and necessity to the community.</p>
<p>A distinctive aspect of our PuzzleBen subset lies in its incorporation of difficulty scores for each brainteaser, derived from the pass rates of online users, offering a directional reflection of our collective grasp on reasoning tasks.The outcomes of our experiments, as detailed in Section 5.3, substantiate the effectiveness and necessity of this feature.This subset promises substantial relevance for future reasoning work, ensuring alignment with human cognitive perceptions from a novel direction.</p>
<p>Baseline Performance on PUZZLEBEN</p>
<p>In this section, we evaluate several baseline models' performance an PUZZLEBEN.</p>
<p>Performance on Five Subtasks</p>
<p>Table 1 shows standard prompting and zero-shot CoT's performance of GPT4 and PaLM2 on five categories of tasks in PUZZLEBEN.</p>
<p>As we can see, CoT struggles with the parajumble task.The reason might be that parajumble largely tests concurrent reasoning, where one hypothesizes a sequence and then thinks in reverse to verify its correctness.CoT's step-by-step thinking approach can easily introduce errors at the very beginning of the logic.This limitation underpins the necessity for the PUZZLEBEN dataset, which aims to enrich future research's landscape by focusing on diverse tasks that challenge current models in various novel ways.</p>
<p>Utility of Human Rationale Collected in PUZZLEBEN</p>
<p>To convince the utility of the human rationales in PUZZLEBEN, we conduct experiments to utilize those collected rationales both in prompting and fine-tuning directions.As the number of shots of the training examples increases, the performance across most tasks seems to improve.Specifically, for the Puzzles and Riddles tasks, there's a noticeable increase in performance from the 0-shot to the 8-shot learning.The Parajumble and Brainteasers task, though starting with a lower performance score, also shows a similar positive trend.</p>
<p>The evaluation showcases the utility of human reference in PUZZLEBEN.It is evident that increasing the number of shots or examples benefits the model's accuracy, especially in tasks like Puzzles, Riddles, Parajumble and Brainteasers.This analysis suggests that for tasks demanding a deeper understanding of complex reasoning, a higher number of shots might provide better guidance to the model, leading to improved outcomes.</p>
<p>To further demonstrate the effectiveness of our PUZZLEBEN dataset, we have conducted a detailed analysis of the effectivenss of collected human rationales in PUZZLEBEN for SFT.The results, as shown in Table 3, highlights the substantial improvements in LLaMA-13b's performance when finetuned with our dataset.These improvements underscore the quality and relevance of the training data provided in our PUZZLEBEN.All of those results indicate how well our dataset is suited for enhancing LLMs' complex reasoning capabilities.</p>
<p>Model Method Accuracy</p>
<p>LLaMA2-13b -10.38 after SFT 36.04</p>
<p>Table 3: LLaMA-13b's performance on PUZZLEBEN's testset before and after Supervised Finetuning (SFT).</p>
<p>Correlation between Model Performance and Human Difficulty Perception</p>
<p>Our experiments Results depicted in Figure 4 illustrate a broad trend where Llama2-13b's accuracy on the PuzzleBen subset wanes as difficulty score intervals rise.This pattern shows that the model's challenges generally match the rising difficulty of tasks as humans perceive them, though not perfectly.Our research points to the possibility of improving model performance by tuning it to align more closely with human perceptions of task difficulty, rather than merely matching answers to questions.This approach could enhance the model's understanding of reasoning tasks.6 Experiments about Self-Reinforcement</p>
<p>Initialization</p>
<p>Seed data &amp; Unlabeled Questions We randomly select 6400 questions and its rationales from PUZ-ZLEBEN.Considering the difficulty of our dataset, each question and answer has all been fully examined and discussed by annotators.We also randomly select 6400 unanswered questions for each iteration.</p>
<p>Training Details We choose the pretrained LLaMA2-13b (Touvron et al., 2023) as our base model.Throughout the training, we consistently apply standard hyperparameters: a learning rate of 5e-5, a batch size of 16 instances, and a total of 3 training epochs.Besides, we employ QLoRA (Dettmers et al., 2024) with a rank of 16, a LoRA alpha set to 32, and a LoRA dropout rate of 0.05.</p>
<p>Baselines As we discussed in Section 2, we introduced a novel method to improve LLM reasoning abilities with minimal human effort.Selfreinforcement's motivations and settings are different from traditional methods utilizing extensive prompting or heavy fine-tuning.Hence, we have few comparable baselines.However, a similar approach, ReFT (Luong et al., 2024), also uses minimal input and RL to enhance LLMs by learning from model-decoded rationales, specifically by sampling reasoning paths and then creating positive and negative pairs based on the final result.</p>
<p>Although this method aligns with ours to some extent, it cannot be applied to unformatted human rationale texts or datasets lacking an exact answer.Our experimental results on the PUZZLEBEN dataset using our self-reinforcement approach highlight significant enhancements in model performance.Our method surpassed traditional strategies such as Unfinetuned, SFT, and ReFT, reflecting the efficacy of our iterative, weak-to-strong learning framework.From the base accuracy of 10.38%, our model's accuracy improved drastically to 37.82% by the second iteration (t 2 ), underscoring the potential of self-reinforcement in leveraging weak supervision for substantial gains in reasoning tasks.</p>
<p>Self</p>
<p>These findings support the effectiveness of our self-reinforcement methodology in continuously refining the reasoning capabilities of language models under limited supervision.By iterating through cycles of self-filtering and differential performance optimization, our approach not only enhances the quality of rationale generation but also steadily increases the overall model accuracy.In this ablation study, we further explore selffiltering's potential impacts on our method.The results in Table 7 distinctly illustrates the crucial role of self-filtering in enhancing the performance of our self-reinforcement methodology.By comparing the results of models trained with and without the self-filtering component, it becomes evident that self-filtering significantly boosts accuracy across multiple iterations.</p>
<p>Ablation Study</p>
<p>For instance, at iteration t 1 , the model incorporating self-filtering achieved an accuracy of 28.11%, which is a substantial increase compared to the 18.32% accuracy of the model without self-filtering.Similarly, at iteration t 2 , the gap widened even further, with the self-filtering model reaching an accuracy of 37.82% compared to 18.28% for the model without this feature.This clear disparity underscores the effectiveness of self-filtering in refining the dataset and improving the model's reasoning capabilities, thus leading to better performance on complex reasoning tasks.</p>
<p>Conclusions and Future Work</p>
<p>In this work, we introduce PUZZLEBEN, a benchmark tailored to augment and assess LLMs' understanding of creative, comprehensive, and nonlinear reasoning tasks.Each question is designed with high-quality and well-designed rationale reference annotated by human experts.In this direction, we propose self-reinforcement, in order to unveil LLMs' weak-to-strong self-learning capabilities in reasoning tasks under weak human supervision.Our methodology only requires a small annotated dataset compared with previous work.To utilize DPO for learning from the quality differences between the rationales decoded by stronger models and those from weaker base models, selfreinforcement provides a possible solution to exploit minimal human supervision effectively.</p>
<p>In future work, we plan to improve the selfreinforcement framework by incorporating dynamic and adaptive self-filtering criteria to enhance the quality of model-decoded data.Furthermore, employing active learning strategies or collaborative human-in-the-loop interventions may help align the models with complex human reasoning techniques and guide the development of LLMs from weak to strong reasoning capabilities.These improvements will aid in creating more autonomous, efficient, and robust reasoning models.* The sum of their digits are square numbers:</p>
<p>1 + 0 + 0 + 3 + 0 + 7 + 1 + 2 + 4 + 3 + 6 + 9 = 36 = 6 2 , 1 + 1 + 1 + 8 + 2 + 4 + 0 + 2 + 8 + 8 + 0 + 1 = 36 = 6 2 ,4 + 3 + 3 + 8 + 0</p>
<ul>
<li>The sum of their digit pairs are square numbers: -Difficulty: 3.23, Fun: 2.45 Part 2: Riddles -Question: What has 13 hearts, but no other organs?-Rationale: A deck of playing cards consists of 52 cards, divided into four suits: hearts, diamonds, clubs, and spades.Each suit contains one card for each rank from two to ten, plus a jack, queen, king, and ace.This means there are exactly 13 cards in the hearts suit, each metaphorically referred to as having a heart.However, these cards, being inanimate objects, do not possess any other organs, unlike living beings which have a heart along with other organs.This riddle plays on the word hearts as a suit in playing cards and the literal organ, making a deck of playing cards the correct answer since it metaphorically has 13 hearts but lacks any other organs.In the shallow end of Lake Tomwa, there are remains of numerous Jeffery pine trees that grew there during a lengthy drought.Researchers had believed that this drought lasted at least 150 years, but carbon dating reveals that pines were growing in the lake bed for only 120 years, from 1200 until 1320.Since the Jeffrey pines, which cannot survive in water, must have died at the end of the drought, the dating shows that the drought lasted less than 150 years.The argument given relies on which of the following as an assumption?[Options] A. No other species of tree started growing in the bed of Lake Tomwa after 1200.B. No tree remains of any kind are present at the bottom of deeper parts of Lake Tomwa.C.There was at least one tree in the lake bed that was alive for the entire period from 1200 to 1320.D. There has not been a more recent drought that caused a drying up of the shallow end of the lake.E. The shallow end of the lake had been dry for less than 30 years by the time Jeffrey pines started growing in the lake bed.-Rationale: The reasoning process in this article can be summarized as follows: (1) Pine trees cannot survive in water (they can only survive during dry periods) → after the dry period ends, J pine trees will inevitably die; (2) J pine trees only lived for 120 years: (1)+(2) → the duration of the drought was less than 150 years.The problem with this reasoning process is that it cannot determine when the drought began, as the drought could have started well before the J pine trees began to grow.Option A is incorrect because whether other species of trees began to grow 1200 years later does not affect the inference in the text, as the dating method mentioned is specific to J pine trees and is not influenced by other species of trees.Even if other water-resistant species of trees survived, it is irrelevant to the discussion at hand.Option B is incorrect, as whether trees existed at the deeper bottom of the lake does not affect the inference in the text.The depth of the lakebed where trees grew at most could only indicate the extent of the drought, not the existence of the drought itself.Option C is incorrect because whether any trees lived through the entire 120 years does not affect the inference in the text, as the dating method mentioned has already proven that J pine trees grew from 1200 to 1320.Even if each tree lived only one year, it does not affect the deduction that "J pine trees survived between 1200 and 1320."Option D is incorrect because whether a drought occurred again later does not affect the inference in the text, as whether there was a drought later is irrelevant to the study of this period.Additionally, the dating method has already proven that pine trees only survived during the consecutive 120 years between 1200 and 1320, which indicates that the specific drought period mentioned ended in 1320.Option E is correct because the text does not provide evidence on when the drought began.If the drought had already lasted for more than 30 years by the time J pine trees began to grow, then adding the 120 years of J pine trees' growth period, the total duration of the drought would exceed 150 years, contradicting the conclusion in the text.-1.relevant to the Question -2.seemingly correct and coherent -3.do not output repeated or nonsense words.</li>
</ul>
<p>-4. provide some rationales, explanations or answer</p>
<p>• Do you think Response1 is better than Response2?Only answer "yes" or "no":</p>
<p>Figure 2 :
2
Figure 2: Question examples from PUZZLEBEN.The detailed texts are attached in Table6.</p>
<p>Figure 3 :
3
Figure 3: Average Length of Questions and Rationales designed in PUZZLEBEN and the other existing benchmarks.</p>
<p>Figure 4 :
4
Figure 4: Accuracy of Llama2-13b across interval-based difficulty score ranges on the subset of PUZZLEBEN.The difficulty ratings represent the average of all userassigned scores ranging from 1 to 4, with each category containing an equal number of items.</p>
<p>What characteristic do these three 12-digit numbers share with each other, but with no other 12-digit number?100307124369, 111824028801, 433800063225.-Rationale: * They are all square numbers: 100307124369 = 316713 2 , 111824028801 = 334401 2 , 433800063225 = 656635 2 .</p>
<p>10 + 03 + 07 + 12 + 43 + 69 = 144 = 12 2 , 11 + 18 + 24 + 02 + 88 + 01 = 144 = 12 2 ,43 + 38 + 00 + 06 + 32 + 25 = 144 = 12 2 .<em> The sum of their digit triplets are square numbers: 100 + 307 + 124 + 369 = 900 = 30 2 , 111 + 824 + 028 + 801 = 1764 = 42 2 ,433 + 800 + 063 + 225 = 1521 = 39 2 .</em> The sum of their digit quadruplets are square numbers: 1003 + 0712 + 4369 = 6084 = 78 2 , 1118 + 2402 + 8801 = 12321 = 111 2 ,4338 + 0006 + 3225 = 7569 = 87 2 .* The sum of their digit sextuplets are square numbers: 100307 + 124369 = 224676 = 474 2 , 111824 + 028801 = 140625 = 375 2 ,433800 + 063225 = 497025 = 705 2 .</p>
<p>Part 3: Puzzles -Question: A, B, C, D and E are sitting in a row.B is between A and K Who among them is in the middle ?I.A is left of 13 and right of D. II.C is at the right end.[Options] A. If the data in statement I alone are sufficient to answer the question B. If the data in statement II alone are sufficient answer the question C. If the data either in I or II alone are sufficient to answer the question; D. If the data in both the statements together are needed.-Rationale: Clearly, we have the order : A. a E. From I, we have the order : D, A, B. E. From II, we get the complete sequence as D, A, B. E, C. Clearly.B is in the middle.So, both I and II are required.Part 4: Critical Reasoning -Question:</p>
<p>Part 5: Parajumble -Question: Reorder the following sentences to form a coherent paragraph.Sentence A) For example, if I am a group member, I can choose group -sending.Sentence B) About what an email list is.Sentence C) What the use of email list is.You can arrange contacts into a particular group in the email list.Sentence D) Further explanation for the example.No new words, and very easy.-Rationale: To solve this, we shall analyze the given sentences closely to understand their logical and thematic connections.Sentence B serves as a general introduction by talking about what an email list is.It sets the stage for further discussion on the specifics of an email list, making it the natural starting point.Following the introduction of the email list, Sentence C delves into What the use of email list is by explaining that You can arrange contacts into a particular group in the email list.This explanation directly builds upon the introductory concept provided in sentence B, expanding the readerś understanding of the functionality and purpose of an email list.Sentence A presents a specific example For example, if I am a group member, I can choose group-sending.This sentence illustrates a practical application of the concept introduced in sentences B and C,</p>
<p>Table 2
2represents the rela-</p>
<p>Table 1 :
1
PaLM2 and GPT4's accuracy on the five tasks in PUZZLEBEN.CR stands for critical reasoning subset.
ModelMethodPuzzles Riddles ParajumbleCRBrainteasersPaLM2Standard Prompting (Brown et al., 2020) Zero-Shot CoT (Madaan et al., 2023)49.45 53.2461.90 63.0325.54 20.0858.39 51.9834.89 41.96GPT4Standard Prompting (Brown et al., 2020) Zero-Shot CoT (Madaan et al., 2023)64.37 81.2267.70 81.9252.17 45.9665.32 63.0152.58 53.53Shots Puzzles Riddles ParajumbleCRBT081.2281.9245.9663.01 53.53182.9280.5346.2765.97 53.02884.9085.6351.4268.73 55.62</p>
<p>Table 2 :
2
GPT4's k-shot ICL performance on ZLEBEN.BT stands for Brainteaser tasks.</p>
<p>Table 4 :
4
LLaMA2-13b self-reinforcement and the baselines' results on PUZZLEBEN with the same labeled seed data set.
-reinforcement Results onPUZZLEBENMethodsIterations AccuracyUnfinetune-10.38SFT-17.33ReFT-22.47self-reinforcement (ours)t128.11self-reinforcement (ours)t237.82</p>
<p>Table 5 :
5
Our method's accuracy with and without selffiltering in each iteration.</p>
<p>Table 7 :
7
Prompting we designed in the stage of self-filtering.Response1 is generated from M 1 while Response2 is from M 0 .We filter out the samples which Response1 is obviously worse than Response0.
SubsetSizeAnnotated Trainset22,528Unannotated Question Set 10,000Testset2,618</p>
<p>Table 8 :
8
Detailed Subset's Size in PUZZLEBEN.</p>
<p>https://www.ahapuzzles.com/
https://www.sawaal.com/
https://cdn.digialm.com/EForms/ configuredHtml/756/84433/Registration.html
https://www.pearsonpte.com/
https://www.ets.org/gre.html
https://www.mba.com/exams/gmat-exam/
LimitationsIt is crucial to recognize that the self-reinforcement process could see improvements with further refinements in self-filtering.Specifically, choosing more impactful positive and negative pairs can greatly enhance the effectiveness of DPO training.This approach aligns with the strategy of leveraging highly capable models or human experts for alignment tasks.Moreover, there remains uncertainty regarding the stability of our model with extensive iterations; specifically, whether the model might experience collapse or increased hallucination phenomena as iterations progress.Introducing a certain proportion of human-annotated data in each iteration could serve as an alignment mechanism, potentially mitigating these issues and ensuring the model remains robust and accurate over long-term training.
Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, arXiv:1905.13319Mathqa: Towards interpretable math word problem solving with operation-based formalisms. 2019arXiv preprint</p>
<p>Learning the structure of generative models without labeled data. Bryan Stephen H Bach, Alexander He, Christopher Ratner, Ré, International Conference on Machine Learning. PMLR2017</p>
<p>Mixmatch: A holistic approach to semisupervised learning. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin A Raffel, Advances in neural information processing systems. 201932</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, arXiv:2308.096872023arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, arXiv:2312.09390Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. Jan Leike, et al. 2023arXiv preprint</p>
<p>Semi-supervised semantic segmentation with cross pseudo supervision. Xiaokang Chen, Yuhui Yuan, Gang Zeng, Jingdong Wang, 2021</p>
<p>Self-play fine-tuning converts weak language models to strong language models. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu, 2024</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Shivani Dohan, Mark Agrawal, Omernick, M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas EckJeff Dean, Slav Petrovand Noah Fiedel. 2022. Palm: Scaling language modeling with pathways</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Antonia Creswell, Murray Shanahan, arXiv:2208.14271Faithful reasoning using large language models. 2022arXiv preprint</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Advances in Neural Information Processing Systems. 202436</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Curriculumnet: Weakly supervised learning from large-scale web images. Sheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong, Matthew R Scott, Dinglong Huang, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Evaluating extractive summarization techniques on news articles. Sreeya Reddy, Kotrakona Harinatha, 2021 International Seminar on Intelligent Technology and Its Applications (ISITIA). IEEE2021Beauty Tatenda Tasara, and Nunung Nurul Qomariyah</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556Training compute-optimal large language models. 2022arXiv preprint</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, 2022</p>
<p>Brainteaser: Lateral thinking puzzles for large language models. Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati, 2023</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, arXiv:1705.035512017arXiv preprint</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning. Seungone Kim, June Se, Doyoung Joo, Joel Kim, Seonghyeon Jang, Jamin Ye, Minjoon Shin, Seo, 2023</p>
<p>Advances in neural information processing systems, 27. Samuli Laine and Timo Aila. Shakir Durk P Kingma, Danilo Mohamed, Max Jimenez Rezende, Welling, arXiv:1610.022422014. 2016arXiv preprintTemporal ensembling for semi-supervised learning</p>
<p>Bin Lei, Chunhua Liao, Caiwen Ding, arXiv:2308.08614Boosting logical reasoning in large language models through a new framework: The graph of thought. 2023arXiv preprint</p>
<p>Making language models better reasoners with step-aware verifier. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. Bowen Baker, Teddy LeeJanarXiv preprint</p>
<p>Ziyi Bill Yuchen Lin, Yichi Wu, Dong-Ho Yang, Xiang Lee, Ren, arXiv:2101.00376Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge. 2021arXiv preprint</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, arXiv:1705.041462017arXiv preprint</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, arXiv:2007.081242020arXiv preprint</p>
<p>The flan collection: Designing data and methods for effective instruction tuning. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Barret Quoc V Le, Jason Zoph, Wei, International Conference on Machine Learning. PMLR2023</p>
<p>Reft: Reasoning with reinforced fine-tuning. Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li, 2024</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.176512023arXiv preprint</p>
<p>. Rishabh Misra, arXiv:2209.114292022News category dataset. arXiv preprint</p>
<p>Yasumasa Onoe, J Q Michael, Eunsol Zhang, Greg Choi, Durrett, arXiv:2109.01653Creak: A dataset for commonsense reasoning over entity knowledge. 2021arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang, Wang , arXiv:2308.031882023arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn, 2023</p>
<p>Aligning large and small language models via chain-of-thought reasoning. Leonardo Ranaldi, Andre Freitas, Proceedings of the 18th Conference of the European Chapter. the Association for Computational Linguistics. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics20241Long Papers</p>
<p>Snorkel: Rapid training data creation with weak supervision. Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, Christopher Ré, Proceedings of the VLDB endowment. International conference on very large data bases. the VLDB endowment. International conference on very large data basesNIH Public Access201711269</p>
<p>Automatic adaptation of object detectors to new domains using selftraining. Aruni Roychowdhury, Prithvijit Chakrabarty, Ashish Singh, Souyoung Jin, Huaizu Jiang, 2019Liangliang Cao, and Erik Learned-Miller</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, Proximal policy optimization algorithms. 2017</p>
<p>The cost of training nlp models: A concise overview. Or Sharir, Barak Peleg, Yoav Shoham, arXiv:2004.089002020arXiv preprint</p>
<p>Learning from noisy labels with deep neural networks: A survey. Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, Jae-Gil Lee, 2022learning systems</p>
<p>Critical thinking: An appeal to reason. Peg Tittle, 2011Routledge</p>
<p>Can llms learn from previous mistakes? investigating llms' errors to boost for reasoning. Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, Jingbo Shang, 2024</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Scott: Selfconsistent chain-of-thought distillation. Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, Xiang Ren, 2023</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Self-training with noisy student improves imagenet classification. Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V Le, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, arXiv:2002.04326Reclor: A reading comprehension dataset requiring logical reasoning. 2020arXiv preprint</p>
<p>. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, 2022aOpt: Open pre-trained transformer language models</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.034932022barXiv preprint</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, arXiv:2205.10625Least-to-most prompting enables complex reasoning in large language models. 2022arXiv preprint</p>
<p>Nisan Daniel M Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
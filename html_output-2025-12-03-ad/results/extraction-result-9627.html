<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9627 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9627</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9627</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-275471664</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.07267v1.pdf" target="_blank">Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics</a></p>
                <p><strong>Paper Abstract:</strong> Scientific team dynamics are critical in determining the nature and impact of research outputs. However, existing methods for classifying author roles based on self-reports and clustering lack comprehensive contextual analysis of contributions. Thus, we present a transformative approach to classifying author roles in scientific teams using advanced large language models (LLMs), which offers a more refined analysis compared to traditional clustering methods. Specifically, we seek to complement and enhance these traditional methods by utilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting, we categorize author roles and demonstrate that GPT-4 outperforms other models across multiple categories, surpassing traditional approaches such as XGBoost and BERT. Our methodology also includes building a predictive deep learning model using 10 features. By training this model on a dataset derived from the OpenAlex database, which provides detailed metadata on academic publications -- such as author-publication history, author affiliation, research topics, and citation counts -- we achieve an F1 score of 0.76, demonstrating robust classification of author roles.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9627.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9627.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary, large decoder-only transformer LLM used in this study to perform few-shot classification of author contribution statements and to generate high-quality labeled data at scale for downstream predictive modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary decoder-only large language model (OpenAI) with strong few-shot capabilities used without fine-tuning; prompted with a few-shot prompt that defines role categories and examples; temperature set very low (0.01) to minimize randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Self-reported 'Author Contribution' fields from papers in four high-impact journals (PNAS, Nature, Science, PLoS One) from 2003–2020, sampled to 250 papers per journal (1,000 papers) with team sizes constrained to 2–8 authors; average authors per paper ≈ 5 producing a 5,000-row evaluation set; further used to infer labels for additional authors (an expanded labelled set up to ~15,000 author rows) and combined with OpenAlex metadata for feature engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>1000</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Classify each author's role (Leadership, Direct Support, Indirect Support) from the paper's 'Author Contribution' text; when multiple activities exist pick the highest-ranking role present (Leadership > Direct Support > Indirect Support).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Few-shot prompting: structured prompts containing role definitions and targeted examples provided to the model; temperature set to 0.01 for deterministic outputs; zero-shot was also explored but produced unstable / ambiguous results. LLM outputs (role labels per author) were used as high-quality labels to 'distill' LLM judgments into a scalable dense neural network classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured classification labels (per-author role: Leadership / Direct Support / Indirect Support) used as synthesized annotations across the corpus; later used to train a predictive DNN.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Example mapping used by prompts: an author contribution containing 'designed, directed, and wrote the manuscript' -> Leadership; 'collected and analyzed data' -> Direct Support; 'provided comments and editing' -> Indirect Support.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison to manually labeled benchmark (human-labeled roles) and quantitative metrics (precision, recall, F1) across classes; direct comparisons to alternative LLMs (Llama3, Llama2, Mistral), and to traditional ML baselines (BERT, RoBERTa, XGBoost). Stratified sampling and test splits were used where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4 achieved top performance: Leadership F1=0.993 (precision=0.995, recall=0.991); Direct Support F1≈0.950; Indirect Support F1≈0.947; reported macro-average F1≈0.963. Paper also reports GPT-4 approaching an overall F1 of ~0.97 for the role-classification task. GPT-4 labels were used to train downstream models; the final dense neural network trained on engineered features (labels seeded by GPT-4) achieved F1≈0.76 (binary leadership vs support).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Very high precision/recall on role classification in few-shot setting; strong contextual language understanding enabling disambiguation of nuanced contribution statements; reduces need for large labeled training sets by producing high-quality labels; deterministic prompting (low temperature) yields reproducible labels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Proprietary model with high computational and cost requirements limiting accessibility and scalability for some users; zero-shot performance unstable compared to few-shot; inference at very large scale (millions of authors) is computationally impractical, necessitating model distillation into cheaper predictors; potential for biased or inconsistent labeling in ambiguous cases though few-shot prompting mitigates some errors.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Zero-shot prompting produced inconsistent or ambiguous role assignments; subtle or overlapping contributions (e.g., 'contributed to design and provided comments') were error-prone unless explicit examples were provided; GPT-4 tendencies to assign leadership more frequently than some other models (model-dependent bias).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9627.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9627.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open/accessible large language model family (70B parameter size) evaluated in the paper for few-shot role classification and compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer LLM (Llama family) used without fine-tuning and with the same few-shot prompts/low-temperature settings; evaluated for classification accuracy relative to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same self-reported author-contribution dataset sampled from PNAS, Nature, Science, and PLoS One (1,000 sampled papers → ~5,000 author-rows) used for LLM evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>1000</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Same role-classification task: determine Leadership/Direct Support/Indirect Support from author contribution statements using few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Few-shot prompting with identical prompt engineering as other LLMs; temperature set to 0.01. Outputs compared to human labels and GPT-4 outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-author role labels (Leadership / Direct Support / Indirect Support).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Using the provided few-shot prompt, Llama3 might label 'coordinated experiments and supervised students' as Leadership (example-based mapping).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Precision/recall/F1 computed against human-labeled benchmark and compared to GPT-4, Llama2, Mistral, BERT/RoBERTa/XGBoost.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Llama3 achieved high precision on Leadership (precision reported 0.996) but notably lower recall (0.851) and lower macro-average F1 (macro F1 ≈ 0.776) than GPT-4; performed worse on Direct Support class (e.g., Direct Support F1≈0.569).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>High precision for Leadership detections in few-shot prompts; open model family enabling accessibility compared to proprietary alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lower recall than GPT-4 for leadership and weaker performance on support classes, indicating it missed many positive instances; overall less reliable cross-class performance in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>High-precision but low-recall pattern for Leadership (misses many leadership instances); poor Direct Support F1 suggests misclassification or under-detection of support roles; tendency to be conservative in labeling leadership compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9627.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9627.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previous-generation Llama family model (70B) evaluated in the study for few-shot author-role classification; demonstrated moderate to poor performance relative to GPT-4 and Llama3.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer LLM (Llama2 70B) evaluated without fine-tuning using the same few-shot prompts and deterministic settings as other LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same sampled dataset of author-contribution statements from 1,000 papers across PNAS, Nature, Science, and PLoS One (≈5,000 author rows) used for LLM evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>1000</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Classify author roles from contribution statements into Leadership, Direct Support, Indirect Support using few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Few-shot prompting, temperature 0.01; outputs compared across models.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-author role labels.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Under the few-shot prompt, Llama2 might assign an author who 'helped run experiments and analyze data' to Direct Support (example-based mapping).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Precision/recall/F1 against human labels and cross-model comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Llama2 showed moderate-to-poor metrics: Leadership F1≈0.908 (precision 0.949, recall 0.870) but very weak Direct Support and Indirect Support F1s (e.g., Direct Support F1≈0.352), producing a macro-average F1≈0.591.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Accessible open model; reasonable performance on Leadership relative to other open models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Substantially worse performance on Direct Support and Indirect Support labels; lower overall reliability for nuanced role distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Tendency to misclassify many Direct Support instances (very low F1 for that class); overall underperformance compared to GPT-4 and Llama3.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9627.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9627.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7x8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7x8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Mistral-model variant evaluated in the paper (denoted Mistral 7x8B) for few-shot classification; showed uneven class tendencies relative to other LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7x8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mistral family LLM variant (multiple 8B experts indicated by '7x8B' naming in the paper) tested without fine-tuning using the same few-shot prompts and deterministic settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7x8B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same sample of 1,000 papers (≈5,000 author instances) from four journals; used for direct LLM comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>1000</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Classify author roles from contribution statements into Leadership / Direct Support / Indirect Support via few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Few-shot prompting (same prompt engineering / temperature 0.01) and class assignment logic as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-author role labels.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Given 'assisted with data collection and manuscript editing,' Mistral might prioritize Direct Support labeling according to its learnt patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Precision/recall/F1 vs human labels and cross-model comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Mistral performance: Leadership F1≈0.951 (precision 0.969, recall 0.933); Direct Support F1≈0.580; Indirect Support F1≈0.783; macro-average F1≈0.771. Mistral tended to classify more authors as Direct Support compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Competitive Leadership detection and balanced performance on Indirect Support; open-model availability benefits accessibility.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Less consistent than GPT-4 overall; class distribution biases (tendency towards Direct Support) observed in label distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Bias to classify more authors as Direct Support relative to GPT-4; inconsistent recall/precision trade-offs across classes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9627.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9627.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-to-DNN distillation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-labeled data distillation into scalable Dense Neural Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline used in the paper where high-quality labels produced by an LLM (GPT-4) are used to train a compact, scalable dense neural network (DNN) classifier over engineered bibliometric features to enable large-scale application.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pipeline: GPT-4 → Dense Neural Network (DNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage approach: (1) GPT-4 infers role labels on a sampled corpus using few-shot prompts; (2) those labels become supervision to train a DNN that consumes ten engineered bibliometric features derived from OpenAlex and publication metadata; DNN architecture: several dense layers with ReLU hidden activations and sigmoid output for binary classification (Leadership vs Support in final experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Features engineered from OpenAlex metadata for authors whose roles were labeled by GPT-4: initially ~5,000 author entries from sampled papers, expanded by further GPT-4 inference to ~15,000 labeled author datapoints for training and evaluation. Features include contribution-to-references, contribution-to-topics, probability of leading, probability of corresponding, career age, citation count, citation impact per year, unique topics, total publications, institutional diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>15000</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Train a scalable classifier to predict whether an author is in a Leadership role (binary: Leadership vs Support) using features derived from bibliometric metadata, with labels provided by GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Label distillation: GPT-4-produced labels used as training targets; DNN trained for 20 epochs with min-max normalized features and stratified train/test split (0.8/0.2). SHAP (Gradient SHAP) used for interpretability to quantify feature importance.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Binary leadership vs support predictions and SHAP-based feature attributions for interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Trained DNN output: author_id X -> predicted Leadership probability 0.87; SHAP indicates high contributions from 'Probability of Leading Correspondence' and 'Probability of Leading'.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Held-out test evaluation using F1 score, comparison to XGBoost baseline and previously reported results (Xu et al.); SHAP used for interpretability analysis to validate feature importance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Dense Neural Network achieved F1 ≈ 0.76 (binary classification). XGBoost trained on same features achieved slightly higher F1 ≈ 0.78. Prior work (Xu et al.) reported F1 ≈ 0.79 on a larger dataset; BERT and RoBERTa fine-tuned on the textual dataset showed F1 ≈ 0.90 and 0.8794 respectively for the multiclass task.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Scalable and computationally efficient at inference time compared to direct LLM inference; enables application to much larger corpora; interpretability via SHAP identifies which bibliometric features drive leadership predictions (e.g., probability of being corresponding author).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance depends on quality of LLM-generated labels; distilled model simplifies text-based nuance into engineered numeric features and thus may lose some contextual subtleties captured by LLM text understanding; XGBoost slightly outperformed the DNN in this work, indicating architecture/feature tuning matters.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Mistakes in LLM labels propagate to the DNN; binary merging of Direct and Indirect Support into a single 'Support' class can mask fine-grained errors; SHAP indicates some features (citations per year, institutional diversity) are only moderately predictive — leadership prediction is dominated by authorship-position proxies which may fail in disciplines with non-standard authorship conventions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9627.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9627.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs for literature synthesis (general mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models applied to literature review, summarization, and meta-analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites and describes broader uses of LLMs in the science-of-science literature: identifying research trends, categorizing scholarly articles, extracting key information, generating literature reviews, mapping scientific fields, and automating synthesis/meta-analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (e.g., ChatGPT/GPT-4, other decoder-only and encoder-decoder LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General description: LLMs (decoder-only and encoder-decoder transformers) are being used in related work to process large scholarly corpora for trend detection, classification, summarization, and automated literature synthesis; methods include few-shot prompting, summarization pipelines and automated extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Referenced uses typically involve large scholarly corpora spanning multiple disciplines, examples in literature include tasks such as literature reviews and meta-analyses across many papers; this paper does not itself perform full literature-level theory distillation but cites such applications.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Synthesize research findings, generate literature reviews, identify trends and map field evolution across many scholarly documents.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Mentioned approaches in related work include few-shot prompting, automated summarization pipelines, extraction of key information for meta-analysis, and classification of research aims; specific retrieval-augmented or chain-of-thought pipelines are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Narrative literature reviews, summaries, classification outputs, synthesized insights for meta-analysis and trend mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Paper cites example application: 'LLMs can assist in generating literature reviews by summarizing key points from numerous papers' (no concrete excerpt produced in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Cited literature evaluates LLM outputs via human expert review, comparisons to gold-standard annotations, and automated metrics in referenced works; this paper itself does not evaluate such synthesis tasks directly.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>This paper does not provide original quantitative results for literature-synthesis applications, but references prior studies showing promise (e.g., ChatGPT for research quality evaluation, automated literature reviews) without reproducing those experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Potential to automate and accelerate literature synthesis, democratize access to synthesized knowledge, and enable large-scale meta-analytic workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Risks include hallucination, uneven quality across domains, lack of reproducibility/transparency in models, and computational/accessibility constraints for proprietary large models; the paper emphasizes that zero-shot behavior can be unstable and that LLM-based synthesis may require careful prompting and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not elaborated in this paper beyond general observations: hallucinations and instability in zero-shot settings, domain mismatch, and challenges in reproducing accurate, evidence-grounded syntheses without human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Openalex: A fully-open index of scholarly works, authors, venues, institutions, and concepts <em>(Rating: 1)</em></li>
                <li>Evaluating research quality with large language models: an analysis of chatgpt's effectiveness with different settings and inputs <em>(Rating: 1)</em></li>
                <li>Scientific progress or societal progress? a language modelbased classification of the aims of the research in scientific publications <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9627",
    "paper_id": "paper-275471664",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A proprietary, large decoder-only transformer LLM used in this study to perform few-shot classification of author contribution statements and to generate high-quality labeled data at scale for downstream predictive modeling.",
            "citation_title": "TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Proprietary decoder-only large language model (OpenAI) with strong few-shot capabilities used without fine-tuning; prompted with a few-shot prompt that defines role categories and examples; temperature set very low (0.01) to minimize randomness.",
            "model_size": null,
            "input_corpus_description": "Self-reported 'Author Contribution' fields from papers in four high-impact journals (PNAS, Nature, Science, PLoS One) from 2003–2020, sampled to 250 papers per journal (1,000 papers) with team sizes constrained to 2–8 authors; average authors per paper ≈ 5 producing a 5,000-row evaluation set; further used to infer labels for additional authors (an expanded labelled set up to ~15,000 author rows) and combined with OpenAlex metadata for feature engineering.",
            "input_corpus_size": 1000,
            "topic_query_description": "Classify each author's role (Leadership, Direct Support, Indirect Support) from the paper's 'Author Contribution' text; when multiple activities exist pick the highest-ranking role present (Leadership &gt; Direct Support &gt; Indirect Support).",
            "distillation_method": "Few-shot prompting: structured prompts containing role definitions and targeted examples provided to the model; temperature set to 0.01 for deterministic outputs; zero-shot was also explored but produced unstable / ambiguous results. LLM outputs (role labels per author) were used as high-quality labels to 'distill' LLM judgments into a scalable dense neural network classifier.",
            "output_type": "Structured classification labels (per-author role: Leadership / Direct Support / Indirect Support) used as synthesized annotations across the corpus; later used to train a predictive DNN.",
            "output_example": "Example mapping used by prompts: an author contribution containing 'designed, directed, and wrote the manuscript' -&gt; Leadership; 'collected and analyzed data' -&gt; Direct Support; 'provided comments and editing' -&gt; Indirect Support.",
            "evaluation_method": "Comparison to manually labeled benchmark (human-labeled roles) and quantitative metrics (precision, recall, F1) across classes; direct comparisons to alternative LLMs (Llama3, Llama2, Mistral), and to traditional ML baselines (BERT, RoBERTa, XGBoost). Stratified sampling and test splits were used where applicable.",
            "evaluation_results": "GPT-4 achieved top performance: Leadership F1=0.993 (precision=0.995, recall=0.991); Direct Support F1≈0.950; Indirect Support F1≈0.947; reported macro-average F1≈0.963. Paper also reports GPT-4 approaching an overall F1 of ~0.97 for the role-classification task. GPT-4 labels were used to train downstream models; the final dense neural network trained on engineered features (labels seeded by GPT-4) achieved F1≈0.76 (binary leadership vs support).",
            "strengths": "Very high precision/recall on role classification in few-shot setting; strong contextual language understanding enabling disambiguation of nuanced contribution statements; reduces need for large labeled training sets by producing high-quality labels; deterministic prompting (low temperature) yields reproducible labels.",
            "limitations": "Proprietary model with high computational and cost requirements limiting accessibility and scalability for some users; zero-shot performance unstable compared to few-shot; inference at very large scale (millions of authors) is computationally impractical, necessitating model distillation into cheaper predictors; potential for biased or inconsistent labeling in ambiguous cases though few-shot prompting mitigates some errors.",
            "failure_cases": "Zero-shot prompting produced inconsistent or ambiguous role assignments; subtle or overlapping contributions (e.g., 'contributed to design and provided comments') were error-prone unless explicit examples were provided; GPT-4 tendencies to assign leadership more frequently than some other models (model-dependent bias).",
            "uuid": "e9627.0",
            "source_info": {
                "paper_title": "Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Llama3-70B",
            "name_full": "Llama 3 70B",
            "brief_description": "An open/accessible large language model family (70B parameter size) evaluated in the paper for few-shot role classification and compared to GPT-4.",
            "citation_title": "TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS",
            "mention_or_use": "use",
            "model_name": "Llama3 70B",
            "model_description": "Decoder-only transformer LLM (Llama family) used without fine-tuning and with the same few-shot prompts/low-temperature settings; evaluated for classification accuracy relative to GPT-4.",
            "model_size": "70B",
            "input_corpus_description": "Same self-reported author-contribution dataset sampled from PNAS, Nature, Science, and PLoS One (1,000 sampled papers → ~5,000 author-rows) used for LLM evaluation.",
            "input_corpus_size": 1000,
            "topic_query_description": "Same role-classification task: determine Leadership/Direct Support/Indirect Support from author contribution statements using few-shot prompts.",
            "distillation_method": "Few-shot prompting with identical prompt engineering as other LLMs; temperature set to 0.01. Outputs compared to human labels and GPT-4 outputs.",
            "output_type": "Per-author role labels (Leadership / Direct Support / Indirect Support).",
            "output_example": "Using the provided few-shot prompt, Llama3 might label 'coordinated experiments and supervised students' as Leadership (example-based mapping).",
            "evaluation_method": "Precision/recall/F1 computed against human-labeled benchmark and compared to GPT-4, Llama2, Mistral, BERT/RoBERTa/XGBoost.",
            "evaluation_results": "Llama3 achieved high precision on Leadership (precision reported 0.996) but notably lower recall (0.851) and lower macro-average F1 (macro F1 ≈ 0.776) than GPT-4; performed worse on Direct Support class (e.g., Direct Support F1≈0.569).",
            "strengths": "High precision for Leadership detections in few-shot prompts; open model family enabling accessibility compared to proprietary alternatives.",
            "limitations": "Lower recall than GPT-4 for leadership and weaker performance on support classes, indicating it missed many positive instances; overall less reliable cross-class performance in this task.",
            "failure_cases": "High-precision but low-recall pattern for Leadership (misses many leadership instances); poor Direct Support F1 suggests misclassification or under-detection of support roles; tendency to be conservative in labeling leadership compared to GPT-4.",
            "uuid": "e9627.1",
            "source_info": {
                "paper_title": "Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Llama2-70B",
            "name_full": "Llama 2 70B",
            "brief_description": "A previous-generation Llama family model (70B) evaluated in the study for few-shot author-role classification; demonstrated moderate to poor performance relative to GPT-4 and Llama3.",
            "citation_title": "TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS",
            "mention_or_use": "use",
            "model_name": "Llama2 70B",
            "model_description": "Decoder-only transformer LLM (Llama2 70B) evaluated without fine-tuning using the same few-shot prompts and deterministic settings as other LLMs.",
            "model_size": "70B",
            "input_corpus_description": "Same sampled dataset of author-contribution statements from 1,000 papers across PNAS, Nature, Science, and PLoS One (≈5,000 author rows) used for LLM evaluation.",
            "input_corpus_size": 1000,
            "topic_query_description": "Classify author roles from contribution statements into Leadership, Direct Support, Indirect Support using few-shot prompts.",
            "distillation_method": "Few-shot prompting, temperature 0.01; outputs compared across models.",
            "output_type": "Per-author role labels.",
            "output_example": "Under the few-shot prompt, Llama2 might assign an author who 'helped run experiments and analyze data' to Direct Support (example-based mapping).",
            "evaluation_method": "Precision/recall/F1 against human labels and cross-model comparison.",
            "evaluation_results": "Llama2 showed moderate-to-poor metrics: Leadership F1≈0.908 (precision 0.949, recall 0.870) but very weak Direct Support and Indirect Support F1s (e.g., Direct Support F1≈0.352), producing a macro-average F1≈0.591.",
            "strengths": "Accessible open model; reasonable performance on Leadership relative to other open models.",
            "limitations": "Substantially worse performance on Direct Support and Indirect Support labels; lower overall reliability for nuanced role distinctions.",
            "failure_cases": "Tendency to misclassify many Direct Support instances (very low F1 for that class); overall underperformance compared to GPT-4 and Llama3.",
            "uuid": "e9627.2",
            "source_info": {
                "paper_title": "Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Mistral-7x8B",
            "name_full": "Mistral 7x8B",
            "brief_description": "A Mistral-model variant evaluated in the paper (denoted Mistral 7x8B) for few-shot classification; showed uneven class tendencies relative to other LLMs.",
            "citation_title": "TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS",
            "mention_or_use": "use",
            "model_name": "Mistral 7x8B",
            "model_description": "Mistral family LLM variant (multiple 8B experts indicated by '7x8B' naming in the paper) tested without fine-tuning using the same few-shot prompts and deterministic settings.",
            "model_size": "7x8B",
            "input_corpus_description": "Same sample of 1,000 papers (≈5,000 author instances) from four journals; used for direct LLM comparisons.",
            "input_corpus_size": 1000,
            "topic_query_description": "Classify author roles from contribution statements into Leadership / Direct Support / Indirect Support via few-shot prompts.",
            "distillation_method": "Few-shot prompting (same prompt engineering / temperature 0.01) and class assignment logic as other models.",
            "output_type": "Per-author role labels.",
            "output_example": "Given 'assisted with data collection and manuscript editing,' Mistral might prioritize Direct Support labeling according to its learnt patterns.",
            "evaluation_method": "Precision/recall/F1 vs human labels and cross-model comparison.",
            "evaluation_results": "Mistral performance: Leadership F1≈0.951 (precision 0.969, recall 0.933); Direct Support F1≈0.580; Indirect Support F1≈0.783; macro-average F1≈0.771. Mistral tended to classify more authors as Direct Support compared to GPT-4.",
            "strengths": "Competitive Leadership detection and balanced performance on Indirect Support; open-model availability benefits accessibility.",
            "limitations": "Less consistent than GPT-4 overall; class distribution biases (tendency towards Direct Support) observed in label distributions.",
            "failure_cases": "Bias to classify more authors as Direct Support relative to GPT-4; inconsistent recall/precision trade-offs across classes.",
            "uuid": "e9627.3",
            "source_info": {
                "paper_title": "Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "LLM-to-DNN distillation pipeline",
            "name_full": "LLM-labeled data distillation into scalable Dense Neural Network",
            "brief_description": "A pipeline used in the paper where high-quality labels produced by an LLM (GPT-4) are used to train a compact, scalable dense neural network (DNN) classifier over engineered bibliometric features to enable large-scale application.",
            "citation_title": "TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS",
            "mention_or_use": "use",
            "model_name": "Pipeline: GPT-4 → Dense Neural Network (DNN)",
            "model_description": "Two-stage approach: (1) GPT-4 infers role labels on a sampled corpus using few-shot prompts; (2) those labels become supervision to train a DNN that consumes ten engineered bibliometric features derived from OpenAlex and publication metadata; DNN architecture: several dense layers with ReLU hidden activations and sigmoid output for binary classification (Leadership vs Support in final experiments).",
            "model_size": null,
            "input_corpus_description": "Features engineered from OpenAlex metadata for authors whose roles were labeled by GPT-4: initially ~5,000 author entries from sampled papers, expanded by further GPT-4 inference to ~15,000 labeled author datapoints for training and evaluation. Features include contribution-to-references, contribution-to-topics, probability of leading, probability of corresponding, career age, citation count, citation impact per year, unique topics, total publications, institutional diversity.",
            "input_corpus_size": 15000,
            "topic_query_description": "Train a scalable classifier to predict whether an author is in a Leadership role (binary: Leadership vs Support) using features derived from bibliometric metadata, with labels provided by GPT-4.",
            "distillation_method": "Label distillation: GPT-4-produced labels used as training targets; DNN trained for 20 epochs with min-max normalized features and stratified train/test split (0.8/0.2). SHAP (Gradient SHAP) used for interpretability to quantify feature importance.",
            "output_type": "Binary leadership vs support predictions and SHAP-based feature attributions for interpretability.",
            "output_example": "Trained DNN output: author_id X -&gt; predicted Leadership probability 0.87; SHAP indicates high contributions from 'Probability of Leading Correspondence' and 'Probability of Leading'.",
            "evaluation_method": "Held-out test evaluation using F1 score, comparison to XGBoost baseline and previously reported results (Xu et al.); SHAP used for interpretability analysis to validate feature importance.",
            "evaluation_results": "Dense Neural Network achieved F1 ≈ 0.76 (binary classification). XGBoost trained on same features achieved slightly higher F1 ≈ 0.78. Prior work (Xu et al.) reported F1 ≈ 0.79 on a larger dataset; BERT and RoBERTa fine-tuned on the textual dataset showed F1 ≈ 0.90 and 0.8794 respectively for the multiclass task.",
            "strengths": "Scalable and computationally efficient at inference time compared to direct LLM inference; enables application to much larger corpora; interpretability via SHAP identifies which bibliometric features drive leadership predictions (e.g., probability of being corresponding author).",
            "limitations": "Performance depends on quality of LLM-generated labels; distilled model simplifies text-based nuance into engineered numeric features and thus may lose some contextual subtleties captured by LLM text understanding; XGBoost slightly outperformed the DNN in this work, indicating architecture/feature tuning matters.",
            "failure_cases": "Mistakes in LLM labels propagate to the DNN; binary merging of Direct and Indirect Support into a single 'Support' class can mask fine-grained errors; SHAP indicates some features (citations per year, institutional diversity) are only moderately predictive — leadership prediction is dominated by authorship-position proxies which may fail in disciplines with non-standard authorship conventions.",
            "uuid": "e9627.4",
            "source_info": {
                "paper_title": "Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "LLMs for literature synthesis (general mention)",
            "name_full": "Large language models applied to literature review, summarization, and meta-analysis",
            "brief_description": "The paper cites and describes broader uses of LLMs in the science-of-science literature: identifying research trends, categorizing scholarly articles, extracting key information, generating literature reviews, mapping scientific fields, and automating synthesis/meta-analyses.",
            "citation_title": "TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS",
            "mention_or_use": "mention",
            "model_name": "various LLMs (e.g., ChatGPT/GPT-4, other decoder-only and encoder-decoder LLMs)",
            "model_description": "General description: LLMs (decoder-only and encoder-decoder transformers) are being used in related work to process large scholarly corpora for trend detection, classification, summarization, and automated literature synthesis; methods include few-shot prompting, summarization pipelines and automated extraction.",
            "model_size": null,
            "input_corpus_description": "Referenced uses typically involve large scholarly corpora spanning multiple disciplines, examples in literature include tasks such as literature reviews and meta-analyses across many papers; this paper does not itself perform full literature-level theory distillation but cites such applications.",
            "input_corpus_size": null,
            "topic_query_description": "Synthesize research findings, generate literature reviews, identify trends and map field evolution across many scholarly documents.",
            "distillation_method": "Mentioned approaches in related work include few-shot prompting, automated summarization pipelines, extraction of key information for meta-analysis, and classification of research aims; specific retrieval-augmented or chain-of-thought pipelines are not detailed in this paper.",
            "output_type": "Narrative literature reviews, summaries, classification outputs, synthesized insights for meta-analysis and trend mapping.",
            "output_example": "Paper cites example application: 'LLMs can assist in generating literature reviews by summarizing key points from numerous papers' (no concrete excerpt produced in this paper).",
            "evaluation_method": "Cited literature evaluates LLM outputs via human expert review, comparisons to gold-standard annotations, and automated metrics in referenced works; this paper itself does not evaluate such synthesis tasks directly.",
            "evaluation_results": "This paper does not provide original quantitative results for literature-synthesis applications, but references prior studies showing promise (e.g., ChatGPT for research quality evaluation, automated literature reviews) without reproducing those experiments here.",
            "strengths": "Potential to automate and accelerate literature synthesis, democratize access to synthesized knowledge, and enable large-scale meta-analytic workflows.",
            "limitations": "Risks include hallucination, uneven quality across domains, lack of reproducibility/transparency in models, and computational/accessibility constraints for proprietary large models; the paper emphasizes that zero-shot behavior can be unstable and that LLM-based synthesis may require careful prompting and validation.",
            "failure_cases": "Not elaborated in this paper beyond general observations: hallucinations and instability in zero-shot settings, domain mismatch, and challenges in reproducing accurate, evidence-grounded syntheses without human oversight.",
            "uuid": "e9627.5",
            "source_info": {
                "paper_title": "Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Openalex: A fully-open index of scholarly works, authors, venues, institutions, and concepts",
            "rating": 1,
            "sanitized_title": "openalex_a_fullyopen_index_of_scholarly_works_authors_venues_institutions_and_concepts"
        },
        {
            "paper_title": "Evaluating research quality with large language models: an analysis of chatgpt's effectiveness with different settings and inputs",
            "rating": 1,
            "sanitized_title": "evaluating_research_quality_with_large_language_models_an_analysis_of_chatgpts_effectiveness_with_different_settings_and_inputs"
        },
        {
            "paper_title": "Scientific progress or societal progress? a language modelbased classification of the aims of the research in scientific publications",
            "rating": 1,
            "sanitized_title": "scientific_progress_or_societal_progress_a_language_modelbased_classification_of_the_aims_of_the_research_in_scientific_publications"
        }
    ],
    "cost": 0.016483249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS
February 26, 2025</p>
<p>Wonduk Seo 
Department of Information Management
Peking University
100871BeijingChina</p>
<p>Yi Bu buyi@pku.edu.cn 
Department of Information Management
Peking University
100871BeijingChina</p>
<p>Peking University Chongqing Research Institute of Big Data
401332ChongqingChina</p>
<p>TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS
February 26, 2025737D4CE6123ED3CB75898DBA982F905BAuthor Role ClassificationLarge Language Models (LLMs)Predictive AnalyticsInterpretability AnalysisFew-shot PromptingFeature Engineering
Scientific team dynamics are critical in determining the nature and impact of research outputs.However, existing methods for classifying author roles based on self-reports and clustering lack comprehensive contextual analysis of contributions.Thus, we present a transformative approach to classifying author roles in scientific teams using advanced large language models (LLMs), which offers a more refined analysis compared to traditional clustering methods.Specifically, we seek to complement and enhance these traditional methods by utilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B, for role classification.Utilizing few-shot prompting, we categorize author roles and demonstrate that GPT-4 outperforms other models across multiple categories, surpassing traditional approaches such as XGBoost and BERT.Our methodology also includes building a predictive deep learning model using 10 features.By training this model on a dataset derived from the OpenAlex database, which provides detailed metadata on academic publications-such as author-publication history, author affiliation, research topics, and citation counts-we achieve an F1 score of 0.76, demonstrating robust classification of author roles.</p>
<p>Introduction</p>
<p>In the dynamic evolution of scientific research, the structure of research teams plays a significant role in shaping the nature and impact of their outputs [1].In such settings, accurately classifying the roles of team members is critical to understanding the mechanisms that drive scientific innovation and productivity.Effective role classification not only helps to recognize individual contributions, but also improves the management and optimization of research teams.Traditionally, methods have focused on categorizing authors into leadership, direct support, and indirect support.Leadership roles typically involve tasks such as designing and directing research, direct support encompasses activities like data collection and analysis, and indirect support includes providing feedback and editing manuscripts.The motivation behind focusing on these three categories is to understand the hierarchical structure and dynamics within scientific teams.Leadership roles significantly influence the direction and impact of a project, and identifying these roles allows to analyze patterns such as how leadership distribution affects team performance and the overall success of collaborative research efforts.Those are categorized based on clustering using terms derived from self-reported data, which refers to the information provided by the authors themselves about their contributions to a paper [2].</p>
<p>While this approach has yielded invaluable insights, including metrics such as the L-ratio, the proportion of leadership roles within teams, they are limited in their ability to capture the full context and nuances of authors' contributions.</p>
<p>arXiv:2501.07267v3 [cs.DL] 25 Feb 2025</p>
<p>Existing methods often lack the depth required to understand the specific context and impact of individual contributions.Furthermore, clustering used in previous research can be limited by their reliance on static data, failing to adapt to the evolving nature of scientific collaboration.For instance, an author who contributed to research design, data analysis, and manuscript writing might be grouped under one general category without distinguishing the importance of each task.This results in a static and incomplete representation of an author's role and impact on the research project.In addition, term-based clustering does not account for the complexity and multifaceted nature of authors' contributions, as it does not differentiate between the importance or context of multiple contributions made simultaneously by an author.This oversimplification can obscure the depth of individual effort and intellectual contribution, leading to a limited and sometimes misleading understanding of team dynamics and research contributions.One of the main challenges in classifying author roles lies in the complexity and nuance of contribution statements.Authors often describe their contributions using sophisticated and context-specific language, making it difficult for traditional models to accurately interpret and categorize these roles.This complexity requires a deep understanding of context and semantics that goes beyond simple keyword matching or basic natural language processing.</p>
<p>Given the significant advances in large language models (LLMs), they have been widely adopted for various science of science tasks such as identifying research trends, categorizing scholarly articles, extracting key information, and mapping the evolution of scientific fields [3,4].Our research explores a different setting: the use of LLMs such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B for role classification in scientific research [5,6,7].These advanced models can analyze patterns from the context of papers and authors, providing a more complete and dynamic understanding of team structures and collaboration dynamics, compared to traditional methods that often fail to capture the depth and context of individual contributions because they rely on static data and predefined categories.Furthermore, LLMs can perform few-shot learning, effectively adapting to new tasks with minimal examples, which is particularly advantageous when labeled data is scarce.They can handle the disambiguation of roles by understanding the interplay of different contributions within the same context, differentiating between leadership and support roles even when an author is involved in multiple tasks.By leveraging LLMs, we aim to overcome this limitation and provide deeper insights into the intricate roles within scientific teams.</p>
<p>We present a comprehensive framework that utilizes such LLMs to classify roles within scientific teams.Our methodology includes a few-shot prompt to accurately categorize author roles.Furthermore, after the LLM inference step, we aim at feature engineering: the framework incorporates ten extracted features reflecting each author's contributions and characteristics, such as contribution to references, contribution to topics, probability of leading, probability of managing correspondence, career age, citation count, unique topics, total publications, citation impact per year, and institutional diversity, by using a dataset derived from the OpenAlex database, which provides extensive metadata on academic publications, including citation counts, author affiliations, research topics, and publication history [8].We further train a deep learning model using 10 extracted features to classify author roles with high accuracy.Moreover, we conducted SHAP (SHapley Additive exPlanations) [9] analysis to investigate the importance of these features in the prediction tasks, providing valuable insights into their contributions.This approach not only complements, but also improves upon traditional methods, providing a more detailed and accurate classification of author roles.By leveraging the advanced capabilities of LLMs, our research aims to transform the landscape of scientific team role classification and provide a more nuanced understanding of team structures and collaboration dynamics.</p>
<p>Related Works</p>
<p>Dynamics of Scientific Teams</p>
<p>The structure and dynamics of scientific teams have been studied extensively, revealing a growing dominance and importance of team science in knowledge production.Research shows that teams tend to produce more highly cited research than individuals, a trend that has increased over time [1].Teams now produce more high-impact research, a distinction once dominated by individual authors [10].Katz and Martin [11] emphasized that collaboration among scientists leads to greater pooling of resources, sharing of knowledge, and combining of expertise, which in turn improves the quality and efficiency of scientific research.The interdisciplinary nature of many scientific teams allows for innovative approaches and solutions to complex problems [12].</p>
<p>Xu et al. [2] introduced the concept of the L-Ratio (Leadership Ratio), which measures the proportion of leadership roles within teams in their study of team structures, and demonstrated that flat team structures, which encourage equal contributions and a less hierarchical organization, are critical for fostering scientific innovation and improving team performance and research outcomes.Their findings are consistent with previous research suggesting that less hierarchical teams foster a more open exchange of ideas and promote creative problem solving [13,14].Furthermore, Wu et al. [15] found that smaller teams tend to produce more disruptive research compared to larger teams, which generally advance existing scientific knowledge.This indicates the critical role of team size in influencing research outcomes.</p>
<p>In addition, Cummings and Kiesler [16] suggested that team diversity, both in terms of disciplinary background and geographic location, is a significant predictor of team innovation and productivity.This diversity brings a variety of perspectives and skills to the table, leading to richer and more innovative outcomes.Moreover, Anicich et al. [17] demonstrated that hierarchical cultural values predict success and mortality in high-stakes teams.However, they also noted the challenges associated with coordinating diverse teams, highlighting the need for effective communication and management strategies to realize the full potential of team diversity.Haeussler and Sauermann provided insights into how the division of labor within teams is influenced by factors such as team size and interdisciplinarity, emphasizing its impact on collaborative knowledge production [18].</p>
<p>Traditional Methods of Role Classification</p>
<p>Traditionally, in the field of information science, clustering methods have been used to classify academic articles into research topics based on citation relationships and keyword analysis.Waltman and Eck [19] developed a methodology for constructing a publication-level classification system that uses citation networks and keyword co-occurrence to identify and group related research topics.This approach allows for a more objective and data-driven classification of research outputs, providing insights into the structure and evolution of scientific fields.Similar clustering techniques have been used in several other domains to categorize entities based on relational data.For example, bibliometric analysis and network clustering have been used to identify influential authors and research trends within specific disciplines [20].These methods often employ algorithms such as community detection and modularity optimization to uncover hidden patterns in complex datasets [21].By using these techniques, researchers can gain a deeper understanding of the collaborative networks and intellectual landscapes that underpin scientific progress.Additionally, Glänzel and Schubert [22] discussed the use of bibliometric methods to map the structure and dynamics of research fields.</p>
<p>However, an increasing focus has emerged on classifying roles within academic research teams, recognizing the importance of understanding team dynamics and individual contributions.Recent studies have proposed frameworks for identifying different types of research teams and their roles.For instance, a study introduced algorithms for team identification that categorize members into project-based, individual-based, and representative groups based on their contributions and collaboration patterns [23].Another work emphasized the need for a unified system to classify essential team roles beyond traditional task and social categories [24].In addition to these quantitative techniques, qualitative approaches have also been used to classify roles within academic research.Conger [25] argued that qualitative research is essential for understanding complex phenomena such as leadership within research teams.These qualitative methods complement quantitative approaches, providing a more complete picture of collaborative processes and individual contributions.Holbrook [26] highlighted the importance of qualitative evaluations in understanding the context and impact of research contributions.Moreover, different traditions of author ranking significantly shape the perception and classification of contributions.In many fields, the first author is typically seen as the primary contributor, while in others the last author, often the senior researcher or principal investigator, carries more prestige [27].These different conventions have implications for the assignment and recognition of roles.Additionally, systems such as the Contributor Roles Taxonomy have been developed to provide a transparent and detailed description of individual contributions [28].It specifies roles such as conceptualization, methodology, and software development, allowing for accurate attribution of credit.</p>
<p>Recently, metrics such as the L-Ratio have been used to measure the weight of leadership roles within research teams, providing a quantitative approach to assess the influence and contributions of different team members [2].This metric has shown that flat and egalitarian teams tend to produce more novel ideas compared to tall, hierarchical teams, highlighting the importance of team structure in fostering innovation.These findings emphasize the role of team dynamics and structure in driving creative and innovative research outcomes.</p>
<p>Large Language Models (LLMs) and Application in Science of Science</p>
<p>The advent of large language models (LLMs) such as GPT-4, Llama, and Mistral models has brought significant advances in text analysis and the production of human-like text, making them valuable tools for role classification in scientific research [29].Recent studies have demonstrated the effectiveness of LLMs in several domains, including scientific text analysis, classification tasks, and information extraction [3].The development of these models can be traced back to the introduction of transformers, a groundbreaking architecture introduced in the paper Attention is All You Need [30].Transformers use self-attention mechanisms to process input data in parallel, allowing models to capture long-range dependencies and contextual relationships more effectively than traditional recurrent neural networks (RNN) [31].In addition to the transformer architecture, several encoder-decoder models have emerged that exploit the strengths of transformers for different applications.In particular, BERT (Bidirectional Encoder Representations from Transformers), RoBERTa [32], and T5 (Text-to-Text Transformer) have been influential.BERT focuses on pre-training a deep bidirectional transformer by predicting masked words in a sentence, allowing it to capture complex context [33], while T5 frames all natural language processing (NLP) tasks as text-to-text problems, enabling the model to learn a variety of tasks through a consistent training framework [34].Decoder-only models, such as the GPT series, have also had a profound impact on the field, using a transformer decoder architecture to generate text by predicting the next word in a sequence, given all previous words [35].</p>
<p>While BERT and its variations have been widely applied in the science of science, the use of LLMs in this field is still emerging.Recent studies have begun to explore the potential of LLMs in evaluating research quality and classifying research aims.For instance, Thelwall et al. investigated the effectiveness of ChatGPT in assessing research quality under various conditions and examined the capability of ChatGPT to identify journal article quality across different academic disciplines [36,37,38].In the context of scientometrics, LLMs are used for various tasks such as identifying research trends, categorizing scholarly articles, extracting key information, and mapping the evolution of scientific fields.These capabilities enable researchers to automate the analysis of large volumes of scientific literature, facilitating more efficient and comprehensive studies of research impact and scholarly networks [3].For instance, LLMs can assist in generating literature reviews by summarizing key points from numerous papers, thus saving researchers significant time and effort [39].Additionally, they can help identify emerging research topics and influential authors by analyzing citation patterns and keyword co-occurrences [4].</p>
<p>Another important application of LLMs in scientometrics is their role in improving the transparency and reproducibility of research.By providing automated tools for literature review and data analysis, LLMs can help ensure that research methods and results are consistently documented and easily accessible.This can facilitate peer review and replication studies, which are critical to validating scientific findings [40].In addition, the use of LLMs to automate the synthesis of research findings can lead to more comprehensive meta-analyses, which are essential for drawing generalized conclusions from multiple studies [41].These advances contribute to a more robust and reliable scientific knowledge base.Furthermore, LLMs are increasingly being used to democratize access to scientific knowledge.By providing sophisticated text analysis and summarization tools, LLMs enable researchers from under-resourced institutions or regions to more effectively engage with the latest scientific literature.This can help bridge the gap between well-funded research institutions and those with fewer resources, promoting a more equitable distribution of scientific knowledge and opportunities for collaboration [42].</p>
<p>Dataset</p>
<p>We first utilize self-reported data collected from papers published in prominent journals such as PNAS, Nature, Science, and PLoS One from 2003 to 2020, which was opened by Xu et al [2].These four journals were selected due to their high impact and influence in the scientific community, ensuring high quality and significant research data.In addition, recent efforts to make scientific research fairer and more transparent have led these leading journals to require detailed reporting of each author's specific contributions.This requirement helps ensure that everyone involved in the research receives proper credit, making these journals ideal for our analysis.Specifically, PLoS One was included as it mandates comprehensive author contribution statements similar to the other journals, providing rich and structured data for analysis.Additionally, its broad scope across various scientific disciplines enables a more diverse representation of research team structures.To ensure consistency and reduce selection bias, we followed the methodology of Xu et al. [2] and stratified our sampling across these four journals.This selection provides a representative dataset that supports future work to understand and improve the dynamics of scientific collaboration.</p>
<p>The dataset focuses primarily on the "Author Contribution" field, which contains detailed information about each author's role and contributions to the paper.This field provides insight into various aspects of authors' participation in the research process, such as their involvement in experimental design, data analysis, manuscript writing, and other key tasks.By analyzing this detailed self-reported data, we were able to accurately classify authors' roles into the categories of Leadership, Direct Support, and Indirect Support.For instance, Leadership roles involve tasks such as designing and directing the research, Direct Support includes helping with data collection and analysis, and Indirect Support includes activities such as providing feedback and editing the manuscript.</p>
<p>In addition, for feature engineering purposes, we use the OpenAlex database, which is a rich resource that provides extensive metadata on academic publications, including citation counts, author affiliations, research topics, and publication history [8].This database allowed us to expand our dataset by incorporating more comprehensive academic profiles and publication records into our analysis, while ensuring that the data was clean and consistent.This included standardizing author names, resolving ambiguities in author identities, and ensuring that all relevant metadata from OpenAlex were correctly linked to the self-reported data.</p>
<p>In more detail, our dataset consists of two main parts for evaluation and modeling.In the first step, for the LLM evaluation, we used a dataset containing papers from 5,000 distinct authors, covering papers published between 2003 and 2020.To ensure a representative sample and maintain quality for validation purposes, we selected 250 papers from each of four prominent journals-PNAS, Nature, Science, and PLoS One.This selection process focused on papers with team sizes ranging from 2 to 8 authors, as these typically offer a clear division of labor and contribution roles.Through data cleaning and matching processes, we successfully aligned approximately 97% of our selected papers to their corresponding records in OpenAlex.The small proportion of papers that could not be mapped were due to inconsistencies in metadata, such as variations in author names or missing identifiers.In the second step, for feature engineering and modeling, we expanded our dataset to 2,000 papers from the same time period by using GPT-4, with 500 papers from each of the four journals.The average team size in both parts is consistently five authors per paper.</p>
<p>Overview of LLM-Based Role Classification Tasks</p>
<p>As shown in Figure 1, to ensure a representative sample and maintain quality for validation purposes, we selected 250 entries from each journal.This selection process focused on papers with between 2 and 8 authors, as these typically offer a clear division of labor and contribution roles.Using GPT-4, we assigned specific contributions to each author, categorizing them as Leadership, Direct Support, or Indirect Support based on predefined criteria.This step involved analyzing the "Author Contribution" field of each paper, which details each author's participation in various research activities such as experimental design, data analysis, manuscript writing, and other key tasks.Using the advanced natural language understanding capabilities of GPT-4, we accurately assigned contributions and categorized roles.</p>
<p>At the end of this assignment process, we compiled a dataset of 5,000 rows.We ensured this by first constraining the number of authors in each paper to be between 2 and 8 and randomly sampling 250 papers from each of the 4 journals, resulting in a total of 1,000 selected papers.Given that the average number of authors per paper was 5, we expanded the dataset to achieve a total of 5,000 rows: R = J × E × Ā where:</p>
<p>• J = 4 (number of journals);</p>
<p>• E = 250 (number of entries selected from each journal); • Ā = 5 (average number of authors per paper).</p>
<p>GPT-4 proved to be effective in accurately assigning papers and contributions, ensuring a balanced and comprehensive dataset for analysis.</p>
<p>Prompt Engineering for Role Classification</p>
<p>Once the contributions were assigned, we evaluated the intrinsic capabilities of the non-fine-tuned LLMs through few-shot prompt engineering.Our focus was on evaluating the performance of these models in accurately classifying roles based on predefined criteria.Each role category -Leadership, Direct Support, and Indirect Support -was evaluated and our expectations for each classification were outlined.</p>
<p>To facilitate this, we created specific prompts for the models, guiding them to categorize and classify research roles based on the activities associated with each role, as illustrated in Figure 2. We also explored zero-shot prompting, where the model relies solely on its pre-trained knowledge without additional examples.However, we observed that the model's predictions in zero-shot mode were less stable, often producing inconsistent or ambiguous role assignments.Without explicit demonstrations, LLMs sometimes misclassified contributions, particularly when role descriptions were subtle or overlapping.In contrast, our few-shot approach, which provided targeted examples, significantly improved classification accuracy by reducing ambiguity and ensuring better role differentiation.</p>
<p>To construct effective prompts, we defined a structured set of role categories and associated key responsibilities, ensuring clarity in classification.These role definitions provided the necessary context for LLMs to accurately assign contributions, as detailed below:</p>
<ol>
<li>Leadership: Responsibilities include designing, conceptualizing, directing, supervising, coordinating, interpreting, conducting, and writing the research.2. Direct Support: Tasks include helping, assisting, preparing, collecting, and analyzing.3. Indirect Support: Tasks include participating, providing, contributing, commenting, editing, and discussing.Given a dataset detailing the tasks of authors in a research project, the models were instructed to analyze each entry and assign roles accordingly.If an entry contained multiple categories, the role was selected from the highest category present.For example, if an author's activities included both "designing" (a Leadership task) and "providing" (an Indirect Support task), the author was classified under "Leadership" because it ranks higher in the role hierarchy.This structured hierarchy ensured that only one role was assigned to each author.</li>
</ol>
<p>LLM Model Configuration and Performance Comparison</p>
<p>Initially, we manually labeled corresponding roles for each author, which provided a benchmark for subsequent automated classification by following the criteria outlined in our prompts, assigning roles based on specific activities.We carefully examined each contribution statement, considering multiple facets such as the nature of the tasks performed, the level of responsibility, and the overall impact on the research.To automate this process, we used LLMs for inference, selecting four models: GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B.For each model, we standardized the prompting parameters to ensure consistency across evaluations.Specifically, we set temperature to 0.01 to minimize randomness in responses and provide more deterministic results.This setup allowed us to directly compare each model in accurately replicating the human-assigned roles.We performed a detailed performance comparison between GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B on three key metrics: F1 score, precision, and recall.</p>
<p>Our results demonstrated the superior ability of GPT-4 to accurately classify roles.As shown in Table 1, GPT-4 consistently led in precision and recall across all role categories, making it the most reliable of the models evaluated.Specifically, in the Leadership classification, GPT-4 achieved a precision of 0.995 and a recall of 0.991, significantly outperforming Llama3 70B, which had a precision of 0.996 but a lower recall of 0.851.For Llama3 70B, while excelling in precision for leadership, it did not perform as well in recall compared to GPT-4.Llama2 70B and Mistral 7x8B have shown moderate to poor performance in all metrics.The bar plot in Figure 3 shows notable differences in the label distributions of the four models-GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B-on the role classification tasks.GPT-4 predominantly assigned authors to the Leadership role more often than the other models, indicating its strong tendency to prioritize leadership activities.</p>
<p>Conversely, Mistral 7x8B and Llama2 70B showed a higher tendency to classify authors in the Direct Support role, suggesting that these models emphasize direct support activities more than GPT-4 and Llama3 70B.For the Indirect Support role, the distribution was relatively balanced across the models, with no single model showing a significantly higher number of classifications.</p>
<p>Given these results, GPT-4 was selected for further role classification of the remaining data due to its higher classification accuracy.We then used GPT-4 to perform additional inference on the extended dataset to ensure comprehensive and accurate classification of author roles within scientific teams.</p>
<p>Comparison of LLMs with Traditional Models</p>
<p>To compare the performance of LLMs with traditional machine learning models for the task of role classification in scientific teams, we used XGBoost, a well-known and robust machine learning algorithm, as a representative of traditional models due to its strengths in classification tasks [43].</p>
<p>For our comparison, XGBoost was used with a boosting learning rate of 0.2.The number of estimators was set to 1000 with a maximum tree depth of 8 for base learners.We split our dataset into training and validation sets using a  We also explored the performance of encoder-based transformer models, specifically BERT and RoBERTa, for this classification task.In our experiments, we fine-tuned both BERT and RoBERTa base models on our dataset using a learning rate of 2 × 10 −5 .Both models were trained for 5 epochs, using a stratified split based on the target variable, with a ratio of 0.2 for the validation set.BERT achieved an average F1 score of 0.9048, and RoBERTa achieved an average F1 score of 0.8794.Although these models also achieved promising results, it is important to emphasize several key advantages of LLMs over traditional machine learning models.First, LLMs, such as GPT-4, can perform classification tasks without requiring any training data.This few-shot capability allows LLMs to generalize from a large body of knowledge and apply it to new, unseen tasks.Second, unlike models that rely heavily on manually labeled training datasets, LLMs can infer roles and classifications directly from text.This significantly reduces the time and effort required for data preparation.Third, LLMs have advanced natural language understanding capabilities that allow them to capture complex contexts and nuances in author contributions.This results in more accurate and contextually relevant classifications.</p>
<p>Despite the lack of fine-tuning, GPT-4 showed an impressive overall score, approaching an F1 score of 0.97.This result shows the potential of LLMs for role classification tasks.One key reason LLMs outperform those models is their ability to handle complex and nuanced contribution statements that may contain ambiguous language or non-standard phrasing.For example, phrases such as "contributed extensively to the work presented in this paper" or "was involved in discussions that developed the understanding of the physical processes" present challenges for rule-based approaches, which rely on specific keywords.LLMs excel at interpreting such statements by capturing the context and inferring the appropriate role classification.This deep contextual understanding allows LLMs to surpass models such as XGBoost and BERT, particularly when authors mention multiple roles without emphasizing a primary one.While machine learning models such as XGBoost and BERT have their strengths, especially in structured data classification, the inherent advantages of LLMs make them a powerful tool for understanding and categorizing scientific roles.</p>
<p>Therefore, our comparison shows that while models such as XGBoost and BERT perform well, LLMs offer superior performance and flexibility, especially in scenarios where labeled data is scarce or when the task requires deep contextual understanding.The use of LLMs for role classification in scientific teams thus represents a promising direction for future research and applications.</p>
<p>Scalable Predictive Modeling for Author Role Classification</p>
<p>To enable large-scale and efficient analysis of author roles in scientific collaborations, we address the computational challenges associated with directly applying GPT-4 for large-scale predictions.While GPT-4 demonstrates exceptional performance in classifying author roles, its inference is computationally expensive and impractical for analyzing millions of authors.To overcome this challenge, we utilize labels generated from GPT-4 to train a dense neural network, which enables efficient classification of new data with minimal computational overhead.</p>
<p>Dataset Construction and Feature Engineering</p>
<p>To build our predictive model, we first constructed a robust dataset derived from the OpenAlex database, which provides extensive bibliometric and metadata on academic publications.We then moved on to building a predictive role classification model using feature engineering.</p>
<p>As shown in Table 3, we focused on 10 predictive features that capture the complex dynamics of academic authorship and contributions.Of these, eight features are derived from Xu et al [2].These features are:</p>
<ol>
<li>Contribution to References assesses the extent to which an author contributes references in their work.8. Total Publications reflects the total number of papers an author has published.
Contribution (References) = Overlap (References) Total (References)</li>
</ol>
<p>Total Publications = (Publications)</p>
<p>These eight features provide a well-rounded assessment of an author's contributions in the context of collaborative research teams.To further improve the model performance, we introduce two additional features to capture aspects of sustained research impact and collaboration diversity not fully represented by the original features:</p>
<ol>
<li>Citation Impact per Year is calculated by taking the total number of citations an author has received across all publications and dividing it by the number of years the author has been active, providing a measure of the average citation impact per year.</li>
</ol>
<p>Citation Impact per Year = Citation (All Publications) Years (Active)</p>
<p>(2) Institutional Diversity.With the inclusion of these features, the model's F1 score improved to approximately 0.76.This demonstrates the positive impact of these additional features on the accuracy of our role classification model.The model was trained for 20 epochs, and its performance was optimized through hyperparameter tuning.By ensuring a balanced and representative sample, we have laid a solid foundation for accurate and reliable author role classification.</p>
<p>We also experimented with the same dataset using XGBoost, configured with a boosting learning rate of 0.2.The number of estimators was set to 1000, and the maximum tree depth was set to 8 for base learners.The XGBoost model achieved a slightly better F1 score of 0.78 compared to the dense neural network.However, considering our goal to develop a model suitable for larger datasets, we chose the dense neural network due to its simplicity and scalability.</p>
<p>Furthermore, in the study by Xu et al., they achieved an F1 score of approximately 0.79 by utilizing a larger dataset and more complex modeling techniques.In contrast, our study achieved an F1 score of 0.76 using a smaller dataset and a relatively simple dense neural network model.This demonstrates that our approach is both robust and efficient, achieving comparable performance with less computational complexity and resource investment.</p>
<p>Interpretability Analysis Using SHAP</p>
<p>To further validate feature importance in our classification task, we apply SHAP (SHapley Additive exPlanations) [9,44] to investigate how each predictive feature contributes to the model's decision-making process.Specifically, we utilize Gradient SHAP, which efficiently estimates feature attributions for deep learning models using gradient-based sampling.the SHAP value for a feature i is calculated as:
ϕ i = S⊆F i / ∈S |S|!(M − |S| − 1)! M ! [f (S ∪ {i}) − f (S)]
where:</p>
<p>• F is the set of all features,</p>
<p>• S is a subset of features excluding i,</p>
<p>• f (S) is the model's prediction using only the features in S,</p>
<p>• ϕ i is the SHAP value for feature i.As shown in Figure 5, Probability of Leading Correspondence and Probability of Leading emerge as the most influential features in distinguishing leadership roles.These features likely capture the responsibility and visibility of an author within a research project, suggesting that consistent first or corresponding authorship is a strong indicator of leadership roles.Their high SHAP values further confirm their direct impact on classification predictions, reinforcing their centrality in the model's decision-making process.</p>
<p>Subsequently, Contribution to References and Contribution to Topics also hold substantial importance, showcasing their role in capturing intellectual contributions.A higher contribution to references indicates an author's influence on shaping discussions within a paper, while topic contributions suggest an author's role in guiding the thematic direction of research.The positive SHAP impact of these features suggests that intellectual involvement plays a critical role in distinguishing different author roles.</p>
<p>Additionally, the newly proposed features in this paper, including: Citation Impact per Year and Institutional Diversity, provide meaningful contributions.Citation Impact per Year accounts for the sustained influence of an author's research over time, making it a valuable indicator of long-term academic impact.Its moderate importance in the SHAP analysis suggests that while citations are significant, they are not the sole determinant of leadership roles.Institutional Diversity, on the other hand, captures the extent of an author's research collaborations across different institutions.A higher value for this feature suggests exposure to diverse academic environments, which may contribute to broader collaborative roles rather than direct leadership.</p>
<p>Conclusions</p>
<p>In this paper, we have presented a transformative approach to classifying author roles within scientific teams using advanced LLMs such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B.By integrating few-shot prompting and extensive feature engineering, we categorized author roles into Leadership, Direct Support, and Indirect Support.Our results showed that GPT-4 achieved the highest accuracy across multiple categories, outperforming other models in role classification tasks.By employing these advanced LLMs, we have improved the understanding and analysis of team roles in scientific research, providing a more detailed and accurate classification than traditional models.</p>
<p>We developed a predictive deep learning model incorporating ten features that capture the complex dynamics of academic authorship and contributions.These features included contribution to references, contribution to topics, probability of leading, probability of managing correspondence, career age, number of citations, unique topics, total publications, citation impact per year, and institutional diversity.The model, trained on a dataset derived from the OpenAlex database and self-reported contribution data, achieved an F1 score of 0.76, indicating robust performance in classifying author roles with a Dense Neural Network (DNN).</p>
<p>In addition, this methodology allows us to further calculate the L-Ratio, which quantifies the proportion of leadership roles within research teams.In future work, by integrating the L-Ratio with other variables, we can explore the relationship between leadership dynamics and various aspects of academic performance.For instance, using the L-Ratio to analyze the long-term performance of junior team members can provide insights into their career trajectories and contributions over time.This analysis aims to highlight the importance of leadership dynamics in scientific collaboration, providing valuable information on how junior researchers' roles and impacts evolve within academic teams.</p>
<p>Overall, our approach not only improves the accuracy of author role classification, but also provides deeper insights into the complex dynamics of scientific teamwork.This work represents a significant step forward in understanding contributions and collaborations within scientific teams, thereby improving our ability to analyze and promote effective scientific research environments.</p>
<p>7 Discussion: Technical and Practical Implications</p>
<p>Our study introduces a technical framework that leverages few-shot prompting with LLMs and a scalable neural network to generate high-quality labels without extensive training data.The use of SHAP for interpretability highlights the key factors-such as the probability of leading-that define leadership roles.Practically, our approach supports a deeper understanding of scientific collaboration.Moreover, the scalable predictive model paves the way for automated monitoring of author contributions across large datasets.</p>
<p>Despite these promising results, our study has limitations that suggest directions for future research.While advanced LLMs such as GPT-4 demonstrate superior performance in author role classification, their proprietary nature and computational requirements may limit accessibility for some researchers.Additionally, our reliance on data derived from the OpenAlex database may not capture the full diversity of author contributions across all scientific discipline and publication venues.To address these limitations, future work could involve exploring zero-shot inference with</p>
<p>Figure 1 :
1
Figure 1: Illustration of the workflow involving data sampling, preprocessing, contribution assigning, and classification task for the LLM-based role classification.</p>
<p>Figure 2 :
2
Figure 2: Illustration of the workflow involving prompt generation, LLM inference, and classification results.</p>
<p>Figure 3 :
3
Figure 3: Label distribution comparison across models.The figure illustrates the distribution of assigned roles-Leadership, Direct Support, and Indirect Support-by four models: GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B.</p>
<p>Figure 4 :
4
Figure 4: Line plot for F1-score by class for each model.</p>
<p>Figure 5 :
5
Figure 5: SHAP summary plot showing feature importance and the directional impact of each feature on the model's predictions.</p>
<p>Table 1 :
1
Comparison of Model Performance on Role Classification Tasks.
IndexModel NameRoleF1-Score Precision RecallLeadership0.9930.9950.9911GPT-4-1106Direct Support Indirect Support0.950 0.9470.945 0.9250.953 0.970Macro Avg.0.9630.9550.972Leadership0.9180.9960.8512Llama3 70BDirect Support Indirect Support0.569 0.8410.414 0.7480.911 0.961Macro Avg.0.7760.7190.907Leadership0.9080.9490.8703Llama2 70BDirect Support Indirect Support0.352 0.5140.314 0.3820.400 0.789Macro Avg.0.5910.5480.686Leadership0.9510.9690.9334Mistral 7x8BDirect Support0.5800.5380.629Indirect Support0.7830.6770.926Macro Avg.0.7710.7280.830</p>
<p>Table 2 :
2
Performance of BERT and RoBERTa Models on Role Classificaiton.
ModelsLearning RateEpoch Avg. F1-Score10.792720.8560BERT-base2 × 10 −530.892540.904850.886910.844420.8570RoBERTa-base2 × 10 −530.819140.863750.8794</p>
<p>Table 3 :
3
Categorization of the 10 Extracted Features.Contribution to Topics assesses the direction and influence of an author's previous work on the current research topic.Probability of Leading indicates the likelihood that an author has often been the first author on previous papers.Probability of Leading Correspondence shows the probability that an author has been the corresponding author on previous papers.Career Age measures the total number of years an author has been active in research.
IndexCategoryFeature Name1 2Contribution MetricsContribution to References Contribution to Topics3 4Leadership MetricsProbability of Leading Probability of Leading Correspondence5Career Duration MetricsCareer Age6 7Citation MetricsCitation Count Citation Impact per Year8Research Diversity MetricsUnique Topics9Publication MetricsTotal Publications10Collaboration Diversity MetricsInstitutional Diversity2. Contribution (Topics) =Overlap (Topics) Total (Topics)3. Probability (Leading) =Num (Times First Author) Total (Papers)4. Probability (Leading Correspondence) =Num (Times Corresponding Author) Total (Papers)5. Career Age = Year (Last Publication) − Year (First Publication)6. Citation Count refers to the total number of citations an author has received for all of his or her previouspublications.Citation Count =Citations (All Publications)7. Unique Topics is operationalized by the number of unique research topics an author has covered in his or herpublication history.UniqueTopics = Count (Unique Keywords)
LLM-Based Role Classification: Methods and EvaluationIn this stage, we focus on classifying and categorizing roles within scientific teams using self-reported data. Our methodology involves the use of open-source instructional LLMs and the GPT-4 model, augmented with few-shot prompting. Through this process, we aim to achieve a detailed categorization of scientific roles, thereby improving our understanding of team dynamics in scientific research environments. This classification serves as a crucial foundation for subsequent predictive modeling. While LLMs provide high-quality classifications, their computational demands make them impractical for large-scale applications. To address this, the LLM-generated labels will be leveraged to train a predictive model capable of efficiently scaling to extensive datasets. Section
details this predictive modeling process, which extends our role classification framework to broader scientific collaborations.
2. Institutional Diversity measures the number of unique institutions an author has been affiliated with or collaborated with over their career.A higher diversity of institutions suggests a broader network and potentially more diverse research experiences and influences.Institutional Diversity = Count (Unique Institutions)These 10 features were selected based on their relevance and ability to comprehensively capture the various dimensions of an author's contributions in collaborative research settings.The combination of these features enables the model to effectively classify authors' roles, ensuring a balanced and comprehensive assessment of their contributions.Data Splitting and NormalizationTo ensure a robust evaluation of the prediction models, we expanded our dataset by further inferring the roles of 10,000 distinct authors using the classification framework developed in the first phase, resulting in a total dataset of about 15,000 data points.Originally, the dataset included three classes: Leadership, Direct Support, and Indirect Support.For the purpose of this binary classification task, we merged the Direct Support and Indirect Support roles into a single "Support" category, while maintaining "Leadership" as a distinct category.This expanded dataset included detailed role classifications based on each author's contributions.The dataset was then divided into training and test sets using stratified sampling to maintain the distribution of the target variable.A ratio of 0.8 to 0.2 was used for the split, stratifying by two types of roles classified as leadership and non-leadership types.Additionally, Normalizing the dataset was a critical step in preparing it for predictive modeling.We applied a min-max scaling to each feature to ensure that they contributed equally to the model, without any single feature disproportionately influencing the results due to its size.The formula used for min-max normalization is given by:Predictive Model Training and PerformanceFor the role classification task, we constructed a Dense Neural Network (DNN) model.The architecture of the DNN was designed to effectively process the 10 predictive features and accurately classify author roles.We chose this model due to its simplicity and adequacy for our task.Dense neural networks are straightforward to implement and require less computational power compared to more complex architectures, making them suitable for large-scale applications.The model can process large datasets quickly, which is essential for applying the classification to extensive corpora of scientific publications.where:• x represents the input vector of the predictive features.• W 1 , W 2 , W 3 are the weight matrices for the hidden layers and output layer.• σ 1 , σ 2 , σ 3 are the ReLU (Rectified Linear Unit) activation functions for the hidden layers and a Sigmoid activation function for the output layer.• b 1 , b 2 , b 3 are the bias vectors for the hidden layers and the output layer.Initially, the model was trained using only the first eight features.This version of the model achieved an F1 score of about 0.74.To improve model performance, we introduced two additional features: (1) Citation Impact per Year and LLMs to obtain more natural and pattern-rich results.Furthermore, analyzing the patterns and insights derived from zero-shot inference could enhance our understanding of author roles and contributions, particularly in handling complex or ambiguous cases that challenge traditional rule-based approaches.Data AvailabilityThe self-reported data analyzed in this paper is available at https://github.com/fenglixu/Self-report-Contribution-Data.All data used in this study are open and available at OpenAlex https://openalex.org/.Competing interestsThe authors do not declare any competing interests.AcknowledgmentsYi Bu acknowledges the support from the National Natural Science Foundation of China (#72474009, #72104007, and #72174016) and from the 2024 Cultural Research Project of Ningbo under Grant WH24-2-4.The authors are grateful to Yifan Tian and Zonghao Yuan who had fruitful discussions with the authors.
The increasing dominance of teams in production of knowledge. Stefan Wuchty, Benjamin F Jones, Brian Uzzi, Science. 31658272007</p>
<p>Flat teams drive scientific innovation. Fengli Xu, Lingfei Wu, James Evans, Proceedings of the National Academy of Sciences. 11923e22009271192022</p>
<p>Ai for social science and social science of ai: A survey. Ruoxi Xu, Yingfei Sun, Mengjie Ren, Shiguang Guo, Ruotong Pan, Hongyu Lin, Le Sun, Xianpei Han, Information Processing &amp; Management. 6131036652024</p>
<p>Predicting research trends with semantic and neural networks with an application in quantum physics. Mario Krenn, Anton Zeilinger, Proceedings of the National Academy of Sciences. 11742020</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Openalex: A fully-open index of scholarly works, authors, venues, institutions, and concepts. Jason Priem, Heather Piwowar, Richard Orr, arXiv:2205.018332022arXiv preprint</p>
<p>Scott Lundberg, arXiv:1705.07874A unified approach to interpreting model predictions. 2017arXiv preprint</p>
<p>Atypical combinations and scientific impact. Brian Uzzi, Satyam Mukherjee, Michael Stringer, Ben Jones, Science. 34261572013</p>
<p>What is research collaboration? Research policy. Sylvan Katz, Ben R Martin, 199726</p>
<p>Studying research collaboration using co-authorships. Göran Melin, Olle Persson, Scientometrics. 361996</p>
<p>Beyond team types and taxonomies: A dimensional scaling conceptualization for team description. Bianca John R Hollenbeck, Maartje E Beersma, Schouten, 2012Academy of Management Review37</p>
<p>Evidence for a collective intelligence factor in the performance of human groups. Anita Williams Woolley, Christopher F Chabris, Alex Pentland, Nada Hashmi, Thomas W Malone, science. 33060042010</p>
<p>Large teams develop and small teams disrupt science and technology. Lingfei Wu, Dashun Wang, James A Evans, Nature. 56677442019</p>
<p>Collaborative research across disciplinary and organizational boundaries. Jonathon N Cummings, Sara Kiesler, Social studies of science. 3552005</p>
<p>Hierarchical cultural values predict success and mortality in high-stakes teams. Eric M Anicich, Roderick I Swaab, Adam D Galinsky, Proceedings of the National Academy of Sciences. 11252015</p>
<p>Division of labor in collaborative knowledge production: The role of team size and interdisciplinarity. Carolin Haeussler, Henry Sauermann, Research Policy. 4961039872020</p>
<p>A new methodology for constructing a publication-level classification system of science. Ludo Waltman, Nees Jan, Van Eck, Journal of the American Society for Information Science and Technology. 63122012</p>
<p>Co-citation analysis, bibliographic coupling, and direct citation: Which citation approach represents the research front most accurately. Kevin W Boyack, Richard Klavans, Journal of the American Society for information Science and Technology. 61122010</p>
<p>Fast unfolding of communities in large networks. Jean-Loup Vincent D Blondel, Renaud Guillaume, Etienne Lambiotte, Lefebvre, Journal of statistical mechanics: theory and experiment. 10P100082008. 2008</p>
<p>Analysing scientific networks through co-authorship. Wolfgang Glänzel, András Schubert, Handbook of quantitative science and technology research: The use of publication and patent statistics in studies of S&amp;T systems. Springer2004</p>
<p>A method for identifying different types of university research teams. Zhe Cheng, Yihuan Zou, Yueyang Zheng, Humanities and Social Sciences Communications. 1112024</p>
<p>Team roles and hierarchic system in group discussion. Group Decision and Negotiation. Manabu Fujimoto, 201625</p>
<p>Qualitative research as the cornerstone methodology for understanding leadership. Jay A Conger, The Leadership Quarterly. 911998</p>
<p>Peer review, interdisciplinarity, and serendipity. Britt Holbrook, 2017</p>
<p>Collaborative research in the social sciences: Multiple authorship and publication credit. James W Endersby, Social Science Quarterly. 1996</p>
<p>Beyond authorship: Attribution, contribution, collaboration, and credit. Amy Brand, Liz Allen, Micah Altman, Marjorie Hlava, Jo Scott, 2015Learned Publishing28</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Attention is all you need. Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm) network. Alex Sherstinsky, Physica D: Nonlinear Phenomena. 4041323062020</p>
<p>Yinhan Liu, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. 2019364arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of naacL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , naacL-HLTMinneapolis, Minnesota20191</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of machine learning research. 211402020</p>
<p>Improving language understanding by generative pre-training. Alec Radford, 2018</p>
<p>Evaluating research quality with large language models: an analysis of chatgpt's effectiveness with different settings and inputs. Thelwall, Journal of Data and Information Science. 2024</p>
<p>Mike Thelwall, Abdallah Yaghi, arXiv:2409.16695which fields can chatgpt detect journal article quality? an evaluation of ref2021 results. 2024arXiv preprint</p>
<p>Scientific progress or societal progress? a language modelbased classification of the aims of the research in scientific publications. A Language Modelbased Classification of the Aims of the Research in Scientific Publications. Mengjia Wu, Gunnar Sivertsen, Lin Zhang, Fan Qi, Yi Zhang, April 22, 20242024</p>
<p>Openai chatgpt generated literature review: Digital twin in healthcare. Ömer Aydın, Enis Karaarslan ; Aydın, Ö Karaarslan, E , Emerging Computer Technologies. Ö. Aydın22022. 2022OpenAI ChatGPT Generated Literature Review: Digital Twin in Healthcare</p>
<p>Chatgpt outperforms crowd workers for text-annotation tasks. Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli, Proceedings of the National Academy of Sciences. 12030e23050161202023</p>
<p>Can chatgpt write a good boolean query for systematic review literature search?. Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2023</p>
<p>Marketing with chatgpt: Navigating the ethical terrain of gpt-based chatbot technology. Pablo Rivas, Liang Zhao, AI. 422023</p>
<p>A comparative analysis of gradient boosting algorithms. Candice Bentéjac, Anna Csörgő, Gonzalo Martínez-Muñoz, Artificial Intelligence Review. 541937-1967, 2021</p>
<p>Scott M Lundberg, Su-In Gabriel G Erion, Lee, arXiv:1802.03888Consistent individualized feature attribution for tree ensembles. 2018arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
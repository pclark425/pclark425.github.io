<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2732 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2732</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2732</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-269457565</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.18978v1.pdf" target="_blank">Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs</a></p>
                <p><strong>Paper Abstract:</strong> There has been a growing interest in developing learner models to enhance learning and teaching experiences in educational environments. However, existing works have primarily focused on structured environments relying on meticulously crafted representations of tasks, thereby limiting the agent’s ability to generalize skills across tasks. In this paper, we aim to enhance the generalization capabilities of agents in open-ended text-based learning environments by integrating Reinforcement Learning (RL) with Large Language Models (LLMs). We investigate three types of agents: (i) RL-based agents that utilize natural language for state and action representations to find the best interaction strategy, (ii) LLM-based agents that leverage the model’s general knowledge and reasoning through prompting, and (iii) hybrid LLM-assisted RL agents that combine these two strategies to improve agents’ performance and generalization. To support the development and evaluation of these agents, we introduce PharmaSimText , a novel benchmark derived from the PharmaSim virtual pharmacy environment designed for practicing diagnostic conversations. Our results show that RL-based agents excel in task completion but lack in asking quality diagnostic questions. In contrast, LLM-based agents perform better in asking diagnostic questions but fall short of completing the task. Finally, hybrid LLM-assisted RL agents enable us to overcome these limitations, highlighting the potential of combining RL and LLMs to develop high-performing agents for open-ended learning environments.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2732.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2732.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflective-LLM-agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflective LLM-based Agent (GPT-4) with Textual Learning Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent (GPT-4) that stores distilled textual 'learnings' from prior trials in a small textual memory (causal statements like 'X is necessary for Y') which is appended to prompts in subsequent trials to adapt and refine strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflective LLM-based Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A prompting-based agent that queries GPT-4 at every step; besides the immediate dialogue history it maintains an explicit textual memory of prior learnings (constructed in causal "X is necessary for Y" format) which is included in prompts for later trials to inform action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>PharmaSimText</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>A text-based interactive educational benchmark adapted from PharmaSim where agents play the role of a pharmacy assistant, ask diagnostic questions, and must identify the most probable cause of a patient's problem across many rephrasings and patient profiles.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>textual learning memory (declarative/episodic style)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Sequential appended textual entries (learned causal statements) stored as plain text and included directly in the LLM prompt for later trials.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Summaries/learned insights from prior trials with the same patient/cause, encoded as causal rules (e.g., 'X is necessary for Y'), plus prior interaction histories for that patient.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Full memory for same patient/cause is included in the prompt (implicit relevance by restricting memory to prior engagements with same patient/cause); no separate retrieval module described.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>After each trial the agent prompts the LLM to evaluate its strategy/outcomes and appends updated learnings to the textual memory before the next trial.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>To adapt and refine action selection and diagnostic planning across repeated trials with the same patient/cause (i.e., for rapid task adaptation and improved decision-making).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative: Reflection (textual memory) did not significantly change the Combined Score for the purely LLM-based agent; it led to shorter diagnostic conversations and reduced Trajectory Quality while modestly improving diagnostic accuracy. No numeric success-rate percentages or exact scores are reported in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Qualitative baseline (none-reflective LLM): longer, higher-quality dialogues (higher Trajectory Quality) but lower diagnostic accuracy compared to reflective variant; no numeric metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Reflection (textual memory) can improve diagnosis accuracy for the LLM agent but often shortens conversations and reduces trajectory quality; overall benefit is mixed and depends on agent type and how reflection is integrated.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>For the LLM-only agent, the reflective memory caused shorter dialogues and reduced conversation quality; reflection alone did not translate to a clear combined improvement. No capacity limits or retrieval failure analyses are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>The paper used a small textual memory of prior learnings encoded in causal format ("X is necessary for Y"); while reflective vs none-reflective was compared, no alternative textual memory architectures were benchmarked, and the causal-format reflective memory was what was evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2732.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2732.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflective-SA-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflective Suggestion-Assisted RL (SA-RL) Agent with LLM Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid agent where an RL agent (DRRN) computes Q-values but an LLM (GPT-4) is prompted to suggest k candidate actions each step; the LLM additionally maintains a reflective textual memory of prior learnings which is used across trials to improve suggestions and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflective Suggestion-Assisted RL (SA-RL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DRRN-based RL agent uses pre-trained fastText embeddings and a summed state updater; at inference the LLM (GPT-4) suggests k actions (k=5 during interactions, k=2 in post-test); in the reflective variant the LLM uses and updates a textual memory of past learnings between trials to refine suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>PharmaSimText</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-based pharmacy diagnostic scenarios requiring asking key questions and selecting the most probable cause; multiple rephrasings and patient profiles test generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>textual learning memory used by the LLM (episodic/semantic distilled memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Sequential appended textual entries (learnings) included in prompts to GPT-4 when generating suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Prior experiences with the same patient/cause, distilled learnings and rules from previous trials used to inform future suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Memory entries are included in prompts for the same patient/cause (implicit relevance); no separate retrieval index or vector DB described.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>After each trial the LLM is prompted to evaluate outcomes and update the textual memory; updated memory is used in subsequent trials.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>To improve LLM-proposed candidate actions and thus help the RL agent generalize to rephrased or unseen phrasings and to increase diagnostic accuracy while retaining dialog quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative: Reflective SA-RL showed considerable improvement in diagnostic accuracy and Combined Score relative to none-reflective variants; SA-RL (reflective) achieved the best combined balance of Post-test Performance and Trajectory Quality among all agents. Exact numeric metrics are not provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Qualitative: None-reflective SA-RL performed worse than reflective SA-RL on diagnostic accuracy and combined metrics; SA-RL (none-reflective) still outperformed purely LLM-based agents in some measures but less so than the reflective version.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Reflection substantially enhanced hybrid agents, particularly SA-RL, because the LLM's broader suggestion space benefits more from iterative, memory-driven refinements; memory helped SA-RL balance conversation quality and diagnostic accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Not explicitly quantified; the paper notes general challenges when LLM suggestions can be invalid (handled by retry and fallback to semantically nearest valid action). No explicit scaling/capacity limits were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Reflective SA-RL (LLM suggestions + textual reflective memory across trials) — this configuration yielded the strongest Combined performance according to the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2732.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2732.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflective-DA-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflective Decision-Assisted RL (DA-RL) Agent with LLM Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid agent where the RL agent (DRRN) proposes its top-k actions and an LLM (GPT-4) selects the best among those; the reflective variant allows the LLM to keep and use a textual memory across trials to influence its selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflective Decision-Assisted RL (DA-RL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DRRN proposes top-k candidate actions (k=5 during interactions, k=2 post-test); GPT-4 is prompted to pick the best action among those; in the reflective variant GPT-4 is given and updates a textual memory of prior learnings between trials.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>PharmaSimText</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Interactive diagnostic dialogues in pharmacy scenarios requiring key questions and selecting the most probable cause over varied phrasings and patient profiles.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>textual reflective memory maintained by the LLM</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Sequential textual memory entries appended to prompts for decision selection.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Prior trial outcomes and distilled learnings for same patient/cause.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Memory included in the LLM prompt for selection; relevance via restricting memory to prior experiences with same patient/cause.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Memory updated after each trial by prompting the LLM to reflect on outcomes and append learnings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>To influence the LLM's selection among RL-suggested candidate actions to improve decision quality and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative: Reflective DA-RL benefited from reflection but less than SA-RL; DA-RL underperformed relative to SA-RL and RL-based agent in some measures, partly due to longer trajectories causing unfamiliar states for the DRRN. No numeric metrics are provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Qualitative: None-reflective DA-RL performed worse than reflective DA-RL; both DA-RL variants generally trailed SA-RL in Combined score.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Reflection improved DA-RL performance but to a lesser extent than SA-RL because DA-RL's decisions are more constrained by the RL-provided top-k candidates, limiting the impact of LLM memory-driven reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>DA-RL suffered when trajectories were long and entered unfamiliar states where the DRRN's suggestions were less accurate, which limited the usefulness of LLM memory-driven selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Reflective DA-RL improves over none-reflective DA-RL but was not as effective as reflective SA-RL; best configuration reported overall was reflective SA-RL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2732.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2732.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRRN-sum-state-updater</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DRRN with Summation-based State Updater (aggregated observation memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Deep Reinforced Relevance Network (DRRN) adapted to PharmaSimText which aggregates the entire observation history by summing pre-trained sentence embeddings to form the current state embedding (e(st) = e(st-1) + e(ot)); summation was chosen after comparing multiple updater strategies for stability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RL-based DRRN (with summation state updater)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DRRN that encodes text states and actions via pre-trained fastText embeddings and RNN/MLP encoders; a state-updater unit maintains full-history context by summing embedded observations to produce the current state embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>PharmaSimText</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-based interactive diagnostic scenarios in a pharmacy setting where full interaction history is necessary to infer the current state and ask key diagnostic questions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>aggregated sequential observation memory (working/state memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Vector-based sequential buffer where observation embeddings are aggregated by elementwise summation into a single state embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>All past observations (embedded via fastText); the aggregated vector represents the whole observation history.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>No retrieval per se — the aggregated state embedding (sum of all observations) is used directly as the current state input to the state encoder/scorer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>After each new observation, the embedded observation is added to the running sum: e(st) = e(st-1) + e(ot).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>To give the DRRN access to the full interaction history necessary for correct Q-value estimation and action selection in a partially observable text environment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative: The summation-based updater led to the most stable training among five tested updater methods (mean, max, sum, LSTM, LSTM+self-attention) and was adopted for experiments; no numeric stability or performance metrics are provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Comparative (ablation across updater designs): Other updater methods (RNN/LSTM variants, pooling) yielded less stable training and suboptimal embeddings when trained end-to-end with RL; explicit numeric baselines are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Aggregating all observation embeddings via summation provided the most stable training signal; training encoders with RL loss alone (without pre-trained embeddings and this updater) led to instability and suboptimal embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>The paper notes instability when relying solely on RL-aligned losses to learn RNN encoders; no analysis of long-term accumulation errors or capacity bounds for the summation approach is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Summation-based state updater (e(st)=e(st-1)+e(ot)) outperformed mean/max pooling and LSTM/LSTM+attention in training stability and was chosen for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization <em>(Rating: 2)</em></li>
                <li>Reflexion: An Autonomous Agent with Dynamic Memory and Self-Reflection <em>(Rating: 2)</em></li>
                <li>Deep Reinforcement Learning with a Natural Language Action Space <em>(Rating: 2)</em></li>
                <li>ScienceWorld: Is Your Agent Smarter than a 5th Grader? <em>(Rating: 1)</em></li>
                <li>Re-Act: Synergizing Reasoning and Acting in Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2732",
    "paper_id": "paper-269457565",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "Reflective-LLM-agent",
            "name_full": "Reflective LLM-based Agent (GPT-4) with Textual Learning Memory",
            "brief_description": "An LLM-based agent (GPT-4) that stores distilled textual 'learnings' from prior trials in a small textual memory (causal statements like 'X is necessary for Y') which is appended to prompts in subsequent trials to adapt and refine strategy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Reflective LLM-based Agent",
            "agent_description": "A prompting-based agent that queries GPT-4 at every step; besides the immediate dialogue history it maintains an explicit textual memory of prior learnings (constructed in causal \"X is necessary for Y\" format) which is included in prompts for later trials to inform action selection.",
            "base_model_size": null,
            "game_benchmark_name": "PharmaSimText",
            "game_description": "A text-based interactive educational benchmark adapted from PharmaSim where agents play the role of a pharmacy assistant, ask diagnostic questions, and must identify the most probable cause of a patient's problem across many rephrasings and patient profiles.",
            "uses_memory": true,
            "memory_type": "textual learning memory (declarative/episodic style)",
            "memory_structure": "Sequential appended textual entries (learned causal statements) stored as plain text and included directly in the LLM prompt for later trials.",
            "memory_content": "Summaries/learned insights from prior trials with the same patient/cause, encoded as causal rules (e.g., 'X is necessary for Y'), plus prior interaction histories for that patient.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Full memory for same patient/cause is included in the prompt (implicit relevance by restricting memory to prior engagements with same patient/cause); no separate retrieval module described.",
            "memory_update_strategy": "After each trial the agent prompts the LLM to evaluate its strategy/outcomes and appends updated learnings to the textual memory before the next trial.",
            "memory_usage_purpose": "To adapt and refine action selection and diagnostic planning across repeated trials with the same patient/cause (i.e., for rapid task adaptation and improved decision-making).",
            "performance_with_memory": "Qualitative: Reflection (textual memory) did not significantly change the Combined Score for the purely LLM-based agent; it led to shorter diagnostic conversations and reduced Trajectory Quality while modestly improving diagnostic accuracy. No numeric success-rate percentages or exact scores are reported in the text.",
            "performance_without_memory": "Qualitative baseline (none-reflective LLM): longer, higher-quality dialogues (higher Trajectory Quality) but lower diagnostic accuracy compared to reflective variant; no numeric metrics reported.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Reflection (textual memory) can improve diagnosis accuracy for the LLM agent but often shortens conversations and reduces trajectory quality; overall benefit is mixed and depends on agent type and how reflection is integrated.",
            "memory_limitations": "For the LLM-only agent, the reflective memory caused shorter dialogues and reduced conversation quality; reflection alone did not translate to a clear combined improvement. No capacity limits or retrieval failure analyses are provided.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "The paper used a small textual memory of prior learnings encoded in causal format (\"X is necessary for Y\"); while reflective vs none-reflective was compared, no alternative textual memory architectures were benchmarked, and the causal-format reflective memory was what was evaluated.",
            "uuid": "e2732.0",
            "source_info": {
                "paper_title": "Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Reflective-SA-RL",
            "name_full": "Reflective Suggestion-Assisted RL (SA-RL) Agent with LLM Memory",
            "brief_description": "A hybrid agent where an RL agent (DRRN) computes Q-values but an LLM (GPT-4) is prompted to suggest k candidate actions each step; the LLM additionally maintains a reflective textual memory of prior learnings which is used across trials to improve suggestions and generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Reflective Suggestion-Assisted RL (SA-RL)",
            "agent_description": "DRRN-based RL agent uses pre-trained fastText embeddings and a summed state updater; at inference the LLM (GPT-4) suggests k actions (k=5 during interactions, k=2 in post-test); in the reflective variant the LLM uses and updates a textual memory of past learnings between trials to refine suggestions.",
            "base_model_size": null,
            "game_benchmark_name": "PharmaSimText",
            "game_description": "Text-based pharmacy diagnostic scenarios requiring asking key questions and selecting the most probable cause; multiple rephrasings and patient profiles test generalization.",
            "uses_memory": true,
            "memory_type": "textual learning memory used by the LLM (episodic/semantic distilled memory)",
            "memory_structure": "Sequential appended textual entries (learnings) included in prompts to GPT-4 when generating suggestions.",
            "memory_content": "Prior experiences with the same patient/cause, distilled learnings and rules from previous trials used to inform future suggestions.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Memory entries are included in prompts for the same patient/cause (implicit relevance); no separate retrieval index or vector DB described.",
            "memory_update_strategy": "After each trial the LLM is prompted to evaluate outcomes and update the textual memory; updated memory is used in subsequent trials.",
            "memory_usage_purpose": "To improve LLM-proposed candidate actions and thus help the RL agent generalize to rephrased or unseen phrasings and to increase diagnostic accuracy while retaining dialog quality.",
            "performance_with_memory": "Qualitative: Reflective SA-RL showed considerable improvement in diagnostic accuracy and Combined Score relative to none-reflective variants; SA-RL (reflective) achieved the best combined balance of Post-test Performance and Trajectory Quality among all agents. Exact numeric metrics are not provided in the text.",
            "performance_without_memory": "Qualitative: None-reflective SA-RL performed worse than reflective SA-RL on diagnostic accuracy and combined metrics; SA-RL (none-reflective) still outperformed purely LLM-based agents in some measures but less so than the reflective version.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Reflection substantially enhanced hybrid agents, particularly SA-RL, because the LLM's broader suggestion space benefits more from iterative, memory-driven refinements; memory helped SA-RL balance conversation quality and diagnostic accuracy.",
            "memory_limitations": "Not explicitly quantified; the paper notes general challenges when LLM suggestions can be invalid (handled by retry and fallback to semantically nearest valid action). No explicit scaling/capacity limits were reported.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Reflective SA-RL (LLM suggestions + textual reflective memory across trials) — this configuration yielded the strongest Combined performance according to the paper's experiments.",
            "uuid": "e2732.1",
            "source_info": {
                "paper_title": "Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Reflective-DA-RL",
            "name_full": "Reflective Decision-Assisted RL (DA-RL) Agent with LLM Memory",
            "brief_description": "A hybrid agent where the RL agent (DRRN) proposes its top-k actions and an LLM (GPT-4) selects the best among those; the reflective variant allows the LLM to keep and use a textual memory across trials to influence its selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Reflective Decision-Assisted RL (DA-RL)",
            "agent_description": "DRRN proposes top-k candidate actions (k=5 during interactions, k=2 post-test); GPT-4 is prompted to pick the best action among those; in the reflective variant GPT-4 is given and updates a textual memory of prior learnings between trials.",
            "base_model_size": null,
            "game_benchmark_name": "PharmaSimText",
            "game_description": "Interactive diagnostic dialogues in pharmacy scenarios requiring key questions and selecting the most probable cause over varied phrasings and patient profiles.",
            "uses_memory": true,
            "memory_type": "textual reflective memory maintained by the LLM",
            "memory_structure": "Sequential textual memory entries appended to prompts for decision selection.",
            "memory_content": "Prior trial outcomes and distilled learnings for same patient/cause.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Memory included in the LLM prompt for selection; relevance via restricting memory to prior experiences with same patient/cause.",
            "memory_update_strategy": "Memory updated after each trial by prompting the LLM to reflect on outcomes and append learnings.",
            "memory_usage_purpose": "To influence the LLM's selection among RL-suggested candidate actions to improve decision quality and generalization.",
            "performance_with_memory": "Qualitative: Reflective DA-RL benefited from reflection but less than SA-RL; DA-RL underperformed relative to SA-RL and RL-based agent in some measures, partly due to longer trajectories causing unfamiliar states for the DRRN. No numeric metrics are provided in text.",
            "performance_without_memory": "Qualitative: None-reflective DA-RL performed worse than reflective DA-RL; both DA-RL variants generally trailed SA-RL in Combined score.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Reflection improved DA-RL performance but to a lesser extent than SA-RL because DA-RL's decisions are more constrained by the RL-provided top-k candidates, limiting the impact of LLM memory-driven reasoning.",
            "memory_limitations": "DA-RL suffered when trajectories were long and entered unfamiliar states where the DRRN's suggestions were less accurate, which limited the usefulness of LLM memory-driven selection.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Reflective DA-RL improves over none-reflective DA-RL but was not as effective as reflective SA-RL; best configuration reported overall was reflective SA-RL.",
            "uuid": "e2732.2",
            "source_info": {
                "paper_title": "Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "DRRN-sum-state-updater",
            "name_full": "DRRN with Summation-based State Updater (aggregated observation memory)",
            "brief_description": "A Deep Reinforced Relevance Network (DRRN) adapted to PharmaSimText which aggregates the entire observation history by summing pre-trained sentence embeddings to form the current state embedding (e(st) = e(st-1) + e(ot)); summation was chosen after comparing multiple updater strategies for stability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RL-based DRRN (with summation state updater)",
            "agent_description": "DRRN that encodes text states and actions via pre-trained fastText embeddings and RNN/MLP encoders; a state-updater unit maintains full-history context by summing embedded observations to produce the current state embedding.",
            "base_model_size": null,
            "game_benchmark_name": "PharmaSimText",
            "game_description": "Text-based interactive diagnostic scenarios in a pharmacy setting where full interaction history is necessary to infer the current state and ask key diagnostic questions.",
            "uses_memory": true,
            "memory_type": "aggregated sequential observation memory (working/state memory)",
            "memory_structure": "Vector-based sequential buffer where observation embeddings are aggregated by elementwise summation into a single state embedding.",
            "memory_content": "All past observations (embedded via fastText); the aggregated vector represents the whole observation history.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "No retrieval per se — the aggregated state embedding (sum of all observations) is used directly as the current state input to the state encoder/scorer.",
            "memory_update_strategy": "After each new observation, the embedded observation is added to the running sum: e(st) = e(st-1) + e(ot).",
            "memory_usage_purpose": "To give the DRRN access to the full interaction history necessary for correct Q-value estimation and action selection in a partially observable text environment.",
            "performance_with_memory": "Qualitative: The summation-based updater led to the most stable training among five tested updater methods (mean, max, sum, LSTM, LSTM+self-attention) and was adopted for experiments; no numeric stability or performance metrics are provided in the text.",
            "performance_without_memory": "Comparative (ablation across updater designs): Other updater methods (RNN/LSTM variants, pooling) yielded less stable training and suboptimal embeddings when trained end-to-end with RL; explicit numeric baselines are not reported.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Aggregating all observation embeddings via summation provided the most stable training signal; training encoders with RL loss alone (without pre-trained embeddings and this updater) led to instability and suboptimal embeddings.",
            "memory_limitations": "The paper notes instability when relying solely on RL-aligned losses to learn RNN encoders; no analysis of long-term accumulation errors or capacity bounds for the summation approach is provided.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Summation-based state updater (e(st)=e(st-1)+e(ot)) outperformed mean/max pooling and LSTM/LSTM+attention in training stability and was chosen for experiments.",
            "uuid": "e2732.3",
            "source_info": {
                "paper_title": "Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
            "rating": 2,
            "sanitized_title": "clin_a_continually_learning_language_agent_for_rapid_task_adaptation_and_generalization"
        },
        {
            "paper_title": "Reflexion: An Autonomous Agent with Dynamic Memory and Self-Reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Deep Reinforcement Learning with a Natural Language Action Space",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_with_a_natural_language_action_space"
        },
        {
            "paper_title": "ScienceWorld: Is Your Agent Smarter than a 5th Grader?",
            "rating": 1,
            "sanitized_title": "scienceworld_is_your_agent_smarter_than_a_5th_grader"
        },
        {
            "paper_title": "Re-Act: Synergizing Reasoning and Acting in Language Models",
            "rating": 1,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        }
    ],
    "cost": 0.0156405,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs</p>
<p>Bahar Radmehr bahar.radmehr@epfl.ch 
Tanja Käser tanja.kaeser@epfl.ch 
Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs
3D5EC92E56EA711138BB3EACAB378BE1Reinforcement LearningLarge Language ModelsText-Based Educational EnvironmentsLearner Models
There has been a growing interest in developing learner models to enhance learning and teaching experiences in educational environments.However, existing works have primarily focused on structured environments relying on meticulously crafted representations of tasks, thereby limiting the agent's ability to generalize skills across tasks.In this paper, we aim to enhance the generalization capabilities of agents in open-ended text-based learning environments by integrating Reinforcement Learning (RL) with Large Language Models (LLMs).We investigate three types of agents: (i) RL-based agents that utilize natural language for state and action representations to find the best interaction strategy, (ii) LLM-based agents that leverage the model's general knowledge and reasoning through prompting, and (iii) hybrid LLM-assisted RL agents that combine these two strategies to improve agents' performance and generalization.To support the development and evaluation of these agents, we introduce PharmaSimText, a novel benchmark derived from the PharmaSim virtual pharmacy environment designed for practicing diagnostic conversations.Our results show that RL-based agents excel in task completion but lack in asking quality diagnostic questions.In contrast, LLM-based agents perform better in asking diagnostic questions but fall short of completing the task.Finally, hybrid LLM-assisted RL agents enable us to overcome these limitations, highlighting the potential of combining RL and LLMs to develop highperforming agents for open-ended learning environments.</p>
<p>INTRODUCTION</p>
<p>Learner models are foundational to the advancement of educational technologies, serving as a versatile tool for a multitude of applications that enhance both teaching and learning experiences [1].By simulating the interactions and data of students, these computational models provide a safe and controlled environment for teacher training, allowing educators to refine their methods without direct implications on actual students [2].They also facilitate the development and evaluation of adaptive learning systems [3] or new algorithms [4].Furthermore, they have been applied for testing theories of learning [5] and foster collaboration skills in students through interacting with virtual peers [6].</p>
<p>Reinforcement learning (RL) offers a promising avenue for developing these learner models/agents [7].Existing works on RL for educational domains have primarily focused on developing techniques for curriculum optimization [8][9][10][11], providing tailored hints and feedback [12,13], and generating educational content [14,15].Only a limited number of works have explored the use of RL-based learner agents that effectively operate in the learning environments [16,17].However, these RL-based learner agents have been studied for structured tasks with well-defined rules, such as mathematics and logic puzzles.In such environments, RL's capabilities are naturally exploited due to the straightforward definition of state and action representations using engineered features obtained from the existing structure [7,16,18].However, the reliance on hand-crafted features and engineered state representations limits the ability of these RL agents to be used in unstructured domains and to generalize their learned skills and knowledge across different tasks.</p>
<p>Recent advances in generative AI, in particular Large Language Models (LLMs), provide new opportunities to drastically improve state-of-the-art educational technology [19].LLMs are capable of generating coherent and contextually relevant content, engaging in meaningful dialogues, and executing specific linguistic tasks without explicit training [20,21].So far, in education, LLMs have mainly been applied for generating educational content [22][23][24], automating grading and feedback processes [25][26][27][28][29][30], and facilitating the development of collaborative systems [31][32][33].Few works have also used LLMs for learner modeling in programming domains [34] or for simulating students' behaviors as a basis for an interactive tool for teacher training [35].However, despite their proficiency in linguistic tasks, LLMs often fall short in decision-making in a constrained environment, a domain where RL agents excel due to their inherent capability to make feasible decisions within a given environment [36].</p>
<p>Given the strengths and limitations of RL and LLM-based agents, recent works have investigated the integration of arXiv:2404.18978v1[cs.LG] 29 Apr 2024</p>
<p>LLMs with RL to design agents that overcome the individual limitations of these agents.For instance, this integration has been used to substantially improve reward design and exploration efficiency in various domains [37][38][39][40].However, most of these approaches have focused on the use of LLMs for training, bearing the risk of taking on LLMs' limitations in decision-making in constrained environments.</p>
<p>In this paper, we investigate the integration of RL and LLMs to create agents with enhanced generalizability in text-based educational environments, focusing on employing the LLM in the inference phase.To support our investigations, we present a novel text-based simulation benchmark, PharmaSim-Text, adapted from the PharmaSim virtual pharmacy environment designed for practicing diagnostic conversations.We present three types of agents: (i) RL-based agents employing natural language based representations, (ii) LLMbased agents invoked through prompting, and (iii) hybrid models where LLMs assist RL agents in the inference phase.</p>
<p>We extensively evaluate all agents based on their ability to engage in effective diagnostic conversations and achieve accurate diagnoses on the PharmaSimText benchmark, focusing on their performance across a range of rephrased scenarios across diverse patient profiles.With our experiments, we aim to address three research questions: Which agent type demonstrates overall superior performance in conducting effective diagnostic conversations and achieving accurate diagnoses for all available patients (RQ1)?How does reflective prompting influence the diagnostic performance and conversation quality of LLM-involved agents (RQ2)?How do diagnostic performance and conversation quality vary among different agent types across diverse patients (RQ3)?Our results demonstrate that a specific type of LLM-assisted RL agent outperforms all other agents in a combined score by effectively balancing accurate diagnosis along with high-quality diagnostic conversations.The source code and benchmark are released on GitHub. 1</p>
<p>RELATED WORK</p>
<p>Given our focus on integrating RL agents and LLMs to create generalizable learner models, we review prior work in developing learner models, explore the growing field of intelligent agents in text-based interactive games and finally discuss recent advancements in integrating RL and LLMs.</p>
<p>Learner agents in educational environments.</p>
<p>There is a large body of research [1] on simulating learners in online environments.Existing research provides rich, but not generalizable learner representations, for example by generating cognitive models from problem-solving demonstrations (e.g., SimStudent [41]) or simulates learners from student models in a data-driven way [42][43][44], leading to less rich, but more generalizable representations.RL is a promising tool to address these limitations.However, in the education domain, this framework has been primarily applied for pedagogical policy induction [8][9][10][11], providing tailored hints [12,13], generating educational content [14,15], and assessing interventions in educational platforms [45,46].Despite its potential, the exploration of RL-based learner agents for effective operation in learning environments remains limited [16,17].Prior 1 https://github.com/epfl-ml4ed/PharmaSimText-LLM-RLwork has for example used Proximal Policy Optimization (PPO) for designing learner models in intelligent tutoring systems [16] or employed neural and symbolic program synthesize to create student attempts in a block-based programming environment [47].In this paper, we develop a series of learner agents for an open-ended educational environment.</p>
<p>Agents for text-based interactive games.The growing interest in developing intelligent agents for text-based interactive games, especially those that mimic real-world scenarios [36,48,49], has led to diverse methodologies encompassing RL [50], behavior cloning (BC) [36], and prompting LLMs [51,52].A well-known example is the game Science-World [36], where players engage in scientific experiments through environment exploration and interaction.Within the RL framework, state-of-the art employs deep reinforced relevance networks (DRRNs) [50], treating text-based interactions as partially-observable Markov decision processes (POMDPs), and learning distinct text representations for observations and actions to estimate Q-values via a scorer network.Within the LLM domain, LLM-based strategies use prompts at each interaction step for strategic planning and action selection.While some studies [51] engage in a single interaction round with the environment, others [52,53] use a multi-round approach, facilitating iterative refinement through repeated attempts.In this paper, we develop a series of agents for a text-based educational environment simulating real-world scenarios happening in a pharmacy.</p>
<p>RL and LLM integration.Recently, LLMs have been used to assist RL agents in various tasks, demonstrating notable advancements in reward design and exploration efficiency.For example, [39] utilized text corpora to pre-train agents, thereby shaping their exploration by suggesting goals based on the agents' current state descriptions.Furthermore, [40] proposed a novel approach to simplify reward design by employing LLMs to generate reward signals from textual prompts that describe desired behaviors.In a similar vein, [37] showcased the innovative application of few-shot LLM prompting to hypothesize world models for RL agents, which improves training sample efficiency and allows agents to correct LLM errors through interaction with the environment.While these studies highlight the synergistic potential of integrating LLMs with RL techniques to achieve more objective-aligned agent behaviors, directed exploration, and efficient training processes, the use of LLMs in the training phase bears the risk of carrying over their limitations in decision-making in constrained environments.A notable gap, therefore, remains in using LLMs to assist RL agents during the inference phase.Specifically, the current body of work has not addressed the use of LLMs to aid RL agents in adapting and transferring their learned skills to novel environments or tasks post-training.In our work, we aim to bridge this gap by focusing on utilizing LLMs as assistants for RL agents during generalization to new settings.</p>
<p>PHARMASIMTEXT BENCHMARK</p>
<p>We created PharmaSimText, a text-based interactive environment, as an infrastructure for developing language agents capable of handling text-based learning tasks and generalizing in them.PharmaSimText is an interactive text-based environment designed based on PharmaSim, a scenario-based learning platform.It simulates real-world interactions be-Figure 1: 'Father Inquiry' Scenario in PharmaSim -A simulated pharmacy setting designed for practicing diagnostic conversational skills, where participants engage with a father seeking guidance for his infant child's diarrhea.tween a pharmacist and a patient in a pharmacy setting.This benchmark includes more than 500 scenario variations that can be used for developing and evaluating learner agents.</p>
<p>PharmaSim</p>
<p>PharmaSim is a scenario-based learning environment designed to support the development of diagnostic skills.In each scenario, a patient comes to the pharmacy and asks for help with a specific problem.The player needs to identify different possible causes of this problem and mark how probable they are while interacting with the environment.Specifically, there are six different types of interactions: asking questions to the patient, seeking help from the pharmacist, searching about different kinds of medicine, looking for the specifications of products available on the shelf, reading about issues related to the problem, and offering a solution, which ends the game and moves the player to the post-test phase.In the post-test phase, players need to list three possible causes, rate their probability, and give an explanation for each of them.The determination of these likelihoods that leads to finding the most probable cause significantly depends on a set of patient inquiries containing essential information, which we henceforth refer to as key questions.</p>
<p>Currently, two different scenarios designed with insights from human experts are available in the game.For example, in one scenario (see Fig. 1), a father visits the pharmacy looking for help with his infant child's diarrhea.The scenario presents four probable causes for the child's condition.The player is required to ask four key questions to the father to gather the essential information needed to find the most probable cause behind the child's diarrhea.The relation between these key questions and the most probable cause of the child's diarrhea is illustrated in Fig. 2. For instance, inquiring about the child's age enables the player to deduce that teething is an improbable cause due to the child's young age.</p>
<p>PharmaSimText</p>
<p>To develop our benchmark, several modifications to Phar-maSim were implemented.</p>
<p>Migration to a text-based environment.As the first step, we did two adaptions to PharmaSim to migrate it to a textbased environment.First, we simplified interactions to two types of actions: asking questions to the patient about various characters phrased similar to PharmaSim as "I want to know about the character's topic."and advancing to the post-test by proposing a solution as "I want to suggest a solution.".Second, we modified the post-test questions to offer a feasible assessment for the agents.To this end, we revised the three causes question to focus solely on the most probable cause of the patient's issue.</p>
<p>Extension of available scenarios.In the next step, we focused on enriching PharmaSimText and enhancing its complexity.For this purpose, we expanded the two scenarios available in the original environment across three dimensions: (1) introducing new patients, (2) varying the scenarios to alternate the most probable cause of each patient's problem, and (3) diversifying patient responses by rephrasing them.Given the scale of extension, relying solely on human expertise was impractical.Instead, we leveraged the generative capabilities of LLMs combined with human insights to develop the scenarios in PharmaSimText.Prior to prompting LLMs for creating scenarios, we structured our expanded scenarios to align with the pharmacy assistant training curriculum of Switzerland.We gathered a set of health problems from the curriculum, assigning each to a fictional patient with a specified age and gender.We further identified a range of illnesses from the curriculum's textbooks, known to manifest symptoms relevant to the chosen problems.</p>
<p>Prompting LLMs for scenario creation.The scenario creation process involved three steps: (1) we prompted the LLM to generate a list of key questions aimed at diagnosing the most probable cause of the patient's problem, (2) the LLM was tasked to simulate patient responses, assuming each illness on the list was the most probable cause behind their problem, and (3) the LLM was prompted to generate answers to common patient inquiries done by pharmacists.We used GPT-4 as the LLM for scenario creation; the exact prompts employed can be found on our GitHub repository (link provided in Footnote 1).To ensure realism and applicability, a human expert has reviewed all of the scenarios and provided feedback including minor changes which were reflected in the final version of the scenarios.Additionally, the LLM was employed to diversify existing patient responses through paraphrasing, enhancing the scenarios' complexity.To further augment this complexity, fictional characters were introduced as distractors, enabling players to engage in more nuanced interactions.</p>
<p>Statistics on the PharmaSimText benchmark.The obtained benchmark contains eight distinct scenarios, each revolving around a unique patient profile.Details about the patients can be found in Table 1.On average, each scenario presents seven potential causes for the patient's problem, resulting in a total of 47 scenario variations.Patient responses in each variation are articulated in ten diverse phrasings to enhance the depth and variability.Furthermore, each scenario necessitates the identification of an average of 7.8 key questions by the player.As a result, PharmaSimText can provide an enriched environment for further studies on agents for textbased interactive tasks and agents' generalizability.</p>
<p>AGENTS FOR PHARMASIMTEXT</p>
<p>We developed three types of agents for PharmaSimText that embody various degrees of RL and LLM synergy: RL-based agents, LLM-based agents, and LLM-assisted RL agents.</p>
<p>RL-based Agents</p>
<p>RL agents learn to interact within an environment by taking actions based on their current state and receiving feedback in the form of rewards or penalties for those actions [54].They try to maximize their obtained cumulative reward over time to effectively learn the best policy for achieving their goal within the environment.One well-known method in RL involves estimating a metric called Q-value, which represents the expected future rewards for taking a certain action in a given state.Deep Q-Networks (DQNs) [55] approximate these Q-values using deep neural networks, enabling handling of complex, high-dimensional environments by learning to predict the Q-values directly from the environmental states.DQNs are trained through interactions with the environment, using their experience to iteratively refine and make their estimations of Q-values more accurate.</p>
<p>Following previous work on text-based games, we utilized state-of-the-art, a DRRN [50] as the RL-based agent for interacting with PharmaSimText.The DRRN is designed to learn distinct representations for the text-based states and actions by employing two separate networks: the state encoder and the action encoder.A scorer network then evaluates these representations to estimate their Q-values.At a given step t in the environment, the current state st and the action taken at are fed into the DRRN.Initially, st and at are encoded as sequences of word embeddings, which are subsequently processed by a Recurrent Neural Network (RNN) within both the state and action encoders to obtain respective embeddings for st and at.Following the RNN layer, a Multi-Layer Perceptron (MLP) in each encoder refines these embeddings into more concise representations.These representations are then concatenated and fed into the scorer network's MLP, which yields an estimation of the Q-value Q(st, at).</p>
<p>In our case, the valid actions at time step t are interactions available in the environment presented to the agent as a list of sentences.After taking each action, the agent will receive For instance, in the scenario related to infant diarrhea if the agent decides to ask about the infant's age, the new observation will be formatted as: Discuss; I want to know about the infant's age; He is 5 months old.Therefore, the agent should consider the full history of its observations to comprehend its current state st in the environment.</p>
<p>We introduced two modifications to adapt the original DRRN to our environment.First, we employed pre-trained sentence embeddings from fastText [56] to generate text representations for both observations and actions.This choice was motivated by previous work showing that training the RNNs in the encoders of a DRRN with a loss function solely aligned with the RL objectives leads to unstable training and suboptimal embeddings [57].Second, unlike the environments that DRRNs were proposed to tackle the tasks in, the observation at a given time step t in PharmaSimText does not suffice for the agent to obtain a notion of the current state in the environment and the whole full observation history is needed as a part of context given to the agent.Therefore, we introduced a unit called the state updater before the state encoder that takes the previous embedded state e(st−1) and the new embedded observation e(ot) and returns the updated state after the current observation st.We experimented with five different methods in the state updater: mean pooling, max pooling, summation, an LSTM layer, and an LSTM layer with self attention.After a series of experiments, we observed the method based on summation led to the most stable training; therefore this method was adopted in our state updater.Formally, this method based on the summation of all the observation embeddings in the history, returns e(st) = e(st−1) + e(ot) as the new embedded state e(st).</p>
<p>LLM-based Agents</p>
<p>The agents based on LLMs prompt an LLM at each step of interacting with the environment to find the best next action to finish the task.These agents can either have only one trial or multiple trials to complete the task along with reflection on their strategy between each trial.We respectively denote these two agent types by none-reflective and reflective.</p>
<p>The none-reflective agent interacts with the LLM by issuing a single prompt that contains the task description, the history of interactions (consisting of the agent's questions and the patient's responses), prior experience with the patient, and valid actions available at the current step to choose the most appropriate subsequent action.The task description is structured as Find the cause behind the patient's problem, while the interaction history is presented as a dialogue between the patient and the agent, with action texts labeled as agent's questions and environment's feedback text as patient responses.To format the valid actions, each action type is formatted as a function along with its permissible input values, which the LLM can interpret.This is complemented by a descriptive text explaining the action's purpose.For instance, the interaction "I want to ask about the subject's topic" is formatted as ask(subject, topic): Asking a question about the subject related to the topic, followed by a list of valid subjects and topics.This meticulous formatting strategy plays an essential role in minimizing the likelihood of the LLM suggesting invalid actions.</p>
<p>Despite efforts to format valid actions to guide the LLM, there are instances where the LLM still proposes an action that is invalid within the PharmaSimText environment.In such cases, we implemented a strategy where the LLM was prompted to suggest an alternative action, repeating this process for a maximum of k = 3 attempts.Should all suggested actions remain invalid, we selected the valid action that has the smallest distance in the natural language embedding space to the k-th suggested action.This approach ensures that the LLM's output is effectively grounded in the set of actions that are feasible within the environment.</p>
<p>The reflective agent employs a prompting strategy akin to that of the none-reflective agent to determine the optimal subsequent action.The none-reflective agent prompt is augmented with a segment including learnings from prior engagements with the same patient having the same cause.This reflective process involves prompting the LLM to evaluate its previous strategies based on the observed outcomes after completing each trial.The agent then updates its textual memory of previous learnings, and the updated memory is used for prompting in the next trial.This approach was inspired by research on self-reflective LLMs, notably the continually learning language agent CLIN [52].Similar to CLIN, we constructed the learning memory using causal formats such as "X is necessary for Y" to guide future interactions.This mechanism enables the reflective agent to dynamically adapt and refine its approach, enhancing its decision-making process over time.</p>
<p>LLM-assisted RL Agents</p>
<p>The perspective of RL-based agents remains limited to their experience during training, potentially hindering the performance in tasks with unfamiliar elements not encountered during their training.To address this, we leveraged LLMs' commonsense reasoning capabilities to augment RL agents' decision-making processes.As shown in Fig. 3, we explored two methods for integrating LLM assistance: Suggestion-Assisted RL (SA-RL) and Decision-Assisted RL (DA-RL).</p>
<p>In the SA-RL approach, at a given time step t, the LLM is prompted to suggest a list of k best actions to be taken at that state called LLM-Suggested t .The actions' Q-values in LLM-Suggested t are then calculated by the RL agent, and the next action is sampled from the probability distribution obtained by taking softmax over the estimated Qvalues.The prompting format here is similar to the LLMbased agents discussed in Section 4.2 containing the task description, the history of interactions, prior experience with the patient, and valid actions at that step.We set k = 5 in the interaction steps and k = 2 in the posttest steps.</p>
<p>In the DA-RL approach, at a given time step t, we collect a list of k most probable actions under the RL agent's policy RL-Suggested t .Then, an LLM is prompted to choose the best action among the actions in RL-Suggested t .The prompting used for this task contains the task description, the history of interactions, prior experience with the patient, and the actions in RL-Suggested t .Therefore, the LLM acts as a decision assistant for the RL agent.Notably, in our implementation, we set k = 5 in the interaction steps and k = 2 in the post-test steps.</p>
<p>Based on whether the LLM is given an opportunity to reflect on its past decisions or not, we obtain two versions of DA-RL and SA-RL approaches, which we distinguish via reflective/none-reflective prefixes.Thus, we study four LLMassisted RL agents: none-reflective-DA-RL, reflective-DA-RL, none-reflective-SA-RL, and reflective-SA-RL.</p>
<p>EXPERIMENTAL EVALUATION</p>
<p>We evaluated our agents in PharmaSimText to assess which agent type demonstrates the most effective diagnostic conversations and accurate diagnoses among all patients (RQ1), to investigate the impact of reflective prompting on the diagnostic performance and interaction quality of LLM-involved agents (RQ2), and to explore how diagnostic performance and conversation quality vary among the different agent types when confronted with different patients (RQ3).</p>
<p>Experimental Setup</p>
<p>Our evaluation was focused on the generalization capabilities of the agents, specifically their ability to navigate tasks featuring not previously encountered elements.We assessed the agents' generalizability across rephrased versions of alreadyencountered scenarios, aiming to measure their reliance on the precise wording of these scenarios.Figure 4 provides insight into our evaluation methodology for generalization, illustrating the diversity created by rephrased answer options in a specific scenario.</p>
<p>We defined agent success in a subtask based on two aspects: identifying the most probable cause of the patient's problem and asking the key questions in the conversation.Here a subtask denotes the combination of a cause and a wording.We therefore introduced three metrics:</p>
<p>• Post-test Performance Score: binary indicator of correct diagnosis of the patient's problem.It measures the agent's ability to identify the most probable cause of the patient's problem.</p>
<p>• Trajectory Quality Score: fraction of key questions involved in the agent's conversation.It measures the quality of the agent's conversation.</p>
<p>• Combined Score: product of the Post-test Performance Score and Trajectory Quality Score.It measures both the above elements together.</p>
<p>Agent Training and Evaluation</p>
<p>We developed and trained all of the agents separately for each patient.In this process, different wordings of subtasks leading to the same cause were split randomly to a training, validation, and test set.Therefore, the training, validation, and test sets included subtasks of all of the causes available for a patient in distinct wordings.Specifically, the agents saw all the causes during training and validation, but not all wordings.In our experiments, 80% of the available wordings for each cause were used for training and the remaining wordings were split in half for the validation and test set.</p>
<p>The RL-based agents were trained using subtasks from the designated training set being given a random subtask at each episode of interaction with the environment.At a given time step t, the agent took an action sampled from a softmax policy obtained from the Q-values of all of the actions available.</p>
<p>The randomness of the softmax policy was controlled using a temperature decaying from 1 to 0.001 linearly during the training.After each interaction, the agent was rewarded using a reward function that awarded the agent a positive reward of +1 when it successfully completed the posttest and penalizes with -1 otherwise.Moreover, each interaction of the agent was penalized by a small negative reward of -0.01.</p>
<p>Following each iteration of training, these agents underwent an evaluation phase using subtasks from the validation set.</p>
<p>The iteration that yielded the highest average Post-test Performance Score on the subtasks in validation set was used for testing and also served as the foundation for the RL component within the LLM-assisted RL agents.</p>
<p>The agents that had an LLM involved in their structures used the GPT-4 model.The LLM-based agents initially gain experience through interactions within the training subtasks.This acquired experience is subsequently leveraged during their engagement with the test subtasks.</p>
<p>RQ1: Efficacy of Different Agent Types</p>
<p>In our first analysis, we aimed to assess the agents' efficacy in diagnostic dialogues and accuracy in diagnoses aggregated over all patients.Figure 5 illustrates the Post-test Performance Score, Trajectory Quality Score, and Combined Score of the different agents.</p>
<p>We observed that the RL-based agent achieved a high Posttest Performance Score, indicating its ability to arrive at the correct diagnosis through a process of trial and error.However, this agent's approach often lacked the depth and nuance of a meaningful diagnostic conversation, reflected in its low Trajectory Quality Score.This observation is probably due to its lack of background knowledge and common sense reasoning.Conversely, the LLM-based agent exhibited a superior capacity for engaging in meaningful diagnostic dialogues, reflected in a higher Trajectory Quality Score.However, the LLM-based agent exhibited a lower Post-test Performance Score than the RL-based agent, indicating that its ability to consistently reach the correct diagnosis is inferior compared to the RL-based agent.</p>
<p>In examining the LLM-assisted RL agents, both DA-RL and SA-RL agents surpassed the LLM-based agent in Post-test Performance Score, indicating that integrating LLM with RL generally improves diagnostic precision of purely LLM- based agents.Notably, the SA-RL agent exhibited superior Post-test Performance Score closely mirroring that of the RL-based agent.The DA-RL's relative under-performance may have stemmed from its longer trajectories compared to the RL-based agent, leading to unfamiliar states where the DRRN struggled to provide accurate diagnoses, thereby affecting the DA-RL's RL-driven suggestions.Furthermore, in terms of engaging in quality diagnostic dialogues, the SA-RL agent was also superior to the DA-RL agent.This superiority is likely due to the RL framework's preference for shorter, more direct solutions, which reduced the action quality suggested by the DRRN in prolonged interactions.This effect was more pronounced in the DA-RL agent, potentially constraining the quality of diagnostic conversations.</p>
<p>In the comparison of the agents in the Combined Score, the SA-RL agent emerged as the standout performer.Unlike its counterparts, the SA-RL agent adeptly navigated the dual challenges posed by the benchmark, demonstrating both a high conversation quality and diagnostic accuracy.This achievement highlights the SA-RL agent's unique capacity to capture the strengths of both RL-based and LLM-based agents through the addition of suggestion-based assistance from LLMs to the RL agents' decision-making process.</p>
<p>To further investigate the results, we performed additional statistical tests.A Kruskal-Wallis test shows significant differences between the agents for the Trajectory Quality Score and Combined Score (ptrajectory &lt; 0.0001 and p combined &lt; 0.001) and a trend to significance for the Post-test Performance Score (p perf ormance = 0.052).Post-hoc comparisons using Mann-Whitney U tests with a Benjamini-Hochberg correction for the Combined Score indicate significant differences between 5 out of 6 pairs of agents supporting prior findings.For instance, the comparison between RL-based agent and SA-RL agent resulted in a p-value smaller than 0.01, and for the comparison between SA-RL agent and LLM-based agent the p-value was smaller than 0.05.</p>
<p>In summary, the experimental outcomes highlight distinct strengths and weaknesses among the agents.The RL-based agent demonstrated proficiency in achieving a high Posttest Performance Score score, but was hindered in engaging in effective diagnostic dialogues due to limited background knowledge.Conversely, the LLM-based agent excelled in conducting high-quality conversations by leveraging its extensive knowledge base, though with less accuracy in diagnoses.The hybrid LLM-assisted RL agents, DA-RL and SA-RL, outperformed the LLM-based agent in diagnostic precision and surpassed the RL-based agent in dialogue quality.</p>
<p>The SA-RL agent achieved both a high conversation quality and diagnostic accuracy, illustrating its effective integration of LLM and RL capabilities.</p>
<p>RQ2: Effect of Reflective Prompting</p>
<p>In our second analysis, we aimed to explore the impact of reflective prompting on the efficacy of LLM-involved agents.</p>
<p>As described in Section 4, none-reflective agents were limited to a single attempt, whereas reflective agents were given three attempts per subtask with opportunities for reflection.Figure 6 illustrates the Post-test Performance Score, Trajectory Quality Score, and Combined Score for none-reflective and reflective LLM-assisted RL and LLM-based agents.</p>
<p>We observed a nuanced impact of reflective prompting on agent performance.Specifically, reflective prompting did not significantly impact the Combined Score of the purely LLMbased agent.For this agent, reflection led to shorter diagnostic conversations by eliminating what the agent considered redundant questions.However, this streamlining resulted in poorer conversation quality without significantly improving diagnosis accuracy, negating the potential diagnosis accuracy gains from reflection.</p>
<p>In contrast, the reflective process considerably enhanced the performance of the hybrid LLM-assisted RL agents.This improvement can be attributed to the reflective phase allowing the agents to reassess and refine their decision-making processes, leading to more accurate diagnoses.The performance boost was particularly notable in SA-RL agents, most likely due to their reliance on the LLM for suggesting potential actions during the interaction phase.This reliance provided a broader scope for reflection to influence decisionmaking, unlike DA-RL agents where decisions were more heavily influenced by the RL-based agent.This finding underscores the value of incorporating reflective mechanisms in enhancing the capabilities of hybrid agents.In summary, our experiment revealed that reflective prompting has a different effect on LLM-based and LLM-assisted RL agents.For the LLM-based agents, reflective prompting led to shorter and lower quality diagnostic conversations, with no significant improvement in diagnostic accuracy.On the other hand, the LLM-assisted RL agents benefited from reflection, showing improvements in diagnostic accuracy.This enhancement was more pronounced for SA-RL agents, which rely more on LLM suggestions.</p>
<p>RQ3: Agent Efficacy for Different Patients</p>
<p>In our final analysis, we investigated the performance of our agents across the different patients.Figure 7 illustrates the Post-test Performance Score, Trajectory Quality Score, and Combined Score for each patient averaged over all of the subtasks available for that patient in PharmaSimText for the RL-based agent as well as the reflective SA-RL, DA-RL, and LLM-based agents.</p>
<p>We again observed that the RL-based agent showed superior Post-test Performance Score across all patients, while the LLM-based agent was not able to identify all causes correctly for five out of the nine patients.The LLM-assisted RL agents managed to overcome this limitation, with the SA-RL agent showing superior performance than the DA-RL agent.The opposite result was found for the Trajectory Quality Score.While the LLM-based agents conducted highquality diagnostic dialogues, the RL-based agent exhibited a suboptimal Trajectory Quality Score for all of the patients, often incorporating merely one or two key questions within its diagnostic conversations, highlighting the extent of its deviation from an effective diagnostic interaction.Again, the LLM-assisted RL agents overcame this limitation, with the SA-RL agent generally showing the highest Trajectory Quality Score scores.</p>
<p>Our examination of the Combined Score revealed that, except for the SA-RL agent, most agents encounter difficulties in scenarios related to Skin and Eye conditions.A closer inspection of their Post-test Performance Score and Trajectory Quality Score metrics suggested that these agents face challenges in different facets of the scenarios related to these specific patients.A particularly noteworthy observation is the superior performance of the SA-RL agent, which overcomes the limitations of purely RL-based and LLM-based agents across all patient categories.</p>
<p>Given the inferior performance of the RL-based agent in the Trajectory Quality Score, we examined the dialogues generated by the RL-based agent and the SA-RL agent within an identical scenario that resulted in a correct diagnosis, as illustrated in Fig. 8.This comparison reveals a pronounced contrast in the conversational dynamics of these two agents.The dialogue led by the SA-RL agent exhibits a flow that is markedly more reminiscent of human-like interaction, in contrast to the RL-based agent's brief conversation.Notably, the RL-based agent's approach is characterized by posing a single key question before directly drawing a conclusion.In comparison, the SA-RL agent engages in a more thorough inquiry, covering a broader spectrum of key questions in a logically sequential manner.</p>
<p>In summary, the hybrid LLM-assisted RL agents manage to ovecome the limitations of solely RL-based and LLM-based agents, with the SA-RL agent demonstrating superior performance across all patients.The RL-based agent exhibits a behavior characterized by short conversation, limiting interactions to very few key questions, while the SA-RL agent follows a more human-like behavior.</p>
<p>DISCUSSION AND CONCLUSION</p>
<p>In this paper, we explored integration of RL and LLMs to enhance learner models in educational technologies.While RL-based agents show promise in structured learning tasks, they struggle with open-ended environments and skill generalization.Conversely, LLMs excel in generating studentlike responses, but fail in constrained action spaces.By combining RL and LLMs, we aimed to develop more generalizable agents for text-based educational settings.We assessed our agents, including RL-based , LLM-based , and hybrid models, on their ability to conduct diagnostic conversations and make accurate diagnoses in our novel benchmark PharmaSimText.To address our first research question, we assessed four agents: one RL-based, one LLM-based, and two integrating LLMs with RL, in rephrased versions of the scenarios related to different patients in PharmaSimText that the agents had not seen before.Effective diagnostic conversations require highquality conversations and accurate The RL agent excelled in finding the correct diagnosis but struggled in comprehensive diagnostic dialogues due to its limited knowledge.The LLM agent was adept in high-quality diagnostic conversations but tended to misdiagnose patients.LLM-RL integrations were able to address these limitations by enhancing the diagnostic accuracy compared to the LLM-based agent and the conversation quality compared to the RLbased agent.Among all agents, the SA-RL agent achieved the best combination of diagnostic accuracy and conversation quality.</p>
<p>The second research question investigated the benefits of reflective prompting of the LLMs in the LLM-involved agents.</p>
<p>To answer this question, we compared the reflective ver-sions of three LLM-involved agents with their none-reflective counterparts.In prior works, reflection showed noticeable improvements in task completion of prompted LLMs [52,53].Therefore, we hypothesized a noticeable drop in the performance of the LLM-involved agents after confining them to only one trial.Our results showed a mixed effect for reflection in the solely LLM-based agent and the hybrid agents.For the LLM-based agent, the reflection improved the diagnostic accuracy of the agent, but it decreased the quality of the agent's conversation by shortening its trajectory.For the hybrid agents, the reflective process increased the diagnostic accuracy.We therefore conclude that the effect of reflective prompting depends on the agent type.</p>
<p>To address the third research question, we analyzed the agents over the three metrics for each of the patients separately.We observed that the agents did not struggle with similar patients.In our subsequent analysis, we looked at an example of the conversations done by the RL-based agent and the SA-RL agent, and we observed that while the RLbased agent conversation seemed rushed, the SA-RL's conversation seemed human-like and followed a sequential logic.To conclude, the proposed LLM integration approach represents a promising step towards agents with generalization capabilities in open-ended text-based educational environments.Furthermore, our implemented benchmark facilitates further research in developing agents with generalization capabilities at a higher level.</p>
<p>Figure 2 :
2
Figure 2: Diagnostic Strategy in the 'Father Inquiry' Scenario of PharmaSim, depicting the process of identifying the most likely cause of an infant's diarrhea.Players must pose four key questions to the father to collect crucial information, enabling the determination of the most probable cause of the child's diarrhea among four potential causes.</p>
<p>Figure 3 :
3
Figure 3: LLM-assisted RL agents.An LLM is prompted to assist the RL agent at the inference time to aid in generalization.In the Suggestion-Assisted RL (SA-RL) agent (left), the LLM suggests k actions at each step for the RL agent to choose from.In the Decision-Assisted RL (DA-RL) agent (right), the LLM selects an action from the top-k choices provided by the RL agent.</p>
<p>Figure 4 :
4
Figure 4: Generalization task, requiring the agents to generalize over different wordings of a scenario.</p>
<p>Figure 5 :
5
Figure 5: Agent Performance on PharmaSimText.Post-test Performance Score (left), Trajectory Quality Score (middle), and Combined Score (right) of the RL-based agent, the reflective-DA-RL agent, the reflective-SA-RL agent, and the reflective-LLM-based agent.In the SA-RL agent, the LLM suggests k actions at each step for the RL agent to choose from.In the DA-RL agent, the LLM selects an action from the top-k choices provided by the RL agent.Scores are averaged across all patients in PharmaSimText.</p>
<p>Figure 6 :
6
Figure 6: Performance of reflective and none-reflective agents on PharmaSimText.Post-test Performance Score (left), Trajectory Quality Score (middle), and Combined Score (right) for none-reflective and reflective DA-RL, SA-RL, and LLM-based agents.</p>
<p>Figure 7 :
7
Figure 7: Performance of different agents in interaction with different patients.Post-test Performance Score (left), Trajectory Quality Score (middle), and Combined Score (right) for RL-based and reflective SA-RL, DA-RL, and LLM-based agents.</p>
<p>Figure 8 :
8
Figure 8: Example diagnostic conversations conducted by the RL-based (top) and SA-RL agents (bottom) with the patient with joint pains in a test subtask with Osteoarthritis as the most probable cause.</p>
<p>One of the limitations of this work is the focus on generalization at a single level of rephrased versions of the scenarios.A few possible generalization levels available Phar-maSimText are: generalizing to a new wording of a known scenario (wording generalization), to a new diagnosis of a known patient (subtask generalization), and to a new patient (task generalization).Our presented experiments are limited to the wording generalization.Further research should be done within different generalization levels to evaluate current agents and propose new agent frameworks that consider the models' confidence in integration and leverage LLM insights for rapid adaptation of RL-based agents to new tasks.Moreover, our proposed reflective process showed limitations in improving the LLM-based agents.This suggests a need for further research for improved reflection in the interactive format of the PharmaSimText benchmark.Moreover, future research should consider evaluating the similarity of behavior of these agents to human students to further facilitate their use cases such as evaluating learning environments and collaborative learning.</p>
<p>Table 1 :
1
Overview
Problem# of PossiblePossible Causes# of KeyCausesQuestionsInfant4Change of diet, Teething, Current medication of the mother,4DiarrheaViral InfectionBreastfeeding-6Engorgement, Plugged Ducts, Cracked Nipples, Mastitis,7relatedThrush, Low Milk SupplyUrological4Prostate Hyperplasia, Cystitis, Urge Incontinence, Stress6IncontinenceSkin-related10Sunburn, Insect Bites, Acne, Eczema, Athlete's Foot, Psoriasis,10Rashes, Warts and Corns, Cold Sores, NeurodermatitisEye-related5Dry Eyes, Allergic Conjunctivitis, Pink Eye, Eye Strain, Stye11Gynecological8UTI, Cystitis, Kidney Stones, Overactive Bladder, Pregnancy,8STI, Stress Incontinence, Fungal InfectionJoint Pain5Osteoarthritis, Muscle Sprains, Tendonitis, Bursitis, Gout9Sore Throat5Common Cold, Influenza, Sinusitis, Pharyngitis, Bronchitis7
of PharmaSimText Scenarios.Every task within the benchmark is centered on a unique health problem, which could stem from various causes.Players must ask several key questions to arrive at a correct diagnosis.</p>
<p>ACKNOWLEDGEMENTSWe thank Dr. Jibril Frej and Dr. Ethan Prihar for their expertise and support.This project was substantially financed by the Swiss State Secretariat for Education, Research and Innovation (SERI).
Simulated Learners in Educational Technology: A Systematic Literature Review and a Turing-like Test. Tanja Käser, Giora Alexandron, International Journal of Artificial Intelligence in Education (IJAIED). 2023</p>
<p>Using Online Practice Spaces to Investigate Challenges in Enacting Principles of Equitable Computer Science Teaching. Kevin Robinson, Keyarash Jahanian, Justin Reich, Proceedings of the Technical Symposium on Computer Science Education (SIGCSE). the Technical Symposium on Computer Science Education (SIGCSE)2018</p>
<p>Predicting the Effects of Skill Model Changes on Student Progress. Steven Daniel Dickison, Tristan Ritter, Thomas K Nixon, Brendon Harris, R Charles Towle, Robert G M Murray, Hausmann, Proceedings of the International Conference on Intelligent Tutoring Systems (ITS), Part II. the International Conference on Intelligent Tutoring Systems (ITS), Part II2010</p>
<p>Kappa Learning: A New Item-Similarity Method for Clustering Educational Items from Response Data. Tanya Nazaretsky, Sara Hershkovitz, Giora Alexandron, Proceedings of the International Conference on Educational Data Mining (EDM). the International Conference on Educational Data Mining (EDM)2019</p>
<p>The Apprentice Learner Architecture: Closing the Loop between Learning Theory and Educational Data. J Christopher, Erik Maclellan, Rony Harpstead, Kenneth R Patel, Koedinger, Proceedings of the International Conference on Educational Data Mining (EDM). the International Conference on Educational Data Mining (EDM)2016</p>
<p>A Teachable Agent Game Engaging Primary School Children to Learn Arithmetic Concepts and Reasoning. Lena Pareto, International Journal of Artificial Intelligence in Education (IJAIED). 2432014</p>
<p>Reinforcement Learning for Education: Opportunities and Challenges. Adish Singla, Anna N Rafferty, Goran Radanovic, Neil T Heffernan, CoRR, abs/2107.088282021</p>
<p>Approximately Optimal Teaching of Approximately Optimal Learners. Jacob Whitehill, Javier R Movellan, IEEE Transactions of Learning Technololy. 1122018</p>
<p>Pick the Moment: Identifying Critical Pedagogical Decisions Using Long-Short Term Rewards. Song Ju, Min Chi, Guojing Zhou, Proceedings of the International Conference on Educational Data Mining (EDM). the International Conference on Educational Data Mining (EDM)2020</p>
<p>Hierarchical Reinforcement Learning for Pedagogical Policy Induction. Guojing Zhou, Hamoon Azizsoltani, Markel Sanz Ausin, Tiffany Barnes, Min Chi, Proceedings of the International Conference on Artificial Intelligence in Education (AIED). the International Conference on Artificial Intelligence in Education (AIED)2019</p>
<p>Faster Teaching via POMDP Planning. Anna N Rafferty, Emma Brunskill, Thomas L Griffiths, Patrick Shafto, Cognitive Science. 4062016</p>
<p>Zero-shot Learning of Hint Policy via Reinforcement Learning and Program Synthesis. Aleksandr Efremov, Ahana Ghosh, Adish Singla, Proceedings of the International Conference on Educational Data Mining (EDM). the International Conference on Educational Data Mining (EDM)2020</p>
<p>Toward Automatic Hint Generation for Logic Proof Tutoring Using Historical Student Data. Tiffany Barnes, John C Stamper, Proceedings of the International Conference on Intelligent Tutoring Systems (ITS). the International Conference on Intelligent Tutoring Systems (ITS)2008</p>
<p>Synthesizing Tasks for Block-based Programming. Z Umair, Maria Ahmed, Aleksandr Christakis, Nigel Efremov, Ahana Fernandez, Abhik Ghosh, Adish Roychoudhury, Singla, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2020</p>
<p>Neural Task Synthesis for Visual Programming. Georgios Victor-Alexandru Padurean, Adish Tzannetos, Singla, Transactions of Machine Learning Research (TMLR). 2024</p>
<p>Learning Expert Models for Educationally Relevant Tasks using Reinforcement Learning. Christopher J Maclellan, Adit Gupta, Proceedings of the International Conference on Educational Data Mining (EDM). the International Conference on Educational Data Mining (EDM)2021</p>
<p>Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis. Rudy Bunel, Matthew J Hausknecht, Jacob Devlin, Rishabh Singh, Pushmeet Kohli, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2018</p>
<p>Aligning Superhuman AI with Human Behavior: Chess as a Model System. Reid Mcilroy-Young, Siddhartha Sen, Jon M Kleinberg, Ashton Anderson, Proceedings of the SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). the SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)2020</p>
<p>. Paul Denny, Sumit Gulwani, Neil T Heffernan, Tanja Käser, Steven Moore, Anna N Rafferty, Adish Singla, Generative AI for Education. </p>
<p>. Advances, Opportunities, Challenges, Corr, abs/2402.015802024</p>
<p>Language Models are Few-Shot Learners. B Tom, Brown, Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS). the Annual Conference on Neural Information Processing Systems (NeurIPS)2020</p>
<p>Sparks of Artificial General Intelligence: Early Experiments with GPT-4. Sébastien Bubeck, CoRR, abs/2303.127122023</p>
<p>A Novel Framework for the Generation of Multiple Choice Question Stems Using Semantic and Machine-Learning Techniques. Ashalatha Archana Praveen Kumar, Manjula Nayak, K Shenoy, Chaitanya , Kaustav Ghosh, International Journal of Artificial Intelligence in Education (IJAIED). 2023</p>
<p>Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models. Sami Sarsa, Paul Denny, Arto Hellas, Juho Leinonen, Proceedings of the Conference on International Computing Education Research (ICER). the Conference on International Computing Education Research (ICER)2022</p>
<p>GPT-4, and Human Tutors. Tung Phung, Victor-Alexandru Padurean, José Cambronero, Sumit Gulwani, Tobias Kohn, Rupak Majumdar, Adish Singla, Gustavo Soares, Proceedings of the Conference on International Computing Education Research. the Conference on International Computing Education Research22023</p>
<p>Automated Distractor and Feedback Generation for Math Multiple-choice Questions via Incontext Learning. Hunter Mcnichols, Wanyong Feng, Jaewook Lee, Alexander Scarlatos, Digory Smith, Simon Woodhead, Andrew Lan, 2023NeurIPS'23 Workshop on Generative AI for Education (GAIED</p>
<p>Large Language Models (GPT) for Automating Feedback on Programming Assignments. Maciej Pankiewicz, Ryan , Shaun Baker, CoRR, abs/2307.001502023</p>
<p>Assessing Student Errors Experimentation Using Artificial Intelligence and Large Language Models: A Comparative Study with Human Raters. Arne Bewersdorff, Kathrin Seßler, Armin Baur, Enkelejda Kasneci, Claudia Nerdel, CoRR, abs/2308.060882023</p>
<p>Comparative Analysis of GPT-4 and Human Graders in Evaluating Praise Given to Students in Synthetic Dialogues. Dollaya Hirunyasiri, Danielle R Thomas, Jionghao Lin, Kenneth R Koedinger, Vincent Aleven, CoRR, abs/2307.020182023</p>
<p>Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation. Tung Phung, Victor-Alexandru Pȃdurean, Anjali Singh, Christopher Brooks, José Cambronero, Sumit Gulwani, Adish Singla, Gustavo Soares, Proceedings of the International Learning Analytics and Knowledge Conference (LAK). the International Learning Analytics and Knowledge Conference (LAK)2024</p>
<p>Learning Gain Differences between ChatGPT and Human Tutor Generated Algebra Hints. Zachary A Pardos, Shreya Bhandari, CoRR, abs/2302.068712023</p>
<p>The AI Teacher Test: Measuring the Pedagogical Ability of Blender and GPT-3 in Educational Dialogues. Anaïs Tack, Chris Piech, Proceedings of the International Conference on Educational Data Mining (EDM). the International Conference on Educational Data Mining (EDM)2022</p>
<p>Generative Agent for Teacher Training: Designing Educational Problem-Solving Simulations with Large Language Model-based Agents for Pre-Service Teachers. Unggi Lee, Sanghyeok Lee, Junbo Koh, Yeil Jeong, Haewon Jung, Gyuri Byun, Yunseo Lee, Jewoong Moon, Jieun Lim, Hyeoncheol Kim, NeurIPS'23 Workshop on Generative AI for Education. 2023</p>
<p>Ruffle&amp;Riley: Towards the Automated Induction of Conversational Tutoring Systems. Robin Schmucker, Meng Xia, Amos Azaria, Tom Mitchell, NeurIPS'23 Workshop on Generative AI for Education (GAIED). 2023</p>
<p>Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming. Hung Manh, Sebastian Nguyen, Adish Tschiatschek, Singla, CoRR, abs/2310.106902023</p>
<p>GPTeach: Interactive TA Training with GPT-based Students. Julia M Markel, Steven G Opferman, James A Landay, Chris Piech, Proceedings of the Conference on Learning @ Scale (L@S). the Conference on Learning @ Scale (L@S)2023</p>
<p>ScienceWorld: Is Your Agent Smarter than a 5th Grader?. Ruoyao Wang, Peter A Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)2022</p>
<p>Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling. Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, Roy Fox, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)2023</p>
<p>Pre-Trained Language Models for Interactive Decision-Making. Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, Yuke Zhu, Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS). the Annual Conference on Neural Information Processing Systems (NeurIPS)2022</p>
<p>Guiding Pretraining in Reinforcement Learning with Large Language Models. Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, Jacob Andreas, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)2023</p>
<p>Reward Design with Language Models. Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2023</p>
<p>A Machine Learning Approach for Automatic Student Model Discovery. Nan Li, William W Cohen, Kenneth R Koedinger, Noboru Matsuda, Proceedings of the International Conference on Educational Data Mining (EDM). the International Conference on Educational Data Mining (EDM)2011</p>
<p>Albert T Corbett, John R Anderson, Knowledge Tracing: Modeling the Acquisition of Procedural Knowledge. User Modeling and User-Adapted Interaction. 20054</p>
<p>Semi-Markov Model for Simulating MOOC Students. Louis Faucon, Lukasz Kidzinski, Pierre Dillenbourg, Proceedings of the International Conference on Educational Data Mining (EDM). the International Conference on Educational Data Mining (EDM)2016</p>
<p>Modeling Interactions Across Skills: A Method to Construct and Compare Models Predicting the Existence of Skill Relationships. Anthony F Botelho, Seth Adjei, Neil T Heffernan, Proceedings of the International Conference on Educational Data Mining (EDM). the International Conference on Educational Data Mining (EDM)2016</p>
<p>Statistical Consequences of Using Multi-Armed Bandits to Conduct Adaptive Educational Experiments. Anna N Rafferty, Joseph Jay Williams, Huiji Ying, Journal of Educational Data Mining (JEDM). 112019</p>
<p>Multi-Armed Bandit Algorithms for Adaptive Learning: A Survey. John Mui, Fuhua Lin, M Ali, Akber Dewan, Proceedings of the International Conference on Artificial Intelligence in Education (AIED). the International Conference on Artificial Intelligence in Education (AIED)2021</p>
<p>From {So-lution Synthesis} to {Student Attempt Synthesis} for Block-Based Visual Programming Tasks. Adish Singla, Nikitas Theodoropoulos, Proceedings of the International Conference on Educational Data Mining (EDM). the International Conference on Educational Data Mining (EDM)2022</p>
<p>SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents. Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap, CoRR, abs/2310.116672023</p>
<p>Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHI-AVELLI Benchmark. Alexander Pan, Chan Jun Shern, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, Jonathan Ng, Hanlin Zhang, Scott Emmons, Dan Hendrycks, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)2023</p>
<p>Deep Reinforcement Learning with a Natural Language Action Space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)2016</p>
<p>Re-Act: Synergizing Reasoning and Acting in Language Models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, R Karthik, Yuan Narasimhan, Cao, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2023</p>
<p>CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization. Prasad Bodhisattwa, Bhavana Majumder, Peter A Dalvi Mishra, Oyvind Jansen, Niket Tafjord, Li Tandon, Chris Zhang, Peter Callison-Burch, Clark, CoRR, abs/2310.101342023</p>
<p>Reflexion: An Autonomous Agent with Dynamic Memory and Self-Reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, CoRR, abs/2303.113662023</p>
<p>Reinforcement Learning: An Introduction. Richard S Sutton, Andrew G Barto, 2018MIT press</p>
<p>Playing Atari with Deep Reinforcement Learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin A Riedmiller, CoRR, abs/1312.56022013</p>
<p>Enriching Word Vectors with Subword Information. Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov, CoRR, abs/1607.046062016</p>
<p>Graph Constrained Reinforcement Learning for Natural Language Action Spaces. Prithviraj Ammanabrolu, Matthew J Hausknecht, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2020</p>            </div>
        </div>

    </div>
</body>
</html>
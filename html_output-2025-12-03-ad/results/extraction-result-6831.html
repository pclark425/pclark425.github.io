<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6831 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6831</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6831</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-274965430</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.16075v1.pdf" target="_blank">Formal Mathematical Reasoning: A New Frontier in AI</a></p>
                <p><strong>Paper Abstract:</strong> AI for Mathematics (AI4Math) is not only intriguing intellectually but also crucial for AI-driven discovery in science, engineering, and beyond. Extensive efforts on AI4Math have mirrored techniques in NLP, in particular, training large language models on carefully curated math datasets in text form. As a complementary yet less explored avenue, formal mathematical reasoning is grounded in formal systems such as proof assistants, which can verify the correctness of reasoning and provide automatic feedback. In this position paper, we advocate for formal mathematical reasoning and argue that it is indispensable for advancing AI4Math to the next level. In recent years, we have seen steady progress in using AI to perform formal reasoning, including core tasks such as theorem proving and autoformalization, as well as emerging applications such as verifiable generation of code and hardware designs. However, significant challenges remain to be solved for AI to truly master mathematics and achieve broader impact. We summarize existing progress, discuss open challenges, and envision critical milestones to measure future success. At this inflection point for formal mathematical reasoning, we call on the research community to come together to drive transformative advancements in this field.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6831.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6831.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NuminaMath</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NuminaMath (AIMO Progress Prize winner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art math LLM described in the paper that combines math-focused continued pretraining, finetuning on step-by-step solutions, and tool-integrated reasoning; evaluated privately in the AIMO Progress Prize.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NuminaMath</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Math-specialized LLM built from a base math LLM, finetuned on a large curated dataset of step-by-step solutions and additional finetuning for tool-integrated traces (Python + SymPy calls). Emphasizes data curation and tool integration.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (base LLM) + tool-integration</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Math pretraining on curated web/math corpora (via DeepSeek pipeline), finetuning on ~860K problem/solution pairs, finetuning on tool-invocation traces (ToRA / MuMath-Code style)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Pretraining on math data, chain-of-thought style finetuning, self-consistency-style decoding strategies implicitly referenced, and tool-integrated reasoning (invoking Python/SymPy during inference).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Invokes external Python tools (e.g., SymPy) during reasoning to perform exact calculations and symbolic manipulations; tool calls are interleaved with natural-language solution steps.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AIMO Progress Prize private test set</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A private test set of 50 intermediate-level high-school/competition math problems used by the AIMO Progress Prize to compare entrants while minimizing data contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>competition-style math problem solving (stepwise solutions, numeric and symbolic answers)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>problems solved</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>29/50 solved (AIMO Progress Prize evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Reported as top performer in that contest; attributed primarily to data curation and tool integration rather than a specific model-size advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Careful math-domain pretraining, large curated finetuning datasets, and tool-integration produce strong performance on private competition-style benchmarks; 'good data is all you need' was highlighted.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance mainly limited to pre-college/competition-level math; scaling informal methods further into advanced/research mathematics faces data scarcity and unverifiable reasoning (hallucination) issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Formal Mathematical Reasoning: A New Frontier in AI', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6831.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6831.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeekMath-Base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeekMath-Base (base math LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A math-focused base LLM used by several top math systems as their starting model; obtained via continued pretraining on filtered Common Crawl math documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeekMath-Base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A transformer-based base math LLM produced by continuing pretraining on math-related web data curated with an automatic filter + manual annotation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (autoregressive)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Curated math documents from Common Crawl (DeepSeek data-selection pipeline), arXiv math papers, MathOverflow, and other web math content.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Standard autoregressive pretraining; used as a foundation for further step-by-step finetuning and tool-integrated finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>base pretraining for downstream math reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A high-quality math-pretraining corpus is a critical ingredient for downstream math LLM performance; several top entrants used DeepSeekMath-Base.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Pretraining alone does not resolve hallucinated reasoning on advanced math; relies heavily on downstream curated finetune data and inference-time tools.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Formal Mathematical Reasoning: A New Frontier in AI', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6831.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6831.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaProof</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaProof</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural/formal system that combines autoformalization with synthetic proof generation and expert iteration to reach IMO silver-medal level; uses a formal proof assistant for verification and data generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AlphaProof</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>System that autoformalizes informal problems into Lean, uses synthetic proof generation in a verifier loop, and applies expert-iteration style retraining to improve theorem-proving performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>neural theorem prover (LLM + formal verifier/Lean) + synthetic data generation</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Autoformalized corpus (reportedly large: autoformalized ~1M informal IMO-like problems into ~100M formal theorems), synthetic proofs produced by the system and expert-iteration finetuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Autoformalization â†’ formal theorem proving with tactic generation + proof search; expert iteration to generate training data; heavy use of test-time search/compute.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Uses the Lean proof assistant as the formal verifier/executor and proof-checker; formal environment provides automatic feedback and guarantees correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>IMO-style problems (formalized) / AIMO/IMO evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Hard olympiad-level mathematics problems formalized and evaluated; in reported work AlphaProof achieved silver-medal standard on IMO problems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>autoformalization + formal theorem proving (proof generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>competition-grade performance (medal-equivalence)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reached silver-medal level on IMO problems (as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Surpassed prior informal-only LLM attempts on olympiad-level problems by leveraging formal verification and synthetic data pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining autoformalization, synthetic proof generation, and formal verification produces breakthroughs on hard olympiad problems that informal LLM-only pipelines could not achieve.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on large synthetic autoformalized datasets and human-formalized seeds; open question whether approach generalizes to research-level mathematics where informal data is scarce and formalization is harder.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Formal Mathematical Reasoning: A New Frontier in AI', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6831.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6831.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaGeometry</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaGeometry</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialist system for olympiad Euclidean geometry that synthetically generates problems, uses a symbolic geometry engine, and a transformer model to propose auxiliary constructions, achieving human-competitive results in geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AlphaGeometry</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A hybrid system combining synthetic theorem/problem generation from axioms, a symbolic geometric reasoning backend for deterministic deductions, and a Transformer to propose creative auxiliary constructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + domain-specific symbolic geometry engine</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Entirely synthetic geometry problems and solutions generated from geometric axioms and the symbolic engine (no human pretraining on human-written geometry proofs required).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Symbolic deterministic deductions for basic geometry facts + neural model to propose auxiliary constructions; proofs executed and checked in a symbolic framework.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Relies on a domain-specific symbolic geometry prover/engine to check deductions and to produce synthetic data for training.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Olympiad geometry problem sets (formalized)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Geometry problems of olympiad style, often requiring ingenious auxiliary constructions; here used to evaluate AlphaGeometry's competence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>domain-specific theorem proving (Euclidean geometry) with auxiliary-construction synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>problem solving success rate (domain-specific benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported high success on domain-specific geometry tasks; achieved results that previous LLM-only methods could not without domain-specific symbolic components.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperformed previous general LLM approaches on olympiad geometry by leveraging domain-specific symbolic reasoning and synthetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Domain-specific symbolic engines plus synthetic data can enable solving problems that general-purpose LLMs struggle with; symbolic components give robustness and verifiable steps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Highly specialized; porting the full approach to other mathematical domains requires heavy redesign and domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Formal Mathematical Reasoning: A New Frontier in AI', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6831.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6831.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI o1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1 (referred to in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A frontier LLM mentioned as having strong math capabilities on AIME-level problems, speculated to combine scaling and inference-time techniques such as search plus neural verifiers; internal details largely unreleased.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unreleased/partially disclosed OpenAI model family reported to employ advanced inference-time scaling, likely combining LLM generation with search and verification modules.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-based LLM with scaled inference/search+verifier (speculative)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not publicly specified; likely large-scale web/pretraining and problem-specific finetuning/data engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Scaling informal approach at inference: combining search with neural verifiers and heavy test-time compute to mitigate hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AIME / other math contest benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>American Invitational Mathematics Examination (AIME) level problems used to evaluate advanced math reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>competition-level mathematics problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>reported capability on AIME problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Described as 'impressive' on AIME; exact numbers not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Suggested to use inference-time scaling (search + verifier) rather than solely training-time scaling; paper notes limited public information.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Inference-time scaling (search + verifiers) is an emerging direction to push informal LLMs beyond their training distribution; effectiveness on advanced math is an open question.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Internal details and reproducible evaluations not public; may still be limited by lack of verifiability for advanced non-numeric proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Formal Mathematical Reasoning: A New Frontier in AI', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6831.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6831.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits multi-step intermediate reasoning traces from autoregressive LLMs to improve performance on complex reasoning tasks like math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A non-model technique applied to transformer LLMs: provide exemplar step-by-step reasoning in the prompt so the model generates intermediate reasoning steps rather than only final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>prompting method used with transformer LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Often combined with finetuning on datasets of step-by-step solutions (e.g., GSM8K-style), but also used zero-shot/few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>In-context demonstration of multi-step reasoning (chain-of-thought); often combined with sampling and selection mechanisms like self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH and other reasoning datasets</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>GSM8K: grade-school math word problems with numeric answers; MATH: competition-style math problems requiring multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>step-by-step mathematical reasoning and problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on final answers (and sometimes intermediate step faithfulness)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported improvements across many arithmetic and reasoning tasks in prior work; specific numeric gains are dataset- and model-dependent and not enumerated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Consistently improves performance over vanilla prompting for many LLMs; effectiveness depends on model scale and quality of exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chain-of-thought helps LLMs produce multi-step reasoning and improves final-answer accuracy on many benchmarks, but it does not eliminate hallucinations and can be brittle.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Generated chains can be unfaithful or hallucinated; not a full substitute for formal verification; effectiveness declines on very hard or distribution-shifted problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Formal Mathematical Reasoning: A New Frontier in AI', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6831.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6831.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that samples multiple chain-of-thought chains from an LLM and aggregates (e.g., majority-vote) their final answers to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoding/aggregation technique used with chain-of-thought: draw many reasoning samples and select the most consistent final answer across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>decoding/aggregation method used with transformers</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Sample multiple reasoning trajectories and aggregate outputs (majority or scoring) to reduce variance and increase correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH, other chain-of-thought benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Benchmarks where chain-of-thought improves performance and where aggregation over multiple samples boosts final accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>step-wise reasoning for math and logic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (final answer)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported to improve chain-of-thought accuracy in prior work; no concrete numbers in this paper beyond referencing the method.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Improves over single-chain decoding in many scenarios (as previously published), particularly for larger LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Aggregation over sampled reasoning chains increases robustness and often yields higher final-answer accuracy than single-sample chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Increases inference compute cost; does not guarantee logical correctness of intermediate steps; still vulnerable to systematic hallucinations across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Formal Mathematical Reasoning: A New Frontier in AI', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6831.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6831.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tool-integrated reasoning (ToRA / MuMath-Code)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tool-integrated reasoning (e.g., ToRA, MuMath-Code)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that interleave LLM natural-language reasoning with calls to external deterministic tools (Python, SymPy, NumPy) to handle exact calculation and symbolic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Tool-integrated reasoning (ToRA / MuMath-Code)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM pipeline finetuned on trajectories that include both natural-language steps and explicit tool invocation (code) to produce solutions that mix model reasoning with deterministic tool outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer LLM + external tool-call interface</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Finetuned on problem/solution datasets augmented with tool invocation traces and execution results (e.g., collected using ToRA, MuMath-Code methods).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Interleave LLM-generated reasoning with calls to symbolic/numeric tools during inference; use tool outputs to ensure exact calculations and reduce hallucinated numeric/symbolic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Common tools: SymPy for symbolic math and expression manipulation; NumPy for numeric computation; tools are called from within the model's generated code or tool-call API and their outputs are fed back into the reasoning trace.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH, AIMO Progress Prize, GSM8K (for tool-augmented variants)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Math benchmarks requiring exact calculation or symbolic manipulations where tool integration reduces model errors due to arithmetic and algebraic mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>numeric computation, symbolic manipulation and stepwise problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on problem-solving benchmarks (final answer correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Noted to improve correctness for calculations and symbolic steps; no single aggregated numeric value reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Improves over pure LLM generation for precise calculation and symbolic manipulation; cited as key ingredient in top systems like NuminaMath.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Tool integration reliably fixes calculation/symbolic mistakes and is a crucial practical technique to improve faithfulness of math solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Tools help computation but do not by themselves verify high-level logical correctness of informal multi-step proofs; formal verification still needed for rigorous correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Formal Mathematical Reasoning: A New Frontier in AI', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6831.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6831.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COPRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COPRA (inference-in-the-loop LLM prover)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that queries frontier LLMs repeatedly inside a proof-search loop and uses Lean error messages and a memory of prior mistakes in the prompt to reduce repeated errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>COPRA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A system that places an LLM inside a proof search loop, feeding back proof-checker (Lean) error messages and a short memory of prior incorrect attempts to inform subsequent in-context predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer LLM guided by formal verifier feedback (Lean) within a search loop</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Uses in-context learning plus previously observed proof traces and failure cases; not necessarily finetuned on formal proof corpora in the original description.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Iterative prompting with verifier feedback: attempt tactic step, get Lean error, include error + memory in next prompt to avoid repeating mistakes; also uses retrieval of lemmas optionally.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Uses Lean proof assistant to execute tactics and return precise error messages which are fed back into the LLM prompt as corrective signals.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Lean theorem proving tasks / Lean-based proof corpora</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Formal-theorem proving tasks in Lean where models must generate tactics leading to a correct proof tree verified by Lean.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formal proof tactic generation and proof search</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>proof success rate / ability to close goals during search</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported improvements through in-context learning from errors, but no exact aggregate figures provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Using verifier error messages and memory in prompts reduces repeat mistakes and improves success compared to vanilla in-context tactic prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Verifier-provided error messages can be effective corrective signals in LLM prompts; using a memory of past mistakes helps avoid repeating them in the same proof attempt.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Many errors do not produce immediate verifier failures (they cause long unproductive search branches); identifying and learning from such non-failing mistakes remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Formal Mathematical Reasoning: A New Frontier in AI', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6831.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6831.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReProver / LeanDojo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReProver (LeanDojo architecture for retrieval-augmented proving)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented theorem-proving architecture that retrieves relevant lemmas from a library and conditions LLM tactic generation on retrieved premises, improving generalization to 'new premises' test splits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LeanDojo: Theorem proving with retrieval-augmented language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ReProver (LeanDojo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Retrieval-augmented transformer model that first retrieves candidate lemmas/definitions from a formal library (e.g., mathlib) and uses them as context for tactic/premise generation and proof search.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>retrieval-augmented transformer + proof search</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Human-written formal proofs (mathlib) and retrieval indices over lemma/definition corpora; trained to use retrieved premises when generating tactics.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Premise retrieval followed by LLM tactic generation and proof search; addresses dynamic libraries and novel-premise generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Integrates with a proof assistant (Lean) to execute tactics and verify generated proof fragments; retrieval stage draws from the formal library.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Lean-based benchmarks (LeanDojo tasks), MiniF2F (cross-system formalized problems)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Formal theorem-proving benchmarks where some test the ability to prove theorems requiring premises not seen during training ('new premises' split).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>premise selection + formal proof generation (tactic-level)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>proof success rate / pass rate on formal theorem benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported nontrivial gains from retrieval compared to non-retrieval models in cited work; exact numbers are in the referenced LeanDojo paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Retrieval-augmented models outperform non-retrieval baselines, especially in 'new premises' settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit retrieval of lemmas/definitions is critical when the library changes over time and helps generalization to theorems requiring novel premises.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Retrieval quality is crucial; standard IR methods (BM25/DPR) are used but more math-specialized retrievers may further improve results; handling continuously growing libraries remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Formal Mathematical Reasoning: A New Frontier in AI', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6831.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6831.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Theorem Prover (tactic gen + search)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Theorem Prover (tactic generation + proof search architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Canonical architecture in modern formal-theorem proving combining a neural policy that generates tactics conditioned on the current goal and a search algorithm (BFS, best-first, MCTS) to assemble full proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural Theorem Prover (tactic generation + proof search)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A two-part system: (1) an LLM/transformer that proposes next tactics given the current goal (policy/value models), and (2) a proof-search algorithm that explores tactic applications to grow a proof tree until all goals are closed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-based policy + search (BFS / best-first / MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Human-written formal proofs from proof assistant libraries (mathlib, Metamath, etc.), optionally augmented with synthetic proofs from expert iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Generate one-step tactics via neural model; assemble via search algorithms to form complete proofs; may use reinforcement learning / expert iteration to improve the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Integrated with proof assistants (Lean, Coq, Isabelle, Metamath) to execute tactics and validate proof steps; search interacts tightly with verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CoqGym, LeanDojo, MiniF2F, PutnamBench, TPTP, FIMO</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Formal theorem-proving benchmarks spanning different proof assistants and difficulty ranges (from simple library lemmas to olympiad-level formalizations).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formal proof generation (tactic-level), premise selection, proof search</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>proof success rate / pass rate on benchmark theorems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Varies widely by system and benchmark; state-of-the-art systems achieve reasonable pass rates on Level-3 tasks but still struggle on hardest benchmarks; no single aggregate number provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Combining tactic-generation models with search substantially outperforms naive whole-proof generation in many contexts, though some systems (Baldur, DeepSeek-Prover) show whole-proof generation can be competitive in latency-sensitive settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decomposing proofs into tactics and using search yields data-efficiency and verifiability advantages; search scaling and value models are important levers for performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Theorem proving is undecidable; infinite discrete action space and need for creativity (novel lemmas/tactics) make scaling hard; compute-vs-model-size tradeoffs in search are unresolved; evaluations across assistants are hard to compare fairly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Formal Mathematical Reasoning: A New Frontier in AI', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6831.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6831.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autoformalization (LLM-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Autoformalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using LLMs to translate informal natural-language mathematics into formal theorem statements or proofs (e.g., to Lean), either zero-/few-shot or via finetuning on synthetic parallel corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Autoformalization with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs (GPT-4, other transformers) used to map informal math statements/proofs to formal languages (Lean, Coq, Isabelle) using few-shot prompts, back-translation, and finetuning on synthetic aligned corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer LLMs performing sequence-to-sequence translation with process supervision</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Small aligned corpora of informal/formal pairs, synthetic corpora produced by auto-informalization, human-written formal corpora (mathlib, Mizar) as targets.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Direct translation, decomposition into smaller formalization steps (chain-of-thought-style process supervision), back-translation to enlarge parallel data, and interaction with formal verifier for iterative correction.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Often integrated with formal proof assistants (Lean, Isabelle) to check whether the autoformalized statements/proofs typecheck; formal feedback used to filter or improve outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofNet, Herald, Con-NF, ICML/ICLR autoformalization challenges</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Benchmarks and datasets aimed at measuring accuracy and faithfulness of informal-to-formal translations at varying difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>translation (informal â†’ formal) of theorem statements and proofs; partial formalization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy of formalization / parsing/success rate / proof-checkable rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example: few-shot LLM formalization achieved ~30% accuracy on competition-level statements in prior cited work (Wu et al.); informalization (formalâ†’informal) is typically higher (~70%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Finetuning on synthetic back-translated corpora improved smaller models' autoformalization accuracy compared to zero-shot prompting alone.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can perform promising autoformalization with few examples; synthetic back-translation and formal-feedback loops (verifier-guided) substantially boost available training data and effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Automatic metrics (BLEU, parse rate) poorly correlate with human judgment; formalization can be under-specified and multiple formalizations exist; creating reliable automatic evaluation remains an open problem.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Formal Mathematical Reasoning: A New Frontier in AI', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6831.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e6831.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarks (GSM8K, MATH, MiniF2F, CoqGym, PutnamBench, FIMO, TPTP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representative strict logical reasoning / math benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of widely used benchmarks referenced in the paper for measuring arithmetic, competition math, and formal theorem-proving capabilities of LLMs and neural provers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GSM8K / MATH / MiniF2F / CoqGym / PutnamBench / FIMO / TPTP (grouped)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Benchmarks span grade-school numeric word problems (GSM8K), competition math (MATH, AIME), cross-system formal olympiad problems (MiniF2F, PutnamBench), and formal-prover corpora and challenge sets (CoqGym, FIMO, TPTP).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>N/A (benchmarks used to evaluate transformer-based LLMs and neural theorem provers)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH, MiniF2F, CoqGym, PutnamBench, FIMO, TPTP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>GSM8K: grade-school math word problems with single-number answers; MATH: competition math (algebra/olympiad style) requiring multi-step reasoning; MiniF2F/PutnamBench: formally encoded olympiad-level problems across assistants; CoqGym/LeanDojo: benchmark suites for formal theorem proving tasks; TPTP: automated theorem proving problems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>arithmetic/word-problem solving, competition math, formal proof generation, automated theorem proving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / proof success rate / pass@k depending on benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper references effective gains on GSM8K/MATH from chain-of-thought and tool integration but cautions that informal methods saturate at pre-college levels; formal benchmarks remain challenging and results vary across methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Chain-of-thought, self-consistency, tool-integrated reasoning and retrieval generally improve performance on GSM8K/MATH; retrieval + proof search improves formal-prover success relative to non-retrieval baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Benchmarks exposing different facets of logical reasoning reveal that informal LLM techniques excel on numeric/competition tasks up to AIME-level, while formal benchmarks test verifiability and remain more challenging; careful evaluation to avoid data contamination is crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Data contamination concerns on public benchmarks (MATH/GSM8K); many benchmarks focus on numeric final answers which are insufficient for evaluating rigorous formal reasoning; cross-system comparisons are difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Formal Mathematical Reasoning: A New Frontier in AI', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AlphaProof <em>(Rating: 2)</em></li>
                <li>AlphaGeometry <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>ToRA: A tool-integrated reasoning agent for mathematical problem solving <em>(Rating: 2)</em></li>
                <li>LeanDojo: Theorem proving with retrieval-augmented language models <em>(Rating: 2)</em></li>
                <li>Autoformalization with large language models <em>(Rating: 2)</em></li>
                <li>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs <em>(Rating: 2)</em></li>
                <li>COPRA <em>(Rating: 2)</em></li>
                <li>MiniF2F: a cross-system benchmark for formal olympiad-level mathematics <em>(Rating: 2)</em></li>
                <li>GSM8K: A dataset for grade-school math problems <em>(Rating: 1)</em></li>
                <li>MATH: Measuring mathematical problem solving with the MATH dataset <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6831",
    "paper_id": "paper-274965430",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "NuminaMath",
            "name_full": "NuminaMath (AIMO Progress Prize winner)",
            "brief_description": "A state-of-the-art math LLM described in the paper that combines math-focused continued pretraining, finetuning on step-by-step solutions, and tool-integrated reasoning; evaluated privately in the AIMO Progress Prize.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "NuminaMath",
            "model_description": "Math-specialized LLM built from a base math LLM, finetuned on a large curated dataset of step-by-step solutions and additional finetuning for tool-integrated traces (Python + SymPy calls). Emphasizes data curation and tool integration.",
            "model_size": null,
            "architecture_type": "transformer (base LLM) + tool-integration",
            "training_data": "Math pretraining on curated web/math corpora (via DeepSeek pipeline), finetuning on ~860K problem/solution pairs, finetuning on tool-invocation traces (ToRA / MuMath-Code style)",
            "reasoning_method": "Pretraining on math data, chain-of-thought style finetuning, self-consistency-style decoding strategies implicitly referenced, and tool-integrated reasoning (invoking Python/SymPy during inference).",
            "external_tool_used": true,
            "external_tool_description": "Invokes external Python tools (e.g., SymPy) during reasoning to perform exact calculations and symbolic manipulations; tool calls are interleaved with natural-language solution steps.",
            "benchmark_name": "AIMO Progress Prize private test set",
            "benchmark_description": "A private test set of 50 intermediate-level high-school/competition math problems used by the AIMO Progress Prize to compare entrants while minimizing data contamination.",
            "task_type": "competition-style math problem solving (stepwise solutions, numeric and symbolic answers)",
            "performance_metric": "problems solved",
            "performance_value": "29/50 solved (AIMO Progress Prize evaluation)",
            "comparison_with_baseline": "Reported as top performer in that contest; attributed primarily to data curation and tool integration rather than a specific model-size advantage.",
            "key_findings": "Careful math-domain pretraining, large curated finetuning datasets, and tool-integration produce strong performance on private competition-style benchmarks; 'good data is all you need' was highlighted.",
            "limitations": "Performance mainly limited to pre-college/competition-level math; scaling informal methods further into advanced/research mathematics faces data scarcity and unverifiable reasoning (hallucination) issues.",
            "uuid": "e6831.0",
            "source_info": {
                "paper_title": "Formal Mathematical Reasoning: A New Frontier in AI",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "DeepSeekMath-Base",
            "name_full": "DeepSeekMath-Base (base math LLM)",
            "brief_description": "A math-focused base LLM used by several top math systems as their starting model; obtained via continued pretraining on filtered Common Crawl math documents.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "DeepSeekMath-Base",
            "model_description": "A transformer-based base math LLM produced by continuing pretraining on math-related web data curated with an automatic filter + manual annotation pipeline.",
            "model_size": "7B",
            "architecture_type": "transformer (autoregressive)",
            "training_data": "Curated math documents from Common Crawl (DeepSeek data-selection pipeline), arXiv math papers, MathOverflow, and other web math content.",
            "reasoning_method": "Standard autoregressive pretraining; used as a foundation for further step-by-step finetuning and tool-integrated finetuning.",
            "external_tool_used": null,
            "external_tool_description": "",
            "benchmark_name": "",
            "benchmark_description": "",
            "task_type": "base pretraining for downstream math reasoning tasks",
            "performance_metric": "",
            "performance_value": "",
            "comparison_with_baseline": "",
            "key_findings": "A high-quality math-pretraining corpus is a critical ingredient for downstream math LLM performance; several top entrants used DeepSeekMath-Base.",
            "limitations": "Pretraining alone does not resolve hallucinated reasoning on advanced math; relies heavily on downstream curated finetune data and inference-time tools.",
            "uuid": "e6831.1",
            "source_info": {
                "paper_title": "Formal Mathematical Reasoning: A New Frontier in AI",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "AlphaProof",
            "name_full": "AlphaProof",
            "brief_description": "A neural/formal system that combines autoformalization with synthetic proof generation and expert iteration to reach IMO silver-medal level; uses a formal proof assistant for verification and data generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "AlphaProof",
            "model_description": "System that autoformalizes informal problems into Lean, uses synthetic proof generation in a verifier loop, and applies expert-iteration style retraining to improve theorem-proving performance.",
            "model_size": null,
            "architecture_type": "neural theorem prover (LLM + formal verifier/Lean) + synthetic data generation",
            "training_data": "Autoformalized corpus (reportedly large: autoformalized ~1M informal IMO-like problems into ~100M formal theorems), synthetic proofs produced by the system and expert-iteration finetuning data.",
            "reasoning_method": "Autoformalization â†’ formal theorem proving with tactic generation + proof search; expert iteration to generate training data; heavy use of test-time search/compute.",
            "external_tool_used": true,
            "external_tool_description": "Uses the Lean proof assistant as the formal verifier/executor and proof-checker; formal environment provides automatic feedback and guarantees correctness.",
            "benchmark_name": "IMO-style problems (formalized) / AIMO/IMO evaluation",
            "benchmark_description": "Hard olympiad-level mathematics problems formalized and evaluated; in reported work AlphaProof achieved silver-medal standard on IMO problems.",
            "task_type": "autoformalization + formal theorem proving (proof generation)",
            "performance_metric": "competition-grade performance (medal-equivalence)",
            "performance_value": "Reached silver-medal level on IMO problems (as reported)",
            "comparison_with_baseline": "Surpassed prior informal-only LLM attempts on olympiad-level problems by leveraging formal verification and synthetic data pipelines.",
            "key_findings": "Combining autoformalization, synthetic proof generation, and formal verification produces breakthroughs on hard olympiad problems that informal LLM-only pipelines could not achieve.",
            "limitations": "Relies on large synthetic autoformalized datasets and human-formalized seeds; open question whether approach generalizes to research-level mathematics where informal data is scarce and formalization is harder.",
            "uuid": "e6831.2",
            "source_info": {
                "paper_title": "Formal Mathematical Reasoning: A New Frontier in AI",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "AlphaGeometry",
            "name_full": "AlphaGeometry",
            "brief_description": "A specialist system for olympiad Euclidean geometry that synthetically generates problems, uses a symbolic geometry engine, and a transformer model to propose auxiliary constructions, achieving human-competitive results in geometry.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "AlphaGeometry",
            "model_description": "A hybrid system combining synthetic theorem/problem generation from axioms, a symbolic geometric reasoning backend for deterministic deductions, and a Transformer to propose creative auxiliary constructions.",
            "model_size": null,
            "architecture_type": "transformer + domain-specific symbolic geometry engine",
            "training_data": "Entirely synthetic geometry problems and solutions generated from geometric axioms and the symbolic engine (no human pretraining on human-written geometry proofs required).",
            "reasoning_method": "Symbolic deterministic deductions for basic geometry facts + neural model to propose auxiliary constructions; proofs executed and checked in a symbolic framework.",
            "external_tool_used": true,
            "external_tool_description": "Relies on a domain-specific symbolic geometry prover/engine to check deductions and to produce synthetic data for training.",
            "benchmark_name": "Olympiad geometry problem sets (formalized)",
            "benchmark_description": "Geometry problems of olympiad style, often requiring ingenious auxiliary constructions; here used to evaluate AlphaGeometry's competence.",
            "task_type": "domain-specific theorem proving (Euclidean geometry) with auxiliary-construction synthesis",
            "performance_metric": "problem solving success rate (domain-specific benchmarks)",
            "performance_value": "Reported high success on domain-specific geometry tasks; achieved results that previous LLM-only methods could not without domain-specific symbolic components.",
            "comparison_with_baseline": "Outperformed previous general LLM approaches on olympiad geometry by leveraging domain-specific symbolic reasoning and synthetic data.",
            "key_findings": "Domain-specific symbolic engines plus synthetic data can enable solving problems that general-purpose LLMs struggle with; symbolic components give robustness and verifiable steps.",
            "limitations": "Highly specialized; porting the full approach to other mathematical domains requires heavy redesign and domain knowledge.",
            "uuid": "e6831.3",
            "source_info": {
                "paper_title": "Formal Mathematical Reasoning: A New Frontier in AI",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "OpenAI o1",
            "name_full": "OpenAI o1 (referred to in paper)",
            "brief_description": "A frontier LLM mentioned as having strong math capabilities on AIME-level problems, speculated to combine scaling and inference-time techniques such as search plus neural verifiers; internal details largely unreleased.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "o1",
            "model_description": "Unreleased/partially disclosed OpenAI model family reported to employ advanced inference-time scaling, likely combining LLM generation with search and verification modules.",
            "model_size": null,
            "architecture_type": "transformer-based LLM with scaled inference/search+verifier (speculative)",
            "training_data": "Not publicly specified; likely large-scale web/pretraining and problem-specific finetuning/data engineering.",
            "reasoning_method": "Scaling informal approach at inference: combining search with neural verifiers and heavy test-time compute to mitigate hallucination.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "AIME / other math contest benchmarks",
            "benchmark_description": "American Invitational Mathematics Examination (AIME) level problems used to evaluate advanced math reasoning",
            "task_type": "competition-level mathematics problem solving",
            "performance_metric": "reported capability on AIME problems",
            "performance_value": "Described as 'impressive' on AIME; exact numbers not provided in this paper",
            "comparison_with_baseline": "Suggested to use inference-time scaling (search + verifier) rather than solely training-time scaling; paper notes limited public information.",
            "key_findings": "Inference-time scaling (search + verifiers) is an emerging direction to push informal LLMs beyond their training distribution; effectiveness on advanced math is an open question.",
            "limitations": "Internal details and reproducible evaluations not public; may still be limited by lack of verifiability for advanced non-numeric proofs.",
            "uuid": "e6831.4",
            "source_info": {
                "paper_title": "Formal Mathematical Reasoning: A New Frontier in AI",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits multi-step intermediate reasoning traces from autoregressive LLMs to improve performance on complex reasoning tasks like math word problems.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "mention",
            "model_name": "Chain-of-Thought prompting",
            "model_description": "A non-model technique applied to transformer LLMs: provide exemplar step-by-step reasoning in the prompt so the model generates intermediate reasoning steps rather than only final answers.",
            "model_size": null,
            "architecture_type": "prompting method used with transformer LLMs",
            "training_data": "Often combined with finetuning on datasets of step-by-step solutions (e.g., GSM8K-style), but also used zero-shot/few-shot.",
            "reasoning_method": "In-context demonstration of multi-step reasoning (chain-of-thought); often combined with sampling and selection mechanisms like self-consistency.",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "GSM8K, MATH and other reasoning datasets",
            "benchmark_description": "GSM8K: grade-school math word problems with numeric answers; MATH: competition-style math problems requiring multi-step reasoning.",
            "task_type": "step-by-step mathematical reasoning and problem solving",
            "performance_metric": "accuracy on final answers (and sometimes intermediate step faithfulness)",
            "performance_value": "Reported improvements across many arithmetic and reasoning tasks in prior work; specific numeric gains are dataset- and model-dependent and not enumerated in this paper.",
            "comparison_with_baseline": "Consistently improves performance over vanilla prompting for many LLMs; effectiveness depends on model scale and quality of exemplars.",
            "key_findings": "Chain-of-thought helps LLMs produce multi-step reasoning and improves final-answer accuracy on many benchmarks, but it does not eliminate hallucinations and can be brittle.",
            "limitations": "Generated chains can be unfaithful or hallucinated; not a full substitute for formal verification; effectiveness declines on very hard or distribution-shifted problems.",
            "uuid": "e6831.5",
            "source_info": {
                "paper_title": "Formal Mathematical Reasoning: A New Frontier in AI",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency decoding",
            "brief_description": "A technique that samples multiple chain-of-thought chains from an LLM and aggregates (e.g., majority-vote) their final answers to improve robustness.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "mention",
            "model_name": "Self-Consistency",
            "model_description": "Decoding/aggregation technique used with chain-of-thought: draw many reasoning samples and select the most consistent final answer across samples.",
            "model_size": null,
            "architecture_type": "decoding/aggregation method used with transformers",
            "training_data": "",
            "reasoning_method": "Sample multiple reasoning trajectories and aggregate outputs (majority or scoring) to reduce variance and increase correctness.",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "GSM8K, MATH, other chain-of-thought benchmarks",
            "benchmark_description": "Benchmarks where chain-of-thought improves performance and where aggregation over multiple samples boosts final accuracy.",
            "task_type": "step-wise reasoning for math and logic tasks",
            "performance_metric": "accuracy (final answer)",
            "performance_value": "Reported to improve chain-of-thought accuracy in prior work; no concrete numbers in this paper beyond referencing the method.",
            "comparison_with_baseline": "Improves over single-chain decoding in many scenarios (as previously published), particularly for larger LLMs.",
            "key_findings": "Aggregation over sampled reasoning chains increases robustness and often yields higher final-answer accuracy than single-sample chain-of-thought.",
            "limitations": "Increases inference compute cost; does not guarantee logical correctness of intermediate steps; still vulnerable to systematic hallucinations across samples.",
            "uuid": "e6831.6",
            "source_info": {
                "paper_title": "Formal Mathematical Reasoning: A New Frontier in AI",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Tool-integrated reasoning (ToRA / MuMath-Code)",
            "name_full": "Tool-integrated reasoning (e.g., ToRA, MuMath-Code)",
            "brief_description": "Approaches that interleave LLM natural-language reasoning with calls to external deterministic tools (Python, SymPy, NumPy) to handle exact calculation and symbolic manipulation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Tool-integrated reasoning (ToRA / MuMath-Code)",
            "model_description": "An LLM pipeline finetuned on trajectories that include both natural-language steps and explicit tool invocation (code) to produce solutions that mix model reasoning with deterministic tool outputs.",
            "model_size": null,
            "architecture_type": "transformer LLM + external tool-call interface",
            "training_data": "Finetuned on problem/solution datasets augmented with tool invocation traces and execution results (e.g., collected using ToRA, MuMath-Code methods).",
            "reasoning_method": "Interleave LLM-generated reasoning with calls to symbolic/numeric tools during inference; use tool outputs to ensure exact calculations and reduce hallucinated numeric/symbolic errors.",
            "external_tool_used": true,
            "external_tool_description": "Common tools: SymPy for symbolic math and expression manipulation; NumPy for numeric computation; tools are called from within the model's generated code or tool-call API and their outputs are fed back into the reasoning trace.",
            "benchmark_name": "MATH, AIMO Progress Prize, GSM8K (for tool-augmented variants)",
            "benchmark_description": "Math benchmarks requiring exact calculation or symbolic manipulations where tool integration reduces model errors due to arithmetic and algebraic mistakes.",
            "task_type": "numeric computation, symbolic manipulation and stepwise problem solving",
            "performance_metric": "accuracy on problem-solving benchmarks (final answer correctness)",
            "performance_value": "Noted to improve correctness for calculations and symbolic steps; no single aggregated numeric value reported in this paper.",
            "comparison_with_baseline": "Improves over pure LLM generation for precise calculation and symbolic manipulation; cited as key ingredient in top systems like NuminaMath.",
            "key_findings": "Tool integration reliably fixes calculation/symbolic mistakes and is a crucial practical technique to improve faithfulness of math solutions.",
            "limitations": "Tools help computation but do not by themselves verify high-level logical correctness of informal multi-step proofs; formal verification still needed for rigorous correctness.",
            "uuid": "e6831.7",
            "source_info": {
                "paper_title": "Formal Mathematical Reasoning: A New Frontier in AI",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "COPRA",
            "name_full": "COPRA (inference-in-the-loop LLM prover)",
            "brief_description": "An approach that queries frontier LLMs repeatedly inside a proof-search loop and uses Lean error messages and a memory of prior mistakes in the prompt to reduce repeated errors.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "COPRA",
            "model_description": "A system that places an LLM inside a proof search loop, feeding back proof-checker (Lean) error messages and a short memory of prior incorrect attempts to inform subsequent in-context predictions.",
            "model_size": null,
            "architecture_type": "transformer LLM guided by formal verifier feedback (Lean) within a search loop",
            "training_data": "Uses in-context learning plus previously observed proof traces and failure cases; not necessarily finetuned on formal proof corpora in the original description.",
            "reasoning_method": "Iterative prompting with verifier feedback: attempt tactic step, get Lean error, include error + memory in next prompt to avoid repeating mistakes; also uses retrieval of lemmas optionally.",
            "external_tool_used": true,
            "external_tool_description": "Uses Lean proof assistant to execute tactics and return precise error messages which are fed back into the LLM prompt as corrective signals.",
            "benchmark_name": "Lean theorem proving tasks / Lean-based proof corpora",
            "benchmark_description": "Formal-theorem proving tasks in Lean where models must generate tactics leading to a correct proof tree verified by Lean.",
            "task_type": "formal proof tactic generation and proof search",
            "performance_metric": "proof success rate / ability to close goals during search",
            "performance_value": "Reported improvements through in-context learning from errors, but no exact aggregate figures provided in this paper.",
            "comparison_with_baseline": "Using verifier error messages and memory in prompts reduces repeat mistakes and improves success compared to vanilla in-context tactic prediction.",
            "key_findings": "Verifier-provided error messages can be effective corrective signals in LLM prompts; using a memory of past mistakes helps avoid repeating them in the same proof attempt.",
            "limitations": "Many errors do not produce immediate verifier failures (they cause long unproductive search branches); identifying and learning from such non-failing mistakes remains challenging.",
            "uuid": "e6831.8",
            "source_info": {
                "paper_title": "Formal Mathematical Reasoning: A New Frontier in AI",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "ReProver / LeanDojo",
            "name_full": "ReProver (LeanDojo architecture for retrieval-augmented proving)",
            "brief_description": "A retrieval-augmented theorem-proving architecture that retrieves relevant lemmas from a library and conditions LLM tactic generation on retrieved premises, improving generalization to 'new premises' test splits.",
            "citation_title": "LeanDojo: Theorem proving with retrieval-augmented language models",
            "mention_or_use": "mention",
            "model_name": "ReProver (LeanDojo)",
            "model_description": "Retrieval-augmented transformer model that first retrieves candidate lemmas/definitions from a formal library (e.g., mathlib) and uses them as context for tactic/premise generation and proof search.",
            "model_size": null,
            "architecture_type": "retrieval-augmented transformer + proof search",
            "training_data": "Human-written formal proofs (mathlib) and retrieval indices over lemma/definition corpora; trained to use retrieved premises when generating tactics.",
            "reasoning_method": "Premise retrieval followed by LLM tactic generation and proof search; addresses dynamic libraries and novel-premise generalization.",
            "external_tool_used": true,
            "external_tool_description": "Integrates with a proof assistant (Lean) to execute tactics and verify generated proof fragments; retrieval stage draws from the formal library.",
            "benchmark_name": "Lean-based benchmarks (LeanDojo tasks), MiniF2F (cross-system formalized problems)",
            "benchmark_description": "Formal theorem-proving benchmarks where some test the ability to prove theorems requiring premises not seen during training ('new premises' split).",
            "task_type": "premise selection + formal proof generation (tactic-level)",
            "performance_metric": "proof success rate / pass rate on formal theorem benchmarks",
            "performance_value": "Reported nontrivial gains from retrieval compared to non-retrieval models in cited work; exact numbers are in the referenced LeanDojo paper.",
            "comparison_with_baseline": "Retrieval-augmented models outperform non-retrieval baselines, especially in 'new premises' settings.",
            "key_findings": "Explicit retrieval of lemmas/definitions is critical when the library changes over time and helps generalization to theorems requiring novel premises.",
            "limitations": "Retrieval quality is crucial; standard IR methods (BM25/DPR) are used but more math-specialized retrievers may further improve results; handling continuously growing libraries remains challenging.",
            "uuid": "e6831.9",
            "source_info": {
                "paper_title": "Formal Mathematical Reasoning: A New Frontier in AI",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Neural Theorem Prover (tactic gen + search)",
            "name_full": "Neural Theorem Prover (tactic generation + proof search architecture)",
            "brief_description": "Canonical architecture in modern formal-theorem proving combining a neural policy that generates tactics conditioned on the current goal and a search algorithm (BFS, best-first, MCTS) to assemble full proofs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Neural Theorem Prover (tactic generation + proof search)",
            "model_description": "A two-part system: (1) an LLM/transformer that proposes next tactics given the current goal (policy/value models), and (2) a proof-search algorithm that explores tactic applications to grow a proof tree until all goals are closed.",
            "model_size": null,
            "architecture_type": "transformer-based policy + search (BFS / best-first / MCTS)",
            "training_data": "Human-written formal proofs from proof assistant libraries (mathlib, Metamath, etc.), optionally augmented with synthetic proofs from expert iteration.",
            "reasoning_method": "Generate one-step tactics via neural model; assemble via search algorithms to form complete proofs; may use reinforcement learning / expert iteration to improve the policy.",
            "external_tool_used": true,
            "external_tool_description": "Integrated with proof assistants (Lean, Coq, Isabelle, Metamath) to execute tactics and validate proof steps; search interacts tightly with verifier.",
            "benchmark_name": "CoqGym, LeanDojo, MiniF2F, PutnamBench, TPTP, FIMO",
            "benchmark_description": "Formal theorem-proving benchmarks spanning different proof assistants and difficulty ranges (from simple library lemmas to olympiad-level formalizations).",
            "task_type": "formal proof generation (tactic-level), premise selection, proof search",
            "performance_metric": "proof success rate / pass rate on benchmark theorems",
            "performance_value": "Varies widely by system and benchmark; state-of-the-art systems achieve reasonable pass rates on Level-3 tasks but still struggle on hardest benchmarks; no single aggregate number provided in the paper.",
            "comparison_with_baseline": "Combining tactic-generation models with search substantially outperforms naive whole-proof generation in many contexts, though some systems (Baldur, DeepSeek-Prover) show whole-proof generation can be competitive in latency-sensitive settings.",
            "key_findings": "Decomposing proofs into tactics and using search yields data-efficiency and verifiability advantages; search scaling and value models are important levers for performance.",
            "limitations": "Theorem proving is undecidable; infinite discrete action space and need for creativity (novel lemmas/tactics) make scaling hard; compute-vs-model-size tradeoffs in search are unresolved; evaluations across assistants are hard to compare fairly.",
            "uuid": "e6831.10",
            "source_info": {
                "paper_title": "Formal Mathematical Reasoning: A New Frontier in AI",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Autoformalization (LLM-based)",
            "name_full": "LLM-based Autoformalization",
            "brief_description": "Using LLMs to translate informal natural-language mathematics into formal theorem statements or proofs (e.g., to Lean), either zero-/few-shot or via finetuning on synthetic parallel corpora.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Autoformalization with LLMs",
            "model_description": "LLMs (GPT-4, other transformers) used to map informal math statements/proofs to formal languages (Lean, Coq, Isabelle) using few-shot prompts, back-translation, and finetuning on synthetic aligned corpora.",
            "model_size": null,
            "architecture_type": "transformer LLMs performing sequence-to-sequence translation with process supervision",
            "training_data": "Small aligned corpora of informal/formal pairs, synthetic corpora produced by auto-informalization, human-written formal corpora (mathlib, Mizar) as targets.",
            "reasoning_method": "Direct translation, decomposition into smaller formalization steps (chain-of-thought-style process supervision), back-translation to enlarge parallel data, and interaction with formal verifier for iterative correction.",
            "external_tool_used": true,
            "external_tool_description": "Often integrated with formal proof assistants (Lean, Isabelle) to check whether the autoformalized statements/proofs typecheck; formal feedback used to filter or improve outputs.",
            "benchmark_name": "ProofNet, Herald, Con-NF, ICML/ICLR autoformalization challenges",
            "benchmark_description": "Benchmarks and datasets aimed at measuring accuracy and faithfulness of informal-to-formal translations at varying difficulty levels.",
            "task_type": "translation (informal â†’ formal) of theorem statements and proofs; partial formalization",
            "performance_metric": "accuracy of formalization / parsing/success rate / proof-checkable rate",
            "performance_value": "Example: few-shot LLM formalization achieved ~30% accuracy on competition-level statements in prior cited work (Wu et al.); informalization (formalâ†’informal) is typically higher (~70%).",
            "comparison_with_baseline": "Finetuning on synthetic back-translated corpora improved smaller models' autoformalization accuracy compared to zero-shot prompting alone.",
            "key_findings": "LLMs can perform promising autoformalization with few examples; synthetic back-translation and formal-feedback loops (verifier-guided) substantially boost available training data and effectiveness.",
            "limitations": "Automatic metrics (BLEU, parse rate) poorly correlate with human judgment; formalization can be under-specified and multiple formalizations exist; creating reliable automatic evaluation remains an open problem.",
            "uuid": "e6831.11",
            "source_info": {
                "paper_title": "Formal Mathematical Reasoning: A New Frontier in AI",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Benchmarks (GSM8K, MATH, MiniF2F, CoqGym, PutnamBench, FIMO, TPTP)",
            "name_full": "Representative strict logical reasoning / math benchmarks",
            "brief_description": "A set of widely used benchmarks referenced in the paper for measuring arithmetic, competition math, and formal theorem-proving capabilities of LLMs and neural provers.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GSM8K / MATH / MiniF2F / CoqGym / PutnamBench / FIMO / TPTP (grouped)",
            "model_description": "Benchmarks span grade-school numeric word problems (GSM8K), competition math (MATH, AIME), cross-system formal olympiad problems (MiniF2F, PutnamBench), and formal-prover corpora and challenge sets (CoqGym, FIMO, TPTP).",
            "model_size": null,
            "architecture_type": "N/A (benchmarks used to evaluate transformer-based LLMs and neural theorem provers)",
            "training_data": "",
            "reasoning_method": "",
            "external_tool_used": null,
            "external_tool_description": "",
            "benchmark_name": "GSM8K, MATH, MiniF2F, CoqGym, PutnamBench, FIMO, TPTP",
            "benchmark_description": "GSM8K: grade-school math word problems with single-number answers; MATH: competition math (algebra/olympiad style) requiring multi-step reasoning; MiniF2F/PutnamBench: formally encoded olympiad-level problems across assistants; CoqGym/LeanDojo: benchmark suites for formal theorem proving tasks; TPTP: automated theorem proving problems.",
            "task_type": "arithmetic/word-problem solving, competition math, formal proof generation, automated theorem proving",
            "performance_metric": "accuracy / proof success rate / pass@k depending on benchmark",
            "performance_value": "Paper references effective gains on GSM8K/MATH from chain-of-thought and tool integration but cautions that informal methods saturate at pre-college levels; formal benchmarks remain challenging and results vary across methods.",
            "comparison_with_baseline": "Chain-of-thought, self-consistency, tool-integrated reasoning and retrieval generally improve performance on GSM8K/MATH; retrieval + proof search improves formal-prover success relative to non-retrieval baselines.",
            "key_findings": "Benchmarks exposing different facets of logical reasoning reveal that informal LLM techniques excel on numeric/competition tasks up to AIME-level, while formal benchmarks test verifiability and remain more challenging; careful evaluation to avoid data contamination is crucial.",
            "limitations": "Data contamination concerns on public benchmarks (MATH/GSM8K); many benchmarks focus on numeric final answers which are insufficient for evaluating rigorous formal reasoning; cross-system comparisons are difficult.",
            "uuid": "e6831.12",
            "source_info": {
                "paper_title": "Formal Mathematical Reasoning: A New Frontier in AI",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AlphaProof",
            "rating": 2,
            "sanitized_title": "alphaproof"
        },
        {
            "paper_title": "AlphaGeometry",
            "rating": 2,
            "sanitized_title": "alphageometry"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "ToRA: A tool-integrated reasoning agent for mathematical problem solving",
            "rating": 2,
            "sanitized_title": "tora_a_toolintegrated_reasoning_agent_for_mathematical_problem_solving"
        },
        {
            "paper_title": "LeanDojo: Theorem proving with retrieval-augmented language models",
            "rating": 2,
            "sanitized_title": "leandojo_theorem_proving_with_retrievalaugmented_language_models"
        },
        {
            "paper_title": "Autoformalization with large language models",
            "rating": 2,
            "sanitized_title": "autoformalization_with_large_language_models"
        },
        {
            "paper_title": "Draft, sketch, and prove: Guiding formal theorem provers with informal proofs",
            "rating": 2,
            "sanitized_title": "draft_sketch_and_prove_guiding_formal_theorem_provers_with_informal_proofs"
        },
        {
            "paper_title": "COPRA",
            "rating": 2
        },
        {
            "paper_title": "MiniF2F: a cross-system benchmark for formal olympiad-level mathematics",
            "rating": 2,
            "sanitized_title": "minif2f_a_crosssystem_benchmark_for_formal_olympiadlevel_mathematics"
        },
        {
            "paper_title": "GSM8K: A dataset for grade-school math problems",
            "rating": 1,
            "sanitized_title": "gsm8k_a_dataset_for_gradeschool_math_problems"
        },
        {
            "paper_title": "MATH: Measuring mathematical problem solving with the MATH dataset",
            "rating": 1,
            "sanitized_title": "math_measuring_mathematical_problem_solving_with_the_math_dataset"
        }
    ],
    "cost": 0.027273,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Formal Mathematical Reasoning: A New Frontier in AI
20 Dec 2024</p>
<p>Kaiyu Yang 
Gabriel Poesia 
Stanford University
3 UC Berkeley</p>
<p>Jingxuan He 
Wenda Li 
University of Edinburgh
5 UTAustin</p>
<p>Kristin Lauter 
Swarat Chaudhuri 
Dawn Song 
Meta Fair 
Formal Mathematical Reasoning: A New Frontier in AI
20 Dec 202438FCB07B88E691C0D3F1B9DEB64D95D8arXiv:2412.16075v1[cs.AI]
AI for Mathematics (AI4Math) is not only intriguing intellectually but also crucial for AI-driven discovery in science, engineering, and beyond.Extensive efforts on AI4Math have mirrored techniques in NLP, in particular, training large language models on carefully curated math datasets in text form.As a complementary yet less explored avenue, formal mathematical reasoning is grounded in formal systems such as proof assistants, which can verify the correctness of reasoning and provide automatic feedback.In this position paper, we advocate for formal mathematical reasoning and argue that it is indispensable for advancing AI4Math to the next level.In recent years, we have seen steady progress in using AI to perform formal reasoning, including core tasks such as theorem proving and autoformalization, as well as emerging applications such as verifiable generation of code and hardware designs.However, significant challenges remain to be solved for AI to truly master mathematics and achieve broader impact.We summarize existing progress, discuss open challenges, and envision critical milestones to measure future success.At this inflection point for formal mathematical reasoning, we call on the research community to come together to drive transformative advancements in this field.Preprint.Under review.</p>
<p>Introduction</p>
<p>Since the early days of AI, researchers have dreamed of building AI systems that can automate mathematical reasoning.The first AI program in history was Newell and Simon's Logic Theorist [1], a theorem proving system that could prove 38 theorems in Principia Mathematica [2].In the decades since then, the center of AI has shifted from symbolic methods to machine learning, and a new field of statistical AI for mathematics (AI4Math) has emerged.One appeal of the field is that mathematical problems are a proxy for a broad array of reasoning and planning tasks.Another attraction is that math plays a foundational role in quantitative disciplines, so AI4Math has the potential to revolutionize AI for science, engineering, and beyond.For these reasons, designers of large language models (LLMs) [3,4] have frequently highlighted LLMs' success in math problems, and there have also been efforts to build AI systems that outperform humans at math competitions [5][6][7].</p>
<p>Given the importance of AI4Math, substantial research has been dedicated to developing math LLMs, using techniques borrowed from natural language processing (NLP).A common approach is to continue pretraining LLMs on math data, such as arXiv papers and web pages from MathOverflow, and then finetune the model on curated datasets of math problems with detailed, step-by-step solutions.We call this the "informal" approach to distinguish it from the formal approach that will be introduced later (Sec.2).Just like LLMs in general, math LLMs have a simple recipe, but the secret sauce is often data curation [8][9][10][11].Carefully curated training data plus inference-time techniques, including chain-of-thought prompting [12], self-consistency [13], and tool use [14], have led to remarkable success on widely used benchmarks such as GSM8K [15] and MATH [16], as well as in the AIMO Progress Prize [6].However, at the time of writing, the success of the informal approach has been mostly limited to high school math not exceeding the AIME level. 1 This raises a key question: How far can we go by scaling up the informal approach?Will it enable math LLMs to solve more challenging competition problems (e.g., IMO, International Mathematical Olympiad) or even problems in mathematical research?</p>
<p>Moving from high school to more advanced mathematics, the informal approach faces challenges that are hard to resolve by merely scaling up the training.First, training math LLMs requires high-quality data, which is scarce in advanced mathematics.For novel research math problems, it is infeasible to find solutions to similar problems on the Internet or manually annotate the data on a large scale.Without scaling up the data, we cannot fully benefit from the scaling laws for LLMs [18,19].Second, solutions to many advanced problems are not numbers that can be evaluated by comparing them with the ground truth.Instead, they carry out a chain of intricate reasoning steps, e.g., a proof.LLMs are notorious for hallucinating seemingly valid reasoning steps, making it challenging to evaluate the correctness of model output or collect useful feedback for learning.These challenges are difficult to address by scaling up the informal approach during training.If training-time scaling is not enough, what else do we need?One emerging direction, exemplified by OpenAI o1 [17], is to scale up the informal approach during inference, potentially combining search with neural verifiers to mitigate hallucinated reasoning [15].While this approach has gained traction, its effectiveness on advanced mathematical problems is an open question.In this position paper, we focus on a complementary approach that is less explored: formal mathematical reasoning.</p>
<p>We consider formal mathematical reasoning broadly as mathematical reasoning grounded in formal systems, including but not limited to first/higher-order logic [20], dependent type theory [21], and computer programs annotated with formal specifications [22].Such formal systems provide environments that can verify the model's reasoning and provide automatic feedback.They stand apart from the "tools" used by modern LLMs [23] in their ability to model the provable truth or falsity of a broad class of propositions.The feedback provided by such systems can mitigate data scarcity; also, such systems enable rigorous test-time checks that resist hallucination.In contrast, informal mathematics refers to math text commonly found in textbooks, research papers, and online math forums.Informal math interleaves natural language with symbols (e.g., L A T E X), but these symbols do not have a self-contained formal semantics, instead relying on informal text to convey significant parts of their meaning.</p>
<p>AlphaProof [7] and AlphaGeometry [5] are two prominent examples of the success of this idea.Before these systems, there were many failed attempts to use LLMs to solve olympiad-level math problems.The key differentiator in the aforementioned systems is the principled use of symbolic representations and proof-checking frameworks.The symbolic components (Lean [24,25] for AlphaProof; a domainspecific geometry system for AlphaGeometry) are used to execute a neural network's reasoning steps and generate high-quality synthetic data, leading to unprecedented mathematical reasoning abilities.</p>
<p>AlphaProof and AlphaGeometry follow in the footsteps of a broader literature on the synergistic use of formal methods and machine learning in mathematical tasks [26][27][28][29][30][31][32][33].This literature includes research on neural theorem proving, i.e., generating formal proofs given formal theorem statements [34][35][36], and autoformalization, i.e., automatically translating informal mathematics into formal mathematics [37].The advent of LLMs has significantly accelerated research in this area.For example, autoformalization was long hampered by the lack of aligned informal-formal pairs for finetuning.LLMs can mitigate this problem by either synthesizing the data [38] or performing autoformalization without finetuning [37].As a result, we are starting to realize autoformalization's potential in bootstrapping the capability of neural theorem provers [39].LLMs are also powerful tools for theorem proving; in particular, recent approaches have exploited LLMs to predict proof steps and fix buggy proofs without explicit training on formal proof data [36,40].</p>
<p>The research infrastructure around LLMs and formal reasoning is rapidly maturing.Lean [24,25]a language for writing formal proofs-has gained popularity among mathematicians, leading to formalized research mathematics [41] and general-purpose mathematical libraries [42].There are now multiple frameworks [35,36] that support the interaction between LLMs and Lean.These frameworks allow the extraction of training data from human-written formal proofs, as well as theorem proving via interaction with the formal environment.Multilingual infrastructures for proof languages like Coq [21] and Isabelle [20] in addition to Lean are also beginning to be built [36].Finally, LLMs have been used to assist human mathematicians in writing formal proofs [43], potentially initiating a data flywheel where growing human-written formal math data leads to more capable LLMs, which in turn eases the creation of more data.</p>
<p>The emerging opportunities of AI for formal mathematical reasoning have led to booming research activities.As a recent survey [44] shows, the number of publications in this field almost doubled in 2023 and is likely to double again in 2024.By combining autoformlization with reinforcement learning, AlphaProof [7] is the first AI to achieve the level of silver medal in IMO.Developments in this field also have immediate applications in formal verification [45][46][47][48], a core computer science problem that has traditionally been among the foremost applications of formal mathematics.While formal verification can lead to software and hardware systems that are exceedingly robust and secure, it has historically been too costly to deploy in all but the most safety-critical applications.AI can drastically reduce this cost by substantially automating the formalization and proof effort needed to formally certify complex systems.This can lead to a future in which mass-produced software and hardware systems are far more robust than they are today.</p>
<p>For all these reasons, we believe AI-based formal mathematical reasoning has reached an inflection point, with significant progress to come in the next few years.However, substantial work remains to be done.This position paper maps out open challenges in data and algorithms, as well as potential routes for future progress.It is not meant to be a comprehensive survey but to provide perspectives on where the field may go next and call on the community to unite to accelerate the progress.While we celebrate the promise of formal mathematical reasoning, it should be seen as complementary to the informal approach.For example, future models could combine natural language reasoning with autoformalization to solve informal problems rigorously (Sec.3.3 and 5.2).</p>
<p>The remainder of this paper is organized as follows: Sec. 2 discusses the informal approach in detail and introduces formal mathematical reasoning.Sec. 3 reviews recent progress in using AI to reason formally.Sec. 4 explores open challenges and future directions, and Sec. 5 proposes milestones for measuring AI's capabilities in formal mathematical reasoning.</p>
<p>AI for Mathematics (AI4Math) and the Formal Turn</p>
<p>Mathematical reasoning is a challenge at the frontier of AI research.In this section, we begin by examining the informal approach to AI4Math and its limitations.Then, we introduce formal mathematical reasoning as a promising path for advancing AI4Math.</p>
<p>State-of-the-art Math LLMs and Their Limitations</p>
<p>A Case Study of NuminaMath.NuminaMath [49] is a math LLM that won the first AIMO Progress Prize in July 2024, successfully solving 29 out of 50 test problems.The test problems were intermediate-level high school math problems newly created and kept private before the evaluation.Therefore, they have very little risk of data contamination compared to public benchmarks such as GSM8K [15] and MATH [16].NuminaMath is an excellent example of state-of-the-art math LLMs, as it encompasses many key ingredients such as math pretraining [10,[50][51][52], finetuning [8,9], and tool-integrated reasoning [14,53] (Fig. 1).Next, we use NuminaMath as an example to elaborate on each ingredient, highlighting the critical role of data. 2.Math pretraining (Fig. 1 Left): Starting from a generic LLM (or a coding LLM such as Code Llama [54]), one can continue to pretrain the model on a large corpus of math-related documents from the web.The result is referred to as the base math LLM.NuminaMath and other top contestants in the AIMO Progress Prize unanimously adopted DeepSeekMath-Base 7B [11] as the base math LLM.Critical to DeepSeekMath's success is data.To retrieve highquality math documents from Common Crawl, the authors of DeepSeekMath engineered a data selection pipeline that combined automatic filtering and manual annotation.</p>
<ol>
<li>Finetuning on step-by-step solutions (Fig.  solving, one can finetune it on a carefully curated dataset consisting of math problems with detailed, step-by-step solutions, e.g., in the form of chain-of-thought [12].This dataset can be constructed by preprocessing and combining heterogeneous sources of problems, e.g., online forums, high school exams, math competitions, problem sets, and manual annotations.</li>
</ol>
<p>The problems and solutions are then augmented and reformatted by LLMs like GPT-4 [3].NuminaMath, for example, has constructed a large dataset of 860K problems and solutions covering high school and competition math [55].</p>
<ol>
<li>Tool-integrated reasoning (Fig. 1 Right): Finetuned math LLMs have acquired general problem-solving skills, but they may still struggle with precise calculation (e.g., 162 Ã— 731) and symbol manipulation (e.g., expanding (x + 2) 8 into powers of x).A simple solution is to outsource these operations to external tools such as SymPy [56].NuminaMath performs tool-integrated reasoning that interleaves reasoning in natural language with tool invocation in Python.The key is, again, data.The model is finetuned on tool-integrated solutions consisting of natural language combined with tool invocation trajectories.NuminaMath follows the approaches in ToRA [14] and MuMath-Code [53] to collect this dataset of math problems with tool-integrated solutions.</li>
</ol>
<p>Data Scarcity.The NuminaMath team summarized: "Good data is all you need" [49].Indeed, training data plays a pivotal role throughout all ingredients of the informal approach.As a result, the success of this approach has been limited to domains where abundant high-quality data can be obtained at low costs.For pre-college math, it is relatively easy to collect problems and solutions on the Internet or annotate them manually.However, it is difficult to extend the informal approach to data-scarce domains such as advanced mathematics.</p>
<p>Advanced mathematics forms the foundation of numerous scientific disciplines.For example, climate modeling depends on partial differential equations.To unlock AI's full potential in scientific discovery, it must be able to learn and apply advanced mathematics.Moreover, the long-term goal of developing human-level AI mathematicians requires AI to handle novel aspects of mathematics.Novelty, by definition, implies difficulty in collecting in-distribution training data.Therefore, moving forward, we see data scarcity as a major roadblock to the informal approach to AI4Math.</p>
<p>Lack of Correctness Verifiability.Besides data scarcity, another challenge is in evaluation, which is essential for AI to make measurable progress.Existing math LLMs are evaluated on benchmarks such as GSM8K and MATH, which consist of math problems whose solution is a single number (or expression).Therefore, evaluation can be done easily, by comparing the model-generated number against the ground truth.While this approach is suitable for pre-college mathematics, it is not directly applicable to more advanced mathematics.Recent work has attempted to adapt this evaluation framework for advanced problems by restricting to problems that have numeric solutions [57].</p>
<p>However, this deviates from common practice in mathematical research.After all, almost all AIME problems have numeric solutions, but none of the Millennium Prize Problems do.</p>
<p>Rather than restricting to numeric solutions, advanced mathematics frequently deals with abstract conjectures and proofs.Verifying proofs can be a daunting task, even for experienced mathematicians.This is evident in the lengthy review process for mathematical manuscripts and the controversies surrounding certain proofs, such as the proof of the abc conjecture [58].The situation becomes even more complicated when LLMs are used to generate proofs, as they are known to hallucinate plausibly.</p>
<p>To address the verification difficulty, researchers have explored self-verification or self-correction.These approaches use LLMs to detect and correct reasoning errors in proofs, based on the assumption that verifying proofs may be easier than generating them.While self-verification has been studied extensively, the results have been mixed [59][60][61].A growing body of work suggests that current LLMs struggle with intrinsic self-verification, i.e., verifying their own generations without relying on external feedback [62][63][64][65][66]. Therefore, it is still an open question whether rigorous proof verification can be achieved within the informal approach.</p>
<p>AI for Formal Mathematical Reasoning</p>
<p>From Informal to Formal.Due to the challenges in data and evaluation, it is difficult to directly extend the informal approach to advanced mathematics.Formal mathematical reasoning helps address these challenges.In this paper, it refers to mathematical reasoning grounded in formal systems, which have a syntax for well-formed formulas and can perform reasoning by manipulating formulas following a set of well-defined inference rules.Examples of formal systems include axiomatic set theory [67,68], higher-order logic [69][70][71], and dependent type theory [72][73][74].They are widely used in mathematics and computer programming.In math, they can express axioms, theorems, and proofs.</p>
<p>In programming, they are used to specify programs and reason about semantics.The connection between mathematical proofs and computer programs is deepened by theoretical results such as the Curry-Howard correspondence [75].</p>
<p>Mathematics expressed in formal systems is called formal mathematics.It is expressive: Almost all mathematics can be expressed by first-order logic with ZFC set theory [76].At the same time, it enforces formal constraints: Formulas must conform to grammar rules, and their manipulation must conform to inference rules that capture valid reasoning.This is similar to how board games like chess and Go are played within predetermined rules and moves.The success of AI on board games [77,78] suggests that a similar approach could be applied to formal mathematics, even though mathematics, with an infinite number of configurations and moves, can be much more challenging than Go.</p>
<p>Specifically, formal systems can be useful environments for AI to learn mathematics.A formal environment can guarantee the soundness of reasoning, provide automatic feedback, and check if the goal has been achieved.This is crucial to addressing the two challenges faced by the informal approach: data scarcity and evaluation.Automatic feedback can serve as learning signals and alleviate the need for human-annotated training data.Rigorous proof verification allows us to evaluate the model's reasoning without worrying about hallucination.</p>
<p>Proof Assistants and Lean.A concrete type of formal systems is called proof assistants, also known as interactive theorem provers.These are software tools that enable humans to write formal proofs about mathematics or verified software.Common examples of proof assistants include Coq [21], Isabelle [20], and Lean [24,25].They have different logical foundations but share similarities from a user's perspective, regardless of whether the "user" is human or AI.For simplicity, we will frequently use Lean as an example to explain key concepts in formal mathematical reasoning, though many ideas can be applied to other proof assistants or formal systems in general.</p>
<p>Fig. 2 demonstrates how Lean is used to formalize mathematics.At its core, Lean is a functional programming language with dependent types [79], making it suitable for writing not only conventional programs but also mathematical definitions, theorems, and proofs.Fig. 2 (Middle) is an example Lean file.First, it defines natural numbers (Nat) as either zero or the successor of another natural number.Then, it defines addition between two natural numbers (add) as a recursive function.Finally, it states and proves the theorem add_zero (âˆ€n âˆˆ N, 0 + n = n).Lean can automatically check if the proof is correct with respect to the theorem statement.Technically, due to the Curry-Howard</p>
<p>Lean file Project</p>
<p>Figure 2: Formalizing mathematics using the Lean proof assistant [24,25].</p>
<p>correspondence, statements in Lean are types, and proofs are expressions.As a result, proof checking is essentially type checking, i.e., verifying if an expression has the specified type.</p>
<p>Let's take a closer look at the proof of add_zero (Fig. 2 Left).In Lean, theorem proving is a backward and interactive process.It begins with the theorem statement as the initial goal, and the user enters a proof step, known as a "tactic".When executed by Lean, the tactic transforms the current goal into a list of sub-goals that are hopefully simpler.The user then inspects the new goals and enters new tactics, repeating this process until there are no goals left.This process implicitly defines a proof tree whose nodes are goals and edges are tactics (Fig. 2 Left).The user plays a key role in theorem proving.While proof assistants like Lean were designed with human users in mind, in formal mathematical reasoning, the user can also be AI or humans in collaboration with AI (Sec.4.3).</p>
<p>Formalizing mathematics using Lean is similar to software development, as shown in Fig. 2 (Right).</p>
<p>Lean files are organized into larger code units such as libraries and projects, which can be opensourced on GitHub and reused by other projects.For example, the formalization of cutting-edge mathematical research like pfr (the Polynomial Freiman-Ruzsa conjecture [80]) often builds upon the basic concepts formalized in mathlib [42], Lean's general-purpose mathematical library.Mathlib currently contains 82,847 definitions and 161,483 theorems, covering a wide range of topics including analysis, algebra, and geometry.It is one of the largest monolithic repositories of formal mathematics and includes a substantial portion of postgraduate-level mathematics.</p>
<p>AI Meets Formal Mathematics.Integrating AI with proof assistants such as Lean can benefit both AI researchers and the proof assistant community (mathematicians, computer scientists, and formal verification engineers).On the one hand, proof assistants provide data and environments for developing AI for formal mathematical reasoning.On the other hand, AI can enhance the user experience of proof assistants by automating simple proofs and suggesting useful lemmas.Given informal mathematics written by humans (e.g., textbooks and papers), autoformalization automatically translates it into formal theorems and proofs.Given theorem statements, theorem proving aims to generate formal proofs.In addition to the statement, a theorem prover may have access to a large library of existing definitions and lemmas, such as mathlib, and can select useful definitions and lemmas from the library.Furthermore, AI for autoformalization and theorem proving can lead to new theorems and/or proofs that can enrich the library and bootstrap its own capability.Figure 4: A neural theorem prover that combines tactic generation and proof search.This architecture is adopted by the majority of existing methods, with only a handful of exceptions [39,40].</p>
<p>Fig. 4 illustrates an architecture commonly adopted by recent neural theorem provers, consisting of two parts: tactic generation and proof search.Given the current proof goal (and optionally a library of definitions and lemmas), a neural network generates suggestions for the next tactic.The network is often trained on human-written proofs and can be further finetuned using reinforcement learning.The generated tactics are assembled into a complete proof by the proof search algorithm.It starts from the theorem statement as the root and grows a search tree iteratively by executing tactics to expand nodes, until a proof is found.The order of expansion is decided by the search algorithm, e.g., classical algorithms such as best-first search (BFS) and Monte Carlo tree search (MCTS).</p>
<p>Informal math</p>
<p>Formal theorem (and proof)</p>
<p>Figure 5: Autoformalization translates informal math to formal theorems and/or proofs automatically.</p>
<p>Fig. 5 illustrates the task of autoformalization, i.e., translating from informal to formal.It takes informal mathematics (e.g., textbooks and papers) and translate it into theorem statements in a formal system such as Lean.In some settings, autoformalization also translates the proof [81,82], which can be viewed as a form of theorem proving given informal proofs as hints.</p>
<p>The formal and informal approaches to reasoning should not be viewed as mutually exclusive, nor do we argue that formal reasoning should entirely supplant informal reasoning.Instead, these approaches can potentially complement each other to enable complex reasoning that is both general and rigorous.For example, autoformalization can be integrated with theorem proving to solve problems formulated in natural language [83].The model can generate reasoning steps in natural language while attempting to autoformalize part of the reasoning, which obtains feedback from the formal environment and helps filter out invalid reasoning.We refer to this integration of formal and informal reasoning as verified reasoning in natural language, which is discussed in detail in Sec.3.3 and 5.2.</p>
<p>Other Directions in AI for Mathematics</p>
<p>While we have highlighted the distinction between informal and formal approaches, AI4Math is a broad and open-ended research field that does not fit neatly into this dichotomy.For example, in addition to generating solutions or proofs, neural networks can also be used to approximate mathematical functions.This includes simple functions such as greatest common divisor [84], eigenvalues [85], and modular arithmetic [86].These functions can be computed by well-known algorithms; however, approximating them through neural networks offers valuable insights into the model's capabilities and mechanistic interpretability.Furthermore, neural networks can approximate more complex functions for which we do not have efficient algorithms, with applications in cryptography [87], theoretical physics [88], control theory [89], and partial differential equations [90].</p>
<p>Collaborative  [96].These works share similarities with formal mathematical reasoning: They also represent mathematical objects as symbols and manipulate them according to well-defined inference rules.However, they are tailored to specific subdomains rather than relying on general-purpose mathematical languages like Lean.Incorporating domain-specific mathematical insights can be valuable, as further discussed in Sec.4.2.</p>
<p>3 Recent Progress in AI for Formal Mathematical Reasoning AI has made substantial progress in formal mathematical reasoning.First, we discuss the progress in two key tasks: autoformalization and theorem proving.Then, we sample two adjacent areas-natural language and code generation-that benefit from verifiable reasoning enabled by the formal approach.</p>
<p>Autoformalization</p>
<p>Pinpointing the origin of the phrase "autoformalization" is challenging, but the pioneers of automated theorem proving in the 1950s and 60s clearly conceived the idea:</p>
<p>The original aim of the writer was to take mathematical textbooks such as Landau on the number system, Hardy-Wright on number theory, Hardy on the calculus, Veblen-Young on projective geometry, the volumes by Bourbaki, as outlines and make the machine formalize all the proofs (fill in the gaps).</p>
<p>-Hao Wang [97] Nevertheless, it was not until the rise of interactive theorem proving (e.g., Automath [98]) that people began to seriously consider automating the labor-intensive formalization process [99].</p>
<p>Rule-Based Autoformalization.Mohan Ganesalingam [100] explored a linguistic foundation for mathematical texts, introducing a theory of types to eliminate ambiguities in both words and symbols.To address the flexibility and complexity of natural languages, many systems have adopted controlled natural language-a restricted subset of natural language governed by formal grammar-to allow users to express mathematical proofs in a way that is both natural and formal.Examples of such systems include Mizar [101], NaProChe [102], ForTheL [103], MathNat [104], and Verbose Lean [105].Strictly speaking, these systems may not qualify as autoformalization since they do not directly handle natural language in its entirety.Simultaneously, a high-level grammar-writing tool called the Grammatical Framework (GF) [106] has been gaining attention.This tool allows for the flexible development of customized grammars to parse mathematical texts directly.GF-based systems include GLIF [107], a framework for symbolic natural language understanding and processing, and GFLean [108], an autoformalization framework that translates natural language statements into Lean's formal language.</p>
<p>Neural and LLM-based Autoformalization.Kaliszyk et al. [109,110] conducted early experiments using machine learning to parse informal mathematical texts, followed by Wang et al. [111,112], who applied neural machine translation techniques to convert informal mathematical statements into the Mizar language.Unlike rule-based methods, machine learning approaches are more flexible and can capture edge cases in natural language that experts might miss when creating rules.Although Wang et al. [112] explored unsupervised machine translation [113], most early machine learning methods for autoformalization relied heavily on an aligned corpus of informal and formal statements, which is difficult to obtain on a large scale.</p>
<p>LLMs like GPT-4 [3] represent a new paradigm of machine learning.These autoregressive models are pretrained on vast amounts of Internet data and can be quickly adapted to various downstream tasks through a few demonstrations, without requiring parameter updates-a capability known as in-context learning.Wu et at.[37] showed that, with fewer than five expert-crafted examples, LLMs can translate between formal and informal mathematical statements to some extent.This finding is promising, as it suggests that we may not need to collect a large aligned corpus of informalformal statements-an almost impossible task given the variety of formal systems-to achieve autoformalization.Another notable observation in Wu et al. [37] is that (auto-)informalization is generally easier than (auto-)formalization.With the same model, about 30% accuracy was achieved in formalizing competition-level math statements, while over 70% accuracy was achieved in informalizing even more challenging undergraduate-level math problems.This insight prompted follow-up research utilizing LLMs-based back-translation, where synthetic aligned corpora were generated by auto-informalizing existing formal statements [38,114].Finetuning a smaller model on this synthetic data led to notable improvements in autoformalization performance.Building on a synthetic corpus, Lu et al. [115] further incorporated additional signals from the formal environment to develop a process-driven autoformalizer.</p>
<p>LLMs have also significantly advanced the translation of natural languages into formal domainspecific languages (DSLs) like SQL [116] and linear temporal logic [117,118].In this paper, we primarily focus on the more expressive formal languages used in foundational proof assistants.These languages are capable of accommodating both statements and proofs of modern mathematics, but come with the challenge of non-static vocabularies (definitions and proof tactics can evolve or expand over time).Nevertheless, autoformalization and NL-to-DSL translation are closely related, sharing techniques such as self-consistency and self-correction.</p>
<p>Application of Autoformalization.Autoformalization serves as a bridge between informal and formal mathematical knowledge, resulting in three immediate applications: (1) data argumentation for training neural proving agents (via autoformalizing informal theorem statements) [37,39], (2) guiding formal theorem proving via informal proofs [81,82], and (3) validating informal reasoning [83,119].</p>
<p>The first two applications are closely related to neural theorem proving, which will be discussed in detail in the next section (Sec.3.2).The third application will be examined in Sec.3.3.</p>
<p>Neural Theorem Proving</p>
<p>Proving theorems in any sufficiently expressive formal system is undecidable [120].Thus, theorem proving inevitably requires heuristics.Deep learning has been widely used for learning heuristics to find proofs in formal systems.Holophrasm [121] was the first system to demonstrate the feasibility of training deep neural networks to guide proof search.Holophrasm used the Metamath formal language [68] as its logical backbone, training gated recurrent unit (GRU) networks [122] on humanwritten formal proofs to serve as policy and value networks in Monte Carlo Tree Search (MCTS)-the same search algorithm used in AlphaGo [78].This training paradigm was expanded in GPT-f [34], which trained a single Transformer model for predicting proof steps in Metamath.GPT-f enjoyed substantial gains from pretraining on informal math-related data (arXiv Math, Math StackExchange, Github).Subsequent approaches have trained richer architectures such as retrieval-augmented Transformers [35] and also exploited zero-shot prompting of general-purpose LLMs [36,40].We now highlight several prominent ideas in this literature.</p>
<p>Expert Iteration.Since a formal theorem proving environment can guarantee the validity of proofs, whenever a model finds a proof of a new theorem, that proof can be used as new training data.In the context of theorem proving, expert iteration consists of alternating between (a) attempting a set of unsolved problems, and (b) finetuning the model using new training data produced during the first phase.This has been shown to lead to improvements in formal theorem proving [123,124], including in the recent work of AlphaProof [7].However, gains tend to diminish after a few iterations.It is still an open problem to obtain continuous improvements, leveraging the potentially unlimited feedback that a formal verifier can provide.</p>
<p>Learning from Mistakes.</p>
<p>A key benefit of formal proof environments is that they can provide error messages when a proof step fails.COPRA [36], an approach that repeatedly asks a frontier LLM to predict proof steps from within a search loop, uses such error messages as part of the LLM's prompts.</p>
<p>COPRA is also equipped with a memory that stores a subset of the incorrect predictions that it made while proving a particular theorem, and this memory is included in the LLM's prompt.Because of frontier LLMs' ability to learn in context, these strategies reduce the odds of similar mistakes being repeated again and again.However, not all mistakes result in immediate failures; many lead to distractions or unproductive paths without making meaningful progress toward the proof goal.</p>
<p>Identifying and learning from such mistakes remains a challenge.</p>
<p>Informal Proof Sketches.Neural theorem proving in formal languages has also benefited from informal proofs, generated in natural language.Notably, Draft, Sketch and Prove (DSP) [81] proposed to first have the LLM generate a "proof sketch", in natural language, to then attempt to formalize this sketch in Isabelle.Lean-STaR [125] proposed to interleave formal and informal reasoning steps in theorem proving in Lean.COPRA, mentioned above, also takes in informal proofs as an optional input and uses them as part of zero-shot proof-step-prediction queries to a general-purpose LLM.</p>
<p>Library Learning.Human mathematicians can leverage a continuously growing library of mathematical results to find new ones.Each new theorem potentially enters this library for reuse later in higher-level work.Building towards this behavior, research in AI for formal mathematics has started to explore the idea of library learning, where not only the search heuristics (neural policy and value functions) improve with more data, but also the symbolic library that is available.Library learning first gained popularity in the context of program synthesis, with systems such as DreamCoder [126] that are capable of discovering increasingly higher-level abstractions from its solutions to previous tasks.For theorem proving, LEGO-Prover [127] demonstrated this idea in Isabelle, by proposing potentially reusable lemmas to aid in the proof of a theorem at hand.Besides new theorems, researchers have also explored learning tactics-procedural proof-generation strategies that shorten otherwise lengthy low-level proofs, often tailored to a particular mathematical domain.Tactic induction has been demonstrated to work in simpler formalisms [128,129], but has not yet been developed for full-fledged environments such as the tactic languages in Lean or Coq.</p>
<p>Premise Selection and Retrieval.A large, changing library of theorems poses challenges for training neural theorem proving models, since the prover should not be limited solely by the lemmas and definitions it was given at training time.One architecture that can accommodate a changing library is retrieval-augmented generation (RAG).In RAG, before attempting to generate (e.g., a proof for a target theorem), we first retrieve potentially useful pieces of data from a database (e.g., lemmas from a mathematical library), putting these in the context that is given to the LLM.Retrieving lemmas that are likely to be used to prove a target theorem is a problem known as premise selection [130], which has been extensively studied both in learning-guided [29,131,132] and symbolic [133][134][135] theorem provers.Even before neural networks were directly applied to generating proofs, they had been shown to be effective as premise selection models in earlier works like DeepMath [29].</p>
<p>ReProver, the architecture introduced in LeanDojo [35], applied retrieval for neural theorem proving, where it first retrieves lemmas from a mathematical library.The COPRA [36] approach also uses retrieved lemmas as part of LLM queries.One desideratum in such retrieval-based approaches, originally identified in LeanDojo, is success on a train/test split named "new premises", where each theorem seen at test time requires at least one premise that has not been seen during training.This setup is closer to real use cases for a theorem prover, where human mathematicians might be formalizing a new domain, frequently building on recently proved lemmas that did not exist in the original training data.</p>
<p>Verified Reasoning in Natural Language</p>
<p>Many reasoning problems expressed in unstructured natural language are difficult to formalize completely.In such cases, it is desirable to still have some form of verification for natural language reasoning.Several works have proposed to "verify" natural language reasoning via trained, few-shot prompted, or symbolic verifiers.For example, the paper introduced the GSM8K dataset [15] of grade-school mathematical word problems proposed finetuning GPT-3 to verify a partial solution.</p>
<p>Combining GPT-3 as a solution generator with the finetuned verifier produced substantial accuracy gains.On this line of work, OpenAI later released PRM800K, a large-scale dataset of humanwritten feedback on step-by-step mathematical solutions, which has been used to explore training verifiers [136].For logical reasoning in natural language, NLProofS [137] proposed a step-wise verifier, trained to evaluate whether a given conclusion is entailed by a set of premises, using it to guide a step-by-step prover.</p>
<p>Step-by-step verifiers can also be obtained by prompting an LLM in a context dedicated to verification, as done in Natural Programs [61].In all of these works, while the verifier cannot formally guarantee the validity of the reasoning, it nonetheless provided a boost in overall performance and faithfulness of the responses.</p>
<p>As opposed to verifying directly in natural language, another line of work has explored using LLMs to first formalize a problem given in natural language, then leverage a symbolic solver to find solutions.In this thread, SatLM [138] and LINC [119] have both leveraged SAT/SMT solvers as a logical backend for reasoning problems, with the LLM only being responsible for parsing the original problem into an appropriate formal counterpart.In both methods, however, the system does not provide a step-by-step solution, since all the reasoning happens inside the solver, in a form that is challenging to translate back to natural language.LogicGuide [139] proposed to use a formal system to constrain the step-by-step deductions from the LLM, producing chain-of-thought reasoning that alternates between formal and natural language.In all of these systems, while the formal reasoning is guaranteed to be sound, it is still difficult to assess whether the natural language problem has been properly formalized.This typically stems from the fact that natural language allows for ambiguity and reliance on common sense or general world knowledge, whereas the formal problem must be fully unambiguous and all of the assumptions must be fully written down, which is typically challenging.</p>
<p>Formal System Verification and Verified Generation</p>
<p>The formal verification of software [140][141][142] and hardware systems [143,144] has long been among the foremost applications of formal mathematics.In this area, one first specifies the correctness and security requirements of a system as formal assertions.Next, theorem proving and model-checking techniques are used to prove that the system satisfies its requirements or, alternatively, to find bugs.</p>
<p>Specifically, deductive theorem proving has been applied to a range of critical systems, including microprocessor designs [145], file systems [146,147], OS kernels [46,148], cryptographic implementations [149,150], compilers [47], distributed systems [151], and network infrastructure [152,153].However, writing formal specifications and proofs that are necessary for verification requires substantial manual effort.For instance, verifying the seL4 OS kernel involved more than 20 person-years of intricate engineering work [154].</p>
<p>AI for formal mathematical reasoning offers a promising way to automate many tedious aspects of theorem proving applied to system verification, drastically reducing its costs.Advances in neural theorem proving, as detailed in Sec.3.2, can be effectively harnessed in software and hardware verification efforts that use formal mathematics, facilitating the generation of initial proofs [36,155,156] and the refinement of existing proofs [40,157].Moreover, LLMs are useful for assisting with various SMT-based verification tasks, including inferring necessary loop invariants [158][159][160][161], generating helpful assertions [162], and translating natural language to formal specifications [163].</p>
<p>A closely related challenge is employing AI to simultaneously generate code with formal proofs of correctness and security.For example, LLMs have recently made remarkable progress in programming tasks [164].However, LLM-generated code can be buggy and insecure [165,166], and some recent research finds LLM-generated code to exhibit more vulnerabilities than human-written code [167].Coupling generation with formal verification is a natural way to prevent such failures.</p>
<p>One possibility here is to first develop a formally verified program (or design) in a framework like Coq and Lean, with AI assistance, and then to translate the developed artifact into a more efficient lower-level implementation using standard compilers.This approach establishes a direct arc between theorem proving and generation.Another possibility is to incorporate LLM-based code and proof generation into a high-level verification-friendly language like Dafny [168] or Verus [169].</p>
<p>The design of formal specifications is a particularly challenging aspect of formal methods.However, the autoformalization techniques mentioned in Sec.3.1 can help generate formal specifications from natural language or code.There are also settings such as transpilation [170] in which specifications come for "free".In transpilation, one starts with code for a system in a source language and uses AI to generate code in a different target language.The two systems are required to be equivalent; thus, the source-language code forms a complete specification for the target-language code.</p>
<p>Open Challenges and Future Directions</p>
<p>Formal mathematical reasoning presents a wealth of challenging problems for AI.Here, we explore several open challenges and promising directions, including data and algorithms for formal mathematical reasoning, AI tools for assisting human mathematicians and proof engineers, as well as integrating AI and formal methods to generate verifiable code.While the discussion inevitably reflects our own perspectives and preferences, we hope it will provide inspiration and a roadmap for the broader community to advance in this field.</p>
<p>Data</p>
<p>How to overcome the scarcity of formal data?</p>
<p>â€¢ Autoformalizing informal math from textbooks, research papers, and lecture notes.</p>
<p>â€¢ Generating synthetic conjectures and proofs from mathematical axioms.</p>
<p>â€¢ Knowledge transfer from different proof frameworks and data-rich modalities such as code.</p>
<p>A key driver of the performance improvements in LLMs has been captured in empirical scaling laws: LLM performance tends to broadly and consistently improve when we grow the model size and data size together [18,19].However, this pace of improvement due to scale alone faces significant challenges when applied to formal mathematics, due to the scarcity of human-created formal proof data.For instance, the Proof Pile dataset [10], which aggregated proofs from six different formal languages (Lean, Coq, Isabelle, HOL Light, Mizar [171], and Metamath [8]), collected only 500MB of formal proofs.This is orders of magnitude smaller than relevant datasets for training LLMs in other domains, such as Python code available on GitHub.The data scarcity issue is even more pronounced in research-level mathematics, where even informal data is limited.At the research frontier, data will arguably always be scarce; by the time we have abundant data on a particular domain, there might not be much novel research left to be done there.</p>
<p>Researchers have been exploring a few different strategies to overcome data scarcity.The first is autoformalization: attempting to automatically formalize informal mathematical texts.We have substantially more informal math data available on the web in the form of textbooks, research papers, lecture notes, and other resources, far exceeding the current formal math libraries.One of the hopes in the field is to create a positive reinforcement loop when attempting to automatically formalize these sources.Since formal proofs can be mechanically checked, if a system successfully translates even a small subset of the available informal math data, it can learn from those translations for training in an expert iteration loop, potentially covering an increasingly larger set with each iteration (Sec.3.2).</p>
<p>The second approach relies on synthetic data generation using a formal system.AlphaGeometry [5] recently took this approach, completely eschewing pretraining on human-written problems and instead relying solely on synthetically generated geometry problems and solutions.This strategy leverages the fact that mathematical axioms contain, in principle, infinite potential data, since they entail all of the provable facts in the domain.If it can be generalized, a significant benefit of this approach would be its applicability to completely new domains of mathematics, where even informal data (such as research papers) might be scarce.By generating synthetic data, AI systems can potentially explore and learn from the vast space of possible mathematical problems and solutions, at a scale that can drastically surpass the pace of human-generated training data.</p>
<p>Autoformalization and synthetic data generation were combined in AlphaProof [7], which autoformalized one million IMO-like informal problems into one hundred million formal theorems, whereas proofs were synthetically generated in an expert iteration loop.It remains an open question to generalize this approach beyond domains where a large number of human-written problems are available, as will be the case in research mathematics.For those domains, we will likely also depend on conjecturing new, unseen statements [172].</p>
<p>A third approach is the use of multilingual data.Different formal proof frameworks tend to have different distributions of proof data; for example, formal software verification efforts have historically used Coq and Isabelle more than Lean, while recent efforts to formalize research mathematics are largely Lean-based.Building AI systems that can interact with different proof environments [36] is one way to get the best of different proof frameworks.An alternative path is to translate formal theorems and proofs across frameworks, but this direction remains relatively unexplored as of now.</p>
<p>Another promising strategy to enhance AI's capabilities in formal mathematical reasoning is transferring knowledge from different modalities.Specifically, the code modality is closely related to formal mathematics as both require symbolic reasoning.This similarity has been exploited to improve AI's skills in mathematics during both inference [173] and training phases [174,175].Prior research has shown that multi-lingual code models often outperform mono-lingual models when provided with equivalent training resources [176] and knowledge of high-resource languages can be transferred to low-resource ones using program translation [177].This raises an interesting research question: how can we leverage datasets of data-rich programming languages such as Python and C/C++ to enhance reasoning in formal mathematical languages?Moreover, considering that current AI models for formal mathematics typically focus on single languages [32,35,178], there is a promising opportunity to develop a multi-lingual model (e.g., combining Lean, Coq, and Isabelle), potentially boosting performance across all these languages.</p>
<p>Algorithms</p>
<p>How to scale up autoformalization?</p>
<p>â€¢ Automatic metrics for evaluating autoformliazed statements.</p>
<p>â€¢ Breaking the autoformalization process into small steps (like in chain-of-thought).</p>
<p>â€¢ Autoformalizing with more interaction with the formal system.</p>
<p>Autoformalization at Scale.As a bridge between informal and formal mathematics, it should be emphasized that autoformalization is a task beyond pattern matching.For example, given an informal statement we might formalize it in multiple ways that are syntactically different but logically equivalent.Alternatively, two syntactically similar formal statements can certainly bear opposite logical meanings.As Jiang et al. [38] observed, neither classic automatic machine translation metric like BLEU [179] nor compiling success rate really correlates with human evaluation, but relying on human evaluation is obviously not scalable.Without an automatic metric to evaluate autoformalized statements, it would be challenging to build a robust statement formalization pipelines.Nevertheless, wrongly autoformalized statements can still be utilized: In AlphaProof [7], DeepSeek-Prover [39], and Lean Workbook [180], agents attempt to simultaneously prove and disprove autoformalized statements, and either will leave a trace to further reinforce the neural proof agents.Still, we want to have an automatic metric more aligned with human judgment so that the quality of autoformalized statements can be truly evaluated at scale.Possible ideas include checking logical equivalence via automated theorem provers [82,181].</p>
<p>Depending on the level of mathematical abstraction, autoformalization can range from a relatively straightforward translation task (e.g., formalizing '1
+ â€¢ â€¢ â€¢ + n = n(n + 1)/2'
) to a hard reasoning task (e.g., formalizing 'every finite group of odd order is solvable') that requires retrieving existing definitions or even inventing new ones.For a hard reasoning task, it is natural to break down the autoformalization process into smaller steps, like in chain-of-thought prompting.For example, Wu et al. [37] observed that a common source of errors in autoformalizing statements is the mismatch between informal and formal definitions, which might be alleviated by retrieving definitions before attempting to formalize the statement.When autoformalizing proofs, a natural approach is to convert a large piece of informal proof into a formal sketch before attempting to fill in the details.An alternative approach, used in Lean-STaR [125], is to sample one natural-language step each time and use it to synthesize the next proof tactic.We envision that through smaller steps and process supervision [136], the performance of existing autoformalization models could be further improved [115].</p>
<p>Finally, given that formalization is an interactive process for human experts, often involving extensive trial and error with the proof assistant, we believe that a good autoformalizer should inherently support such interactivity.Formalizing statements may require introducing new definitions and data types (e.g., in the 2024 IMO P5), while autoformalizing proofs could benefit from a recursive approach to address gaps in informal proofs.Both aspects call for autoformalization to be more interactive [182].</p>
<p>How to improve the model architecture for mathematical reasoning?</p>
<p>â€¢ Multi-step reasoning, long contexts, abstractions, and hierarchical planning.</p>
<p>â€¢ Controlled studies on synthetic benchmarks for diagnosing reasoning failures.</p>
<p>â€¢ Scaffolding the model with inference-time techniques such as retrieval and search.</p>
<p>Models for Mathematical Reasoning.Machine learning models for mathematics should possess two key capabilities: First, the model should be capable of memorization, allowing it to store mathematical knowledge, such as facts, definitions, and existing theorems.Second, the model should be able to reason effectively about its knowledge, which necessitates multi-step reasoning, handling long contexts, learning abstractions, and hierarchical planning.In recent years, Transformer-based Language models [183] have become the leading architecture for AI mathematical reasoning [34,37,50].</p>
<p>Transformers excel at memorization: They are the most scalable architectures to date for Internet-scale pretraining [18,19], during which their model parameters are adapted to internalize mathematical knowledge in the pretraining data.Although a precise understanding of how Transformers extract and memorize knowledge is still nascent [184], their effectiveness is widely recognized in practice [185].</p>
<p>However, whether Transformers can reason logically is an open question.On the one hand, they have demonstrated exceptional performance on numerous reasoning benchmarks [16,186].On the other hand, they still exhibit reasoning flaws, even in simple settings [187].For example, Wang et al. [188] show that Transformers learn to reason only through grokking, which occurs when a model is trained far beyond the point of overfitting-a scenario not applicable to pretrained LLMs.Zhang et al. [189] find that Transformers fail to learn true reasoning that is generalizable across different data distributions.Furthermore, LLMs have been found to fall short on planning [190][191][192].</p>
<p>Formal mathematics provides a valuable domain for understanding and improving Transformers' capabilities in reasoning and planning, as well as for developing alternative architectures [193][194][195][196][197]. When experimenting with new architectures, we often face the dilemma that these models underperform standard LLMs because they do not readily benefit from large-scale pretraining, whereas limiting to pretrained models would restrict the model choice.Formal mathematical reasoning may help address this dilemma through synthetic benchmarks [198].These benchmarks can be procedurally generated based on the formal system's inference rules, and they can have adjustable knobs for controlling the difficulty levels or testing a specific capability, such as generalization to longer reasoning chains [199].Additionally, performing well on these simple benchmarks typically requires only small models that can be trained within days using a few GPUs.This setup enables controlled scientific experiments, which help diagnose the model's failures and discover insights that can potentially be scaled up in follow-up studies.</p>
<p>Besides designing an architecture that inherently excels at reasoning, another highly effective approach is to scaffold the model with external techniques like chain-of-thought [12], retrieval [35], and search [124].Next, we will discuss some of these topics in detail.</p>
<p>How to search for proofs effectively?</p>
<p>â€¢ Scaling up the search to leverage more test-time compute.</p>
<p>â€¢ Systematic evaluation of models, search algorithms, and hyperparameters.</p>
<p>â€¢ Value models for assessing and prioritizing proof goals.</p>
<p>Proof Search and Test-Time Compute.</p>
<p>Search and learning are the two most important classes of techniques for utilizing massive amounts of computation in AI research.</p>
<p>-Richard Sutton [200] Proof search is fundamental to many formal reasoning systems.In particular, most existing neural theorem provers (Fig. 4) generate proofs by combining tactic generation with search algorithms such as breadth-first search [201], best-first search [35], or Monte Carlo Tree Search (MCTS) [124].</p>
<p>Scaling up the proof search to leverage an enormous amount of test-time compute has been crucial to the success of AlphaGeometry [5] and AlphaProof [7] on IMO problems.Furthermore, despite originating from formal reasoning [202], proof search has recently gained prominence in natural language reasoning [136,137,203,204].There, LLMs' reasoning is scored by a verifier model (also referred to as "process-supervised reward model") and is searched to arrive at a final solution, much like searching for a proof.Scaling test-time computation during search has led to promising results [205] and is likely a key component of OpenAI o1 [17]. 3espite the importance of proof search, many myths and trade-offs surrounding it remain unexplored.First, is proof search truly necessary?Baldur [40] and DeepSeek-Prover [39] use LLMs to generate whole proofs directly, without search.This approach offers substantially lower latency, making it attractive in interactive applications such as proof completion in code editors.A widely held belief supporting proof search is that decomposing proofs into individual steps can improve data efficiency and generalization.While plausible, we are unaware of empirical evidence from a systematic comparison between search and whole-proof generation.Second, should we use small models or big models in proof search [206]?Given a fixed compute budget, smaller models allow exploring more steps.For instance, Graph2Tac [207] suggests that simple models like k-nearest neighbors can perform competitively with Transformers.In addition, how do different search algorithms compare (e.g., MCTS vs. best-first search) ?What is the effect of decoding algorithms (e.g., sampling vs. beam search) and hyperparameters (e.g., temperature)?How to search for proofs efficiently with access to high-end GPUs (typical in AI research) vs. consumer CPUs (typical among Lean users)?</p>
<p>To resolve these myths, it is necessary to systematically evaluate existing theorem proving methods, as this will provide clarity and guide the development of future provers.However, such an evaluation is currently lacking and would require substantial effort.Conducting a fair and unified evaluation of theorem provers presents significant challenges.It is unclear how to compare provers targeting different proof assistants.While cross-system benchmarks like MiniF2F [208] and PutnamBench [209] have formalized problems across multiple proof assistants such as Isabelle and Lean, this alone does not imply the results are comparable.The difficulty of proving a theorem can vary widely between proof assistants due to the varying levels of proof automation.Even within a single proof assistant, a theorem prover's performance is multifaceted and depends on resource constraints (e.g., hardware and time limits), making it difficult to consolidate performance into a single metric.A comprehensive evaluation that carefully addresses these challenges would be immensely valuable to the community.</p>
<p>Despite the challenges in evaluation, researchers have been exploring various directions to improve proof search.Effective search requires prioritizing the most promising goals for further exploration.</p>
<p>Mathematicians often rely on intuition to gauge a goal's promise, and the popular MCTS algorithm can be viewed as its Monte Carlo estimation.One fruitful direction is developing value models for assessing proof goals, through finetuning [124,210] or by prompting instruction-following LLMs as demonstrated in Tree of Thoughts [211].However, assessing the promise of proof goals remains a challenging task.Minor changes in the goal can impact its provability.Furthermore, while positive examples (promising goals) can be extracted from existing proofs, negative examples are much harder to obtain.One approach is to use the current model to generate negative examples during proof search and try to bootstrap the model carefully.Alternatively, when working in a narrow domain, we can leverage domain-specific knowledge to evaluate proof goals (more on domain-specific provers later).</p>
<p>Proof search is important, but it alone does not solve theorem proving.Unlike Go, a fundamental challenge in theorem proving is a discrete, infinite action space whose structure is not fully understood.This unbounded nature makes it difficult for models to generate effective actions through supervised learning or exploration via reinforcement learning.Proof search cannot succeed if the model cannot produce high-quality actions in the first place.Many believe that mathematics requires creativity, and in the context of theorem proving, creativity can manifest as actions exceeding the current model's capabilities, akin to the "divine move (ç¥žä¹‹ä¸€æ‰‹)"-a legendary concept in Go.We would not expect to find them if the action space were an infinite, unstructured list.Fortunately, mathematics is highly structured, making it possible-though still challenging-to find the divine moves [212].In the remainder of this subsection, we will explore several ways to leverage structures in mathematics, including hierarchies, abstractions, external knowledge, and domain-specific knowledge.</p>
<p>How to exploit hierarchies in theorem proving?</p>
<p>â€¢ Decomposing large, high-level proof goals into smaller goals progressively.</p>
<p>Exploiting Hierarchies in Theorem Proving.Theorem proving can benefit from exploiting the natural hierarchies that organize mathematical results: Big theorems follow from smaller lemmas, and even those lemmas can be thought of as progressively accomplishing smaller sub-goals during the proof, until each goal is small enough to be "obvious" in informal proofs, or closed in a single step in a proof assistant.Several existing theorem proving systems attempted to exploit this hierarchy.For example, Draft, Sketch, and Prove (DSP) [81] used an informal proof (written by LLMs or humans) to obtain a formal "proof sketch"-a skeleton of the formal proof with "holes", i.e., open goals left unproven, which yields a hierarchical structure for the formal proof.However, even a single open goal in the proof sketch might require significant effort to prove.POETRY [213] proposed to recursively decompose proof goals using an LLM.It verifies that each of the intermediate decompositions is valid: When a larger proof goal is decomposed into several smaller goals, it must be provable, assuming the smaller goals can be proved.On another use of hierarchy, LEGO-prover [127] proposes separate, potentially helpful lemmas, that it tries to prove first when it fails to prove a given theorem directly.While these works have begun to exploit the potential of hierarchical decomposition, it is still a significant challenge to decompose realistic high-level goals (or sometimes even individual informal proof steps) with the current capabilities of LLMs.Ideally, we would like humans to be able to provide high-level targets, and let AI do the work of progressively closing the gaps between what is currently known (e.g., the current proof state, or the existing library) and what it needs to achieve.</p>
<p>How to learn mathematical abstractions?</p>
<p>â€¢ Learning to construct new definitions, lemmas, and tactics in full-fledged proof assistants.</p>
<p>Learning Mathematical Abstractions.While learning mathematics, humans can learn progressively more sophisticated mathematical abstractions.We start learning and operating on natural numbers by counting; years later, those operations show up in solving equations, but don't require as much attention anymore.Later on, even solving entire systems of equations becomes the simplest step in the context of harder problems.One research challenge is how to allow machines to progressively construct these abstractions.In interactive theorem provers, these abstractions are encapsulated in new definitions, lemmas, and tactics-these encode proof strategies that are helpful in a particular domain or kind of proof goal.In principle, these forms of abstraction aren't essential for formally representing mathematics, since what they do can always be repeated inline in a given context.Yet, progressively developing new abstractions is central to the human practice of mathematics.</p>
<p>Most theorem proving systems focus on taking in a set of definitions, lemmas, and tactics, leveraging those to prove new theorems.However, several recent lines of research have proposed methods for learning abstractions.The interactive theorem proving community has developed several methods for lemma synthesis [214], typically with the goal of helping a user prove a particular theorem interactively [215].In AI for mathematics, LEGO-Prover recently used LLMs to propose and prove new lemmas that also get added to its library, helping it prove further theorems.Lemma mining from existing proof corpora has also been explored, such as in HOL Light [216] and Metamath [217]: these lemmas, not explicitly factored out by humans, are still useful for automation.On learning tactics, Peano [128] and LEMMA [218] have proposed to learn simple proof strategies from an agent's own solutions to past mathematical problems, in a bootstrapping fashion.These so far have been demonstrated only in simpler formal systems, and it is still an open challenge to synthesize entirely new tactics in full-fledged formal theorem proving languages.</p>
<p>How to utilize existing mathematical knowledge?</p>
<p>â€¢ Tailored retrievers for formal mathematical reasoning.</p>
<p>â€¢ Handling dynamically growing knowledge bases.</p>
<p>Incorporating Information from Knowledge Bases.Another direction is to explicitly incorporate knowledge from databases of pre-existing lemmas and definitions.To some extent, LLMs used in math reasoning tasks are repositories of mathematical knowledge.However, some of the knowledge relevant to proofs may not be represented in the LLM's pretraining data; even if this knowledge were present, it may not be easy to retrieve.Therefore, the use of an explicit retrieval mechanism can help.</p>
<p>Among existing methods, ReProver [35] and COPRA [36] use retrieval mechanisms and achieve nontrivial performance gains from their use.These approaches use standard retrieval mechanisms, such as BM25 [219] and Dense Passage Retrieval [220].It is possible that retrieval mechanisms more tailored to formal mathematics would perform better.For example, one can imagine developing retrieval methods based on structured, neurosymbolic embeddings that use vector representations of math facts while also allowing symbolic methods to filter out irrelevant facts.</p>
<p>Another angle is to consider scenarios in which the external knowledge base grows over time.</p>
<p>For example, one can imagine a mathematical reasoning system that decomposes high-level proof objectives into subgoals, caches a subset of these subgoals as modules, and appropriate modules for use in subsequent (or concurrent) proof efforts.Deciding which subgoals are "interesting" enough to be modularized in this way is an interesting challenge.</p>
<p>How to reconcile the specialist and generalist approaches?</p>
<p>â€¢ Generalist methods for identifying cross-domain connections.</p>
<p>â€¢ Specialists for effectiveness in individual domains and collaboration with mathematicians.</p>
<p>â€¢ Combining generalists and specialists, e.g., by equipping LLMs with domain-specific tools.</p>
<p>Generalist vs. Specialist.Mathematics encompasses a wide range of subdomains, and so far, our discussions have largely remained domain-agnostic.In principle, most mathematical domains can be formalized in proof assistants such as Lean, enabling LLMs to perform tasks such as theorem proving and autoformalization.LLMs only need to process the data as plain text, without accounting for the specifics of each domain.This "generalist" approach has clear merits: it is generally applicable and facilitates knowledge sharing across domains.Modern mathematics is too vast for any individual to master everything.LLMs, however, with massive training data, can easily surpass human mathematicians in terms of breadth of knowledge.Numerous historical mathematical breakthroughs resulted from uncovering connections between seemingly unrelated subjects.For example, Wiles's proof of Fermat's Last Theorem [221] emerged from identifying connections between elliptic curves and modular forms.In the long term, it is desirable to have LLM-powered generalist AI to augment human mathematicians in identifying such cross-domain connections.</p>
<p>However, each mathematical domain possesses its own idiosyncrasies and unique techniques.The generalist approach risks missing the opportunity to exploit these domain-specific insights.Many AI4Math systems are specialists in a particular domain.A notable example is AlphaGeometry [5], which specializes in proving Euclidean geometry theorems in math olympiads.It consists of three key components: (1) an algorithm that generates synthetic theorems and proofs; (2) a symbolic reasoning engine for deducing basic geometric properties; and (3) a Transformer model for introducing auxiliary constructions (points, lines, or circles not present in the original problem).Each component is designed specifically for 2D Euclidean geometry and leverages domain knowledge to achieve efficiency.While some high-level ideas in AlphaGeometry could potentially be applied to other domains, adapting the entire system would require substantial redesign.</p>
<p>Specialists like AlphaGeometry are highly effective in their domains, often solving problems that are currently challenging to address in a fully general manner.These systems leverage domain knowledge in various ways, e.g., for proof automation, evaluation, finding counterexamples, or numerical computation [82,94,222,223].Building specialists for other mathematical domains will continue to be a fruitful research direction, particularly for collaborating with mathematicians who want to use AI for their domain of interest (Sec.2.3).In addition, narrow domains offer a rich set of controllable tasks and environments [198] for investigating the reasoning capabilities of AI models.</p>
<p>The Bitter Lesson [200] stresses the great power of general-purpose methods, and AI researchers always strive for generality.Even when designing specialists, we value insights that have the potential to generalize.Despite this, we anticipate that AI math specialists will coexist with generalists in the foreseeable future.An exciting avenue lies in combining the two, such as by equipping a general LLM with domain-specific algorithms as tools.Ultimately, AI might become powerful enough to learn and exploit the unique characteristics of each domain in a universal way.</p>
<p>Tools for Assisting Human Mathematicians</p>
<p>How can AI better assist humans in formal mathematics?</p>
<p>â€¢ Resources, incentives, and engineering efforts to improve usability and user-friendliness.</p>
<p>â€¢ Behavioral studies of how mathematicians work with formal tools.</p>
<p>â€¢ Tools that enable large, distributed collaboration.</p>
<p>While improving performance on standardized benchmarks is a means to coordinate efforts and measure progress in AI, many significant challenges only arise when trying to build tools that integrate with the workflow of mathematicians using proof assistants.Working mathematicians often report that the bottleneck for the adoption of tools, rather than technical features, is their usability and user-friendliness [224,225].To productively integrate AI tools for assisting mathematicians, the field can drastically benefit from behavioral studies of mathematicians working with formal tools in an ecological fashion.We note that the Human-Computer Interaction and Programming Languages communities have both collaborated in such studies for users of regular programming languages, leading to insights into how human programmers learn and conceptualize their tools [226], their confusions and challenges, and ultimately how to improve these tools and the way we teach them [227].We envision that many methodologies from such work can be transported to proof assistants, ensuring that the tools we build serve the needs of human mathematicians.</p>
<p>Besides aiding the workflows of individuals, one key advantage of proof assistants that has been highlighted is that they enable large-scale collaboration among mathematicians [224,228].Tools such as Lean Blueprints [229] allow large mathematical projects to be conceptually broken down into modular components, and each of those can be worked out independently.Trust in other's work, which traditionally has either required personal trust or understanding and checking all of their work, is now facilitated by a formal proof checker.These tools are likely to be just a first generation of what formal proof assistants can provide for collaboration in mathematics.In the future, collaboration might happen between both humans and computers in a distributed fashion [228]; AI agents might make autonomous contributions to human projects, and vice-versa, mediated by formal tools that guarantee correctness.This vision can likely borrow from previous experiences in distributed, collaborative computing platforms from other fields, like Folding@Home [230], where anyone can contribute computing power to protein folding.The same is possible for proving mathematical theorems.</p>
<p>Formal Verification and Verified Generation</p>
<p>How can AI assist humans in developing correct and secure software?</p>
<p>â€¢ Incorporating formal methods into AI-aided system design and implementation.</p>
<p>â€¢ Enhancing AI capabilities for formal software and hardware verification.</p>
<p>â€¢ Coupling AI-based generation and formal verification.</p>
<p>In alignment with our stand for AI4Math, we envision a growing necessity for developing formal reasoning techniques for AI-based software and hardware generation.These techniques can provide developers with assurance on the correctness and security of generated artifacts-an indispensable step for deployment, which currently often requires significant manual effort.While syntactical correctness can be guaranteed by constrained decoding [231,232], ensuring other semantic properties, such as those validated by static analysis and compilers, remains an open challenge.Moreover, formal reasoning can assist programmers in understanding AI-generated code that they did not write themselves.Recent research demonstrates this through the live demonstration of runtime values [233].</p>
<p>It is a promising direction to explore the use of other kinds of formal methods in this context.</p>
<p>Formal verification bears some resemblance to the research mathematics setting but also poses unique challenges.For example, a necessary but challenging step for formal verification is encoding the target system semantics and the correctness requirements in the proof assistant.This process is akin to formalization of informal mathematics [37]; however, while statements in mathematics research tend to assert properties of established mathematical objects, theorems in formal verification typically concern bespoke procedures and datatypes.Also, proofs in formal verification tend to be more repetitive and heavy on case-splits and inductive reasoning about recursive functions and datatypes.Finally, unlike statements in mathematics research, real-world software and hardware systems are characterized by large codebases and frequent changes.For instance, the verification of the seL4 [46] operating system kernel consists of about 200,000 lines of specifications and proofs in Isabelle.Verification of these systems requires not only theorem proving but also rigorous management of specifications and proofs [142].An exciting yet underexplored direction is leveraging AI's strong capabilities in code and mathematics to enhance the entire process of proof engineering [142].</p>
<p>It is natural to couple formal verification and AI-based generation into approaches that simultaneously generate code (or designs), formal specifications (i.e., pre-/post-conditions, loop invariants, and helper assertions), and proofs.Given these generated artifacts, a program verifier or a theorem prover can be called to check if the code is consistent with the specifications and proofs.This approach has been explored in recent research [168,[234][235][236] and holds great potential in reducing verification efforts and enhancing software and hardware reliability.However, a key challenge is ensuring the trustworthiness of the generated specifications-that they are neither too strong nor too weak and accurately reflect developers' intent.</p>
<p>Milestones and Success Measures</p>
<p>Having outlined directions for advancing this field, a key question is: how can we effectively measure progress?Inspired by the levels of automation for self-driving cars [237], we propose a framework for categorizing AI's capabilities in formal mathematical reasoning.Focusing on individual areas such as theorem proving and autoformalization, we define various capability levels and review existing benchmarks for evaluation.Existing benchmarks fall short when it comes to evaluating higher-level capabilities, particularly in nascent areas like conjecturing.To address these gaps, we highlight the pressing need for new benchmarks and, in many cases, new evaluation methodologies.CoqGym [33], LeanDojo [35], MiniF2F [208], PutnamBench [209], TPTP [238], FIMO [239] 4 Contributing to formalization projects autonomously New benchmarks of code and metadata from GitHub, similar to SWE-bench [240] 5 Solving problems and discovering new math beyond the human level New benchmarks and evaluation methodology for unknown territories Most of the current effort in AI for formal mathematics has been centered around automated theorem proving.Here, given a theorem, we want AI to produce a proof.Formal systems, such as Lean, offer an immense advantage to pose this problem, since once a proof is found, it is guaranteed to be correct, even if it might be difficult to parse by humans.We now discuss milestones and benchmarks in AI for formal theorem proving (Table 1).</p>
<p>Theorem Proving</p>
<p>The most basic capability level (Level 0) is simply recognizing a correct formal proof.It is already present in systems such as Lean [24,25], Coq [21], Isabelle [20], Agda [241], and many others, and can be obtained from a wide range of logical foundations.Though proof checking is essentially a solved problem for formal proofs, it can still be incredibly challenging to fully formalize even existing, well-understood informal proofs, to the level where they can be mechanically verified by a formal system.Human mathematicians, in contrast, are able to evaluate incomplete, "hand-wavy" arguments, often being able to find flaws and point out counterexamples even in the absence of a complete proof.Thus, even our proof checking systems can still be improved in this direction by requiring less effort from the user and putting more responsibility on the verifier.We currently do not have a good benchmark for the verification of incomplete, high-level proofs.Since these proofs often have large gaps that are left unwritten, checking them requires being able to fill those gaps, which is generally equivalent to the full problem of producing proofs (which the next capability levels are about).For benchmarking progress in formalizing mathematics, one folkloric standard used to compare different proof assistants is Freek Wiedijk's list [242] of one hundred theorems.As of today, all theorems in the list have already been formalized (by humans) in at least one proof assistant.</p>
<p>Moving beyond giving feedback on proof correctness, we can think of varying levels of systems that can help humans develop proofs.Level 1 would be to suggest potentially useful pieces of data, without attempting to write proofs yet.Here we include library search engines, such as Moogle [243], Loogle [244], and LeanSearch [245], which can find useful theorems or definitions, or "copilots" [43,246] that can generate contextual completions interactively.One common use case is to search for relevant definitions from the current goal and see what is already known about those (e.g., see theorems involving the sin function).These tools can be highly helpful, though the main job of deciding what to search for and how to develop the proof is still a human responsibility.</p>
<p>Capability Level 2 and above contain systems that generate proofs, either fully or partially.The most basic level of proof automation is (human-implemented) tactics: domain-specific procedures that are capable of automating certain classes of proofs, often serving as simpler steps in larger proofs.An example would be a tactic such as omega in Lean and Coq, which can automatically solve a large class of equality and inequality proof goals, or hammer tactics [247], which outsource the current proof goal to an external (most commonly first-order) automated theorem prover.While limited in domain, these procedures can already lower the burden on human users to formalize results.Still, up to this level, no learning-based system is involved: this level of automation mostly reflects the human ingenuity required to engineer domain-specific methods for producing proofs.</p>
<p>At Level 3, we include systems that can automatically prove theorems in a domain-general fashion, albeit still limited to simple theorems.This encompasses most current neural theorem provers [35,213,222]: current systems are typically trained on human-written proofs, and can generally be applied to any domain.Several of the most recent benchmarks in neural theorem proving target this level of capability: CoqGym [33], LeanDojo [35], MiniF2F [208], PutnamBench [209], TPTP [238] and FIMO [239] are prominent examples of such evaluations.However, in practice, we are still limited to relatively simple proofs, typically not the most time-consuming ones in the context of a larger project.Moreover, current systems generally assume a static library of definitions and previous lemmas, and they target well-posed statements (in fact, in these evaluation benchmarks, we know all the statements to be true, since humans have already proved them).These systems can still be helpful by proving technical lemmas or closing gaps in larger proofs.</p>
<p>Level 3 is the last capability level covered by existing evaluations.One level above, we have all the activities that human mathematicians engage in while developing formalization projects.These go much beyond proving lemmas that are given to them, as evaluated in neural theorem proving benchmarks.In a new formalization, a large part of the effort lies in choosing how to formally describe the domain of interest in the first place: what definitions to use, and how to break down the main results of interest into sufficiently small lemmas. 4These activities happen at the formal and informal levels simultaneously: for instance, in Lean, mathematicians have been using Blueprints [229] as a way to structure projects, divide up the work, and generally reason about how a large result might be broken down into manageable components.A system of capability Level 4 should be able to plan and execute formalization projects autonomously, breaking down larger results, stating new definitions and lemmas, and potentially exploring different alternatives as the project develops.This would already significantly accelerate progress in formal mathematics.For instance, we can expect recent papers to be formalized by AI at this stage.To evaluate Level 4 systems, it would be helpful to have new benchmarks constructed from the GitHub metadata, such as issues and commits, of real-world formalization projects.SWE-bench [240], a benchmark from software engineering, offers a model, but similar benchmarks for formal mathematics are yet to be created.</p>
<p>Finally, we might one day expect to have systems that go beyond what human mathematicians have been able to accomplish, solving existing open problems, or perhaps even formulating new interesting problems on their own.Currently, formal systems are typically used to formalize results that human mathematicians have first done informally.If one day AI becomes able to autonomously make mathematical discoveries, we can expect these discoveries to be made in a formal system.Otherwise, the cost of checking AI outputs will be too high, just as it currently is for verifying human mathematical proofs at the highest levels.Being able to solve problems beyond human level would constitute capability Level 5.While this seems categorically out of reach for any of the current AI systems, one fundamental challenge will be to be able to measure progress meaningfully towards this open-ended goal.A major difficulty with measuring progress towards mathematical discovery is that our current evaluations only test knowledge of current mathematics, while at this level we will want systems to be able to reason about new domains, which we ourselves might not know much about.</p>
<p>Verified Reasoning in Natural Language</p>
<p>Table 2: Verified reasoning in natural language: capability levels and benchmarks for evaluation.</p>
<p>Level Capability Evaluation and benchmarks 0</p>
<p>Stepwise natural language reasoning w/o verification GSM8K [15], AQuA [248] 1</p>
<p>Stepwise natural language reasoning with neural verification MATH [16], Fallacies [249], Process-Bench [65] 2 Tool-integrated reasoning using SymPy, NumPy, etc. MATH [16], AIMO Progress Prize [250] 3</p>
<p>Reasoning seamlessly in natural language and formal systems such as Lean New benchmarks for evaluating final answers and intermediate steps; problems difficult to formalize, e.g., IMO combinatorics 4 Complex mathematical reasoning and planning in real-world applications Downstream applications such as travel planning and calendar scheduling Theorem proving requires both the problem and the solution to be fully formalized, which can be overly rigid for many real-world applications.Even highly structured domains, such as IMO, have problems that are difficult to formalize (e.g., geometry and combinatorics problems), let alone everyday applications.How can we enable complex, rigorous reasoning without formalizing every aspect of a problem?A promising direction is to use AI to reason seamlessly between formal systems and natural language.Such AI should be able to conduct logical reasoning, perform numerical calculations, and generate solutions in a way that is both rigorous and human-understandable.While the resulting reasoning chain may not constitute a formal proof, it could still include parts that can be verified semi-automatically, potentially under human oversight.We refer to this capability as verified reasoning in natural language and propose a framework for understanding its varying levels (Table 2).</p>
<p>Level 0 involves generating step-by-step reasoning in natural language without verification.</p>
<p>The prevalent approach is to combine LLMs with chain-of-thought [12], but it frequently generates reasoning steps that are brittle, incorrect, or unfaithful [61,251,252].To address these limitations, Level 1 capability introduces verification alongside generation, requiring AI to assess the correctness of reasoning steps.This verification can be done by the same model that generates the reasoning [253] or by a different model [136].The result of verification can be used to improve reasoning.For example, we can generate many reasoning chains and use the verifier to select the best one [15]; the model can iteratively correct its own reasoning [254]; or it can search for solutions step by step, maximizing a correctness score produced by the verifier [137,203].</p>
<p>Many existing benchmarks can be useful for evaluating Level 0 and Level 1, e.g., math word problems in GSM8K [15] and AQuA [248], as well as commonsense reasoning and question answering benchmarks like CommonSenseQA [255] and HotpotQA [256].Level 1 particularly requires benchmarks that are challenging for autoregressive generation.For instance, the MATH dataset [16] includes competition-level math problems that require complex reasoning and planning, where the verification capability in Level 1 becomes essential [136].In addition to evaluating Level 1 indirectly through the final answer accuracy, it is valuable to directly measure whether the model can identify reasoning errors; Fallacies [249] and ProcessBench [65] are initial steps in that direction.</p>
<p>One limitation with standard benchmarks like MATH is data contamination [257], which may have partially contributed to LLMs' impressive, near-saturating performance.While data contamination is widely recognized, it is difficult to measure or fully eliminate its impact on evaluation outcomes.</p>
<p>To mitigate data contamination, it is useful to dynamically generate benchmarks in a controllable manner.For example, the GSM-Symbolic [187] benchmark is generated by substituting various numbers into GSM8K-style templates.It effectively reveals reasoning flaws in current LLMs, calling into question whether they have achieved Level 0 and Level 1 robustly.Another potential solution to data contamination, adopted by FrontierMath [57] and the AIMO Progress Prize [6], is to keep the benchmark private and allows the model to access it during evaluation via a secure mechanism.</p>
<p>Moving to higher capability levels, we aim to make reasoning more rigorous and trustworthy.Relying solely on neural networks for verification becomes insufficient due to their brittleness and lack of interpretability.To address this limitation, the model can generate reasoning that can be partially verified by external tools.These tools, built on well-established algorithms and thoroughly tested implementations, offer greater interpretability and trustworthiness than neural networks alone.In Level 2, models can leverage external tools to perform computation that neural networks struggle to learn reliably, such as numerical calculations with NumPy [258] and symbol manipulation with SymPy [56].Many recent math LLMs adopt this approach (called toolintegrated reasoning in Sec.2.1).Their performance can be evaluated on math problems requiring intricate computations, such as those in the MATH dataset or AIME.</p>
<p>Tools like NumPy and SymPy are effective for computation but not suited for reasoning.In Level 3, models should be able to use external tools to perform rigorous logical reasoning.For instance, when generating reasoning chains, the model could interleave natural language with formal reasoning steps in Lean.Unlike theorem proving, where theorem statements are predefined, here the model dynamically generates the "statements" during inference.Furthermore, instead of formalizing the entire problem, the model can selectively determine which parts of reasoning to process using formal systems versus natural language, seamlessly integrating them to construct the solution.To our knowledge, no existing systems has achieved this kind of integration.Current approaches [83,119,138,259] attempt to autoformalize the entire problem and apply theorem proving techniques, which is challenging for problems that cannot be fully formalized.</p>
<p>With recent advances in tool-using LLM agents [14], the seamless integration of formal and informal reasoning may soon be within reach.However, new benchmarks are required to evaluate this capability.Unlike current benchmarks like MATH, these new benchmarks should evaluate not only the final answer but also the quality of reasoning that led to it.Additionally, they should include math problems that resist complete formalization.A promising candidate for such a benchmark could be IMO in the same format as human contestants, 5 aligning with the evaluation protocol in the AIMO Prize [6].During evaluation, human graders can bypass the formal reasoning steps and focus on checking the natural language reasoning and their interface with formal steps.Beyond correctness, we could also use verification effort as an additional metric: an ideal solution would be both correct and easily verifiable with minimal human effort.</p>
<p>Real-world applications that involve complex reasoning are often not purely mathematical problems, yet they contain significant mathematical components along with other components such as commonsense and human preferences.Level 4 requires AI to recognize the mathematics in everyday tasks and apply rigorous reasoning.For example, in scenarios like travel planning [192] or calendar scheduling [260], AI can potentially formulate these tasks as constraint satisfaction problems and solve them using appropriate solvers such as mixed-integer linear programming (MILP) solvers.Achieving this capability would open up a wide range of new AI applications.Autoformalization involves automatic translation between informal and formal representation of mathematical knowledge.In Table 3, the most basic capability level (Level 0) is to store (i.e., encode and check) formal knowledge so that manual formalization is viable with enough human effort.With the maturity of modern proof assistants, almost all mathematical proofs can be formally encoded and checked, thanks to the expressiveness of the underlying logic of those systems.Admittedly, some proofs involving diagrams or higher-dimensional structures could be challenging to formalize in existing systems, and may even require further study in alternative foundations, e.g., homotopy type theory [262].We argue that most proofs with an axiomatic foundation can be tweaked into existing formal frameworks (the results may not look as natural as the informal proofs).With a formal encoding being viable, it is a matter of human efforts to transform the knowledge in different representations; therefore we consider this level to be mostly achieved.</p>
<p>Autoformalization</p>
<p>At Level 1, models should generate reasonable candidates for auto-(in)formalization, supported by an infrastructure to continuously gather and store human feedback, enabling a virtuous feedback loop to improve model performance over time.Being exposed to both informal and formal knowledge, existing LLMs have already learned to align concepts and generate reasonable syntactical translation given formal/informal counterparts.However, what is mainly lacking at this level is a system to gather and store the feedback: humans might have interacted with the model to get the revised candidates and use them in a formal or informal setting (e.g., writing a paper), but neither the interaction process nor the end results (i.e., aligned informal-formal pairs) have been properly recorded.The Isabelle Parallel Corpus [263] has been an early attempt to build a non-static parallel corpus.The Formal Abstracts project [264], initiated by Tom Hales, aimed to link standard mathematical publications with a Lean-based formulation that includes the main theorems but omits their proofs.By focusing solely on the statements, this project offered a potential framework for aligning contemporary mathematical statements with formalized counterparts.Unfortunately, the Formal Abstracts project did not gain traction for various reasons.As a less ambitious but more practical framework, Lean Blueprint [229] focuses on individual formalization projects, allowing users to create human-readable "blueprints" that can later be linked to Lean formalizations.This approach has been successfully applied in high-profile projects such as the Liquid Tensor Experiment [41] and the formalization of the Polynomial Freiman-Ruzsa conjecture. 6However, Lean Blueprint is used primarily as a one-off project planning tool, and informal proofs are not synchronized with their formal counterparts after the final formal proofs are integrated into Lean's Mathlib.In short, current autoformalization pipelines are not yet mature enough for daily use.Even when autoformalization techniques are applied in a project (e.g., in Lean Blueprint), the results are recorded statically and in various formats (e.g., briefly mentioned in the comment sections of formal proofs), making them difficult to collect and prone to desynchronization as the formal statements evolve.A milestone we envision for this level is a centralized system that enables automatic translation between informal statements and, ideally, multiple formal representations.This system would allow users to submit revised results, and it would keep informal-formal pairs synchronized as the formal statements continue to develop.</p>
<p>Using the aligned corpus collected in Level 1, AI models at Level 2 should be capable of performing robust and faithful translations between informal and formal statements, approaching humanlevel accuracy.The main obstacle anticipated at this level is the automatic evaluation of translated (formal) statements.Early experiments have shown a misalignment between automatic metrics, such as parsing rate and BLEU scores, and human evaluations.Fortunately, recent work incorporating symbolic equivalence [82,181,265] may help establish a standard automatic metric that aligns better with human preferences for evaluating translated formal statements.Model performance at this level could be assessed using human-curated benchmarks, including challenges from the ICML 2024 Math-AI workshop [266,267], ProofNet [114], Herald [261], and Con-NF [265].</p>
<p>Levels 3, 4, and 5 go beyond pattern matching and focus more on reasoning.Level 3 models should be capable of inferring missing information when autoformalizing statements and proofs, and can flag situations where an information gap cannot be filled.When formalizing mathematics, we frequently deal with underspecified problems with missing or implicit assumptions in mathematical statements and hand-waived steps in proofs.Bridging these information gaps requires robust theorem proving and reasoning capabilities in the AI models: Proof gaps may be addressed through neural or symbolic theorem proving (Sec.5.1), while missing assumptions can be resolved using abductive reasoning or counterexamples [268,269].The main challenge at this level is for the models to identify information gaps-such as assessing the likelihood that a statement is provable or can be adjustedeven when the reasoning model cannot bridge the gap immediately.This challenge is closely related to conjecturing (see Sec. 5.4) and has been examined briefly in some early explorations [270].At Level 4, AI models should be able to self-correct when they encounter erroneous or inconsistent inputs.At this stage, the autoformalization model focuses more on capturing human intentions and may rely on its own self-consistency to eliminate errors.Advancements here will be closely linked to natural language reasoning (Sec.5.2).Finally, at Level 5, AI models should be able to invent novel mathematical definitions that can potentially reduce proof complexity.At this level, an AI model is closer to a "theory builder" that can reshape the proving process through better abstraction or concept formulation.For instance, filters (i.e., a set of sets satisfying certain properties) are rarely taught in standard math curriculum.However, as a convenient abstraction in mathematical analysis, they have become a widely adopted concept for formalizing limits in various proof systems, including Isabelle [271], Lean [42], HOL Light [272], and Coq [273].Automatically devising definitions like filters is what we hope an AI model can achieve at this stage.In Table 4, Level 0 would be to formulate conjectures in the context of a problem, or a particular target result.These conjectures might be lemmas that, if proved, would help in proving a larger theorem of interest.This is related to how LEGO-Prover [127] proposes lemmas for itself to attempt as helpers in the context of the current theorem.These simpler, targeted conjectures can be evaluated in the extent that they facilitate proving the theorems they were meant to support.</p>
<p>Conjecturing</p>
<p>More generally, at Level 1, we might expect a conjecturing agent to be able to formulate conjectures in a given domain, without necessarily aiming at a particular theorem [275].While difficult to evaluate directly, these conjectures might still be evaluated in (a) how often the system can prove those conjectures and (b) whether the proofs of those conjectures, when found, serve as useful training data for proving other theorems in that domain.Minimo [172] is a system that implements this conjecturing-proving loop, showing that such conjectures can be generated only from the axioms of a given arbitrary domain, their difficulty can be targeted by a language model attempting to generate increasingly harder but provable conjectures, and that training on them improves the agent at unseen theorems.This paradigm has the potential to allow us to train systems for domains that have little or no human data available, which will necessarily be the case in new mathematical domains.However, this so far has only been shown to be practical in relatively small domains (on the order of a few dozens of axioms).It is still an open challenge to do this at the level of large libraries, such as Lean's mathlib, and to maintain steady progress after many iterations of conjecturing (Minimo was tested with up to 5, and starts to saturate).</p>
<p>Going even beyond that frontier, future AI systems might be able to formulate conjectures that go beyond current domains of mathematics.Some expect that a major milestone for future AI systems will be to both conjecture and prove a high-level, interesting mathematical result.While that goes far beyond current systems, such an achievement would mark an era where AIs work side-by-side with human mathematicians in expanding our collective knowledge of mathematics.</p>
<p>Formal Verification and Verified Generation</p>
<p>Table 5: Formal verification and verified generation: capability levels and benchmarks for evaluation.</p>
<p>Level Capability Evaluation and benchmarks 0 Code generation without verification HumanEval [164], MBPP [276], VerilogEval [277] 1 Verifying simple properties of small programs and designs miniCodeProps [278]; HumanEval/MBPP-Verus [236]; new benchmarks for generating formal specifications and verifiable code 2</p>
<p>Verifying and synthesizing entire projects with complex functional and security properties Selene [279] 3</p>
<p>Proof and system maintenance Benchmarks constructed from the change history (GitHub metadata) of verified systems 4 Helping users generate, explain, and debug formal specifications</p>
<p>As mentioned previously, the challenges in applying AI to formal verification and verified system generation are subtly different from those in the research mathematics setting.It is natural to define a ladder of capabilities for this context based on the sophistication of the generation and verification tasks (Table 5).In capability Level 1, AI can handle small-scale verification tasks, which involve verifying small blocks of code, or small designs, against relatively simple properties, and synthesize small pieces of verified code.This stage is critical as it sets the foundation for understanding how AI can be scaled up to real-world system verification efforts.Several existing benchmarks have targeted verification at this capability level [160,236,278].For instance, miniCodeProps [278] considers code properties that are viewed as a "minimum level of competency" for automated neural theorem provers.However, GPT-4o can only prove around 35% of the properties.Therefore, there is significant room for improvement in AI's ability to handle even these fundamental tasks.To the best of our knowledge, there is no existing high-quality benchmark for generating verifiable code together with the formal specification even at Level 0.</p>
<p>As we move to larger-scale systems, the complexity of both the code and the specifications increases significantly.In Level 2, AI should provide assistance in verifying and synthesizing entire projects and addressing complex properties.Examples of such properties include preventing memory safety issues [280], enforcing access control [46], and proving the equivalence of programs [281].These efforts can improve the automation of various security-related tasks, such as C-to-Rust translation [282] and reverse engineering [283].Achieving this level of verification involves decomposing large systems into smaller, verifiable components, a task that is currently performed by humans [148].</p>
<p>Advanced AI techniques are required to tackle this challenge, such as agentic approaches [36,192] that involve planning and problem-solving to navigate the intricate dependencies and interactions within large codebases and designs.Benchmarks for this level should incorporate project-level context, similar to repository-level code generation [284].This can be achieved by repurposing existing verified systems [47,148] to create tasks for AI.A recent benchmark called Selene explores this direction using the extensive codebases of seL4 [279].</p>
<p>System designs and implementations constantly evolve, and so must their proofs to ensure that the desired properties remain verified.At Level 3, AI systems are expected to go beyond generation to proof and system maintenance.When developers update the system or proof engineers decide to refactor proofs [285], AI at this capability level should provide assistance, reducing the manual efforts needed for verification even further.To effectively evaluate AI tools at this capability level, benchmarks can be constructed by leveraging the change history of verified systems [286].These benchmarks should capture a variety of scenarios, including minor bug fixes, major feature additions, and comprehensive refactoring efforts.The AI must demonstrate proficiency not only in generating proofs at a repository-level context [287] but also in reasoning about code and proof changes [40].</p>
<p>With Levels 1-3, AI systems possess the capabilities to generate and manage proofs, assuming the specifications expressing the properties that the generated artifacts must satisfy are provided.However, writing specifications is a significant challenge for formal verification, as it requires abstracting and converting user requirements into formal specification languages.In capability Level 4, AI systems make another leap by aiding users in deriving formal specifications, including specification generation, explanation, and debugging.To benchmark specification assistance, we can again leverage verified codebases and designs, but instead of generating proofs and code given the specifications, we treat the code and proofs as ground truth and use them to evaluate specifications produced by the model.AI systems need strong natural language understanding to interpret user requirements and translate them into formal specifications.They should also have interactive capabilities to engage with users, offering suggestions and clarifications.Moreover, they should be able to validate specifications against known best practices and standards, ensuring they are robust and comprehensive.</p>
<p>Conclusion and Discussion</p>
<p>In this position paper, we advocated for formal mathematical reasoning as a new frontier in AI.By grounding reasoning in formal systems like Lean, this approach enables the training and evaluation of AI models whose reasoning can be rigorously verified, which holds the potential to significantly advance fields such as mathematics and software verification, as well as applications that require complex and rigorous reasoning.The advent of large language models has created opportunities for formal mathematical reasoning in AI, marking an inflection point for the field.For key tasks such as theorem proving and autoformalization, we discussed recent advancements, future directions, and milestones for measuring progress.AI for formal mathematical reasoning is a nascent area that integrates insights from formal mathematics, programming languages, and machine learning.We hope this position paper can present coherent perspectives that unite previously fragmented efforts across these fields, fostering discussion, community building, and a clear roadmap for the future.</p>
<p>The narrative in this paper is rooted in the approach of AI and machine learning researchers, emphasizing general-purpose learning algorithms applied to well-defined tasks that can be automatically evaluated using benchmarks.While this paradigm has dominated AI research in recent decades, it has limitations.Mathematicians have explored many ways of using AI in their work, including brainstorming ideas and inspirations, writing assistance, and organizing or searching mathematical literature.Many of these use cases, however, resist straightforward evaluation through benchmarks or automated metrics.Even in areas like theorem proving-where automated evaluation is feasibleperformance on benchmarks may not fully capture what human users find meaningful or helpful (Sec.4.3).For example, benchmarks such as LeanDojo [35] and PutnamBench [209] often fail to measure how the prover performs on new and evolving formalization projects, such as formalizing the proof of Fermat's Last Theorem [288].This gap underscores the need for human-centered evaluation approaches that draw on insights from human-computer interaction and cognitive science [289,290].</p>
<p>Figure 1 :
1
Figure 1: State-of-the-art math LLMs such as NuminaMath[49] typically undergo three stages: math pretraining, finetuning on step-by-step solutions, and further finetuning on tool-integrated solutions that interleave natural language reasoning with Python tool invocation.</p>
<p>Figure 3 :
3
Figure 3: Common tasks using AI for formal mathematical reasoning in proof assistants.</p>
<p>Fig. 3
3
Fig.3includes common tasks at the intersection of AI and formal mathematics in proof assistants.Given informal mathematics written by humans (e.g., textbooks and papers), autoformalization automatically translates it into formal theorems and proofs.Given theorem statements, theorem proving aims to generate formal proofs.In addition to the statement, a theorem prover may have access to a large library of existing definitions and lemmas, such as mathlib, and can select useful definitions and lemmas from the library.Furthermore, AI for autoformalization and theorem proving can lead to new theorems and/or proofs that can enrich the library and bootstrap its own capability.</p>
<p>Table 1 :
1
Theorem proving: capability levels and benchmarks for evaluation.
Level CapabilityEvaluation and benchmarks0Checking formal proofsAchieved by modern proof assistants1Assisting humans to develop proofs by suggest-Human-centered evaluationing definitions, lemmas, proof steps, etc.2Human-implemented tactics for automatingDomain-specific benchmarksdomain-specific proof goals3Proving simple theorems automatically in adomain-general fashion</p>
<p>Table 3 :
3
Autoformalization: capability levels and benchmarks for evaluation.
Level CapabilityEvaluation and benchmarks0Representing knowledge in formal systems toAchieved by modern proof assistantssupport manual formalization1Generating autoformalization candidates andInfrastructure to collect and store human feedbackcollecting human feedback2Robust and faithful translations between infor-ProofNet [114], Herald [261]; automatic evalua-mal and formaltion of formal statements by equivalence checking3Inferring missing information and flagging sit-New benchmarks constructed from real-world for-uations when a gap cannot be filledmalization projects4Self-correcting erroneous or inconsistent in-puts by understanding human intentions5Proposing novel definitions that can reduceproof complexity</p>
<p>Table 4 :
4
[274]cturing: capability levels and benchmarks for evaluation.g., different from all those made before), since it is possible to systematically generate new but uninteresting or unimportant conjectures.Interesting conjectures, such as Fermat's last theorem or the Kepler conjecture[274], can often drive the work of mathematicians for centuries, as the pursuit of a proof or disproof unravels.But one fundamental challenge here is exactly in understanding what interesting means in this context, and finding ways to evaluate it.Tackling this challenge would answer questions such as: How do we computationally define interestingness?What is the relationship between interestingness and usefulness?How does interestingness change over time, e.g., as special cases are proved or analogous results in other domains are disproved?
Level CapabilityEvaluation and benchmarks0Generating conjectures useful for proving a target theorem Existing theorem proving benchmarks1Generating conjectures useful for proving theorems in aparticular domain2Understanding interestingness computationally and gener-New benchmarks and evaluationating interesting conjectures in new mathematical domainsmethodology for unknown territories
While proving theorems is a significant part of doing mathematics, a task that necessarily comes before proving a statement is coming up with the statement to prove in the first place.At that stage (before a proof), such a statement is a conjecture, and we hope that AI agents might be able to formulate conjectures by themselves.Conjectures must not only be novel in a formal sense (e.</p>
<p>Some of the most difficult problems in MATH and the AIMO Progress Prize are at the level of AIME (American Invitational Mathematics Examination). OpenAI o1[17] was also evaluated on AIME problems.
OpenAI o1[17] demonstrated impressive math capabilities on AIME. It has likely incorporated additional techniques beyond those in Sec. 2.1, but public information on its inner workings remains limited.
This is largely speculative, as there is limited public information about the internals of o1.
Many definitions that are mathematically equivalent turn out to be very different in how easily formalizable they are in formal systems (e.g., formalizing real numbers as Cauchy sequences turns out to be easier in type theory than as Dedekind cuts, even though mathematically the theory of reals can be developed either way).
Instead of the original IMO problems, AlphaGeometry[5] and AlphaProof[7] used problems manually formalized by human experts, which did not include combinatorics problems.
https://teorth.github.io/pfr
AcknowledgementsWe gratefully acknowledge Jeremy Avigad, Albert Q. Jiang, Zhaoyu Li, Peter O'Hearn, Daniel Selsam, Armando Solar-Lezama, and Terence Tao for providing valuable feedback on an initial version of this paper.
The logic theory machine-a complex information processing system. IRE Transactions on information theory. Allen Newell, Herbert Simon, 1956</p>
<p>. Alfred North, Whitehead , Bertrand Russell, Principia Mathematica. 11927</p>
<p>arXiv:2303.08774OpenAI. GPT-4 technical report. 2023. 1, 4, 9arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, TimothÃ©e Lachaux, Baptiste Lacroix, Naman RoziÃ¨re, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971Edouard Grave, and Guillaume Lample. LLaMa: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Solving olympiad geometry without human demonstrations. Yuhuai Trieu H Trinh, Wu, He Quoc V Le, Thang He, Luong, Nature. 15222024. 1, 2, 12</p>
<p>. Xtx Markets, Aimo Prize, 22</p>
<p>AI achieves silver-medal standard solving international mathematical olympiad problems. 2024. 1, 2, 3, 10, 131522problems-at-silver-medal-level/</p>
<p>MetaMath: Bootstrap your own mathematical questions for large language models. Longhui Yu, Weisen Jiang, Han Shi, Y U Jincheng, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, International Conference on Learning Representations (ICLR). 2024312</p>
<p>MAmmoTH: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, International Conference on Learning Representations (ICLR). 2024</p>
<p>Llemma: An open language model for mathematics. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus Mcaleer, Albert Q Jiang, Jia Deng, Stella Biderman, Sean Welleck, International Conference on Learning Representations (ICLR). 202412</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Yu Li, Daya Wu, Guo, arXiv:2402.03300DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. 202413arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Neural Information Processing Systems (NeurIPS). </p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, International Conference on Learning Representations (ICLR). 2023</p>
<p>ToRA: A tool-integrated reasoning agent for mathematical problem solving. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, International Conference on Learning Representations (ICLR). 2024. 1, 3, 4, 22</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021. 1, 2, 3, 112122arXiv preprint</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track. 2021. 1, 3, 142122</p>
<p>Learning to reason with LLMs. Openai, 315</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, Neural Information Processing Systems (NeurIPS). 20221214</p>
<p>When scaling meets LLM finetuning: The effect of data, model and finetuning method. Biao Zhang, Zhongtao Liu, Colin Cherry, Orhan Firat, International Conference on Learning Representations (ICLR). 20241214</p>
<p>Isabelle/HOL: a proof assistant for higherorder logic. Tobias Nipkow, Markus Wenzel, Lawrence C Paulson, 2002. 2, 5, 20</p>
<p>The Coq proof assistant reference manual: Version 6.1. Bruno Barras, Samuel Boutin, Cristina Cornes, JudicaÃ«l Courant, Jean-Christophe Filliatre, Eduardo Gimenez, Hugo Herbelin, Gerard Huet, Cesar Munoz, Chetan Murthy, Inria. 1997. 2, 5, 20PhD thesis</p>
<p>Dafny: An automatic program verifier for functional correctness. M Rustan, Leino, International Conference on Logic for Programming Artificial Intelligence and Reasoning (LPAR). 2010</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Neural Information Processing Systems (NeurIPS). 2024</p>
<p>The Lean theorem prover (system description). Leonardo De Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, Jakob Von Raumer, International Conference on Automated Deduction (CADE). 2015. 2, 5, 6, 20</p>
<p>The Lean 4 theorem prover and programming language. Leonardo De, Moura , Sebastian Ullrich, International Conference on Automated Deduction (CADE). 2021. 2, 5, 6, 20</p>
<p>MaLeCoP machine learning connection prover. Josef Urban, JiÅ™Ã­ VyskoÄil, Petr Å tÄ›pÃ¡nek, International Conference on Automated Reasoning with Analytic Tableaux and Related Methods. 2011</p>
<p>Reinforcement learning of theorem proving. Cezary Kaliszyk, Josef Urban, Henryk Michalewski, Miroslav OlÅ¡Ã¡k, Neural Information Processing Systems (NeurIPS). 2018</p>
<p>TacticToe: learning to prove with tactics. Thibault Gauthier, Cezary Kaliszyk, Josef Urban, Ramana Kumar, Michael Norrish, Journal of Automated Reasoning. 2021</p>
<p>DeepMath -deep sequence models for premise selection. Geoffrey Irving, Christian Szegedy, Alexander A Alemi, Niklas EÃ©n, FranÃ§ois Chollet, Josef Urban, Neural Information Processing Systems (NeurIPS). 2016</p>
<p>HolStep: A machine learning dataset for higher-order logic theorem proving. Cezary Kaliszyk, FranÃ§ois Chollet, Christian Szegedy, International Conference on Learning Representations (ICLR). 2017</p>
<p>Deep Network Guided Proof Search. Sarah Loos, Geoffrey Irving, Christian Szegedy, Cezary Kaliszyk, International Conference on Logic for Programming, Artificial Intelligence and Reasoning (LPAR). 2017</p>
<p>GamePad: A learning environment for theorem proving. Daniel Huang, Prafulla Dhariwal, Dawn Song, Ilya Sutskever, International Conference on Learning Representations (ICLR). 2019</p>
<p>Learning to prove theorems via interacting with proof assistants. Kaiyu Yang, Jia Deng, International Conference on Machine Learning (ICML). 20191920</p>
<p>Generative language modeling for automated theorem proving. Stanislas Polu, Ilya Sutskever, arXiv:2009.033932020914arXiv preprint</p>
<p>LeanDojo: Theorem proving with retrieval-augmented language models. Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, Anima Anandkumar, Neural Information Processing Systems (NeurIPS). 2023. 2, 9, 10, 13, 14, 15, 17, 192027</p>
<p>An in-context learning agent for formal theorem-proving. Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, Swarat Chaudhuri, Conference on Language Modeling (COLM). 2024. 2, 9, 10, 11, 131726</p>
<p>Autoformalization with large language models. Yuhuai Wu, Albert Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, Christian Szegedy, Neural Information Processing Systems (NeurIPS). 2022. 2, 91319</p>
<p>Multi-language diversity benefits autoformalization. Albert Q Jiang, Wenda Li, Mateja Jamnik, 2024913</p>
<p>Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, Xiaodan Liang, arXiv:2405.14333DeepSeek-Prover: Advancing theorem proving in LLMs through large-scale synthetic data. 2024. 2, 7, 91315arXiv preprint</p>
<p>Baldur: Whole-proof generation and repair with large language models. Emily First, Markus N Rabe, Talia Ringer, Yuriy Brun, ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE). 2023. 2, 7, 9, 111526</p>
<p>Mathlib Community, Completion of the liquid tensor experiment. 2022223</p>
<p>The mathlib community. The Lean mathematical library. Certified Programs and Proofs (CPP). 2020624</p>
<p>Peiyang Song, Kaiyu Yang, Anima Anandkumar, arXiv: Arxiv-2404.12534Towards large language models as copilots for theorem proving in Lean. 2024320arXiv preprint</p>
<p>A survey on deep learning for theorem proving. Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, Xujie Si, Conference on Language Modeling (COLM). 2024</p>
<p>Verified software toolchain: (invited talk). Andrew W Appel, European Symposium on Programming. 2011</p>
<p>Formal verification of an OS kernel. Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick, David Cock, Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski, Michael Norrish, Thomas Sewell, Harvey Tuch, Simon Winwood, Symposium on Operating systems principles (SOSP). 2009426</p>
<p>CompCert-a formally verified optimizing compiler. Xavier Leroy, Sandrine Blazy, Daniel KÃ¤stner, Bernhard Schommer, Markus Pister, Christian Ferdinand, Embedded Real Time Software and Systems (ERTS). 201626</p>
<p>Ironclad Apps: End-to-end security via automated full-system verification. Chris Hawblitzel, Jon Howell, Arjun Jacob R Lorch, Bryan Narayan, Danfeng Parno, Brian Zhang, Zill, Symposium on Operating Systems Design and Implementation. 2014</p>
<p>Yann Fleureau, Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Costa Shengyi, Kashif Huang, Rasul, How NuminaMath won the 1st AIMO Progress Prize. 202434</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Neural Information Processing Systems (NeurIPS). 202214</p>
<p>Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, arXiv:2402.06332InternLM-Math: Open math large language models toward verifiable reasoning. 2024arXiv preprint</p>
<p>OpenWebMath: An open dataset of high-quality mathematical web text. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, Jimmy Ba, International Conference on Learning Representations (ICLR). 2024</p>
<p>MuMath-Code: Combining tool-use large language models with multi-perspective data augmentation for mathematical reasoning. Weihao Shuo Yin, Zhilong You, Guoqiang Ji, Jinfeng Zhong, Bai, arXiv:2405.07551202434arXiv preprint</p>
<p>Code Llama: Open foundation models for code. Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, JÃ©rÃ©my Remez, Rapin, arXiv:2308.129502023arXiv preprint</p>
<p>NuminaMath: The largest public dataset in AI4Maths with 860k pairs of competition math problems and solutions. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Q Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, Stanislas Polu, </p>
<p>SymPy: symbolic computing in Python. Aaron Meurer, Christopher P Smith, Mateusz Paprocki, OndÅ™ej ÄŒertÃ­k, B Sergey, Matthew Kirpichev, Amit Rocklin, Sergiu Kumar, Jason K Ivanov, Sartaj Moore, Singh, PeerJ Computer Science. 4222017</p>
<p>Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily De Oliveira, Santos, arXiv:2411.04872A benchmark for evaluating advanced mathematical reasoning in AI. 2024422arXiv preprint</p>
<p>Titans of mathematics clash over epic proof of ABC conjecture. Erica Klarreich, 2018</p>
<p>LLMs cannot find reasoning errors, but can correct them! In Findings of the. Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, Victor CÈƒrbune, 2024ACL</p>
<p>SelfCheck: Using LLMs to zero-shot check their own step-by-step reasoning. Ning Miao, Yee Whye Teh, Tom Rainforth, International Conference on Learning Representations (ICLR). 2024</p>
<p>Deductive verification of chain-of-thought reasoning. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, Neural Information Processing Systems (NeurIPS). 20241121</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, International Conference on Learning Representations (ICLR). 2024</p>
<p>On the self-verification limitations of large language models on reasoning and planning tasks. Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati, arXiv:2402.081152024arXiv preprint</p>
<p>CRITIC: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, International Conference on Learning Representations (ICLR). 2024</p>
<p>Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin, arXiv:2412.06559ProcessBench: Identifying process errors in mathematical reasoning. essBench: Identifying process errors in mathematical reasoning20242122arXiv preprint</p>
<p>The counterfeit conundrum: Can code language models grasp the nuances of their incorrect generations?. Alex Gu, Wen-Ding Li, Naman Jain, Theo X Olausson, Celine Lee, Koushik Sen, Armando Solar-Lezama, Findings of the Association for Computational Linguistics: ACL. 2024</p>
<p>Tarski Grothendieck set theory. Andrzej Trybulec, Journal of Formalized Mathematics. 51989</p>
<p>Metamath: a computer language for mathematical proofs. Norman Megill, David A Wheeler, 201959</p>
<p>From LCF to HOL: a short history. Mike Gordon, 2000</p>
<p>Metatheory and reflection in theorem proving: A survey and critique. John Harrison, 1995Technical report</p>
<p>A formulation of the simple theory of types. Lawrence C Paulson, International Conference on Computer Logic. 1988for Isabelle</p>
<p>Intuitionistic type theory. Per Martin, -LÃ¶f , Giovanni Sambin, 1984Bibliopolis Naples</p>
<p>The calculus of constructions. T Coquand, GÃ©rard Huet, RR-0530Inria. May 1986Technical Report</p>
<p>Lean4Lean: Towards a formalized metatheory for the Lean theorem prover. Mario Carneiro, arXiv:2403.140642024arXiv preprint</p>
<p>The formulae-as-types notion of construction. To HB Curry: Essays on Combinatory Logic, Lambda Calculus and Formalism. Howard William, 1980</p>
<p>Yehoshua Bar-Hillel, and Azriel Levy. Foundations of set theory. Adolf Abraham, Fraenkel, 1973</p>
<p>Behind Deep Blue: Building the computer that defeated the world chess champion. Feng-Hsiung Hsu, 2002Princeton University Press</p>
<p>Mastering the game of Go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, Nature. 592016</p>
<p>Functional programming in Lean. David Thrane, Christiansen , 2023</p>
<p>W T Gowers, Ben Green, Freddie Manners, Terence Tao, arXiv:2311.05762On a conjecture of Marton. 2023arXiv preprint</p>
<p>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li, Mateja Jamnik, Guillaume Lample, Yuhuai Wu, International Conference on Learning Representations (ICLR). 2023. 7, 91016</p>
<p>Autoformalizing Euclidean geometry. Logan Murphy, Kaiyu Yang, Jialiang Sun, Zhaoyu Li, Anima Anandkumar, Xujie Si, International Conference on Machine Learning (ICML). 2024. 7, 91324</p>
<p>Don't trust: Verify-grounding LLM quantitative reasoning with autoformalization. Jin Peng Zhou, Charles E Staats, Wenda Li, Christian Szegedy, Kilian Q Weinberger, Yuhuai Wu, International Conference on Learning Representations (ICLR). 2024922</p>
<p>Learning the greatest common divisor: explaining Transformer predictions. Francois Charton, International Conference on Learning Representations (ICLR). 2024</p>
<p>FranÃ§ois Charton, arXiv:2112.01898Linear algebra with Transformers. 2021arXiv preprint</p>
<p>Machine learning for modular multiplication. Kristin Lauter, Cathy Yuanchen Li, Krystal Maughan, Rachel Newton, Megha Srivastava, arXiv:2402.192542024arXiv preprint</p>
<p>Attacking lattice cryptography with Transformers. Emily Wenger, Mingjie Chen, Francois Charton, Kristin E Lauter, Salsa, Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Transforming the bootstrap: Using Transformers to compute scattering amplitudes in planar N = 4 super Yang-Mills theory. Tianji Cai, Garrett W Merz, FranÃ§ois Charton, Niklas Nolte, Matthias Wilhelm, Kyle Cranmer, Lance J Dixon, Machine Learning: Science and Technology. 82024</p>
<p>Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic Transformers. Alberto Alfarano, FranÃ§ois Charton, Amaury Hayat, Neural Information Processing Systems (NeurIPS). 2024</p>
<p>Physics informed deep learning (part I): Data-driven solutions of nonlinear partial differential equations. Maziar Raissi, Paris Perdikaris, George Em Karniadakis, arXiv:1711.105612017arXiv preprint</p>
<p>Mathematical discoveries from program search with large language models. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, Nature. 82024</p>
<p>Terence Tao, H Van, Vu, Additive combinatorics. Cambridge University Press2006</p>
<p>Learning to unknot. Sergei Gukov, James Halverson, Fabian Ruehle, Piotr SuÅ‚kowski, Machine Learning: Science and Technology. 82021</p>
<p>Adam Zsolt, Wagner , arXiv:2104.14516Constructions in combinatorics via neural networks. 2021817arXiv preprint</p>
<p>FranÃ§ois Charton, Jordan S Ellenberg, Adam Zsolt Wagner, Geordie Williamson, Patternboost, arXiv:2411.00566Constructions in mathematics with a little help from AI. 2024arXiv preprint</p>
<p>Gergely BÃ©rczi, Adam Zsolt Wagner, arXiv:2411.19734A note on small percolating sets on hypercubes via generative AI. 2024arXiv preprint</p>
<p>Toward mechanical mathematics. Wang Hao, IBM Journal of Research and Development. 81960</p>
<p>Automath, a language for mathematics. Nicolaas Govert de Bruijn. 81983</p>
<p>The QED manifesto. Anonymous, International Conference on Automated Deduction (CADE). 1994</p>
<p>The language of mathematics. Mohan Ganesalingam, 2013Springer</p>
<p>Mizar in a nutshell. Adam Grabowski, Artur Kornilowicz, Adam Naumowicz, Journal of Formalized Reasoning. 322010</p>
<p>The Naproche system. Marcos Daniel KÃ¼hlwein, Peter Cramer, Bernhard Koepke, SchrÃ¶der, International Conference on Intelligent Computer Mathematics (CICM). 2009</p>
<p>ForTheL-the language of formal theories. Konstantin Vershinin, Andrey Paskevich, International Journal of Information Theories and Applications. 82000</p>
<p>MathNat-mathematical text in a controlled natural language. Muhammad Humayoun, Christophe Raffalli, Special issue: Natural Language Processing and its Applications. 2010</p>
<p>Teaching mathematics using lean and controlled natural language. Patrick Massot, 15th International Conference on Interactive Theorem Proving (ITP 2024). 2024Schloss Dagstuhl-Leibniz-Zentrum fÃ¼r Informatik</p>
<p>Grammatical framework. Aarne Ranta, Journal of Functional Programming. 82004</p>
<p>GLIF: A declarative framework for symbolic natural language understanding. Jan , Frederik Schaefer, Michael Kohlhase, Workshop on Formal and Cognitive Reasoning. 2020</p>
<p>GFLean: An autoformalisation framework for lean via GF. Shashank Pathak, arXiv:2404.012342024arXiv preprint</p>
<p>Learning to parse on aligned corpora. Cezary Kaliszyk, Josef Urban, JiÅ™Ã­ VyskoÄil, International Conference on Interactive Theorem Proving (ITP). 2015rough diamond</p>
<p>System description: statistical parsing of informalized Mizar formulas. Cezary Kaliszyk, Josef Urban, JirÃ­ Vyskocil, International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC). 2017</p>
<p>First experiments with neural translation of informal to formal mathematics. Qingxiang Wang, Cezary Kaliszyk, Josef Urban, International Conference on Intelligent Computer Mathematics (CICM). 2018</p>
<p>Exploration of neural machine translation in autoformalization of mathematics in Mizar. Qingxiang Wang, Chad Brown, Cezary Kaliszyk, Josef Urban, Certified Programs and Proofs (CPP). 2020</p>
<p>Unsupervised machine translation using monolingual corpora only. Guillaume Lample, Alexis Conneau, Ludovic Denoyer, Marc'aurelio Ranzato, International Conference on Learning Representations (ICLR). 2018</p>
<p>ProofNet: Autoformalizing and formally proving undergraduate-level mathematics. Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers, Dragomir Radev, Jeremy Avigad, arXiv:2302.1243320232324arXiv preprint</p>
<p>Jianqiao Lu, Yingjia Wan, Zhengying Liu, Yinya Huang, Jing Xiong, Chengwu Liu, Jianhao Shen, Hui Jin, Jipeng Zhang, Haiming Wang, Zhicheng Yang, arXiv:2406.01940Jing Tang, and Zhijiang Guo. Process-driven autoformalization in Lean 4. 2024914arXiv preprint</p>
<p>Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, Nan Tang, arXiv:2406.01265The dawn of natural language to SQL: Are we fully ready?. 2024arXiv preprint</p>
<p>Data-efficient learning of natural language to linear temporal logic translators for robot task specification. Jiayi Pan, Glen Chou, Dmitry Berenson, International Conference on Robotics and Automation (ICRA). </p>
<p>Cook2LTL: Translating cooking recipes to LTL formulae using large language models. Angelos Mavrogiannis, Christoforos Mavrogiannis, Yiannis Aloimonos, International Conference on Robotics and Automation (ICRA). 2024</p>
<p>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, Roger Levy, Conference on Empirical Methods in Natural Language Processing. 20231122</p>
<p>Ãœber formal unentscheidbare sÃ¤tze der Principia Mathematica und verwandter systeme I. Monatshefte fÃ¼r Mathematik und Physik. Kurt GÃ¶del, 1931</p>
<p>Holophrasm: a neural automated theorem prover for higher-order logic. Daniel Whalen, arXiv:1608.026442016arXiv preprint</p>
<p>Empirical evaluation of gated recurrent neural networks on sequence modeling. Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, arXiv:1412.35552014arXiv preprint</p>
<p>Formal mathematics statement curriculum learning. Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, Ilya Sutskever, International Conference on Learning Representations (ICLR). 2023</p>
<p>HyperTree proof search for neural theorem proving. Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, Xavier Martinet, Neural Information Processing Systems (NeurIPS). 20221415</p>
<p>Haohan Lin, Zhiqing Sun, Yiming Yang, Sean Welleck Lean-Star, arXiv:2407.10040Learning to interleave thinking and proving. 20241014arXiv preprint</p>
<p>DreamCoder: growing generalizable, interpretable knowledge with wake-sleep bayesian program learning. Kevin Ellis, Lionel Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lore Anaya Pozo, Luke Hewitt, Armando Solar-Lezama, Joshua B Tenenbaum, Philosophical Transactions of the Royal Society A. 102023</p>
<p>LEGO-Prover: Neural theorem proving with growing libraries. Huajian Xin, Haiming Wang, Chuanyang Zheng, Lin Li, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, International Conference on Learning Representations (ICLR). 20231625</p>
<p>Peano: learning formal mathematical reasoning. Gabriel Poesia, Noah D Goodman, Philosophical Transactions of the Royal Society A. 10162023</p>
<p>Learning search control knowledge for equational deduction. Sa, Schulz, 2000IOS Press230</p>
<p>Overview and evaluation of premise selection techniques for large theory mathematics. Daniel KÃ¼hlwein, Evgeni Twan Van Laarhoven, Josef Tsivtsivadze, Tom Urban, Heskes, Automated Reasoning: 6th International Joint Conference, IJCAR 2012. Manchester, UKSpringerJune 26-29, 2012. 20126</p>
<p>Premise selection for mathematics by corpus analysis and kernel methods. Jesse Alama, Tom Heskes, Daniel KÃ¼hlwein, Evgeni Tsivtsivadze, Josef Urban, Journal of automated reasoning. 52102014</p>
<p>Stateful premise selection by recurrent neural networks. Bartosz Piotrowski, Josef Urban, arXiv:2004.082122020arXiv preprint</p>
<p>MaSh: machine learning for Sledgehammer. Jasmin Christian Daniel KÃ¼hlwein, Cezary Blanchette, Josef Kaliszyk, Urban, International Conference on Interactive Theorem Proving (ITP). 2013</p>
<p>Lightweight relevance filtering for machine-generated resolution problems. Jia Meng, Lawrence C Paulson, Journal of Applied Logic. 2009</p>
<p>Thor: Wielding hammers to integrate language models and automated theorem provers. Albert Qiaochu Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz OdrzygÃ³ÅºdÅº, Piotr MiÅ‚oÅ›, Yuhuai Wu, Mateja Jamnik, Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, International Conference on Learning Representations (ICLR). Bowen Baker, Teddy LeeJan. 20241422</p>
<p>Generating natural language proofs with verifier-guided search. Kaiyu Yang, Jia Deng, Danqi Chen, Conference on Empirical Methods in Natural Language Processing. 20221522</p>
<p>SatLM: Satisfiability-aided language models using declarative prompting. Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett, Neural Information Processing Systems (NeurIPS). 202322</p>
<p>Certified deductive reasoning with language models. Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, Noah D Goodman, Transactions on Machine Learning Research (TMLR). 112024</p>
<p>An axiomatic basis for computer programming. Charles Antony, Richard Hoare, Communications of the ACM. 111969</p>
<p>Formal verification of a realistic compiler. Xavier Leroy, Communications of the ACM. 2009</p>
<p>QED at large: A survey of engineering of formally verified software. Talia Ringer, Karl Palmskog, Ilya Sergey, Milos Gligoric, Zachary Tatlock, Foundations and TrendsÂ® in Programming Languages. 11192019</p>
<p>Formal verification: an essential toolkit for modern VLSI design. Erik Seligman, Tom Schubert, Kiran Achutha, Kumar, 202311</p>
<p>Microprocessor assurance and the role of theorem proving. Shilpi Goel, Sandip Ray, Handbook of Computer Architecture. 202211</p>
<p>ACL2 theorems about commercial microprocessors. Bishop Brock, Matt Kaufmann, Moore Strother, 1996. 11International Conference on Formal Methods in Computer-Aided Design. </p>
<p>Using crash Hoare logic for certifying the FSCQ file system. Haogang Chen, Daniel Ziegler, Tej Chajed, Adam Chlipala, Nickolai Frans Kaashoek, Zeldovich, Symposium on Operating Systems Principles (SOSP). 2015</p>
<p>Verifying the DaisyNFS concurrent and crash-safe file system with sequential reasoning. Tej Chajed, Joseph Tassarotti, Mark Theng, Nickolai Frans Kaashoek, Zeldovich, Symposium on Operating Systems Design and Implementation (OSDI). 2022</p>
<p>CertiKOS: An extensible architecture for building certified concurrent OS kernels. Ronghui Gu, Zhong Shao, Hao Chen, Xiongnan Newman Wu, Jieung Kim, Vilhelm SjÃ¶berg, David Costanzo, Symposium on Operating Systems Design and Implementation. 201626</p>
<p>Implementing TLS with verified cryptographic security. Karthikeyan Bhargavan, CÃ©dric Fournet, Markulf Kohlweiss, Alfredo Pironti, Pierre-Yves Strub, Symposium on Security and Privacy. 2013</p>
<p>Computer-aided security proofs for the working cryptographer. Gilles Barthe, Benjamin GrÃ©goire, Sylvain Heraud, Santiago Zanella BÃ©guelin, Advances in Cryptology (CRYPTO). 2011</p>
<p>Grove: a separation-logic library for verifying distributed systems. Upamanyu Sharma, Ralf Jung, Joseph Tassarotti, Frans Kaashoek, Nickolai Zeldovich, Symposium on Operating Systems Principles (SOSP). 2023</p>
<p>Everest: Towards a verified, drop-in replacement of HTTPS. Karthikeyan Bhargavan, Barry Bond, Antoine Delignat-Lavaud, CÃ©dric Fournet, Chris Hawblitzel, Catalin Hritcu, Samin Ishtiaq, Markulf Kohlweiss, Rustan Leino, Jay R Lorch, In Summit on Advances in Programming Languages. 112017</p>
<p>Automated verification of an in-production DNS authoritative engine. Naiqian Zheng, Mengqi Liu, Yuxing Xiang, Linjian Song, Dong Li, Feng Han, Nan Wang, Yong Ma, Zhuo Liang, Dennis Cai, Symposium on Operating Systems Principles (SOSP). 2023</p>
<p>Proof engineering considered essential. Gerwin Klein, International Symposium on Formal Methods. 2014</p>
<p>Generating correctness proofs with neural networks. Alex Sanchez-Stern, Yousef Alhessi, Lawrence Saul, Sorin Lerner, In SIGPLAN International Workshop on Machine Learning and Programming Languages. 112020</p>
<p>QEDCartographer: Automating formal verification using reward-free reinforcement learning. Alex Sanchez-Stern, Abhishek Varghese, Zhanna Kaufman, Dylan Zhang, Talia Ringer, Yuriy Brun, International Conference on Software Engineering (ICSE). 2025</p>
<p>Proof automation with large language models. Minghai Lu, Benjamin Delaware, Tianyi Zhang, International Conference on Automated Software Engineering (ASE). 2024</p>
<p>Can large language models reason about program invariants. Kexin Pei, David Bieber, Kensen Shi, Charles Sutton, Pengcheng Yin, International Conference on Machine Learning (ICML). 2023</p>
<p>Lemur: Integrating large language models in automated program verification. Haoze Wu, Clark Barrett, Nina Narodytska, International Conference on Learning Representations (ICLR). 2024</p>
<p>DafnyBench: A benchmark for formal software verification. Chloe Loughridge, Qinyi Sun, Seth Ahrenbach, Federico Cassano, Chuyue Sun, Ying Sheng, Anish Mudide, Md Rakib Hossain, Nada Misu, Max Amin, Tegmark, arXiv:2406.08467202426arXiv preprint</p>
<p>Code2Inv: A deep learning framework for program verification. Xujie Si, Aaditya Naik, Hanjun Dai, Mayur Naik, Le Song, International Conference on Computer Aided Verification (CAV). 2020</p>
<p>Laurel: Generating Dafny assertions using large language models. Eric Mugnier, Emmanuel Anaya Gonzalez, Ranjit Jhala, Nadia Polikarpova, Yuanyuan Zhou, arXiv:2405.167922024arXiv preprint</p>
<p>nl2spec: interactively translating unstructured natural language to temporal logics with large language models. Matthias Cosler, Christopher Hahn, Daniel Mendoza, Frederik Schmitt, Caroline Trippel, International Conference on Computer Aided Verification (CAV). 2023</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 20211125arXiv preprint</p>
<p>Asleep at the keyboard? assessing the security of Github Copilot's code contributions. Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri, Symposium on Security and Privacy. 2022</p>
<p>Lost in translation: A study of bugs introduced by large language models while translating code. Rangeet Pan, Reza Ali, Rahul Ibrahimzada, Divya Krishna, Lambert Sankar, Michele Pouguem Wassi, Boris Merler, Raju Sobolev, Saurabh Pavuluri, Reyhaneh Sinha, Jabbarvand, International Conference on Software Engineering (ICSE). 2024</p>
<p>Do users write more insecure code with AI assistants?. Neil Perry, Megha Srivastava, Deepak Kumar, Dan Boneh, Conference on Computer and Communications Security (CCS). 2023</p>
<p>Towards AI-assisted synthesis of verified Dafny methods. Md Rakib, Hossain Misu, Cristina V Lopes, Iris Ma, James Noble, ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE20241219</p>
<p>Verus: Verifying rust programs using linear ghost types. Andrea Lattuada, Travis Hance, Chanhee Cho, Matthias Brun, Isitha Subasinghe, Yi Zhou, Jon Howell, Bryan Parno, Chris Hawblitzel, Proceedings of the ACM on Programming Languages. 122023</p>
<p>Sahil Bhatia, Jie Qiu, Niranjan Hasabnis, Sanjit A Seshia, Alvin Cheung, arXiv:2406.03003Verified code transpilation with LLMs. 202412arXiv preprint</p>
<p>MizAR 60 for Mizar 50. Jan Jakubuv, Karel Chvalovská»³, Zarathustra Goertzel, Cezary Kaliszyk, Mirek OlÅ¡Ã¡k, Bartosz Piotrowski, Stephan Schulz, Martin Suda, Josef Urban, International Conference on Interactive Theorem Proving (ITP). 202312</p>
<p>Learning formal mathematics from intrinsic motivation. Gabriel Poesia, David Broman, Nick Haber, Noah D Goodman, Neural Information Processing Systems (NeurIPS). 20241325</p>
<p>PAL: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning (ICML). 2023</p>
<p>Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, Y K Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang, arXiv:2401.14196DeepSeek-Coder: When the large language model meets programming-the rise of code intelligence. 2024arXiv preprint</p>
<p>The Llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Multi-lingual evaluation of code generation models. Ben Athiwaratkun, Krishna Sanjay, Zijian Gouda, Xiaopeng Wang, Yuchen Li, Ming Tian, Tan, Uddin Wasi, Shiqi Ahmad, Qing Wang, Mingyue Sun, Shang, International Conference on Learning Representations (ICLR). 2023</p>
<p>Knowledge transfer from high-resource to low-resource programming languages for code LLMs. Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Carolyn Jane Anderson, Michael Greenberg, Abhinav Jangda, Arjun Guha, arXiv:2308.098952023arXiv preprint</p>
<p>FVEL: Interactive formal verification environment with large language models via theorem proving. Xiaohan Lin, Qingxing Cao, Yinya Huang, Haiming Wang, Jianqiao Lu, Zhengying Liu, Linqi Song, Xiaodan Liang, Neural Information Processing Systems (NeurIPS). 2024</p>
<p>BLEU: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Annual Meeting of the Association for Computational Linguistics (ACL). 2002</p>
<p>Lean Workbook: A largescale lean problem set formalized from natural language math problems. Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, Kai Chen, arXiv:2406.038472024arXiv preprint</p>
<p>Autoformalize mathematical statements by symbolic equivalence and semantic consistency. Zenan Li, Yifan Wu, Zhaoyu Li, Xinming Wei, Xian Zhang, Fan Yang, Xiaoxing Ma, Neural Information Processing Systems (NeurIPS). 20241324</p>
<p>A promising path towards autoformalization and general artificial intelligence. Christian Szegedy, International Conference on Intelligent Computer Mathematics (CICM). 202014</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, 2017. 14Neural Information Processing Systems. </p>
<p>Physics of language models: Part 3.1, knowledge storage and extraction. Zeyuan Allen, -Zhu , Yuanzhi Li, International Conference on Machine Learning (ICML). </p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim RocktÃ¤schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Conference on Empirical Methods in Natural Language Processing. 201914</p>
<p>Challenging BIG-Bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael SchÃ¤rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, Findings of the Association for Computational Linguistics: ACL. 202314</p>
<p>Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, arXiv:2410.05229Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. GSM-Symbolic: Understanding the limitations of mathematical reasoning in large language models. 20241422arXiv preprint</p>
<p>Grokked Transformers are implicit reasoners: A mechanistic journey to the edge of generalization. Boshi Wang, Xiang Yue, Yu Su, Huan Sun, Neural Information Processing Systems (NeurIPS). </p>
<p>On the paradox of learning to reason from data. Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van Den, Broeck, International Joint Conference on Artificial Intelligence (IJCAI). </p>
<p>On the planning abilities of large language models: a critical investigation. Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati, Neural Information Processing Systems (NeurIPS). </p>
<p>PlanBench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, Neural Information Processing Systems (NeurIPS). 2024</p>
<p>TravelPlanner: A benchmark for real-world planning with language agents. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su, International Conference on Machine Learning (ICML). 20242326</p>
<p>RWKV: Reinventing RNNs for the Transformer era. Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, Findings of the Association for Computational Linguistics: EMNLP. 202314</p>
<p>Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Tri Dao, Conference on Language Modeling (COLM). 2023</p>
<p>Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, Lingpeng Kong, arXiv:2402.07754Diffusion of thoughts: Chain-of-thought reasoning in diffusion language models. 2024arXiv preprint</p>
<p>Learning iterative reasoning through energy diffusion. Yilun Du, Jiayuan Mao, Joshua B Tenenbaum, 2024</p>
<p>Matthew Ho, Vincent Zhu, Xiaoyin Chen, Moksh Jain, Nikolay Malkin, Edwin Zhang, arXiv:2410.13224ProofFlow: Preliminary study on generative flow network language model tuning for formal reasoning. 202414arXiv preprint</p>
<p>INT: An inequality benchmark for evaluating generalization in theorem proving. Yuhuai Wu, Albert Jiang, Jimmy Ba, Roger Baker Grosse, International Conference on Learning Representations (ICLR). 20211418</p>
<p>Exploring length generalization in large language models. Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, Behnam Neyshabur, Neural Information Processing Systems (NeurIPS). </p>
<p>The bitter lesson. Incomplete Ideas (blog). Richard Sutton, 20191518</p>
<p>HOList: An environment for machine learning of higher order logic theorem proving. Kshitij Bansal, Sarah Loos, Markus Rabe, Christian Szegedy, Stewart Wilcox, International Conference on Machine Learning (ICML). 201915</p>
<p>Computer science as empirical inquiry: symbols and search. Allen Newel, Herbert A Simon, Communications of the ACM. 151976</p>
<p>ReST-MCTS*: LLM self-training via process reward guided tree search. Dan Zhang, Sining Zhoubian, Yisong Yue, Yuxiao Dong, Jie Tang, Neural Information Processing Systems (NeurIPS). 20241522</p>
<p>Monte Carlo tree search boosts reasoning via iterative preference learning. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P Lillicrap, Kenji Kawaguchi, Michael Shieh, arXiv:2405.00451202415arXiv preprint</p>
<p>Scaling LLM test-time compute optimally can be more effective than scaling model parameters. Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar, arXiv:2408.03314202415arXiv preprint</p>
<p>An empirical analysis of compute-optimal inference for problem-solving with language models. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, Yiming Yang, arXiv:2408.00724202415arXiv preprint</p>
<p>Graph2Tac: Online representation learning of formal math concepts. Lasse Blaauwbroek, Mirek OlÅ¡Ã¡k, Jason Rute, Fidel Ivan Schaposnik, Jelle Massolo, Vasily Piepenbrock, Pestun, International Conference on Machine Learning (ICML). 202415</p>
<p>MiniF2F: a cross-system benchmark for formal olympiad-level mathematics. Kunhao Zheng, Jesse Michael Han, Stanislas Polu, International Conference on Learning Representations (ICLR). 20221520</p>
<p>PutnamBench: Evaluating neural theorem-provers on the Putnam mathematical competition. George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, Swarat Chaudhuri, Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track. 20241527</p>
<p>Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, Kai Chen, arXiv:2410.15700InternLM2.5-StepProver: Advancing automated theorem proving via expert iteration on large-scale Lean problems. 202415arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Neural Information Processing Systems (NeurIPS). 202315</p>
<p>How can it be feasible to find proofs?. Timothy Gowers, </p>
<p>Proving theorems recursively. Haiming Wang, Huajian Xin, Zhengying Liu, Wenda Li, Yinya Huang, Jianqiao Lu, Zhicheng Yang, Jing Tang, Jian Yin, Zhenguo Li, Xiaodan Liang, Neural Information Processing Systems (NeurIPS). 20241620</p>
<p>Lemma synthesis for automating induction over algebraic data types. Weikun Yang, Grigory Fedyukovich, Aarti Gupta, International Conference on Principles and Practice of Constraint Programming (CP). 201916</p>
<p>Data-driven lemma synthesis for interactive proofs. Aishwarya Sivaraman, Alex Sanchez-Stern, Bretton Chen, Sorin Lerner, Todd Millstein, Proceedings of the ACM on Programming Languages. 162022</p>
<p>Lemma mining over HOL Light. Cezary Kaliszyk, Josef Urban, International Conference on Logic for Programming Artificial Intelligence and Reasoning (LPAR). 2013</p>
<p>REFACTOR: Learning to extract theorems from proofs. Jin Peng Zhou, Yuhuai Wu, Qiyang Li, Roger Baker Grosse, International Conference on Learning Representations (ICLR). 202416</p>
<p>Zhening Li, Gabriel Poesia, Omar Costilla-Reyes, Noah Goodman, Armando Solar-Lezama, arXiv:2211.08671Lemma: Bootstrapping high-level mathematical reasoning with learned symbolic abstractions. 202216arXiv preprint</p>
<p>The probabilistic relevance framework: BM25 and beyond. Foundations and TrendsÂ® in Information Retrieval. Stephen Robertson, Hugo Zaragoza, 200917</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, Conference on Empirical Methods in Natural Language Processing. 202017</p>
<p>Modular elliptic curves and Fermat's Last Theorem. Andrew Wiles, Annals of mathematics. 171995</p>
<p>Proving olympiad algebraic inequalities without human demonstrations. Chenrui Wei, Mengzhou Sun, Wei Wang, Neural Information Processing Systems (NeurIPS). 20241720</p>
<p>Proving olympiad inequalities by synergizing LLMs and symbolic reasoning. Anonymous, International Conference on Learning Representations (ICLR). 2025</p>
<p>AI will become mathematicians' 'co-pilot'. Scientific American. Christoph DrÃ¶sser, June 2024</p>
<p>Renaissance Philanthropy. AI for Math Fund. </p>
<p>The role of working memory in program tracing. Will Crichton, Maneesh Agrawala, Pat Hanrahan, Conference on Human Factors in Computing Systems (CHI). 202118</p>
<p>A grounded conceptual model for ownership types in Rust. Will Crichton, Gavin Gray, Shriram Krishnamurthi, Proceedings of the ACM on Programming Languages. 182023</p>
<p>Proofs and conversations. Talia Ringer, 2024</p>
<p>Patrick Massot, Lean blueprint. 1823</p>
<p>Folding@home: Achievements from over 20 years of citizen science herald the exascale era. Vijay S Vincent A Voelz, Gregory R Pande, Bowman, Biophysical journal. 182023</p>
<p>Guiding LLMs the right way: Fast, non-invasive constrained generation. Luca Beurer-Kellner, Marc Fischer, Martin Vechev, International Conference on Machine Learning (ICML). 202419</p>
<p>Improving LLM code generation with grammar augmentation. Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, Gagandeep Singh, arXiv:2403.01632202419arXiv preprint</p>
<p>Validating AI-generated code with live programming. Kasra Ferdowsi, Ruanqianqian Huang, Michael B James, Nadia Polikarpova, Sorin Lerner, Conference on Human Factors in Computing Systems (CHI). 202419</p>
<p>Clover: Closed-loop verifiable code generation. Chuyue Sun, Ying Sheng, Oded Padon, Clark Barrett, International Symposium on AI Verification. 202419</p>
<p>AutoVerus: Automated proof generation for Rust code. Chenyuan Yang, Xuheng Li, Md Rakib Hossain, Jianan Misu, Weidong Yao, Yeyun Cui, Chris Gong, Shuvendu Hawblitzel, Lahiri, Shuai Jacob R Lorch, Lu, arXiv:2409.130822024arXiv preprint</p>
<p>AlphaVerus: Bootstrapping formally verified code generation through self-improving translation and treefinement. Pranjal Aggarwal, Bryan Parno, Sean Welleck, arXiv:2412.0617620241926arXiv preprint</p>
<p>Taxonomy and definitions for terms related to driving automation systems for on-road motor vehicles. The Society of Automotive Engineers (SAE). </p>
<p>The TPTP problem library. Geoff Sutcliffe, Christian Suttner, Journal of Automated Reasoning. 19201998</p>
<p>FIMO: A challenge formal dataset for automated theorem proving. Chengwu Liu, Jianhao Shen, Huajian Xin, Zhengying Liu, Ye Yuan, Haiming Wang, Wei Ju, Chuanyang Zheng, Yichun Yin, Lin Li, Ming Zhang, Qun Liu, arXiv:2309.0429520231920arXiv preprint</p>
<p>SWE-bench: Can language models resolve real-world GitTub issues. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R Narasimhan, International Conference on Learning Representations (ICLR). 20241921</p>
<p>A brief overview of Agda-a functional language with dependent types. Ana Bove, Peter Dybjer, Ulf Norell, International Conference on Theorem Proving in Higher Order Logics. 200920</p>
<p>Formalizing 100 theorems. Freek Wiedijk, </p>
<p>Moogle: Semantic search over mathlib4. Morph Labs, </p>
<p>Joachim Breitner, Loogle!. 2024</p>
<p>Zihan Qin, and Bin Dong. A semantic search engine for Mathlib4. Guoxiong Gao, Haocheng Ju, Jiedong Jiang, arXiv:2403.13310202420arXiv preprint</p>
<p>Thomas Dohmke, GitHub Copilot X: The AI-powered developer experience. 202320</p>
<p>Sledgehammer: Judgement day. Sascha BÃ¶hme, Tobias Nipkow, International Joint Conference on Automated Reasoning (IJCAR). 201020</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, Annual Meeting of the Association for Computational Linguistics (ACL). 20172122</p>
<p>A closer look at the self-verification abilities of large language models in logical reasoning. Ruixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu, Changshui Zhang, Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). 20242122</p>
<p>AIMO Progress Prize. Xtx Markets, July 2024. 2024</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael SchÃ¤rli, Denny Zhou, International Conference on Machine Learning (ICML). 2023</p>
<p>Measuring faithfulness in chain-ofthought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, arXiv:2307.137022023arXiv preprint</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, Findings of the Association for Computational Linguistics: EMNLP. 202322</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang, Wang , arXiv:2308.03188202322arXiv preprint</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). 201922</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, Conference on Empirical Methods in Natural Language Processing (EMNLP). 201822</p>
<p>Generalization or memorization: Data contamination and trustworthy evaluation for large language models. Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Ge Li, Findings of the Association for Computational Linguistics: ACL. 202422</p>
<p>Array programming with NumPy. Jarrod Charles R Harris, Millman, J StÃ©fan, Ralf Van Der Walt, Pauli Gommers, David Virtanen, Eric Cournapeau, Julian Wieser, Sebastian Taylor, Nathaniel J Berg, Smith, Nature. 222020</p>
<p>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Wang, Findings of the Association for Computational Linguistics: EMNLP. 202322</p>
<p>Swaroop Huaixiu Steven Zheng, Hugh Mishra, Xinyun Zhang, Minmin Chen, Azade Chen, Le Nova, Hou, Heng-Tze, Quoc V Cheng, Ed H Le, Denny Chi, Zhou, arXiv:2406.04520NATURAL PLAN: Benchmarking LLMs on natural language planning. 202423arXiv preprint</p>
<p>Guoxiong Gao, Yutong Wang, Jiedong Jiang, Qi Gao, Zihan Qin, Tianyi Xu, Bin Dong, arXiv:2410.10878Herald: A natural language annotated Lean 4 dataset. 20242324arXiv preprint</p>
<p>Homotopy Type Theory. Peter Aczel, Benedikt Ahrens, Thorsten Altenkirch, Steve Awodey, Bruno Barras, Andrej Bauer, Yves Bertot, Marc Bezem, Thierry Coquand, Eric Finster, 2013Univalent Foundations of Mathematics23</p>
<p>A parallel corpus for natural language machine translation to Isabelle. Anthony Bordg, Yiannos Stathopoulos, Lawrence Paulson, Work-in-progress papers presented at the Conference on Intelligent Computer Mathematics (CICM) Informal Proceedings. 202223</p>
<p>. Thomas Hales, </p>
<p>Rethinking and improving autoformalization: towards a faithful metric and a dependency retrieval-based approach. Anonymous, International Conference on Learning Representations (ICLR). 2024</p>
<p>ICML 2024 Challenges on Automated Math Reasoning. </p>
<p>Mustard: Mastering uniform synthesis of theorem and proof data. Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao, Huajian Xin, Haiming Wang, Zhenguo Li, Linqi Song, Xiaodan Liang, International Conference on Learning Representations (ICLR). 202424</p>
<p>What is a proof?. Alan Bundy, Mateja Jamnik, Andrew Fugard, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 242005</p>
<p>Nitpick: A counterexample generator for higher-order logic based on a relational model finder. Jasmin , Christian Blanchette, Tobias Nipkow, International Conference on Interactive Theorem Proving (ITP). 201024</p>
<p>Machine learning and information theory concepts towards an AI mathematician. Yoshua Bengio, Nikolay Malkin, Bulletin of the American Mathematical Society. 242024</p>
<p>Type classes and filters for mathematical analysis in isabelle/hol. Johannes HÃ¶lzl, Fabian Immler, Brian Huffman, Interactive Theorem Proving: 4th International Conference, ITP 2013. Rennes, FranceSpringerJuly 22-26, 2013. 2013424</p>
<p>The HOL Light theory of euclidean space. John Harrison, Journal of Automated Reasoning. 50242013</p>
<p>Coquelicot: A user-friendly library of real analysis for coq. Sylvie Boldo, Catherine Lelay, Guillaume Melquiond, Mathematics in Computer Science. 9242015</p>
<p>Formal proof. Notices of the AMS. C Thomas, Hales, 200825</p>
<p>First neural conjecturing datasets and experiments. Josef Urban, Jan Jakubuv, International Conference on Intelligent Computer Mathematics (CICM). 202025</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.07732202125arXiv preprint</p>
<p>Verilogeval: Evaluating large language models for Verilog code generation. Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, Haoxing Ren, International Conference on Computer Aided Design (ICCAD). 202325</p>
<p>miniCodeProps: a minimal benchmark for proving code properties. Evan Lohn, Sean Welleck, arXiv:2406.1191520242526arXiv preprint</p>
<p>Selene: Pioneering automated proof in software verification. Lichen Zhang, Shuai Lu, Nan Duan, Annual Meeting of the Association for Computational Linguistics (ACL). 20242526</p>
<p>Infer: An automatic program verifier for memory safety of C programs. Cristiano Calcagno, Dino Distefano, NASA Formal Methods Symposium. 201126</p>
<p>Semantic program alignment for equivalence checking. Berkeley Churchill, Oded Padon, Rahul Sharma, Alex Aiken, Programming Language Design and Implementation (PLDI). 201926</p>
<p>VERT: Verified equivalent Rust transpilation with few-shot learning. Aidan Zh Yang, Yoshiki Takashima, Brandon Paulsen, Josiah Dodds, Daniel Kroening, arXiv:2404.18852202426arXiv preprint</p>
<p>Scalable validation of binary lifters. Sandeep Dasgupta, Sushant Dinesh, Deepan Venkatesh, Christopher W Vikram S Adve, Fletcher, Programming Language Design and Implementation (PLDI). 202026</p>
<p>RepoCoder: Repository-level code completion through iterative retrieval and generation. Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, Weizhu Chen, Conference on Empirical Methods in Natural Language Processing. </p>
<p>Proof Repair. Talia Ringer, 202126University of Washington</p>
<p>Proof repair infrastructure for supervised models: Building a large proof repair dataset. Tom Reichel, Andrew Henderson, Andrew Touchet, Talia Gardner, Ringer, International Conference on Interactive Theorem Proving (ITP). </p>
<p>Jiewen Hu, Thomas Zhu, Sean Welleck, arXiv:2408.03350miniCTX: Neural theorem proving with. 202426long-) contexts. arXiv preprint</p>
<p>Fermat's Last Theorem. Kevin Buzzard, </p>
<p>Mathematical capabilities of ChatGPT. Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Petersen, Julius Berner, Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track. 202427</p>
<p>Evaluating language models for mathematics through interactions. Katherine M Collins, Albert Q Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua B Tenenbaum, William Hart, 2024PNAS27</p>            </div>
        </div>

    </div>
</body>
</html>
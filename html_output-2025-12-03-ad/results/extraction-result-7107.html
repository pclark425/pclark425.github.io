<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7107 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7107</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7107</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-95ce6f77e26b496ffb705a0a3b54f2fb7a6d2452</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/95ce6f77e26b496ffb705a0a3b54f2fb7a6d2452" target="_blank">ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work makes one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via the ChemBERTa model, and suggests that transformers offer a promising avenue of future work for molecular representation learning and property prediction.</p>
                <p><strong>Paper Abstract:</strong> GNNs and chemical fingerprints are the predominant approaches to representing molecules for property prediction. However, in NLP, transformers have become the de-facto standard for representation learning thanks to their strong downstream task transfer. In parallel, the software ecosystem around transformers is maturing rapidly, with libraries like HuggingFace and BertViz enabling streamlined training and introspection. In this work, we make one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. ChemBERTa scales well with pretraining dataset size, offering competitive downstream performance on MoleculeNet and useful attention-based visualization modalities. Our results suggest that transformers offer a promising avenue of future work for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem suitable for large-scale self-supervised pretraining.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7107.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7107.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemBERTa (RoBERTa-based transformer for chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RoBERTa-derived, BERT-style encoder transformer pretrained with masked language modeling on large SMILES corpora to produce molecular representations for downstream property prediction (fine-tuned on MoleculeNet tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemBERTa (RoBERTa implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-only transformer (BERT-style), MLM pretraining; fine-tuned encoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6 layers, 12 attention heads (72 attention mechanisms); exact parameter count not reported</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on curated canonical SMILES from PubChem (dataset curated of 77M SMILES; subsets of 100K, 250K, 1M, and 10M used for experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not used to directly generate novel compounds; trained with masked language modeling (MLM) objective (15% token masking) to learn molecular representations.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (primary); also experimented with SELFIES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecular property prediction (toxicity, blood-brain barrier penetration, on-target inhibition) — evaluated on BBBP, ClinTox (CT_TOX), HIV, Tox21 (SR-p53).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>HuggingFace RoBERTa implementation for model, BertViz for attention visualization, DeepChem for tokenizers/tutorials/finetuning pipelines; Chemprop used as baselines; RDKit used for fingerprint baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>PubChem 77M (pretraining corpus; subsets 100K/250K/1M/10M used); MoleculeNet datasets (BBBP, ClinTox CT_TOX, HIV, Tox21 SR-p53) for finetuning/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROC-AUC (ROC) and Precision-Recall AUC (PRC) reported for classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>For ChemBERTa pretrained on 10M PubChem compounds: BBBP ROC=0.643 PRC=0.620; ClinTox (CT_TOX) ROC=0.733 PRC=0.975; HIV ROC=0.622 PRC=0.119; Tox21 (SR-p53) ROC=0.728 PRC=0.207. Scaling pretraining from 100K to 10M gave mean ΔROC-AUC = +0.110 and ΔPRC-AUC = +0.059 across BBBP, ClinTox, Tox21.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Does not beat state-of-the-art GNN baselines (Chemprop D-MPNN) on evaluated tasks; lower PRC-AUC on some imbalanced tasks (e.g., Tox21); sample efficiency lower than graph models; potential overfitting on smaller pretraining subsets; computational cost/environmental impact (estimated ~17.1 kg CO2 eq for pretraining reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7107.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7107.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELFIES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-referencing Embedded Strings (SELFIES)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 100% robust molecular string representation designed so every SELFIES string maps to a valid molecule, intended to improve robustness of generative models and molecular string modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-referencing embedded strings (selfies): A 100% robust molecular string representation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SELFIES (string representation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>molecular string representation (not a neural model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Used by converting molecules to SELFIES for pretraining ChemBERTa in an experiment described in the paper (pretraining on PubChem subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not used here to generate molecules; used as an input representation for MLM pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SELFIES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Intended to support robust molecular generation and representation learning; here evaluated for downstream property prediction robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used as an alternative input representation for ChemBERTa pretraining pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>PubChem subsets (e.g., PubChem-1M for the experiment comparing SMILES vs SELFIES).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Downstream ROC/PRC-AUC on Tox21 SR-p53 reported for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>No significant difference observed in downstream performance on the Tox21 SR-p53 task between SMILES and SELFIES pretraining (no quantitative improvement reported).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Although SELFIES guarantees syntactic validity of strings, the paper reports no observed downstream advantage in their experiments and notes more benchmarking is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7107.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7107.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2Seq RNN generation (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RNN sequence-to-sequence models for molecular generation / fingerprints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequence-to-sequence recurrent neural network models trained on SMILES have been used in cheminformatics to learn continuous latent representations and to generate focused molecule libraries for lead optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating focused molecule libraries for drug discovery with recurrent neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RNN seq2seq molecular generators (various RNN architectures referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>sequence-to-sequence recurrent neural network (encoder-decoder RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Typically trained on SMILES corpora (e.g., ZINC or other public compound sets) — exact datasets not specified in this paper beyond related-work citations.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct SMILES generation via RNN sequence models, often used for focused library generation and latent-space exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Lead optimization and focused library generation for drug discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Mentioned as a prior approach; the paper does not detail performance or limitations beyond noting these models have been previously used for auxiliary lead optimization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7107.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7107.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Segler RNN library gen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generating focused molecule libraries for drug discovery with recurrent neural networks (Segler et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RNN-based approach used in prior work to generate focused molecular libraries for drug discovery via SMILES sequence generation and reinforcement or likelihood-based sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating focused molecule libraries for drug discovery with recurrent neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RNN-based generative model (Segler et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>sequence-to-sequence / recurrent generative model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on SMILES corpora (as reported in the cited work; the current paper only references the work).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct SMILES generation for focused library creation (cited prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Focused library generation for drug discovery; lead optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Referenced as existing literature; the current paper does not report limitations or quantitative outcomes from that study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7107.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7107.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gómez-Bombarelli VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic chemical design using a data-driven continuous representation of molecules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variational autoencoder approach that learns continuous latent representations of molecules enabling property optimization and de novo design via latent-space search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic chemical design using a data-driven continuous representation of molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Grammar VAE / VAE molecular generator (Gómez-Bombarelli et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>variational autoencoder (encoder-decoder generative model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on SMILES datasets (as per the cited work; not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Latent-space sampling and optimization followed by decoder-to-SMILES generation.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (continuous latent representation learned)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Automatic chemical design and property optimization (de novo molecule generation).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Mentioned in related work; current paper does not provide further experimental or limitation details.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7107.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7107.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Molecular Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecular Transformer (transformer for reaction prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based model applied in chemistry for reaction prediction and uncertainty-calibrated predictions of reaction outcomes using SMILES as input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Molecular Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder transformer for sequence-to-sequence reaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on reaction SMILES corpora (citation in related work; details not given in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Sequence-to-sequence prediction of reaction products given reactants/reagents (not direct de novo molecule generation but applicable to synthesis prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (reaction SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Reaction outcome prediction, uncertainty-calibrated reaction forecasting (useful in synthesis planning).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Referenced as an application of transformers to chemistry (reaction prediction); not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7107.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7107.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Smiles Transformer (pre-trained molecular fingerprint model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer pretrained on SMILES to produce molecular fingerprints for low-data drug discovery tasks; cited as related work in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SMILES Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based pretrained encoder (BERT-like objective likely)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on SMILES datasets (the cited work used ChEMBL/ZINC-scale corpora; exacts not provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not primarily a generator; used to produce fingerprints/representations for property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Low-data drug discovery / molecular property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Referenced as prior transformer work for molecular representation; detailed performance not recapitulated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7107.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7107.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Smiles-bert: large scale unsupervised pre-training for molecular property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-style pretraining approach on SMILES noted in prior literature for molecular property prediction; cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Smiles-bert: large scale unsupervised pre-training for molecular property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SMILES-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>BERT-style encoder pretrained on SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on large SMILES corpora (cited work); specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not used for de novo generation in the cited context; used for representation learning for property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecular property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Cited as prior large-scale unsupervised pretraining work; not analyzed in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7107.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7107.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Byte-Pair Encoding (BPE) tokenizer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Byte-Pair Encoding tokenizer (BPE) from HuggingFace tokenizers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A subword tokenization strategy (hybrid character/word-level) used as the default tokenizer for ChemBERTa pretraining; compared to a chemistry-specific SmilesTokenizer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Byte-Pair Encoding (BPE) tokenizer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>tokenizer (BPE subword segmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>max vocab size set to 52K tokens in pretraining runs</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Learned/used on SMILES corpora during ChemBERTa pretraining (PubChem subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Tokenizes SMILES strings into subword tokens</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Input tokenization for transformer pretraining and finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>HuggingFace tokenizers library</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>PubChem subsets used during pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Downstream ΔPRC-AUC observed when compared to SmilesTokenizer (SmilesTokenizer outperformed BPE by ΔPRC-AUC = +0.015 on Tox21 SR-p53 in a 1M-pretrained comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>SmilesTokenizer narrowly outperformed BPE by ΔPRC-AUC = +0.015 on the Tox21 SR-p53 downstream task (experiment on PubChem-1M pretrained models).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>BPE is general-purpose; chemically-aware tokenization (SmilesTokenizer) showed marginal improvements in this paper, suggesting tokenization choice affects downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7107.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7107.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SmilesTokenizer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SmilesTokenizer (regex-based tokenizer from DeepChem)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemistry-aware tokenizer based on a SMILES regex from prior work, released as part of DeepChem and compared against BPE; used to tokenize SMILES for ChemBERTa pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SmilesTokenizer (regex-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>tokenizer (chemistry-specific tokenization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Applied to SMILES from PubChem subsets during ChemBERTa pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (tokenized according to chemistry-aware regex units)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Input tokenization for transformer pretraining / molecular representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>DeepChem tokenizers API</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>PubChem-1M experiment used for tokenizer comparison</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>PRC-AUC on Tox21 SR-p53 downstream task (used to compare tokenizers).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>SmilesTokenizer narrowly outperformed BPE by ΔPRC-AUC = +0.015 on Tox21 SR-p53 (experiment with PubChem-1M pretrained models).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Paper notes further benchmarking needed to validate tokenization benefits across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7107.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7107.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PubChem 77M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PubChem 77M curated SMILES dataset (released by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated set of 77 million canonicalized SMILES from PubChem prepared by the authors for large-scale self-supervised pretraining of ChemBERTa (subsets used for experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PubChem 77M curated SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>dataset (SMILES corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>77 million unique SMILES (dataset); subsets of 100K, 250K, 1M, 10M used experimentally</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Canonicalized and globally shuffled SMILES extracted from PubChem; used for MLM pretraining of ChemBERTa.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Pretraining corpus for molecular representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Preprocessing used RDKit for canonicalization (implied); hosted/processed by Reverie Labs as stated.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Pretraining on the largest subset (10M) took ~48 hours on a single NVIDIA V100 GPU; scaling pretraining size correlated with improved downstream AUC metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Full 77M pretraining left to future work due to engineering/resource constraints; observed overfitting when training for 10 epochs on the 10M subset, leading to 3-epoch run on that subset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generating focused molecule libraries for drug discovery with recurrent neural networks. <em>(Rating: 2)</em></li>
                <li>Automatic chemical design using a data-driven continuous representation of molecules. <em>(Rating: 2)</em></li>
                <li>Grammar variational autoencoder. <em>(Rating: 2)</em></li>
                <li>Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction. <em>(Rating: 2)</em></li>
                <li>Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery. <em>(Rating: 1)</em></li>
                <li>Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery. <em>(Rating: 1)</em></li>
                <li>Smiles-bert: large scale unsupervised pre-training for molecular property prediction. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7107",
    "paper_id": "paper-95ce6f77e26b496ffb705a0a3b54f2fb7a6d2452",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "ChemBERTa",
            "name_full": "ChemBERTa (RoBERTa-based transformer for chemistry)",
            "brief_description": "A RoBERTa-derived, BERT-style encoder transformer pretrained with masked language modeling on large SMILES corpora to produce molecular representations for downstream property prediction (fine-tuned on MoleculeNet tasks).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChemBERTa (RoBERTa implementation)",
            "model_type": "encoder-only transformer (BERT-style), MLM pretraining; fine-tuned encoder",
            "model_size": "6 layers, 12 attention heads (72 attention mechanisms); exact parameter count not reported",
            "training_data_description": "Pretrained on curated canonical SMILES from PubChem (dataset curated of 77M SMILES; subsets of 100K, 250K, 1M, and 10M used for experiments).",
            "generation_method": "Not used to directly generate novel compounds; trained with masked language modeling (MLM) objective (15% token masking) to learn molecular representations.",
            "chemical_representation": "SMILES (primary); also experimented with SELFIES",
            "target_application": "Molecular property prediction (toxicity, blood-brain barrier penetration, on-target inhibition) — evaluated on BBBP, ClinTox (CT_TOX), HIV, Tox21 (SR-p53).",
            "constraints_used": null,
            "integration_with_external_tools": "HuggingFace RoBERTa implementation for model, BertViz for attention visualization, DeepChem for tokenizers/tutorials/finetuning pipelines; Chemprop used as baselines; RDKit used for fingerprint baselines.",
            "dataset_used": "PubChem 77M (pretraining corpus; subsets 100K/250K/1M/10M used); MoleculeNet datasets (BBBP, ClinTox CT_TOX, HIV, Tox21 SR-p53) for finetuning/evaluation.",
            "evaluation_metrics": "ROC-AUC (ROC) and Precision-Recall AUC (PRC) reported for classification tasks.",
            "reported_results": "For ChemBERTa pretrained on 10M PubChem compounds: BBBP ROC=0.643 PRC=0.620; ClinTox (CT_TOX) ROC=0.733 PRC=0.975; HIV ROC=0.622 PRC=0.119; Tox21 (SR-p53) ROC=0.728 PRC=0.207. Scaling pretraining from 100K to 10M gave mean ΔROC-AUC = +0.110 and ΔPRC-AUC = +0.059 across BBBP, ClinTox, Tox21.",
            "experimental_validation": false,
            "challenges_or_limitations": "Does not beat state-of-the-art GNN baselines (Chemprop D-MPNN) on evaluated tasks; lower PRC-AUC on some imbalanced tasks (e.g., Tox21); sample efficiency lower than graph models; potential overfitting on smaller pretraining subsets; computational cost/environmental impact (estimated ~17.1 kg CO2 eq for pretraining reported).",
            "uuid": "e7107.0",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "SELFIES",
            "name_full": "SELF-referencing Embedded Strings (SELFIES)",
            "brief_description": "A 100% robust molecular string representation designed so every SELFIES string maps to a valid molecule, intended to improve robustness of generative models and molecular string modeling.",
            "citation_title": "Self-referencing embedded strings (selfies): A 100% robust molecular string representation.",
            "mention_or_use": "use",
            "model_name": "SELFIES (string representation)",
            "model_type": "molecular string representation (not a neural model)",
            "model_size": null,
            "training_data_description": "Used by converting molecules to SELFIES for pretraining ChemBERTa in an experiment described in the paper (pretraining on PubChem subsets).",
            "generation_method": "Not used here to generate molecules; used as an input representation for MLM pretraining.",
            "chemical_representation": "SELFIES",
            "target_application": "Intended to support robust molecular generation and representation learning; here evaluated for downstream property prediction robustness.",
            "constraints_used": null,
            "integration_with_external_tools": "Used as an alternative input representation for ChemBERTa pretraining pipelines.",
            "dataset_used": "PubChem subsets (e.g., PubChem-1M for the experiment comparing SMILES vs SELFIES).",
            "evaluation_metrics": "Downstream ROC/PRC-AUC on Tox21 SR-p53 reported for comparison.",
            "reported_results": "No significant difference observed in downstream performance on the Tox21 SR-p53 task between SMILES and SELFIES pretraining (no quantitative improvement reported).",
            "experimental_validation": false,
            "challenges_or_limitations": "Although SELFIES guarantees syntactic validity of strings, the paper reports no observed downstream advantage in their experiments and notes more benchmarking is needed.",
            "uuid": "e7107.1",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Seq2Seq RNN generation (general)",
            "name_full": "RNN sequence-to-sequence models for molecular generation / fingerprints",
            "brief_description": "Sequence-to-sequence recurrent neural network models trained on SMILES have been used in cheminformatics to learn continuous latent representations and to generate focused molecule libraries for lead optimization.",
            "citation_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks.",
            "mention_or_use": "mention",
            "model_name": "RNN seq2seq molecular generators (various RNN architectures referenced)",
            "model_type": "sequence-to-sequence recurrent neural network (encoder-decoder RNN)",
            "model_size": null,
            "training_data_description": "Typically trained on SMILES corpora (e.g., ZINC or other public compound sets) — exact datasets not specified in this paper beyond related-work citations.",
            "generation_method": "Direct SMILES generation via RNN sequence models, often used for focused library generation and latent-space exploration.",
            "chemical_representation": "SMILES",
            "target_application": "Lead optimization and focused library generation for drug discovery.",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Mentioned as a prior approach; the paper does not detail performance or limitations beyond noting these models have been previously used for auxiliary lead optimization tasks.",
            "uuid": "e7107.2",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Segler RNN library gen",
            "name_full": "Generating focused molecule libraries for drug discovery with recurrent neural networks (Segler et al.)",
            "brief_description": "An RNN-based approach used in prior work to generate focused molecular libraries for drug discovery via SMILES sequence generation and reinforcement or likelihood-based sampling.",
            "citation_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks.",
            "mention_or_use": "mention",
            "model_name": "RNN-based generative model (Segler et al.)",
            "model_type": "sequence-to-sequence / recurrent generative model",
            "model_size": null,
            "training_data_description": "Trained on SMILES corpora (as reported in the cited work; the current paper only references the work).",
            "generation_method": "Direct SMILES generation for focused library creation (cited prior work).",
            "chemical_representation": "SMILES",
            "target_application": "Focused library generation for drug discovery; lead optimization.",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Referenced as existing literature; the current paper does not report limitations or quantitative outcomes from that study.",
            "uuid": "e7107.3",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Gómez-Bombarelli VAE",
            "name_full": "Automatic chemical design using a data-driven continuous representation of molecules",
            "brief_description": "A variational autoencoder approach that learns continuous latent representations of molecules enabling property optimization and de novo design via latent-space search.",
            "citation_title": "Automatic chemical design using a data-driven continuous representation of molecules.",
            "mention_or_use": "mention",
            "model_name": "Grammar VAE / VAE molecular generator (Gómez-Bombarelli et al.)",
            "model_type": "variational autoencoder (encoder-decoder generative model)",
            "model_size": null,
            "training_data_description": "Trained on SMILES datasets (as per the cited work; not detailed in this paper).",
            "generation_method": "Latent-space sampling and optimization followed by decoder-to-SMILES generation.",
            "chemical_representation": "SMILES (continuous latent representation learned)",
            "target_application": "Automatic chemical design and property optimization (de novo molecule generation).",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Mentioned in related work; current paper does not provide further experimental or limitation details.",
            "uuid": "e7107.4",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Molecular Transformer",
            "name_full": "Molecular Transformer (transformer for reaction prediction)",
            "brief_description": "A transformer-based model applied in chemistry for reaction prediction and uncertainty-calibrated predictions of reaction outcomes using SMILES as input.",
            "citation_title": "Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction.",
            "mention_or_use": "mention",
            "model_name": "Molecular Transformer",
            "model_type": "encoder-decoder transformer for sequence-to-sequence reaction prediction",
            "model_size": null,
            "training_data_description": "Trained on reaction SMILES corpora (citation in related work; details not given in this paper).",
            "generation_method": "Sequence-to-sequence prediction of reaction products given reactants/reagents (not direct de novo molecule generation but applicable to synthesis prediction).",
            "chemical_representation": "SMILES (reaction SMILES)",
            "target_application": "Reaction outcome prediction, uncertainty-calibrated reaction forecasting (useful in synthesis planning).",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Referenced as an application of transformers to chemistry (reaction prediction); not evaluated in this paper.",
            "uuid": "e7107.5",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "SMILES Transformer",
            "name_full": "Smiles Transformer (pre-trained molecular fingerprint model)",
            "brief_description": "A transformer pretrained on SMILES to produce molecular fingerprints for low-data drug discovery tasks; cited as related work in the paper.",
            "citation_title": "Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery.",
            "mention_or_use": "mention",
            "model_name": "SMILES Transformer",
            "model_type": "transformer-based pretrained encoder (BERT-like objective likely)",
            "model_size": null,
            "training_data_description": "Pretrained on SMILES datasets (the cited work used ChEMBL/ZINC-scale corpora; exacts not provided here).",
            "generation_method": "Not primarily a generator; used to produce fingerprints/representations for property prediction.",
            "chemical_representation": "SMILES",
            "target_application": "Low-data drug discovery / molecular property prediction",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Referenced as prior transformer work for molecular representation; detailed performance not recapitulated in this paper.",
            "uuid": "e7107.6",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "SMILES-BERT",
            "name_full": "Smiles-bert: large scale unsupervised pre-training for molecular property prediction",
            "brief_description": "A BERT-style pretraining approach on SMILES noted in prior literature for molecular property prediction; cited as related work.",
            "citation_title": "Smiles-bert: large scale unsupervised pre-training for molecular property prediction.",
            "mention_or_use": "mention",
            "model_name": "SMILES-BERT",
            "model_type": "BERT-style encoder pretrained on SMILES",
            "model_size": null,
            "training_data_description": "Pretrained on large SMILES corpora (cited work); specifics not provided in this paper.",
            "generation_method": "Not used for de novo generation in the cited context; used for representation learning for property prediction.",
            "chemical_representation": "SMILES",
            "target_application": "Molecular property prediction",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Cited as prior large-scale unsupervised pretraining work; not analyzed in detail here.",
            "uuid": "e7107.7",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Byte-Pair Encoding (BPE) tokenizer",
            "name_full": "Byte-Pair Encoding tokenizer (BPE) from HuggingFace tokenizers",
            "brief_description": "A subword tokenization strategy (hybrid character/word-level) used as the default tokenizer for ChemBERTa pretraining; compared to a chemistry-specific SmilesTokenizer.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Byte-Pair Encoding (BPE) tokenizer",
            "model_type": "tokenizer (BPE subword segmentation)",
            "model_size": "max vocab size set to 52K tokens in pretraining runs",
            "training_data_description": "Learned/used on SMILES corpora during ChemBERTa pretraining (PubChem subsets).",
            "generation_method": null,
            "chemical_representation": "Tokenizes SMILES strings into subword tokens",
            "target_application": "Input tokenization for transformer pretraining and finetuning",
            "constraints_used": null,
            "integration_with_external_tools": "HuggingFace tokenizers library",
            "dataset_used": "PubChem subsets used during pretraining",
            "evaluation_metrics": "Downstream ΔPRC-AUC observed when compared to SmilesTokenizer (SmilesTokenizer outperformed BPE by ΔPRC-AUC = +0.015 on Tox21 SR-p53 in a 1M-pretrained comparison).",
            "reported_results": "SmilesTokenizer narrowly outperformed BPE by ΔPRC-AUC = +0.015 on the Tox21 SR-p53 downstream task (experiment on PubChem-1M pretrained models).",
            "experimental_validation": false,
            "challenges_or_limitations": "BPE is general-purpose; chemically-aware tokenization (SmilesTokenizer) showed marginal improvements in this paper, suggesting tokenization choice affects downstream performance.",
            "uuid": "e7107.8",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "SmilesTokenizer",
            "name_full": "SmilesTokenizer (regex-based tokenizer from DeepChem)",
            "brief_description": "A chemistry-aware tokenizer based on a SMILES regex from prior work, released as part of DeepChem and compared against BPE; used to tokenize SMILES for ChemBERTa pretraining.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SmilesTokenizer (regex-based)",
            "model_type": "tokenizer (chemistry-specific tokenization)",
            "model_size": null,
            "training_data_description": "Applied to SMILES from PubChem subsets during ChemBERTa pretraining.",
            "generation_method": null,
            "chemical_representation": "SMILES (tokenized according to chemistry-aware regex units)",
            "target_application": "Input tokenization for transformer pretraining / molecular representation learning",
            "constraints_used": null,
            "integration_with_external_tools": "DeepChem tokenizers API",
            "dataset_used": "PubChem-1M experiment used for tokenizer comparison",
            "evaluation_metrics": "PRC-AUC on Tox21 SR-p53 downstream task (used to compare tokenizers).",
            "reported_results": "SmilesTokenizer narrowly outperformed BPE by ΔPRC-AUC = +0.015 on Tox21 SR-p53 (experiment with PubChem-1M pretrained models).",
            "experimental_validation": false,
            "challenges_or_limitations": "Paper notes further benchmarking needed to validate tokenization benefits across tasks.",
            "uuid": "e7107.9",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "PubChem 77M",
            "name_full": "PubChem 77M curated SMILES dataset (released by authors)",
            "brief_description": "A curated set of 77 million canonicalized SMILES from PubChem prepared by the authors for large-scale self-supervised pretraining of ChemBERTa (subsets used for experiments).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PubChem 77M curated SMILES",
            "model_type": "dataset (SMILES corpus)",
            "model_size": "77 million unique SMILES (dataset); subsets of 100K, 250K, 1M, 10M used experimentally",
            "training_data_description": "Canonicalized and globally shuffled SMILES extracted from PubChem; used for MLM pretraining of ChemBERTa.",
            "generation_method": null,
            "chemical_representation": "SMILES",
            "target_application": "Pretraining corpus for molecular representation learning",
            "constraints_used": null,
            "integration_with_external_tools": "Preprocessing used RDKit for canonicalization (implied); hosted/processed by Reverie Labs as stated.",
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": "Pretraining on the largest subset (10M) took ~48 hours on a single NVIDIA V100 GPU; scaling pretraining size correlated with improved downstream AUC metrics.",
            "experimental_validation": false,
            "challenges_or_limitations": "Full 77M pretraining left to future work due to engineering/resource constraints; observed overfitting when training for 10 epochs on the 10M subset, leading to 3-epoch run on that subset.",
            "uuid": "e7107.10",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks.",
            "rating": 2
        },
        {
            "paper_title": "Automatic chemical design using a data-driven continuous representation of molecules.",
            "rating": 2
        },
        {
            "paper_title": "Grammar variational autoencoder.",
            "rating": 2
        },
        {
            "paper_title": "Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction.",
            "rating": 2
        },
        {
            "paper_title": "Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery.",
            "rating": 1
        },
        {
            "paper_title": "Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery.",
            "rating": 1
        },
        {
            "paper_title": "Smiles-bert: large scale unsupervised pre-training for molecular property prediction.",
            "rating": 1
        }
    ],
    "cost": 0.01653875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction</h1>
<p>Seyone Chithrananda<br>University of Toronto<br>seyone.chithrananda@utoronto.ca<br>Gabriel Grand<br>Reverie Labs<br>gabe@reverielabs.com<br>Bharath Ramsundar<br>DeepChem<br>bharath.ramsundar@gmail.com</p>
<h4>Abstract</h4>
<p>GNNs and chemical fingerprints are the predominant approaches to representing molecules for property prediction. However, in NLP, transformers have become the de-facto standard for representation learning thanks to their strong downstream task transfer. In parallel, the software ecosystem around transformers is maturing rapidly, with libraries like HuggingFace and BertViz enabling streamlined training and introspection. In this work, we make one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. While not at state-of-the-art, ChemBERTa scales well with pretraining dataset size, offering competitive downstream performance on MoleculeNet and useful attention-based visualization modalities. Our results suggest that transformers offer a promising avenue of future work for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem suitable for large-scale self-supervised pretraining.</p>
<h2>1 Motivation</h2>
<p>Molecular property prediction has seen a recent resurgence thanks to the success of graph neural networks (GNNs) on various benchmark tasks [1, 2, 3, 4, 5, 6]. However, data scarcity remains a fundamental challenge for supervised learning in a domain in which each new labelled data point requires costly and time-consuming laboratory testing. Determining effective methods to make use of large amounts of unlabeled structure data remains an important unsolved challenge.
Over the past two years, the transformer [7, 8] has emerged as a robust architecture for learning self-supervised representations of text. Transformer pretraining plus task-specific finetuning provides substantial gains over previous approaches to many tasks in natural language processing (NLP) [9, 10, 11]. Meanwhile, software infrastructure for transformers is maturing rapidly: HuggingFace [12] provides streamlined pretraining and finetuning pipelines, while packages like BertViz [13] offer sophisticated interfaces for attention visualization. Given the availability of millions of SMILES strings, transformers offer an interesting alternative to both expert-crafted and GNN-learned fingerprints. In particular, the masked language-modeling (MLM) pretraining task [8] commonly used for BERT-style architectures is analogous to atom masking tasks used in graph settings [14]. Moreover, since modern transformers are engineered to scale to massive NLP corpora, they offer practical advantages over GNNs in terms of efficiency and throughput.
Though simple in concept, the application of transformers to molecular data presents several questions that are severely underexplored. For instance: How does pretraining dataset size affect downstream task performance? What tokenization strategies work best for SMILES? Does replacing SMILES</p>
<p>with a more robust string representation like SELFIES [15] improve performance? We aim to address these questions via one of the first systematic evaluations of transformers on molecular property prediction tasks.</p>
<h1>2 Related Work</h1>
<p>In cheminformatics, there is a long tradition of training language models directly on SMILES to learn continuous latent representations [16, 17, 18]. Typically, these are RNN sequence-to-sequence models and their goal is to facilitate auxiliary lead optimization tasks; e.g., focused library generation [19]. Thus far, discussion of the transformer architecture in chemistry has been largely focused on a particular application to reaction prediction [20].
Some recent work has pretrained transformers for molecular property prediction and reported promising results [21, 22]. However, the datasets used for pretraining have been relatively small ( 861 K compounds from ChEMBL and 2M compounds from ZINC, respectively). Other work has used larger pretraining datasets ( 18.7 M compounds from ZINC) [23] but the effects of pretraining dataset size, tokenizer, and string representation were not explored. In still other work, transformers were used for supervised learning directly without pretraining [24].
Recently, a systematic study of self-supervised pretraining strategies for GNNs helped to clarify the landscape of those methods [14]. Our goal is to undertake a similar investigation for transformers to assess the viability of this architecture for property prediction.</p>
<h2>3 Methods</h2>
<p>ChemBERTa is based on the RoBERTa [25] transformer implementation in HuggingFace [12]. Our implementation of RoBERTa uses 12 attention heads and 6 layers, resulting in 72 distinct attention mechanisms. So far, we have released 15 pre-trained ChemBERTa models on the Huggingface's model hub; these models have collectively received over 30,000 Inference API calls to date. ${ }^{1}$
We used the popular Chemprop library for all baselines [6]. We trained the directed Message Passing Neural Network (D-MPNN) with default hyperparameters as well as the sklearn-based [26] Random Forest (RF) and Support Vector Machine (SVM) models from Chemprop, which use 2048-bit Morgan fingerprints from RDKit [27, 28].</p>
<h3>3.1 PreTraining on PubChem 77M</h3>
<p>We adopted our pretraining procedure from RoBERTa, which masks $15 \%$ of the tokens in each input string. We used a max. vocab size of 52 K tokens and max. sequence length of 512 tokens. We trained for 10 epochs on all PubChem subsets except for the 10M subset, on which we trained for 3 epochs to avoid observed overfitting. Our hypothesis is that, in learning to recover masked tokens, the model forms a representational topology of chemical space that should generalize to property prediction tasks.</p>
<p>For pretraining, we curated a dataset of 77M unique SMILES from PubChem [29], the world's largest open-source collection of chemical structures. The SMILES were canonicalized and globally shuffled to facilitate large-scale pretraining. We divided this dataset into subsets of $100 \mathrm{~K}, 250 \mathrm{~K}, 1 \mathrm{M}$, and 10M. Pretraining on the largest subset took approx. 48 hours on a single NVIDIA V100 GPU. We make this dataset publicly available and leave pretraining on the full 77 M set to future work.</p>
<h3>3.2 Finetuning on MoleculeNet</h3>
<p>We evaluated our models on several classification tasks from MoleculeNet [30] selected to cover a range of dataset sizes ( $1.5 \mathrm{~K}-41.1 \mathrm{~K}$ examples) and medicinal chemistry applications (brain penetrability, toxicity, and on-target inhibition). These included the BBBP, ClinTox, HIV, and Tox21 datasets. For datasets with multiple tasks, we selected a single representative task: the clinical toxicity (CT_TOX) task from ClinTox and the p53 stress-response pathway activation (SR-p53) task from</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>| | BBBP | | ClinTox (CT_TOX) | | HIV | | Tox21 (SR-p53) | |
| | 2,039 | | 1,478 | | 41,127 | | 7,831 | |
| | ROC | PRC | ROC | PRC | ROC | PRC | ROC | PRC |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ChemBERTa 10M | 0.643 | 0.620 | 0.733 | 0.975 | 0.622 | 0.119 | 0.728 | 0.207 |
| D-MPNN | 0.708 | 0.697 | 0.906 | 0.993 | 0.752 | 0.152 | 0.688 | 0.429 |
| RF | 0.681 | 0.692 | 0.693 | 0.968 | 0.780 | 0.383 | 0.724 | 0.335 |
| SVM | 0.702 | 0.724 | 0.833 | 0.986 | 0.763 | 0.364 | 0.708 | 0.345 |</p>
<p>Table 1: Comparison of ChemBERTa pretrained on 10M PubChem compounds and Chemprop baselines on selected MoleculeNet tasks. We report both ROC-AUC and PRC-AUC to give a full picture of performance on class-imbalanced tasks.</p>
<p>Tox21. For each dataset, we generated an 80/10/10 train/valid/test split using the scaffold splitter from DeepChem [31]. During finetuning, we appended a linear classification layer and backpropagated through the base model. We finetuned models for up to 25 epochs with early stopping on ROCAUC. We release a tutorial in DeepChem which allows users to go through loading a pre-trained ChemBERTa model, running masked prediction tasks, visualizing the attention of the model on several molecules, and fine-tuning the model on the Tox21 SR-p53 dataset.</p>
<h2>4 Results</h2>
<p>On the MoleculeNet tasks that we evaluated, ChemBERTa approaches, but does not beat, the strong baselines from Chemprop (Table 1). Nevertheless, downstream performance of ChemBERTa scales well with more pretraining data (Fig. 1). On average, scaling from 100K to 10M resulted in $\Delta$ ROC-AUC $=+0.110$ and $\Delta$ PRC-AUC $=+0.059$. (HIV was omitted from this analysis due to resource constraints.) These results suggest that ChemBERTa learns more robust representations with additional data and is able to leverage this information when learning downstream tasks.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Scaling the pretraining size (100K, 250K, 1M, 10M) produces consistent improvements in downstream task performance on BBBP, ClinTox, and Tox21. Mean $\Delta$ AUC across all three tasks with a 68% confidence interval is shown in light blue.</p>
<p>While Tox21 ROC-AUC is better than the baselines, PR-AUC is considerably lower.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (a) Attention in GNNs highlights a problematic ketone in a Tox21 compound. (b) Attention over SMILES tokens in ChemBERTa provides a close analogue to graph attention. (c) Neural stack trace enables fine-grained introspection of neuron behavior. (b - c) produced via BertViz [13].</p>
<h3>4.1 Tokenizers</h3>
<p>Our default tokenization strategy uses a Byte-Pair Encoder (BPE) from the HuggingFace tokenizers library [12]. BPE is a hybrid between character and word-level representations, which allows for the handling of large vocabularies in natural language corpora. Motivated by the intuition that rare and unknown words can often be decomposed into multiple known subwords, BPE finds the best word segmentation by iteratively and greedily merging frequent pairs of characters [32]. We compare this tokenization algorithm with a custom SmilesTokenizer based on a regex from [20], which we have released as part of DeepChem [31].3</p>
<p>To compare tokenizers, we pretrained two identical models on the PubChem-1M set. The pretrained models were evaluated on the Tox21 SR-p53 task. We found that the SmilesTokenizer narrowly outperformed BPE by ΔPRC-AUC = +0.015. Though this result suggests that a more semantically-relevant tokenization may provide performance benefits, further benchmarking on additional datasets is needed to validate this finding.</p>
<h3>4.2 SMILES vs. SELFIES</h3>
<p>In addition to SMILES, we pretrained ChemBERTA on SELFIES (SELF-referencing Embedded Strings) [15]. SELFIES is an alternate molecular string representation designed for machine learning. Because every valid SELFIES corresponds to a valid molecule, we hypothesized that SELFIES would lead to a more robust model. However, we found no significant difference in downstream performance on the Tox21 SR-p53 task. Further benchmarking is needed to validate this finding.</p>
<h3>4.3 Attention Visualization</h3>
<p>We used BertViz [13] to inspect the attention heads of ChemBERTa (SmilesTokenizer version) on Tox21, and contrast them to the molecular graph visualization of an attention-based GNN. We found certain neurons that were selective for chemically-relevant functional groups, and aromatic rings. We also observed other neurons that tracked bracket closures – a finding in keeping with results on attention-based RNNs showing the ability to track nested parentheses [33, 34].</p>
<h2>5 Discussion</h2>
<p>In this work, we introduce ChemBERTa, a transformer architecture for molecular property prediction. Initial results show that MLM pretraining provides a boost in predictive power for models on selected downstream tasks from MoleculeNet. However, with the possible exception of Tox21, ChemBERTa still performs below state-of-the-art on these tasks.</p>
<p>Our current analysis covers only a small portion of the hypothesis space we hope to explore. We plan to expand our evaluations to all of MoleculeNet, undertake more systematic hyperparameter</p>
<p><sup>3</sup>https://deepchem.readthedocs.io/en/latest/tokenizers.html#smilestokenizer</p>
<p>tuning, experiment with larger masking rates, and explore multitask finetuning. In parallel, we aim to scale up pretraining, first to the full PubChem 77M dataset, then to even larger sets like ZINC-15 (with 270 million compounds). This work will require us to improve our engineering infrastructure considerably.</p>
<p>As we scale up, we are also actively investigating methods to improve sample efficiency. Alternative text-based pretraining methods like ELECTRA may be useful [10]. Separately, there is little question that graph representations provide useful inductive biases for learning molecular structures. Recent hybrid graph transformer models [22, 35] may provide better sample efficiency while retaining the scalability of attention-based architectures.</p>
<h1>Broader Impact</h1>
<p>A core goal of AI for drug discovery is to accelerate the development of new and potentially life-saving medicines. Research to improve the accuracy and generalizability of molecular property prediction methods contributes directly to these aims. Nevertheless, machine learning-and particularly largescale pretraining of the form we undertake here-is a resource-intensive process that has a growing carbon footprint [36]. According to the Machine Learning Emissions Calculator (https://mlco2. github.io/impact), we estimate that our pretraining generated roughly $17.1 \mathrm{~kg} \mathrm{CO}_{2}$ eq (carbondioxide equivalent) of emissions. Fortunately, Google Cloud Platform, which we used for this work, is certified carbon-neutral and offsets $100 \%$ of emissions (https://cloud.google.com/ sustainability). Even as we advocate for further exploration of large-scale pretraining for property prediction, we also encourage other researchers to be mindful of the environmental impact of these efforts and opt for sustainable cloud compute solutions where possible.</p>
<h2>Acknowledgments and Disclosure of Funding</h2>
<p>We would like to thank the Tyler Cowen and the Emergent Ventures fellowship for providing the research grant to S.C. for cloud computing and various research expenses, alongside the Thiel Foundation for funding the grant. Thanks to Mario Krenn, Alston Lo, Akshat Nigam, Professor Alan Aspuru-Guzik and the entire Aspuru-Guzik group for early discussions and mentorship regarding the potiential for applying large-scale transformers on molecular strings, as well as in motivating the utilization of SELFIES in this work.</p>
<p>We would also like to thank the entire DeepChem team for their support and early discussions on fostering the ChemBERTa concept, and helping with designing and hosting the Tokenizers API and ChemBERTa tutorial. Thanks to the Reverie team for authorizing our usage of the PubChem 77M dataset, which was processed, filtered and split by them.</p>
<h2>References</h2>
<p>[1] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. In Advances in neural information processing systems, pages 22242232, 2015.
[2] Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8):595-608, 2016.
[3] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.
[4] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
[5] Connor W Coley, Regina Barzilay, William H Green, Tommi S Jaakkola, and Klavs F Jensen. Convolutional embedding of attributed molecular graphs for physical property prediction. Journal of chemical information and modeling, 57(8):1757-1772, 2017.
[6] Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al. Analyzing learned molec-</p>
<p>ular representations for property prediction. Journal of chemical information and modeling, 59(8):3370-3388, 2019.
[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017.
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
[9] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.
[10] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.
[11] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.
[12] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. ArXiv, pages arXiv-1910, 2019.
[13] Jesse Vig. A multiscale visualization of attention in the transformer model. CoRR, abs/1906.05714, 2019.
[14] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.
[15] Mario Krenn, Florian Hase, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-referencing embedded strings (selfies): A 100\% robust molecular string representation. Machine Learning: Science and Technology, 2020.
[16] Zheng Xu, Sheng Wang, Feiyun Zhu, and Junzhou Huang. Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery. In Proceedings of the 8th ACM international conference on bioinformatics, computational biology, and health informatics, pages 285-294, 2017.
[17] Matt J Kusner, Brooks Paige, and José Miguel Hernández-Lobato. Grammar variational autoencoder. arXiv preprint arXiv:1703.01925, 2017.
[18] Rafael Gómez-Bombarelli, Jennifer N Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alán Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268-276, 2018.
[19] Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1):120-131, 2018.
[20] Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee. Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction. ACS central science, 5(9):1572-1583, 2019.
[21] Shion Honda, Shoi Shi, and Hiroki R Ueda. Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery. arXiv preprint arXiv:1911.04738, 2019.
[22] Łukasz Maziarka, Tomasz Danel, Sławomir Mucha, Krzysztof Rataj, Jacek Tabor, and Stanisław Jastrzębski. Molecule attention transformer. arXiv preprint arXiv:2002.08264, 2020.
[23] Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics, pages 429-436, 2019.</p>
<p>[24] Benson Chen, Regina Barzilay, and Tommi Jaakkola. Path-augmented graph transformer network. arXiv preprint arXiv:1905.12712, 2019.
[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.
[26] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikitlearn: Machine learning in python. the Journal of machine Learning research, 12:2825-2830, 2011.
[27] David Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal of chemical information and modeling, 50(5):742-754, 2010.
[28] Greg Landrum et al. Rdkit: Open-source cheminformatics. 2006.
[29] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2019 update: improved access to chemical data. Nucleic acids research, 47(D1):D1102-D1109, 2019.
[30] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513-530, 2018.
[31] B Ramsundar, P Eastman, E Feinberg, J Gomes, K Leswing, A Pappu, M Wu, and V Pande. Deepchem: Democratizing deep-learning for drug discovery, quantum chemistry, materials science and biology, 2016.
[32] Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. Byte pair encoding: A text compression scheme that accelerates pattern matching. Technical report, Technical Report DOI-TR-161, Department of Informatics, Kyushu University, 1999.
[33] Mirac Suzgun, Sebastian Gehrmann, Yonatan Belinkov, and Stuart M Shieber. Lstm networks can perform dynamic counting. arXiv preprint arXiv:1906.03648, 2019.
[34] Xiang Yu, Ngoc Thang Vu, and Jonas Kuhn. Learning the dyck language with attention-based seq2seq models. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 138-146, 2019.
[35] Anonymous. Modelling drug-target binding affinity using a bert based graph neural network. Submitted to International Conference on Learning Representations, 2021.
[36] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ The main model directory can be viewed here. Each model includes the specific tokenizer (BPE, SMILEStokenized), representation (SMILES, SELFIES) and number of training steps ( $\left\lceil 150 \mathrm{k}^{\prime}\right.$ ) appended in its name.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
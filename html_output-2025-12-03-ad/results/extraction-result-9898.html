<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9898 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9898</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9898</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-d13b6f2067565943f1f2f8319f8f6c120704e75c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d13b6f2067565943f1f2f8319f8f6c120704e75c" target="_blank">LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is revealed that, despite strong results on Ruler, all models struggled with long text generation on LongGenBench, particularly as text length increased, suggesting that current LLMs are not yet equipped to meet the demands of real-world, long-form text generation.</p>
                <p><strong>Paper Abstract:</strong> Current benchmarks like Needle-in-a-Haystack (NIAH), Ruler, and Needlebench focus on models' ability to understand long-context input sequences but fail to capture a critical dimension: the generation of high-quality long-form text. Applications such as design proposals, technical documentation, and creative writing rely on coherent, instruction-following outputs over extended sequences - a challenge that existing benchmarks do not adequately address. To fill this gap, we introduce LongGenBench, a novel benchmark designed to rigorously evaluate large language models' (LLMs) ability to generate long text while adhering to complex instructions. Through tasks requiring specific events or constraints within generated text, LongGenBench evaluates model performance across four distinct scenarios, three instruction types, and two generation-lengths (16K and 32K tokens). Our evaluation of ten state-of-the-art LLMs reveals that, despite strong results on Ruler, all models struggled with long text generation on LongGenBench, particularly as text length increased. This suggests that current LLMs are not yet equipped to meet the demands of real-world, long-form text generation.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9898.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9898.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LongGenBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic benchmark introduced in this paper to evaluate long-context LLMs' ability to generate instruction-following super-long-form text (16K and 32K tokens) across four scenario families and three instruction-types, using automated, segment-level verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Various long-context LLMs (ten models evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Ten evaluated models include 8 open-source and 2 closed-source models ranging from ~2.8B to 72B parameters (including MoE variants) with claimed context windows between 32K and 128K tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / Evaluation of long-form text generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Synthetic scenario-driven generation tasks (Diary, Menu, Skyscraper, Urban Planning) with injected specific instructions (Single/Range/Periodic); outputs are split into subtasks and evaluated via an automated segmented evaluation pipeline (binary checks per segment) and aggregate metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Main Task Completion (Completion Rate, CR), Specific Task Instruction Completion-1 (STIC-1), Specific Task Instruction Completion-2 (STIC-2), and a weighted average wAvg (CR × STIC-2); additional probes include average output length and qualitative failure-mode analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Four scenario templates (Temporal: Diary, Menu; Spatial: Skyscraper, Urban Planning) with short/long versions, each task populated by SI/RI/PI templates; 800 examples per scenario per length (16K and 32K).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LongGenBench revealed substantial degradation in instruction adherence and completeness as generation length increased: many models that perform well on long-input benchmarks performed poorly generating 16K–32K-token outputs; periodic instructions were hardest; aggregated metrics show drop-offs (detailed per-model CR/STIC in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Synthetic tasks focus on instruction-following rather than open-ended creativity or deep factual verification; automated binary segment checks reduce evaluation to presence/absence and may miss nuance; human evaluation remains costly for long outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>The benchmark favors automated, segment-level checks to scale evaluation; authors validated the binary evaluator against 300 human-labeled items (perfect match in that validation), but note broader LLM-as-judge biases and interpretability issues compared to detailed human review.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use check_sets of segment-level instructions, split long outputs into subtasks for targeted verification, validate automated evaluators against human labels, and prefer STIC-2 for holistic completeness reporting (authors consider simplifying to STIC-2 only).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9898.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9898.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CR/STIC/wAvg</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Completion Rate (CR), Specific Task Instruction Completion-1 (STIC-1), Specific Task Instruction Completion-2 (STIC-2), weighted average (wAvg)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of complementary quantitative metrics defined in this paper to evaluate whether long-form generated outputs complete required subtasks and adhere to specific injected instructions, with STIC-1 focusing on correctness on produced outputs and STIC-2 measuring adherence relative to the whole expected instruction set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Applies to any evaluated long-context LLM</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Metric-agnostic; used to score outputs from the ten tested LLMs in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation metrics for long-form, instruction-driven generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>CR = (# completed subtasks) / (total subtasks); STIC-1 = (fulfilled SI + RI + PI) / (total outputs to specific task instructions) where denominator is only produced outputs; STIC-2 = (fulfilled SI + RI + PI) / (total specific task instructions) where denominator is all instruction items; wAvg = CR × STIC-2 used as an overall performance scalar.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Measures task completeness (CR), instruction adherence on produced outputs (STIC-1), instruction adherence relative to entire task (STIC-2), and an overall combined score (wAvg).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Defined and applied within LongGenBench across the four scenarios and SI/RI/PI checks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Metrics expose different failure modes: a model can have high STIC-1 on produced segments but low CR (incomplete outputs) causing low STIC-2; example from paper (short-version): LLaMA3.1-8B CR 93.5%, STIC-1 23.4%, STIC-2 22.0%; Qwen2-7B CR 60.0%, STIC-1 27.9%, STIC-2 16.1% (Table 6). Authors note STIC-2 provides a more holistic picture.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>STIC-1 can be misleading when outputs are partial (denominator limited to produced outputs); binary nature of instruction checks can miss partial correctness or quality gradations; aggregating by multiplication (wAvg) is a design choice that may compress interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Metrics are automated and scalable compared to manual assessment; authors validated the binary checks against human labels for a subset (300 points) with full agreement, but emphasize human evaluation captures nuance beyond binary fulfilment.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report CR alongside STIC-2 (or adopt STIC-2 alone) to capture both completeness and adherence; use segment-level checks and disclose denominators; validate automated metrics against human judgments on samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9898.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9898.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SegEval Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Segmented evaluation pipeline (Algorithm 1) with Check_Set and binary LLM-based verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured pipeline that decomposes a long LLM output into subtasks, matches them to a precomputed check_set of specific instructions, and evaluates each subtask with a binary eval(A_i, T_i) (yes/no), aggregating results to compute completion metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Llama3.1-8B used as the automated evaluator; tested models are the ten long-context LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Llama3.1-8B-Instruct (8B parameters, 128K context) is used by authors to run binary classification evaluations on extracted segments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Automated evaluation methods for long-form generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>1) Generate A from task prompt T; 2) Split A into {A1..Am}; 3) For each T_i in check_set, find matching A_i and run eval(A_i,T_i) using an LLM (binary yes/no); 4) Aggregate per-segment binary results into CR/STIC metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary fulfillment per segment (Yes/No) aggregated into Completion Rate, STIC-1 and STIC-2; validated against human labels on a 300-sample subset.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to LongGenBench generated outputs and its task-specific check_sets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Authors report the Llama3.1-8B-based binary evaluation matched human labels on 300 manually-labeled data points; the pipeline enables scalable verification of instruction adherence across very long outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Binary evaluations simplify quality to presence/absence and cannot measure partial compliance, fluency, factual correctness, or reasoning quality; LLM evaluators themselves can exhibit biases and ordering sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Binary LLM evaluator matched human labels in the validation sample, but the authors caution that LLM-based evaluation has broader biases identified in related work and cannot fully replace thorough human review for nuanced judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Break long outputs into smaller, verifiable segments (check_set), use automated LLM evaluators for large-scale binary checks but validate them against human labels, and be explicit about what the binary check measures (presence vs. quality).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9898.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9898.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SI/RI/PI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single Instruction (SI), Range Instruction (RI), Periodic Instruction (PI) task templates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three task control modalities used to inject specific constraints into generated long outputs: SI inserts a single specific item at a particular position; RI requires insertion across a contiguous range; PI requires repeating an item at fixed intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Applies to evaluated LLMs under LongGenBench</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Templates applied across tested LLMs to create controlled instruction-following checks.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Task design for evaluating instruction adherence in long-form generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Generate tasks by splicing main task prompts with STI (5 single, 1 range, 1 periodic per main prompt), then verify presence and correct placement of SI/RI/PI items in the output using the segmented evaluation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correct placement and presence of items for SI, full coverage for RI across specified ranges, and correct periodicity for PI; aggregated via STIC-1/STIC-2.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used to synthesize the LongGenBench check_sets across four scenarios; authors use >20 templates per instruction type and randomize locations to ensure coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Across models, adherence ranking observed: Single > Range > Periodic; periodic tasks show the largest performance drop, indicating greater reasoning/temporal maintenance difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Periodic tasks demand higher temporal/recurrence reasoning and are prone to misplacement or omission as sequence length grows; range tasks add complexity through multi-item coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>These structured injections provide more easily verifiable ground truth than open-ended creative evaluation and allow automated checking not possible for some human-judged criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use a mixture of SI/RI/PI to probe different aspects of instruction-following; randomize positions and templates to avoid overfitting; include periodic checks to stress long-range consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9898.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9898.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ruler (comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ruler: What's the real context size of your long-context language models?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An existing synthetic long-context benchmark (cited in this paper) used to evaluate models' long-input handling; used here for comparison to LongGenBench output-focused evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ruler: What's the real context size of your long-context language models?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Various (as used by Ruler in original work)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Ruler is a synthetic benchmark designed to probe context-size and long-input tasks across LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Long-context comprehension and retrieval evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Ruler evaluates long-input comprehension tasks (e.g., NIAH-style retrieval/multi-hop tracing) and is contrasted with LongGenBench which focuses on long-output generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Long-input retrieval/comprehension accuracy and tracing capabilities (not the STIC metrics used in LongGenBench).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Ruler synthetic tasks (input-focused) cited as a complementary baseline to LongGenBench's output-focused tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Correlation between Ruler (input performance) and LongGenBench (output performance) is moderate: Pearson correlation = 0.51 at 16K and 0.66 at 32K, indicating overlap but also substantial differences between input-handling and output-generation skills.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Ruler tests input retention/retrieval, which does not fully predict long-output instruction adherence; models good at long-input retrieval can still fail at generating long, instruction-compliant outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Ruler focuses on measurable retrieval tasks; LongGenBench complements it by testing generation and instruction following, highlighting that input handling and output coherence are distinct capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Evaluate both input-based (Ruler-like) and output-based (LongGenBench-like) capabilities when assessing long-context LLMs, as excelling at one does not guarantee excelling at the other.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9898.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9898.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Empirical Findings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical results on ten long-context LLMs using LongGenBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Summary of quantitative and qualitative findings from applying LongGenBench to ten state-of-the-art long-context models, showing systematic performance degradation with increased output length and identifying common failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Evaluated models include GPT-4o-mini, GPT-4o, Llama3.1-8B/70B, Qwen2-7B/72B, Mistral-v0.2, Mixtral-8x7B, FILM-7B, LongWriter-llama3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Models range from ~2.8B to 72B parameters, include Mixture-of-Experts architectures (e.g., Mixtral), and have claimed context lengths from 32K to 128K tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG / Model capability evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>All models were prompted with LongGenBench tasks (16K and 32K generation targets) and scored with CR/STIC-1/STIC-2/wAvg using the segmented pipeline; inference used vLLM with greedy decoding and BFloat16 on 8× NVIDIA A800 GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Completion Rate (CR), STIC-1, STIC-2, average generated length for successful runs, and qualitative failure-mode analysis (forgetting, repetition, premature stopping).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>LongGenBench synthetic tasks: 800 examples per scenario per length (16K/32K).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Key findings: models frequently failed to maintain instruction adherence as output length increased (notable performance drop after ~4,000 tokens); at 16K, top performers included Qwen2-72B and GPT-4o; at 32K Llama3.1-8B performed notably well relative to larger models; periodic tasks were hardest; 45% of long outputs showed significant repetition; common failures include premature termination, omission of subtasks, forgetting/misplacing instructions, and repetitive content.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance influenced by architecture, model size, and instruction-tuning data distribution; many instruction-tuning datasets are short (<200 tokens) which likely impedes long-output instruction following; automated metrics capture presence but not deeper rationality or factual consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Automated metrics (binary checks) scaled evaluation and matched human labels on a validated subset, but detailed human review still required for assessing rationality, temporal realism, and nuanced coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Improve instruction-tuning with longer examples, consider architectural changes to maintain long-range coherence, include domain-specific temporally annotated data where relevant, and continue validating automated evaluators against human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9898.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9898.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-judge critique</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limitations and biases of LLM-based evaluators (LLM-as-a-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper discusses and cites prior work on biases and interpretability issues when using LLMs to evaluate text (sensitivity to presentation order, verbosity preference), and reports using LLM-based binary checks while validating them with human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>General: LLM evaluators (e.g., GPT-family, Llama series) discussed in related work and used for evaluation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>LLM evaluators often vary in architecture and size; their outputs (judgments) can be biased by prompt formatting and verbosity.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation methodology for NLG</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Authors used an LLM (Llama3.1-8B) as a binary evaluator for segment checks but discuss broader literature critiquing LLM-as-judge approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Bias awareness: presentation-order sensitivity, verbosity preference, potential misalignment with human judgments on nuanced metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Discussed in relation to MT-Bench, FairEval and other meta-evaluation efforts (cited in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Authors found their specific Llama3.1-8B binary evaluator matched human labels on a 300-sample validation, but caution that general LLM evaluators have known biases and that interpreting their outputs can be difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLM evaluators can be biased and brittle; may favor verbose outputs and be sensitive to prompt structure; they are less interpretable than human ratings for complex qualitative judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>While LLM-based evaluation scales and can align well on simple binary checks, human evaluation remains necessary for assessing nuance, rationality, and quality beyond presence/absence.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use LLM evaluators for scalable, well-scoped checks (e.g., binary presence) but validate against human labels, use meta-evaluation benchmarks, and avoid relying solely on LLM-judges for complex qualitative assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ruler: What's the real context size of your long-context language models? <em>(Rating: 2)</em></li>
                <li>Needlebench: Can llms do retrieval and reasoning in 1 million context window? <em>(Rating: 2)</em></li>
                <li>Needle In A Haystack - pressure testing LLMs <em>(Rating: 1)</em></li>
                <li>LongBench: A bilingual, multitask benchmark for long context understanding <em>(Rating: 2)</em></li>
                <li>Longwriter: Unleashing 10,000+ word generation from long context llms <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with MT-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Proxyqa: An alternative framework for evaluating long-form text generation with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9898",
    "paper_id": "paper-d13b6f2067565943f1f2f8319f8f6c120704e75c",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "LongGenBench",
            "name_full": "LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs",
            "brief_description": "A synthetic benchmark introduced in this paper to evaluate long-context LLMs' ability to generate instruction-following super-long-form text (16K and 32K tokens) across four scenario families and three instruction-types, using automated, segment-level verification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Various long-context LLMs (ten models evaluated)",
            "llm_description": "Ten evaluated models include 8 open-source and 2 closed-source models ranging from ~2.8B to 72B parameters (including MoE variants) with claimed context windows between 32K and 128K tokens.",
            "scientific_domain": "Natural Language Generation / Evaluation of long-form text generation",
            "evaluation_method": "Synthetic scenario-driven generation tasks (Diary, Menu, Skyscraper, Urban Planning) with injected specific instructions (Single/Range/Periodic); outputs are split into subtasks and evaluated via an automated segmented evaluation pipeline (binary checks per segment) and aggregate metrics.",
            "evaluation_criteria": "Main Task Completion (Completion Rate, CR), Specific Task Instruction Completion-1 (STIC-1), Specific Task Instruction Completion-2 (STIC-2), and a weighted average wAvg (CR × STIC-2); additional probes include average output length and qualitative failure-mode analysis.",
            "benchmark_or_dataset": "Four scenario templates (Temporal: Diary, Menu; Spatial: Skyscraper, Urban Planning) with short/long versions, each task populated by SI/RI/PI templates; 800 examples per scenario per length (16K and 32K).",
            "results_summary": "LongGenBench revealed substantial degradation in instruction adherence and completeness as generation length increased: many models that perform well on long-input benchmarks performed poorly generating 16K–32K-token outputs; periodic instructions were hardest; aggregated metrics show drop-offs (detailed per-model CR/STIC in Table 3).",
            "limitations_or_challenges": "Synthetic tasks focus on instruction-following rather than open-ended creativity or deep factual verification; automated binary segment checks reduce evaluation to presence/absence and may miss nuance; human evaluation remains costly for long outputs.",
            "comparison_to_human_or_traditional": "The benchmark favors automated, segment-level checks to scale evaluation; authors validated the binary evaluator against 300 human-labeled items (perfect match in that validation), but note broader LLM-as-judge biases and interpretability issues compared to detailed human review.",
            "recommendations_or_best_practices": "Use check_sets of segment-level instructions, split long outputs into subtasks for targeted verification, validate automated evaluators against human labels, and prefer STIC-2 for holistic completeness reporting (authors consider simplifying to STIC-2 only).",
            "uuid": "e9898.0",
            "source_info": {
                "paper_title": "LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CR/STIC/wAvg",
            "name_full": "Completion Rate (CR), Specific Task Instruction Completion-1 (STIC-1), Specific Task Instruction Completion-2 (STIC-2), weighted average (wAvg)",
            "brief_description": "Set of complementary quantitative metrics defined in this paper to evaluate whether long-form generated outputs complete required subtasks and adhere to specific injected instructions, with STIC-1 focusing on correctness on produced outputs and STIC-2 measuring adherence relative to the whole expected instruction set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Applies to any evaluated long-context LLM",
            "llm_description": "Metric-agnostic; used to score outputs from the ten tested LLMs in the paper.",
            "scientific_domain": "NLG evaluation metrics for long-form, instruction-driven generation",
            "evaluation_method": "CR = (# completed subtasks) / (total subtasks); STIC-1 = (fulfilled SI + RI + PI) / (total outputs to specific task instructions) where denominator is only produced outputs; STIC-2 = (fulfilled SI + RI + PI) / (total specific task instructions) where denominator is all instruction items; wAvg = CR × STIC-2 used as an overall performance scalar.",
            "evaluation_criteria": "Measures task completeness (CR), instruction adherence on produced outputs (STIC-1), instruction adherence relative to entire task (STIC-2), and an overall combined score (wAvg).",
            "benchmark_or_dataset": "Defined and applied within LongGenBench across the four scenarios and SI/RI/PI checks.",
            "results_summary": "Metrics expose different failure modes: a model can have high STIC-1 on produced segments but low CR (incomplete outputs) causing low STIC-2; example from paper (short-version): LLaMA3.1-8B CR 93.5%, STIC-1 23.4%, STIC-2 22.0%; Qwen2-7B CR 60.0%, STIC-1 27.9%, STIC-2 16.1% (Table 6). Authors note STIC-2 provides a more holistic picture.",
            "limitations_or_challenges": "STIC-1 can be misleading when outputs are partial (denominator limited to produced outputs); binary nature of instruction checks can miss partial correctness or quality gradations; aggregating by multiplication (wAvg) is a design choice that may compress interpretability.",
            "comparison_to_human_or_traditional": "Metrics are automated and scalable compared to manual assessment; authors validated the binary checks against human labels for a subset (300 points) with full agreement, but emphasize human evaluation captures nuance beyond binary fulfilment.",
            "recommendations_or_best_practices": "Report CR alongside STIC-2 (or adopt STIC-2 alone) to capture both completeness and adherence; use segment-level checks and disclose denominators; validate automated metrics against human judgments on samples.",
            "uuid": "e9898.1",
            "source_info": {
                "paper_title": "LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SegEval Pipeline",
            "name_full": "Segmented evaluation pipeline (Algorithm 1) with Check_Set and binary LLM-based verification",
            "brief_description": "A structured pipeline that decomposes a long LLM output into subtasks, matches them to a precomputed check_set of specific instructions, and evaluates each subtask with a binary eval(A_i, T_i) (yes/no), aggregating results to compute completion metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Llama3.1-8B used as the automated evaluator; tested models are the ten long-context LLMs",
            "llm_description": "Llama3.1-8B-Instruct (8B parameters, 128K context) is used by authors to run binary classification evaluations on extracted segments.",
            "scientific_domain": "Automated evaluation methods for long-form generation",
            "evaluation_method": "1) Generate A from task prompt T; 2) Split A into {A1..Am}; 3) For each T_i in check_set, find matching A_i and run eval(A_i,T_i) using an LLM (binary yes/no); 4) Aggregate per-segment binary results into CR/STIC metrics.",
            "evaluation_criteria": "Binary fulfillment per segment (Yes/No) aggregated into Completion Rate, STIC-1 and STIC-2; validated against human labels on a 300-sample subset.",
            "benchmark_or_dataset": "Applied to LongGenBench generated outputs and its task-specific check_sets.",
            "results_summary": "Authors report the Llama3.1-8B-based binary evaluation matched human labels on 300 manually-labeled data points; the pipeline enables scalable verification of instruction adherence across very long outputs.",
            "limitations_or_challenges": "Binary evaluations simplify quality to presence/absence and cannot measure partial compliance, fluency, factual correctness, or reasoning quality; LLM evaluators themselves can exhibit biases and ordering sensitivity.",
            "comparison_to_human_or_traditional": "Binary LLM evaluator matched human labels in the validation sample, but the authors caution that LLM-based evaluation has broader biases identified in related work and cannot fully replace thorough human review for nuanced judgments.",
            "recommendations_or_best_practices": "Break long outputs into smaller, verifiable segments (check_set), use automated LLM evaluators for large-scale binary checks but validate them against human labels, and be explicit about what the binary check measures (presence vs. quality).",
            "uuid": "e9898.2",
            "source_info": {
                "paper_title": "LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SI/RI/PI",
            "name_full": "Single Instruction (SI), Range Instruction (RI), Periodic Instruction (PI) task templates",
            "brief_description": "Three task control modalities used to inject specific constraints into generated long outputs: SI inserts a single specific item at a particular position; RI requires insertion across a contiguous range; PI requires repeating an item at fixed intervals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Applies to evaluated LLMs under LongGenBench",
            "llm_description": "Templates applied across tested LLMs to create controlled instruction-following checks.",
            "scientific_domain": "Task design for evaluating instruction adherence in long-form generation",
            "evaluation_method": "Generate tasks by splicing main task prompts with STI (5 single, 1 range, 1 periodic per main prompt), then verify presence and correct placement of SI/RI/PI items in the output using the segmented evaluation pipeline.",
            "evaluation_criteria": "Correct placement and presence of items for SI, full coverage for RI across specified ranges, and correct periodicity for PI; aggregated via STIC-1/STIC-2.",
            "benchmark_or_dataset": "Used to synthesize the LongGenBench check_sets across four scenarios; authors use &gt;20 templates per instruction type and randomize locations to ensure coverage.",
            "results_summary": "Across models, adherence ranking observed: Single &gt; Range &gt; Periodic; periodic tasks show the largest performance drop, indicating greater reasoning/temporal maintenance difficulty.",
            "limitations_or_challenges": "Periodic tasks demand higher temporal/recurrence reasoning and are prone to misplacement or omission as sequence length grows; range tasks add complexity through multi-item coverage.",
            "comparison_to_human_or_traditional": "These structured injections provide more easily verifiable ground truth than open-ended creative evaluation and allow automated checking not possible for some human-judged criteria.",
            "recommendations_or_best_practices": "Use a mixture of SI/RI/PI to probe different aspects of instruction-following; randomize positions and templates to avoid overfitting; include periodic checks to stress long-range consistency.",
            "uuid": "e9898.3",
            "source_info": {
                "paper_title": "LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Ruler (comparison)",
            "name_full": "Ruler: What's the real context size of your long-context language models?",
            "brief_description": "An existing synthetic long-context benchmark (cited in this paper) used to evaluate models' long-input handling; used here for comparison to LongGenBench output-focused evaluation.",
            "citation_title": "Ruler: What's the real context size of your long-context language models?",
            "mention_or_use": "mention",
            "llm_name": "Various (as used by Ruler in original work)",
            "llm_description": "Ruler is a synthetic benchmark designed to probe context-size and long-input tasks across LLMs.",
            "scientific_domain": "Long-context comprehension and retrieval evaluation",
            "evaluation_method": "Ruler evaluates long-input comprehension tasks (e.g., NIAH-style retrieval/multi-hop tracing) and is contrasted with LongGenBench which focuses on long-output generation.",
            "evaluation_criteria": "Long-input retrieval/comprehension accuracy and tracing capabilities (not the STIC metrics used in LongGenBench).",
            "benchmark_or_dataset": "Ruler synthetic tasks (input-focused) cited as a complementary baseline to LongGenBench's output-focused tasks.",
            "results_summary": "Correlation between Ruler (input performance) and LongGenBench (output performance) is moderate: Pearson correlation = 0.51 at 16K and 0.66 at 32K, indicating overlap but also substantial differences between input-handling and output-generation skills.",
            "limitations_or_challenges": "Ruler tests input retention/retrieval, which does not fully predict long-output instruction adherence; models good at long-input retrieval can still fail at generating long, instruction-compliant outputs.",
            "comparison_to_human_or_traditional": "Ruler focuses on measurable retrieval tasks; LongGenBench complements it by testing generation and instruction following, highlighting that input handling and output coherence are distinct capabilities.",
            "recommendations_or_best_practices": "Evaluate both input-based (Ruler-like) and output-based (LongGenBench-like) capabilities when assessing long-context LLMs, as excelling at one does not guarantee excelling at the other.",
            "uuid": "e9898.4",
            "source_info": {
                "paper_title": "LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Empirical Findings",
            "name_full": "Empirical results on ten long-context LLMs using LongGenBench",
            "brief_description": "Summary of quantitative and qualitative findings from applying LongGenBench to ten state-of-the-art long-context models, showing systematic performance degradation with increased output length and identifying common failure modes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Evaluated models include GPT-4o-mini, GPT-4o, Llama3.1-8B/70B, Qwen2-7B/72B, Mistral-v0.2, Mixtral-8x7B, FILM-7B, LongWriter-llama3.1-8B",
            "llm_description": "Models range from ~2.8B to 72B parameters, include Mixture-of-Experts architectures (e.g., Mixtral), and have claimed context lengths from 32K to 128K tokens.",
            "scientific_domain": "NLG / Model capability evaluation",
            "evaluation_method": "All models were prompted with LongGenBench tasks (16K and 32K generation targets) and scored with CR/STIC-1/STIC-2/wAvg using the segmented pipeline; inference used vLLM with greedy decoding and BFloat16 on 8× NVIDIA A800 GPUs.",
            "evaluation_criteria": "Completion Rate (CR), STIC-1, STIC-2, average generated length for successful runs, and qualitative failure-mode analysis (forgetting, repetition, premature stopping).",
            "benchmark_or_dataset": "LongGenBench synthetic tasks: 800 examples per scenario per length (16K/32K).",
            "results_summary": "Key findings: models frequently failed to maintain instruction adherence as output length increased (notable performance drop after ~4,000 tokens); at 16K, top performers included Qwen2-72B and GPT-4o; at 32K Llama3.1-8B performed notably well relative to larger models; periodic tasks were hardest; 45% of long outputs showed significant repetition; common failures include premature termination, omission of subtasks, forgetting/misplacing instructions, and repetitive content.",
            "limitations_or_challenges": "Performance influenced by architecture, model size, and instruction-tuning data distribution; many instruction-tuning datasets are short (&lt;200 tokens) which likely impedes long-output instruction following; automated metrics capture presence but not deeper rationality or factual consistency.",
            "comparison_to_human_or_traditional": "Automated metrics (binary checks) scaled evaluation and matched human labels on a validated subset, but detailed human review still required for assessing rationality, temporal realism, and nuanced coherence.",
            "recommendations_or_best_practices": "Improve instruction-tuning with longer examples, consider architectural changes to maintain long-range coherence, include domain-specific temporally annotated data where relevant, and continue validating automated evaluators against human judgments.",
            "uuid": "e9898.5",
            "source_info": {
                "paper_title": "LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLM-as-judge critique",
            "name_full": "Limitations and biases of LLM-based evaluators (LLM-as-a-judge)",
            "brief_description": "Paper discusses and cites prior work on biases and interpretability issues when using LLMs to evaluate text (sensitivity to presentation order, verbosity preference), and reports using LLM-based binary checks while validating them with human labels.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_name": "General: LLM evaluators (e.g., GPT-family, Llama series) discussed in related work and used for evaluation tasks",
            "llm_description": "LLM evaluators often vary in architecture and size; their outputs (judgments) can be biased by prompt formatting and verbosity.",
            "scientific_domain": "Evaluation methodology for NLG",
            "evaluation_method": "Authors used an LLM (Llama3.1-8B) as a binary evaluator for segment checks but discuss broader literature critiquing LLM-as-judge approaches.",
            "evaluation_criteria": "Bias awareness: presentation-order sensitivity, verbosity preference, potential misalignment with human judgments on nuanced metrics.",
            "benchmark_or_dataset": "Discussed in relation to MT-Bench, FairEval and other meta-evaluation efforts (cited in paper).",
            "results_summary": "Authors found their specific Llama3.1-8B binary evaluator matched human labels on a 300-sample validation, but caution that general LLM evaluators have known biases and that interpreting their outputs can be difficult.",
            "limitations_or_challenges": "LLM evaluators can be biased and brittle; may favor verbose outputs and be sensitive to prompt structure; they are less interpretable than human ratings for complex qualitative judgments.",
            "comparison_to_human_or_traditional": "While LLM-based evaluation scales and can align well on simple binary checks, human evaluation remains necessary for assessing nuance, rationality, and quality beyond presence/absence.",
            "recommendations_or_best_practices": "Use LLM evaluators for scalable, well-scoped checks (e.g., binary presence) but validate against human labels, use meta-evaluation benchmarks, and avoid relying solely on LLM-judges for complex qualitative assessments.",
            "uuid": "e9898.6",
            "source_info": {
                "paper_title": "LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ruler: What's the real context size of your long-context language models?",
            "rating": 2
        },
        {
            "paper_title": "Needlebench: Can llms do retrieval and reasoning in 1 million context window?",
            "rating": 2
        },
        {
            "paper_title": "Needle In A Haystack - pressure testing LLMs",
            "rating": 1
        },
        {
            "paper_title": "LongBench: A bilingual, multitask benchmark for long context understanding",
            "rating": 2
        },
        {
            "paper_title": "Longwriter: Unleashing 10,000+ word generation from long context llms",
            "rating": 2
        },
        {
            "paper_title": "Judging llm-as-a-judge with MT-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "Proxyqa: An alternative framework for evaluating long-form text generation with large language models",
            "rating": 1
        }
    ],
    "cost": 0.018890999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LongGenbench: Benchmarking Long-Form Generation in Long Context LLMs</h1>
<p>Yuhao Wu ${ }^{1}$, Ming Shan Hee ${ }^{1}$, Zhiqing $\mathbf{H u}^{1}$ and Roy Ka-Wei Lee ${ }^{1}$<br>${ }^{1}$ Singapore University of Technology and Design<br>{wu_yuhao,mingshan_hee, zhiqing_hu}@mymail.sutd.edu.sg<br>roy_lee@sutd.edu.sg</p>
<h4>Abstract</h4>
<p>Current benchmarks like "Needle-in-a-Haystack" (NIAH), Ruler, and Needlebench focus on models' ability to understand long-context input sequences but fail to capture a critical dimension: the generation of high-quality long-form text. Applications such as design proposals, technical documentation, and creative writing rely on coherent, instruction-following outputs over extended sequences-a challenge that existing benchmarks do not adequately address. To fill this gap, we introduce LongGenBench, a novel benchmark designed to rigorously evaluate large language models' (LLMs) ability to generate long text while adhering to complex instructions. Through tasks requiring specific events or constraints within generated text, LongGenBench evaluates model performance across four distinct scenarios, three instruction types, and two generation-lengths ( 16 K and 32 K tokens). Our evaluation of ten state-of-the-art LLMs reveals that, despite strong results on Ruler, all models struggled with long text generation on LongGenBench, particularly as text length increased. This suggests that current LLMs are not yet equipped to meet the demands of real-world, long-form text generation. We opensource LongGenBench to promote comprehensive evaluation and improvement in this critical area, with code and data available at https://github.com/ mozhu621/LongGenBench.</p>
<h2>1 Introduction</h2>
<p>Recent advances in large language models (LLMs) have dramatically enhanced their ability to process long text sequences, supporting applications that range from document summarization to creative writing. Leading models such as GPT-4 (Achiam et al., 2023), LLaMa-3.2 (Dubey et al., 2024), and Claude 2.1 (Anthropic, 2024a) manage context windows of up to 128 K tokens, with the Claude 3 series (Anthropic, 2024b) handling inputs exceeding 1 million tokens. However, while much attention has been given to these models' ability to retrieve and understand long input text sequences, far less focus has been placed on their ability to generate coherent and high-quality long-form text outputs-a critical requirement for tasks such as design proposals and creative writing.</p>
<p>Long-form text generation is crucial for real-world applications that require detailed, well-structured narratives, such as document summarization (Kumar et al., 2024), creative writing (Hua \&amp; Wang, 2020; Hu et al., 2022), and comprehensive question answering (Stelmakh et al., 2022; Lee et al., 2023; Bai et al., 2024). Despite this importance, current benchmarks are limited in their ability to assess long-form generation, focusing instead on shorter text outputs ( $\leq 2 \mathrm{~K}$ tokens) (Fan et al., 2018; 2019a; Dasigi et al., 2021), making them unsuitable for tasks requiring outputs of $\geq 16 \mathrm{~K}$ tokens (Bai et al., 2024). The challenge is further compounded by the lack of robust methods for evaluating these long sequences. The ability to follow instructions is essential for long text generation (Reversed NIAH ${ }^{1}$ ), just as effective information retrieval is fundamental for processing long-context inputs (NIAH(Kamradt, 2023)). However, current benchmarks do not adequately assess whether the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Comparison of Long-Context LLM Benchmarks. For the retrieval tasks’ datasets, we measure length based on the number of processing tokens, while for the generation tasks’ datasets, we calculate the average number of generation words produced by LLMs. 'Long-length’ indicates if LLMs to analyze or generate text that is at least 8K token.</p>
<table>
<thead>
<tr>
<th>Type of Task</th>
<th>Benchmark</th>
<th>Type of Data</th>
<th>Avg Len</th>
<th>Long-Length</th>
</tr>
</thead>
<tbody>
<tr>
<td>Retrieval</td>
<td>Longbench<em>Bai et al. (2023)</em></td>
<td>hybrid</td>
<td>$\sim 8$k</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td></td>
<td>NIAH<em>Kamradt (2023)</em></td>
<td>synthetic</td>
<td>Any</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td></td>
<td>Ruler<em>Hsieh et al. (2024)</em></td>
<td>synthetic</td>
<td>Any</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Generation</td>
<td>ELi5<em>Fan et al. (2019b)</em></td>
<td>hybrid</td>
<td>$\sim 0.2$K</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td></td>
<td>Longwrite<em>Bai et al. (2024)</em></td>
<td>synthetic</td>
<td>$\sim 2.7$K</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td></td>
<td>LongGenBench<em>Ours</em></td>
<td>synthetic</td>
<td>$\sim 20$K</td>
<td>$\checkmark$</td>
</tr>
</tbody>
</table>
<p>generated text adheres to the specific directives of a prompt. For instance, a prompt may require incorporating specific information at a certain point in a lengthy document, but evaluations often fail to verify the model’s compliance with such instructions. This oversight represents a significant shortcoming in benchmarking, particularly because performance under explicit constraints typically predicts outcomes in tasks with more implicit constraints, such as story generation or academic paper production. If a model struggles with explicit requirements, it is likely to underperform in scenarios with subtler constraints.</p>
<p>Manual evaluations, while thorough, are both costly and impractical at scale. Meanwhile, automated evaluations using "LLM-as-a-judge" methods <em>Zheng et al. (2024)</em> often yield results that are difficult to interpret and may not align with human judgments, raising concerns about their reliability. This highlights the need for more specialized benchmarks capable of reliably assessing the quality of super-long-form text generation.</p>
<p>To address this gap, we present LongGenBench, a novel benchmark designed to evaluate the quality of super-long-form text generated by long-context LLMs. Unlike existing benchmarks that primarily test retrieval or reasoning over long inputs, LongGenBench focuses on the model’s ability to generate content that follows complex instructions over extended sequences. Our benchmark introduces tasks that reflect real-world generation challenges, such as diary writing, menu planning, and urban design, where the text must adhere to specific constraints provided in the prompt. These tasks assess whether models can correctly incorporate specific details at designated points in the text, ensuring the generated content meets the requirements laid out in the prompt. By evaluating texts up to 32K tokens, LongGenBench is the first benchmark to systematically test the ability to generate instruction-compliant long-form content across extended lengths. Table 1 summarizes the different benchmarks supporting long-context retrieval and generation tasks.</p>
<p>The evaluation tasks are organized into four distinct scenarios: Diary Writing, Menu Design, Skyscraper Design, and Urban Planning, each with varying complexity. The scenarios involve sub-tasks such as single instance, range, and periodicity, simulating realistic constraints that a model must account for. This setup allows us to measure the model’s ability to generate detailed, contextually rich outputs that satisfy a wide array of criteria.</p>
<p>In summary, our major contributions are as follows:</p>
<ul>
<li>To the best of our knowledge, this is the first study to address the challenge of super-long-form generation in long-context language models, highlighting the critical importance of generating coherent, high-quality text in extended contexts.</li>
<li>We introduce LongGenBench , a comprehensive dataset that provides a diverse set of tasks specifically designed to evaluate the super-long-form generation capabilities of LLMs across varying token lengths (16K and 32K) and levels of text complexity.</li>
<li>We perform extensive experiments on both open-source and closed-source models, revealing that despite their advanced capabilities, most models struggle significantly with super-long-form generation tasks, particularly in maintaining instruction adherence and coherence over long outputs.</li>
</ul>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: LongGenBench Overview: 1) Scenario Selection: Select from four scenarios—Diary, Menu Design, Skyscraper Design, and Urban Planning—each offered in both short and long versions to determine the main task prompt. 2) Task Instruction: Employ the template libraries SI (Single), RI (Range), and PI (Periodic) to generate tasks characterized by random times or locations, along with the corresponding prompts and verification sets. 3) Instruction Synthesis: Integrate all prompts generated in the prior step to create a comprehensive set of instructions with a final check-set. 4) Example: An illustration of Sophia’s weekly diary task is provided as an example.</p>
<h1>2 LongGenBench Benchmark</h1>
<h3>2.1 Task Definition</h3>
<p>Evaluating the quality of super-long-form generation presents a unique set of challenges due to the inherent complexity of long texts. Traditional human evaluation methods, while precise, are expensive and not scalable. Although using large language models for assessment is feasible, their lack of interpretability often hampers their utility. Thus, we focus on the "instruction-following" task in super-long-form generation, where the most must include specific details in the generated text. This task reflects real-world scenarios that require a high degree of attention to detail over extended sequences, such as technical documentation or detailed design proposals. In this study, we define a task type termed Strictly Sequential Tasks, which involves the sequential completion of subtasks $\mathbf{T}=\left{T_{1}, T_{2}, T_{3}, \ldots, T_{n}\right}^{\mathbf{2}}$, where each subtask is responsible for generating a specific volume of text. For instance, an LLM might be tasked with designing a 100 -floor skyscraper, specifying the content and purpose of each floor.</p>
<h3>2.2 Four Distinct Scenario Setups</h3>
<p>To comprehensively assess the long-form generation capabilities of models, we have devised four distinct task scenarios to supplement our predefined tasks, as illustrated in Figure 1 (1). These scenarios fall into two categories: Temporal (Diary Writing, Menu Design) and Spatial (Skyscraper Design, Urban Planning). Moreover, each scenario incorporates both short and long versions to assess the effectiveness of various output lengths.</p>
<p>These scenarios were carefully chosen to reflect both creative and technical long-form generation tasks. They offer a diverse set of challenges by including temporal tasks (e.g., Diary Writing)</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>that require maintaining consistent information over time and spatial tasks (e.g., Urban Planning) that test the model's ability to handle spatial relationships and detailed designs. These scenarios mirror real-world applications, from planning documents to creative writing, and thus provide a comprehensive evaluation of long-context LLMs. Table 2 offers comprehensive descriptions for each scenario, with each designed around a unique template to generate 100 different task instructions ${ }^{3}$.</p>
<p>Table 2: Scenario task descriptions</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Scenarios</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Task Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Temporal</td>
<td style="text-align: center;">Diary</td>
<td style="text-align: center;">Weekly Diary <br> Daily Diary</td>
<td style="text-align: center;">Generate entries for each week of the year <br> Generate entries for each day of the year</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Menu</td>
<td style="text-align: center;">Weekly Menu <br> Daily Menu</td>
<td style="text-align: center;">Plan menus for each week of the year <br> Plan menus for each day of the year</td>
</tr>
<tr>
<td style="text-align: center;">Spatial</td>
<td style="text-align: center;">Skyscraper Design</td>
<td style="text-align: center;">100-floor Design <br> 361-floor Design</td>
<td style="text-align: center;">Develop a design for a 100-floor skyscraper <br> Develop a design for a 300-floor skyscraper</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Urban Planning</td>
<td style="text-align: center;">10x10 block Design <br> 19x19 block Design</td>
<td style="text-align: center;">Design an urban layout on a 10x10 block grid <br> Design an urban layout on a 19x19 block grid</td>
</tr>
</tbody>
</table>
<h1>2.3 Specific Task Instruction</h1>
<p>To enhance task control and flexibility, we have developed three distinct task settings:</p>
<ul>
<li>Single Instruction $(S I)$ : Injects specific information at a unique point within the generated text.</li>
</ul>
<p>$$
T_{S}=\left{T_{s_{1}}, T_{s_{2}}, \ldots\right}
$$</p>
<ul>
<li>Range Instruction (RI): Requires the model to incorporate information within specified ranges of the text.</li>
</ul>
<p>$$
T_{R}=\left{T_{R_{t}}, T_{R_{t+1}}, \ldots, T_{R_{t+j}}\right}
$$</p>
<ul>
<li>Periodic Instruction (PI): Distributes information at predefined intervals throughout the text.</li>
</ul>
<p>$$
T_{P}=\left{T_{P_{n}}, T_{P_{2 n}}, \ldots, T_{P_{m+n}}\right}
$$</p>
<ul>
<li>Check Set: Includes tasks for all three aforementioned settings.</li>
</ul>
<p>$$
\text { Check_set }=\left{T_{S}, T_{R}, T_{P}\right}
$$</p>
<p>For example, in the design of a 100-floor skyscraper, the Single Instruction may specify that the 34th floor hosts an aerial gym and the 54th floor houses a law firm. The Range Instruction might designate floors 1 through 9 as a comprehensive shopping mall, whereas the Periodic Instruction could dictate that starting from the 20th floor, every 10th floor incorporates a small aerial garden.</p>
<p>We utilize over 20 templates for each type of instruction, with the floors or locations being randomly assigned to ensure task diversity. These settings, applied via various templates, guarantee controlled coverage across all textual positions, thus facilitating a comprehensive and efficient evaluation, as illustrated in Figure 1 (2).</p>
<p>Through this approach, we generate the main task instructions $T$ and simultaneously acquire the corresponding Check_set, which supports subsequent evaluations and constructs a task conducive to super-long-form generation. Subsequently, we splice the main task prompt with the specific task instructions (STI) ${ }^{4}$ and add the generation prompt to form the final evaluation data.</p>
<h3>2.4 evaluation Metric</h3>
<p>To quantitatively evaluate performance for LongGenBench tasks, we introduce three complementary metrics:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Main Task Completion. This metric evaluates the extent to which all designated subtasks are accomplished. The completion rate is quantified using the following equation:</p>
<p>$$
\text { Completion Rate }(\mathrm{CR})=\frac{\text { Number of Completed Subtasks }}{\text { Total Number of Subtasks }} \times 100 \%
$$</p>
<p>In this context, the numerator denotes the count of subtasks successfully executed by the model, and the denominator represents the total number of subtasks defined in the Strictly Sequential Task. For instance, does the model consistently complete a diary entry for each day without omitting any dates?</p>
<p>Specific Task Instruction Completion (STIC-1). This metric evaluates the model's adherence to specific task instructions. We calculate the completion counts for the Single Instruction (SI), Range Instruction (RI), and Periodic Instruction (PI). STIC-1 quantifies how well the model follows these instructions across subtasks, focusing on whether the instructions are correctly implemented. For example, in the Skyscraper Design task, if the model is instructed to place an aerial gym on the 34th floor and consistently places it on a different floor, it would receive a lower STIC-1 score.</p>
<p>$$
\text { STIC-1 }=\frac{\text { Single Instruction }+ \text { Range Instruction }+ \text { Periodic Instruction }}{\text { Total Number of Outputs to Specific Task Instructions }}
$$</p>
<p>Specific Task Instruction Completion-2 (STIC-2). STIC-2 provides a more granular assessment by measuring the overall completion of specific task instructions, including their presence and execution quality across all subtasks. In addition to adherence, it assesses whether the model consistently follows these instructions throughout the entire task. For instance, if the model periodically repeats certain elements but not at the required intervals, it would affect its STIC-2 score.</p>
<p>$$
\text { STIC-2 }=\frac{\text { Single Instruction }+ \text { Range Instruction }+ \text { Periodic Instruction }}{\text { Total Number of Specific Task Instructions }}
$$</p>
<p>STIC-1 is primarily concerned with the completion rate of instructions that result in sub-scenarios, focusing on whether instructions are correctly executed. In contrast, STIC-2 assesses the overall completion of the specific instruction task, including the presence of sub-scenarios and their completion status ${ }^{5}$.</p>
<h1>2.5 eVALUATIONS PiPELINE</h1>
<p>Our evaluation process follows a structured pipeline: First, we use a long-context LLM to complete the task instruction $T$, generating an answer $A$, which is then divided into sub-tasks as $A=$ $\left{A_{1}, A_{2}, \ldots, A_{n}\right}$. Next, based on the specific instructions in the check_set, we identify the relevant sub-tasks within $A$. Finally, we evaluate each sub-task by eval $\left(A_{i}, T_{i}\right)$ to compute the final completion score, as detailed in Algorithm 1. This pipeline ensures that the evaluation is both systematic and comprehensive, assessing the model's performance across different instruction settings and levels of complexity ${ }^{6}$. While LongGenBench primarily evaluates the model's ability to follow detailed instructions, future work could expand the benchmark to include more open-ended tasks that assess creativity and logical reasoning. This would provide a broader evaluation of a model's capabilities in generating coherent, engaging, and logically sound long-form text.</p>
<h2>3 EXPERIMENTS</h2>
<h3>3.1 Experimental Setup</h3>
<p>Models. We selected ten long-context large language models (LLMs), comprising eight open-source and two closed-source models. These models range in size from 7B to 72B parameters, with one featuring a Mixture of Experts (MoE) architecture. The claimed context lengths of these models vary from 32 K to 128 K tokens $^{7}$. These models were selected to represent a diverse array of architectures,</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Evaluations Pipeline
    Initialization:
    Task instructions \(\rightarrow T\)
    Tested long context \(\mathrm{LM} \rightarrow\) model
    Set of Special Task Instruction for evaluation matching \(\rightarrow\) Check_Set
    Main Process:
    Use Tested model to get Answer for \(T \rightarrow A\)
    \(A \rightarrow\left\{A_{1}, A_{2}, \ldots, A_{m}\right\}\), split into subtasks
    empty set for storing evaluations \(\rightarrow E\)
    for each \(T_{i}\) in Check_Set do
        if there is \(A_{i}\) matching \(T_{i}\) then
            \(\operatorname{eval}\left(A_{i}, T_{i}\right) \rightarrow E_{i}\)
            \(E \rightarrow\) Add \(E_{i}\) to \(E\)
        end if
    end for
    \(\sum E \rightarrow\) Score, compute the final completion score
    return Score
</code></pre></div>

<p>covering both Mixture of Experts and standard transformer designs, as well as a range of parameter sizes. This diversity ensures a comprehensive evaluation of their ability to handle long-context tasks.</p>
<p>Inference Setup. We utilized the vLLM (Kwon et al., 2023) system, which optimizes key-value (KV) cache memory for efficient large-scale inference. This system is crucial for handling long-form generation efficiently, reducing memory overhead, and maximizing inference throughput. Inferences were performed using BFloat16 precision on $8 \times$ NVIDIA A800 GPUs, employing greedy decoding to generate the outputs. This setup ensured consistency and efficiency in the inference process.</p>
<p>Task Configurations. For each scenario, we generated 800 examples at two specified lengths: 16 K tokens and 32 K tokens. The generation was based on designated templates for each model, ensuring task-specific relevance. The tasks were selected to reflect both creative and technical long-form generation challenges, such as diary writing, urban planning, and skyscraper design. To ensure the relevance of the generated content and prevent off-topic responses or refusals to answer, we prefixed each task input with a carefully curated answer prompt designed to guide the model's output. The tasks were specifically selected to test the models' ability to generate instruction-following long-form content in both creative and technical contexts. For example: In the Urban Planning task, models were tasked with generating a detailed plan for a new urban district, including descriptions of key facilities such as parks, schools, and transportation systems.</p>
<p>Evaluation Metric. We evaluated model performance using the three metrics defined in Section 2.4: Main Task Completion, Specific Task Instruction Completion-1 (STIC-1), and Specific Task Instruction Completion-2 (STIC-2). These metrics provided a comprehensive assessment of the models' ability to adhere to instructions and generate coherent long-form text.</p>
<h1>3.2 Main Result</h1>
<p>The results of the long-form text generation tasks for both Short-version (16K) and Long-version (32K) tokens are summarized in Table 3.</p>
<p>Main Task Completion. Significant disparities in performance across models primarily stem from differences in architecture and training datasets. Notably, models with varying parameter sizes, such as Llama3.1-8B-instruction (Dubey et al., 2024) (under 10 billion parameters), Qwen-72B (Yang et al., 2024) (over 20 billion parameters), and GPT-4o-mini (OpenAI, 2024a) (a closed-source model), have demonstrated superior efficacy, successfully completing most primary tasks in full. In contrast, some models struggle with these tasks, exhibiting limitations such as: 1) models responding solely to specified subtasks, neglecting others, and 2) models halting after only completing the initial task segment, despite prompts requiring full sequential subtask completion. This issue may originate from the current instructional tuning data, which could cause partial responses in complex, lengthy tasks.</p>
<p>Table 3: Long-form generation Performance of selected models evaluated at length from 16k and 32k. The weighted average score (wAvg) is the product of CR and STIC-2, used to represent the model’s final performance at the given task length. Note that the GPT-4-32K is currently closed for use, and the longest versions that can be used are the GPT-4o and GPT-4o-mini 16K output limitation.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Claimed Length</th>
<th>Short-version (16K)</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Long-version (32K)</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>CR</td>
<td>STIC-1</td>
<td>STIC-2</td>
<td>Len.</td>
<td>wAvg</td>
<td>CR</td>
<td>STIC-1</td>
<td>STIC-2</td>
<td>Len.</td>
<td>wAvg</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Models with 7-10B Parameters</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mamba-2.8B</td>
<td>2K</td>
<td>11.3%</td>
<td>23.8%</td>
<td>2.1%</td>
<td>902</td>
<td>0.2%</td>
<td>5.6%</td>
<td>29.8%</td>
<td>1.6%</td>
<td>864</td>
<td>0.1%</td>
</tr>
<tr>
<td>FILM-7B</td>
<td>32K</td>
<td>36.0%</td>
<td>9.9%</td>
<td>3.9%</td>
<td>6280</td>
<td>1.4%</td>
<td>37.4%</td>
<td>30.9%</td>
<td>10.9%</td>
<td>13775</td>
<td>4.1%</td>
</tr>
<tr>
<td>Mistrial-v0.2-7B</td>
<td>32K</td>
<td>81.8%</td>
<td>22.0%</td>
<td>17.4%</td>
<td>7296</td>
<td>14.3%</td>
<td>48.2%</td>
<td>37.0%</td>
<td>16.3%</td>
<td>16146</td>
<td>7.9%</td>
</tr>
<tr>
<td>Phi-3-mini-3.8B</td>
<td>128K</td>
<td>22.9%</td>
<td>27.6%</td>
<td>5.4%</td>
<td>4165</td>
<td>1.2%</td>
<td>7.4%</td>
<td>46.9%</td>
<td>2.4%</td>
<td>2613</td>
<td>0.2%</td>
</tr>
<tr>
<td>LLama3.1-8B</td>
<td>128K</td>
<td>93.5%</td>
<td>23.4%</td>
<td>22.0%</td>
<td>8804</td>
<td>20.6%</td>
<td>77.6%</td>
<td>26.5%</td>
<td>18.9%</td>
<td>17354</td>
<td>14.6%</td>
</tr>
<tr>
<td>Qwen2-7B</td>
<td>128K</td>
<td>60.0%</td>
<td>27.9%</td>
<td>16.1%</td>
<td>5138</td>
<td>9.7%</td>
<td>40.0%</td>
<td>31.7%</td>
<td>12.6%</td>
<td>9617</td>
<td>5.0%</td>
</tr>
<tr>
<td>FILM-7B</td>
<td>128K</td>
<td>36.0%</td>
<td>9.9%</td>
<td>3.9%</td>
<td>6280</td>
<td>1.4%</td>
<td>37.4%</td>
<td>30.9%</td>
<td>10.9%</td>
<td>13775</td>
<td>4.1%</td>
</tr>
<tr>
<td>LongWriter-llama3.1-8B</td>
<td>128K</td>
<td>46.0%</td>
<td>22.6%</td>
<td>9.8%</td>
<td>11036</td>
<td>4.5%</td>
<td>34.5%</td>
<td>33.6%</td>
<td>10.0%</td>
<td>19925</td>
<td>3.5%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Models Larger Than 20B Parameters</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mixtral-8x7B</td>
<td>32K</td>
<td>83.0%</td>
<td>35.4%</td>
<td>28.0%</td>
<td>8113</td>
<td>23.3%</td>
<td>60.5%</td>
<td>39.9%</td>
<td>22.3%</td>
<td>15839</td>
<td>13.5%</td>
</tr>
<tr>
<td>Phi-3.5-8x7B</td>
<td>128K</td>
<td>26.9%</td>
<td>46.4%</td>
<td>11.3%</td>
<td>5430</td>
<td>3.0%</td>
<td>7.4%</td>
<td>62.9%</td>
<td>6.0%</td>
<td>6633</td>
<td>0.4%</td>
</tr>
<tr>
<td>LLama3.1-70B</td>
<td>128K</td>
<td>79.3%</td>
<td>24.9%</td>
<td>21.1%</td>
<td>8055</td>
<td>16.7%</td>
<td>63.1%</td>
<td>35.8%</td>
<td>21.7%</td>
<td>15197</td>
<td>13.7%</td>
</tr>
<tr>
<td>Qwen2-72B</td>
<td>128K</td>
<td>94.3%</td>
<td>25.5%</td>
<td>24.0%</td>
<td>8013</td>
<td>22.7%</td>
<td>66.2%</td>
<td>27.5%</td>
<td>17.4%</td>
<td>19845</td>
<td>11.5%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Closed-source Model</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4o-mini</td>
<td>128K</td>
<td>97.0%</td>
<td>29.0%</td>
<td>27.9%</td>
<td>8940</td>
<td>26.9%</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>128K</td>
<td>67.2%</td>
<td>34.9%</td>
<td>19.9%</td>
<td>9055</td>
<td>12.5%</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
</tbody>
</table>
<p>Especially in GPT-4o(OpenAI, 2023), it recognizes that this task will generate a long output and only provides a few examples.</p>
<p>STIC-1 and STIC-2. The STIC-1 metric revealed strong performance in adhering to task instructions for models like Mixtral-8x7B and GPT-4o-mini, particularly in shorter sequences. However, a significant drop in STIC-2 scores for several models indicates that maintaining instruction adherence over longer text sequences remains a challenge. This performance degradation emphasizes the need for better tuning and architectural modifications to improve long-term coherence. The MoE model, Mistral-8x7B generally outperformed dense counterparts in instruction-following over extended sequences, but both model types struggled with STIC-2 in longer generations.</p>
<p>A common failure mode observed across multiple models was the tendency to forget or misinterpret instructions as the sequence length increased. For example, in the Skyscraper Design task, some models correctly described the initial few floors but deviated from the original plan as the task progressed, particularly in the 32K token setting. This highlights the memory retention issue in long-context models, which often leads to a loss of coherence and adherence to task instructions. Examples of failures where models struggled to follow instructions are provided in Appendix F.</p>
<p>Length (Number of words). We calculated the average output word count for models that consistently completed all subtasks, achieving at least an 80% completion rate in sub-scenarios, excluding data from unsuccessful attempts. Most models substantially exceeded previous benchmarks for long-form generation tasks in terms of output length. Notably, the LongWriter (Bai et al., 2024) model excelled, efficiently meeting word count requirements for each subtask. Given the results and the weighted average score (wAvg) at a sequence length of 16K, the open-source Qwen2-72B and the closed-source GPT-4o models demonstrated optimal performance. At a sequence length of 32K, the Llama3.1-8B model, outperformed models with larger parameters, highlighting its efficiency in managing extended lengths.</p>
<h3>3.3 Accuracy Trend with Varying Sequence Length</h3>
<p>As illustrated in Figure 2, there is a clear decline in model performance as output length increases. Models exhibited strong adherence to initial instructions at shorter sequence lengths, but performance gradually degraded as the text generation extended beyond the 4,000-token threshold. This degradation</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The right side of the figure illustrates the model's performance on specific instruction tasks at 16 K as sequence length increases, whereas the left side depicts performance at 32 K . All curves have been smoothed with a Moving Average.
aligns with trends identified in the NIAH dataset and underscores the challenge of maintaining instruction adherence and coherence over long outputs.</p>
<p>This deviation becomes particularly pronounced when outputs exceed 4,000 tokens, where adherence to instructions significantly diminishes, and further deterioration is observed as outputs approach 16,000 tokens. In contrast, tasks involving shorter outputs, such as those in the NIAH dataset or simpler multi-needle tasks, showed near-perfect performance, highlighting the disparity in model behavior across different sequence lengths.</p>
<p>Potential reasons for this decline include limitations in the self-attention mechanism used in transformers, which may struggle to maintain meaningful context over long sequences. Additionally, models trained with limited long-form data may overfit to shorter patterns, leading to a loss of coherence in extended generations. These findings suggest that architectural changes or improved training strategies may be necessary to overcome these challenges in future iterations of LLMs.</p>
<h1>3.4 Three Specific Task Instructions</h1>
<p>Figure 3a presents the model's performance metrics across various task types: single, range, and periodic. The model demonstrated comparable proficiency in both single and range tasks, reflecting its capability to follow direct and straightforward instructions effectively. However, the slight reduction in performance for range tasks suggests that additional complexity, such as processing multiple data points within a defined range, introduces a marginal increase in cognitive load for the model.</p>
<p>The most significant decline in performance was observed in periodic tasks, where the model struggled to interpret instructions that required recurring events, such as "every four weeks starting from week 10 ." These tasks demand a higher degree of reasoning and temporal awareness, which may challenge the model's capacity to maintain consistency over extended sequences. As a result, outcomes for periodic tasks were considerably poorer compared to single and range tasks, which have clearer and more well-defined parameters. The model's performance hierarchy can generally be summarized as single $&gt;$ range $&gt;$ periodic, highlighting the increased difficulty associated with periodic tasks. This trend underscores the need for future improvements in long-context models, particularly in handling more complex, time-based instructions.</p>
<h3>3.5 Comparison with Long-Context Input</h3>
<p>We examine the relationship between a model's ability to handle long inputs and its performance on long outputs. Specifically, we investigate whether a model's capacity to manage long-range inputs corresponds to improved performance on long-range outputs. For this analysis, we use the RULER dataset, a synthetic benchmark designed to flexibly configure sequence length and task complexity, making it ideal for comprehensive evaluations of long-context LLMs. We compare the models' performance on sequences of the same length, as shown in Figure 3b, which indicates a significant performance gap between input handling and output performance. At 16 K tokens, the Pearson correlation coefficient is 0.51 , while at 32 K tokens, it increases to 0.66 , suggesting that there is some</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The left Fig displays the models' performance on three different task settings, with the red line representing the average for each category. The right fig shows the performance and correlation of the Ruler and LongGenBench at the same length settings.
overlap in the skills required for managing long inputs and generating long outputs, but these tasks are not entirely equivalent.</p>
<p>Handling long inputs primarily requires the model to retain and process existing information, while generating long outputs demands more complex reasoning, memory retention, and coherence management over extended sequences. Thus, models that excel in long-input retrieval may still struggle with long-form generation, particularly in tasks requiring strict instruction adherence over time. This distinction highlights the need for models to be optimized for both input handling and output generation to achieve consistent performance in long-context tasks.</p>
<h1>4 ANALYSIS AND LIMITATIONS</h1>
<p>Richness of Content. Despite efforts to design sub-scenarios that enhance task diversity and richness, the model's outputs tend to converge as output volume increases. This results in a homogenization of recorded events, even when differences in time and location should introduce variety. Such convergence not only degrades overall performance but also diminishes the diversity of the generated content, leading to repetitive and predictable outputs. In our experiments, approximately $45 \%$ of long outputs exhibited significant repetition, even when the model was given varied time or location prompts. Adjusting parameters like repetition_penalty during inference has shown limited success in mitigating this issue, highlighting the need for more advanced techniques to maintain content richness over long sequences.</p>
<p>Rationality of Content. While our current research focuses primarily on evaluating instructionfollowing capabilities, a more comprehensive analysis of content rationality and coherence is needed. For example, when tasked with generating a diary, the model should ensure that all recorded activities align with the specified careers. However, in many instances, this logical consistency is lacking. Additionally, temperature records in virtual diary entries often fail to reflect realistic temporal changes. For instance, in a San Francisco's diary task, we would expect temperatures to vary from cooler ( $0-10$ degrees Celsius) at the beginning of the year to warmer (20-30 degrees Celsius) by mid-year. Yet, the model consistently generates warmer temperatures throughout, even into December. These issues may arise due to the model's limited exposure to temporally varied datasets, particularly in diary or climate-related contexts. Future work could address this by incorporating more domain-specific and temporally annotated data during fine-tuning.</p>
<p>Instruction Data. A significant performance discrepancy between models' abilities to handle long-range inputs (such as Ruler (Hsieh et al., 2024)) and their long-form output generation can likely be attributed to the length distribution of instruction-tuning data. Most instruction-tuning datasets are brief, typically under 200 tokens, and lack the extended instructional content necessary for generating</p>
<p>longer outputs. This gap suggests that organizing or synthesizing instruction-tuning data with longer, more comprehensive examples could be a valuable direction for future research. Potential solutions include applying transfer learning techniques from models trained on long-form datasets or using data augmentation methods to synthesize longer instructional content from existing short-form data.</p>
<p>Generalizability. LongGenBench effectively evaluates instruction-following in creative and technical tasks but may not fully capture the creativity and specialized knowledge required for abstract reasoning or unconstrained storytelling. Future versions could include open-ended tasks like creative fiction writing and legal document drafting, which demand intricate narratives and precision. Expanding in this direction would enhance the benchmark's versatility while providing deeper insights into LLMs' capabilities. However, LongGenBench's current focus on instruction adherence offers a strong foundation for evaluating practical, instruction-driven long-form text generation.</p>
<h1>5 Related Work</h1>
<p>Instruction Following. Recent advances in instruction tuning models (Ouyang et al., 2022; Rafailov et al., 2024; OpenAI, 2022; Taori et al., 2023; Chiang et al., 2023) have underscored the need for scalable evaluation methods. LLMs have been used as evaluators, showing better alignment with human judgments than traditional metrics like BLEU (Papineni et al., 2002). However, LLM evaluations suffer from biases, such as sensitivity to presentation order and preference for verbose outputs (Wang et al., 2024; Pezeshkpour \&amp; Hruschka, 2023; Zheng et al., 2023). To mitigate these biases, meta-evaluation benchmarks like FairEval, MT-Bench, and LLMEval ${ }^{2}$ (Wang et al., 2024; Zheng et al., 2023; Zhang et al., 2023) have been proposed. While recent studies have focused on improving LLM evaluations with diverse strategies (Zheng et al., 2023; Li et al., 2023; Zhang et al., 2023; Chan et al., 2023), they typically do not address longer context lengths.</p>
<p>Long-context Benchmarks and Tasks. Existing benchmarks focus on models handling long inputs. For instance, ZeroSCROLLS (Shaham et al., 2023) and LongBench (Bai et al., 2023) tackle tasks like long-document QA and query-based summarization. Synthetic benchmarks, like NeedleBench (Li et al., 2024) and Ruler (Hsieh et al., 2024), offer better control over variables such as sequence length and complexity. NeedleBench introduces the Ancestral Trace Challenge (ATC), while Ruler evaluates models across tasks like NIAH and multi-hop tracing. However, these benchmarks largely focus on input comprehension and do not assess long-form text generation, which is the primary focus of LongGenBench.</p>
<p>Long-form Text Generation. Research in long-form generation spans applications like story generation (Fan et al., 2019c; Xu et al., 2020), paragraph completion (Kang \&amp; Hovy, 2020), sustained conversation (Xu et al., 2022), and comprehensive QA (Fan et al., 2019a; Dasigi et al., 2021; Stelmakh et al., 2022; Lee et al., 2023). However, existing models and evaluation methods (Liu et al., 2023; Chiang \&amp; Lee, 2023; Liu et al., 2024; Bai et al., 2024) face challenges in maintaining quality over long outputs, often being limited by shorter text lengths (typically under 2000 tokens) (Shen et al., 2023). Recent work (Tan et al., 2024) seeks to improve evaluation criteria, but the gap between model capabilities and benchmark text lengths remains. In contrast, LongGenBench evaluates models on their ability to handle much longer sequences, with tasks requiring adherence to instructions over extended outputs ( $16 \mathrm{~K}+$ tokens).</p>
<h2>6 Conclusion</h2>
<p>We introduced LongGenBench, a synthetic benchmark that evaluates long-form generation capabilities of language models by testing their ability to follow instructions over extended sequences. In evaluating nine advanced models with context sizes ranging from 32 K to 128 K tokens, we observed significant performance degradation compared to benchmarks like "Ruler" with common failure modes including premature task termination, incomplete responses, disregard for instructions, and repetitive content generation. These results highlight key challenges for current models in handling long-form tasks and underscore the need for advancements in model architecture and training data to improve coherence, instruction adherence, and content diversity over extended outputs.</p>
<h1>REFERENCES</h1>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, and Jian-Guang Lou. Make your llm fully utilize the context. arXiv preprint arXiv:2404.16811, 2024.</p>
<p>Anthropic. Introducing claude 2.1, 2024a. URL https://www.anthropic.com/index/ claude-2-1. Accessed: 2024-01-23.</p>
<p>Anthropic. Introducing the next generation of claude, 2024b. URL https://www. anthropic. com/news/claude-3-family. Accessed: 2024-03-27.</p>
<p>Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv preprint arXiv:2408.07055, 2024.</p>
<p>Yushi Bai et al. LongBench: A bilingual, multitask benchmark for long context understanding. arXiv:2308.14508, 2023.</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023.</p>
<p>David Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 15607-15631. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.870. URL https: //doi.org/10.18653/v1/2023.acl-long. 870.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/.</p>
<p>Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 4599-4610. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.NAACL-MAIN.365. URL https: //doi.org/10.18653/v1/2021.naacl-main.365.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Iryna Gurevych and Yusuke Miyao (eds.), Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 889-898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://aclanthology.org/P18-1082.</p>
<p>Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: long form question answering. In Anna Korhonen, David R. Traum, and Lluís Márquez (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 3558-3567. Association for Computational Linguistics, 2019a. doi: 10.18653/V1/P19-1346. URL https://doi.org/ 10.18653/v1/p19-1346.</p>
<p>Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Anna Korhonen, David Traum, and Lluís Márquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3558-3567, Florence, Italy, July 2019b. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/P19-1346.</p>
<p>Angela Fan, Mike Lewis, and Yann N. Dauphin. Strategies for structuring story generation. In Anna Korhonen, David R. Traum, and Lluís Márquez (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 2650-2660. Association for Computational Linguistics, 2019c. doi: 10.18653/V1/P19-1254. URL https://doi.org/10.18653/v1/p19-1254.</p>
<p>Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: What's the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024.</p>
<p>Zhe Hu, Hou Pong Chan, Jiachen Liu, Xinyan Xiao, Hua Wu, and Lifu Huang. PLANET: dynamic content planning in autoregressive transformers for long-form text generation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 2288-2305. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.163. URL https://doi.org/10.18653/v1/2022. acl-long. 163 .</p>
<p>Xinyu Hua and Lu Wang. PAIR: planning and iterative refinement in pre-trained transformers for long text generation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 781-793. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.EMNLP-MAIN.57. URL https://doi.org/10. 18653/v1/2020.emnlp-main.57.</p>
<p>Albert Q Jiang et al. Mixtral of experts. arXiv:2401.04088, 2024.
Gregory Kamradt. Needle In A Haystack - pressure testing LLMs. Github, 2023. URL https: //github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main.</p>
<p>Dongyeop Kang and Eduard H. Hovy. Plan ahead: Self-supervised text planning for paragraph completion task. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 6533-6543. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.EMNLP-MAIN.529. URL https://doi.org/10.18653/v1/2020. emnlp-main. 529 .</p>
<p>Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan A Rossi, Franck Dernoncourt, Hanieh Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, et al. Longlamp: A benchmark for personalized long-form text generation. arXiv preprint arXiv:2407.11016, 2024.</p>
<p>Woosuk Kwon et al. Efficient memory management for large language model serving with paged attention. In Proc. of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.</p>
<p>Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-In Lee, and Moontae Lee. QASA: advanced question answering on scientific articles. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 19036-19052. PMLR, 2023. URL https://proceedings.mlr.press/v202/lee23n.html.</p>
<p>Mo Li, Songyang Zhang, Yunxin Liu, and Kai Chen. Needlebench: Can llms do retrieval and reasoning in 1 million context window? arXiv preprint arXiv:2407.11963, 2024.</p>
<p>Ruosen Li, Teerth Patel, and Xinya Du. Prd: Peer rank and discussion improve large language model based evaluations. arXiv preprint arXiv:2307.02762, 2023.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 2511-2522. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.emnlp-main.153.</p>
<p>Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, and Nigel Collier. Aligning with human judgement: The role of pairwise preference in large language model evaluators. arXiv preprint arXiv:2403.16950, 2024.</p>
<p>Mistral.AI. La plateforme, 2023. URL https://mistral.ai/news/la-plateforme/.
OpenAI. ChatGPT, 2022. URL https://chat.openai.com.
OpenAI. GPT-4 Technical Report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.</p>
<p>OpenAI. Gpt-4o mini: Advancing cost-efficient intelligence, 2024a. URL https://openai.com/ index/gpt-4o-mini-advancing-cost-efficient-intelligence/. Accessed: 2024-08-31.</p>
<p>OpenAI. Hello gpt-4o, 2024b. URL https://openai.com/index/hello-gpt-4o/. Accessed: 2024-08-31.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311-318, 2002.</p>
<p>Pouya Pezeshkpour and Estevam Hruschka. Large language models sensitivity to the order of options in multiple-choice questions. arXiv preprint arXiv:2308.11483, 2023.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. ZeroSCROLLS: A zero-shot benchmark for long text understanding. In EMNLP, 2023.</p>
<p>Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and Lidong Bing. Large language models are not yet human-level evaluators for abstractive summarization. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pp. 4215-4233. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.findings-emnlp. 278.</p>
<p>Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: factoid questions meet longform answers. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 8273-8288. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN.566. URL https://doi.org/10 . 18653/v1/2022.emnlp-main. 566.</p>
<p>Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, and Linqi Song. Proxyqa: An alternative framework for evaluating long-form text generation with large language models. arXiv preprint arXiv:2401.15042, 2024.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94409450, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https : //aclanthology.org/2024.acl-long.511.</p>
<p>Thomas Wolf et al. Huggingface's Transformers: State-of-the-art natural language processing. arXiv:1910.03771, 2019.</p>
<p>Jing Xu, Arthur Szlam, and Jason Weston. Beyond goldfish memory: Long-term opendomain conversation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 5180-5197. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.356. URL https://doi.org/10.18653/v1/2022.acl-long.356.</p>
<p>Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul Puri, Pascale Fung, Anima Anandkumar, and Bryan Catanzaro. MEGATRON-CNTRL: controllable story generation with external knowledge using large-scale language models. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 2831-2845. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.EMNLP-MAIN.226. URL https : //doi.org/10.18653/v1/2020.emnlp-main. 226.</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024.</p>
<p>Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862, 2023.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https : //openreview.net/forum?id=uccHPGDlao.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.</p>
<h1>A Symbol Definitions and Descriptions</h1>
<p>This table 4 presents definitions and descriptions for various symbols used in task-related contexts, providing an overview of the key terminologies and their roles.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Symbol</th>
<th style="text-align: center;">Definition</th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$T$</td>
<td style="text-align: center;">Main Task</td>
<td style="text-align: center;">The primary goal or task to be completed, such as designing a skyscraper or writing a diary.</td>
</tr>
<tr>
<td style="text-align: center;">$T_{i}$</td>
<td style="text-align: center;">Subtask</td>
<td style="text-align: center;">A smaller portion of the main task, each responsible for a specific part, e.g., designing a specific floor.</td>
</tr>
<tr>
<td style="text-align: center;">$T_{S}$</td>
<td style="text-align: center;">Single Instruction Task</td>
<td style="text-align: center;">A task requiring the model to inject specific information at a unique point in the generated text.</td>
</tr>
<tr>
<td style="text-align: center;">$T_{R}$</td>
<td style="text-align: center;">Range Instruction Task</td>
<td style="text-align: center;">A task requiring the model to incorporate information within a specified range of the generated content.</td>
</tr>
<tr>
<td style="text-align: center;">$T_{P}$</td>
<td style="text-align: center;">Periodic Instruction Task</td>
<td style="text-align: center;">A task that distributes specific information at predetermined intervals throughout the text.</td>
</tr>
<tr>
<td style="text-align: center;">$T_{S_{i}}$</td>
<td style="text-align: center;">Single Instruction Task i</td>
<td style="text-align: center;">Represents an individual task from the Single Instruction Task set, focusing on a specific point in the text.</td>
</tr>
<tr>
<td style="text-align: center;">$T_{R_{i}}$</td>
<td style="text-align: center;">Range Instruction Task i</td>
<td style="text-align: center;">Represents an individual task from the Range Instruction Task set, applied across a specific range.</td>
</tr>
<tr>
<td style="text-align: center;">$T_{P_{i}}$</td>
<td style="text-align: center;">Periodic Instruction Task i</td>
<td style="text-align: center;">Represents an individual task from the Periodic Instruction Task set, recurring periodically throughout the text.</td>
</tr>
<tr>
<td style="text-align: center;">$\overline{C R}$</td>
<td style="text-align: center;">Completion Rate</td>
<td style="text-align: center;">The percentage of successfully completed subtasks out of the total number of subtasks, used to evaluate task performance.</td>
</tr>
<tr>
<td style="text-align: center;">$\overline{S T I C-1}$</td>
<td style="text-align: center;">Specific Task Instruction Completion-1</td>
<td style="text-align: center;">Evaluates how well the model follows specific task instructions, including Single, Range, and Periodic Instructions. Focuses on whether the instructions are executed correctly.</td>
</tr>
<tr>
<td style="text-align: center;">$\overline{S T I C-2}$</td>
<td style="text-align: center;">Specific Task Instruction Completion-2</td>
<td style="text-align: center;">Provides a more granular assessment, measuring not only adherence to instructions but also the consistency of execution throughout all subtasks. It looks at both presence and execution quality.</td>
</tr>
<tr>
<td style="text-align: center;">$A$</td>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">Represents the complete response generated by the model for the main task.</td>
</tr>
<tr>
<td style="text-align: center;">$A_{i}$</td>
<td style="text-align: center;">Subtask Answer</td>
<td style="text-align: center;">Represents the specific answer or output generated for an individual subtask, corresponding to $T_{i}$.</td>
</tr>
</tbody>
</table>
<p>Table 4: Symbol Definitions and Descriptions</p>
<h2>B PROMPT TEMPLATES FOR FOUR TASK SCENARIOS</h2>
<p>Below are the example templates for the four task scenarios: Diary Writing, Menu Design, Skyscraper Design, and Urban Planning. In the Diary Writing template, professions and names are customizable variables, allowing for flexibility in the generated content.</p>
<h1>Diary for 2018</h1>
<p>Emma is a photographer with a passion for chronicling her vibrant life through weekly diary entries. Captures:</p>
<p>1) Single Instruction (SI): Birthdays of family members, wedding anniversaries, etc.;
2) Range Instruction (RI): Family beach vacation in Maui, Week-long road trip across the Pacific Coast Highway.
3) Periodic Instruction (PI): Attend golf lessons at the local club, Join a weekend hiking group, etc.;
4) Weekly updates on weather changes, work developments, family life, and other interesting topics.
5) Use '#*#' to separate each weekly entry (e.g. example)</p>
<p>Generate a complete weekly diary for Emma for the entire year of 2018. Start from January 1st, a Monday, marking the first week, and continue through to December 31st, the end of the 52nd week. Ensure that the diary consists of 52 entries, one for each week. Each diary entry should be at least 200 words. When the design of all 52 weeks is complete, use '<strong><em> finished </em></strong>' to indicate the end of the document. Ensure clarity and continuity without any interruptions or omissions in the narrative throughout the year.
*** started ***
#*# Week 1 (January 1st - January 7th):</p>
<h2>Menu for 2018</h2>
<p>As Chef Roy, a world-renowned chef at a globally renowned restaurant, you are tasked with designing an entire year's menu for 2018. Your menu should be varied and innovative, adhering to the following guidelines:</p>
<p>1) Single Instruction (SI): ("Independence Day Celebration", "2018-07-04", "American Apple Pie"), ("Summer Solstice Celebration", None, "Midsummer Night's Fish Fry"), etc;
2) Range Instruction (RI): ("Mushroom Season Specials", "Various Mushroom Dishes"), "Seafood Season Instructions", "Fresh Seafood Dishes", etc.;
3) Periodic Instruction (PI): ("Seafood Fridays", 2, "Fish and Chips"), ("Monthly Steak Night", 3, "Prime Ribeye Steak"), etc.;
4) Use '#<em>#' to separate each weekly menu (e.g. example) Generate a comprehensive weekly menu diary for the entire year of 2018, start from January 1st, a Monday, marking the first week, and continuing until December 31, the end of the 52nd week. Ensure that the diary consists of 52 entries, one for each week. Each weekly menu must include a detailed description of the offerings, featuring at least two options for appetizers, main courses, side dishes, desserts, and drinks. Ideally, between 200 and 220 words per menu description to ensure thoroughness and richness of detail. Conclude the diary with '</em>*<em> finished' to signify the completion of the year's menu planning. Ensure clarity and continuity without any interruptions or omissions in the menu throughout the year.
*** started ***
#</em># Menu Week 1 (January 1st - January 7th):",</p>
<h2>Skyscraper Design</h2>
<p>Construct a skyscraper with 100 floors. Please follow the detailed floor assignments below:</p>
<p>1) Single Instruction (SI): office, conference room, retail store, etc;
2) Range Instruction (RI): hospital with various departments, corporate headquarters for a major company, etc.;
3) Periodic Instruction (PI): outdoor terrace, sky garden, etc.;
4) Document each floor independently with detailed descriptions of the intended facilities, architectural features, and unique design elements.
5) Use '#*#' to separate the documentation for each floor (e.g. example).</p>
<p>Ensure that the document consists of 100 entries, each containing at least 150 words. Ensure clarity and continuity without any interruptions or omissions in the narrative throughout the document. When the design of all 100 floors is complete, use '**<em> finished' to indicate the end of the document.
*** started ***
#</em># Floor 1:</p>
<h1>Urban Planning</h1>
<p>Design a vibrant and diverse city using a 10x10 block grid, numbered from 1 to 100. Arrange the blocks sequentially from left to right and top to bottom. Ensure that each block is uniquely planned to reflect a wide array of city facilities, highlighting the rich urban environment and cultural diversity.</p>
<p>1) Single Instruction (SI): theater, museum, etc.;
2) Large Instruction (LI): shopping, fashion, industrial park, etc.;
3) Periodic Instruction (PI): public restroom, convenience store, etc.;
4) Document each block independently with detailed descriptions of the intended facilities, architectural features, and unique design elements.
5) Use '#*#' to separate the documentation for each block like (e.g. example)</p>
<p>Ensure that the document consists of 100 entries, each containing at least 150 words. Ensure that the document contains detailed descriptions for each block, with a minimum of 150 words per description. Ensure clarity and continuity in the narrative throughout the document without any interruptions or omissions. When all block assignments are complete, use '**<em> finished' to indicate the end of the document.
*** started ***
#</em>#Block $1(0,0)$ :</p>
<h2>C Evaluation pipeline</h2>
<p>The evaluation pipeline is designed to systematically assess the ability of long-context language models (LLMs) to follow specific, complex instructions. The process can be summarized in three key steps:</p>
<h2>Step 1. Generation of Outputs from the Long-context LLM</h2>
<p>Given an input task $(T)$ that describes a set of instructions, we prompt the LLM to generate detailed outputs. The output $(A)$ comprises a list of descriptions, represented as:</p>
<p>$$
A=\left{A_{1}, A_{2}, \ldots, A_{n}\right}
$$</p>
<h2>Example: Given the prompt (ref Appendix B)</h2>
<p>Construct a skyscraper with 100 floors. The floor assignments are detailed as follows:</p>
<ul>
<li>Specific floor requirement: Designate Floor 11 for a small art gallery.</li>
<li>Range floor requirement: Allocate Floors 32 to 39 for corporate headquarters of a major company.</li>
<li>...</li>
</ul>
<p>The LLM generates a response describing each floor in detail, such as:</p>
<h2>Answer:</h2>
<ul>
<li>Floor 1: ... Lobby ...</li>
<li>Floor 11: ... Small art gallery ...</li>
<li>Floor 32: ... Corporate headquarters ...</li>
<li>Floor $n$ : . .</li>
</ul>
<p>Step 2. Extracting and Matching Relevant Floor Assignments (Check Set)
From the initial input ("T"), we create a check set containing specific floor assignments to verify if the LLM correctly follows the instructions.</p>
<p>For the example above, the check set includes:</p>
<h1>Check Set:</h1>
<ul>
<li>Floor 11: Small art gallery</li>
<li>Floor 32: Corporate headquarters</li>
<li>Floor 33: Corporate headquarters</li>
<li>...</li>
</ul>
<p>We then extract the relevant parts of the LLM output ("A") that correspond to the floor assignments described in the check set.</p>
<p>Step 3. Evaluation Using Llama 3.1-8B instruction Model</p>
<p>For each extracted pair, we use the Llama 3.1-8B model to evaluate whether the output (" $A_{t}$ ") for a given task segment (" $T_{s i}$ ") has correctly fulfilled the specified instruction.</p>
<p>This evaluation task is framed as a simple binary classification problem, which aims to determine if the specific instruction was fulfilled ("yes" or "no"). The prompt used for this evaluation is as follows:</p>
<h2>Evaluation Prompts</h2>
<ul>
<li>Example 1: XXXX Answer: Analysis $+# * #$ Yes</li>
<li>Example 2: XXXX Answer: Analysis $+# * #$ No</li>
</ul>
<p>Context: Long-context model output: "Floor 11: . . . small art gallery . . ."
Instructions: Does this context include 'small art gallery'?
Answer: Please refer to the above example, provide your analysis, and respond with either #<em># Yes or #</em># No.</p>
<p>Notably, this binary evaluation is straightforward. We manually labeled 300 data points, and the model's output matched human evaluations for all cases.</p>
<p>By using this process, we transform the evaluation of long-context text generation into multiple evaluations of smaller segments. This enables systematic and thorough verification of how well the LLM follows the instructions for each specific task (as detailed in the check set).</p>
<h2>D Models</h2>
<p>In this benchmark, we evaluated ten LLMs, including both open-source and closed-source models. These models vary in parameter size and context length capabilities, which are crucial factors in their performance on long-form text generation tasks. The key details for each model are outlined in Table 5. These include closed-source models like GPT-4o-mini and GPT-4o, which support a context length of 128 K tokens and serve as state-of-the-art baselines for long-context handling. Open-source models, such as Llama3.1-8B and Llama3.1-70B, offer similar context lengths and represent the latest in large-scale, open-access LLMs. Qwen2-7B and Qwen2-72B, developed by Qwen, also support 128 K tokens and handle complex long-text tasks. Additionally, we evaluated Mixture of Experts (MoE) models like Mistral-v0.2 and Mixtral-8x7B, both with context lengths of 32K tokens, focusing on memory efficiency and scalability. FILM-7B, designed for creative and technical tasks, supports 128 K tokens and excels in generating detailed, context-rich content. Finally, Longwrite-llama3.1-8B, based on Llama3.1, is optimized for long-form narrative tasks with a context window of 128 K tokens. Together, these models offer a diverse representation of advancements in long-context LLMs, showcasing their ability to handle long-form, instruction-driven generation tasks.</p>
<p>Table 5: Information of evaluated and analyzed models in LongGenBench.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Aligned</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Context Length</th>
<th style="text-align: center;">Huggingface (Wolf et al., 2019) / API</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4o-mini (OpenAI, 2024a)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">128 K</td>
<td style="text-align: center;">gpt-4-mini</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o (OpenAI, 2024b)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">128 K</td>
<td style="text-align: center;">gpt-4o-2024-08-06</td>
</tr>
<tr>
<td style="text-align: left;">Llama3.1-8B-Instruct (Dubey et al., 2024)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">8 B</td>
<td style="text-align: center;">128 K</td>
<td style="text-align: center;">meta-llama/Meta-Llama-3.1-8B-Instruct</td>
</tr>
<tr>
<td style="text-align: left;">Llama3.1-72B-Instruct (Dubey et al., 2024)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">70 B</td>
<td style="text-align: center;">128 K</td>
<td style="text-align: center;">meta-llama/Meta-Llama-3.1-70B-Instruct</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2-7B-Instruct (Yang et al., 2024)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">7 B</td>
<td style="text-align: center;">128 K</td>
<td style="text-align: center;">Qwen/Qwen2-7B-Instruct</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2-72B-Instruct (Yang et al., 2024)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">72 B</td>
<td style="text-align: center;">128 K</td>
<td style="text-align: center;">Qwen/Qwen2-72B-Instruct</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-v0.2 (Mistral.AI, 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">7 B</td>
<td style="text-align: center;">32 K</td>
<td style="text-align: center;">mistralai/Mistral-7B-Instruct-v0.2</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-8x7B (Jiang et al., 2024)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">8 x 7 B</td>
<td style="text-align: center;">32 K</td>
<td style="text-align: center;">mistralai/Mistral-8x22B-Instruct-v0.1</td>
</tr>
<tr>
<td style="text-align: left;">FILM-7B (An et al., 2024)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">7 B</td>
<td style="text-align: center;">128 K</td>
<td style="text-align: center;">In2Training/FILM-7B</td>
</tr>
<tr>
<td style="text-align: left;">Longwrite-llama3.1-8B (Bai et al., 2024)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">8 B</td>
<td style="text-align: center;">128 K</td>
<td style="text-align: center;">THUDM/LongWriter-llama3.1-8b</td>
</tr>
</tbody>
</table>
<h1>E Explanation of Metrics</h1>
<h2>E. 1 STIC-1 AND STIC-2</h2>
<p>We appreciate the feedback and have provided an example using the results from Table 3 of our experiments (specifically comparing LLaMA3.1-8B and Qwen2 under the short-version setting).
As shown in Table 6, Qwen2's STIC-1 score is higher than that of LLaMA3.1-8B, while its STIC-2 score is lower. This difference can be attributed to the Completion Rate (CR) of each model. Qwen2 has a significantly lower CR compared to LLaMA3.1-8B. Specifically, Qwen2 typically achieves around $60 \%$ completion for tasks (e.g., designing a 100 -story skyscraper but stopping at roughly 60 stories). On the other hand, LLaMA3.1-8B generally completes around 93 layers ( $93 \%$ completion).</p>
<p>In the case of STIC-1, we are evaluating the correctness of the output based on the number of layers that are actually generated. Qwen2 demonstrates a higher completion rate when the denominator consists of the 60 layers it has output (compared to LLaMA3.1-8B, which has a denominator of 93 layers).</p>
<p>For STIC-2, however, we consider the entirety of the expected output. Since Qwen2 lacks the remaining 30 layers, the STIC-2 score is lower when the denominator becomes the entire requirement (as the missing output significantly affects its score).</p>
<p>As mentioned in our paper, STIC-2 is designed to take into account a more comprehensive perspective on output completeness. We are considering simplifying our metrics by using only STIC-2, as it may be easier to understand and provide a more holistic evaluation.</p>
<p>Table 6: Comparison of STIC-1 and STIC-2 Scores</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Length</th>
<th style="text-align: center;">CR</th>
<th style="text-align: center;">STIC-1</th>
<th style="text-align: center;">STIC-2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLaMA3.1-8B</td>
<td style="text-align: center;">128 K</td>
<td style="text-align: center;">$93.5 \%$</td>
<td style="text-align: center;">$23.4 \%$</td>
<td style="text-align: center;">$22.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2-7B</td>
<td style="text-align: center;">128 K</td>
<td style="text-align: center;">$60.0 \%$</td>
<td style="text-align: center;">$27.9 \%$</td>
<td style="text-align: center;">$16.1 \%$</td>
</tr>
</tbody>
</table>
<h2>E. 2 Example explanation</h2>
<p>To further illustrate the concepts, we have constructed a 3-level building for illustration ${ }^{8}$ :</p>
<ul>
<li>Consider a 3-level building with the following constraints:</li>
<li>$T_{S_{1}}$ : "Floor 1 must have a coffee shop."</li>
<li>$T_{S .2}$ : "Floor 1 must have a reception desk."</li>
<li>$T_{P}:\left{T_{P_{1}}, T_{P_{2}}, T_{P_{3}}\right}$, where each $T_{P_{i}}$ means "Floor i must have a washroom."</li>
</ul>
<p>The model generates the following output:</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>"floor1: coffee shop, washroom; floor2: washroom."</p>
<p>In this scenario, the check_set is $\left{T_{S_{1}}, T_{S_{2}}, T_{P_{1}}, T_{P_{2}}, T_{P_{3}}\right}$. Note that $T_{P}$ applies to all three floors, requiring separate evaluation for each $T P_{i}$.</p>
<p>With the current model output, the completion rate (CR) for the main task is $2 / 3$. Although the task requires outputs for three floors, the model only provided outputs for two floors.</p>
<p>For STIC-1, we consider how accurately the model has outputted information at the floor level. Since the model output only contains two floors, we evaluate the constraints for these two floors to determine if they are fully met. For these two floors, the constraints are $T_{S_{1}}, T_{S_{2}}, T_{P_{1}}, T_{P_{2}}$, totaling 4 constraints. The model has correctly fulfilled 3 out of these 4 requirements, resulting in STIC-1 of $3 / 4$.</p>
<p>For STIC-2, we evaluate the entire check_set, which consists of $T_{S_{1}}, T_{S_{2}}, T_{P_{1}}, T_{P_{2}}, T_{P_{3}}$. The model has fulfilled 3 out of these 5 requirements, so STIC-2 equals $3 / 5$.</p>
<p>The distinction between STIC-1 and STIC-2 allows identification of the specific reasons for any drop in performance. It helps determine whether the issue lies in the model's inability to follow instructions for a given output or whether it lacks a complete output in the first place. For example, in the case of a lower STIC-2, the low score may be due to incorrect outputs for some floors or due to incomplete outputs for the floors. In such cases, CR and STIC-1 can be used together to further evaluate and make informed judgments.</p>
<h1>F Error Analysis</h1>
<p>We showcase two error generated output that highlight the common issues faced by large language models in maintaining coherence and instruction-following over extended sequences. In the Diary for 2018 scenario, while the model initially adheres to the instructions for weeks 6 and 7, it progressively diverges from the given timeline, misplacing events and generating off-topic content by week 15 and beyond. Similarly, in the Skyscraper Design task, the model starts with correct descriptions for specific floors, such as Floor 11 designated as an art gallery, but it fails to maintain consistency with the subsequent instructions, particularly in the range and periodic requirements for floors 32-39 and every 15th floor. These examples illustrate the challenges long-context models face in sustaining adherence to task instructions, particularly as sequence length increases.</p>
<h1>Diary for 2018</h1>
<p>Noah is a photographer with a passion for chronicling his vibrant life through weekly diary entries. Captures in daily:</p>
<p>1) Family member birthday: husband (yourself) (birthday on March 08), wife (birthday on July 09), child_1 (birthday on November 06).
2) Joining a fitness and wellness retreat in Bali in week 6-7.
3) Join a local running club for weekend marathons every $\mathbf{5}$ weeks on weekends, starting from week 10 .
$\qquad$
$======================$ Output $======================$
$\qquad$
Week 6 (February 5th - February 11th): Noah had joined a fitness and wellness retreat in Bali in this week. $\qquad$ $\checkmark$
Week 7 (February 12th - February 18th): Continuing in Bali, I spent the week participating in various workshops and seminars. .... $\checkmark$
$\qquad$
Week 15 (April 9th - April 15th): The family is in full spring mode, with kids spending time outside, exploring, and enjoying the nice weather. ..... $X$
$\qquad$
Week 35 (August 27th - September 2nd): The summer heat has subsided, and we've been enjoying the milder weather 2013 going on hikes, playing in the park, and exploring our neighborhood. .... $X$
$\qquad$
Week 45 (November 5th - November 11th): The kids have been enjoying their school routines, and we've been making the most of our evenings together. ..... $X$</p>
<h2>Skyscraper Design</h2>
<p>Construct a skyscraper with 100 floors. Please follow the detailed floor assignments below:</p>
<p>1) Specific floors requirements: Designate Floor $\mathbf{1 1}$ for small art gallery use. ....
2) Range floors requirements: Allocate Floors $\mathbf{3 2}$ to $\mathbf{3 9}$ for corporate headquarters for a major company.
3) Periodic floors requirements: Include a sky garden every $\mathbf{1 5}$ floors, starting from Floor $\mathbf{3 0}$. ..... $\qquad$
$======================$ Output $======================$
$\qquad$
Floor 11: Designated for art gallery use, Floor 11 is a sophisticated and flexible space designed to celebrate visual arts..... $\checkmark$
$\qquad$
Floor 32: Floor 32 serves dual purposes, housing a renowned photography studio and corporate offices. ..... $X$
$\qquad$
Floor 34: Transitioning into a leisure space, Floor 34 hosts a small cinema, providing an exclusive entertainment venue within the skyscraper. ..... $X$
$\qquad$
Floor 60: This floor houses a luxury watch and timepiece atelier, celebrating the art of horology and fine craftsmanship. ..... $X$
$\qquad$
Floor 90: Floor 90 offers a dynamic e-commerce and digital marketing center focused on online business innovation and consumer engagement strategies. ..... $X$</p>
<h2>G Different Prompt Format Compare</h2>
<p>The two prompt formats differ primarily in the structure and arrangement of the instructions within the prompt. In Prompt - 1, the order follows a sequence of Single Instruction (SI), Range Instruction (RI), and Periodic Instruction (PI). Conversely, in Prompt - 2, this order is altered by swapping the positions of SI, RI, and PI. Additionally, the Generate prompt, which is a critical component of the task, was rewritten in Prompt - 2 by GPT-4.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ This example is adapted from ICLR reviewer bHUs. We are deeply grateful for their insightful comment and in-depth discussion, which significantly improved the clarity of this paper.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
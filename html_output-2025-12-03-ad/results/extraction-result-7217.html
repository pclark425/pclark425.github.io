<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7217 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7217</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7217</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-264452026</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.findings-emnlp.717.pdf" target="_blank">HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Theory of Mind (ToM) is the ability to reason about one's own and others' mental states. ToM plays a critical role in the development of intelligence, language understanding, and cognitive processes. While previous work has primarily focused on first and second-order ToM, we explore higher-order ToM, which involves recursive reasoning on others' beliefs. We introduce HI-TOM, a Higher Order Theory of Mind benchmark. Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs. We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7217.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7217.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art instruction-tuned large language model from OpenAI based on the Transformer architecture, used here as an off-the-shelf evaluator of higher-order Theory of Mind (ToM) on the HI-TOM benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large Transformer-based language model from OpenAI, used via API in a zero-shot multiple-choice evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>HI-TOM (Higher-order Theory of Mind benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>theory of mind / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice stories (Sally-Anne-like) with 0th–4th order ToM questions per story; measures ability to reason about agents' beliefs (including deceptive communications) across recursive belief orders.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard accuracy and joint accuracy (joint requires all lower-order answers in the same story to be correct).</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy for GPT-4 on HI-TOM is under 60% (all evaluated LLMs <60%); near-perfect on 0th-order questions; drops sharply with order — GPT-4 achieves >20% on 4th-order (standard accuracy) but joint accuracy approaches near-zero at higher orders; performance degrades with story length and with deceptive communications.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot; two prompting styles evaluated: Vanilla Prompting (VP, multiple-choice without explanation) and Chain-of-Thought Prompting (CoTP, step-by-step explanation).</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Key observations: CoTP provided insignificant gains and sometimes hurt performance with deception; GPT-4 shows position bias (better when correct answer is first or last container); GPT-4 often repeats first-order answers across higher orders (72.4% match on 2nd/3rd/4th with its 1st-order predictions), suggesting superficial strategies; error analyses report types such as insufficient reasoning depth, commonsense errors, hallucinations, temporal ignorance, and spurious causal inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7217.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7217.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI instruction-tuned large language model (GPT-3.5 series) evaluated off-the-shelf on the HI-TOM higher-order ToM benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned Transformer-based LLM from OpenAI, evaluated zero-shot on multiple-choice ToM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>HI-TOM (Higher-order Theory of Mind benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>theory of mind / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same HI-TOM multiple-choice stories (0th–4th order) used to probe recursive belief reasoning, including deceptive communications in some stories.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard accuracy and joint accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy below 60% (same overall bound reported for all tested LLMs); joint and standard accuracies decline substantially with increasing ToM order and story length. GPT-3.5 shows higher frequencies of commonsense errors, hallucinations, and spurious causal inference relative to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot; Vanilla Prompting (VP) and Chain-of-Thought Prompting (CoTP) evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>CoTP did not substantially improve performance; error analysis (300 step-by-step responses sampled across GPT-4 and GPT-3.5) indicates GPT-3.5 has more hallucinations and spurious causal inference than GPT-4; performance patterns (degradation with order, effect of deception, position bias) align with GPT-4 but at lower absolute performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7217.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7217.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-following LLM from Anthropic evaluated on the HI-TOM higher-order ToM benchmark under zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude (instant variant reported as Claude-instant in supplement)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based instruction-tuned language model from Anthropic, used off-the-shelf to answer HI-TOM multiple-choice ToM questions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>HI-TOM (Higher-order Theory of Mind benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>theory of mind / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice HI-TOM stories probing 0th–4th order recursive belief reasoning, with and without deceptive communications.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard accuracy and joint accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported joint/standard accuracies follow the general trend of decline with ToM order; Claude's overall joint accuracy performance is reported as better than Guanaco but lower than GPT-4; all evaluated models remain below 60% overall accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot; evaluated under both VP and CoTP prompting styles.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Under VP prompting Claude exhibited roughly uniform ~50% performance across orders in some settings (reported in supplement), and showed the same position bias (better for first/last containers) as GPT-4; performance deteriorates with deceptive communications and higher ToM order.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7217.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7217.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Guanaco 65B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Guanaco (65B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open/academic LLM (Guanaco 65B) evaluated on HI-TOM to benchmark higher-order ToM performance relative to other models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Guanaco 65B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large Transformer-based language model (65B parameter variant) evaluated zero-shot on the HI-TOM benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>HI-TOM (Higher-order Theory of Mind benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>theory of mind / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>HI-TOM multiple-choice stories with 0th–4th order questions; includes stories with deceptive agent communications to stress-test belief reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard accuracy and joint accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Joint and standard accuracies drop with increasing ToM order and story length; Guanaco's overall joint accuracy is worse than Claude and GPT models; all evaluated LLMs reported below 60% overall accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot; evaluated with Vanilla Prompting and Chain-of-Thought Prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Supplementary figures (Figure 9) show Guanaco's joint accuracy trends aligning with GPT-4 and GPT-3.5 (decline with order and story length); Claude outperforms Guanaco in joint accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural theory-of-mind? on the limits of social intelligence in large lms <em>(Rating: 2)</em></li>
                <li>Revisiting the evaluation of theory of mind through question answering <em>(Rating: 2)</em></li>
                <li>Theory of mind may have spontaneously emerged in large language models <em>(Rating: 2)</em></li>
                <li>Large language models fail on trivial alterations to theory-of-mind tasks <em>(Rating: 2)</em></li>
                <li>Towards a holistic landscape of situated theory of mind in large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7217",
    "paper_id": "paper-264452026",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A state-of-the-art instruction-tuned large language model from OpenAI based on the Transformer architecture, used here as an off-the-shelf evaluator of higher-order Theory of Mind (ToM) on the HI-TOM benchmark.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Instruction-tuned large Transformer-based language model from OpenAI, used via API in a zero-shot multiple-choice evaluation.",
            "model_size": null,
            "test_name": "HI-TOM (Higher-order Theory of Mind benchmark)",
            "test_category": "theory of mind / reasoning",
            "test_description": "Multiple-choice stories (Sally-Anne-like) with 0th–4th order ToM questions per story; measures ability to reason about agents' beliefs (including deceptive communications) across recursive belief orders.",
            "evaluation_metric": "Standard accuracy and joint accuracy (joint requires all lower-order answers in the same story to be correct).",
            "human_performance": null,
            "llm_performance": "Overall accuracy for GPT-4 on HI-TOM is under 60% (all evaluated LLMs &lt;60%); near-perfect on 0th-order questions; drops sharply with order — GPT-4 achieves &gt;20% on 4th-order (standard accuracy) but joint accuracy approaches near-zero at higher orders; performance degrades with story length and with deceptive communications.",
            "prompting_method": "Zero-shot; two prompting styles evaluated: Vanilla Prompting (VP, multiple-choice without explanation) and Chain-of-Thought Prompting (CoTP, step-by-step explanation).",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Key observations: CoTP provided insignificant gains and sometimes hurt performance with deception; GPT-4 shows position bias (better when correct answer is first or last container); GPT-4 often repeats first-order answers across higher orders (72.4% match on 2nd/3rd/4th with its 1st-order predictions), suggesting superficial strategies; error analyses report types such as insufficient reasoning depth, commonsense errors, hallucinations, temporal ignorance, and spurious causal inference.",
            "uuid": "e7217.0",
            "source_info": {
                "paper_title": "HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "GPT-3.5-Turbo",
            "brief_description": "An OpenAI instruction-tuned large language model (GPT-3.5 series) evaluated off-the-shelf on the HI-TOM higher-order ToM benchmark.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "Instruction-tuned Transformer-based LLM from OpenAI, evaluated zero-shot on multiple-choice ToM tasks.",
            "model_size": null,
            "test_name": "HI-TOM (Higher-order Theory of Mind benchmark)",
            "test_category": "theory of mind / reasoning",
            "test_description": "Same HI-TOM multiple-choice stories (0th–4th order) used to probe recursive belief reasoning, including deceptive communications in some stories.",
            "evaluation_metric": "Standard accuracy and joint accuracy.",
            "human_performance": null,
            "llm_performance": "Overall accuracy below 60% (same overall bound reported for all tested LLMs); joint and standard accuracies decline substantially with increasing ToM order and story length. GPT-3.5 shows higher frequencies of commonsense errors, hallucinations, and spurious causal inference relative to GPT-4.",
            "prompting_method": "Zero-shot; Vanilla Prompting (VP) and Chain-of-Thought Prompting (CoTP) evaluated.",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "CoTP did not substantially improve performance; error analysis (300 step-by-step responses sampled across GPT-4 and GPT-3.5) indicates GPT-3.5 has more hallucinations and spurious causal inference than GPT-4; performance patterns (degradation with order, effect of deception, position bias) align with GPT-4 but at lower absolute performance.",
            "uuid": "e7217.1",
            "source_info": {
                "paper_title": "HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Claude",
            "name_full": "Claude (Anthropic)",
            "brief_description": "An instruction-following LLM from Anthropic evaluated on the HI-TOM higher-order ToM benchmark under zero-shot settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude (instant variant reported as Claude-instant in supplement)",
            "model_description": "Transformer-based instruction-tuned language model from Anthropic, used off-the-shelf to answer HI-TOM multiple-choice ToM questions.",
            "model_size": null,
            "test_name": "HI-TOM (Higher-order Theory of Mind benchmark)",
            "test_category": "theory of mind / reasoning",
            "test_description": "Multiple-choice HI-TOM stories probing 0th–4th order recursive belief reasoning, with and without deceptive communications.",
            "evaluation_metric": "Standard accuracy and joint accuracy.",
            "human_performance": null,
            "llm_performance": "Reported joint/standard accuracies follow the general trend of decline with ToM order; Claude's overall joint accuracy performance is reported as better than Guanaco but lower than GPT-4; all evaluated models remain below 60% overall accuracy.",
            "prompting_method": "Zero-shot; evaluated under both VP and CoTP prompting styles.",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Under VP prompting Claude exhibited roughly uniform ~50% performance across orders in some settings (reported in supplement), and showed the same position bias (better for first/last containers) as GPT-4; performance deteriorates with deceptive communications and higher ToM order.",
            "uuid": "e7217.2",
            "source_info": {
                "paper_title": "HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Guanaco 65B",
            "name_full": "Guanaco (65B)",
            "brief_description": "An open/academic LLM (Guanaco 65B) evaluated on HI-TOM to benchmark higher-order ToM performance relative to other models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Guanaco 65B",
            "model_description": "A large Transformer-based language model (65B parameter variant) evaluated zero-shot on the HI-TOM benchmark.",
            "model_size": "65B",
            "test_name": "HI-TOM (Higher-order Theory of Mind benchmark)",
            "test_category": "theory of mind / reasoning",
            "test_description": "HI-TOM multiple-choice stories with 0th–4th order questions; includes stories with deceptive agent communications to stress-test belief reasoning.",
            "evaluation_metric": "Standard accuracy and joint accuracy.",
            "human_performance": null,
            "llm_performance": "Joint and standard accuracies drop with increasing ToM order and story length; Guanaco's overall joint accuracy is worse than Claude and GPT models; all evaluated LLMs reported below 60% overall accuracy.",
            "prompting_method": "Zero-shot; evaluated with Vanilla Prompting and Chain-of-Thought Prompting.",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Supplementary figures (Figure 9) show Guanaco's joint accuracy trends aligning with GPT-4 and GPT-3.5 (decline with order and story length); Claude outperforms Guanaco in joint accuracy.",
            "uuid": "e7217.3",
            "source_info": {
                "paper_title": "HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural theory-of-mind? on the limits of social intelligence in large lms",
            "rating": 2,
            "sanitized_title": "neural_theoryofmind_on_the_limits_of_social_intelligence_in_large_lms"
        },
        {
            "paper_title": "Revisiting the evaluation of theory of mind through question answering",
            "rating": 2,
            "sanitized_title": "revisiting_the_evaluation_of_theory_of_mind_through_question_answering"
        },
        {
            "paper_title": "Theory of mind may have spontaneously emerged in large language models",
            "rating": 2,
            "sanitized_title": "theory_of_mind_may_have_spontaneously_emerged_in_large_language_models"
        },
        {
            "paper_title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "rating": 2,
            "sanitized_title": "large_language_models_fail_on_trivial_alterations_to_theoryofmind_tasks"
        },
        {
            "paper_title": "Towards a holistic landscape of situated theory of mind in large language models",
            "rating": 2,
            "sanitized_title": "towards_a_holistic_landscape_of_situated_theory_of_mind_in_large_language_models"
        }
    ],
    "cost": 0.0103985,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models</p>
<p>Yinghui He 
University of Michigan Westlake University</p>
<p>Yufan Wu 
University of Michigan Westlake University</p>
<p>Yilin Jia 
University of Michigan Westlake University</p>
<p>Rada Mihalcea mihalcea@umich.edu 
University of Michigan Westlake University</p>
<p>Yulong Chen 
University of Michigan Westlake University</p>
<p>Naihao Deng 
University of Michigan Westlake University</p>
<p>HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models
760E86CFE2ED6A3962E0D89AAD21AA12
Theory of Mind (ToM) is the ability to reason about one's own and others' mental states.ToM plays a critical role in the development of intelligence, language understanding, and cognitive processes.While previous work has primarily focused on first and second-order ToM, we explore higher-order ToM, which involves recursive reasoning on others' beliefs.We introduce HI-TOM, a Higher Order Theory of Mind benchmark.Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higherorder ToM tasks, demonstrating the limitations of current LLMs.We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP.</p>
<p>Introduction</p>
<p>Theory of Mind (ToM) refers to the ability to understand and reason about the mental states of others such as intentions and beliefs, and also to distinguish them from one's own (Premack and Woodruff, 1978).Such an ability has been considered a crucial point in the development of intelligence functions (Premack and Woodruff, 1978;Bretherton and Beeghly, 1982;Frith and Frith, 2003), and previous research has demonstrated that ToM reasoning is highly related to linguistic and cognitive processes (Perner, 1991;Sperber and Wilson, 2002).ToM has thus been widely used as a protocol to evaluate the language understanding and reasoning ability of intelligence agents (Premack and Woodruff, 1978;Takano et al., 2006), such as young children (Osterhaus and Koerber, 2021).</p>
<p>With the recent advance in large language models (LLMs), research has been undertaken to evaluate the language skills of LLMs using ToM (Sap et al., 2022;Ullman, 2023).Most of the previous work has been confined to first-order and secondorder ToM, where LLMs are asked to perform inference on others' belief of reality in one or two passes, e.g., the first and second-order questions in Figure 2 (see Section 2 for a more comprehensive discussion of ToM background and the evaluation of ToM in LLMs).</p>
<p>Higher-order ToM, referring to third-order reasoning and beyond, requires recursive reasoning on others' beliefs in multiple passes.Figure 1 shows a higher-order ToM example from the TV series Friends.In Figure 1, a character says "They don't know that we know they know we know" when she and the other character try to recursively identify the situation.Such an example underscores that human beings are capable of higher-order ToM in daily interactions.In addition, evidence shows that higher-order ToM is not only essential to communicate effectively in complicated scenarios, such as multi-party conversations (Liddle and Nettle, 2006;De Weerd et al., 2015;Ridinger and McBride, 2017;De Weerd et al., 2022), but it also enables better emotional support and empathetic communication (Mitchell and Phillips, 2015).However, because of a lack of higher-order ToM datasets in the NLP community, there is significantly less research on higher-order ToM compared to the lower orders.</p>
<p>Previous work has mainly constructed ToM The milk is in the fridge!(Sally secretly told Anne)</p>
<p>The milk is on the table! Figure 2: A sample from HI-TOM dataset, which contains communications among agents, and questions that address 0-th (reality) to 3-rd ToM reasoning.. benchmarks using automatic story generation scripts.Although simple and inexpensive, this method cannot be directly extended to generating stories of higher-order ToMs because the generated stories contain insufficient information for raising a higher-order question.In this paper, we build upon previous work and introduce HI-TOM, a multiplechoice question benchmark consisting of Sally-Anne-like stories (Figure 2), specifically designed for higher-order ToM evaluation.Unlike previous datasets, HI-TOM contains questions from zerothorder to fourth-order ToM, and incorporates agent communications in the stories.We manually check the quality of the constructed data, and empirically find that HI-TOM presents greater diversity and challenges compared to previous datasets.
Milk Table Box
We experiment with various LLMs, including GPT-4 (OpenAI, 2023), GPT-3.5-turbo (OpenAI, 2022), Claude, and Guanaco (Dettmers et al., 2023), on HI-TOM under a zero-shot setting.Furthermore, we test the chain-of-thought prompting (Wei et al., 2022) and conduct a thorough analysis of LLMs' performances on different story types in HI-TOM and their failure cases.Our work demonstrates that the claim of LLMs having genuine ToM abilities (Kosinski, 2023;Bubeck et al., 2023) is questionable, especially in the cases of higher-order ToM, where several rounds of recursive reasoning are required.To our knowledge, we are the first to introduce a benchmark for evaluat-ing higher-order ToM reasoning and analyzing the abilities of current LLMs on high-order ToM.Furthermore, we share our thoughts on the future of NLP and the way forward with LLMs of enhancing LLMs from the perspective of human intelligence, understanding humans through the lens of LLMs, and enhancing LLMs' ToM abilities for better NLP applications.We release our dataset and code at https://github.com/ying-hui-he/Hi-ToM_dataset.</p>
<p>Background and Related Work</p>
<p>Theory of Mind.Most of prior work focuses on first or second-order ToM (Nematzadeh et al., 2018;Le et al., 2019;Sap et al., 2022), while higherorder ToM (third-order and beyond) remains underexplored.The concept of "orders" refers to the number of mental state attributions that are required to answer a particular question or reason about a particular scenario.For instance, a third-order ToM question can be "Where does Anne think that Sally thinks that Isabella searches for the milk?",where Sally's reasoning about Isabella is of second-order, and Anna's reasoning on Sally's reasoning is of third-order.</p>
<p>Higher-order ToM is useful in social interaction such as maintaining social networks (Liddle and Nettle, 2006), winning limited bidding (de Weerd and Verheij, 2011), efficiently cooperation (De Weerd et al., 2015;Ridinger and McBride, 2017), and unpredictable negotiations (De Weerd et al., 2022).Researchers from cognitive science investigate second-order and higher-order ToM among young children via complex forms of falsebelief tests, such as the Sally-Anne false-belief experiment (Baron-Cohen et al., 1985).</p>
<p>Evaluating ToM in LLMs.Sap et al. (2022) find that GPT-3's ToM ability is well below humans on the TOMI dataset (Le et al., 2019), which is a ToM evaluation dataset consisting of questions up to the second order.Kosinski (2023); Bubeck et al. (2023) show the promising performance of recent LLMs such as GPT-3.5 and GPT-4 on ToM tasks.However, it is questionable whether LLMs have genuine ToM ability, especially for higher-order ToM.Ullman (2023) find that for GPT-3.5, small variations that maintain the principles of ToM can cause a flip of the answer.Different from previous work that only evaluates LLMs' ToM ability up to the second order, we take a step forward and evaluate LLMs' ability in higher-order ToM settings.Also, we are the first one pioneering in adding the deceptive communication protocol in ToM setups, which takes an initial step toward evaluating LLMs' ability in real-world scenarios.Concurrent to our work, Ma et al. (2023) surveyed the existing ToM benchmarks and conducted preliminary experiments on situated evaluation of ToM for LLMs.</p>
<p>The HI-TOM Dataset</p>
<p>To systematically examine how effectively LLMs reason Theory of Mind (ToM) at different orders, each story is coupled with five questions that require the zeroth to fourth level of ToM reasoning, respectively.Following Nematzadeh et al. (2018) and Le et al. (2019), we automatically generate HI-TOM stories.Additionally, we manually review the generated stories, questions, and answers to ensure that they are consistent with each other, and they are logically correct.</p>
<p>Dataset Design</p>
<p>Story Design.HI-TOM stories consist of four fundamental elements: rooms, objects, containers, and agents, as shown in Table 1.A story narrates events occurring in one or more rooms, where multiple objects are placed inside their respective containers.Each story features five rational agents.Each story comprises one to three chapters.Each chapter corresponds to a single round in the objectfinding game.In each chapter, we design multiple actions and optional communication protocols among agents:</p>
<p>• Entry: At least one agent enters one room, where they observe all the objects, other agents, and their actions in that room (e.g., Figure 2 Scene 1).</p>
<p>• Object Movement: When in a room, each agent can choose whether to move an object before their exit.Such actions are done in a sequential manner.In other words, the later agent can only perform such an action after the former agent  moves the object (or not) and leaves the room (Scenes 2, 3, and 4).</p>
<p>• Agent communication: Outside the room, agents may be involved in two types of communications: public, where an agent shares information with every agent, and private, where an agent only speaks to another agent privately, or they can remain silent (Scene 5).</p>
<p>For agent communications, we set the shared information to be deceptive in order to emulate the dynamics of the complicated social life.It also adds another layer of complexity to the ToM reasoning process, which requires the answerers to not only reason about an agent's perceptions of other agents knowledge of the objects location, but also reason about whether an agent would trust another agent.In this way, we evolved the simplistic toy stories in the previous dataset (Le et al., 2019), and solved a core problem in the previous evaluations that the answers may be simply found from the objects original or final location.</p>
<p>Moreover, we pose a constraint that the listener would update their world knowledge based on the information given by the speaker if the speaker exits the room later than the listener.This is based on the assumption that the listener would be unaware of any changes after their exit, but the speaker might possess more up-to-date knowledge as they leave later.Additionally, we assume that Alex and Sally, who give out information publicly or privately, will believe that all the listeners trust their respective information.A full assumption list is attached to each story, as shown in Table 7 in Appendix B.1.</p>
<p>Each chapter involves entry and object movement, while agent communication is optional.In HI-TOM, half of the stories have at least one chapter with agent communications, while the other half only contains chapters without communications.</p>
<p>Question-Answer Design.Following Le et al. (2019), for each story, we provide five questions that are progressively built from lower-order questions to higher ones as shown in Table 3.</p>
<p>Order Question
0th
Where is O really?1st</p>
<p>Where does A1 think O is? 2nd</p>
<p>Where does A2 think A1 thinks O is? 3rd</p>
<p>Where does A3 think A2 thinks A1 thinks O is?
4th
Where does A4 think A3 thinks A2 thinks A1 thinks O is?</p>
<p>Table 3: Questions asked in a story involving object O and five agents.A1 to A4 are randomly chosen from the five agents.0th, 1st, 2nd, 3rd, and 4th represent the ToM orders of the questions.</p>
<p>Following Sap et al. (2022), we adopt the multiple-choice setting and provide the correct answer along with several distractor choices.</p>
<p>Data Generation</p>
<p>To generate the aforementioned ToM stories with higher-order questions among agents, we adapt the generation scripts from Nematzadeh et al. (2018), which are originally limited to first or second-order ToM stories.</p>
<p>Our script takes a list of story components Rooms, Objects, Containers, Agents, as well as the number of chapters as inputs, and outputs a story with chapters along with five questions from zeroth to fourth order ToM.</p>
<p>For the generation of each chapter, we randomly choose the story components and fit them into the chapter template.Specifically, we randomly determine whether or not each agent moves the object to another container.Then, we incorporate agent communications in certain chapters, where we use the phrases "publicly claim" and "privately tell" to encode public and private communications.</p>
<p>To generate the questions and answers for each story, we integrate the relevant story components into a predefined question template.Subsequently, we utilize an answer generator to track the actions  2019), we further incorporate distractor sentences that relate an agent with a random container, such as "Jack likes the red_container".This reduces the regularity and predictability of the stories.Table 2 shows an example one-chapter story with agent communications and random distractors.</p>
<p>Dataset Characteristics</p>
<p>Table 4 shows a comparison between HI-TOM and the other ToM datasets.First, unlike previous datasets, HI-TOM is the only benchmark that contains third and fourth-order stories, which suggests that HI-TOM is more challenging and requires higher-order ToM reasoning.Also, we first introduce communications among agents, which poses greater challenges to LLMs to reason about human interactions.In addition, stories in HI-TOM are significantly longer, with a larger number of agents and containers per story.This requires the LLMs' capability to comprehend the complete storyline and reason about each agent's beliefs.</p>
<p>Notably, HI-TOM features a larger pool of potential answers and a balanced distribution of correct answers throughout the story.In ToM/ToM-easy and TOMI, all the correct answers appear within the last two containers or the only two containers in the corresponding story.In contrast, the proportions of correct answers appearing in the first, second, third, and final quarters of the HI-TOM stories are 28.7%, 27.2%, 18.8%, and 25.3%, respectively.The even distribution of correct answers eliminates the position bias of correct answers being concentrated in specific segments of the stories in HI-TOM.</p>
<p>Experimental Setup</p>
<p>Models</p>
<p>We evaluate the following four LLMs on HI-TOM:</p>
<ol>
<li>GPT-3.5-Turbo (OpenAI, 2022) and GPT-4 (OpenAI, 2023)  We adhere to the default parameter configurations across all the examined language models.</li>
</ol>
<p>Methods</p>
<p>For each HI-TOM story, we conduct trials using two prompting styles: Vanilla Prompting (VP) and Chain-of-Thought Prompting (CoTP).In VP prompting, the model needs to pick the best answer from a given set of options without explanation.CoTP prompting requires the model to offer a stepby-step explanation of its thought process along with the answer.Appendix C.1 provides an example (Table 8) and more details of our prompting methods.</p>
<p>Evaluation</p>
<p>We evaluate the model performance using both standard accuracy (hereafter referred to as accuracy) and joint accuracy.Adapted from (Le et al., 2019), joint accuracy represents a more stringent metric than standard accuracy.It considers an answer as correct only when the related question, along with all preceding, lower-order questions within the same story are answered correctly.For instance, the third-order question in Table 3 is considered correct only if the model correctly answers the zeroth, first, second, and third-order questions above it.Joint accuracy effectively reveals the model's genuine ability in higher-order ToM reasoning, as the model may only reason the higherorder ToM correctly if it is able to reason the lowerorder ToM because the higher-order question is a † www.anthropic.recursive successor of the lower-order ones for the same story.</p>
<p>Experimental Results</p>
<p>Table 5 presents the accuracy scores of the four LLMs.All the LLMs we evaluate exhibit less than 60% accuracy scores, demonstrating that HI-TOM is challenging even for the most sophisticated LLMs. Figure 3 depicts the joint accuracy scores of GPT-4 and GPT-3.5 under various settings.As the story length decreases or the ToM order increases, LLMs' performance decreases across various settings.In addition, LLMs perform worse when there are deceptive agent communications involved in the story.The trend observed in Guanaco and Claude aligns with that of GPT-4 and GPT-3.5, as shown in Appendix C.2.The experimental results also reveal the following noteworthy patterns:</p>
<p>CoTP prompting yields insignificant performance gains.We observe no substantial improvement in accuracy when transitioning from VP to CoTP in 5. Furthermore, in the assessments involving stories with deception, the switch in prompting methods even leads to a decrease in accuracy.We hypothesize that as there are more steps involved, there are higher chances of deceptive information misleading steps in between.The chain may then amplify the error in that step, leading to a cascade of errors throughout the reasoning process.) signifies accuracy results on stories with deception, while other results pertain to non-deceptive stories.Increased ToM order leads to decreased performances.As the ToM order increases from the zeroth to the fourth, the joint accuracy goes from near perfect to near zero.We also observe the drastic decline in the conventional accuracy scores in Appendix C.2.</p>
<p>LLMs' performance decreases as there are more deception communications involved.The performance drops when deception communications are involved.Table 5 and Figure 3 reveals a worse performance on stories with agent communications.</p>
<p>To further investigate the models' reasoning abilities in handling deceptive agent communications, we plot the resulting accuracy versus the number of deceptive communication sentences ("deception times") per story for GPT-4 in Figure 4.As shown in Figure 4, as deception times increase from 0 to 4, the joint accuracy experiences drops of 32%, 18.1%, 16%, and 11% respectively for the four ToM orders.This suggests that the deceptive agent communications challenge the LLMs in their ToM reasoning process.</p>
<p>6 Discussion and Analyses</p>
<p>Underlying Patterns in Correct LLM Predictions</p>
<p>Although the overall accuracy of the models leaves room for improvement, we observe a higher frequency of correct model choices under specific conditions.We thus examine the scenarios where models have higher answer accuracy.</p>
<p>LLMs handle answers that appear at the beginning and end better.When dealing with long three-chapter stories, LLMs frequently overlook key information, such as the movement of a specific container or agent conversations.Yet, they tend to pay special attention to the beginning and the end of the story.</p>
<p>In Figure 5, we highlight GPT-4's higher performance when the correct answer aligns with the first or last container mentioned in the story, as compared to other cases, as demonstrated by the higher values on the diagonal.This suggests that LLMs are better at handling answers that appear at the beginning or at the end.In contrast, The accuracy when the correct answers are the middle containers (i.e.neither the first nor the last) is similar to those that are not in those containers, as shown in Figure 14 in Appendix C.2.We observe similar patterns of the Claude model focusing on the beginning and end of stories, as shown in Figure 15 in Appendix C.2.Our findings about position bias in LLMs align with other works on LLMs (Wang et al., 2023).LLMs perform better if the answers across orders are the same.We observe that LLMs perform better on question sets where the higher-order answer coincides with a lower-order answer.In Figure 6, we see a clear performance disparity between LLMs answering correctly if the answers are the same across orders versus the answers being different across orders.However, this may result from LLMs' tendency to predict the same answers across orders.We find that 72.4%, 64.6%, and 59.8% of GPT-4's second, third, and fourth-order answers match their first-order responses.In contrast, only 30.9%, 20.9%, and 22.2% of the corresponding correct answers in HI-TOM are the same as their first-order answers.This suggests that GPT-4's enhanced performance on certain questions may be due to the coincidence of correct answers across different ToM orders.</p>
<p>Classifying Reasoning Errors</p>
<p>To provide a comprehensive overview of the failure cases of LLMs in ToM reasoning, we manually evaluate a total of 300 step-by-step responses across all ToM orders by ourselves, comprising 150 from each of GPT-4 and GPT-3.5.Table 6 describes the five most prevalent error types with corresponding examples.Figure 7 provides the frequencies of these errors in GPT-4's responses across different orders.We also show the results for GPT-3.5 and do a comparison between the two LLMs in Appendix C.2.As the ToM order increases, LLMs tend to demonstrate a higher frequency of errors.Here we provide hypotheses and discussions for each of the error types: Insufficient reasoning depth.We notice that LLMs tend to skip steps in their reasoning process and end up with an answer to a lower-order question, as we observed earlier in Section 6.1.One reason can be that the pre-training corpus often  consists of simple patterns rather than complex and nuanced reasoning scenarios, leading to its frequent simplification of the questions.In addition, LLMs may possess a limited contextual understanding of the story.They may struggle to retain and integrate information from multiple steps or make connections across different parts of the text, leading to oversimplification of the question.</p>
<p>Commonsense errors.LLMs have demonstrated remarkable performance on standard benchmarks of commonsense reasoning (Bian et al., 2023).However, when it comes to ToM reasoning, even advanced models like GPT-4 are prone to making mistakes in handling commonsense knowledge.</p>
<p>One key aspect that contributes to these errors is the disparity between the models' knowledge of commonsense facts and their ability to effectively apply that knowledge in the complex reasoning process.While LLMs may possess a vast amount of explicit commonsense knowledge, they can struggle to appropriately utilize this knowledge while avoiding overgeneralization.In addition, the frequent commonsense errors in HI-TOM might be due to that HI-TOM is newly constructed, and therefore LLMs have never seen such data before.In contrast, LLMs' pre-training corpus might contain the data in the publicly available commonsense benchmarks, leading to the high performances on LLMs on those benchmarks (Magar and Schwartz, 2022).</p>
<p>Hallucinations.</p>
<p>Hallucination is a wellknown phenomenon in LLMs' generation process (McKenna et al., 2023).In our experiments, LLMs may have relied on superficial cues and statistical associations to answer the questions, rather than gaining a solid understanding of the underlying context and meaning.Hence, they might resort to fabricating baseless details to bridge the logic gap between the true story and their erroneous responses.</p>
<p>Error Types Description Example</p>
<dl>
<dt>Insufficient Reasoning-Depth Oversimplify the question and skip the required multistep reasoning.</dt>
<dd>
<p>Where does Jack think Hannah thinks William thinks the carrot is?</p>
</dd>
<dd>
<p>green_box, as that's where Jack last saw it.</p>
</dd>
</dl>
<p>Commonsense Errors</p>
<p>Generate outputs that violate common sense.: . . .Aiden exited the pantry after step 8, but he can still witness the move in the pantry after the exit, so . . .</p>
<p>Hallucinations</p>
<p>Fabricate ungrounded details or facts.(In the story, Benjamin saw a cat, but did not talk about it) : . . .But there's another twist.Suppose Ella also learns that Benjamin lied about seeing a cat to distract everyone from his real plan . . .Temporal Ignorance Confuse or ignore the temporal order of events.: . . .Lily exited the hallway (step 8) after Amelia moved the corn to the red_basket (step 11), . . .</p>
<p>Spurious Causal Inference</p>
<dl>
<dt>Attribute a cause-and-effect relationship between unrelated events.</dt>
<dd>
<p>. . .Carter privately told Emma that the tomato is in the green_drawer.Private communications are not heard by others, so Emma has no reason to doubt Carter's information.</p>
</dd>
</dl>
<p>Table 6: Types of reasoning errors commonly made by LLMs, with their description and example erroneous responses ( ) to questions ( ) from our experiment results on GPT-4.Lack of temporal information.We observe that LLMs' understanding of the sequence of agent actions is often skewed, as the actions are closely listed in HI-TOM stories.This confusion in temporal order is also found in Yuan et al. (2023).This error may be attributed to biases inherent in the pre-training corpus which leads to LLMs' lack of genuine understanding of temporal relations.Spurious Causal Inference.The current learning paradigm for LLMs is designed to capture the statistical correlations among the data (Devlin et al., 2019).Through such a paradigm, it is difficult for LLMs to capture the underlying logic behind these correlations (Jin et al., 2023).As a result, LLMs may make incorrect or misleading causal inferences based solely on these superficial patterns.</p>
<p>Implications on the Future of NLP</p>
<p>We believe our work has important implications on the future directions of NLP with respect to the two-way relation between artificial intelligence and human intelligence.</p>
<p>Enhance LLMs' ToM ability from the perspective of human intelligence.According to Kahneman (2011), human decisions are supported and guided by the cooperation of two capabilities or two systems: System 1 for intuitive, imprecise, fast, and often unconscious decisions ("thinking fast"), and System 2 for more complex situations with logical and rational thinking ("thinking slow").This theory has inspired works in computer vision and natural language processing communities to explicitly equip models with the two systems (Hill et al., 2021;Miech et al., 2021).</p>
<p>Through our examination of LLMs' ToM ability, we find failure cases that resemble the characteristics of System 1 thinking; for instance, LLMs may invent causes and intentions ("hallucination"), or substitute an easier question for a difficult one ("insufficient reasoning-depth").Furthermore, the significant performance drop from zeroth to fourth order ToM in HI-TOM suggests that LLMs may be more inclined to System 1 thinking rather than System 2 thinking, as higher-order ToM requires careful in-depth logical inference.</p>
<p>However, there exists a line of research combining the symbolic reasoning process (Simon and Newell, 1971;Winograd, 1971), which aligns with System 2 thinking, with connectionist paradigm or neural learning (Rumelhart et al., 1986;Le-Cun et al., 2015), which captures the intuitive and pattern recognition aspects of System 1 thinking (Shavlik, 1994;Hitzler, 2022).This integration holds the promise of enabling AI systems to perform complex tasks that require both logical deduction and statistical generalization.We believe our findings of ToM limitations of LLMs alongside this previous line of research clearly points to a direction where neural and symbolic approaches are combined in order to achieve abilities that are more closely aligned to human intelligence.</p>
<p>Understanding humans through the lens of LLMs.ToM plays a crucial role in understanding human intelligence, as it is an important aspect of human cognition that enables us to make inferences about others' thoughts, emotions, and behaviors.Enabling progress in ToM reasoning in LLMs entails progress in emulating the functioning of human mind, which in turn offers intriguing possibilities for gaining insights into human interactions and the emergence of intelligence.While it is important to recognize that the analogy between human and artificial intelligence has its limitations and is a subject of debate within the NLP community (Bender et al., 2021), recent research has explored the extrinsic understanding of human interactions through multi-agent systems (Park et al., 2023).This approach allows us to observe how LLMs can mimic and simulate aspects of human behavior and communication.By studying LLMs and their intrinsic properties, such as the emergence of intelligence, we can gain valuable insights into the fundamental processes underlying human cognition (Wijmans et al., 2023).Researchers have also developed methods to elicit human-like behavior from LLMs, providing further opportunities to explore and understand the capabilities and limitations of these models (Belrose et al., 2023).While LLMs offer a close-up view of human-like language processing, it is crucial to approach the topic with caution and recognize the complexities and nuances of human intelligence and behavior.</p>
<p>Enhance LLMs' ToM abilities for better NLP applications.In daily life, our ToM ability plays a vital role in understanding others' intentions, therefore helping us in our communication.In HI-TOM, we enable higher-order ToM reasoning, which in turn can lead to improvements in LLMs performance on tasks such as deception detection, emotional support, empathetic communication, and others.Additionally, since LLMs represent foundational models that are used across various NLP tasks and applications, enhancing the abilities of LLMs opens up exciting possibilities for improving specific NLP tasks that benefit from these models.</p>
<p>Conclusion</p>
<p>In this paper, we introduce HI-TOM, the first ToM benchmark that contains higher-order ToM tasks.We demonstrated that LLMs' performance suffers a significant drop in ToM tasks from lower to higher order.By proposing HI-TOM, we hope to address the challenges of ToM in complicated scenarios and spark further research on enhancing the reasoning ability of LLMs.</p>
<p>Furthermore, we present our insights on the future of NLP and discuss potential directions for enhancing LLMs.Our aim is to stimulate research that draws inspiration from human intelligence, strives to understand humans better, and ultimately leads to the development of NLP applications that better cater to the needs of humans.B HI-TOM Details</p>
<p>B.1 Assumptions</p>
<p>Our simplified deception and belief mechanisms are based on four assumptions.Table 7 shows the original assumption list we attach to each story in the dataset and prompt into LLMs.</p>
<p>Note: You should assume the following.</p>
<p>(1) An agent witnesses everything and every movement before exiting a room.</p>
<p>(2) An agent A can infer another agent B's mental state only if A and B have been in the same room, or have private or public interactions.</p>
<p>(3) Note that every agent tend to lie.What a character tells others doesn't affect his actual belief.An agent tend to trust a agent that exited the room later than himself.The exit order is known to all agents.(4) Agents in private communications know that others won't hear them, but they know that anyone can hear any public claims.</p>
<p>B.2 Story Generation Details</p>
<p>Algorithm 1 and Algorithm 2 provide the pseudocode for the generation process of each chapter and the whole story in HI-TOM.</p>
<p>In Algorithm 1, the function MOVE is employed to populate the story components into the template, thereby producing a sentence that describes the movement.The function COMMUNICATE generates content related to the "tell" action.Meanwhile, the RANDOM_DISTRACTOR function introduces random distractors into the story.</p>
<p>In Algorithm 2, the question generator Q_GEN randomly picks the agents and the object appearing in the story and populates them into a predefined question template.Then, the answer generator A_GEN generates the answer to the corresponding question based on a dictionary that traces the beliefs of different orders of each agent.In our experiments, the average number of tokens in a single prompt is 453.3, and the total token number of our prompts on each model is 543968, including VP and CoTP prompts on stories with or without deception.</p>
<p>Table 8 is a sample CoTP prompt in our experiments.We specify the range of the story, question, choices, and assumptions to enhance the models' understanding.We also order each line of the story to indicate the chronological order.The provided answer choices are all the containers appearing in the story.</p>
<p>C.2 Supplementary Results</p>
<p>Figure 9 and Figure 8 shows the detailed joint accuracy results of Guanaco and Claude.The joint accuracy generally decreases as the story length and the ToM orders increase, aligning with the results of GPT-4 and GPT-3.5.The overall joint accuracy performance of Claude is better than Guanaco.</p>
<p>Figure 10 to Figure 13 show the standard accuracy results of GPT-4, GPT-3.5-turbo,Claudeinstant and Guanaco 65B, as the break-down details of Table 5.Among the four models, GPT-4 has the highest and most stable performance, reaching nearly perfect accuracy on the zeroth order and higher than 20% on the fourth.</p>
<p>Under CoTP prompting, each model reaches a high performance on zeroth-order questions, espe-   for i ← 1 to l do 3:</p>
<p>randomly choose room,obj, conts, agents 4:</p>
<p>chap ← CHAP(room, obj, conts, agents) 5:</p>
<p>add chap into story 6:</p>
<p>end for 7:</p>
<p>question ← Q_GEN(Agents, Objects) 8:</p>
<p>answer ← A_GEN(Agents, Objects) 9:</p>
<p>return story, question, answer 10: end function cially for stories without agent communications, and their performance deteriorates with increased ToM order and story length.Yet, under VP prompting, Claude and Guanaco exhibit a uniform performance of around 50% across all the orders.Figure 14 illustrates the performance comparison of GPT-4 between the case when the correct answer is in a certain position in the middle of the story, and the case when it is not.We observe that GPT-4 does not significantly perform better when the correct answer lies in the middle of the story.This serves as a contrast to Figure 5    Figure 16 details the appearance frequency of the five reasoning errors in the step-by-step responses of GPT-3.5.Compared to the error ratios of GPT-4 (Figure 7), the frequencies of commonsense errors, hallucinations, and spurious causal inference are significantly higher, implying GPT-3.5'simmature perceptions of the world and its deficient logical reasoning abilities.The occurrence of insufficient reasoning depth and temporal ignorance escalates  in higher-order responses.</p>
<p>The comparison between Figure 7 and Figure 16 yields that GPT-4 has not resolved the errors of commonsense, insufficient reasoning depth, and temporal ignorance, while hallucinations and spurious causal inference have been largely addressed.</p>
<p>Figure 1 :
1
Figure 1: A scene shot from the TV series Friends that exhibits fourth-order Theory of Mind (ToM).</p>
<p>Figure 3 :
3
Figure 3: Joint accuracy of GPT-4 and GPT-3.5 on HI-TOM stories w/ or w/o deceptive agent communications.The x-axis stands for ToM orders, and the y-axis is for story lengths (number of chapters).CoTP and VP respectively represent chain-of-thought and multiple-choice-w/o-explanation prompting styles.The devil sign (</p>
<p>Figure 4 :
4
Figure 4: Joint accuracy of GPT-4 on HI-TOM stories with 0 to 4 sentences of deceptive agent communication.0th-order(reality) accuracy is not included, since the answer to the real room of the objects is not affected by deceptive communications.</p>
<p>Figure 5 :
5
Figure5: Frequency of GPT-4 correctly or incorrectly answering a question of a three-chapter story, based on whether or not the correct answer is the last or first container mentioned in the story."Last"/"First" and "¬Last"/"¬First" indicate whether or not the correct answer lies at the last/first container.</p>
<p>same as the 1st-order answer Correct answer different from the 1st-order answer</p>
<p>Figure 6 :
6
Figure 6: Standard accuracy of GPT-4 on 2nd, 3rd, and 4th-order questions, categorized by whether the correct answer matches the corresponding 1st-order answer.</p>
<p>Figure 7 :
7
Figure 7: Ratio of GPT-4 answers containing the five reasoning errors.The x-axis corresponds to ToM orders.</p>
<p>Figure 8 :
8
Figure 8: Joint accuracy results of Claude-instant.</p>
<p>Figure 9 :
9
Figure 9: Joint accuracy results of Guanaco 65B.</p>
<p>Figure 10 :
10
Figure 10: Standard accuracy results of GPT-4.</p>
<p>Figure14illustrates the performance comparison of GPT-4 between the case when the correct answer is in a certain position in the middle of the story, and the case when it is not.We observe that GPT-4 does not significantly perform better when the correct answer lies in the middle of the story.This serves as a contrast to Figure5, highlighting the better ability of GPT-4 to capture answers at</p>
<p>Figure 14 :
14
Figure 14: Performance of GPT-4 in the cases when the correct answer does or does not lie in a certain position.</p>
<p>Figure 15
15
Figure15shows similar observations for Claude.The plots for the last and first positions of containers show a higher frequency in the top-left and bottom-right cells, while the plots for other positions do not imply such a pattern.</p>
<p>Figure 15 :
15
Figure 15: Performance of Claude in the cases when the correct answer does or does not lie in a certain position.</p>
<p>Figure 16 :
16
Figure 16: Ratio of the five reasoning errors in GPT-3.5'sresponses.</p>
<p>Table 1 :
1
Basic components, numbers of choices for each component (Num.), and their examples in HI-TOM stories.
Component Num.ExampleRoom Object Container Agent30 37 39 40kitchen , bedroom lemon , peach red_envelope , blue_bottle Jack , Ella , Noah</p>
<p>Table 2
2: An example HI-TOM one-chapter story with agent communications. Random distractors are inserted in lines 6 and 11, where the latter introduces "red_box" as a distractive answer choice.</p>
<p>Table 4 :
4
Comparison between HI-TOM and other datasets.1st, 2nd, 3rd, and 4th refer to whether a dataset contains story-question pairs of a specific ToM order.Comm.stands for the existence of agent communications.#Line, #Agent, and #Container represent average number per story.
DatasetsToM/ToM-easy TOMI HI-TOM1st 2nd 3rd 4th Comm. #Line #Agent #Container15.05 3.22 58.86 2.75 226.47 5 7.39
of all agents and derive the correct answer to each question.Further details and pseudocode related to our story generation process can be found in Appendix B.2.Additionally, based onLe et al. (</p>
<p>Table 5 :
5
Standard accuracy results of the four tested models on HI-TOM stories."w/o dec." and "w/ dec." indicate accuracy in stories with and without deception, respectively.The performance increase and decrease from VP to CoTP prompting style are highlighted.
com/index/introducing-claude</p>
<p>Erik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee, Ari S Morcos, and Dhruv Batra.2023.Emergence of maps in the memories of blind navigation agents.arXiv preprint arXiv:2301.13261.
A Author ContributionsYufan Wu and Yinghui He conceived of the ideaand planned the experiments. Yufan Wu andTerry Winograd. 1971. Procedures as a represen-tation for data in a computer program for under-standing natural language. Technical report, MAS-SACHUSETTS INST OF TECH CAMBRIDGE PROJECT MAC. Chenhan Yuan, Qianqian Xie, and Sophia Ananiadou. 2023. Zero-shot temporal relation extraction with chatgpt. arXiv preprint arXiv:2304.05454.Yinghui He took the lead in writing the script for dataset generation. Yilin Jia, Yufan Wu, and Yinghui He carried out the experiments of testing language models. Yulong Chen and Naihao Deng supervised the project. Naihao Deng came up with the high-level idea for the paper, which was later refined in team discussions. All authors contributed to the writing and editing of this paper. Specifically,Yufan Wu and Yinghui He drafted the paper. YilinJia contributed to the writing for experiment setups.Rada Mihalcea, Yulong Chen, and Naihao Denghelped edit the paper.</p>
<p>Table 7 :
7
Assumption list attached to each HI-TOM story and prompt into LLMs.</p>
<p>Algorithm 1 HI-TOM Chapter Generation Algorithm Input: agents, room, conts, obj Output: chap 1: function CHAP(agents, room, conts, obj)
2: 3: 4: 5: 6: 7: 8: 9: 10: 11: end function for agent in agents do set no_move to random boolean value move ← MOVE(agent, conts, obj, no_move) add move into chap end for com ← COMMUNICATE(agents) rd ← RANDOM_DISTRACTOR(agents, conts) add com and rd into chap return chapC Experiment Details
C.1 Prompting inputs</p>
<p>AcknowledgementWe thank the anonymous reviewers for their valuable feedback and discussion.This paper's draft version was accepted to the non-archival track of the ToM workshop at ICML 2023.We would also like to extend our appreciation to the reviewers from the ToM workshop for their feedback.Ethical ConsiderationsThe data used in our study were collected from the API of language models.No sensitive or personal information was included.Our research aims at examining the high-order ToM reasoning ability of LLMs, and we demonstrate the insufficient higher-order ToM abilities of the current LLMs.There is no direct misuse of our findings.However, we recognize that future LLMs with stronger ToM abilities may become more powerful at generating misinformation and even manipulating people if used by some ill-intended parties.Therefore, we advocate for the responsible use of LLMs and the associated technologies.We adhere to the principles of transparency and openness.All methods and findings are reported completely and honestly.Furthermore, we will make our code public upon acceptance.We invite readers to utilize this resource for a more comprehensive understanding of our methods and results.LimitationsThe limitations of our work can be stated from the following perspectives.3. We acknowledge that our dataset was constructed based on specific rules, which means its dialog syntax may differ from genuine conversations.In the real world, higher-order interactions might occur in a more implicit manner, embedded within more intricate dialogues and questions.We plan to address this in future research.4. We share our thoughts on the future of NLP and research with LLMs, hoping to stimulate research that draws inspiration from human intelligence, understands humans better, and serves humans better.We admit that there exist alternative ways of moving forward on NLP research.We welcome feedback and open discussion on how we can collectively advance NLP research.
Does the autistic child have a theory of mind?. Simon Baron-Cohen, Alan M Leslie, Uta Frith, Cognition. 2111985</p>
<p>Eliciting latent predictions from transformers with the tuned lens. Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev Mckinney, Stella Biderman, Jacob Steinhardt, arXiv:2303.081122023arXiv preprint</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. Xianpei Ning Bian, Le Han, Hongyu Sun, Yaojie Lin, Ben Lu, He, arXiv:2303.164212023arXiv preprint</p>
<p>Talking about internal states: The acquisition of an explicit theory of mind. Inge Bretherton, Marjorie Beeghly, Developmental psychology. 1869061982</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Higher-order theory of mind in the tacit communication game. Harmen De Weerd, Rineke Verbrugge, Bart Verheij, Biologically Inspired Cognitive Architectures. 112015</p>
<p>Higher-order theory of mind is especially useful in unpredictable negotiations. Harmen De Weerd, Rineke Verbrugge, Bart Verheij, Autonomous Agents and Multi-Agent Systems. 362302022</p>
<p>The advantage of higher-order theory of mind in the game of limited bidding. Harmen De, Weerd , Bart Verheij, Proceedings Workshop Reasoning about other Minds, CEUR Workshop Proceedings. Workshop Reasoning about other Minds, CEUR Workshop2011751</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, arXiv:2305.14314Qlora: Efficient finetuning of quantized llms. 2023arXiv preprint</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Development and neurophysiology of mentalizing. Uta Frith, Christopher D Frith, Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences. 3582003. 1431</p>
<p>Neuro-symbolic artificial intelligence: The state of the art. Felix Hill, Olivier Tieleman, Nathaniel Tamara Von Glehn, Hamza Wong, Stephen Merzic, Clark, International Conference on Learning Representations. Pascal Hitzler2021. 2022Grounded language learning fast and slow</p>
<p>Can large language models infer causation from correlation?. Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, 2023Mona Diab, and Bernhard Schölkopf</p>
<p>Thinking, fast and slow. Daniel Kahneman, 2011macmillan</p>
<p>Theory of mind may have spontaneously emerged in large language models. Michal Kosinski, 2023</p>
<p>Revisiting the evaluation of theory of mind through question answering. Matthew Le, Y-Lan Boureau, Maximilian Nickel, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019</p>
<p>Yann Lecun, Yoshua Bengio, Geoffrey Hinton, Deep learning. nature. 2015521</p>
<p>Higher-order theory of mind and social competence in school-age children. Bethany Liddle, Daniel Nettle, Journal of Cultural and Evolutionary Psychology. 43-42006</p>
<p>Towards a holistic landscape of situated theory of mind in large language models. Ziqiao Ma, Jacob Sansom, Run Peng, Joyce Chai, Findings of the Association for Computational Linguistics: EMNLP 2023. Association for Computational Linguistics2023</p>
<p>Data contamination: From memorization to exploitation. Inbal Magar, Roy Schwartz, 10.18653/v1/2022.acl-short.18Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20222Short Papers)</p>
<p>Mohammad Javad Hosseini, Mark Johnson, and Mark Steedman. Nick Mckenna, Tianyi Li, Liang Cheng, arXiv:2305.14552Sources of hallucination by large language models on inference tasks. 2023arXiv preprint</p>
<p>Thinking fast and slow: Efficient text-to-visual retrieval with transformers. Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, Andrew Zisserman, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>The overlapping relationship between emotion perception and theory of mind. L C Rachel, Louise H Mitchell, Phillips, Neuropsychologia. 702015</p>
<p>Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, Thomas L Griffiths, arXiv:1808.09352Evaluating theory of mind in question answering. 2018arXiv preprint</p>
<p>Chatgpt: Optimizing language models for dialogue. OpenAI. Christopher Osterhaus and Susanne Koerber. 2021. The development of advanced theory of mind in middle childhood: A longitudinal study from age 5 to 10 years. OpenAI. 2023. Gpt-4 technical report. TB OpenAI. 2022Child Development. 925</p>
<p>Sung Joon, Park, C Joseph, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. 2023arXiv preprint</p>
<p>Understanding the representational mind. Josef Perner, 1991The MIT Press</p>
<p>Does the chimpanzee have a theory of mind?. David Premack, Guy Woodruff, Behavioral and brain sciences. 141978</p>
<p>Theory of mind ability and cooperation. Manuscript. Garret Ridinger, Michael Mcbride, 2017Univ. California, Irvine</p>
<p>Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations. David E Rumelhart, James L Mcclelland, Group Research, 10.7551/mitpress/5236.001.00011986The MIT Press</p>
<p>Maarten Sap, Ronan Lebras, Daniel Fried, Yejin Choi, arXiv:2210.13312Neural theory-of-mind? on the limits of social intelligence in large lms. 2022arXiv preprint</p>
<p>Combining symbolic and neural learning. Jude W Shavlik, Machine Learning. 199414</p>
<p>Human problem solving: The state of the theory in 1970. A Herbert, Allen Simon, Newell, American psychologist. 2621451971</p>
<p>Dan Sperber, Deirdre Wilson, Pragmatics, modularity and mind-reading. Mind &amp; language. 200217</p>
<p>Asymmetry between even and odd levels of recursion in a theory of mind. Masanori Takano, Takaya Arita, Proceedings of ALife X. ALife X2006</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>Large language models fail on trivial alterations to theory-of-mind tasks. Tomer Ullman, arXiv:2302.083992023arXiv preprint</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, 2023</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022arXiv preprint</p>
<p>Read the following story and answer the multiple-choice question. Think step-by-step. Provide the answer first, and then explain it. Story</p>
<p>. Jack William, Charlotte , Noah , </p>
<p>The carrot is in the red_basket. </p>
<p>William made no movements and stayed in the hall for 1 minute. </p>
<p>Hannah saw a mouse. </p>
<p>Jack made no movements and stayed in the hall for 1 minute. </p>
<p>Charlotte made no movements and stayed in the hall for 1 minute. </p>
<p>Charlotte exited the hall. </p>
<p>Charlotte dislikes the tomato. </p>
<p>Noah moved the carrot to the green_envelope. </p>
<p>Hannah moved the carrot to the red_basket. </p>
<p>. Jack William, Charlotte , Noah , </p>
<p>Charlotte publicly claimed that carrot is in the green_envelope. </p>
<p>You should assume the following. (1) An agent witnesses everything and every movements before exiting a room. (2) An agent A can infer another agent B's mental state only if A and B have been in the same room, or have private or public interactions. (3) Note that every agent tend to lie. What a character tells others doesn't affect his actual belief. An agent tend to trust a agent that exited the room later than himself. The exit order is known to all agents. B Red_Basket, C Blue_Container, D , E Green_Drawer, F Blue_Bucket, G Green_Cupboard, H Red_Bottle, I Green_Treasure_Chest, J Blue_Cupboard, K Red_Pantry, L Red_Container, M Blue_Bathtub, N Red_Envelope, O Note, Hannah privately told Charlotte that the carrot is in the blue_container. Question: Where does Charlotte think Jack thinks Hannah thinks William thinks the carrot is? Choices: A. green_envelope,. 4) Agents in private communications know that others won't hear them, but they know that anyone can hear any public claims</p>            </div>
        </div>

    </div>
</body>
</html>
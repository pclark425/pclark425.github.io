<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-978 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-978</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-978</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-23.html">extraction-schema-23</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <p><strong>Paper ID:</strong> paper-5613003</p>
                <p><strong>Paper Title:</strong> <a href="http://www.cis.upenn.edu/~mkearns/papers/barbados/klc-pomdp.ps.Z" target="_blank">Planning and Acting in Partially Observable Stochastic Domains</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable mdps (pomdps). We then outline a novel algorithm for solving pomdps o(cid:11) line and show how, in some cases, a (cid:12)nite-memory controller can be extracted from the solution to a pomdp. We conclude with a discussion of how our approach relates to previous work, the complexity of (cid:12)nding exact solutions to pomdps, and of some possibilities for (cid:12)nding approximate solutions.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e978.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e978.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>POMDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Partially Observable Markov Decision Process</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A formal probabilistic planning model in which the agent maintains a probability distribution (belief state) over a discrete set of world states and selects actions to maximize expected discounted reward under uncertain observations and stochastic transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Partially Observable Markov Decision Process (POMDP) framework</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A decision-theoretic framework used throughout the paper: defined by tuple ⟨S, A, T, R, Ω, O⟩ where S is a finite set of states, A actions, T transition probabilities, R rewards, Ω observations, and O observation probabilities. An agent maintains a belief state b (probability distribution over S), updates it via a state-estimator SE(b,a,o), and plans in belief-space to compute policies that map beliefs to actions. The paper develops algorithms (value iteration, witness algorithm) to compute piecewise-linear convex value functions over the belief simplex and extract finite-state controllers (plan graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Belief-state POMDP (probability distribution over discrete states)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>States: finite set S enumerated explicitly; Actions: finite A; Transitions: stochastic T(s,a,s') specifying P(s'|s,a); Observations: stochastic O(s',a,o) specifying P(o|s',a). The world model is fully probabilistic (aleatoric uncertainty in transitions and observations); belief state b(s) represents the agent's subjective probability over S and evolves via Bayesian updates (SE).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>state uncertainty (belief over discrete states), aleatoric uncertainty in action outcomes and observations</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Probability distributions (belief states) updated by Bayes rule; expected-value computations over belief space and piecewise-linear representations of value functions</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Value iteration in belief space (with Q-functions per action), backed by the Witness algorithm for efficient backups</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>POMDPs provide a principled probabilistic symbolic world model for planning under observation and transition uncertainty; belief states are a sufficient statistic and allow planning in continuous belief space. The optimal belief-space value function is piecewise-linear and convex, enabling finite representations via sets of vectors (policy trees). No large-language-models or text-based environments are considered in this work.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e978.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e978.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Belief State (b)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Belief State (probability distribution over world states)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probability vector over the discrete world states used as the agent's sufficient statistic for history, updated by Bayesian inference after each action-observation pair (SE function).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Belief-state representation / State Estimator (SE)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The paper defines belief state b with components b(s) summing to 1. The state-estimator SE(b,a,o) computes the posterior b' using SE formula: b'(s') = O(s',a,o) * Σ_s T(s,a,s') * b(s) / Pr(o|a,b). This belief is used as the state for planning (belief MDP).</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Probability distribution (belief) over discrete symbolic states</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Represents uncertainty explicitly as a probability mass over enumerated symbolic states; transitions and observations are probabilistic, and the belief update applies Bayes' rule to compute posterior belief after each action and observation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>Epistemic/state uncertainty (agent's belief about which discrete state is current) and captures aleatoric noise in observations/transitions</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Exact probability distributions on finite state spaces (Bayesian belief updates), maintained and used directly in planning</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Belief-space dynamic programming / value iteration (backups operate on belief distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Maintaining explicit belief states is central to reasoning about information-gathering vs. world-changing actions; belief-space planning permits explicit value-of-information tradeoffs. The paper formalizes the belief update (SE) and demonstrates that belief states make the process Markov.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e978.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e978.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Witness Algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Witness Algorithm for POMDP backups</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm introduced in the paper to compute per-action Q-functions as parsimonious sets of policy-tree vectors for belief-space value-iteration backups, avoiding exhaustive generation of candidate trees by searching for witness belief points via linear programming.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Witness algorithm (inner loop for Q-function construction)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each value-iteration step, for each action a the algorithm incrementally constructs a set U_a of useful t-step policy-tree vectors representing Q_a^t by: (1) initializing with a policy-tree best for some belief; (2) searching for witness belief states where the current approximation underestimates the true Q-function via linear programs; (3) when a witness is found, building the corresponding policy tree by choosing best (t−1)-step subtrees for each observation and adding it to U_a; (4) repeating until no witness exists. This yields parsimonious vector sets for efficient backups.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Belief-state POMDP (vector-based, piecewise-linear value representation)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Represents t-step Q-functions and value functions as finite sets of vectors (one per useful policy tree). Each vector gives V_p(s) for world states s and thus defines a linear function over belief simplex. The witness algorithm identifies vectors needed to represent the upper surface (max over linear pieces).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>Belief/state uncertainty and stochastic action/observation outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Piecewise-linear convex representation of value functions over probability distributions (beliefs); linear programming to find witness beliefs where approximation is insufficient</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Value iteration with Witness-based backups (linear-programming witness search + pruning)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The witness algorithm reduces practical computation compared to exhaustive enumeration by directly generating only those policy-tree vectors that are necessary, using LP-based witness search. It retains exactness and improves empirical performance on many problems; still exponential in worst case and not applied to text/LLM settings in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e978.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e978.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plan Graph / Finite-state Controller</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plan Graph (finite-state controller extracted from POMDP value function)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A finite directed graph (finite-state controller) extracted from converged belief-space value functions that specifies actions at nodes and observation-labeled transitions between nodes, enabling execution without maintaining explicit belief vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Plan graph / finite-state controller</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>After value-iteration converges (or two successive value functions equal), regions of belief space map to policy-tree roots; by collapsing identical policy trees across levels, one obtains a finite directed graph where each node specifies an action and outgoing edges labeled by observations lead to next nodes. The agent executes by maintaining only a pointer to the current node rather than a full belief.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Discrete finite-state controller derived from belief-space POMDP (symbolic nodes correspond to belief regions)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Encodes behavior as nodes/actions and observation-conditioned transitions; implicitly represents a partition of the belief simplex into regions (each node covers a region) but does not require explicit on-line belief updates for execution, although the model was computed from probabilistic transitions/observations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>State uncertainty is handled during offline planning (belief region partitioning); controller implicitly accounts for observation noise by mapping observations to nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Finite-state controller derived from piecewise-linear value function that was constructed from belief-space probabilities; no explicit LLM uncertainty handling</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Extraction from converged value-iteration / Witness outputs into a finite-state plan graph</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Plan graphs provide compact finite-memory controllers that capture optimal/near-optimal behavior without on-line belief maintenance; size depends on observation reliability and can grow large as observations become less informative. The paper demonstrates extraction on toy domains (e.g., tiger) but does not apply to text environments or LLM outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e978.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e978.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Policy Trees / Piecewise-linear Value</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Policy Trees and Piecewise-linear Convex Value Function Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Non-stationary t-step policies are represented as policy trees (depth-t) whose value functions are linear in belief; the optimal t-step value is the pointwise maximum over these linear functions, yielding a piecewise-linear convex function.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Policy-trees representation for belief-space value functions</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Each t-step policy tree encodes the action at the root and subtrees for each possible observation; the vector V_p(s) gives values per world state and induces a linear function over beliefs. The optimal t-step value V_t(b)=max_p b·V_p is thus piecewise-linear and convex. The algorithms operate on parsimonious sets of such vectors and prune dominated vectors via LP.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Vector-based piecewise-linear representation over belief simplex (derived from symbolic discrete states)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>States/actions/observations are discrete and symbolic; each policy-tree vector stores expected returns per world state under that policy-tree, and maximization over these linear functions produces the value surface over belief distributions. The representation is probabilistic in that vectors encode expected values under stochastic transitions and observation models.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>State and outcome uncertainty captured by expectation values in the vectors and by the belief distribution</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Explicit expectation calculations (vector values) over probabilistic transitions and observations; dominated-vector pruning via linear programming</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Value iteration operating on sets of policy-tree vectors, pruning dominated vectors to form parsimonious representations</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Representing value functions as maxima of linear functions induced by policy trees enables exact dynamic-programming backups for finite-horizon POMDPs; pruning dominated vectors is essential to tractability. These methods are applicable to symbolic probabilistic world models but are not evaluated on text-based/LLM-driven tasks in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e978.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e978.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Incremental Pruning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Incremental Pruning algorithm for POMDP backups</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exact algorithm (cited in the paper) that interleaves generation and pruning of policy-tree vectors to perform efficient backups in POMDP value iteration, empirically faster than earlier witness/exhaustive methods on many benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Incremental Pruning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an alternative exact algorithm (Cassandra, Littman, Zhang) that reduces intermediate set sizes by pruning non-useful vectors during generation, improving empirical performance for backups. The paper references it in the context of other POMDP solution approaches but does not implement it here.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Belief-state POMDP (vector-based piecewise-linear backups)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Operates on sets of vectors representing partial value functions over beliefs; transitions and observations are probabilistic and used to compute new vectors and prune dominated ones incrementally.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>State and observation uncertainty handled via belief-based vector backups</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Probability distributions / belief updates and pruning via dominance tests (LP)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Incremental pruning (exact dynamic-programming backups for POMDPs)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incremental pruning is noted as a fast exact method for POMDP backups (cited work) and is empirically competitive with or faster than the witness algorithm; the paper points readers to it as related work rather than applying it to new domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Incremental Pruning: A simple, fast, exact method for partially observable Markov decision processes <em>(Rating: 2)</em></li>
                <li>The Optimal Control of Partially Observable Markov Processes <em>(Rating: 2)</em></li>
                <li>Exact and Approximate Algorithms for Partially Observable Markov Decision Problems <em>(Rating: 2)</em></li>
                <li>Probabilistic planning with information gathering and contingent execution <em>(Rating: 1)</em></li>
                <li>An algorithm for probabilistic planning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-978",
    "paper_id": "paper-5613003",
    "extraction_schema_id": "extraction-schema-23",
    "extracted_data": [
        {
            "name_short": "POMDP",
            "name_full": "Partially Observable Markov Decision Process",
            "brief_description": "A formal probabilistic planning model in which the agent maintains a probability distribution (belief state) over a discrete set of world states and selects actions to maximize expected discounted reward under uncertain observations and stochastic transitions.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Partially Observable Markov Decision Process (POMDP) framework",
            "system_description": "A decision-theoretic framework used throughout the paper: defined by tuple ⟨S, A, T, R, Ω, O⟩ where S is a finite set of states, A actions, T transition probabilities, R rewards, Ω observations, and O observation probabilities. An agent maintains a belief state b (probability distribution over S), updates it via a state-estimator SE(b,a,o), and plans in belief-space to compute policies that map beliefs to actions. The paper develops algorithms (value iteration, witness algorithm) to compute piecewise-linear convex value functions over the belief simplex and extract finite-state controllers (plan graphs).",
            "world_model_type": "Belief-state POMDP (probability distribution over discrete states)",
            "world_model_description": "States: finite set S enumerated explicitly; Actions: finite A; Transitions: stochastic T(s,a,s') specifying P(s'|s,a); Observations: stochastic O(s',a,o) specifying P(o|s',a). The world model is fully probabilistic (aleatoric uncertainty in transitions and observations); belief state b(s) represents the agent's subjective probability over S and evolves via Bayesian updates (SE).",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "state uncertainty (belief over discrete states), aleatoric uncertainty in action outcomes and observations",
            "uncertainty_method": "Probability distributions (belief states) updated by Bayes rule; expected-value computations over belief space and piecewise-linear representations of value functions",
            "planning_algorithm": "Value iteration in belief space (with Q-functions per action), backed by the Witness algorithm for efficient backups",
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": false,
            "ablation_results": null,
            "key_findings": "POMDPs provide a principled probabilistic symbolic world model for planning under observation and transition uncertainty; belief states are a sufficient statistic and allow planning in continuous belief space. The optimal belief-space value function is piecewise-linear and convex, enabling finite representations via sets of vectors (policy trees). No large-language-models or text-based environments are considered in this work.",
            "uuid": "e978.0"
        },
        {
            "name_short": "Belief State (b)",
            "name_full": "Belief State (probability distribution over world states)",
            "brief_description": "A probability vector over the discrete world states used as the agent's sufficient statistic for history, updated by Bayesian inference after each action-observation pair (SE function).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Belief-state representation / State Estimator (SE)",
            "system_description": "The paper defines belief state b with components b(s) summing to 1. The state-estimator SE(b,a,o) computes the posterior b' using SE formula: b'(s') = O(s',a,o) * Σ_s T(s,a,s') * b(s) / Pr(o|a,b). This belief is used as the state for planning (belief MDP).",
            "world_model_type": "Probability distribution (belief) over discrete symbolic states",
            "world_model_description": "Represents uncertainty explicitly as a probability mass over enumerated symbolic states; transitions and observations are probabilistic, and the belief update applies Bayes' rule to compute posterior belief after each action and observation.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "Epistemic/state uncertainty (agent's belief about which discrete state is current) and captures aleatoric noise in observations/transitions",
            "uncertainty_method": "Exact probability distributions on finite state spaces (Bayesian belief updates), maintained and used directly in planning",
            "planning_algorithm": "Belief-space dynamic programming / value iteration (backups operate on belief distributions)",
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": false,
            "ablation_results": null,
            "key_findings": "Maintaining explicit belief states is central to reasoning about information-gathering vs. world-changing actions; belief-space planning permits explicit value-of-information tradeoffs. The paper formalizes the belief update (SE) and demonstrates that belief states make the process Markov.",
            "uuid": "e978.1"
        },
        {
            "name_short": "Witness Algorithm",
            "name_full": "Witness Algorithm for POMDP backups",
            "brief_description": "An algorithm introduced in the paper to compute per-action Q-functions as parsimonious sets of policy-tree vectors for belief-space value-iteration backups, avoiding exhaustive generation of candidate trees by searching for witness belief points via linear programming.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Witness algorithm (inner loop for Q-function construction)",
            "system_description": "At each value-iteration step, for each action a the algorithm incrementally constructs a set U_a of useful t-step policy-tree vectors representing Q_a^t by: (1) initializing with a policy-tree best for some belief; (2) searching for witness belief states where the current approximation underestimates the true Q-function via linear programs; (3) when a witness is found, building the corresponding policy tree by choosing best (t−1)-step subtrees for each observation and adding it to U_a; (4) repeating until no witness exists. This yields parsimonious vector sets for efficient backups.",
            "world_model_type": "Belief-state POMDP (vector-based, piecewise-linear value representation)",
            "world_model_description": "Represents t-step Q-functions and value functions as finite sets of vectors (one per useful policy tree). Each vector gives V_p(s) for world states s and thus defines a linear function over belief simplex. The witness algorithm identifies vectors needed to represent the upper surface (max over linear pieces).",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "Belief/state uncertainty and stochastic action/observation outcomes",
            "uncertainty_method": "Piecewise-linear convex representation of value functions over probability distributions (beliefs); linear programming to find witness beliefs where approximation is insufficient",
            "planning_algorithm": "Value iteration with Witness-based backups (linear-programming witness search + pruning)",
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": false,
            "ablation_results": null,
            "key_findings": "The witness algorithm reduces practical computation compared to exhaustive enumeration by directly generating only those policy-tree vectors that are necessary, using LP-based witness search. It retains exactness and improves empirical performance on many problems; still exponential in worst case and not applied to text/LLM settings in this paper.",
            "uuid": "e978.2"
        },
        {
            "name_short": "Plan Graph / Finite-state Controller",
            "name_full": "Plan Graph (finite-state controller extracted from POMDP value function)",
            "brief_description": "A finite directed graph (finite-state controller) extracted from converged belief-space value functions that specifies actions at nodes and observation-labeled transitions between nodes, enabling execution without maintaining explicit belief vectors.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Plan graph / finite-state controller",
            "system_description": "After value-iteration converges (or two successive value functions equal), regions of belief space map to policy-tree roots; by collapsing identical policy trees across levels, one obtains a finite directed graph where each node specifies an action and outgoing edges labeled by observations lead to next nodes. The agent executes by maintaining only a pointer to the current node rather than a full belief.",
            "world_model_type": "Discrete finite-state controller derived from belief-space POMDP (symbolic nodes correspond to belief regions)",
            "world_model_description": "Encodes behavior as nodes/actions and observation-conditioned transitions; implicitly represents a partition of the belief simplex into regions (each node covers a region) but does not require explicit on-line belief updates for execution, although the model was computed from probabilistic transitions/observations.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "State uncertainty is handled during offline planning (belief region partitioning); controller implicitly accounts for observation noise by mapping observations to nodes.",
            "uncertainty_method": "Finite-state controller derived from piecewise-linear value function that was constructed from belief-space probabilities; no explicit LLM uncertainty handling",
            "planning_algorithm": "Extraction from converged value-iteration / Witness outputs into a finite-state plan graph",
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": false,
            "ablation_results": null,
            "key_findings": "Plan graphs provide compact finite-memory controllers that capture optimal/near-optimal behavior without on-line belief maintenance; size depends on observation reliability and can grow large as observations become less informative. The paper demonstrates extraction on toy domains (e.g., tiger) but does not apply to text environments or LLM outputs.",
            "uuid": "e978.3"
        },
        {
            "name_short": "Policy Trees / Piecewise-linear Value",
            "name_full": "Policy Trees and Piecewise-linear Convex Value Function Representation",
            "brief_description": "Non-stationary t-step policies are represented as policy trees (depth-t) whose value functions are linear in belief; the optimal t-step value is the pointwise maximum over these linear functions, yielding a piecewise-linear convex function.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Policy-trees representation for belief-space value functions",
            "system_description": "Each t-step policy tree encodes the action at the root and subtrees for each possible observation; the vector V_p(s) gives values per world state and induces a linear function over beliefs. The optimal t-step value V_t(b)=max_p b·V_p is thus piecewise-linear and convex. The algorithms operate on parsimonious sets of such vectors and prune dominated vectors via LP.",
            "world_model_type": "Vector-based piecewise-linear representation over belief simplex (derived from symbolic discrete states)",
            "world_model_description": "States/actions/observations are discrete and symbolic; each policy-tree vector stores expected returns per world state under that policy-tree, and maximization over these linear functions produces the value surface over belief distributions. The representation is probabilistic in that vectors encode expected values under stochastic transitions and observation models.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "State and outcome uncertainty captured by expectation values in the vectors and by the belief distribution",
            "uncertainty_method": "Explicit expectation calculations (vector values) over probabilistic transitions and observations; dominated-vector pruning via linear programming",
            "planning_algorithm": "Value iteration operating on sets of policy-tree vectors, pruning dominated vectors to form parsimonious representations",
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": false,
            "ablation_results": null,
            "key_findings": "Representing value functions as maxima of linear functions induced by policy trees enables exact dynamic-programming backups for finite-horizon POMDPs; pruning dominated vectors is essential to tractability. These methods are applicable to symbolic probabilistic world models but are not evaluated on text-based/LLM-driven tasks in this paper.",
            "uuid": "e978.4"
        },
        {
            "name_short": "Incremental Pruning",
            "name_full": "Incremental Pruning algorithm for POMDP backups",
            "brief_description": "An exact algorithm (cited in the paper) that interleaves generation and pruning of policy-tree vectors to perform efficient backups in POMDP value iteration, empirically faster than earlier witness/exhaustive methods on many benchmarks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Incremental Pruning",
            "system_description": "Cited as an alternative exact algorithm (Cassandra, Littman, Zhang) that reduces intermediate set sizes by pruning non-useful vectors during generation, improving empirical performance for backups. The paper references it in the context of other POMDP solution approaches but does not implement it here.",
            "world_model_type": "Belief-state POMDP (vector-based piecewise-linear backups)",
            "world_model_description": "Operates on sets of vectors representing partial value functions over beliefs; transitions and observations are probabilistic and used to compute new vectors and prune dominated ones incrementally.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "State and observation uncertainty handled via belief-based vector backups",
            "uncertainty_method": "Probability distributions / belief updates and pruning via dominance tests (LP)",
            "planning_algorithm": "Incremental pruning (exact dynamic-programming backups for POMDPs)",
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Incremental pruning is noted as a fast exact method for POMDP backups (cited work) and is empirically competitive with or faster than the witness algorithm; the paper points readers to it as related work rather than applying it to new domains.",
            "uuid": "e978.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Incremental Pruning: A simple, fast, exact method for partially observable Markov decision processes",
            "rating": 2
        },
        {
            "paper_title": "The Optimal Control of Partially Observable Markov Processes",
            "rating": 2
        },
        {
            "paper_title": "Exact and Approximate Algorithms for Partially Observable Markov Decision Problems",
            "rating": 2
        },
        {
            "paper_title": "Probabilistic planning with information gathering and contingent execution",
            "rating": 1
        },
        {
            "paper_title": "An algorithm for probabilistic planning",
            "rating": 1
        }
    ],
    "cost": 0.016502,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Planning and Acting in Partially Observable Stochastic Domains
12 February 1998</p>
<p>Leslie Pack Kaelbling 
Microelectronics and Computer Technology Corporation
3500 West Balcones Center Drive Austin78759-5398TXUSA</p>
<p>Michael L Littman 
Anthony R Cassandra </p>
<p>Computer Science Department
Brown University
1910, 02912-1910Box, ProvidenceRIUSA</p>
<p>Department of Computer Science
Duke University Durham
27708-0129NCUSA</p>
<p>Planning and Acting in Partially Observable Stochastic Domains
12 February 1998F67EE20B08BA6B396BA21CDEBE2636BBPreprint submitted to Elsevier Preprintplanninguncertaintypartially observable Markov decision processes
In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains.We begin by introducing the theory of Markov decision processes (mdps) and partially observable mdps (pomdps).We then outline a novel algorithm for solving pomdps o line and show how, in some cases, a nite-memory controller can be extracted from the solution to a pomdp.We conclude with a discussion of how our approach relates to previous work, the complexity of nding exact solutions to pomdps, and of some possibilities for nding approximate solutions.</p>
<p>observations of its world.Its actions are not completely reliable, however.Sometimes, when it intends to move, it stays where it is or goes too far; sometimes, when it intends to turn, it overshoots.It has similar problems with observation.Sometimes a corridor looks like a corner; sometimes a T-junction looks like an L-junction.How can such an error-plagued robot navigate, even given a map of the corridors?</p>
<p>In general, the robot will have to remember something about its history of actions and observations and use this information, together with its knowledge of the underlying dynamics of the world (the map and other information), to maintain an estimate of its location.Many engineering applications follow this approach, using methods like the Kalman lter 26] to maintain a running estimate of the robot's spatial uncertainty, expressed as an ellipsoid or normal distribution in Cartesian space.This approach will not do for our robot, though.Its uncertainty may be discrete: it might be almost certain that it is in the north-east corner of either the fourth or the seventh oors, though it admits a chance that it is on the fth oor, as well. 1</p>
<p>Introduction</p>
<p>In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains.Problems like the one described above can be modeled as partially observable Markov decision processes (pomdps).Of course, we are not interested only in problems of robot navigation.Similar problems come up in factory process control, oil exploration, transportation logistics, and a variety of other complex real-world situations.This is essentially a planning problem: given a complete and correct model of the world dynamics and a reward structure, nd an optimal way to behave.In the arti cial intelligence (AI) literature, a deterministic version of this problem has been addressed by adding knowledge preconditions to traditional planning systems 43].Because we are interested in stochastic domains, however, we must depart from the traditional AI planning model.Rather than taking plans to be sequences of actions, which may only rarely execute as expected, we take them to be mappings from situations to actions that specify the agent's behavior no matter what may happen.In many cases, we may not want a full policy; methods for developing partial policies and conditional plans for completely observable domains are the subject of much current interest 15,61,13].A weakness of the methods described in this paper is that they require the states of the world to be represented enumeratively, rather than through compositional representations such as Bayes nets or probabilistic operator descriptions.However, this work has served as a substrate for development of algorithms for more complex and e cient representations 6].Section 6 describes the relation between the present approach and prior research in more detail.</p>
<p>One important facet of the pomdp approach is that there is no distinction drawn between actions taken to change the state of the world and actions taken to gain information.This is important because, in general, every action has both types of e ect.Stopping to ask questions may delay the robot's arrival at the goal or spend extra energy; moving forward may give the robot information that it is in a dead-end because of the resulting crash.</p>
<p>Thus, from the pomdp perspective, optimal performance involves something akin to a \value of information" calculation, only more complex; the agent chooses between actions based on the amount of information they provide, the amount of reward they produce, and how they change the state of the world.</p>
<p>This paper is intended to make two contributions.The rst is to recapitulate work from the operations-research literature 36,42,56,59,64] and to describe its connection to closely related work in AI.The second is to describe a novel algorithmic approach for solving pomdps exactly.We begin by introducing the theory of Markov decision processes (mdps) and pomdps.We then outline a novel algorithm for solving pomdps o line and show how, in some cases, a nite-memory controller can be extracted from the solution to a pomdp.We conclude with a brief discussion of related work and of approximation methods.</p>
<p>Markov Decision Processes</p>
<p>Markov decision processes serve as a basis for solving the more complex partially observable problems that we are ultimately interested in.An mdp is a model of an agent interacting synchronously with a world.As shown in Figure 1, the agent takes as input the state of the world and generates as output actions, which themselves a ect the state of the world.In the mdp framework, it is assumed that, although there may be a great deal of uncertainty about the e ects of an agent's actions, there is never any uncertainty about the agent's AGENT Actions States WORLD Fig. 1.An mdp models the synchronous interaction between agent and world.current state|it has complete and perfect perceptual abilities.Markov decision processes are described in depth in a variety of texts 3,49]; we will just brie y cover the necessary background.</p>
<p>Basic Framework</p>
<p>A Markov decision process can be described as a tuple hS;A;T;Ri, where S is a nite set of states of the world; A is a nite set of actions; T : S A ! (S) is the state-transition function, giving for each world state and agent action, a probability distribution over world states (we write T (s; a; s 0 ) for the probability of ending in state s 0 , given that the agent starts in state s and takes action a); and R : S A ! R is the reward function, giving the expected immediate reward gained by the agent for taking each action in each state (we write R(s; a) for the expected reward for taking action a in state s).In this model, the next state and the expected reward depend only on the previous state and the action taken; even if we were to condition on additional previous states, the transition probabilities and the expected rewards would remain the same.This is known as the Markov property|the state and reward at time t +1 is dependent only on the state at time t and the action at time t.</p>
<p>In fact, mdps can have in nite state and action spaces.The algorithms that we describe in this section apply only to the nite case; however, in the context of pomdps, we will consider a class of mdps with uncountably in nite state spaces.</p>
<p>Acting Optimally</p>
<p>We would like our agents to act in such a way as to maximize some measure of the long-run reward received.One such framework is nite-horizon optimality, in which the agent should act in order to maximize the expected sum of reward that it gets on the next k steps; it should maximize E " k?1 X t=0 r t # ; where r t is the reward received on step t.This model is somewhat inconvenient, because it is rare that an appropriate k will be known exactly.We might prefer to consider an in nite lifetime for the agent.The most straightforward is the in nite-horizon discounted model, in which we sum the rewards over the in nite lifetime of the agent, but discount them geometrically using discount factor 0 &lt; &lt; 1; the agent should act so as to optimize
E " 1 X t=0 t r t # :
In this model, rewards received earlier in its lifetime have more value to the agent; the in nite lifetime is considered, but the discount factor ensures that the sum is nite.This sum is also the expected amount of reward received if a decision to terminate the run is made on each step with probability 1? .The larger the discount factor (closer to 1), the more e ect future rewards have on current decision making.In our forthcoming discussions of nite-horizon optimality, we will also use a discount factor; when it has value one, it is equivalent to the simple nite-horizon case described above.</p>
<p>A policy is a description of the behavior of an agent.We consider two kinds of policies: stationary and non-stationary.A stationary policy, : S !A, is a situation-action mapping that speci es, for each state, an action to be taken.The choice of action depends only on the state and is independent of the time step.A non-stationary policy is a sequence of situation-action mappings, indexed by time.The policy t is to be used to choose the action on the t thto-last step as a function of the current state, s t .In the nite-horizon model, the optimal policy is not typically stationary: the way an agent chooses its actions on the last step of its life is generally going to be very di erent from the way it chooses them when it has a long life ahead of it.In the in nitehorizon discounted model, the agent always has a constant expected amount of time remaining, so there is no reason to change action strategies: there is a stationary optimal policy.Given a policy, we can evaluate it based on the long-run value that the agent expects to gain from executing it.In the nite-horizon case, let V ;t (s) be the expected sum of reward gained from starting in state s and executing nonstationary policy for t steps.Clearly, V ;1 (s) = R(s; 1 (s)); that is, on the last step, the value is just the expected reward for taking the action speci ed by the nal element of the policy.Now, we can de ne V ;t (s) inductively as
V ;t (s) = R(s; t (s)) + X s 0 2S
T (s; t (s); s 0 )V ;t?1 (s 0 ) :</p>
<p>The t-step value of being in state s and executing non-stationary policy is the immediate reward, R(s; t (s)), plus the discounted expected value of the remaining t ? 1 steps.To evaluate the future, we must consider all possible resulting states s 0 , the likelihood of their occurrence T (s; t (s); s 0 ), and their (t ?1)-step value under policy , V ;t?1 (s 0 ).In the in nite-horizon discounted case, we write V (s) for the expected discounted sum of future reward for starting in state s and executing policy .It is recursively de ned by
V (s) = R(s; (s)) + X s 0 2S
T (s; (s); s 0 )V (s 0 ) :</p>
<p>The value function, V , for policy is the unique simultaneous solution of this set of linear equations, one equation for each state s.</p>
<p>Now we know how to compute a value function, given a policy.Sometimes, we will need to go the opposite way, and compute a greedy policy given a value function.It really only makes sense to do this for the in nite-horizon discounted case; to derive a policy for the nite horizon, we would need a whole sequence of value functions.Given any value function V , a greedy policy with respect to that value function, V , is de ned as
V (s) = argmax a 2 4 R(s; a) + X s 0 2S
T (s; a; s 0 )V (s 0 ) 3 5 :</p>
<p>This is the policy obtained by, at every step, taking the action that maximizes expected immediate reward plus the expected discounted value of the next state, as measured by V .</p>
<p>What is the optimal nite-horizon policy, ?The agent's last step is easy: it should maximize its nal reward.So
1 (s) = argmax a R(s; a) :
The optimal situation-action mapping for the t th step, t , can be de ned in terms of the optimal (t?1)-step value function V t?1 ;t?1 (written for simplicity as V t?1 ):
t (s) = argmax a 2 4 R(s; a) + X s 0 2S
T (s; a; s 0 )V t?1 (s 0 ) 3 5 ;</p>
<p>V t?1 is derived from t?1 and V t?2 .</p>
<p>In the in nite-horizon discounted case, for any initial state s, we want to execute the policy that maximizes V (s).Howard 24] showed that there exists a stationary policy, , that is optimal for every starting state.The value function for this policy, V , also written V , is de ned by the set of equations
V (s) = max a 2 4 R(s; a) + X s 0 2S T (s; a; s 0 )V (s 0 )3 5 ;
which has a unique solution.An optimal policy, , is just a greedy policy with respect to V .</p>
<p>Another way to understand the in nite-horizon value function, V , is to approach it by using an ever-increasing discounted nite horizon.As the horizon, t, approaches in nity, V t approaches V .This is only guaranteed to occur when the discount factor, , is less than 1, which tends to wash out the details of exactly what happens at the end of the agent's life.</p>
<p>Computing an Optimal Policy</p>
<p>There are many methods for nding optimal policies for mdps.In this section, we explore value iteration because it will also serve as the basis for nding policies in the partially observable case.</p>
<p>Value iteration proceeds by computing the sequence V t of discounted nitehorizon optimal value functions, as shown in Table 1 (the superscript is omitted, because we shall henceforth only be considering optimal value functions).It makes use of an auxiliary function, Q a t (s), which is the t-step value of starting in state s, taking action a, then continuing with the optimal (t?1)step non-stationary policy.The algorithm terminates when the maximum difference between two successive value functions (known as the Bellman error magnitude) is less than some .</p>
<p>It can be shown 62] that there exists a t , polynomial in jSj, jAj, the magnitude of the largest value of R(s; a), and 1=(1 ?), such that the greedy policy with respect to V t is equal to the optimal in nite-horizon policy, .Rather than calculating a bound on t in advance and running value iteration for that long, we instead use the following result regarding the Bellman error magnitude 66] in order to terminate with a near-optimal policy.If jV t (s)?V t?1 (s)j &lt; for all s, then the value of the greedy policy with respect to V t does not di er from V by more than 2 =(1 ? ) at any state.That is, max s2S jV V t (s) ?V (s)j &lt; 2 1 ?: end loop until jV t (s) ?V t?1 (s)j &lt; for all s 2 S Table 1 The value-iteration algorithm for nite state-space mdps.</p>
<p>It is often the case that Vt = long before V t is near V ; tighter bounds may be obtained using the span semi-norm on the value function 49].</p>
<p>Partial Observability</p>
<p>For mdps we can compute the optimal policy and use it to act by simply executing (s) for current state s.What happens if the agent is no longer able to determine the state it is currently in with complete reliability?A naive approach would be for the agent to map the most recent observation directly into an action without remembering anything from the past.In our hallway navigation example, this amounts to performing the same action in every location that looks the same|hardly a promising approach.Somewhat better results can be obtained by adding randomness to the agent's behavior: a policy can be a mapping from observations to probability distributions over actions 55].Randomness e ectively allows the agent to sometimes choose di erent actions in di erent locations with the same appearance, increasing the probability that it might choose a good action; in practice deterministic observation-action mappings are prone to getting trapped in deterministic loops 32].</p>
<p>In order to behave truly e ectively in a partially observable world, it is necessary to use memory of previous actions and observations to aid in the disambiguation of the states of the world.The pomdp framework provides a systematic method of doing just that.</p>
<p>POMDP Framework</p>
<p>A partially observable Markov decision process can be described as a tuple hS;A;T;R; ; Oi, where S, A, T , and R describe a Markov decision process; is a nite set of observations the agent can experience of its world; and O : S A ! ( ) is the observation function, which gives, for each action and resulting state, a probability distribution over possible observations (we write O(s 0 ; a; o) for the probability of making observation o given that the agent took action a and landed in state s 0 ).</p>
<p>A pomdp is an mdp in which the agent is unable to observe the current state.Instead, it makes an observation based on the action and resulting state. 4he agent's goal remains to maximize expected discounted future reward.</p>
<p>Problem Structure</p>
<p>We decompose the problem of controlling a pomdp into two parts, as shown in Figure 2. The agent makes observations and generates actions.It keeps an internal belief state, b, that summarizes its previous experience.The component labeled SE is the state estimator: it is responsible for updating the belief state based on the last action, the current observation, and the previous belief state.The component labeled is the policy: as before, it is responsible for generating actions, but this time as a function of the agent's belief state rather than the state of the world.</p>
<p>What, exactly, is a belief state?One choice might be the most probable state of the world, given the past experience.Although this might be a plausible basis for action in some cases, it is not su cient in general.In order to act e ectively, an agent must take into account its own degree of uncertainty.If it is lost or confused, it might be appropriate for it to take sensing actions such as asking for directions, reading a map, or searching for a landmark.</p>
<p>In the pomdp framework, such actions are not explicitly distinguished: their informational properties are described via the observation function.</p>
<p>Our choice for belief states will be probability distributions over states of the  Fig. 3.In this simple pomdp environment, the empty squares are all indistinguishable on the basis of their immediate appearance, but the evolution of the belief state can be used to model the agent's location.</p>
<p>world.These distributions encode the agent's subjective probability about the state of the world and provide a basis for acting under uncertainty.Furthermore, they comprise a su cient statistic for the past history and initial belief state of the agent: given the agent's current belief state (properly computed), no additional data about its past actions or observations would supply any further information about the current state of the world 1,56].This means that the process over belief states is Markov, and that no additional data about the past would help to increase the agent's expected reward.</p>
<p>To illustrate the evolution of a belief state, we will use the simple example depicted in Figure 3; the algorithm for computing belief states is provided in the next section.There are four states in this example, one of which is a goal state, indicated by the star.There are two possible observations: one is always made when the agent is in state 1, 2, or 4; the other, when it is in the goal state.There are two possible actions: east and west.These actions succeed with probability 0:9, and when they fail, the movement is in the opposite direction.If no movement is possible in a particular direction, then the agent remains in the same location.</p>
<p>Assume that the agent is initially equally likely to be in any of the three nongoal states.Thus, its initial belief state is 0:333 0:333 0:000 0:333 , where the position in the belief vector corresponds to the state number.</p>
<p>If the agent takes action east and does not observe the goal, then the new belief state becomes 0:100 0:450 0:000 0:450 .If it takes action east again, and still does not observe the goal, then the probability mass becomes concentrated in the right-most state: 0:100 0:164 0:000 0:736 .Notice that as long as the agent does not observe the goal state, it will always have some non-zero belief that it is in any of the non-goal states, since the actions have non-zero probability of failing.</p>
<p>Computing Belief States</p>
<p>A belief state b is a probability distribution over S. We let b(s) denote the probability assigned to world state s by belief state b.The axioms of probability require that 0 b(s) 1 for all s 2 S and that P s2S b(s) = 1.The state estimator must compute a new belief state, b 0 , given an old belief state b, an action a, and an observation o.The new degree of belief in some state s 0 , b 0 (s 0 ), can be obtained from basic probability theory as follows: The denominator, Pr(oja; b), can be treated as a normalizing factor, independent of s 0 , that causes b 0 to sum to 1.The state estimation function SE(b; a; o) has as its output the new belief state b 0 .Thus, the state-estimation component of a pomdp controller can be constructed quite simply from a given model.</p>
<p>Finding an Optimal Policy</p>
<p>The policy component of a pomdp agent must map the current belief state into action.Because the belief state is a su cient statistic, the optimal policy is the solution of a continuous space \belief mdp."It is de ned as follows:</p>
<p>B, the set of belief states, comprise the state space; A, the set of actions, remains the same; The reward function may seem strange; the agent appears to be rewarded for merely believing that it is in good states.However, because the state estimator is constructed from a correct observation and transition model of the world, the belief state represents the true occupation probabilities for all states s 2 S, and therefore the reward function represents the true expected reward to the agent.</p>
<p>This belief mdp is such that an optimal policy for it, coupled with the correct state estimator, will give rise to optimal behavior (in the discounted in nitehorizon sense) for the original pomdp 59,1].The remaining problem, then, is to solve this mdp.It is very di cult to solve continuous space mdps in the general case, but, as we shall see in the next section, the optimal value function for the belief mdp has special properties that can be exploited to simplify the problem.</p>
<p>Value Functions for POMDPs</p>
<p>As in the case of discrete mdps, if we can compute the optimal value function, then we can use it to directly determine the optimal policy.This section concentrates on nding an approximation to the optimal value function.We approach the problem using value iteration to construct, at each iteration, the optimal t-step discounted value function over belief space.</p>
<p>Policy Trees</p>
<p>When an agent has one step remaining, all it can do is take a single action.</p>
<p>With two steps to go, it can take an action, make an observation, then take  another action, perhaps depending on the previous observation.In general, an agent's non-stationary t-step policy can be represented by a policy tree as shown in Figure 4.It is a tree of depth t that speci es a complete t-step non-stationary policy.The top node determines the rst action to be taken.Then, depending on the resulting observation, an arc is followed to a node on the next level, which determines the next action.This is a complete recipe for t steps of conditional behavior. 5ow, what is the expected discounted value to be gained from executing a policy tree p?It depends on the true state of the world when the agent starts.
O 2 O k O 1 O 2 O k A A A A A A A A O 1 t
In the simplest case, p is a 1-step policy tree (a single action).The value of executing that action in state s is V p (s) = R(s; a(p)) ; where a(p) is the action speci ed in the top node of policy tree p.More generally, if p is a t-step policy tree, then
V p (s) = R(s; a(p)) + Expected value of the future = R(s; a(p)) + X s 0 2S
Pr(s 0 js;a(p))
X o i 2 Pr(o i js 0 ; a(p))V o i (p) (s 0 ) = R(s; a(p)) + X s 0 2S T (s; a(p); s 0 ) X o i 2 O(s 0 ; a(p); o i )V o i (p) (s 0 ) ;
where o i (p) is the (t ?1)-step policy subtree associated with observation o i at the top level of a t-step policy tree p.The expected value of the future is computed by rst taking an expectation over possible next states, s 0 , then considering the value of each of those states.The value depends on which policy subtree will be executed which, itself, depends on which observation is made.So, we take another expectation, with respect to the possible observations, of the value of executing the associated subtree, o i (p), starting in state s 0 .</p>
<p>Because the agent will never know the exact state of the world, it must be able to determine the value of executing a policy tree p from some belief state b.This is just an expectation over world states of executing p in each state:
V p (b) = X s2S b(s)V p (s) :
It will be useful, in the following exposition, to express this more compactly.</p>
<p>If we let p = hV p (s 1 ); : : :
; V p (s n )i, then V p (b) = b p .
Now we have the value of executing the policy tree p in every possible belief state.To construct an optimal t-step non-stationary policy, however, it will generally be necessary to execute di erent policy trees from di erent initial belief states.Let P be the nite set of all t-step policy trees.Then
V t (b) = max p2P b p :
That is, the optimal t-step value of starting in belief state b is the value of executing the best policy tree in that belief state.This de nition of the value function leads us to some important geometric insights into its form.Each policy tree p induces a value function V p that is linear in b, and V t is the upper surface of this collection of functions.So, V t is piecewise-linear and convex.Figure 5 illustrates this property.Consider a world with only two states.In such a world, a belief state consists of a vector of two non-negative numbers, hb(s 1 ); b(s 2 )i, that sum to 1.Because of this constraint, a single number is su cient to describe the belief state.The value function associated with a policy tree p 1 , V p 1 , is a linear function of b(s 1 ) and is shown in the gure as a line.The value functions of other policy trees are similarly represented.Finally, V t is the maximum of all the V p i at each point in the belief space, giving us the upper surface, which is drawn in the gure with a bold line.</p>
<p>When there are three world states, a belief state is determined by two values (again because of the simplex constraint, which requires the individual values to be non-negative and sum to 1).The belief space can be seen as the triangle in two-space with vertices (0; 0), (1; 0), and (0; 1).The value function associated with a single policy tree is a plane in three space, and the optimal value function is a bowl shape that is composed of planar facets; a typical example is shown in Figure 6, but it is possible for the \bowl" to be tipped on its side or to degenerate to a single plane.This general pattern repeats itself in higher dimensions, but becomes di cult to contemplate and even harder to draw!
0 1 b s 1 ( ) V p 1 V p 2 V p 3 expected t-step discounted value
Fig. 5.The optimal t-step value function is the upper surface of the value functions associated with all t-step policy trees.</p>
<p>(0, 1)</p>
<p>(1, 0) (0, 0)
s 1 s 2
Fig. 6.A value function in three dimensions is made up of the upper surface of a set of planes.</p>
<p>The convexity of the optimal value function makes intuitive sense when we think about the value of belief states.States that are in the \middle" of the belief space have high entropy|the agent is very uncertain about the real underlying state of the world.In such belief states, the agent cannot select actions very appropriately and so tends to gain less long-term reward.In lowentropy belief states, which are near the corners of the simplex, the agent can take actions more likely to be appropriate for the current state of the world and, so, gain more reward.This has some connection to the notion of \value of information," 25] where an agent can incur a cost to move it from a highentropy to a low-entropy state; this is only worthwhile when the value of the information (the di erence in value between the two states) exceeds the cost of gaining the information.</p>
<p>Given a piecewise-linear convex value function and the t-step policy trees from which it was derived, it is straightforward to determine the optimal situationaction mapping for execution on the t th step from the end.The optimal value
0 1 V p 1 V p 2 V p 3 expected t-step discounted value a p 1 ( ) a p 2 ( ) a p 3 ( )
Fig. 7.The optimal t-step situation-action mapping is determined by projecting the optimal value function back down onto the belief space.</p>
<p>function can be projected back down onto the belief space, yielding a partition into polyhedral regions.Within each region, there is some single policy tree p such that b p is maximal over the entire region.The optimal action for each belief state in this region is a(p), the action in the root node of policy tree p; furthermore, the entire policy tree p can be executed from this point by conditioning the choice of further actions directly on observations, without updating the belief state (though this is not necessarily an e cient way to represent a complex policy).Figure 7 shows the projection of the optimal value function down into a policy partition in the two-dimensional example introduced in Figure 5; over each of the intervals illustrated, a single policy tree can be executed to maximize expected reward.</p>
<p>Value Functions as Sets of Vectors</p>
<p>It is possible, in principle, that every possible policy tree might represent the optimal strategy at some point in the belief space and, hence, that each would contribute to the computation of the optimal value function.Luckily, however, this seems rarely to be the case.There are generally many policy trees whose value functions are totally dominated by or tied with value functions associated with other policy trees.Figure 8 shows a situation in which the value function associated with policy tree p d is completely dominated by (everywhere less than or equal to) the value function for policy tree p b .The situation with the value function for policy tree p c is somewhat more complicated; although it is not completely dominated by any single value function, it is completely dominated by p a and p b taken together.</p>
<p>Given a set of policy trees, Ṽ, it is possible to de ne a unique 6 minimal subset V that represents the same value function.We will call this a parsimonious representation of the value function, and say that a policy tree is useful if it is a component of the parsimonious representation of the value function.
0 1 b s 1 ( ) V p a V p c V p b expected t-step discounted value V p d
Fig. 8.Some policy trees may be totally dominated by others and can be ignored.</p>
<p>Given a vector, , and a set of vectors V, we de ne R( ; V) to be the region of belief space over which dominates; that is, R( ; V) = fb j b &gt; b ~ ; for all ~ 2 V ? and b 2 Bg : It is relatively easy, using a linear program, to nd a point in R( ; V) if one exists, or to determine that the region is empty 9].</p>
<p>The simplest pruning strategy, proposed by Sondik 58,42], is to test R( ; Ṽ) for every in Ṽ and remove those that are nowhere dominant.A much more e cient pruning method was proposed by Lark and White 64] and is described in detail by Littman 35] and by Cassandra 9].Because it has many subtle technical details, it is not described here.</p>
<p>One Step of Value Iteration</p>
<p>The value function for a pomdp can be computed using value iteration, with the same basic structure as for the discrete mdp case.The new problem, then, is how to compute a parsimonious representation of V t from a parsimonious representation of V t?1 .</p>
<p>One of the simplest algorithms for solving this problem 58,42], which we call exhaustive enumeration, works by constructing a large representation of V t , then pruning it.We let V stand for a set of policy trees, though for each tree we need only actually store the top-level action and the vector of values, .The idea behind this algorithm is the following: V t?1 , the set of useful (t ?1)-step policy trees, can be used to construct a superset V + t of the useful t-step policy trees.A t-step policy tree is composed of a root node with an associated action a and j j subtrees, each a (t ?1)-step policy tree.We propose to restrict our choice of subtrees to those (t ?1)-step policy trees that were useful.For any belief state and any choice of policy subtree, there is always a useful subtree that is at least as good at that state; there is never any reason to include a non-useful policy subtree.</p>
<p>The time complexity of a single iteration of this algorithm can be divided into two parts: generation and pruning.There are jAjjV t?1 j j j elements in V + t : there are jAj di erent ways to choose the action and all possible lists of length j j may be chosen from the set V t?1 to form the subtrees.The value functions for the policy trees in V + t can be computed e ciently from those of the subtrees.Pruning requires one linear program for each element of the starting set of policy trees and does not add to the asymptotic complexity of the algorithm.</p>
<p>Although it keeps parsimonious representations of the value functions at each step, this algorithm still does more much work than may be necessary.Even if V t is very small, it goes through the step of generating V + t , which always has size exponential in j j.In the next sections, we present the Witness algorithm and some complexity analysis, and then brie y outline some other algorithms for this problem that attempt to be more e cient than the approach of exaustively generating V + t .</p>
<p>The Witness Algorithm</p>
<p>To improve the complexity of the value-iteration algorithm, we must avoid generating V + t ; instead, we would like to generate the elements of V t directly.</p>
<p>If we could do this, we might be able to reach a computation time per iteration that is polynomial in jSj, jAj, j j, jV t?1 j, and jV t j.Cheng 10] and Smallwood and Sondik 56] also try to avoid generating all of V + t by constructing V t directly.However, their algorithms still have worst-case running times exponential in at least one of the problem parameters 34].In fact, the existence of an algorithm that runs in time polynomial in jSj, jAj, j j, jV t?1 j, and jV t j would settle the long-standing complexity-theoretic question \Does NP=RP?" in the a rmative 34], so we will pursue a slightly di erent approach.</p>
<p>Instead of computing V t directly, we will compute, for each action a, a set Q a t of t-step policy trees that have action a at their root.We can compute V t by taking the union of the Q a t sets for all actions and pruning as described in the previous section.The witness algorithm is a method for computing Q a t in time polynomial in jSj, jAj, j j, jV t?1 j, and jQ a t j (speci cally, run time is polynomial in the size of the inputs, the outputs, and an important intermediate result).</p>
<p>It is possible that the Q a t are exponentially larger than V t , but this seems to be rarely the case in practice.</p>
<p>In what sense is the witness algorithm superior to previous algorithms for solving pomdps, then?Experiments indicate that the witness algorithm is faster in practice over a wide range of problem sizes 34].The primary complexitytheoretic di erence is that the witness algorithm runs in polynomial time in the number of policy trees in Q a t .There are example problems that cause the other algorithms, although they never construct the Q a t 's directly, to run in time exponential in the number of policy trees in Q a t .That means, if we restrict ourselves to problems in which jQ a t j is polynomial, that the running time of Witness is polynomial.It is worth noting, however, that it is possible to create families of pomdps that Cheng's linear support algorithm (sketched in Section 4.5) can solve in polynomial time that take the witness exponential time to solve; they are problems in which S and V t are very small and Q a t is exponentially larger for some action a.</p>
<p>From the de nition of the state estimator SE and the t-step value function V t (b), we can express Q a t (b) (recall that this is the value of taking action a in belief state b and continuing optimally for t ? 1 steps) formally as
Q a t (b) = X s2S b(s)R(s; a) + X o2 Pr(oja; b)V t?1 (b 0 o ) ;
where b 0 o is the belief state resulting from taking action a and observing o from belief state b; that is, b 0 = SE(b; a; o).Since V is the value of the best action, we have V t (b) = max a Q a t (b).</p>
<p>Using arguments similar to those in Section 4.1, we can show that these Qfunctions are piecewise-linear and convex and can be represented by collections of policy trees.Let Q a t be the collection of policy trees that specify Q a t .Once again, we can de ne a unique minimal useful set of policy trees for each Q function.Note that the policy trees needed to represent the function V t are a subset of the policy trees needed to represent all of the Q a t functions: V t S a Q a t .This is because maximizing over actions and then policy trees is the same as maximizing over the pooled sets of policy trees.</p>
<p>The code in Table 2 outlines our approach to solving pomdps.The basic structure remains that of value iteration.At iteration t, the algorithm has a representation of the optimal t-step value function.Within the value-iteration loop, separate Q-functions for each action, represented by parsimonious sets of policy trees, are returned by calls to witness using the value function from the previous iteration.The union of these sets forms a representation of the optimal value function.Since there may be extraneous policy trees in the combined set, it is pruned to yield the useful set of t-step policy trees, V t .</p>
<p>Witness inner loop</p>
<p>The basic structure of the witness algorithm is as follows.We would like to nd a minimal set of policy trees for representing Q a t for each a.We consider the Q-functions one at a time.The set U a of policy trees is initialized with a single policy tree, with action a at the root, that is the best for some arbitrary belief state (this is easy to do, as described in the following paragraph).At each V 1 := fh0; 0; : : : ; 0ig t := 1 loop t := t + 1 foreach a in A Q a t := witness(V t?1 ; a) prune S a Q a t to get V t until sup b jV t (b) ?V t?1 (b)j &lt; Table 2 Outer loop of the witness algorithm.</p>
<p>iteration we ask, Is there some belief state b for which the true value Q a t (b), computed by one-step lookahead using V t?1 , is di erent from the estimated value Qa t (b), computed using the set U a ?We call such a belief state a witness because it can, in a sense, testify to the fact that the set U a is not yet a perfect representation of Q a t (b).Note that for all b, Qa t (b) Q a t (b); the approximation is always an underestimate of the true value function.</p>
<p>Once a witness is identi ed, we nd the policy tree with action a at the root that will yield the best value at that belief state.To construct this tree, we must nd, for each observation o, the (t ?1)-step policy tree that should be executed if observation o is made after executing action a.If this happens, the agent will be in belief state b 0 = SE(b; a; o), from which it should execute the (t ?1)-step policy tree p o 2 V t?1 that maximizes V po (b 0 ).The tree p is built with subtrees p o for each observation o.We add the new policy tree to U a to improve the approximation.This process continues until we can prove that no more witness points exist and therefore that the current Q-function is perfect.</p>
<p>Identifying a witness</p>
<p>To nd witness points, we must be able to construct and evaluate alternative policy trees.If p is a t-step policy tree, o i an observation, and p 0 a (t ?1)-step policy tree, then we de ne p new as a t-step policy tree that agrees with p in its action and all its subtrees except for observation o i , for which o i (p new ) = p 0 .Figure 9 illustrates the relationship between p and p new .Now we can state the witness theorem 34]: The true Q-function, Q a t , di ers from the approximate Q-function, Qa t , if and only if there is some p 2 U a , o 2 , and p 0 2 V t?1 for which there is some b such that
V pnew (b) &gt; V p(b) ;(1)
for all p 2 U a .That is, if there is a belief state, b, for which p new is an improvement over all the policy trees we have found so far, then b is a witness.... Fig. 9.A new policy tree can be constructed by replacing one of its subtrees.</p>
<p>there are no witness points.A proof of this theorem is included in the appendix.</p>
<p>Checking the witness condition</p>
<p>The witness theorem requires us to search for a p 2 U a , an o 2 , a p 0 2 V t?1 and a b 2 B such that Condition 1 holds, or to guarantee that no such quadruple exists.Since U a , , and V t?1 are nite and (we hope) small, checking all combinations will not be too time consuming.However, for each combination, we need to search all the belief states to test Condition 1.This we can do using linear programming.</p>
<p>For each combination of p, o and p 0 we compute the policy tree p new , as described above.For any belief state b and policy tree p 2 U a , V pnew (b) ?V p(b)</p>
<p>gives the advantage of following policy tree p new instead of p starting from b.</p>
<p>We would like to nd a b that maximizes the advantage over all policy trees p the algorithm has found so far.</p>
<p>The linear program in Table 3 solves exactly this problem.The variable is the minimum amount of improvement of p new over any policy tree in U a at b.It has a set of constraints that restrict to be a bound on the di erence and a set of simplex constraints that force b to be a well-formed belief state.It then seeks to maximize the advantage of p new over all p 2 U a .Since the constraints are all linear, this can be accomplished by linear programming.The total size of the linear program is one variable for each component of the belief state and one representing the advantage, plus one constraint for each policy tree in U, one constraint for each state, and one constraint to ensure that the belief state sums to one. 7f the linear program nds that the biggest advantage is not positive, that is, that 0, then p new is not an improvement over all p trees.Otherwise, it is and b is a witness point.3 The linear program used to nd witness points.</p>
<p>A single step of value iteration</p>
<p>The complete value-iteration step starts with an agenda containing any single useful policy tree and with U a empty.It takes a policy tree o the top of the agenda and uses it as p new in the linear program of Table 3 to determine whether it is an improvement over the policy trees in U a .If a witness point is discovered, the best policy tree for that point is calculated and added to U a and all policy trees that di er from the current policy tree in a single subtree are added to the agenda.If no witness points are discovered, then that policy tree is removed from the agenda.When the agenda is empty, the algorithm terminates.</p>
<p>Since we know that no more than Q a t witness points are discovered (each adds a tree to the set of useful policy trees), only jV t?1 jj jjjQ a t j trees can ever be added to the agenda (in addition to the one tree in the initial agenda).</p>
<p>Each linear program solved has jSj variables and no more than 1 + jSj + jQ a t j constraints.Each of these linear programs either removes a policy tree from the agenda (this happens at most 1 + (jV t?1 j ?1)j jjQ a t j times) or a witness point is discovered (this happens at most jQ a t j times).These facts imply that the running time of a single pass of value iteration using the witness algorithm is bounded by a polynomial in the size of the state space (jSj), the size of the action space (jAj), the number of policy trees in the representation of the previous iteration's value function (jV t?1 j), the number of observations (j j), and the number of policy trees in the representation of the current iteration's Q-functions ( P a jQ a t j).Note that we must assume that the number of bits of precision used in specifying the model is polynomial in these quantities since the polynomial running time of linear programming is expressed as a function of the input precision 54].</p>
<p>Alternative Approaches</p>
<p>The witness algorithm is by no means the only exact algorithm for solving nite-horizon pomdps.The rst such algorithm was described by Sondik 58,56].</p>
<p>The one-pass works by identifying linear regions of the value function one at a time.For each one, it creates a set of constraints that form the border of the true region, then searches those borders to determine whether another region exists beyond the border.Although the algorithm is sophisticated and, in principle, avoids exhaustively enumerating the set of possibly useful policy trees at each iteration, it appears to run more slowly than the simpler enumeration methods in practice, at least for problems with small state spaces 10].</p>
<p>In the process of motivating the one-pass algorithm, Sondik 58] applies the same ideas to nding Q-functions instead of the complete value function.The resulting algorithm might be called the two-pass algorithm 9], and, its form is much like the witness algorithm because it rst constructs each separate Q-function, then combines the Q-functions together to create the optimal value function.Although it appears that the algorithm attracted no attention and was never implemented in over 25 years after the completion of Sondik's dissertation, it was recently implemented and found to be faster than any of the algorithms that predated the witness algorithm 9].</p>
<p>As pointed out in Section 4, value functions in belief space have a natural geometric interpretation.For small state spaces, algorithms that exploit this geometry are quite e cient 16].An excellent example of this is Cheng's linear support algorithm 10].This algorithm can be viewed as a variation of the witness algorithm in which witness points are sought at the corners of regions of the approximate value function de ned by the algorithm's equivalent of the set U. In two dimensions, these corners can be found easily and e ciently; the linear support algorithm can be made to run in low-order polynomial time for problems with two states.In higher dimensions, more complex algorithms are needed and the number of corners is often exponential in the dimensionality.Thus, the geometric approaches are useful only in pomdps with extremely small state spaces.</p>
<p>Zhang and Liu 67] describe the incremental-pruning algorithm, later generalized by Cassandra, Littman, and Zhang 7].This algorithm is simple to implement and empirically faster than the witness algorithm, while sharing its good worst-case complexity in terms of P a jQ a t j.The basic algorithm works like the exhaustive enumeration algorithm described in Section 4.3, but di ers in that it repeatedly prunes out non-useful policy trees during the generation procedure.As a result, compared to exhaustive enumeration, very few nonuseful policy trees are considered and the algorithm runs extremely quickly.</p>
<p>White and Scherer 65] propose an alternative approach in which the reward function is changed so that all of the algorithms discussed in this chapter will tend to run more e ciently.This technique has not yet been combined with the witness algorithm, and may provide some improvement.</p>
<p>The In nite Horizon</p>
<p>In the previous section, we showed that the optimal t-step value function is always piecewise-linear and convex.This is not necessarily true for the in nitehorizon discounted value function; it remains convex 63], but may have innitely many facets.Still, the optimal in nite-horizon discounted value function can be approximated arbitrarily closely by a nite-horizon value function for a su ciently long horizon 59,51].</p>
<p>The optimal in nite-horizon discounted value function can be approximated via value iteration, in which the series of t-step discounted value functions is computed; the iteration is stopped when the di erence between two successive results is small, yielding an arbitrarily good piecewise-linear and convex approximation to the desired value function.From the approximate value function we can extract a stationary policy that is approximately optimal.</p>
<p>Sondik 59] and Hansen 23] have shown how to use algorithms like witness algorithm that perform exact dynamic-programming backups in pomdps in a policy-iteration algorithm to nd exact solutions to many in nite-horizon problems.</p>
<p>Understanding Policies</p>
<p>In this section we introduce a very simple example and use it to illustrate some properties of pomdp policies.Other examples are explored in an earlier paper 8].</p>
<p>The Tiger Problem</p>
<p>Imagine an agent standing in front of two closed doors.Behind one of the doors is a tiger and behind the other is a large reward.If the agent opens the door with the tiger, then a large penalty is received (presumably in the form of some amount of bodily injury).Instead of opening one of the two doors, the agent can listen, in order to gain some information about the location of the tiger.Unfortunately, listening is not free; in addition, it is also not entirely accurate.There is a chance that the agent will hear a tiger behind the left-hand door when the tiger is really behind the right-hand door, and vice versa.</p>
<p>We refer to the state of the world when the tiger is on the left as s l and when it is on the right as s r .The actions are left, right, and listen.The reward for opening the correct door is +10 and the penalty for choosing the door with the tiger behind it is ?100.The cost of listening is ?1.There are only two possible observations: to hear the tiger on the left (tl) or to hear the tiger on the right (tr).Immediately after the agent opens a door and receives a reward or penalty, the problem resets, randomly relocating the tiger behind one of the two doors.</p>
<p>The transition and observation models can be described in detail as follows.The listen action does not change the state of the world.The left and right actions cause a transition to world state s l with probability .5 and to state s r with probability .5 (essentially resetting the problem).When the world is in state s l , the listen action results in observation tl with probability 0.85 and the observation tr with probability 0.15; conversely for world state s r .</p>
<p>No matter what state the world is in, the left and right actions result in either observation with probability 0.5.</p>
<p>Finite-Horizon Policies</p>
<p>The optimal undiscounted nite-horizon policies for the tiger problem are rather striking in the richness of their structure.Let us begin with the situationaction mapping for the time step t = 1, when the agent only gets to make a single decision.If the agent believes with high probability that the tiger is on the left, then the best action is to open the right door; if it believes that the tiger is on the right, the best action is to open the left door.But what if the agent is highly uncertain about the tiger's location?The best thing to do is listen.Guessing incorrectly will incur a penalty of ?100, whereas guessing correctly will yield a reward of +10.When the agent's belief has no bias either way, it will guess wrong as often as it guesses right, so its expected reward for opening a door will be (?100 + 10)=2 = ?45.Listening always has value ?1, which is greater than the value of opening a door at random. Figure 10 shows the optimal 1-step non-stationary policy.Each of the policy trees is shown as a node; below each node is the belief interval 8 over which the policy tree dominates; inside each node is the action at the root of the policy tree. 8The belief interval is speci ed in terms of b(s l ) only since b(s r ) = 1 ?b(s l ).Fig. 10.The optimal situation-action mapping for t = 1 for the tiger problem shows that each of the three actions is optimal for some belief state.We now move to the case in which the agent can act for two time steps.The optimal 2-step non-stationary policy begins with the situation-action mapping for t = 2 shown in Figure 11.This situation-action mapping has a surprising property: it never chooses to act, only to listen.Why?Because if the agent were to open one of the doors at t = 2, then, on the next step, the tiger would be randomly placed behind one of the doors and the agent's belief state would be reset to (0:5; 0:5).So after opening a door, the agent would be left with no information about the tiger's location and with one action remaining.We just saw that with one step to go and b = (0:5; 0:5) the best thing to do is listen.Therefore, if the agent opens a door when t = 2, it will listen on the last step.It is a better strategy to listen when t = 2 in order to make a more informed decision on the last step.</p>
<p>Another interesting property of the 2-step non-stationary policy is that there are multiple policy trees with the same action at the root.This implies that the value function is not linear, but is made up of ve linear regions.The belief states within a single region are similar in that when they are transformed, via SE(b; a; o), the resulting belief states will all lie in the same belief region de ned by the situation-action mapping for t = 1.In other words, every single belief state in a particular region r of the situation-action mapping for t = 2, will, for the same action and observation, be transformed to a belief state that lies in some region r 0 of the situation-action mapping for t = 1.This relationship is shown in Figure 12.</p>
<p>The optimal non-stationary policy for t = 3 also consists solely of policy trees with the listen action at their roots.If the agent starts from the uniform belief state, b = (0:5; 0:5), listening once does not change the belief state enough to make the expected value of opening a door greater than that of listening.The argument for this parallels that for the t = 2 case.This argument for listening in the rst steps no longer applies after t = 3;  the optimal situation-action mappings for t &gt; 3 all choose to open a door for some belief states.Figure 13 shows the structure that emerges in the optimal non-stationary policy for t = 4. Notice that for t = 3 there are two nodes that do not have any incoming arcs from t = 4.This happens because there is no belief state at t = 4 for which the optimal action and any resulting observation generates a new belief state that lies in either of the regions de ned by the unused nodes at t = 3.This graph can also be interpreted as a compact representation of all of the useful policy trees at every level.The forest of policy trees is transformed into a directed acyclic graph by collapsing all of the nodes that stand for the same policy tree into one.Fig. 14.The optimal non-stationary policy for large t converges.</p>
<p>In nite-Horizon Policies</p>
<p>When we include a discount factor to decrease the value of future rewards, the structure of the nite-horizon pomdp value function changes slightly.As the horizon t increases, the rewards received for the nal few steps have decreasing in uence on the situation-action mappings for earlier time steps and the value function begins to converge.In many discounted pomdp problems, the optimal situation-action mapping for large t looks much the same as the optimal situation-action mapping for t ? 1. Figure 14 shows a portion of the optimal non-stationary policy for the discounted nite-horizon version of the tiger problem for large values of t.Notice that the structure of the graph is exactly the same from one time to the next.The vectors for each of the nodes, which together de ne the value function, di er only after the fteenth decimal place.This structure rst appears at time step t = 56 and remains constant through t = 105.When t = 105, the precision of the algorithm used to calculate the situation-action mappings can no longer discern any di erence between the vectors' values for succeeding intervals.At this point, we have an approximately optimal value function for the in nite-horizon discounted problem.</p>
<p>This pomdp has the property that the optimal in nite-horizon value function has a nite number of linear segments.An associated optimal policy has a nite description and is called nitely transient 58,51,9].Pomdps with nitely transient optimal policies can sometimes be solved in nite time using value iteration.In pomdps with optimal policies that are not nitely transient, the in nite-horizon value function has an in nite number of segments; on these problems the sets V t grow with each iteration.The best we can hope for is to solve these pomdps approximately.It is not known whether there is a way of using the value-iteration approach described in this paper for solving all pomdps with nitely transient optimal policies in nite time; we conjecture that there is.The only nite-time algorithm that has been described for solving pomdps with nitely transient optimal policies over the in nite horizon is a version of policy iteration described by Sondik 58].The simpler policy- iteration algorithm due to Hansen 23] has not been proven to converge for all such pomdps. 9</p>
<p>Plan Graphs</p>
<p>One drawback of the pomdp approach is that the agent must maintain a belief state and use it to select an optimal action on every step; if the underlying state space or that V is large, then this computation can be expensive.In many cases, it is possible to encode the policy in a graph that can be used to select actions without any explicit representation of the belief state 59]; we refer to such graphs as plan graphs.Recall Figure 14, in which the algorithm has nearly converged upon an in nite-horizon policy for the tiger problem.Because the situation-action mappings at every level have the same structure, we can make the non-stationary policy into a stationary one by redrawing the edges from one level to itself as if it were the succeeding level.This rearrangement of edges is shown in Figure 15, and the result is redrawn in Figure 16 as a plan graph.</p>
<p>Some of the nodes of the graph will never be visited once either door is opened and the belief state is reset to (0:5; 0:5).If the agent always starts in a state 9 As a technical aside, if there are pomdps that have nitely transient optimal policies for which neither value iteration nor Hansen's policy-iteration algorithm converges, the tiger problem is a good candidate.This is because the behavior of these algorithms on this problem appears to be extremely sensitive to the numerical precision used in comparisons|the better the precision, the longer the algorithms take to converge.In fact, it may be the case that imprecision is necessary for the algorithms to converge on this problem, although it is di cult to test this without detailed formal analysis.Sondik's proof that his policy-iteration algorithm converges depends on controlled use of imprecision and we have not studied how that could best be used in the context of value iteration.Given the initial belief state of (0:5; 0:5) for the tiger problem, some nodes of the plan graph can be trimmed.</p>
<p>of complete uncertainty, then it will never be in a belief state that lies in the region of these non-reachable nodes.This results in a simpler version of the plan graph, shown in Figure 17.The plan graph has a simple interpretation: keep listening until you have heard the tiger twice more on one side than the other.</p>
<p>Because the nodes represent a partition of the belief space and because all belief states within a particular region will map to a single node on the next level, the plan graph representation does not require the agent to maintain an on-line representation of the belief state; the current node is a su cient representation of the current belief.In order to execute a plan graph, the initial belief state is used to choose a starting node.After that, the agent need only maintain a pointer to a current node in the graph.On every step, it takes the action speci ed by the current node, receives an observation, then follows the arc associated with that observation to a new node.This process continues inde nitely.</p>
<p>A plan graph is essentially a nite-state controller.It uses the minimal possible amount of memory to act optimally in a partially observable environment.It is a surprising and pleasing result that it is possible to start with a discrete problem, reformulate it in terms of a continuous belief space, then map the continuous solution back into a discrete controller.Furthermore, the extraction of the controller can be done automatically from two successive equal value functions.</p>
<p>It is also important to note that there is no known a priori bound on the size of the optimal plan graph in terms of the size of the problem.In the tiger problem, for instance, if the probability of getting correct information from the listen action is reduced from 0.85 to 0.65, then the optimal plan graph, shown in Figure 18, is much larger, because the agent must hear the tiger on one side 5 times more than in the other before being su ciently con dent to act.As the observation reliability decreases, an increasing amount of memory is required.</p>
<p>Related Work</p>
<p>In this section, we examine how the assumptions of the pomdp model relate to earlier work on planning in AI.We consider only models with nite state and action spaces and static underlying dynamics, as these assumptions are consistent with the majority of work in this area.Our comparison focuses on issues of imperfect knowledge, uncertainty in initial state, the transition model, the observation model, the objective of planning, the representation of domains, and plan structures.The most closely related work to our own is that of Kushmerick, Hanks, and Weld 30] on the Buridan system, and Draper, Hanks and Weld 14] on the C-Buridan system.</p>
<p>Imperfect Knowledge</p>
<p>Plans generated using standard mdp algorithms and classical planning algorithms10 assume that the underlying state of the process will be known with certainty during plan execution.In the mdp framework, the agent is informed of the current state each time it takes an action.In many classical planners (e.g., snlp 39], ucpop 45]), the current state can be calculated trivially from the known initial state and knowledge of the deterministic operators.</p>
<p>The assumption of perfect knowledge is not valid in many domains.Research on epistemic logic 43,44,52] relaxes this assumption by making it possible to reason about what is and is not known at a given time.Unfortunately, epistemic logics have not been used as a representation in automatic planning systems, perhaps because the richness of representation they provide makes e cient reasoning very di cult.</p>
<p>A step towards building a working planning system that reasons about knowledge is to relax the generality of the logic-based schemes.The approach of cnlp 46] uses three-valued propositions where, in addition to true and false, there is a value unknown, which represents the state when the truth of the proposition is not known.Operators can then refer to whether propositions have an unknown value in their preconditions and can have the value in their e ects.This representation for imperfect knowledge is only appropriate when the designer of the system knows, in advance, what aspects of the state will be known and unknown.It is insu cient for multiple agents reasoning about each others' knowledge and for representing certain types of correlated uncertainty 20].</p>
<p>Formulating knowledge as predicate values that are either known or unknown makes it impossible to reason about gradations of knowledge.For example, an agent that is fairly certain that it knows the combination to a lock might be willing to try to unlock it before seeking out more precise knowledge.Reasoning about levels of knowledge is quite common and natural in the pomdp framework.As long as an agent's state of knowledge can be expressed as a probability distribution over possible states of the world, the pomdp perspective applies.</p>
<p>Initial State</p>
<p>Many classical planning systems (snlp, ucpop, cnlp) require the starting state to be known during the planning phase.An exception is the U-Plan 38] system, which creates a separate plan for each possible initial state with the aim of making these plans easy to merge to form a single plan.Conditional planners typically have some aspects of the initial state unknown.If these aspects are important to the planning process, they are tested during execution.</p>
<p>In the pomdp framework, the starting state is not required to be known precisely and can instead be represented as a probability distribution over possible states.Buridan and C-Buridan also use probability distributions over states as an internal representation of uncertainty, so they can deal with initial-state uncertainty in much the same way.</p>
<p>Transition Model</p>
<p>In classical planning systems, operators have deterministic e ects.The plans constructed are brittle, since they apply to a speci c starting state and require the trajectory through the states to go exactly as expected.Many domains are not easily modeled with deterministic actions, since an action can have di erent results, even when applied in exactly the same state.</p>
<p>Extensions to classical planning, such as cnlp 46] and Cassandra 48] have considered operators with nondeterministic e ects.For each operator, there is a set of possible next states that could occur.A drawback of this approach is that it gives no information about the relative likelihood of the possible outcomes.These systems plan for every possible contingency to ensure that the resulting plan is guaranteed to lead to a goal state.</p>
<p>Another approach used in modeling nondeterministic actions is to de ne a probability distribution over the possible next states.This makes it possible to reason about which of the resulting states are more likely and makes it possible to assess whether a plan is likely to reach the goal even if it is not guaranteed to do so.This type of action model is used in mdps and pomdps as well as in Buridan and C-Buridan.Other work 5,15,19] has used representations that can be used to compute probability distributions over future states.</p>
<p>Observation Model</p>
<p>When the starting state is known and actions are deterministic, there is no need to get feedback from the environment when executing a plan.However, if the starting state is unknown or the actions have nondeterministic e ects, more e ective plans can be built by exploiting feedback, or observations, from the environment concerning the identity of the current state.Completely observable and completely unobservable models are particularly clean but are unrealistic.The pomdp and C-Buridan frameworks model partially observable environments, in that observations provide some information about the underlying state, but not enough to guarantee that it will be known with certainty.This model provides for a great deal of expressiveness (both completely observable and completely unobservable models can be viewed as special cases), but is quite di cult to solve.It is an interesting and powerful model because it allows systems to reason about taking actions to gather knowledge that will be important for later decision making.</p>
<p>Objective</p>
<p>The job of a planner is to nd a plan that satis es a particular objective; most often, the objective is a goal of achievement,that is, to arrive at some state that is in a set of problem-speci c goal states.When probabilistic information is available concerning the initial state and transitions, a more general objective can be used|reaching a goal state with su cient probability (see, for example, work on Buridan and C-Buridan).</p>
<p>A popular alternative to goal attainment is maximizing total expected discounted reward (total-reward criterion).Under this objective, each action results in an immediate reward that is a function of the current state.The exponentially discounted sum of these rewards over the execution of a plan ( nite or in nite horizon) constitutes the value of the plan.This objective is used extensively in most work with mdps and pomdps, including ours.Several authors (for example, Koenig 27]) have pointed out that, given a completely observable problem stated as one of goal achievement, reward functions can be constructed so that a policy that maximizes reward can be used to maximize the probability of goal attainment in the original problem.This shows that the total-reward criterion is no less general than goal achievement in completely observable domains.The same holds for nite-horizon partially observable domains.</p>
<p>Interestingly, a more complicated transformation holds in the opposite direction: any total expected discounted reward problem (completely observable or nite horizon) can be transformed into a goal-achievement problem of similar size 12,69].Roughly, the transformation simulates the discount factor by introducing an absorbing state with a small probability of being entered on each step.Rewards are then simulated by normalizing all reward values to be between zero and one and then \siphoning o " some of the probability of absorption equal to the amount of normalized reward.The (perhaps counterintuitive) conclusion is that goal-attainment problems and reward-type problems are computationally equivalent.</p>
<p>There is a qualitative di erence in the kinds of problems typically addressed with pomdp models and those addressed with planning models.Quite frequently, pomdps are used to model situations in which the agent is expected to go on behaving inde nitely, rather than simply until a goal is achieved.Given the inter-representability results between goal-probability problems and discounted-optimality problems, it is hard to make technical sense of this difference.In fact, many pomdp models should probably be addressed in an average-reward context 17].Using a discounted-optimal policy in a truly in nite-duration setting is a convenient approximation, similar to the use of a situation-action mapping from a nite-horizon policy in receding horizon control.</p>
<p>Littman 35] catalogs some alternatives to the total-reward criterion, all of which are based on the idea that the objective value for a plan is based on a summary of immediate rewards over the duration of a run.Koenig and Simmons 28] examine risk-sensitive planning and showed how planners for the total-reward criterion could be used to optimize risk-sensitive behavior.Haddawy et al. 21] looked at a broad family of decision-theoretic objectives that make it possible to specify trade-o s between partially satisfying goals quickly and satisfying them completely.Bacchus, Boutilier, and Grove 2] show how some richer objectives based on evaluations of sequences of actions can actually be converted to total-reward problems.Other objectives considered in planning systems, aside from simple goals of achievement, include goals of maintenance and goals of prevention 15]; these types of goals can typically be represented using immediate rewards as well.</p>
<p>Representation of Problems</p>
<p>The propositional representations most often used in planning have a number of advantages over the at state-space representations associated with mdps and pomdps.The main advantage comes from their compactness|just as with operator schemata, which can represent many individual actions in a single operator, propositional representations can be exponentially more concise than a fully expanded state-based transition matrix for an mdp.</p>
<p>Algorithms for manipulating compact (or factored) pomdps have begun to appear 14,6]|this is a promising area for future research.At present, however, there is no evidence that these algorithms result in improved planning time signi cantly over the use of a \ at" representation of the state space.</p>
<p>Plan Structures</p>
<p>Planning systems di er in the structure of the plans they produce.It is important that a planner be able to express the optimal plan if one exists for a given domain.We brie y review some popular plan structures along with domains in which they are su cient for expressing optimal behavior.Traditional plans are simple sequences of actions.They are su cient when the initial state is known and all actions are deterministic.A slightly more elaborate structure is the partially ordered plan (generated, for example, by snlp and ucpop), or the parallel plan 4].In this type of plan, actions can be left unordered if all orderings are equivalent under the performance metric.</p>
<p>When actions are stochastic, partially ordered plans can still be used (as in Buridan), but contingent plans can be more e ective.The simplest kind of contingent or branching plan is one that has a tree structure (as generated by cnlp or Plinth).In such a plan, some of the actions have di erent possible outcomes that can be observed, and the ow of execution of the plan is conditioned on the outcome.Branching plans are su cient for representing optimal plans for nite-horizon domains.Directed acyclic graphs (DAGs) can represent the same class of plans, but potentially do so much more succinctly, because separate branches can share structure.C-Buridan uses a representation of contingent plans that also allows for structure sharing (although of a di erent type than our DAG-structured plans).Our work on pomdps nds DAG-structured plans for nite-horizon problems.</p>
<p>For in nite-horizon problems, it is necessary to introduce loops into the plan representation 57,31].(Loops might also be useful in long nite-horizon pomdps for representational succinctness.)A simple loop-based plan representation de-picts a plan as a labeled directed graph.Each node of the graph is labeled with an action and there is one labeled outgoing edge for each possible outcome of the action.It is possible to generate this type of plan graph for some pomdps 47,59,8,22,23].</p>
<p>For completely observable problems with a high branching factor, a more convenient representation is a policy, which maps the current state (situation) to a choice of action.Because there is an action choice speci ed for all possible initial states, policies are also called universal plans 53].This representation is not appropriate for pomdps, since the underlying state is not fully observable.However, pomdp policies can be viewed as universal plans over belief space.</p>
<p>It is interesting to note that there are in nite-horizon pomdps for which no nite-state plan is su cient.Simple 2-state examples can be constructed for which optimal behavior requires counting (i.e., a simple stack machine); there is reason to believe that general pushdown automata and perhaps even Turing machines are necessary to represent optimal plans in general.This argues that, in the limit, a plan is actually a program.Several techniques have been proposed recently for searching for good program-like controllers in pomdps 68,29].We restrict our attention to the simpler nite-horizon case and a small set of in nite-horizon problems that have optimal nite-state plans.</p>
<p>Extensions and Conclusions</p>
<p>The pomdp model provides a rm foundation for work on planning under uncertainty in action and observation.It gives a uniform treatment of action to gain information and action to change the world.Although they are derived through the domain of continuous belief spaces, elegant nite-state controllers may sometimes be constructed using algorithms such as the witness algorithm.However, experimental results 34] suggest that even the witness algorithm becomes impractical for problems of modest size (jSj &gt; 15 and j j &gt; 15).</p>
<p>Our current work explores the use of function-approximation methods for representing value functions and the use of simulation in order to concentrate the approximations on the frequently visited parts of the belief space 33].The results of this work are encouraging and have allowed us to get a very good solution to an 89-state, 16-observation instance of a hallway navigation problem similar to the one described in the introduction.We are optimistic and hope to extend these techniques (and others) to get good solutions to large problems.Another area that is not addressed in this paper is the acquisition of a world model.One approach is to extend techniques for learning hidden Markov models 50,60] to learn pomdp models.Then, we could apply algorithms of the type described in this paper to the learned models.Another approach is to combine the learning of the model with the computation of the policy.This approach has the potential signi cant advantage of being able to learn a model that is complex enough to support optimal (or good) behavior without making irrelevant distinctions; this idea has been pursued by Chrisman 11] and McCallum 40,41].</p>
<p>A Appendix</p>
<p>Theorem 1 Let U a be a non-empty set of useful policy trees, and Q a t be the complete set of useful policy trees.Then U a 6 = Q a t if and only if there is some tree p 2 U a , observation o 2 , and subtree p 0 2 V t?1 for which there is some belief state b such that
V pnew (b) &gt; V p(b) (A.1)
for all p 2 U a , where p new is a t-step policy tree that agrees with p in its action and all its subtrees except for observation o , for which o (p new ) = p 0 .</p>
<p>Note that we are de ning two trees to be equal if they have the same value function; this makes it unnecessary to deal with the e ect of ties in the set U a .</p>
<p>PROOF.The \if" direction is easy since the b can be used to identify a policy tree missing from U a .</p>
<p>The \only if" direction can be rephrased as: If U a 6 = Q a t then there is a belief state b, a p 2 U a , and a p new such that p new has a larger value than any other p 2 U a at b. Start by picking some p 2 Q a t ?U a and choose any b such that p has the highest value at b (there must be such a b since p is useful).Let Now, p is the policy tree in Q a t ?U a that has the highest value at b, and p is the policy tree in U a that has the highest value at b.By construction, V p (b) &gt; V p (b).Now, if p and p di er in only one subtree, then we are done, p can serve as a p new in the theorem.There must be a o satisfying this inequality since otherwise we get the contradiction for all p 2 U a .Therefore, the policy trees p and p new , the observation o , p 0 = o (p ), and the belief state b satisfy the conditions of the theorem.</p>
<p>Fig. 2 .
2
Fig.2.A pomdp agent can be decomposed into a state estimator (SE) and a policy ( ).</p>
<p>b 0 (s 0 ) = Pr(s 0 jo;a;b) = Pr(ojs 0 ; a; b) Pr(s 0 ja;b) Pr(oja; b) = Pr(ojs 0 ; a) P s2S Pr(s 0 ja;b;s)Pr(sja;b) Pr(oja; b) = O(s 0 ; a; o) P s2S T (s; a; s 0 )b(s) Pr(oja; b)</p>
<p>(b; a; b 0 ) is the state-transition function, which is de ned as (b; a; b 0 ) = Pr(b 0 ja;b) a) is the reward function on belief states, constructed from the original reward function on world states: (b; a) = X s2S b(s)R(s; a) :</p>
<p>Fig. 4 .
4
Fig.4.A t-step policy tree captures a sequence of t steps, each of which can be conditioned on the outcome of previous actions.Each node is labelled with the action that should be taken if it is reached.</p>
<p>in U a : V pnew (b) ?V p(b)</p>
<p>Fig. 11 .
11
Fig.11.The optimal situation-action mapping for t = 2 in the tiger problem consists only of the listen action.</p>
<p>Fig. 12 .Fig. 13 .
1213
Fig.12.The optimal non-stationary policy for t = 2 illustrates belief state transformations from t = 2 to t = 1.It consists of ve separate poicy trees.</p>
<p>Fig. 15 .
15
Fig.15.Edges can be rearranged to form a stationary policy.</p>
<p>Fig. 16 .Fig. 17
1617
Fig.16.The optimal in nite-horizon policy for the tiger problem can be drawn as a plan graph.This structure counts the relative number of times the tiger was heard on the left as compared to the right.</p>
<p>Fig.17.Given the initial belief state of (0:5; 0:5) for the tiger problem, some nodes of the plan graph can be trimmed.</p>
<p>Fig. 18 .
18
Fig.18.More memory is needed in the tiger problem when listening reliability is reduced to 0:65.</p>
<p>If p and p di er in more than one subtree, we will identify another policy tree that can act as p new .Choose an observation o 2</p>
<p>2 O 2 O 2 O
222
(s 0 ; a(p); o i )V o i (p) (s 0 ) 1 A = V p (b) :De ne p new to be identical to p except that in the place of subtree o (p), we put o (p ).From this, it follows that V pnew (b) (s 0 ; a(p new ); o i )V o i (pnew) (s 0 ) (s 0 ; a(p); o i )V o i (p) (s 0 )</p>
<p>If observations reveal the precise identity of the current state, the planning model is called \completely observable."The mdp model, as well as some planning systems such as cnlp and Plinth 18,19] assume complete observability.Other systems, such as Buridan and maxplan 37], have no observation model and can attack \completely unobservable" problems.Classical planning systems typically have no observation model, but the fact that the initial state is known and operators are deterministic means that they can also be thought of as solving completely observable problems.</p>
<p>Then, given an uncertain estimate of its location, the robot has to decide what actions to take. In some cases, it might be su cient to ignore its uncertainty and take actions that would be appropriate for the most likely location. In other cases, it might be better for the robot to take actions for the purpose of gathering information, such as searching for a landmark or reading signs on the wall. In general, it will take actions that ful ll both purposes simultaneously.
It is possible to formulate an equivalent model in which the observation depends on the previous state instead of, or in addition to, the resulting state, but it complicates the exposition and adds no more expressive power; such a model could be converted into a pomdp model as described above, at the cost of expanding the state space.
Policy trees are essentially equivalent to \decision trees" as used in decision theory to represent a sequential decision policy; but not to \decision trees" as used in machine learning to compactly represent a single-stage decision rule.
We assume here that two policy trees with the same value function are identical.
In many linear-programming packages, all variables have implicit non-negativity constraints, so the b(s) 0 constraints are not needed.
By \classical planning" we mean linear or partial-order planners using STRIPSlike operators.
1Supported in part by NSF grants IRI-9453383 and IRI-9312395.2Supported in part by DARPA/Rome Labs Planning Initiative grant F30602-95-1-00203Supported in part by Bellcore and NSF CAREER grant IRI-9702576.
Optimal control of Markov decision processes with incomplete state estimation. K J Astr Om, Journal of Mathematical Analysis and Applications. 101965</p>
<p>Rewarding behaviors. Fahiem Bacchus, Craig Boutilier, Adam Grove, Proceedings of the Thirteenth National Conference on Arti cial Intelligence, pages 1160{1167. the Thirteenth National Conference on Arti cial Intelligence, pages 1160{1167AAAI Press/The MIT Press1996</p>
<p>Dynamic Programming and Optimal Control. Dimitri P Bertsekas, Athena Scienti c. 121995</p>
<p>Fast planning through planning graph analysis. L Avrim, Merrick L Blum, Furst, Arti cial Intelligence. 901{21997</p>
<p>Planning with external events. Jim Blythe, Proceedings of the Tenth Conference on Uncertainty in Arti cial Intelligence. the Tenth Conference on Uncertainty in Arti cial Intelligence1994</p>
<p>Computing optimal policies for partially observable decision processes using compact representations. Craig Boutilier, David Poole, Proceedings of the Thirteenth National Conference on Arti cial Intelligence, pages 1168{1175. the Thirteenth National Conference on Arti cial Intelligence, pages 1168{1175AAAI Press/The MIT Press1996</p>
<p>Incremental Pruning: A simple, fast, exact method for partially observable Markov decision processes. Anthony Cassandra, Michael L Littman, Nevin L Zhang, Proceedings of the Thirteenth Annual Conference on Uncertainty in Arti cial Intelligence (UAI{97). the Thirteenth Annual Conference on Uncertainty in Arti cial Intelligence (UAI{97)San Francisco, CAMorgan Kaufmann Publishers1997</p>
<p>Acting optimally in partially observable stochastic domains. Anthony R Cassandra, Leslie Pack Kaelbling, Michael L Littman, Proceedings of the Twelfth National Conference on Arti cial Intelligence, pages 1023{1028. the Twelfth National Conference on Arti cial Intelligence, pages 1023{1028Seattle, WA1994</p>
<p>Exact and Approximate Algorithms for Partially Observable Markov Decision Problems. Anthony Rocco, Cassandra , May 1998Department of Computer Science, Brown UniversityPhD thesis</p>
<p>Algorithms for Partially Observable Markov Decision Processes. Hsien-Te Cheng, 1988British Columbia, CanadaUniversity of British ColumbiaPhD thesis</p>
<p>Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. Lonnie Chrisman, Proceedings of the Tenth National Conference on Arti cial Intelligence. the Tenth National Conference on Arti cial IntelligenceSan Jose, CaliforniaAAAI Press1992</p>
<p>The complexity of stochastic games. Anne Condon, Information and Computation. 962February 1992</p>
<p>Planning under time constraints in stochastic domains. Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, Ann Nicholson, Arti cial Intelligence. 761-21995</p>
<p>Probabilistic planning with information gathering and contingent execution. Denise Draper, Steve Hanks, Dan Weld, 93-12-04December 1993Seattle, WAUniversity of WashingtonTechnical Report</p>
<p>Anytime synthetic projection: Maximizing the probability of goal satisfaction. Mark Drummond, John Bresina, Proceedings of the Eighth National Conference on Arti cial Intelligence. the Eighth National Conference on Arti cial IntelligenceMorgan Kaufmann1990</p>
<p>The optimal search for a moving target when the search path is constrained. James N Eagle, Operations Research. 3251984</p>
<p>On the average cost optimality equation and the structure of optimal policies for partially observable Markov processes. Emmanuel Fern Andez-Gaucherand, Aristotle Arapostathis, Steven I Marcus, Annals of Operations Research. 291991</p>
<p>Conditional linear planning. Robert P Goldman, Mark S Boddy, The Second International Conference on Arti cial Intelligence Planning Systems. Kristian Hammond, The AAAI Press / The MIT Press1994</p>
<p>Epsilon-safe planning. Robert P Goldman, Mark S Boddy, Proceedings of the 10th Conference on Uncertainty in Arti cial Intelligence (UAI94). the 10th Conference on Uncertainty in Arti cial Intelligence (UAI94)Seattle, WA1994</p>
<p>Representing uncertainty in simple planners. Robert P Goldman, Mark S Boddy, Proceedings of the 4th International Conference on Principles of Knowledge Representation and Reasoning (KR94). the 4th International Conference on Principles of Knowledge Representation and Reasoning (KR94)1994</p>
<p>Utility models for goal-directed decisiontheoretic planners. Peter Haddawy, Steve Hanks, 93-06-04June 1993Department of Computer Science and Engineering, University of WashingtonTechnical Report</p>
<p>Cost-e ective sensing during plan execution. Eric A Hansen, Proceedings of the Twelfth National Conference on Arti cial Intelligence, pages 1029{1035. the Twelfth National Conference on Arti cial Intelligence, pages 1029{1035AAAI Press/The MIT Press1994</p>
<p>An improved policy iteration algorithm for partially observable MDPs. Eric A Hansen, Advances in Neural Information Processing Systems. 199810In press</p>
<p>Dynamic Programming and Markov Processes. Ronald A Howard, 1960The MIT PressCambridge, Massachusetts</p>
<p>Information value theory. Ronald A Howard, IEEE Transactions on Systems Science and Cybernetics. 21August 1966SSC-</p>
<p>A new approach to linear ltering and prediction problems. Transactions of the American Society of Mechanical Engineers. R E Kalman, Journal of Basic Engineering. 82March 1960</p>
<p>Optimal probabilistic and decision-theoretic planning using Markovian decision theory. Sven Koenig, UCB/CSD 92/685May 1992BerkeleyTechnical Report</p>
<p>Risk-sensitive planning with probabilistic decision graphs. Sven Koenig, Reid G Simmons, Proceedings of the 4th International Conference on Principles of Knowledge Representation and Reasoning. the 4th International Conference on Principles of Knowledge Representation and Reasoning1994</p>
<p>Genetic Programming: On the Programming of Computers by Means of Natural Selection. John R Koza, 1992The MIT Press</p>
<p>An algorithm for probabilistic planning. Nicholas Kushmerick, Steve Hanks, Daniel S Weld, Arti cial Intelligence. 761-2September 1995</p>
<p>Generating optimal policies for high-level plans with conditional branches and loops. Shieu-Hong Lin, Thomas Dean, Proceedings of the Third European Workshop on Planning. the Third European Workshop on Planning1995</p>
<p>Memoryless policies: Theoretical limitations and practical results. L Michael, Littman, From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior. Dave Cli, Philip Husbands, Jean-Arcady Meyer, Stewart W Wilson, Cambridge, MAThe MIT Press1994</p>
<p>Learning policies for partially observable environments: Scaling up. L Michael, Anthony R Littman, Leslie Cassandra, Kaelbling Pack, Proceedings of the Twelfth International Conference on Machine Learning. M P Singh, the Twelfth International Conference on Machine LearningSan Francisco, CAMorgan Kaufmann1995. 1998Reprinted in Readings in Agents</p>
<p>E cient dynamic-programming updates in partially observable Markov decision processes. L Michael, Anthony R Littman, Leslie Cassandra, Kaelbling Pack, CS-95-191996Providence, RIBrown UniversityTechnical Report</p>
<p>Algorithms for Sequential Decision Making. Michael Lederman, Littman , CS-96-09February 1996Department of Computer Science, Brown UniversityAlso Technical Report</p>
<p>A survey of algorithmic methods for partially observable Markov decision processes. William S Lovejoy, Annals of Operations Research. 2811991</p>
<p>MAXPLAN: A new approach to probabilistic planning. Stephen M Majercik, Michael L Littman, CS-1998-011998Department of Computer Science, Duke UniversityTechnical ReportSubmitted for review</p>
<p>A method for planning given uncertain and incomplete information. T M Mansell, Proceedings of the 9th Conference on Uncertainty in Arti cial Intelligence. the 9th Conference on Uncertainty in Arti cial IntelligenceMorgan Kaufmann PublishersJuly 1993</p>
<p>Systematic nonlinear planning. David Mcallester, David Rosenblitt, Proceedings of the 9th National Conference on Arti cial Intelligence. the 9th National Conference on Arti cial Intelligence1991</p>
<p>Overcoming incomplete perception with utile distinction memory. R Andrew Mccallum, Proceedings of the Tenth International Conference on Machine Learning. the Tenth International Conference on Machine LearningAmherst, MassachusettsMorgan Kaufmann1993190</p>
<p>Instance-based utile distinctions for reinforcement learning with hidden state. R Andrew Mccallum, Proceedings of the Twelfth International Conference on Machine Learning. the Twelfth International Conference on Machine LearningSan Francisco, CAMorgan Kaufmann1995</p>
<p>A survey of partially observable Markov decision processes: Theory, models, and algorithms. George E Monahan, Management Science. 281January 1982</p>
<p>A formal theory of knowledge and action. Robert C Moore, Formal Theories of the Commonsense World. Jerry R Hobbs, Robert C Moore, Norwood, New JerseyAblex Publishing Company1985</p>
<p>Knowledge preconditions for actions and plans. L Morgenstern, Proceedings of the 10th International Joint Conference on Arti cial Intelligence. the 10th International Joint Conference on Arti cial Intelligence1987</p>
<p>UCPOP: A sound, complete, partial order planner for ADL. J S Penberthy, D Weld, Proceedings of the third international conference on principles of knowledge representation and reasoning. the third international conference on principles of knowledge representation and reasoning1992</p>
<p>Conditional nonlinear planning. Mark A Peot, David E Smith, Proceedings of the First International Conference on Arti cial Intelligence Planning Systems. the First International Conference on Arti cial Intelligence Planning Systems1992</p>
<p>A feasible computational approach to in nite-horizon partially-observed Markov decision problems. Loren K Platzman, January 1981Georgia Institute of Technology, Atlanta, GATechnical report</p>
<p>Planning for contingencies: A decision-based approach. Louise Pryor, Gregg Collins, Journal of Arti cial Intelligence Research. 41996</p>
<p>Markov Decision Processes|Discrete Stochastic Dynamic Programming. Martin L Puterman, 1994John Wiley &amp; Sons, IncNew York, NY</p>
<p>A tutorial on hidden Markov models and selected applications in speech recognition. Lawrence R Rabiner, Proceedings of the IEEE. 772February 1989</p>
<p>Optimal control for partially observable Markov decision processes over an in nite horizon. Katsushige Sawaki, Akira Ichikawa, Journal of the Operations Research Society of Japan. 211March 1978</p>
<p>The frame problem and knowledgeproducing actions. R B Scherl, H J Levesque, Proceedings of the 11th National Conference on Arti cial Intelligence. the 11th National Conference on Arti cial Intelligence1993</p>
<p>Universal plans for reactive robots in unpredictable environments. Marcel J Schoppers, Proceedings of the International Joint Conference on Arti cial Intelligence. the International Joint Conference on Arti cial Intelligence198710</p>
<p>Theory of Linear and Integer Programming. Alexander Schrijver, 1986Wiley-InterscienceNew York, NY</p>
<p>Model-free reinforcement learning for non-Markovian decision problems. Pal Satinder, Tommi Singh, Michael I Jaakkola, Jordan, Proceedings of the Eleventh International Conference on Machine Learning. the Eleventh International Conference on Machine LearningSan Francisco, CaliforniaMorgan Kaufmann1994</p>
<p>The optimal control of partially observable Markov processes over a nite horizon. Richard D Smallwood, Edward J Sondik, Operations Research. 211973</p>
<p>Representation and evaluation of plans with loops. Working notes for the 1995 Stanford Spring Symposium on Extended Theories of Action. David E Smith, Mike Williamson, 1995</p>
<p>The Optimal Control of Partially Observable Markov Processes. Edward Sondik, 1971Stanford UniversityPhD thesis</p>
<p>The optimal control of partially observable Markov processes over the in nite horizon: Discounted costs. Edward J Sondik, Operations Research. 2621978</p>
<p>Hidden Markov model induction by Bayesian model merging. Andreas Stolcke, Stephen Omohundro, Advances in Neural Information Processing Systems 5. Stephen Jos E Hanson, Jack D Cowan, C Lee Giles, San Mateo, CaliforniaMorgan Kaufmann199311</p>
<p>Control strategies for a stochastic planner. Jonathan Tash, Stuart Russell, Proceedings of the 12th National Conference on Arti cial Intelligence. the 12th National Conference on Arti cial Intelligence1994</p>
<p>Solving H-horizon, stationary Markov decision problems in time proportional to log(H). Paul Tseng, Operations Research Letters. 951990</p>
<p>Application of Jensen's inequality for adaptive suboptimal design. C C White, D Harrington, Journal of Optimization Theory and Applications. 3211980</p>
<p>Partially observed Markov decision processes: A survey. Chelsea C White, Iii , Annals of Operations Research. 321991</p>
<p>Solution procedures for partially observed Markov decision processes. Chelsea C White, William T Scherer, 791{ 797Operations Research. 375September-October 1989</p>
<p>Tight performance bounds on greedy policies based on imperfect value functions. Ronald J Williams, Leemon C Baird, Iii , NU- CCS-93-14November 1993Boston, MANortheastern University, College of Computer ScienceTechnical Report</p>
<p>Planning in stochastic domains: Problem characteristics and approximation. Nevin L Zhang, Wenju Liu, HKUST-CS96-311996Department of Computer Science, Hong Kong University of Science and TechnologyTechnical Report</p>
<p>Incremental self-improvement for lifetime multi-agent reinforcement learning. Jieyu Zhao, J , H Schmidhuber, From Animals to Animats: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior. Pattie Maes, Maja J Mataric, Jean-Arcady Meyer, Jordan Pollack, Stewart W Wilson, The MIT Press1996</p>
<p>The complexity of mean payo games on graphs. Uri Zwick, Mike Paterson, Theoretical Computer Science. 1581{21996</p>            </div>
        </div>

    </div>
</body>
</html>
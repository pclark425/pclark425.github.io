<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7794 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7794</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7794</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-276647576</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.20309v1.pdf" target="_blank">EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements have positioned AI, and particularly Large Language Models (LLMs), as transformative tools for scientific research, capable of addressing complex tasks that require reasoning, problem-solving, and decision-making. Their exceptional capabilities suggest their potential as scientific research assistants but also highlight the need for holistic, rigorous, and domain-specific evaluation to assess effectiveness in real-world scientific applications. This paper describes a multifaceted methodology for Evaluating AI models as scientific Research Assistants (EAIRA) developed at Argonne National Laboratory. This methodology incorporates four primary classes of evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open Response to evaluate advanced reasoning and problem-solving skills; 3) Lab-Style Experiments involving detailed analysis of capabilities as research assistants in controlled environments; and 4) Field-Style Experiments to capture researcher-LLM interactions at scale in a wide range of scientific domains and applications. These complementary methods enable a comprehensive analysis of LLM strengths and weaknesses with respect to their scientific knowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of LLM advancements, we designed the methodology to evolve and adapt so as to ensure its continued relevance and applicability. This paper describes the methodology state at the end of February 2025. Although developed within a subset of scientific domains, the methodology is designed to be generalizable to a wide range of scientific domains.</p>
                <p><strong>Cost:</strong> 0.032</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7794.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7794.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EAIRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluating AI models as scientific Research Assistants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A holistic evaluation methodology combining four complementary techniques (MCQ Benchmarks, Open-Response Benchmarks, Lab-style Experiments, and Field-style Experiments) plus cross-cutting aspects of trust/safety, uncertainty quantification, and scalable software infrastructure, designed to assess LLMs as scientific research assistants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (general scientific research)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation methodology / framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>EAIRA methodology (MCQ, Open-Response, Lab-style, Field-style)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Combine fast breadth checks (MCQs), deeper open-response evaluations, controlled expert-driven lab-style end-to-end experiments, and large-scale field-style interaction logging and automated analysis, with cross-cutting UQ and safety checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>various (accuracy for MCQs; rubric scores for open-response; expert A-F or numeric rubric for lab experiments; 1-5 Likert-like criteria for field experiments; UQ metrics like AUC/entropy)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>MCQ accuracy (% correct); open-response rubric scores (1–5 or composite); lab-style expert grades (A–F or numeric per-skill scales); field-style criteria scored 1–5; UQ metrics (AUC for uncertainty separation, entropy values).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Lab-style: domain experts (PhD-level) in controlled sessions with proctors; Field-style JAM session: 180 conversations, 125 valid transcripts, expert filtering; lab experiments: 3 experiments ~20 hours, ~100 prompts, multiple experts.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Described aggregated outcomes across methods: e.g., field JAM session: Importance 82% rated very important/critical; Strength 59% reported improvement; novelty only 21%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Lab-style experiments are resource intensive and low-scale; field-style requires careful annotation and validation; evaluation must evolve as LLMs advance; LLM-as-a-judge pipelines need human validation due to hallucination risk.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7794.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STaR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Skills, Trust, and Reliability evaluation framework (STaR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular, scalable software evaluation framework tailored for HPC environments that orchestrates datasets, prompting, model adapters, inference backends, UQ, and metrics aggregation for LLM evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation software framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>STaR evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Modular pipeline (data layer, prompting layer, model adapter, result layer) to run scenarios, generate prompts (few-shot/CoT), query models (local or API), aggregate results and compute general and uncertainty metrics; supports distributed GPU and model-parallel execution.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>standard benchmark metrics (accuracy, task-specific metrics) plus UQ metrics and resource metrics (GPU hours)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Task accuracy (%), UQ metrics (AUC/entropy), GPU-hours (A100 hours), aggregated composite scores as defined per benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>supports MMLU-Pro, OpenLLM Leaderboard V2, Wildbench, DecodingTrust, domain benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>Designed to improve reproducibility via standardized runners/adapters and result management</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Proof-of-concept evaluations on Polaris with Llama variants; Table VI reports per-task metrics and GPU hours (e.g., Llama-3.3-70B-Instruct: IFEval 0.6745, GPU Hours 81.39).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Work in progress; requires HPC-specific adaptations; integration with varied backends remains ongoing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7794.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AGIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic continuous Generation and validation of Increasingly Large MCQ benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid manual+automated workflow (AGIL) for scalable MCQ benchmark generation and validation using expert-authored gold standards plus LLM-based automated generation and LLM-as-a-judge validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (AI for science)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark generation/validation process</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>AGIL MCQ generation and validation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Human domain experts author seed MCQs and validate; LLMs generate additional MCQs from papers; automated LLM-as-a-judge evaluates MCQ criteria enabling scalable acceptance of high-quality MCQs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>acceptance rate; judge-predicted acceptance accuracy; inter-reviewer agreement; model accuracy on MCQs</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Acceptance: binary accepted/rejected; judge prediction accuracy (%); MCQ accuracy (% correct by evaluated LLMs); difficulty categories (easy/medium/hard).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AI4S (initial 980 MCQs: 720 manual, 260 auto)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>140 domain experts participated in hackathons; manual reviews: 588 reviews, 317 assessed, 254 accepted; difficulty labeled as easy/medium/hard.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Automatic validation model (Mistral Large 2) predicted acceptance with 72% accuracy; manual reviewer agreement on acceptance was 61%; Llama-3-8B accuracy on 254 accepted AI4S MCQs = 20.08% (±0.0252).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Quality of manually validated automatically-generated MCQs comparable to manually generated ones; AI4S judged more challenging than GPQA (AI4S avg quality 4.55 vs GPQA 4.45).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>High difficulty to craft and validate high-quality scientific MCQs; potential dataset contamination; need to keep portions private to avoid training contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7794.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI4S</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI for Science multi-domain MCQ benchmark (AI4S)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-domain MCQ benchmark spanning scientific fields (Computer Science, Astrophysics, Climate, Physics, Chemistry) combining human- and automatically-generated MCQs to evaluate domain knowledge and reasoning at graduate-level difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (CS, astrophysics, climate, physics, chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark dataset</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>AI4S MCQ evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluate LLMs on MCQs with four distractors (20% random baseline); compute accuracy, difficulty-normalized accuracy, and quality scores via judge criteria (eight criteria rated 1–5 or pass/fail).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (raw and normalized), quality scores per criteria (1–5), judge acceptance rate</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy = % correct; normalized accuracy adjusts for chance; quality average across seven rated criteria (1–5); correct criterion Pass=5/Fail=0.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AI4S (980 MCQs created; 254 accepted so far)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Manual validation by experts; difficulty levels assigned; acceptance criteria used across reviewers; multiple reviews per question where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Llama-3-8B on 254 accepted MCQs: accuracy 0.2008 (±0.0252). Llama-3-70B accuracy 0.2598. AI4S average quality 4.55 vs GPQA 4.45. Automatic judge acceptance prediction 72% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>AI4S more challenging than GPQA; LLMs perform worse on AI4S than on GPQA (e.g., Llama 3-70B: 26% vs GPQA 49%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Risk of redundancy in automatically generated MCQs; need for multiple expert reviewers per question; contamination concerns if benchmark fully public.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7794.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Astronomy Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Astronomy domain-specific MCQ benchmark (Astronomy Benchmark from ARAA corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatically generated MCQ dataset (4425 MCQs) created by transcribing Annual Review of Astronomy and Astrophysics articles and using a long-context LLM to produce five MCQs per paper, intended to probe specialized astronomy knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astronomy/astrophysics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>domain-specific MCQ benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Astronomy MCQ evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluate LLMs on domain-specific MCQs, report accuracy and compute performance-to-cost tradeoffs across models and model classes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%) and compute cost per performance (cost per 0.1M tokens and compute-to-accuracy scaling)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy = % correct on MCQs; cost per 0.1M tokens = monetary cost metric; compute-to-accuracy slope measured (e.g., 3.5 percentage points per 10x price within model families).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Astronomy Benchmark (4425 MCQs from 885 ARAA articles)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Questions generated automatically and designed to be specific yet independent of sections; validation performed when used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Claude-3.5-Sonnet accuracy 85.0%; GPT-4o 80.4%; Gemini-1.5-Pro 77.6%. Observed compute vs accuracy scaling: ~3.5 percentage points per 10x price within model series.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Open-weight models lag proprietary ones; performance varies across subfields and languages; automatic generation may bias toward recent or well-covered topics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7794.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Climate Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Climate domain-specific MCQ benchmark (from IPCC sections)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatically generated MCQ dataset (752 questions) derived from section-wise parsing of IPCC reports using OCR and LLM-based generation to evaluate LLM competence in climate science.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>climate science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>domain-specific MCQ benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Climate MCQ evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use section-level inputs from IPCC to prompt LLMs to output one MCQ per section; evaluate models by MCQ accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%) on climate MCQs</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy = % correct answers across the 752 items.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Climate Benchmark (752 MCQs from IPCC reports)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Automated generation supplemented by human oversight recommended; noted need to remove redundant/semantically similar questions.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-4o achieved 87.34% accuracy; Llama 3.8 achieved 78.48%; Phi 4 scored 54.43%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Automated generation sometimes produces semantically redundant questions; human oversight needed to ensure diversity and quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7794.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCQ Benchmarks (general list)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiple-choice question benchmarks (MMLU, MMLU-PRO, GSM8K, GSM1K, MATH, ARC, HellaSwag, MoleculeQA, MolPuzzle, ChemBench, GPQA, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variety of established multiple-choice benchmarks used to rapidly evaluate LLM factual recall and domain reasoning across many subjects, from general knowledge to domain-specific chemistry and math.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark datasets</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>MCQ benchmark evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Present LLMs with multiple-choice questions and compute accuracy; some adapted to multiple-choice to reduce output ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>% of correct answers; random baselines depend on number of choices (e.g., 25% for 4 choices, 20% for 5 choices).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MMLU, MMLU-PRO, GSM8K, GSM1K, MATH, ARC, HellaSwag, MoleculeQA, MolPuzzle, ChemBench, GPQA</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>MCQs test breadth but not depth/iterative problem solving; risk of dataset saturation and training contamination; must adapt as models improve.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7794.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-response scorers (statistical)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>N-gram statistical scorers (BLEU, ROUGE, METEOR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatic n-gram overlap metrics that compare generated text to reference answers by measuring precision/recall of matching n-grams, used for open-response evaluation but limited in semantic/ reasoning assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / open-response evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BLEU / ROUGE / METEOR statistical scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute n-gram overlap between model output and gold reference(s); BLEU emphasizes precision, ROUGE recall, METEOR considers synonyms and ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEU, ROUGE, METEOR scores (typically 0–1 or 0–100 scaled)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Score where higher indicates greater n-gram overlap (BLEU precision-based; ROUGE recall-oriented; METEOR harmonic precision/recall with synonym matching).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Do not capture semantics or reasoning; can be gamed and insensitive to correct but lexically different answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7794.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-response scorers (embedding & LLM judges)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding-based and LLM-as-a-judge approaches (BERTScore, CheckEmbed, LLM-as-a-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Semantic similarity evaluators using contextual embeddings (e.g., BERTScore, CheckEmbed) and recent LLM-as-a-judge methods that rate or pairwise-compare open responses when reference answers are absent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>open-response evaluation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metrics/frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BERTScore / CheckEmbed / LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>BERTScore: cosine similarity aggregated over token embeddings; CheckEmbed: embedding comparison for verification; LLM-as-a-judge: instruct an LLM to score or compare answers single-wise or pairwise, producing scores and rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>semantic similarity scores (cosine similarity aggregated), judge scores (1–n scale), pairwise preference outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>BERTScore yields precision/recall/F1-like similarity measures (0–1); LLM judge outputs numeric ratings (e.g., 1–5 or 1–10) or preference labels.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>LLM-as-a-judge advantages: scalability and explainability; limitations include position bias, verbosity bias, self-enhancement bias, and grading math/reasoning challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used in automatic validation of AI4S MCQs with Mistral Large 2 as judge achieving 72% predictive accuracy vs human acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLM-as-a-judge can be biased and requires further validation against human judgments; embedding methods may miss fine-grained reasoning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7794.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciCode</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciCode — Scientific Code Generation Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A manually curated benchmark of 80 research-level coding problems decomposed into 338 subproblems with gold solutions and test cases to evaluate LLMs' ability to write and integrate scientific code across multiple disciplines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational science / multiple scientific domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>open-response code-generation benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SciCode automatic test-case evaluation + background vs no-background modes</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide LLMs with problem prompts (zero-shot or with background) requiring implementation of Python functions for subproblems; evaluate using gold test cases and require all subproblems + integration to be correct to mark main problem solved.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>main-problem solved rate (%), subproblem accuracy (%), code test-case pass rates</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Main problem solved when all subproblems and integration pass tests; reported as percent of problems solved; subproblem accuracy is fraction of subproblems passing tests.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciCode (80 problems, 338 subproblems)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Problems annotated and verified by at least two senior researchers to ensure correctness and avoid public dataset overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>Gold solutions and test cases allow reproducible automated evaluation of generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>State-of-the-art models solved few main problems; example: top models solved 7.7% of main SciCode problems; open-weight models performed poorly overall.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>With scientific background provided, models improved; however, still far below human-level performance on integrated research-level coding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Generated solutions accumulate errors across subproblems; models need domain background to improve; benchmark is challenging for current LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7794.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALDbench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALDbench — Atomic Layer Deposition materials synthesis benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-response benchmark in materials synthesis focused on atomic layer deposition (ALD), containing expert-generated questions and human-evaluated responses across four rubric criteria (Overall quality, Specificity, Relevance, Accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science / chemistry (atomic layer deposition)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>open-response materials synthesis evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ALDbench expert rubric evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>PhD-level experts generate domain-appropriate questions and rate model responses on four criteria using 1–5 rubrics; statistical correlation analysis used to relate difficulty/specificity to scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>per-response rubric scores (Overall quality, Specificity, Relevance, Accuracy; 1–5), composite quality score</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Each criterion rated 1 (lowest) to 5 (highest); composite score average of criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ALDbench (curated questions by six PhD experts)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Seven PhD-level human experts reviewed GPT-4o outputs; each response graded on four criteria with provided rubrics.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Model composite quality score 3.7; 36% of questions received at least one below-average score; detected at least five hallucination instances; statistically significant negative correlations between question difficulty/specificity and response quality (p-values 0.033, 0.016, 0.007).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Open-response grading is labor-intensive and subjective; requires expert involvement; LLM hallucinations present safety concerns in high-consequence synthesis domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7794.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lab-style experiments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lab-style end-to-end experiments (expert-proctored)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Controlled, expert-run multi-turn interactive experiments where domain experts attempt to solve real research problems with LLM assistance; responses and interactions are recorded and scored by experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>domain-specific (examples: parallel/distributed computing, chemistry, biology)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>interactive evaluation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Lab-style expert evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Assign a realistic research problem to experts who iteratively interact with multiple LLMs using consistent prompts; experts analyze each response and provide granular and high-level scoring (A–F scales or enumerated per-skill rubrics); measure metrics like number of prompts needed to reach key insights.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>expert qualitative grades (A–F), per-skill rubric scores, number of prompts to solution, correctness/conciseness/precision metrics</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>High-level grade scale A (human reference) to F (worst); number of prompts metric counts interactions required to arrive at key insight.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Experiments used five domain experts (one per experiment), three initial experiments ~20 hours of interactions ~100 prompts; scoring included detailed per-prompt analysis and overall scales.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Example: For zero-overhead checkpointing problem, GPT-4o required five prompts to reach key insight while Argo/O1-preview reached it in one prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Experts compared model outputs to human researcher performance using A–F scale; lab-style provides fine-grained capability assessment vs human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Highly manual, low scalability, high expert time cost, limited coverage/generalizability across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7794.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Field-style experiments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Field-style (in-the-wild) experiments and JAM sessions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large-scale capture and automated analysis of real researcher–LLM interactions (in-the-wild), using aggregate behavioral signals and LLM-as-a-judge pipelines to infer model strengths and weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Argo/O1-preview (example in JAM session)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain scientific workflows</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>real-world interaction evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Field-style evaluation (JAM session + automated LLM-as-a-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Collect thousands of conversations between researchers and models during routine workflows or structured JAM sessions; ask participants to self-rate across criteria (Novelty, Productivity, Solution, Strength, Importance) and apply LLM-as-a-judge (e.g., Llama-3.3-70B-Instruct) to transcripts to score many scientific criteria automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>user self-report Likert scores (1–5), LLM-as-a-judge numeric scores (1–10) across 29 criteria, usage behavioral signals (rephrases, abandonment), aggregated strength/weakness summaries</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Self-report: 1–5 scale; LLM-as-a-judge: 1–10 or 0 for N/A per criterion; behavioral signals used as proxy for failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Argonne JAM session dataset (180 conversations, filtered to 125 challenging transcripts)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>180 conversations recorded; participants self-scored; later 125 transcripts evaluated by Llama-3.3-70B-Instruct with prompts to score 29 scientific criteria, summaries batched for synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Self-report: Importance 82% rated very important/critical; Strength 59% improvement; Productivity 51% equated to PhD/postdoc contributions; Solution quality 50% strong/exceptional; Novelty 21%. LLM-as-a-judge pipeline produced 29-criteria scored summaries (proof-of-concept).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLM-as-a-judge outputs need human validation due to hallucinations; self-reporting gives coarse diagnostics; annotating at scale with fine-grained labels is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7794.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-judge evaluation method</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using an LLM to grade or compare other LLMs' outputs either pairwise or single-answer grading to scale open-response evaluation when gold references are unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral Large 2 (used as judge in AI4S); Llama-3.3-70B-Instruct (used for JAM transcript analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation methodology (cross-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automated evaluation method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-as-a-judge (pairwise and single grading)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt an LLM to score a single response on multiple criteria or to compare two responses and choose a better one; can return numeric scores and rationales used for automated acceptance or ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>judge accuracy vs human (predict acceptance), numeric rubric scores, pairwise preferences</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Predictive accuracy = % agreement with human accept/reject; numeric rubrics often 1–5 or 1–10 scales; pairwise outputs return better/tie.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used Mistral Large 2 to evaluate MCQs on eight criteria in one prompt; judge-predicted acceptance accuracy 72%; inter-reviewer human acceptance agreement 61%.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Judge (Mistral Large 2) predicted whether a question would be accepted with 72% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Biases include position bias, verbosity bias, self-enhancement bias; limited capability grading math/reasoning; requires calibration and human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e7794.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Uncertainty quantification (UQ) techniques</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UQ methods for black-box LLMs (Question Rephrasing, sampling, semantic entropy, aggregation, entropy-based AUC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of methods to assess input and output uncertainty for black-box LLMs including question rephrasing sensitivity, sampling-based repeated queries, semantic entropy measures, and entropy/AUC scores to separate correct vs incorrect predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 / GPT-4 (evaluated examples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>chemistry experiments and general evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>uncertainty quantification techniques</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Question Rephrasing + sampling + entropy-based UQ</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Assess input uncertainty by rephrasing questions/presentations (e.g., alternate SMILES) and measuring sensitivity; assess output uncertainty via sampling repeated queries and compute entropy; evaluate discriminative power via AUC of uncertainty metric separating correct/incorrect answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>AUC (uncertainty discriminative power), accuracy, F1, entropy measures</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>AUC ranges 0–1 (higher better at separating correct vs incorrect by uncertainty); accuracy and F1 standard definitions; entropy in bits or normalized units.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Chemistry property datasets: BBBP, HIV, Tox21; forward reaction prediction tasks</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>AUC for original SMILES ranged 0.546–0.774; entropy-based output-uncertainty AUCs ranged 0.86–0.99 indicating good uncertainty signal despite limited accuracy; reformulated inputs reduced accuracy and F1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Good UQ signals do not imply high base accuracy; sensitivity to input format (e.g., SMILES variants) highlights fragility; black-box access limits internal-method UQ.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e7794.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CHEMRISK</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CHEMRISK — Chemical Risk Detection Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemical risk benchmark developed with LLNL to assess LLM handling of chemical knowledge across chemical understanding, molecular design, and molecular synthesis tasks using MCQ and open-form items with SMILES/SELFIES representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o and o1 (evaluated subset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>chemistry / energetics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>safety-focused benchmark dataset</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>CHEMRISK benchmark tasks (SMILES-to-Name, property prediction, forward synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Curated base data from CSD and USPTO; evaluate LLMs on MCQ-style and free-form tasks such as SMILES-to-name conversion, property regression/ranking, and (retrosynthesis/forward synthesis) with accuracy and other metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy per task</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy = fraction correct on multiple-choice Q&A tasks; other tasks (regression) may use RMSE/AUC but preliminary results reported accuracy for MCQ conversions.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>CHEMRISK (constructed dataset using CSD, USPTO proxies)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Benchmark developed with domain experts at LLNL and manual quality checks performed on base data.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>On a subset: GPT-4o and o1 achieved perfect accuracy (1.0) on SMILES-to-Name; o1 achieved 1.0 on molecular design where GPT-4o scored 0.4; both achieved 0.6 on molecular synthesis task subset.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Preliminary proof-of-concept; more development needed for broader safety evaluation across chemical subdomains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e7794.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SafetyBench / SALAD-Bench / DecodingTrust / TrustLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Safety and trustbenchmarks (SafetyBench, SALAD-Bench, DecodingTrust, TrustLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sets of benchmarks and frameworks to evaluate LLM safety properties like toxicity, bias, privacy, hallucinations, and machine ethics using MCQs and open-response tests across multiple categories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>safety / ethics evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark collections and evaluation frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SafetyBench / SALAD-Bench / DecodingTrust / TrustLLM</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Large sets of categorized MCQs (SafetyBench: 11,435 MCQs across seven categories), hierarchical safety taxonomies (SALAD-Bench), and open-response trust evaluations (DecodingTrust); TrustLLM covers six dimensions across >30 datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>task-specific accuracy/classification metrics; aggregated safety scores and leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Fraction correct on MCQ safety items; scores per dimension aggregated across datasets; some provide public leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SafetyBench, SALAD-Bench, DecodingTrust datasets, TrustLLM datasets</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Few safety benchmarks target high-consequence scientific domains (chemistry/biology/nuclear) specifically; need domain-specific safety evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e7794.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WildBench / HaluEval-Wild</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WildBench and HaluEval-Wild (in-the-wild evaluation datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks designed to collect challenging user queries and real-world interactions to analyze LLM robustness and hallucinations in natural usage scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / in-the-wild evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dataset / evaluation approach</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>WildBench / HaluEval-Wild</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Collect real user queries and responses to evaluate LLM performance and hallucination propensity in naturalistic settings; analyze patterns and failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>failure/hallucination rates, user-behavior proxies, task-specific accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Proportion of interactions exhibiting hallucinations or failure; engagement signals used to infer failures.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>WildBench, HaluEval-Wild</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Mostly non-scientific domains previously; adaptation to scientific contexts required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.18">
                <h3 class="extraction-instance">Extracted Data Instance 18 (e7794.18)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenLLM Leaderboard V2 / Harness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenLLM Leaderboard V2 evaluated via EleutherAI LM Evaluation Harness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation suite used with STaR to run standardized benchmark tasks across open models with aggregated task metrics, also reporting computational resource usage (GPU hours).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama variants (examples in Table VI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark suite and harness</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>OpenLLM Leaderboard V2 via Harness</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Run a collection of six challenging tasks using the Harness runner and compute per-task metrics (e.g., IFEval, BBH, MATH, GPQA, MuSR, MMLU-PRO) along with GPU hours consumed.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>task-specific metrics (e.g., IFEval, BBH scores), GPU hours</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Task metric scores (normalized), GPU hours measured in A100/GPU-hours.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>OpenLLM Leaderboard V2 tasks</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table VI reports model scores and GPU Hours; e.g., Llama-3.3-70B-Instruct IFEval 0.6745 and GPU Hours 81.39.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Resource intensive for large models; harness requires HPC-specific adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.19">
                <h3 class="extraction-instance">Extracted Data Instance 19 (e7794.19)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CHEM datasets (CSD, USPTO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cambridge Structural Database (CSD) and United States Patent and Trademark Office (USPTO) datasets used as CHEMRISK base</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Established chemical data sources used to curate ground truth data for tasks such as property regression, synthesis, and SMILES/name conversions in CHEMRISK.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>data sources</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>dataset curation for CHEMRISK</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Curate structural/property/synthesis examples from CSD and reaction patents in USPTO to build standard evaluation tasks for chemical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>CSD, USPTO (as CHEMRISK base)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Manual quality checks applied to curated base data.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Domain-specific curation required to reduce noise and ensure quality for safety-sensitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.20">
                <h3 class="extraction-instance">Extracted Data Instance 20 (e7794.20)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lab/Field scoring rubrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation rubrics and criteria (AI4S acceptance criteria, ALDbench 4-criterion rubric, JAM 5-criterion self-evaluation, LLM-as-a-judge 29 criteria)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Explicit scoring rubrics used across experiments: AI4S eight acceptance criteria for MCQs; ALDbench uses Overall Quality, Specificity, Relevance, Accuracy (1–5); JAM session asked Novelty, Productivity, Solution, Strength, Importance (1–5); LLM-as-a-judge prompt scores 29 scientific skills (1–10).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation / measurement</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>criteria / rubrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Explicit multi-criteria rubrics (AI4S, ALDbench, JAM, LLM-as-a-judge 29-criteria)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use domain-specific rubrics to evaluate outputs across multiple qualitative dimensions, enabling diagnostic insights beyond single accuracy metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>per-criterion numeric scales (1–5, 1–10, Pass/Fail), composite scores, acceptance decisions</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Examples: AI4S criteria rated 1–5 or Pass/Fail for correctness; ALDbench four criteria 1–5 where 5=excellent; JAM 1–5; LLM-as-a-judge 1–10 per criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Rubrics applied by PhD-level experts in ALDbench; JAM session self-evaluations by participating researchers; AI4S manual reviewers used acceptance rubric.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Examples: ALDbench composite score 3.7; AI4S acceptance rate 254 accepted out of assessed set; JAM session aggregated percentages reported for each 1–5 criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Human rubric application is time-consuming; inter-rater variability exists (e.g., 61% agreement reported for acceptance).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.21">
                <h3 class="extraction-instance">Extracted Data Instance 21 (e7794.21)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HaloScope</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HaloScope (embedding-based hallucination detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embedding-based approach to detect hallucinations in LLM outputs by leveraging unlabeled generations and semantic signals to distinguish truthful from hallucinated content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>hallucination detection / UQ</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>tool / method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>HaloScope-inspired embedding-based hallucination detection</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use embedding similarity and other semantic invariances to flag outputs likely to be hallucinations; proposed as a technique to improve trust/UQ.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>detection accuracy, false positive/negative rates, embedding-similarity thresholds</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Standard classification metrics apply (e.g., ROC/AUC for detection capability).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Embedding methods may miss subtle factual errors; require calibration and validation in domain contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7794.22">
                <h3 class="extraction-instance">Extracted Data Instance 22 (e7794.22)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CACTUS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CACTUS (Chemistry agent connecting tool-usage to science)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent-evaluation technique for multi-turn chemistry tasks that connects tool usage to scientific evaluation; proposed to be adopted for agent evaluations in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>chemistry / agent evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>agent evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>CACTUS agent evaluation techniques</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluate multi-turn chemistry agents by measuring correct tool invocation, reasoning across turns, and domain-specific outcomes; used as inspiration for future multi-turn chemistry task evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>task success rates, tool-invocation correctness, multi-turn coherence metrics</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Success rates as % tasks completed correctly; per-turn correctness and utility metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Mentioned as future integration; not applied directly in current experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring massive multitask language understanding <em>(Rating: 2)</em></li>
                <li>GPQA: A graduate-level Google-proof Q&A benchmark <em>(Rating: 2)</em></li>
                <li>Scicode: A research coding benchmark curated by scientists <em>(Rating: 2)</em></li>
                <li>Benchmarking large language models for materials synthesis: The case of atomic layer deposition <em>(Rating: 2)</em></li>
                <li>Safetybench: Evaluating the safety of large language models with multiple choice questions <em>(Rating: 2)</em></li>
                <li>DecodingTrust: A comprehensive assessment of trustworthiness in GPT models <em>(Rating: 2)</em></li>
                <li>WildBench: Benchmarking LLMs with challenging tasks from real users in the wild <em>(Rating: 1)</em></li>
                <li>HaloScope: Harnessing unlabeled LLM generations for hallucination detection <em>(Rating: 1)</em></li>
                <li>Generating with confidence: Uncertainty quantification for black-box large language models <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7794",
    "paper_id": "paper-276647576",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "EAIRA",
            "name_full": "Evaluating AI models as scientific Research Assistants",
            "brief_description": "A holistic evaluation methodology combining four complementary techniques (MCQ Benchmarks, Open-Response Benchmarks, Lab-style Experiments, and Field-style Experiments) plus cross-cutting aspects of trust/safety, uncertainty quantification, and scalable software infrastructure, designed to assess LLMs as scientific research assistants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "multi-domain (general scientific research)",
            "theory_type": "evaluation methodology / framework",
            "evaluation_method_name": "EAIRA methodology (MCQ, Open-Response, Lab-style, Field-style)",
            "evaluation_method_description": "Combine fast breadth checks (MCQs), deeper open-response evaluations, controlled expert-driven lab-style end-to-end experiments, and large-scale field-style interaction logging and automated analysis, with cross-cutting UQ and safety checks.",
            "evaluation_metric": "various (accuracy for MCQs; rubric scores for open-response; expert A-F or numeric rubric for lab experiments; 1-5 Likert-like criteria for field experiments; UQ metrics like AUC/entropy)",
            "metric_definition": "MCQ accuracy (% correct); open-response rubric scores (1–5 or composite); lab-style expert grades (A–F or numeric per-skill scales); field-style criteria scored 1–5; UQ metrics (AUC for uncertainty separation, entropy values).",
            "dataset_or_benchmark": null,
            "human_evaluation_details": "Lab-style: domain experts (PhD-level) in controlled sessions with proctors; Field-style JAM session: 180 conversations, 125 valid transcripts, expert filtering; lab experiments: 3 experiments ~20 hours, ~100 prompts, multiple experts.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "Described aggregated outcomes across methods: e.g., field JAM session: Importance 82% rated very important/critical; Strength 59% reported improvement; novelty only 21%.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Lab-style experiments are resource intensive and low-scale; field-style requires careful annotation and validation; evaluation must evolve as LLMs advance; LLM-as-a-judge pipelines need human validation due to hallucination risk.",
            "uuid": "e7794.0",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "STaR",
            "name_full": "Skills, Trust, and Reliability evaluation framework (STaR)",
            "brief_description": "A modular, scalable software evaluation framework tailored for HPC environments that orchestrates datasets, prompting, model adapters, inference backends, UQ, and metrics aggregation for LLM evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "multi-domain",
            "theory_type": "evaluation software framework",
            "evaluation_method_name": "STaR evaluation framework",
            "evaluation_method_description": "Modular pipeline (data layer, prompting layer, model adapter, result layer) to run scenarios, generate prompts (few-shot/CoT), query models (local or API), aggregate results and compute general and uncertainty metrics; supports distributed GPU and model-parallel execution.",
            "evaluation_metric": "standard benchmark metrics (accuracy, task-specific metrics) plus UQ metrics and resource metrics (GPU hours)",
            "metric_definition": "Task accuracy (%), UQ metrics (AUC/entropy), GPU-hours (A100 hours), aggregated composite scores as defined per benchmark.",
            "dataset_or_benchmark": "supports MMLU-Pro, OpenLLM Leaderboard V2, Wildbench, DecodingTrust, domain benchmarks",
            "human_evaluation_details": null,
            "automated_falsifiability_check": null,
            "reproducibility_assessment": "Designed to improve reproducibility via standardized runners/adapters and result management",
            "reported_results": "Proof-of-concept evaluations on Polaris with Llama variants; Table VI reports per-task metrics and GPU hours (e.g., Llama-3.3-70B-Instruct: IFEval 0.6745, GPU Hours 81.39).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Work in progress; requires HPC-specific adaptations; integration with varied backends remains ongoing.",
            "uuid": "e7794.1",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "AGIL",
            "name_full": "Automatic continuous Generation and validation of Increasingly Large MCQ benchmarks",
            "brief_description": "A hybrid manual+automated workflow (AGIL) for scalable MCQ benchmark generation and validation using expert-authored gold standards plus LLM-based automated generation and LLM-as-a-judge validation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "multi-domain (AI for science)",
            "theory_type": "benchmark generation/validation process",
            "evaluation_method_name": "AGIL MCQ generation and validation",
            "evaluation_method_description": "Human domain experts author seed MCQs and validate; LLMs generate additional MCQs from papers; automated LLM-as-a-judge evaluates MCQ criteria enabling scalable acceptance of high-quality MCQs.",
            "evaluation_metric": "acceptance rate; judge-predicted acceptance accuracy; inter-reviewer agreement; model accuracy on MCQs",
            "metric_definition": "Acceptance: binary accepted/rejected; judge prediction accuracy (%); MCQ accuracy (% correct by evaluated LLMs); difficulty categories (easy/medium/hard).",
            "dataset_or_benchmark": "AI4S (initial 980 MCQs: 720 manual, 260 auto)",
            "human_evaluation_details": "140 domain experts participated in hackathons; manual reviews: 588 reviews, 317 assessed, 254 accepted; difficulty labeled as easy/medium/hard.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Automatic validation model (Mistral Large 2) predicted acceptance with 72% accuracy; manual reviewer agreement on acceptance was 61%; Llama-3-8B accuracy on 254 accepted AI4S MCQs = 20.08% (±0.0252).",
            "comparison_to_human_generated": true,
            "comparison_results": "Quality of manually validated automatically-generated MCQs comparable to manually generated ones; AI4S judged more challenging than GPQA (AI4S avg quality 4.55 vs GPQA 4.45).",
            "limitations_noted": "High difficulty to craft and validate high-quality scientific MCQs; potential dataset contamination; need to keep portions private to avoid training contamination.",
            "uuid": "e7794.2",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "AI4S",
            "name_full": "AI for Science multi-domain MCQ benchmark (AI4S)",
            "brief_description": "A multi-domain MCQ benchmark spanning scientific fields (Computer Science, Astrophysics, Climate, Physics, Chemistry) combining human- and automatically-generated MCQs to evaluate domain knowledge and reasoning at graduate-level difficulty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "multi-domain (CS, astrophysics, climate, physics, chemistry)",
            "theory_type": "benchmark dataset",
            "evaluation_method_name": "AI4S MCQ evaluation",
            "evaluation_method_description": "Evaluate LLMs on MCQs with four distractors (20% random baseline); compute accuracy, difficulty-normalized accuracy, and quality scores via judge criteria (eight criteria rated 1–5 or pass/fail).",
            "evaluation_metric": "accuracy (raw and normalized), quality scores per criteria (1–5), judge acceptance rate",
            "metric_definition": "Accuracy = % correct; normalized accuracy adjusts for chance; quality average across seven rated criteria (1–5); correct criterion Pass=5/Fail=0.",
            "dataset_or_benchmark": "AI4S (980 MCQs created; 254 accepted so far)",
            "human_evaluation_details": "Manual validation by experts; difficulty levels assigned; acceptance criteria used across reviewers; multiple reviews per question where possible.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Llama-3-8B on 254 accepted MCQs: accuracy 0.2008 (±0.0252). Llama-3-70B accuracy 0.2598. AI4S average quality 4.55 vs GPQA 4.45. Automatic judge acceptance prediction 72% accuracy.",
            "comparison_to_human_generated": true,
            "comparison_results": "AI4S more challenging than GPQA; LLMs perform worse on AI4S than on GPQA (e.g., Llama 3-70B: 26% vs GPQA 49%).",
            "limitations_noted": "Risk of redundancy in automatically generated MCQs; need for multiple expert reviewers per question; contamination concerns if benchmark fully public.",
            "uuid": "e7794.3",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Astronomy Benchmark",
            "name_full": "Astronomy domain-specific MCQ benchmark (Astronomy Benchmark from ARAA corpus)",
            "brief_description": "Automatically generated MCQ dataset (4425 MCQs) created by transcribing Annual Review of Astronomy and Astrophysics articles and using a long-context LLM to produce five MCQs per paper, intended to probe specialized astronomy knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "astronomy/astrophysics",
            "theory_type": "domain-specific MCQ benchmark",
            "evaluation_method_name": "Astronomy MCQ evaluation",
            "evaluation_method_description": "Evaluate LLMs on domain-specific MCQs, report accuracy and compute performance-to-cost tradeoffs across models and model classes.",
            "evaluation_metric": "accuracy (%) and compute cost per performance (cost per 0.1M tokens and compute-to-accuracy scaling)",
            "metric_definition": "Accuracy = % correct on MCQs; cost per 0.1M tokens = monetary cost metric; compute-to-accuracy slope measured (e.g., 3.5 percentage points per 10x price within model families).",
            "dataset_or_benchmark": "Astronomy Benchmark (4425 MCQs from 885 ARAA articles)",
            "human_evaluation_details": "Questions generated automatically and designed to be specific yet independent of sections; validation performed when used in experiments.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Claude-3.5-Sonnet accuracy 85.0%; GPT-4o 80.4%; Gemini-1.5-Pro 77.6%. Observed compute vs accuracy scaling: ~3.5 percentage points per 10x price within model series.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Open-weight models lag proprietary ones; performance varies across subfields and languages; automatic generation may bias toward recent or well-covered topics.",
            "uuid": "e7794.4",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Climate Benchmark",
            "name_full": "Climate domain-specific MCQ benchmark (from IPCC sections)",
            "brief_description": "An automatically generated MCQ dataset (752 questions) derived from section-wise parsing of IPCC reports using OCR and LLM-based generation to evaluate LLM competence in climate science.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "climate science",
            "theory_type": "domain-specific MCQ benchmark",
            "evaluation_method_name": "Climate MCQ evaluation",
            "evaluation_method_description": "Use section-level inputs from IPCC to prompt LLMs to output one MCQ per section; evaluate models by MCQ accuracy.",
            "evaluation_metric": "accuracy (%) on climate MCQs",
            "metric_definition": "Accuracy = % correct answers across the 752 items.",
            "dataset_or_benchmark": "Climate Benchmark (752 MCQs from IPCC reports)",
            "human_evaluation_details": "Automated generation supplemented by human oversight recommended; noted need to remove redundant/semantically similar questions.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "GPT-4o achieved 87.34% accuracy; Llama 3.8 achieved 78.48%; Phi 4 scored 54.43%.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Automated generation sometimes produces semantically redundant questions; human oversight needed to ensure diversity and quality.",
            "uuid": "e7794.5",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MCQ Benchmarks (general list)",
            "name_full": "Multiple-choice question benchmarks (MMLU, MMLU-PRO, GSM8K, GSM1K, MATH, ARC, HellaSwag, MoleculeQA, MolPuzzle, ChemBench, GPQA, etc.)",
            "brief_description": "A variety of established multiple-choice benchmarks used to rapidly evaluate LLM factual recall and domain reasoning across many subjects, from general knowledge to domain-specific chemistry and math.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "multi-domain",
            "theory_type": "benchmark datasets",
            "evaluation_method_name": "MCQ benchmark evaluation",
            "evaluation_method_description": "Present LLMs with multiple-choice questions and compute accuracy; some adapted to multiple-choice to reduce output ambiguity.",
            "evaluation_metric": "accuracy (%)",
            "metric_definition": "% of correct answers; random baselines depend on number of choices (e.g., 25% for 4 choices, 20% for 5 choices).",
            "dataset_or_benchmark": "MMLU, MMLU-PRO, GSM8K, GSM1K, MATH, ARC, HellaSwag, MoleculeQA, MolPuzzle, ChemBench, GPQA",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "MCQs test breadth but not depth/iterative problem solving; risk of dataset saturation and training contamination; must adapt as models improve.",
            "uuid": "e7794.6",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Open-response scorers (statistical)",
            "name_full": "N-gram statistical scorers (BLEU, ROUGE, METEOR)",
            "brief_description": "Automatic n-gram overlap metrics that compare generated text to reference answers by measuring precision/recall of matching n-grams, used for open-response evaluation but limited in semantic/ reasoning assessment.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP / open-response evaluation",
            "theory_type": "evaluation metrics",
            "evaluation_method_name": "BLEU / ROUGE / METEOR statistical scoring",
            "evaluation_method_description": "Compute n-gram overlap between model output and gold reference(s); BLEU emphasizes precision, ROUGE recall, METEOR considers synonyms and ordering.",
            "evaluation_metric": "BLEU, ROUGE, METEOR scores (typically 0–1 or 0–100 scaled)",
            "metric_definition": "Score where higher indicates greater n-gram overlap (BLEU precision-based; ROUGE recall-oriented; METEOR harmonic precision/recall with synonym matching).",
            "dataset_or_benchmark": null,
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Do not capture semantics or reasoning; can be gamed and insensitive to correct but lexically different answers.",
            "uuid": "e7794.7",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Open-response scorers (embedding & LLM judges)",
            "name_full": "Embedding-based and LLM-as-a-judge approaches (BERTScore, CheckEmbed, LLM-as-a-judge)",
            "brief_description": "Semantic similarity evaluators using contextual embeddings (e.g., BERTScore, CheckEmbed) and recent LLM-as-a-judge methods that rate or pairwise-compare open responses when reference answers are absent.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "open-response evaluation / NLP",
            "theory_type": "evaluation metrics/frameworks",
            "evaluation_method_name": "BERTScore / CheckEmbed / LLM-as-a-judge",
            "evaluation_method_description": "BERTScore: cosine similarity aggregated over token embeddings; CheckEmbed: embedding comparison for verification; LLM-as-a-judge: instruct an LLM to score or compare answers single-wise or pairwise, producing scores and rationales.",
            "evaluation_metric": "semantic similarity scores (cosine similarity aggregated), judge scores (1–n scale), pairwise preference outcomes",
            "metric_definition": "BERTScore yields precision/recall/F1-like similarity measures (0–1); LLM judge outputs numeric ratings (e.g., 1–5 or 1–10) or preference labels.",
            "dataset_or_benchmark": null,
            "human_evaluation_details": "LLM-as-a-judge advantages: scalability and explainability; limitations include position bias, verbosity bias, self-enhancement bias, and grading math/reasoning challenges.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Used in automatic validation of AI4S MCQs with Mistral Large 2 as judge achieving 72% predictive accuracy vs human acceptance.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "LLM-as-a-judge can be biased and requires further validation against human judgments; embedding methods may miss fine-grained reasoning errors.",
            "uuid": "e7794.8",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "SciCode",
            "name_full": "SciCode — Scientific Code Generation Benchmark",
            "brief_description": "A manually curated benchmark of 80 research-level coding problems decomposed into 338 subproblems with gold solutions and test cases to evaluate LLMs' ability to write and integrate scientific code across multiple disciplines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computational science / multiple scientific domains",
            "theory_type": "open-response code-generation benchmark",
            "evaluation_method_name": "SciCode automatic test-case evaluation + background vs no-background modes",
            "evaluation_method_description": "Provide LLMs with problem prompts (zero-shot or with background) requiring implementation of Python functions for subproblems; evaluate using gold test cases and require all subproblems + integration to be correct to mark main problem solved.",
            "evaluation_metric": "main-problem solved rate (%), subproblem accuracy (%), code test-case pass rates",
            "metric_definition": "Main problem solved when all subproblems and integration pass tests; reported as percent of problems solved; subproblem accuracy is fraction of subproblems passing tests.",
            "dataset_or_benchmark": "SciCode (80 problems, 338 subproblems)",
            "human_evaluation_details": "Problems annotated and verified by at least two senior researchers to ensure correctness and avoid public dataset overlap.",
            "automated_falsifiability_check": true,
            "reproducibility_assessment": "Gold solutions and test cases allow reproducible automated evaluation of generated code.",
            "reported_results": "State-of-the-art models solved few main problems; example: top models solved 7.7% of main SciCode problems; open-weight models performed poorly overall.",
            "comparison_to_human_generated": true,
            "comparison_results": "With scientific background provided, models improved; however, still far below human-level performance on integrated research-level coding tasks.",
            "limitations_noted": "Generated solutions accumulate errors across subproblems; models need domain background to improve; benchmark is challenging for current LLMs.",
            "uuid": "e7794.9",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ALDbench",
            "name_full": "ALDbench — Atomic Layer Deposition materials synthesis benchmark",
            "brief_description": "An open-response benchmark in materials synthesis focused on atomic layer deposition (ALD), containing expert-generated questions and human-evaluated responses across four rubric criteria (Overall quality, Specificity, Relevance, Accuracy).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_size": null,
            "scientific_domain": "materials science / chemistry (atomic layer deposition)",
            "theory_type": "open-response materials synthesis evaluation",
            "evaluation_method_name": "ALDbench expert rubric evaluation",
            "evaluation_method_description": "PhD-level experts generate domain-appropriate questions and rate model responses on four criteria using 1–5 rubrics; statistical correlation analysis used to relate difficulty/specificity to scores.",
            "evaluation_metric": "per-response rubric scores (Overall quality, Specificity, Relevance, Accuracy; 1–5), composite quality score",
            "metric_definition": "Each criterion rated 1 (lowest) to 5 (highest); composite score average of criteria.",
            "dataset_or_benchmark": "ALDbench (curated questions by six PhD experts)",
            "human_evaluation_details": "Seven PhD-level human experts reviewed GPT-4o outputs; each response graded on four criteria with provided rubrics.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Model composite quality score 3.7; 36% of questions received at least one below-average score; detected at least five hallucination instances; statistically significant negative correlations between question difficulty/specificity and response quality (p-values 0.033, 0.016, 0.007).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Open-response grading is labor-intensive and subjective; requires expert involvement; LLM hallucinations present safety concerns in high-consequence synthesis domains.",
            "uuid": "e7794.10",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Lab-style experiments",
            "name_full": "Lab-style end-to-end experiments (expert-proctored)",
            "brief_description": "Controlled, expert-run multi-turn interactive experiments where domain experts attempt to solve real research problems with LLM assistance; responses and interactions are recorded and scored by experts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "domain-specific (examples: parallel/distributed computing, chemistry, biology)",
            "theory_type": "interactive evaluation protocol",
            "evaluation_method_name": "Lab-style expert evaluation",
            "evaluation_method_description": "Assign a realistic research problem to experts who iteratively interact with multiple LLMs using consistent prompts; experts analyze each response and provide granular and high-level scoring (A–F scales or enumerated per-skill rubrics); measure metrics like number of prompts needed to reach key insights.",
            "evaluation_metric": "expert qualitative grades (A–F), per-skill rubric scores, number of prompts to solution, correctness/conciseness/precision metrics",
            "metric_definition": "High-level grade scale A (human reference) to F (worst); number of prompts metric counts interactions required to arrive at key insight.",
            "dataset_or_benchmark": null,
            "human_evaluation_details": "Experiments used five domain experts (one per experiment), three initial experiments ~20 hours of interactions ~100 prompts; scoring included detailed per-prompt analysis and overall scales.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Example: For zero-overhead checkpointing problem, GPT-4o required five prompts to reach key insight while Argo/O1-preview reached it in one prompt.",
            "comparison_to_human_generated": true,
            "comparison_results": "Experts compared model outputs to human researcher performance using A–F scale; lab-style provides fine-grained capability assessment vs human baseline.",
            "limitations_noted": "Highly manual, low scalability, high expert time cost, limited coverage/generalizability across domains.",
            "uuid": "e7794.11",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Field-style experiments",
            "name_full": "Field-style (in-the-wild) experiments and JAM sessions",
            "brief_description": "Large-scale capture and automated analysis of real researcher–LLM interactions (in-the-wild), using aggregate behavioral signals and LLM-as-a-judge pipelines to infer model strengths and weaknesses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Argo/O1-preview (example in JAM session)",
            "model_size": null,
            "scientific_domain": "multi-domain scientific workflows",
            "theory_type": "real-world interaction evaluation",
            "evaluation_method_name": "Field-style evaluation (JAM session + automated LLM-as-a-judge)",
            "evaluation_method_description": "Collect thousands of conversations between researchers and models during routine workflows or structured JAM sessions; ask participants to self-rate across criteria (Novelty, Productivity, Solution, Strength, Importance) and apply LLM-as-a-judge (e.g., Llama-3.3-70B-Instruct) to transcripts to score many scientific criteria automatically.",
            "evaluation_metric": "user self-report Likert scores (1–5), LLM-as-a-judge numeric scores (1–10) across 29 criteria, usage behavioral signals (rephrases, abandonment), aggregated strength/weakness summaries",
            "metric_definition": "Self-report: 1–5 scale; LLM-as-a-judge: 1–10 or 0 for N/A per criterion; behavioral signals used as proxy for failure modes.",
            "dataset_or_benchmark": "Argonne JAM session dataset (180 conversations, filtered to 125 challenging transcripts)",
            "human_evaluation_details": "180 conversations recorded; participants self-scored; later 125 transcripts evaluated by Llama-3.3-70B-Instruct with prompts to score 29 scientific criteria, summaries batched for synthesis.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Self-report: Importance 82% rated very important/critical; Strength 59% improvement; Productivity 51% equated to PhD/postdoc contributions; Solution quality 50% strong/exceptional; Novelty 21%. LLM-as-a-judge pipeline produced 29-criteria scored summaries (proof-of-concept).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "LLM-as-a-judge outputs need human validation due to hallucinations; self-reporting gives coarse diagnostics; annotating at scale with fine-grained labels is challenging.",
            "uuid": "e7794.12",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "LLM-as-a-judge",
            "name_full": "LLM-as-a-judge evaluation method",
            "brief_description": "Using an LLM to grade or compare other LLMs' outputs either pairwise or single-answer grading to scale open-response evaluation when gold references are unavailable.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral Large 2 (used as judge in AI4S); Llama-3.3-70B-Instruct (used for JAM transcript analysis)",
            "model_size": null,
            "scientific_domain": "evaluation methodology (cross-domain)",
            "theory_type": "automated evaluation method",
            "evaluation_method_name": "LLM-as-a-judge (pairwise and single grading)",
            "evaluation_method_description": "Prompt an LLM to score a single response on multiple criteria or to compare two responses and choose a better one; can return numeric scores and rationales used for automated acceptance or ranking.",
            "evaluation_metric": "judge accuracy vs human (predict acceptance), numeric rubric scores, pairwise preferences",
            "metric_definition": "Predictive accuracy = % agreement with human accept/reject; numeric rubrics often 1–5 or 1–10 scales; pairwise outputs return better/tie.",
            "dataset_or_benchmark": null,
            "human_evaluation_details": "Used Mistral Large 2 to evaluate MCQs on eight criteria in one prompt; judge-predicted acceptance accuracy 72%; inter-reviewer human acceptance agreement 61%.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Judge (Mistral Large 2) predicted whether a question would be accepted with 72% accuracy.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Biases include position bias, verbosity bias, self-enhancement bias; limited capability grading math/reasoning; requires calibration and human oversight.",
            "uuid": "e7794.13",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Uncertainty quantification (UQ) techniques",
            "name_full": "UQ methods for black-box LLMs (Question Rephrasing, sampling, semantic entropy, aggregation, entropy-based AUC)",
            "brief_description": "A set of methods to assess input and output uncertainty for black-box LLMs including question rephrasing sensitivity, sampling-based repeated queries, semantic entropy measures, and entropy/AUC scores to separate correct vs incorrect predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 / GPT-4 (evaluated examples)",
            "model_size": null,
            "scientific_domain": "chemistry experiments and general evaluation",
            "theory_type": "uncertainty quantification techniques",
            "evaluation_method_name": "Question Rephrasing + sampling + entropy-based UQ",
            "evaluation_method_description": "Assess input uncertainty by rephrasing questions/presentations (e.g., alternate SMILES) and measuring sensitivity; assess output uncertainty via sampling repeated queries and compute entropy; evaluate discriminative power via AUC of uncertainty metric separating correct/incorrect answers.",
            "evaluation_metric": "AUC (uncertainty discriminative power), accuracy, F1, entropy measures",
            "metric_definition": "AUC ranges 0–1 (higher better at separating correct vs incorrect by uncertainty); accuracy and F1 standard definitions; entropy in bits or normalized units.",
            "dataset_or_benchmark": "Chemistry property datasets: BBBP, HIV, Tox21; forward reaction prediction tasks",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "AUC for original SMILES ranged 0.546–0.774; entropy-based output-uncertainty AUCs ranged 0.86–0.99 indicating good uncertainty signal despite limited accuracy; reformulated inputs reduced accuracy and F1.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Good UQ signals do not imply high base accuracy; sensitivity to input format (e.g., SMILES variants) highlights fragility; black-box access limits internal-method UQ.",
            "uuid": "e7794.14",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "CHEMRISK",
            "name_full": "CHEMRISK — Chemical Risk Detection Benchmark",
            "brief_description": "A chemical risk benchmark developed with LLNL to assess LLM handling of chemical knowledge across chemical understanding, molecular design, and molecular synthesis tasks using MCQ and open-form items with SMILES/SELFIES representations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o and o1 (evaluated subset)",
            "model_size": null,
            "scientific_domain": "chemistry / energetics",
            "theory_type": "safety-focused benchmark dataset",
            "evaluation_method_name": "CHEMRISK benchmark tasks (SMILES-to-Name, property prediction, forward synthesis)",
            "evaluation_method_description": "Curated base data from CSD and USPTO; evaluate LLMs on MCQ-style and free-form tasks such as SMILES-to-name conversion, property regression/ranking, and (retrosynthesis/forward synthesis) with accuracy and other metrics.",
            "evaluation_metric": "accuracy per task",
            "metric_definition": "Accuracy = fraction correct on multiple-choice Q&A tasks; other tasks (regression) may use RMSE/AUC but preliminary results reported accuracy for MCQ conversions.",
            "dataset_or_benchmark": "CHEMRISK (constructed dataset using CSD, USPTO proxies)",
            "human_evaluation_details": "Benchmark developed with domain experts at LLNL and manual quality checks performed on base data.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "On a subset: GPT-4o and o1 achieved perfect accuracy (1.0) on SMILES-to-Name; o1 achieved 1.0 on molecular design where GPT-4o scored 0.4; both achieved 0.6 on molecular synthesis task subset.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Preliminary proof-of-concept; more development needed for broader safety evaluation across chemical subdomains.",
            "uuid": "e7794.15",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "SafetyBench / SALAD-Bench / DecodingTrust / TrustLLM",
            "name_full": "Safety and trustbenchmarks (SafetyBench, SALAD-Bench, DecodingTrust, TrustLLM)",
            "brief_description": "Sets of benchmarks and frameworks to evaluate LLM safety properties like toxicity, bias, privacy, hallucinations, and machine ethics using MCQs and open-response tests across multiple categories.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "safety / ethics evaluation",
            "theory_type": "benchmark collections and evaluation frameworks",
            "evaluation_method_name": "SafetyBench / SALAD-Bench / DecodingTrust / TrustLLM",
            "evaluation_method_description": "Large sets of categorized MCQs (SafetyBench: 11,435 MCQs across seven categories), hierarchical safety taxonomies (SALAD-Bench), and open-response trust evaluations (DecodingTrust); TrustLLM covers six dimensions across &gt;30 datasets.",
            "evaluation_metric": "task-specific accuracy/classification metrics; aggregated safety scores and leaderboards",
            "metric_definition": "Fraction correct on MCQ safety items; scores per dimension aggregated across datasets; some provide public leaderboards.",
            "dataset_or_benchmark": "SafetyBench, SALAD-Bench, DecodingTrust datasets, TrustLLM datasets",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Few safety benchmarks target high-consequence scientific domains (chemistry/biology/nuclear) specifically; need domain-specific safety evaluations.",
            "uuid": "e7794.16",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "WildBench / HaluEval-Wild",
            "name_full": "WildBench and HaluEval-Wild (in-the-wild evaluation datasets)",
            "brief_description": "Benchmarks designed to collect challenging user queries and real-world interactions to analyze LLM robustness and hallucinations in natural usage scenarios.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / in-the-wild evaluation",
            "theory_type": "dataset / evaluation approach",
            "evaluation_method_name": "WildBench / HaluEval-Wild",
            "evaluation_method_description": "Collect real user queries and responses to evaluate LLM performance and hallucination propensity in naturalistic settings; analyze patterns and failure modes.",
            "evaluation_metric": "failure/hallucination rates, user-behavior proxies, task-specific accuracy",
            "metric_definition": "Proportion of interactions exhibiting hallucinations or failure; engagement signals used to infer failures.",
            "dataset_or_benchmark": "WildBench, HaluEval-Wild",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Mostly non-scientific domains previously; adaptation to scientific contexts required.",
            "uuid": "e7794.17",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "OpenLLM Leaderboard V2 / Harness",
            "name_full": "OpenLLM Leaderboard V2 evaluated via EleutherAI LM Evaluation Harness",
            "brief_description": "An evaluation suite used with STaR to run standardized benchmark tasks across open models with aggregated task metrics, also reporting computational resource usage (GPU hours).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama variants (examples in Table VI)",
            "model_size": null,
            "scientific_domain": "general LLM evaluation",
            "theory_type": "benchmark suite and harness",
            "evaluation_method_name": "OpenLLM Leaderboard V2 via Harness",
            "evaluation_method_description": "Run a collection of six challenging tasks using the Harness runner and compute per-task metrics (e.g., IFEval, BBH, MATH, GPQA, MuSR, MMLU-PRO) along with GPU hours consumed.",
            "evaluation_metric": "task-specific metrics (e.g., IFEval, BBH scores), GPU hours",
            "metric_definition": "Task metric scores (normalized), GPU hours measured in A100/GPU-hours.",
            "dataset_or_benchmark": "OpenLLM Leaderboard V2 tasks",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Table VI reports model scores and GPU Hours; e.g., Llama-3.3-70B-Instruct IFEval 0.6745 and GPU Hours 81.39.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Resource intensive for large models; harness requires HPC-specific adaptations.",
            "uuid": "e7794.18",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "CHEM datasets (CSD, USPTO)",
            "name_full": "Cambridge Structural Database (CSD) and United States Patent and Trademark Office (USPTO) datasets used as CHEMRISK base",
            "brief_description": "Established chemical data sources used to curate ground truth data for tasks such as property regression, synthesis, and SMILES/name conversions in CHEMRISK.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "chemistry",
            "theory_type": "data sources",
            "evaluation_method_name": "dataset curation for CHEMRISK",
            "evaluation_method_description": "Curate structural/property/synthesis examples from CSD and reaction patents in USPTO to build standard evaluation tasks for chemical reasoning.",
            "evaluation_metric": null,
            "metric_definition": null,
            "dataset_or_benchmark": "CSD, USPTO (as CHEMRISK base)",
            "human_evaluation_details": "Manual quality checks applied to curated base data.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Domain-specific curation required to reduce noise and ensure quality for safety-sensitive tasks.",
            "uuid": "e7794.19",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Lab/Field scoring rubrics",
            "name_full": "Evaluation rubrics and criteria (AI4S acceptance criteria, ALDbench 4-criterion rubric, JAM 5-criterion self-evaluation, LLM-as-a-judge 29 criteria)",
            "brief_description": "Explicit scoring rubrics used across experiments: AI4S eight acceptance criteria for MCQs; ALDbench uses Overall Quality, Specificity, Relevance, Accuracy (1–5); JAM session asked Novelty, Productivity, Solution, Strength, Importance (1–5); LLM-as-a-judge prompt scores 29 scientific skills (1–10).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "evaluation / measurement",
            "theory_type": "criteria / rubrics",
            "evaluation_method_name": "Explicit multi-criteria rubrics (AI4S, ALDbench, JAM, LLM-as-a-judge 29-criteria)",
            "evaluation_method_description": "Use domain-specific rubrics to evaluate outputs across multiple qualitative dimensions, enabling diagnostic insights beyond single accuracy metrics.",
            "evaluation_metric": "per-criterion numeric scales (1–5, 1–10, Pass/Fail), composite scores, acceptance decisions",
            "metric_definition": "Examples: AI4S criteria rated 1–5 or Pass/Fail for correctness; ALDbench four criteria 1–5 where 5=excellent; JAM 1–5; LLM-as-a-judge 1–10 per criterion.",
            "dataset_or_benchmark": null,
            "human_evaluation_details": "Rubrics applied by PhD-level experts in ALDbench; JAM session self-evaluations by participating researchers; AI4S manual reviewers used acceptance rubric.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Examples: ALDbench composite score 3.7; AI4S acceptance rate 254 accepted out of assessed set; JAM session aggregated percentages reported for each 1–5 criterion.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Human rubric application is time-consuming; inter-rater variability exists (e.g., 61% agreement reported for acceptance).",
            "uuid": "e7794.20",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "HaloScope",
            "name_full": "HaloScope (embedding-based hallucination detection)",
            "brief_description": "An embedding-based approach to detect hallucinations in LLM outputs by leveraging unlabeled generations and semantic signals to distinguish truthful from hallucinated content.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "hallucination detection / UQ",
            "theory_type": "tool / method",
            "evaluation_method_name": "HaloScope-inspired embedding-based hallucination detection",
            "evaluation_method_description": "Use embedding similarity and other semantic invariances to flag outputs likely to be hallucinations; proposed as a technique to improve trust/UQ.",
            "evaluation_metric": "detection accuracy, false positive/negative rates, embedding-similarity thresholds",
            "metric_definition": "Standard classification metrics apply (e.g., ROC/AUC for detection capability).",
            "dataset_or_benchmark": null,
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Embedding methods may miss subtle factual errors; require calibration and validation in domain contexts.",
            "uuid": "e7794.21",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "CACTUS",
            "name_full": "CACTUS (Chemistry agent connecting tool-usage to science)",
            "brief_description": "An agent-evaluation technique for multi-turn chemistry tasks that connects tool usage to scientific evaluation; proposed to be adopted for agent evaluations in future work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "chemistry / agent evaluation",
            "theory_type": "agent evaluation framework",
            "evaluation_method_name": "CACTUS agent evaluation techniques",
            "evaluation_method_description": "Evaluate multi-turn chemistry agents by measuring correct tool invocation, reasoning across turns, and domain-specific outcomes; used as inspiration for future multi-turn chemistry task evaluations.",
            "evaluation_metric": "task success rates, tool-invocation correctness, multi-turn coherence metrics",
            "metric_definition": "Success rates as % tasks completed correctly; per-turn correctness and utility metrics.",
            "dataset_or_benchmark": null,
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Mentioned as future integration; not applied directly in current experiments.",
            "uuid": "e7794.22",
            "source_info": {
                "paper_title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring massive multitask language understanding",
            "rating": 2,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "GPQA: A graduate-level Google-proof Q&A benchmark",
            "rating": 2,
            "sanitized_title": "gpqa_a_graduatelevel_googleproof_qa_benchmark"
        },
        {
            "paper_title": "Scicode: A research coding benchmark curated by scientists",
            "rating": 2,
            "sanitized_title": "scicode_a_research_coding_benchmark_curated_by_scientists"
        },
        {
            "paper_title": "Benchmarking large language models for materials synthesis: The case of atomic layer deposition",
            "rating": 2,
            "sanitized_title": "benchmarking_large_language_models_for_materials_synthesis_the_case_of_atomic_layer_deposition"
        },
        {
            "paper_title": "Safetybench: Evaluating the safety of large language models with multiple choice questions",
            "rating": 2,
            "sanitized_title": "safetybench_evaluating_the_safety_of_large_language_models_with_multiple_choice_questions"
        },
        {
            "paper_title": "DecodingTrust: A comprehensive assessment of trustworthiness in GPT models",
            "rating": 2,
            "sanitized_title": "decodingtrust_a_comprehensive_assessment_of_trustworthiness_in_gpt_models"
        },
        {
            "paper_title": "WildBench: Benchmarking LLMs with challenging tasks from real users in the wild",
            "rating": 1,
            "sanitized_title": "wildbench_benchmarking_llms_with_challenging_tasks_from_real_users_in_the_wild"
        },
        {
            "paper_title": "HaloScope: Harnessing unlabeled LLM generations for hallucination detection",
            "rating": 1,
            "sanitized_title": "haloscope_harnessing_unlabeled_llm_generations_for_hallucination_detection"
        },
        {
            "paper_title": "Generating with confidence: Uncertainty quantification for black-box large language models",
            "rating": 1,
            "sanitized_title": "generating_with_confidence_uncertainty_quantification_for_blackbox_large_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.0316735,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants PREPRINT
27 Feb 2025</p>
<p>Franck Cappello cappello@anl.gov 
Mathematics and Computer Science Division
Argonne National Laboratory</p>
<p>Sandeep Madireddy smadireddy@anl.gov 
Mathematics and Computer Science Division
Argonne National Laboratory</p>
<p>Robert Underwood 
Mathematics and Computer Science Division
Argonne National Laboratory</p>
<p>Neil Getty 
Data Science and Learning Division
Argonne National Laboratory, xi Applied Materials Division
Argonne National Laboratory, ‡ Computational Science Division
Argonne National Laboratory</p>
<p>The Ohio State University, ‡ ‡ Rochester Institute of Technology</p>
<p>Nicholas Lee 
Ping Chia 
Data Science and Learning Division
Argonne National Laboratory, xi Applied Materials Division
Argonne National Laboratory, ‡ Computational Science Division
Argonne National Laboratory</p>
<p>The Ohio State University, ‡ ‡ Rochester Institute of Technology</p>
<p>Nesar Ramachandra 
Josh Nguyen 
Murat Kec ¸eli ‡Tanwi Mallick 
Mathematics and Computer Science Division
Argonne National Laboratory</p>
<p>Zilinghan Li 
Marieme Ngom 
Mathematics and Computer Science Division
Argonne National Laboratory</p>
<p>Data Science and Learning Division
Argonne National Laboratory, xi Applied Materials Division
Argonne National Laboratory, ‡ Computational Science Division
Argonne National Laboratory</p>
<p>The Ohio State University, ‡ ‡ Rochester Institute of Technology</p>
<p>Chenhui Zhang 
Massachusetts Institute of Technology</p>
<p>Angel Yanguas-Gil Xi 
Evan Antoniuk 
Mathematics and Computer Science Division
Argonne National Laboratory</p>
<p>Bhavya Kailkhura 
Mathematics and Computer Science Division
Argonne National Laboratory</p>
<p>Minyang Tian 
Mathematics and Computer Science Division
Argonne National Laboratory</p>
<p>Data Science and Learning Division
Argonne National Laboratory, xi Applied Materials Division
Argonne National Laboratory, ‡ Computational Science Division
Argonne National Laboratory</p>
<p>The Ohio State University, ‡ ‡ Rochester Institute of Technology</p>
<p>Yufeng Du 
Mathematics and Computer Science Division
Argonne National Laboratory</p>
<p>Data Science and Learning Division
Argonne National Laboratory, xi Applied Materials Division
Argonne National Laboratory, ‡ Computational Science Division
Argonne National Laboratory</p>
<p>The Ohio State University, ‡ ‡ Rochester Institute of Technology</p>
<p>Yuan-Sen Ting 
Data Science and Learning Division
Argonne National Laboratory, xi Applied Materials Division
Argonne National Laboratory, ‡ Computational Science Division
Argonne National Laboratory</p>
<p>The Ohio State University, ‡ ‡ Rochester Institute of Technology</p>
<p>Azton Wells 
Data Science and Learning Division
Argonne National Laboratory, xi Applied Materials Division
Argonne National Laboratory, ‡ Computational Science Division
Argonne National Laboratory</p>
<p>The Ohio State University, ‡ ‡ Rochester Institute of Technology</p>
<p>Bogdan Nicolae 
Mathematics and Computer Science Division
Argonne National Laboratory</p>
<p>Avinash Maurya 
Mathematics and Computer Science Division
Argonne National Laboratory</p>
<p>M Mustafa Rafique 
Eliu Huerta 
Data Science and Learning Division
Argonne National Laboratory, xi Applied Materials Division
Argonne National Laboratory, ‡ Computational Science Division
Argonne National Laboratory</p>
<p>The Ohio State University, ‡ ‡ Rochester Institute of Technology</p>
<p>Bo Li 
Department of Computer Science
The University of Chicago</p>
<p>∥ University of Pennsylvania</p>
<p>Ian Foster 
Data Science and Learning Division
Argonne National Laboratory, xi Applied Materials Division
Argonne National Laboratory, ‡ Computational Science Division
Argonne National Laboratory</p>
<p>The Ohio State University, ‡ ‡ Rochester Institute of Technology</p>
<p>Rick Stevens 
Data Science and Learning Division
Argonne National Laboratory, xi Applied Materials Division
Argonne National Laboratory, ‡ Computational Science Division
Argonne National Laboratory</p>
<p>The Ohio State University, ‡ ‡ Rochester Institute of Technology</p>
<p>Simon Raskar 
Sixbert Corrodi 
Srutarshi Muhoza 
Stefano Banerjee 
Stu Fenu 
Susan Hannay 
Babinec 
Grass Suyin 
Tadbhagya Wang 
Taemin Kumar 
Tanjin Kim 
Tanwi He 
Taylor Mallick 
Tekin Childers 
Temitope Bicer 
Tianhao Oproudek 
Tianyi Gu 
Tiffany Li 
Tim Kinnibrugh 
Tim Ashenfelter 
Tim Hobbs 
Tim Nguyen 
Timothy Williams 
Todd Suzuki 
Tom Munson 
Tom Brettin 
Tom Peterka 
Troy Uram 
Utkarsh Arco- Mano 
Valerie Nanda 
Varuni Taylor 
VenkataDevesh Sastry 
SeethiVincent Reddy 
Vincent Chirio 
Vincenzo Chia 
Vivian Cappello 
Vrindaa Sullivan 
Wei Somjit 
Weixing Li 
Wen Cheng 
Xiaoyang Zhuang 
Xuli Liu 
Yang Wu 
Yanna Wang 
Yawei Chen 
Yeni Yang 
Yeonjun Li 
Yildiz Jeong 
Yinghu Ocrun 
Yiqi Piao 
Yue Yu 
Yuejun Cao 
Yuepeng Yan 
Yuri Zhang 
Zhao Oksuzian 
Zhi Zixuan 
Zixuan Zhou 
Zhao </p>
<p>Lawrence Livermore National Laboratory</p>
<p>Siba Dowell, Sibendu SomKesharwani, Siby Platthottam, Sid</p>
<p>EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants PREPRINT
27 Feb 202563FE573B94F62A2B111E6979FACAA3E9arXiv:2502.20309v1[cs.AI]componentformattingstylestylinginsert
Recent advancements have positioned AI, and particularly Large Language Models (LLMs) as transformative tools for scientific research, capable of addressing complex tasks that require reasoning, problem-solving, and decision-making.Their exceptional capabilities suggest their potential as scientific research assistants, but also highlight the need for holistic, rigorous, and domain-specific evaluation to assess effectiveness in realworld scientific applications.This paper describes a multifaceted methodology for Evaluating AI models as scientific Research Assistants (EAIRA) developed at Argonne National Laboratory.This methodology incorporates four primary classes of evaluations.1) Multiple Choice Questions to assess factual recall; 2) Open Response to evaluate advanced reasoning and problemsolving skills; 3) Lab-Style Experiments involving detailed analysis of capabilities as research assistants in controlled environments; and 4) Field-Style Experiments to capture researcher-LLM interactions at scale in a wide range of scientific domains and applications.These complementary methods enable a comprehensive analysis of LLM strengths and weaknesses with respect to their scientific knowledge, reasoning abilities, and adaptability.Recognizing the rapid pace of LLM advancements, we designed the methodology to evolve and adapt so as to ensure its continued relevance and applicability.This paper describes the methodology's state at the end of February 2025.Although developed within a subset of scientific domains, the methodology is designed to be generalizable to a wide range of scientific domains.</p>
<p>I. INTRODUCTION</p>
<p>Recent advances in Large Language Models (LLMs) have greatly broadened conceptions of what AI may be able to accomplish in the near future.Models such as OpenAI's GPT O1 [1], Google's Gemini [2], and Anthropic's Claude [3] are transforming traditional natural language understanding (NLU) tasks like summarization, information extraction, translation, and classification with enhanced contextual depth and adaptability.They are also exhibiting promising potential beyond NLU, with measurable progress on tasks such as mathematical problem solving, multi-step reasoning, and symbolic logic-and achieving significant milestones such as passing the Uniform BAR exam and medical licensing exams [4].Such achievements highlight their potential to emulate abstraction, logical deduction, and domain-specific expertise.This evolution from NLU towards addressing complex, domain-specific challenges with minimal human guidance has propelled LLMs into a pivotal role for next-generation AI systems, positioning them as a cornerstone technology in the quest toward more general-purpose AI, and potentially, artificial general intelligence (AGI).</p>
<p>Building on these advancements, scientists are now beginning to assess separately the suitability and potential impact of LLMs on a wide variety of specific tasks within specific fields of research and discovery [5].This work has led to exciting demonstrations of LLMs as transformative tools for such tasks predicting molecular properties [6], uncovering genomic patterns [7], interpreting astrophysical data [8], solving mathematical problems [9], and even creating and manipulating tools for simulations and analysis [10].</p>
<p>These developments have also led scientists to envision the use of LLMs and transformers as research assistants that can not only automate individual research tasks but also engage with scientific problems in depth by taking advantage of growing multi-step reasoning skills that complement their expanding contextual understanding.This vision suggests a new holistic approach in which LLMs interface with relevant tools, operate (quasi-)autonomously on research challenges, identify relevant literature, summarize findings, propose experimental designs, and even autonomously run, and generate insights from, physical and computational experiments [11], [12].We identify two main challenges that must be addressed before LLMs can be broadly adopted by the scientific community as effective and trustworthy research assistants.First, researchers need ways to measure and evaluate LLM capabilities in the different stages and tasks of the scientific research process.Such evaluations can both guide LLM applications and integration with other tools, and provide benchmarks for developers to improve their LLMs and supporting systems.Second, as with other research tools and techniques, researchers need ways to assess confidence in the results produced, in order to decide whether or not they are trustworthy.A comprehensive, rigorous, accurate, transparent and community-approved evaluation methodology is necessary to address these two challenges.</p>
<p>This paper introduces work undertaken at Argonne National Laboratory within the AuroraGPT project to develop a research methodology that addresses these two challenges.The paper makes three main contributions: (i) a holistic methodology for LLM evaluation; (ii) two novel evaluation techniques (lab-style and field-style experiments); and (iii) improvements in existing state-of-the-art evaluation techniques for the specific role of research assistant.</p>
<p>First, we propose an overarching methodology for assessing the scientific knowledge, skills, and safety of AI models.As shown in Table I, this methodology encompasses four complementary techniques: 1) Multiple Choice Question (MCQ) Benchmarks, which measure factual recall and reasoning capabilities in structured formats to provide fast assessment of a model's breadth of knowledge; 2) Open Response Benchmarks, which test a model's ability to generate detailed openended responses and write or debug code for computational tasks giving a fast but more in-depth analysis of knowledge; 3) Lab-style Experiments, simulating various tasks in the end-to-end research process to assess model performance on those tasks and thus to provide understanding of real-world strengths and weaknesses; and 4) Field-style Experiments, capturing real-world interactions at scale to analyze user needs, model strengths, and broad capability trends, and to diagnose areas and sources of weakness in realistic scenarios.We also consider trustworthiness, i.e., alignment with ethical and safety standards, and discuss the Software Infrastructure required to implement these methodologies effectively.</p>
<p>Second, our application of "lab-style" and "field-style" techniques to LLM evaluation represents a novel approach when conducted at this scale and with such a diversity of topics with practicing scientists.These techniques go beyond existing testing methodologies: they assess in real situations the suitability of LLMs for open and unstructured problems which are both common in research and difficult to assess with either MCQs or open-response questions.</p>
<p>Third, we present improvements to techniques used within the community, including a multi-domain AI for science benchmark, called "AI4S," and the Skills, Trust, and Reliability (STaR) evaluation framework, a scalable software evaluation infrastructure.We also summarize and contextualize the research done by our team in domain-specific benchmarks, open response benchmarks, and uncertainty quantification, and note where we employ tools from other teams to complete our holistic evaluation methodology.</p>
<p>Together, these efforts aim to establish a robust methodology for evaluating the capabilities of LLMs as trusted scientific assistants.In the following sections, the collection of evaluations and scoring was based on voluntary participation of researchers and contributors.</p>
<p>The following sections describe the related work and the details of our proposed methodology.The last section discusses next steps.</p>
<p>II. RELATED WORK</p>
<p>Research on LLM evaluation encompasses various techniques relevant for the evaluation of LLMs as research assistants.Here we discuss that work and note gaps that our proposed methodology attempts to fill.</p>
<p>A. Multiple-choice Question (MCQ) Benchmarks</p>
<p>MCQ benchmarks offer a structured framework for assessing LLM performance across various domains.Notable examples are Massive Multitask Language Understanding (MMLU) [13] and MMLU-PRO [14], which evaluate general knowledge and reasoning in more than 50 subjects, including humanities, sciences and engineering.In the realm of mathematical reasoning, GSM8K [15], GSM1K [16], and MATH [9] are prominent.GSM8K and GSM1K addresses gradeschool level problems, while MATH focuses on high-school and competition-level questions.Both benchmarks have been enhanced with multiple-choice adaptations to streamline evaluation and minimize ambiguity in model outputs [17].Other significant MCQ benchmarks include ARC [18], which tests scientific reasoning, and HellaSwag [19], which challenges models with complex commonsense reasoning scenarios that, while easy for humans, are especially hard for state-of-theart models.In the field of chemistry, MCQ benchmarks include MoleculeQA [20], which comprises 61,574 MCQs, each with three distractors, focusing on factual information about molecules; MolPuzzle [21], a multimodal benchmark with over 23,000 question-answer pairs, structured with interlinked sequential sub-tasks, each providing multiple choices; and ChemBench [22], with over 2700 questions, primarily MCQs.Few of the many other MCQ benchmarking efforts in the literature are science-domain focused and validated, and as LLM capabilities improve, there is an increasing need to generate more difficult questions and leverage synergies between domain expertize and LLM judges.The multi-domain benchmark AI4S that we present below is an attempt towards that end.</p>
<p>B. Open Response Benchmarks</p>
<p>While MCQ benchmarks restrict responses to predefined options, Open Response benchmarks require that LLMs produce detailed, unconstrained outputs that can be evaluated for coherence, accuracy, and relevance.Notable examples include NarrativeQA [23], which challenges models to generate summaries or interpret longer narratives, and HotpotQA [24], which demands multi-hop reasoning with synthesized answers derived from multiple sources.Similarly, HybridQA [25] combines textual and tabular data, requiring LLMs to provide coherent and comprehensive answers.For mathematical reasoning, GSM8K [15] and MATH [9] test model ability to solve complex, multi-step problems with free-form solutions.In chemistry, benchmarks have been developed to assess LLM open-response capabilities.ChemistryQA [26] comprises 4500 complex questions that require reasoning and calculations, evaluating model ability to generate detailed, accurate responses.ChemLLMBenchmark [27] consists of eight practical chemistry tasks that necessitate understanding, reasoning, and explanatory skills, with evaluations focusing on the quality and depth of model-generated answers.The opendomain TOMG-Bench [28] molecule generation benchmark comprises tasks such as molecule editing, optimization, and customized generation, each requiring models to produce specific molecular structures or modifications based on textual descriptions.</p>
<p>Recent advancements have expanded the scope of openresponse benchmarks to address specific evaluation challenges and domains.The Open-LLM-Leaderboard (OSQ-bench) [29] transitions from multiple-choice formats to open-style questions, eliminating issues like selection bias and random guessing, while emphasizing models' ability to generate coherent, contextually accurate answers.FrontierMath [30] presents hundreds of exceptionally challenging mathematics problems that require models to generate detailed solutions, testing advanced reasoning and problem-solving skills.Similarly, Humanity's Last Exam [31] crowdsources complex questions from experts across fields to evaluate how closely LLMs approximate expert-level capabilities, highlighting their potential and limitations in addressing real-world challenges.</p>
<p>A difficulty with Open Response benchmarks is that evaluating their responses is inherently challenging due to their unstructured nature, requiring time-intensive analysis, subjective interpretation, and careful management of biases and data overload.These challenges are closely tied to uncertainty quantification (UQ), as the variability in interpretations and outcomes necessitates robust techniques to quantify and mitigate uncertainty in the evaluation process, ensuring reliable and consistent insights.In addressing these challenges, it is also important to keep track of multiple model versions and to use them consistently [32], so as to enable reproducibility [33].</p>
<p>C. Lab-Style Experiments</p>
<p>Laboratory-style Experiments with LLMs involve controlled settings in which researchers systematically evaluate model performance on specific tasks, enabling precise measurement of capabilities and limitations.In chemistry, for instance, [34] developed Coscientist, which employs LLMs to plan and execute experimental procedures based on simple human prompts, and evaluated its performance by assigning it the task of identifying synthetic procedures for seven molecules of varying complexity.Similarly, [35] proposed an approach that combines LLMs with task and motion planning to translate natural language instructions into robot-executable plans, evaluating their system through simulations in a box-packing domain.In behavioral strategy research, [36] reproduced human laboratory experiments using LLMs and compared their performance to human participants to analyze the extent to which LLMs can emulate human decision-making processes.These laboratory-style experiments provide valuable insights into applications and can inform the development of more advanced AI systems.However, comprehensive end-to-end evaluations of LLMs on scientific tasks, similar to human performance assessments, remain scarce.Such evaluations are crucial to understanding how LLMs can mimic or augment human researchers in tackling complex scientific challenges.</p>
<p>D. Field-Style Experiments</p>
<p>Field-style Experiments, also referred to as "in-the-wild" studies, involve observing and analyzing user interactions with LLM in real-world settings.This approach contrasts with controlled Lab-style Experiments by capturing nonpredefined user interactions with LLMs, providing valuable insights into how LLMs perform across diverse, unstructured tasks.Recent studies have leveraged this methodology to assess various aspects of LLM performance.For instance, WildBench [37] is designed to evaluate LLMs using realworld user interactions, enabling a comprehensive analysis of model capabilities in practical scenarios.Similarly, HaluEval-Wild [38] was designed to evaluate hallucinations in LLMs by collecting challenging user queries from real-world interactions.Shen et al. [39] conducted a study characterizing and evaluating user-LLM interactions, providing insights into user behavior and model performance and highlight the importance of understanding user needs and expectations to improve LLM utility and user satisfaction.While such analyses are currently lacking in scientific domains, developing field-style experiments specifically for scientific research presents a significant opportunity to provide critical insights into how researchers interact with AI models, thereby enhancing the creation of AI assistants tailored to scientific inquiry.</p>
<p>E. Safety Evaluation</p>
<p>In addition to the general-purpose evaluations discussed above, comprehensive evaluations must rigorously assess alignment with ethical standards, robustness against jailbreaks, and adaptability to complex real-world scenarios [37], [40].Safety evaluations are also cross-cutting :MCQs, Open-Response benchmarks, and Lab-style and Field-style Experiments.SafetyBench [41], for example, employs 11,435 MCQs in seven categories (e.g., bias, toxicity) to systematically test model adherence to ethical and safety standards in both English and Chinese.Similarly, SALAD-Bench [42] proposed 4000 MCQs, and part of the larger dataset structured into a detailed hierarchy of 6 domains, 16 tasks, and 66 categories.BigBench [43] has a subset of its tasks focused on safety evaluation in regards to toxicity, bias and truthfullness tat are MCQs.However, there has not been a focused MCQ-based safety evaluation benchmarks that take the nuances aspects of science, especially high consequential ones such as the the chemisty, biology, radiation, and nuclear (CBRN).We discuss one such effort on risks in chemistry in this work.Open-response evaluations, such as those in DecodingTrust [44] and TrustLLM [45], examine nuanced safety aspects, including hallucinations, privacy violations, and machine ethics.TrustLLM evaluates LLMs across six dimensions, including fairness and safety, using over 30 datasets to identify critical safety gaps, while DecodingTrust introduces an eightdimensional framework that probes issues like toxicity and ethical reasoning, with results published on a widely accessible leaderboard for transparency.However, such evaluations for scientific use cases are scarce [40] and thus provide an opportunity to develop them in the future.</p>
<p>The safety red-teaming methodologies can be effectively interpreted along the lines of lab-style and field-style experiments.In lab-style red-teaming, researchers interact directly with LLMs and systematically introduce adversarial prompts to identify vulnerabilities such as biases, hallucinations, or ethical compliance issues.For example, [46] present a framework for red-teaming experiments on LLMs by generating numerical questions and puzzles to evaluate the models' performance on elementary calculations and algebraic tasks.This approach provides detailed feedback at each step, allowing iterative improvements and a deeper understanding of model limitations.In contrast, field-style red-teaming assesses LLMs by analyzing large-scale human interactions in real-world environments.This method captures diverse and unpredictable user inputs, offering insights into how models perform "in the wild."For instance, [47] discuss automating red-teaming by training a separate red team LLM with reinforcement learning to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM.This large-scale approach identifies practical weaknesses and vulnerabilities that may not surface in controlled lab settings, contributing to model robustness across varied real-world scenarios.By employing both lab-style and field-style red-teaming strategies, researchers can comprehensively evaluate and enhance the safety, reliability, and ethical performance of LLMs across different contexts.That said, significant work still needs to be done in designing such experiments for safety and trust scenarios in science.</p>
<p>III. ESTABLISHING A METHODOLOGY TO EVALUATE LLMS AS RESEARCH ASSISTANTS</p>
<p>The overarching objective of using LLMs as research assistants is to accelerate the research process.</p>
<p>In Table II  As discussed in the related work section, most LLM evaluations take the form of MCQ benchmarks, which are a subset of broader Q&amp;A evaluations.However, while these benchmarks serve a useful purpose in quickly assessing the breadth of knowledge of LLMs, that cannot, by their very nature, assess the depth of reasoning, contextual understanding, and iterative problem-solving required in the various steps of the scientific research process [48], [49].</p>
<p>To address these limitations, it is critical to complement MCQ benchmarks with end-to-end evaluations in realistic contexts that assess both reasoning across multiple dimensions and the ability to plan and adapt across multi-step tasks.Therefore, we propose Evaluating AI models as scientific Research Assistants (EAIRA), a structured evaluation methodology (Table I) that combines four techniques: two existing approaches that allow for quick and repeated assessment of the breadth of LLM abilities (MCQ Benchmarks and Open-Response Benchmarks) plus two new approaches for endto-end evaluation of LLMs as scientific assistants (Lab-style Experiments and Field-style Experiments).</p>
<p>MCQ benchmarks serve to test foundational knowledge and domain-specific expertise across diverse scientific fields.In addition to previously developed benchmarks, members of our team have developed three new MCQ benchmarks to address topic gaps in tasks for which MCQs are well suited.1) Astro and 2) Climate are domain-specific benchmarks primarily generated through automated techniques to ensure the scalability of benchmark generation to serve new and evolving topics in science.3) AI4S is a multi-domain "AI for science" MCQ benchmark, which integrates human question generation and validation with automated generation and validation methods to achieve a balanced, high-quality dataset that can be developed with moderate effort.By leveraging both human and automated techniques, we can assess the strengths and weaknesses of automated methods deployed in Astro and Climate in order to guide future research in improving their generation.</p>
<p>Open-response benchmarks serve to test more detailed knowledge and to generate open-ended responses or code that assesses a model's dynamic capability to handle unstructured, complex tasks, moving closer to reflecting the flexibility and adaptability required in real-world research scenarios while still facilitating a fast, automated evaluation.For this, our team has previously developed two benchmarks: SciCode, which assesses the ability of LLMs to develop code which is highly dependent on the knowledge of the context of a scientific domain to correctly implement, and ALDbench, which assesses the ability of LLMs to describe methods for synthesizing materials using atomic layer deposition.</p>
<p>Lab-style Experiments perform evaluation by domain experts of AI models as they assist across all research tasks in real situations.This expert-reviewed method provides a comprehensive assessment of the relevance, effectiveness, and iterative improvement of a model over time (e.g., 4-12 hours per attempt).This technique goes beyond open response by using a human proctor to guide an expert to iteratively interact with the model in order to assess multi-stage planning and response to results in the context of a scientific domain.Because of the interactivity, it provides very granular feedback about what models are or are not capable of performing giving a unique insight into the strengths and weaknesses of models and possible paths for improvement.However, this capability comes at the cost of extensive effort by experts and proctors.</p>
<p>Field-style Experiments (inspired by, and adapting "in-thewild" evaluations [39], [50] for the scientific context) analyze automatically thousands of prompts, responses, and user behaviors during real-world interactions between researchers and AI assistants.This approach allows for large-scale capture and analysis of user needs, model strengths and weaknesses, and usage trends.It differs from lab-style experiments in that: 1) experts are not guided by a proctor, 2) analysis of interactions and feedback is automated, and 3) experiments can be conducted in the background as experts perform routine tasks (e.g., by capturing outputs of a site-wide LLM inference service or API proxy).These differences greatly improve the scalability of realistic end-to-end experiments but at the loss of the fine-grained granularity of the strengths and weaknesses detected by the lab-style approach.</p>
<p>In applying these four techniques, three key cross-cutting aspects must be considered: ethics, trust and safety; reliable uncertainty quantification; and scalable software infrastructure.These three aspects ensure that LLMs are "aligned" with human values, produce and qualify results correctly under uncertainty; and can be evaluated efficiently at scale.Trust and safety evaluations must address ethical alignment, defend against jailbreak attempts, and adapt to complex real-world contexts.Multiple-choice strategies [41] can identify biases, toxicity, and compliance gaps, while open-response tasks [44] can probe subtle issues such as hallucinations and machine ethics.Lab-style red-teaming [46] systematically challenges the model with adversarial prompts in controlled settings to expose stepwise weaknesses, while field-style red-teaming [47] tests the model's robustness amid unpredictable realworld inputs, unveiling vulnerabilities that may remain hidden in laboratory conditions.Reliable uncertainty quantification (UQ) is equally critical to establish trust in AI-driven scientific research assistants, as it systematically gauges model confidence and highlights potential inaccuracies [51]- [53].UQ insights guide targeted refinements in MCQ, open-response, and lab-or field-style evaluations, ensuring that areas of high uncertainty are addressed in scientific and real-world contexts [54], [55].Together, robust safety evaluations and UQ foster transparency, accountability, and trust, facilitating the responsible integration of LLMs into critical scientific research [40], [56].Finally, the scalable software infrastructure enables rapid evaluation to keep pace with rapid changes in the field of AI research.The framework needs to incorporate distributed task parallelism [57] and fast inference capabilities [58].</p>
<p>The following subsections present greater detail and results of the four techniques and three aspects of our evaluation methodology.</p>
<p>A. Domain-Specific MCQ Benchmarks</p>
<p>Domain-specific benchmarks are crucial for evaluating LLMs in specialized fields, as they address the limitations of general benchmarks that often fail to capture the complexities of domain-specific tasks.Without tailored benchmarks [8], LLMs risk over training on well-established datasets, leading to inflated performance that does not translate to real-world applicability.These benchmarks are essential for guiding targeted improvements, ensuring that models meet the specific demands of scientific research, and providing a baseline to understand their strengths and weaknesses.By capturing the nuanced challenges of individual domains, such benchmarks foster the effective and ethical deployment of LLMs, enabling their potential to accelerate discovery and innovation across disciplines.We now discuss the domain-specific benchmarking efforts that we conducted in Astronomy and Climate modeling.</p>
<p>1) Astronomy: The Astronomy Benchmark [8] assesses LLM performance in a manner that reflects the interdisciplinary nature of astronomy, testing both factual recall and the ability to connect insights across subfields.This benchmark was generated automatically using an LLM to compose MCQs from astronomy papers.To assemble a rich repository of scientific knowledge, we leveraged the Annual Review of Astronomy and Astrophysics (ARAA), a selective review journal renowned for its comprehensive overviews authored by leading experts.</p>
<p>The Nougat optical character recognition (OCR) tool was used to transcribe 885 ARAA articles over the years 1963 to 2023.Each transcribed paper was processed using Gemini-1.5-Pro,a long-context LLM capable of handling up to one million tokens, to generate five multiple-choice questions (MCQs) per paper.The questions were designed to be specific, yet independent of the article sections, with generalized answers and balanced options to avoid bias.This process yielded a total of 4425 MCQs covering diverse topics such as quasar density decline at high redshifts and subgrid feedback model calibration in simulations.</p>
<p>Sample question from Astronomy benchmark dataset</p>
<p>How does the presence of stellar companions influence the formation and detection of exoplanets?The Astronomy Benchmark has been then used to assess both the accuracy and computational cost of dozens of different closed and open LLM variants.This study revealed disparities in LLM performance across general-purpose and specialized tasks that highlight significant performance gaps and performance-to-cost ratios.While frontier models like Claude-3.5-Sonnetexcelled in the Astronomy Benchmark with an accuracy of 85.0%, outperforming GPT-4o (80.4%) and Gemini-1.5-Pro(77.6%), these differences are less evident in general-purpose benchmarks such as MMLU [29].A study of how Astronomy Benchmark performance varied with compute costs showed that, roughly speaking, each 3.5 percentage points increased accuracy was associated with 10-fold increase in price, within most given series of models such as GPT, Gemini or Claude.An analysis of the cost per 0.1 M tokens showed that the cost for a desired performance can vary by more than three orders of magnitude across different models: see Figure 2 in [8].</p>
<p>The study also showed that open-weight models, though improving, lag behind proprietary ones, with older versions underperforming by as much as 30% on specialized tasks.Performance also varies significantly between English-focused and non-English-focused models, with the latter struggling in areas like theoretical astronomy and advanced instrumentation, reflecting gaps in training datasets.However, recent astronomy subfields, such as post-1990 advancements, exhibit narrower performance gaps which may be due to model's ability to handling historical context or older scientific consensus.These findings emphasize the importance of domain-specific benchmarks to assess not only performance in specialized tasks but also the performance-to-cost ratio crucial for user adoption in assisting with scientific research.This work also shows that performance varies across sub-fields.</p>
<p>2) Climate: Climate and weather forecasting presents multifaceted challenges that demand interdisciplinary knowledge and reasoning, making it an ideal testbed for evaluating LLM capabilities.However, existing benchmarks for climate science are limited [59]- [61].Thus we developed a Climate Benchmark, a set of MCQs focused on the urgent and complex domain of climate science.As the manual creation of such MCQs is labor intensive and climate scientists are already overcommitted, we employed automated methods to develop the Climate Benchmark.We adopted as our source material the Intergovernmental Panel on Climate Change (IPCC) reports [62], which are authoritative assessments synthesizing the latest scientific research on climate change, its impacts and potential solutions.These reports serve as a foundation for informed decision-making and international negotiations on climate action, highlighting the urgency of addressing the complex and interconnected challenges posed by changing climate.The reports are typically extensive, often exceeding 1000 pages, with each chapter and section addressing specific topics related to climate change.To generate questions on the various topics discussed in the report, we parsed the PDFs section-wise, using Nougat as was done for the Astronomy Benchmark.We then employed OpenAI's GPT4 to create one multiple-choice question, consisting of one correct answer and four distractors, for each section, with the prompt designed to create an MCQ based on the provided scientific text, ensuring that the question evaluates a broader understanding of climate science principles without referencing the specific report.This process resulted in a total of 752 questions on diverse topics, ranging from factors influencing vulnerability to climate change to primary strategies for risk reduction.This systematic approach ensured comprehensive coverage and alignment with the content of the IPCC reports.</p>
<p>We employed the Climate Benchmark to assess the performance of multiple LLMs, including GPT-4o, Llama 3.8B, and Phi 4, on specialized climate science tasks.Among these models, GPT-4o performed the best, achieving an accuracy of 87.34%, demonstrating its effectiveness in handling complex climate-related tasks.GPT-4o was followed by Llama 3.8, which achieved an accuracy of 78.48%, and Phi 4, which scored 54.43%.This performance disparity highlights the need for continued refinement and optimization of models to bridge the gap in specialized applications.</p>
<p>The development of the Climate Benchmark provided useful insights into the creation and evaluation of MCQ datasets for scientific applications.The climate-focused MCQs, derived from IPCC reports, were designed to assess knowledge recall and decision-making, emphasizing accurate understanding of scientific concepts.While primarily testing factual knowledge, these questions establish a strong foundation for expanding into tasks that require more complex reasoning and application in climate science.However, the automatic generation of MCQs sometimes produced semantically similar questions that differed only slightly in phrasing or structure while testing the same core information.This observation highlights the need for robust evaluation mechanisms to eliminate redundancy and ensure diversity within the dataset.While automatic MCQ generation greatly accelerates the creation of benchmark questions, it cannot replace a rigorous evaluation process.A combination of LLM-based evaluators and human oversight is crucial for maintaining the benchmark's quality, relevance, and accuracy, ensuring that it meets the standards required for research-focused benchmarks.</p>
<p>B. Scalable AI4S MCQ Benchmark</p>
<p>The overarching goal of the AI4S benchmark is to evaluate the knowledge extension of LLMs across many different scientific domains.In that respect, it is similar to GPQA [63].However, AI4S design focuses on the quality and scalability of MCQ generation and validation.While GPQA has only 448 MCQs, our objective is to generate and validate thousands of MCQs and continuously add MCQs as LLMs progress in their capabilities.</p>
<p>Current MCQ benchmarks, including GPQA, have two main limitations: 1) they are quickly saturated because of the fast progress of LLMs.2) there is a high risk of contamination (benchmark included in the training sets) if the MCQs are made public.These two limitations arise from the static aspect of the current benchmarks.They are developed once and do not evolve.The current practice is to develop new versions [64] when the initial benchmark is saturated [65] and to open only a portion of the benchmark MCQs to avoid contamination.</p>
<p>To address the two limitations, we explore a novel approach to develop scalable generation and validation of MCQ benchmarks for the evaluation of LLMs capabilities: Automatic continuous Generation and validation of Increasingly Large MCQ benchmarks: AGIL.</p>
<p>Building on the experience gained from the Astronomy and Climate domain-specific benchmarks, we are developing an AGIL process by which we combine manual and automated methods to generate and validate MCQs.</p>
<p>We present here our initial finding towards the creation of a 1000-MCQ AI4S benchmark that spans five scientific The AGIL approach enables the integration of automatically accepted MCQs after the validation of their difficulty and quality.domains: Computer Science, Astrophysics, Climate, Physics, and Chemistry.Our goal with this first study is to assess key aspects of the AGIL approach: validity of the difficulty level, quality of the generated MCQs compared to GPQA ones, quality of the automatically generated MCQs compared to manually generated ones, and quality of the automatically validated MCQs.</p>
<p>In the AGIL approach (Figure 1), manual MCQ generation and validation by experts is critical to provide high-quality, domain-specific MCQs that serve as a gold standard for evaluating LLM performance and developing automatic MCQ generation and validation workflows.The goal of automated generation and validation (LLM as a judge technique [66]) by LLMs is to address the scalability issues of manual generation and validation while keeping their quality levels.</p>
<p>For the manual generation and validation of MCQs we organized hackathons which engaged 140 domain experts (PhDs and other research staff) 1 .We offered the participants to generate MCQs from scientific papers of their choosing, including their own.The manually generated MCQs were crafted by using a purpose-built authoring interface (Appendix, Figure 9) that allowed contributors to test their questions on smaller models like Llama 3 before submission so as to ensure a baseline difficulty threshold.Manual validation used another specifically created form (see Appendix, Figure 10).The tool used for the MCQ generation and validation is available on github (https://github.com/auroraGPT-ANL/questions-ui)Automated MCQ generation leveraged LLMs such as GPT-4 with domain-specific prompts to create MCQs from scientific 1 the exercises in this paper were done with volunteer Argonne employees, who understood that the goal of this effort is to identify opportunities to improve models.While the research team offered a rubric for notes and observations, the participants were free to use whatever rubric they preferred.The following is the general approach that was used.papers across fields such as climate science, physics, and chemistry, with validation guided by the LLM-as-a-Judge technique.Both the prompts used for automatic MCQ generation and validation, as well as the rubrics for validation and reviews, were informed by the manual generation and validation, as well as by the experience gained during the domain-specific benchmark development discussed earlier.</p>
<p>This process generated 980 MCQs (720 manually and 260 automatically).Of 588 total manually generated reviews, 317 MCQs have been assessed so far, of which 254 were accepted.Acceptance was subject to the following criteria: appropriate difficulty, relevant, complete and correct answers and distractors, controversial answers, mathematic requirements, as well as relevant skills and domain selection.The small percentage of accepted MCQs 25% illustrates the difficulty of generating and validating high-quality scientific MCQs.</p>
<p>In addition, domain experts categorized the accepted MCQs into easy, medium, and hard levels, capturing a spectrum of difficulty that mirrors real-world scientific challenges.This multi-level structure enables the AI4S benchmark to evaluate both the foundational and advanced capabilities of LLMs, offering an assessment of their strengths and limitations.</p>
<p>To evaluate the merits of our AGIL approach, we performed several tests to evaluate the quality of (i) the level classification, (ii) the AI4S benchmark compared to GPQA, (iii) the automatically generated MCQs, and (iv) the automatic MCQ validation.</p>
<p>For all tests, we used the STaR framework (see section VI) with five shots.The first row of the table Table III shows the overall performance of Llama 3 8B on the 254 MCQs.The resulting accuracy of 20% correspond to a random guess.The next three rows show results for MCQs grouped by their human-identified levels of difficulty.We see that the  III show the performance of other Llama-3 models for the AI4S accepted MCQs.We observe that Llama 3-70B performs less well on all AI4S-accepted MCQs (26% of questions answered correctly on all difficulty levels) than on GPQA (49% of questions answered correctly).Note that while GPQA has one correct answer and three distractors, meaning 25% accuracy for random responses, AI4S MCQs have four distractors and thus only 20% random response accuracy.We conclude that AI4S is a more challenging LLM benchmark than GPQA.</p>
<p>To obtain a finer quality comparison between AI4S and GPQA MCQs, we use automatic MCQ validation to quantify the quality of every MCQ on the first seven criteria (N/A value for the eighth criterion on GPQA).Table IV shows the scores of the two benchmarks on the seven criteria.On average, AI4S MCQs (average of 4.55) reach the same quality level as the GPQA MCQs (average of 4.45).Overall, these results validate the quality of AI4S compared to GPQA.</p>
<p>We used the acceptance criteria to compare the quality of the manually and automatically generated MCQs.This comparison reveals that the quality of manually validated, automatically generated questions is on par with manually validated/generated ones.Overall these results show that automatic MCQ generation can be used to generate MCQs with a manual verification step.</p>
<p>To evaluate the quality of automatic MCQ validation, we compare it with manual validation.We used Mistral Large 2 as the judge, prompting it to evaluate each MCQs on a scale of 1-5 for each of eight criteria and also asking it to provide concise rationale for each score.All criteria were evaluated in a single prompt.We expanded each criteria specification to define each level on the scale for all criteria explicitly.The prompts used for the judge are in the Appendix, Figure 17.We observe the accuracy of a model trained on these judgements to predict whether a question will be accepted/rejected to be 72%.For comparison, of the 144 questions with multiple reviews, only 61% of reviewers agreed on acceptance.</p>
<p>During the development of the AI4S benchmark using our proposed AGIL approach, several key lessons emerged.Generating high-quality scientific MCQs manually is a challenging task for multiple reasons: crafting questions with distinguishable levels of difficulty (undergraduates, PhD students, postdocs, and staff) is non-trivial, and creating distractors that are plausible but not overly confusing requires significant effort and precision.Validation of these questions proved even more demanding, as finding appropriate reviewers for difficult and specialized topics can be challenging, and ideally, each question requires validation by three experts to ensure reliability.Our goal is to continue the development of the AGIL approach to address this difficulty issue and to continuously generate AI4S MCQs from the large pool of available scientific papers.We will release the benchmark using a sliding-window approach, keeping a significant portion of newly generated MCQs (e.g.70%) not public.</p>
<p>C. Open Response Benchmarks</p>
<p>Next we discuss the open-ended benchmarks.These are essential for evaluating the reasoning, creativity, and problemsolving abilities of LLMs, particularly in scientific domains.Unlike MCQs, which primarily test factual recall or singlestep reasoning, open-ended tasks engage models with complex, real-world problems that require multi-step reasoning, synthesis of knowledge across domains, and adaptive problemsolving.For instance, while an MCQ might test a model's recall of a specific scientific fact, an open-ended task could require the model to design an experiment, analyze data, or propose solutions to unsolved research questions.This format aligns better with the exploratory nature of scientific inquiry, offering a more comprehensive assessment of a model's capabilities.However, existing open-ended benchmarks often fall short in capturing the depth and realism needed for scientific evaluations.Many rely on synthetic tasks that fail to reflect the intricacies of real-world scientific challenges, such as multidisciplinary reasoning or generating accurate code for practical applications.</p>
<p>While open responses questions are versatile in capability, their evaluation is more complicated compared to MCQs.Different assessment approaches are applied, depending on the task at hand.A first class of statistical scorer approaches looks at co-occurrence of n-grams (sequences of letters or words) in LLM outputs vs. supplied ground truth responses.In this context, widely used scores are BLEU (BiLingual Evaluation Understudy) compares LLM outputs against ground truths.It calculates the precision for each matching n-gram (n consecutive words) between an LLM output and expected output.ROUGE (Recall-Oriented Understudy for Gisting Evaluation) evaluates text summaries and computes recall by comparing the overlap of n-grams between LLM outputs and expected outputs.It determines the proportion (0-1) of n-grams in the reference present in the LLM output.METEOR (Metric for Evaluation of Translation with Explicit Ordering) calculates scores by assessing both precision (n-gram matches) and recall (n-gram overlaps) and leverages linguistic databases to account for synonyms.Statistical scorers do not take any semantics and reasoning capabilities into account.</p>
<p>A second class are embedding approaches that seek to compare the semantics of LLM responses and reference answers.Here, some widely used scores include BERTScore, that relies on pre-trained language models (e.g., BERT) and computes the cosine similarity between the contextual embeddings of the LLM responses and references.These similarities are then aggregated to produce a final score.Other tools such as CheckEmbed [67] can be used to compare the semantics of LLM responses and reference answers.The third and most recent class is the LLM-as-a-judge methods which tackle the problem of evaluating LLM open responses when no reference answer is available.This approach currently has two variations.In the "Pairwise comparison" version, an LLM judge is presented with a question and two answers, and tasked to determine which one is better or declare a tie.In the "Single answer grading" version, an LLM judge is asked to directly assign a score to a single answer.In principle, LLM-as-ajudge can offer several key benefits: consistency, scalability, and explainability.However, the approach also has limitations: position bias (first answer better in "Pairwise comparison"), verbosity bias (longer answer better), self-enhancement bias (self-generated answer better) and limited capability to grade math and reasoning questions [66].Moreover, the reliability of such evaluations is still the subject of research.</p>
<p>We now discuss two open-ended benchmarks, one for scientific code generation and another for atomic layer deposition in Material Science.</p>
<p>1) SciCode -Scientific Code Generation Benchmark: The SciCode Benchmark is a set of manually curated coding problems designed to assess LLM capabilities for solving complex scientific coding problems across diverse domains.By providing tasks that reflect real-world challenges and require multi-step reasoning, SciCode allows models to be tested in contexts that align closely with the demands of scientific research [68].SciCode includes problems across a range of scientific domains, including computational mechanics, quan-tum information, quantum chemistry, ecology, and molecular modeling.It consists of 80 main problems, decomposed into 338 intermediate steps, enabling a structured approach to assessing model capabilities.Solving each individual problem requires that an LLM implement multiple Python functions corresponding to subproblems and integrate those functions into a cohesive solution: see Figure 14.Each problem is accompanied by a gold-standard solution and multiple test cases so as to permit robust and reliable automatic evaluation.</p>
<p>Each SciCode problem is meticulously annotated and verified by at least two senior researchers to ensure accuracy, and is drawn from real-world research tasks, maintaining relevance to practical applications.Problems are curated to avoid overlap with publicly available datasets and thus to test the deep scientific knowledge and analytical skills of LLMs by requiring the decomposition and integration of complex problems into comprehensive solutions.Additionally, SciCode allows for flexible evaluation of model capabilities in varied setups, enabling adjustments like providing background information or conditioning on previous solutions.</p>
<p>The SciCode Benchmark is configured to assess LLM capabilities to solve SciCode problems by using zero-shot prompts, maintaining a general approach while creating distinct prompts for various evaluation setups to guide the model on the tasks, as described in detail in [68].The prompts remain consistent across models and fields, incorporating instructions for the main and sub-problems, as well as code from previous subproblems.We evaluated the coding capabilities of several state-of-the-art LLMs using the SciCode benchmark, focusing on three key aspects to assess their performance.First, the Impact of Scientific Background was analyzed by testing models in two modes: without background, to evaluate inherent scientific knowledge and reasoning, and with background, to focus on coding and instruction-following capabilities.The results showed significant performance improvements with background information, highlighting the limitations of current LLMs in scientific reasoning.Second, the comparison between Gold vs. Generated Solutions revealed insights into the challenges of realistic evaluations.While gold solutions accurately address each problem, generated solutions introduce error accumulation, creating a more practical and demanding evaluation scenario.Lastly, the assessment of Main vs. Subproblems provided a nuanced understanding of model performance.A main problem was considered solved only when all subproblem solutions and the integrated result were accurate.Additionally, SciCode's design allows independent evaluation of subproblems, enabling precise analysis of models' reasoning and coding abilities across discrete tasks.These evaluation dimensions underscore the benchmark's rigor in testing LLMs for real-world scientific applications.</p>
<p>We summarize the findings of our studies using several state-of-the-art models in Figure 2.These results show that SciCode is a difficult benchmark for current LLMs.Consistent with our observations on proprietary models, open-weight LLMs under test also showed their lack of capabilities in Fig. 2. The performance of various LLMs on SciCode problems.This histogram displays the accuracy (vertical axis, 0% to 100%) of various state-of-the-art LLMs (listed on the horizontal axis) in solving both main problems (red) and their associated subproblems (blue) within SciCode.To solve a main problem, LLMs must implement one Python function per subproblem and integrate them into a comprehensive solution.SciCode provides gold-standard solutions and multiple test cases for reliable automatic evaluation.These consistently poor results highlight the need for LLMs that incorporate scientific knowledge and advanced reasoning to better assist researchers.</p>
<p>solving any main problem despite being able to solve a number of sub-problems correctly.</p>
<p>The SciCode project provides insights into the challenges of evaluating LLMs in scientific coding tasks, highlighting significant gaps in current capabilities.Despite recent advancements, state-of-the-art models like OpenAI's o1-preview and Claude3.5-Sonnetsolve only a small fraction (7.7%) of the main problems, underscoring the disparity between existing LLMs and the deep scientific reasoning required for real-world research.SciCode is designed to address this gap by focusing on real-world, research-level problems across diverse natural science fields, including mathematics, physics, chemistry, and biology.Sourced from peer-reviewed work, these problems test LLMs' ability to generalize to less familiar domains.By decomposing problems into subproblems with detailed annotations, SciCode rigorously evaluates models' coding, reasoning, and knowledge integration capabilities.While providing scientific background information improves model performance, the persistent struggle of LLMs with these tasks emphasizes their current limitations in handling complex scientific challenges.The project highlights the importance of high-quality data, domain-specific validation, and carefully curated problems to advance the development of AI tools for scientific research.The findings indicate substantial progress is needed to enhance scientific reasoning and background knowledge integration in LLMs to enable their effective application in real-world scenarios.</p>
<p>2) ALDbench -Materials Synthesis Benchmark: An area that lacked relevant benchmarks is materials synthesis.This is particularly important for potential applications of LLMs in automated materials discovery or as AI research assistants.LLMs underpinning such capabilities need to exhibit both the ability to reason about specific processes (for instance to avoid unsafe conditions or transfer ideas across reactors and process conditions) and have a robust understanding of the literature (to build on existing process knowledge and avoid known dead ends).</p>
<p>As such capabilities appear hard to evaluate by using either MCQs or the statistical scorer or embedding approaches described earlier.we developed a new open-response benchmark ALDbench on materials synthesis, and in particular on a synthesis technique called atomic layer deposition [69].Here we targeted a range of difficulty spanning from graduate level to PhD-level domain expert current with the state of the art.A model's ability to perform at a domain expert level is paramount whenever models are expected to assist in decision making processes that involve costly experiments.Beyond its applied interest in areas such as energy and microelectronics [70], this domain brings together multiple topics that are commonplace in chemistry-driven synthesis, including metalorganic and inorganic molecules, gas-surface kinetics and heterogeneous reactions, and gas phase transport.Evaluating LLM capabilities in this field can provide insights with wide applicability to other material synthesis techniques.To compile the benchmark, we asked six PhD-level human experts to generate "questions that a researcher or a graduate student who is not familiar with a specific process/application would ask an AI assistant."The curated questions could be grouped into four categories: 1) how to grow, where the query is about material synthesis; 2) specific questions about ALD processes, comprising more in-depth queries about a process or material; 3) general ALD knowledge, with questions about the synthesis technique; and 4) applications.</p>
<p>The human experts were then asked to grade the questions using a scale of 1 to 5 on two criteria with the following rubrics similar to the AI4S benchmark: (1) Difficulty: 1-Easy, early graduate; 5-Hard, top expert; (2) Specificity: 1-General; 5-Specific, quantitative.</p>
<p>Each response is then graded using four criteria with the following rubrics: (1) Overall quality: 1-Very low quality; 5-Excellent; (2) Specificity: 1-Too broad; 5-Targeted; (3) Relevance: 1-Irrelevant fluff; 5-Relevant answer; (4) Accuracy: 1-All made up; 5-All correct.The use of multiple criteria allowed us to probe aspects of the generation process, such as relevance or specificity of the response, that are not easily captured by benchmarks focused on accuracy.</p>
<p>We ran this benchmark using an instance of OpenAI's GPT-4o, with seven PhD-level human experts reviewing model responses.Details are in the ALDbench paper [69].The model responses received a composite quality score of 3.7, consistent with a passing grade.However, 36% of the questions received at least one below average score.When we carried out an in-depth analysis of the responses we identified at least five instances of hallucination.In Figure 3 we show the distribution of mean scores for all the questions in the benchmark and the four criteria evaluated by the human experts.</p>
<p>We also explored statistical correlations between the dif-ficulty and specificity of each question and the human expert scores for each evaluation criteria.For each (question, response) pair we computed p-values using the Fisher exact test to evaluate the statistical significance of the correlation.We found statistically significant correlations between question difficulty and response quality (p 0 = 0.033), question difficulty and relevance (p 0 = 0.016), and question specificity and response accuracy (p 0 = 0.007).In all three cases, higher difficulty or specificity correlated with lower scores.These results emphasize the need to evaluate LLMs across multiple criteria beyond traditional metrics of difficulty and accuracy.</p>
<p>Our results show that highly targeted, open-response benchmarks can provide information about LLM performance in scientific domains that is complementary to MCQs or natural language processing benchmarks.The methodology developed in this work allowed us to probe in depth model performance in a specific domain.With the aid of a small team of PhD-level experts we were able to identify instances of hallucinations and explore model responses in a level of detail that it is hard to accomplish using automatic evaluation methods.The extension of this approach to other domains, such as energy storage or microelectronics, is trivial.Moreover, as a byproduct of this effort, we collected a small dataset of questions and human rated responses across four different evaluation criteria.As we explore other domains we can use this data to train or validate automatic question evaluation approaches for openended benchmarks.</p>
<p>D. End-to-End Evaluations</p>
<p>Although MCQ benchmarks are effective in testing factual recall and reasoning within constrained formats, and openended benchmarks gauge the generation of detailed and flexible responses, these methods do not capture the iterative and complexity of scientific problem solving.End-to-end evaluations attempts to address this gap by assessing, in real situations, the models responses for assisting researchers in solving scientific problems.We propose two novel types of end-to-end methods in the context of scientific research: Labstyle and field-style experiments.</p>
<p>1) Lab-style experiments: These experiments are designed to evaluate the capabilities of AI models to assist researchers in performing the typical tasks (Table I) to solve scientific problems.Note that, in real situations, these tasks are often repeated several times to solve problems.By capturing and evaluating all interactions between research and LLMs while attempting to solve a research problem, a labstyle experiment can capture accurately the complex reality of solving complex research problems.It can thus provides a unique perspective on the "distance" between the ideal scientific assistant and the current capabilities of AI models.</p>
<p>The setup for lab-style experiments involves defining a specific scientific problem and presenting it to multiple AI models for comparison.Each model is manually tasked with assisting in all the research tasks using the same prompts, ensuring consistency across evaluations.Prompts and responses are meticulously recorded, and domain experts analyze and Fig. 4. Example of multi-turn interaction between a researcher and several LLMs used as research assistants in an attempt to repeat the research developed for the HPDC24 paper.Because of space limitations, the figure does not show the models' responses and research analysis.The full interaction for the HPDC24 experiment can be found here: https://tinyurl.com/yv4awky3comment on each response to assess model relevance, effectiveness, and overall performance.The response of each model in every step is compared to that of a human assistant, typically at the post-PhD level.Performance is characterized by evaluated against criteria such as correctness, conciseness, and precision.</p>
<p>We have conducted lab-style experiments with not only different LLMs but also different versions of individual LLMs, with the latter studies permitting evaluation of improvements over time.For example, a relevant metric of progress across model generation is the number of prompts needed to solve a particular research problem.By focusing on real-world scientific workflows and expert evaluation, lab-style experiments provide a practical approach to observe AI model improvements as scientific research assistants.</p>
<p>To date, we have performed three lab-style experiments with five domain experts (one expert per experiment) 1 each of whom provided a problem to solve, generated prompts covering the different research steps, and analyzed model responses.Three experiments were related to parallel/distributed computing (scheduling of a directed acylic graph; solving a partial differential equation; zero-overhead checkpointing) and were selected carefully from three categories: open problem (no known solution), published problem (solution known), and recently published problem (solution known).For the "published problem," the paper is more than two years old, and thus we assume that AI model developers had access to the paper.For the "recently published problem," the experiment was performed just a few months after the publication; here, we assume that many models were not trained with the paper.</p>
<p>The experts with whom we worked in these experiments had never previously used AI models as research assistants.The experts provided two levels of analysis: 1) a detailed analysis of the responses received for individual prompts, and 2) highlevel scoring compared to a human researcher using the A, B, C, D, E, F scale (A being the human reference, F being the worst possible score).In total, the three experiments cover about 20 hours of interactions, about 100 prompts, and about 250 pages of testing and analysis.We used 10 AI models (not all models were used in all three experiments): O1-preview, GPT-4o, GPT-3.5, Claude3 Sonnet, Claude Haiku, Mistral, Llama3 70B, Llama3 405B, Perplexity Pro, and Gemini 1.5 (not all models were tested on all prompts because some models produced incorrect responses before reaching the end of the evaluation.)</p>
<p>We show in Figure 4 the beginning of the multi-turn interaction and in Figure 5 part of our initial high-level scoring of several AI models for the zero-overhead checkpointing problem.The lab style experiment was performed on August 20, 2024.The scoring reflects the performance of the models during the experiment.This problem falls into the category of recently published problems.The goal was to check whether the models could reproduce the analysis and design of the LLM checkpointing system presented in the 2024 ACM International Symposium on High-Performance Parallel and Distributed Computing (HPDC24) best paper [71].The most important part of the experiment was to assess each model's ability to 1) identify the fundamental observation (non-mutable parameters and optimizer state during the forward and backward passes of LLM training) and 2) propose a design for an asynchronous checkpointing system that exploits this observation.</p>
<p>Based on these initial three experiments, we performed two other experiments on open problems in Chemistry and Biology.These experiments used the same overall interaction collection approach and compared more recent reasoning models (O1-preview, OpenAI O3-mini, Gimini 2.0 experimental).We also used a more rigorous scoring system, defining precisely every score level for every evaluated skill.From these experiments, we developed a Lab-style experiment tool to collect the problem setup (Figure 11), every prompt-response-assessment (Figure 12), and final assessments (Figure 13).This collection tool is available on github (https://github.com/auroraGPT-ANL/questions-ui)</p>
<p>Our experience with the "Lab-style experiment" method allows for several observations regarding its utility and limitations.Unlike traditional benchmarking, this method places AI models in real-world research scenarios, enabling evaluators to directly assess their knowledge, capabilities, and overall usefulness for specific tasks.By relying on multi-turn prompting and open responses, the method also tests the propensity of AI models to digress (e.g., for the HPDC24 paper experiment, a model had a tendency to focus on the consistency aspect of checkpointing, which is not relevant in that context, instead of focusing on overhead reduction) and hallucinate (as seen with earlier models in 2024 that frequently generated fabricated scientific references).However, this approach is not yet scalable and remains narrow in coverage; it requires significant manual effort, with two researchers spending 5-6 hours analyzing and comparing models for specific tasks.The specificity of the addressed research problems further limits the generalizability of the findings.Despite these constraints, the method excels in two aspects 1) providing a fine-grain capability assessment of LLMs as scientific assistants in a realistic context and 2) tracking model progress across generations.For instance, in solving the zero-overhead checkpointing problem, successive model iterations demonstrated improved efficiency, reducing the number of prompts needed to reach the key insight-from five prompts with GPT4o to just one prompt with Argo/O1-preview, which incorporates a science-oriented preprompt.This result highlights the method's potential to reveal meaningful advancements in AI capabilities over time.</p>
<p>2) Field-style experiments: This method takes inspiration from previous studies analyzing user-LLM interactions at scale [38], [39], [50].The "In the wild" method captures and analyzes all the interactions between volunteer users and AI models.This method provides additional critical information for the development and improvement of AI models for science: a precise understanding of researcher needs and requirements regarding AI assistants (e.g.: What task do researchers ask AI models to perform?What are their expectations regarding model responses?How frequently do researchers use AI models?); a deeper understanding of AI models strengths and weakness (by analyzing the thousands of prompts and responses); a window on the trends behind the use of AI models as research assistant (e.g., increased usage frequency, increased number of users, nature and distribution variations of the performed tasks); and tracking of AI model progress across generations.Ideally, this method will analyze online thousands of user-LLM interactions.Compared to the "Labstyle experiment" method, users are not expected to evaluate LLM responses directly.Instead, evaluation is indirect, based on the study of the flow of prompts and responses.This method leverages user behavior as a signal to diagnose LLM failure modes [72].For example, a user submitting rephrasing questions, providing feedback, or abandoning the interaction are signs of LLM weaknesses in understanding user intent.The flow can then be analyzed to diagnose potential sources of weakness [73].Previous "in-the-wild" experiments focused on nonscientific domains.The Field-style experiment method adapts the "in-the-wild" approach to the scientific context by defining criteria and scoring specific to the scientific methodology.</p>
<p>On November 1, 2024, Argonne organized a JAM session that captured 180 conversations between Argonne researchers and Argo/O1-preview 1 Researchers were asked to bring to the JAM session evaluation a scientific problem that they would work on with Argo/O1-preview as a research assistant.At the end of the session, the researchers evaluated their experience according to five criteria: Novelty, Productivity, Solution, Strength, and Importance.(This approach is consistent with that followed in a much smaller study conducted at Los Alamos National Laboratory.)Five possible responses were proposed for each question, corresponding to a score of 1 to 5.</p>
<p>The scores produced by the Argonne researchers indicated that: 1) Importance: 82% of researchers consider that AI models such as Argo/O1-preview are "very important" or "critical" to their team's success, 2) Strength: at 59%, they consider that AI models significantly or noticeably improve productivity, 3) Productivity: 51% of researchers compare AI models such as Argo/O1-preview to PhD students or postdocs, 4) Solution quality: 50% of the researchers consider that AI models such as Argo/O1-preview produce exceptional or strong solutions, 5) Novelty: only 21% of the researchers consider that AI models such as Argo/O1-preview provide notably novel or groundbreaking solutions.(Argo is Argonne's API proxy for OpenAI models including o1 preview.Because OpenAI does not make it's system prompt available, this needs to be recreated.We note the system prompt used for Argo in the Appendix, Figure 15.)Although informative about the needs expressed by researchers to access models such as Argo/O1-Preview, the outcomes of the November 1, 2024 JAM session evaluation did not identify the specific strong and weak science skills of Argo/O1-preview.To understand the reasons behind the perceived weaknesses of Argo/ O1-preview, we use LLama-3.3-70B-Instruct for an LLM-as-a-judge approach to analyze the recorded conversations and score the performance of the Argo/O1-preview concerning scientific skills.</p>
<p>The goal is to develop a pipeline (workflow) to analyze the recorded conversations to assess the strengths and weaknesses of LLMs as scientific assistants.From the 180 JAM session survey responses, we manually filtered a subset of 125 that had valid transcripts and were sufficiently challenging scientific problems, requiring PhD level domain expertise and reasoning capability.We generated an initial version of the prompt to analyze the conversations using Gemini experimental 1206 and refined it manually.We chose Llama-3.3-70B-Instruct as the highest performing open model on several benchmarks including GPQA, and presented it with each transcript formatted into a detailed LLM-as-a-judge prompt (see Appendix, Figure 18) to evaluate 29 scientific criteria.The responses contained strengths, weaknesses, and examples/evidence from each conversation, as well as a formatted scoring from 1-10 for each of the criteria.The model was instructed to identify skills that did not apply to a given conversation rather than give an actual score.A score of 0 reflects a situation in which the model could not assign a score to the criteria because it determined that the criteria were not applicable.After obtaining the responses for every conversation, the model was provided batches of 25 to summarize, with specific instructions that these summaries would be used to synthesize a final summary.Batches were used, as the total token count of all the responses was 164K, larger than the default 128K window of Llama 3.3.</p>
<p>Figure 6 presents the results of the conversation analysis pipeline as a proof-of-concept.The presented results need human validation: LLMs are known to hallucinate and present overly positive assessments of outputs compared to human reviewers.Additionally, the field-style experiments revealed that 59% of researchers reported noticeable productivity improvements using LLMs, while 51% likened the LLM's contributions to those of PhD students or postdocs.However, only 21% rated the models as delivering notably novel or groundbreaking solutions.These results should not be considered a complete or definitive assessment of human assessment of LLM performance.</p>
<p>The Field-style method for analyzing the strengths and weaknesses of LLMs is still in its nascent stages.Our analysis Fig. 6.LLM-generated summary of detected LLM strengths and weaknesses in 125 scientist-Argo/O1-preview conversations of scientist-LLM conversations represents the first attempt to use an LLM-as-a-judge to evaluate the performance of an LLM as a research assistant.Insights from the JAM session organized at Argonne highlight several lessons.First, scoring user-LLM interactions holistically with a small set of criteria (five in this case) permits only a high-level evaluation, insufficient for diagnosing specific sources of LLM weaknesses.Second, recording user-LLM interactions with detailed annotations, such as identifying the skills required for each prompt and scoring individual responses, offers greater diagnostic potential.While this detailed approach is not feasible for general scientist-LLM dialogues, it can be implemented in structured, especially organized sessions.Lastly, while LLMsas-a-judge offers a scalable mechanism for analyzing user-LLM interactions, the current implementation remains a proof of concept.Additional research and validation are necessary to build confidence in the results produced by this analysis pipeline.</p>
<p>IV. RELIABILITY AND UQ OF EVALUATIONS</p>
<p>The success of LLMs in scientific domains, such as chemistry, biology, and physics, has been remarkable, but their trustworthiness as scientific assistants remains a significant concern.These models, including GPT, Claude, and Llama, are prone to generating unreliable or fabricated responses, often referred to as hallucinations [52].Understanding and quantifying uncertainty in LLM outputs is essential to ensure safe, reliable, and informed decision making, particularly in scientific domains.Traditional uncertainty quantification (UQ) techniques, which rely on accessing internal model parameters [74], face challenges due to the black-box nature of modern LLMs like GPT-4, Claude 3, and Gemini, which are primarily accessible as API services.Recent research has focused on developing novel approaches to assess uncertainty directly from model outputs, such as semantic entropy [75], sampling-based methods, and aggregation techniques [51], [76].These techniques aim to evaluate input sensitivity and output consistency, highlighting where models are most vulnerable.By improving transparency and trust, these UQ strategies play a crucial role in responsible AI deployment.Addressing these challenges is vital for leveraging LLMs in scientific applications, where errors can have substantial consequences.Moving forward, advancing UQ methods and enhancing LLM interpretability will be key to making these models safer and more robust in critical scientific and industrial domains.</p>
<p>Inspired by psychological assessments in which the same question is asked in different ways to test consistency, we propose a technique called Question Rephrasing [55] to quantify the uncertainty in LLM outputs.This approach involves rephrasing a given question while preserving its original semantic meaning and comparing LLM responses before and after rephrasing to assess input uncertainty.In addition, we adopt a sampling method that repeatedly queries an LLM with identical inputs to evaluate output uncertainty.We applied these methods to assess GPT-3.5 and GPT-4 performance on tasks in the chemistry domain, specifically property prediction and forward reaction prediction.Input uncertainty helps determine the LLM's sensitivity to variations in molecular representations (e.g., alternative SMILES notations), while output uncertainty evaluates the inherent variability in LLM predictions.These techniques allow us to systematically explore how robust and reliable LLMs are in handling different forms of input and producing consistent output.Below, we outline our approach: Our experiments revealed that ChatGPT-4 exhibited a notable sensitivity to Question Rephrasing.We view this sensitivity as providing insight into the input uncertainty of the model.We observed that variations in the input format, such as rephrasing or using alternative SMILES representations, led to differences in the consistency of model responses.For example, in property prediction tasks using chemistry property datasets like BBBP, HIV, and Tox21 [27], we noted changes in model performance metrics such as accuracy and F1 score when the inputs were reformulated.The AUC scores is the Area Under the ROC Curve, and indicates ability of the model to differentiate correct vs. wrong responses, with 1.0 being the means perfect separation (the model always assigns higher confidence/lower uncertainty to correct answers than to incorrect ones).AUC for original SMILES ranged between 0.546 and 0.774, suggesting only moderate uncertainty in predict response correctly.When using reformulated inputs, model performance generally declined, as indicated by decreased accuracy and F1 scores in most datasets.Furthermore, in the forward reaction prediction tasks, GPT-3.5 and GPT-4 performed poorly, with noticeable declines when molecular representation variations were introduced.Although output uncertainty metrics, such as entropy-based measures, provided high AUC scores (ranging from 0.86 to 0.99 indicating better uncertainty awareness), overall accuracy was limited, highlighting the need for improved LLM understanding of chemical knowledge.These findings emphasize that while uncertainty metrics can indicate response reliability, significant improvements are needed to make LLMs reliable in critical scientific applications.</p>
<p>V. SAFETY EVALUATIONS Safe and secure deployment of AI systems in scientific domains is paramount.As LLMs increasingly support critical applications in fields like biosecurity, cybersecurity, and chemistry, ensuring their safety and alignment is also essential to maintaining trustworthiness.Hence, it is critical to integrate into our proposed methodology to evaluate LLMs as research assistants rigorous safety and alignment evaluation techniques.</p>
<p>To this end, we discuss below the CHEMRISK benchmark as one of our efforts in this direction.The CHEMRISK benchmark (Figure 8) addresses a critical need in the era of increasingly powerful large language models (LLMs) such as Claude, chatGPT, and others.As these models become more capable of understanding and generating chemical information, it is essential for organizations like the Department of Energy to have robust, standardized benchmarks to evaluate how models handle potentially sensitive chemical knowledge.CHEMRISK provides a comprehensive framework for assessing LLMs' capabilities across three key domains: chemical understanding, molecular design, and molecular synthesis.The benchmark employs both multiple-choice and free-form questions, using standard molecular representations (SMILES and SELFIES) to ensure broad applicability.</p>
<p>A. CHEMRISK Chemical Risk Detection Benchmark</p>
<p>CHEMRISK is designed as an evolving benchmark, developed in collaboration with domain experts at Lawrence Livermore National Laboratory (LLNL).The benchmark focuses on crystalline density and heat of formation (HoF)-proxy properties that are fundamental to understanding energetic materials.High crystalline density often correlates with increased performance in energetic materials, while heat of formation helps characterize potential energy content and stability.The benchmark encompasses diverse tasks including: property regression for predicting experimental densities, molecular ranking, forward synthesis prediction, retrosynthesis prediction, masked SMILES completion, SMILES correction, and molecular name-to-structure conversion (and vice versa).Base data is curated from established chemical databases, including the Cambridge Structural Database (CSD) and United States Patent and Trademark Office (USPTO), and has undergone manual quality checks, ensuring the benchmark's reliability.By providing a standardized framework for evaluating LLMs' chemical reasoning capabilities for energetics, CHEMRISK aims to foster responsible development of AI systems while helping identify potential safety risks in chemical AI applications.In Table V, we present preliminary results from exploring the GPT-4o and o1 models on a subset of the CHEMRISK benchmark.Specifically, we focus the following multiplechoice Q&amp;A tasks in each category: SMILES-to-Name, property prediction, and forward synthesis.For both models, we utilize the system prompt outlined in Appendix Figure 16.The results indicate that while both GPT-4o and o1 demonstrate perfect accuracy (1.0) in chemical understanding tasks like SMILES-to-Name conversion, o1 outperforms GPT-4o in molecular design challenges, achieving perfect accuracy compared to GPT-4o's 0.4.Both models show identical moderate performance (0.6) in molecular synthesis tasks.CHEMRISK serves as a proof-of-concept proxy benchmark, laying the foundation for continued development and the creation of additional benchmarks for other domains using a similar methodology.</p>
<p>Sample question from CHEMRISK benchmark</p>
<p>Given the molecule with the following SMILES representation: Cc1c(cnc(N)c1N(=O)=O)N(=O)=O, what is the experimentally measured crystalline density of the molecule?A) 1.145 g/cm³ B) 1.364 g/cm³ C) 1.739 g/cm³ D) 1.925 g/cm³</p>
<p>VI. SCALABLE SOFTWARE INFRASTRUCTURE</p>
<p>As previous sections show, a comprehensive evaluation of LLMs as research assistants already requires the execution of many benchmarks for skills and safety assessments.We do not expect the evaluation workload to be reduced in the future.In contrast, as more capable LLMs appear, more research domains will be interested in using them, which will trigger the development of new evaluation benchmarks.This situation places ever-growing demands on software and computational infrastructure.Existing evaluation software platforms, such as HELM [77], EleutherAI's LM Evaluation Harness [78], and DecodingTrust [44], have made significant strides in this area, but exhibit certain limitations that impede comprehensive and scalable evaluations, particularly within high-performance computing (HPC) environments like those at Argonne National Laboratory.A critical shortcoming of current frameworks is their limited scalability and inefficiency in handling large-scale models.Many existing software platforms are not optimized for parallel processing across multiple GPUs or computing nodes, resulting in prolonged evaluation times and increased computational costs.This inefficiency becomes particularly problematic when assessing large LLMs that demand substantial computational resources.In addition, inconsistencies in evaluation methodologies and a lack of standardization further hinder comprehensive evaluations.The absence of consistent benchmarks and metrics across platforms and organizations complicates model comparisons, exacerbated by dataset biases, contamination, and the rapid evolution of LLMs outpacing evaluation strategies.</p>
<p>To address the challenges of scalable and comprehensive LLM evaluation, we are developing the Skills, Trust, and Reliability (STaR) evaluation framework, tailored for HPC systems at Argonne National Laboratory.STaR builds upon the general architecture of evaluation platforms, which typically involve a sequence of specifications (files or configuration flags) to instantiate controllers and manage communication through states.Central to these platforms are Runners, which act as top-level components orchestrating workflows that handle Scenarios-benchmarks comprising static datasets like Hellaswag or GSM8K, or dynamic scripts such as those in Chain-of-Thought Hub [79].A Data Pre-Processor translates these Scenarios into formatted prompts, which are passed to Adapters interfacing with LLMs through libraries such as Hugging Face [80], vLLM [81], or OpenAI APIs.Executors like Slurm [82] or Ray [83] enable processing of prompts, and the results are aggregated into metrics, such as accuracy.</p>
<p>Expanding on this general framework, STaR introduces a modular architecture comprising a data layer, prompting layer, model adapter, and result layer to streamline the evaluation process.The data layer ingests datasets, such as MMLU-Pro [14], and constructs evaluation instances, while the prompting layer generates standardized prompts using techniques such as few-shot and chain-of-thought reasoning [84].The model adapter queries models in multiple modes, including locally loaded instances for smaller models, Parsl [57] for job bundling, and OpenAI-compatible inference backends (e.g., vLLM [81] and DeepSpeed FastGen [85]) for larger models deployed on HPC systems like Polaris and Aurora.The result layer aggregates responses, computes general and UQ metrics, and organizes results into comprehensive scores, providing nuanced insight into model performance.</p>
<p>STaR supports widely used benchmark libraries, including EleutherAI-Harness [78], DecodingTrust [44], Wildbench [37], and domain-specific benchmarks.It also integrates uncertainty quantification approaches [55] to enhance the reliability of evaluations.Designed for scalability, STaR incorporates data-parallel capabilities to distribute workloads across multiple GPUs and model-parallel solutions to handle large models exceeding single-GPU memory limits.It aims  to simplify deployment with a unified one-step installation process and a consistent command-line interface, while results management supports standardized local tracking and optional database integration.STaR is a work in progress, with ongoing refinements aimed at seamless integration with Argonne's infrastructure and addressing limitations identified in existing platforms.By prioritizing scalability, standardization, and efficiency, STaR aims to establish a robust evaluation framework that meets the evolving needs of LLM assessment in scientific research environments.As a proof of concept, we performed evaluations on Polaris [86], and benchmarked them with various open-source models with sizes from 7B to 70B.We used OpenLLM Leaderboard V2 (through Harness), a commonly used benchmark suite consisting of six challenging tasks, to evaluate the performance of Llama-2-7B and Llama-3-8B, Llama-3.1-8B, and their corresponding chat or instruct models, as well as Llama-3.3-70B-Instruct, the most recent and advanced model in the Llama series.Table VI presents the evaluation results for the models on the benchmark, along with the GPU hours required for the evaluations.</p>
<p>For safety evaluations, we use DecodingTrust, which, unlike frameworks such as Harness and HELM, is lightweight, highly compatible with HPC platforms, containerizable.Deploying DecodingTrust on the Polaris HPC system required key modifications, including integrating Parsl for efficient job bundling, adapting the framework to align with Polaris's queue configurations for optimized task distribution, and implementing a unified inference interface.These adaptations allowed the framework to harness Polaris' computational capabilities while retaining its flexibility and commitment to trustworthiness assessments.</p>
<p>On Polaris, DecodingTrust efficiently managed a variety of evaluation tasks, ranging from straightforward classification assessments to computationally intensive open-ended analyses.Classification tasks such as Adversarial Demonstration Robustness, Fairness, and Machine Ethics required minimal computational resources, with each task consuming approximately 0.5 A100 hours.In contrast, open-ended evaluations, including Toxicity and Stereotype Bias, were significantly more resource intensive, especially for models exceeding 70B parameters, with some tasks demanding up to 24 A100 hours per evaluation.By leveraging Polaris's HPC infrastructure, De-codingTrust successfully scaled its evaluation pipeline, balancing lightweight classification tasks with resource-heavy openended evaluations to provide a comprehensive assessment of model trustworthiness.</p>
<p>VII. CONCLUSIONS AND NEXT STEPS</p>
<p>As LLMs continue to expand the notions of what AI can accomplish, there remain two main challenges to address to enable the broad adoption of LLMs by the scientific community as research assistants: a holistic understanding of the capabilities of LLMs and a strong confidence in the results produced by them.To address this, our proposed methodology features four techniques: multiple choice questions, open-response questions, lab-style experiments, and field-style experiments which complement each other to form a comprehensive, rigorous, and realistic assessment of the capabilities of AI systems.Underneath the four approaches are three cross-cutting aspects, including trust and safety, reliable uncertainty quantification, and scalable software infrastructure which support our approach.In addition to proposing the holistic methodology, our team has advanced the state-of-theart in each of the techniques and aspects.</p>
<p>Multiple choice questions are a key technique to evaluate LLMs because of their ability to quickly assess a breadth of knowledge; our team extends beyond existing benchmarks with automatically-generated domain-specific MCQ benchmarks in Astronomy and Climate and the multi-domain AI4S Benchmark with both human and automatically curated with human reviews have revealed significant gaps in knowledge recall and reasoning in LLMs.We find that our new AI4S benchmark is more challenging for LLMs than benchmarks like GPQA.This increased difficulty stems from the AI4S benchmark design, which integrates manual and automatic question generation to create diverse and nuanced questions that assess reasoning and domain-specific knowledge.The Astronomy benchmark also highlighted disparities in performance and cost-efficiency across frontier models, as well as across English and non-English language models.The Climate benchmarks showed that models like GPT-4o struggled both to produce assessments of fine-grained knowledge and questions that vary in style and content.The AI4S Benchmark highlighted significant disparities in performance across models due to its rigorous evaluation across multiple domains.</p>
<p>Open response questions similarly serve an important role, allowing a more detailed but still fast assessment of model performance.Open-ended benchmarks such as SciCode provide realistic and challenging coding problems across fields such as physics, biology, and materials science, rigorously testing model abilities to reason, recall knowledge, and generate accurate code.Similarly, the ALDbench materials science benchmark allowed experts to uncover hallucinations and evaluate responses with precision, generating datasets valuable for further refining evaluation methods.</p>
<p>A key innovation of our approach is to incorporate more realistic end-to-end experiments with lab-style and field-style experiments which more closely reflect the in-depth and iterative problem-solving practiced by scientists.lab-style experiments provided holistic assessments of LLM capabilities in research workflows, including hypothesis generation, analysis, and reporting.For example, experiments revealed variability in performance across models, with GPT-4o requiring five prompts to address a checkpointing problem, while Argo/O1-preview re-solved the problem with just one prompt.Field-style experiments, which analyzed real-world interactions between scientists and LLMs at scale, offered quantitative insights into model strengths and weaknesses as research assistants.These studies identified the remarkable performance of LLMs compared to PhD students and postdocs while struggling to present novel or ground-breaking results.</p>
<p>Together, our combination of domain-specific and multidomain MCQs, open-ended benchmarks, and end-to-end experiments provides a holistic framework for assessing LLMsone that we argue points to a new methodology able not only to quantify current model limitations but also to guide targeted improvements aimed at aligning model capabilities with the nuanced demands of real-world scientific research.</p>
<p>Looking ahead, we are seeking to expand evaluation benchmarks to comprehensively assess LLM capabilities across diverse scientific domains while also incorporating advanced methodologies for trustworthiness, uncertainty quantification (UQ), and iterative evaluation.We anticipate conducting more and refined lab-style experiments to provide yet precise assessments of AI model capabilities for specific research problems, with the goal of both improving scalability and coverage and tracking model progress across generations.We also aim to refine the Field-style experiments method capturing real-world interactions between scientists and LLMs and to leverage feedback to align automated scoring with human judgments through instruct-tuned models.New benchmarks for Retrieval-Augmented Generation (RAG) and multimodal narrative assessments will target domains like biology, weather/climate, and cosmology where Argonne has access to substantial quantities of scientific simulation results and data, utilizing constructs such as aggregation and multihop scenarios to evaluate performance across modalities.We will adopt agent evaluation techniques from frameworks like CACTUS [87] for multi-turn chemistry tasks, and automated red-teaming [88] will enhance safety evaluations by systematically identifying vulnerabilities such as biases and hallucinations.We will also advance trustworthiness and UQ through embeddingbased approaches, including tools inspired by HaloScope [52], to capture subtle input variations and distinguish between truthful and hallucinated outputs.These initiatives will enable evaluations to remain rigorous, scalable, and reflective of realworld scientific challenges.</p>
<p>Evaluating LLMs capabilities as research assistants at scale require a powerful infrastructure.We envision the STaR framework evolving into a scalable, efficient, and user-friendly platform for HPC environments, with a modular architecture that supports dynamic and multimodal evaluations.Scalable inference backends, such as vLLM [81] and DeepSpeed Fast-Gen [85], will enable efficient handling of large benchmarks and models.Collaborations with other national laboratories and NIST will contribute to consistent proxy benchmarks for safety evaluations.By integrating these capabilities, STaR will strive to enable robust, scalable, and reliable assessments of LLMs, fostering impactful applications in scientific discovery while prioritizing safety and computational efficiency.</p>
<p>This paper presents the current state of the effort at Argonne National Laboratory to establish a methodology to evaluate LLMs capabilities as research assistants.We envision this effort as a continuous one because LLMs continue to progress and we will need to increase the difficulty of the different tests as LLMs progress.</p>
<p>VIII. ACKNOWLEGEMENTS</p>
<p>Argo Science Assistant System Prompt</p>
<p>You are an AI language model named Argo that is a highly knowledgeable AI assistant specializing in scientific domains such as physics, chemistry, biology, mathematics, and engineering.Your goal is to provide clear, detailed, and accurate explanations to scientific questions.Use precise terminology, include relevant equations or formulas when necessary, and break down complex concepts into understandable parts.Organize your responses by separating conceptual sections with descriptive sub-header titles to enhance readability.If applicable, cite credible sources or reference landmark studies to support your answers.</p>
<p>Chemical Risk Evaluation System Prompt</p>
<p>You are an expert in synthetic and computational chemistry with extensive knowledge in organic, inorganic, and organometallic chemistry.Your role is to solve complex chemistry problems by providing accurate and detailed insights into synthesis pathways, reaction mechanisms, chemical properties, and safety considerations.You are well-versed in retrosynthesis, modern synthetic methods, and analytical techniques for structure verification for energetics.Additionally, you excel at interpreting chemical databases and computational predictions to propose efficient and feasible synthetic routes.When responding, ensure that your answers: 1) are concise and actionable, 2) comply with any specified constraints (e.g., have specific atoms or substructures), and 3) produce a chemically valid output in SMILES or SELFIES format.</p>
<p>(</p>
<p>A) Stellar companions can dilute transit signals, potentially leading to misclassification of planets and inaccurate parameter estimations.Additionally, their gravitational influence can suppress planet formation in close binary systems.(B) Stellar companions provide additional sources of gravitational perturbations, enhancing planet formation by promoting planetesimal accretion and facilitating the formation of gas giants.(C) Stellar companions contribute to the metallicity enrichment of planetary systems, leading to the formation of more massive and diverse planets, including super-Earths and hot Jupiters.(D) Stellar companions act as gravitational lenses, increasing the detectability of exoplanets through microlensing events and enabling the discovery of planets at greater distances from their host stars.</p>
<p>Fig. 1 .
1
Fig.1.The AGIL approach to generate scalable MCQ benchmarks.The current version of the AI4S benchmark contains only manually accepted MCQs.The AGIL approach enables the integration of automatically accepted MCQs after the validation of their difficulty and quality.</p>
<p>Fig. 3 .
3
Fig. 3. Distribution of the mean scores of GPT-4o responses to all questions in the ALDbench benchmark.</p>
<p>Fig. 5 .
5
Fig. 5.A partial scoring of several models used as AI assistants on August 20, 2024, to solve the zero-overhead checkpointing problem.The results highlight the strengths and weaknesses of different models for the different research steps.We note that the RAG model (Perplexity Pro) has a decisive advantage in several steps for this particular problem.Other models struggle in most steps.</p>
<p>Fig. 7 .
7
Fig. 7. SMILES representation variants of Aspirin.While all structures depict the same molecule, their SMILES representations are different, which introduces input variations.Top left: Canonical SMILES representation of Aspirin.Rest: Five SMILES variations of Aspirin.</p>
<p>Fig. 8 .
8
Fig. 8. CHEMRISK is a chemical risk detection benchmark.</p>
<p>Fig. 9 .
9
Fig. 9. MCQ Authoring Interface with example question.</p>
<p>Fig. 10 .
10
Fig. 10.MCQ Reviewing Interface with example question.</p>
<p>Fig. 11 .
11
Fig. 11.Labstyle experiment collection interface: Problem setup.</p>
<p>Fig. 12 .
12
Fig.12.abstyle experiment collection interface: Prompt, response, assessment.</p>
<p>Fig. 13 .
13
Fig. 13.abstyle experiment collection interface: Final assessment.</p>
<p>Fig. 14 .
14
Fig.14.A SciCode main problem is divided into multiple simpler subproblems for ease of implementation.Docstrings outline the requirements and specify the input-output formats.The scientific background is provided by expert annotators to offer necessary context and guidance.</p>
<p>Fig. 15 .
15
Fig. 15.Argo System Prompt.</p>
<p>Fig. 16 .
16
Fig. 16.ChemRisk System Prompt.</p>
<p>TABLE I OUR
I
PROPOSED METHODOLOGY FOR EVALUATING LLMS AS SCIENTIFIC ASSISTANTS COMBINES FOUR COMPLEMENTARY TECHNIQUES, LISTED IN COLUMNS 2-5 BELOW, TO ASSESS THEIR CAPABILITIES.PURPLE TEXT INDICATES PRIOR CONTRIBUTIONS BY THE AUTHORS, BLUE TEXT NEW CONTRIBUTIONS IN THIS PAPER, AND BLACK TEXT METHODS ADAPTED FROM EXISTING WORK THAT WE INCLUDE FOR A COMPLETE APPROACH.</p>
<p>TABLE II SKILLS
II
EVALUATED BY EACH OF THE EVALUATION TECHNIQUES.LAB-STYLE EXPERIMENTS FOCUS ON DETAILED ANALYSIS IN CONTROLLED ENVIRONMENT.FIELD-STYLE EXPERIMENTS FOCUS ON ANALYZING RESEARCHERS-LLMS INTERACTIONS AT SCALE IN NATURAL SETTING.</p>
<p>TABLE III PERFORMANCE
III
METRICS FOR LLAMA 3 MODELS ACROSS VARIOUS AI4S BENCHMARK LEVELS.
ModelTasknsamplesacc (stderr)acc norm (stderr)Llama-3-8Baccepted254 0.2008 (±0.0252)0.2717 (±0.0280)Llama-3-8Baccepted easy81 0.2222 (±0.0465)0.3210 (±0.0522)Llama-3-8Baccepted medium116 0.1983 (±0.0372)0.2672 (±0.0413)Llama-3-8Baccepted hard57 0.1404 (±0.0464)0.1930 (±0.0527)Llama-3.1-8Baccepted254 0.1969 (±0.0250)0.2638 (±0.0277)Llama-3-70Baccepted254 0.2598 (±0.0276)0.3701 (±0.0304)Llama-3.1-70Baccepted254 0.2520 (±0.0273)0.3386 (±0.0298)Llama 3 8B results are consistent with human-identified diffi-culty levels, with Llama 3 8B achieving the best score on easyMCQs (22%) and significantly below random performance onhard MCQs (14%). These results validate the quality of thelevel classification. The next three rows of Table</p>
<p>TABLE VI HARNESS
VI
EVALUATION RESULTS FOR SEVEN LLAMA VARIANTS ON THE OPENLLM V2 BENCHMARK.
ModelIFEval ↑BBH ↑MATH ↑GPQA ↑ MuSR ↑ MMLU-PRO ↑ GPU HoursLlama-2-7B0.25430.34750.01210.27180.37030.18485.05Llama-2-7B-chat0.35380.36760.01890.27350.40340.20004.93Llama-3-8B0.15360.46000.03170.31460.36770.32487.62Llama-3-8B-Instruct0.48250.48850.08080.30200.38230.35806.50Llama-3.1-8B0.12280.46520.04380.30700.38490.32606.94Llama-3.1-8B-Instruct0.49240.50580.13600.31630.39950.37898.31Llama-3.3-70B-Instruct0.67450.69940.33910.47150.48540.547781.39
This material is based upon work supported by Laboratory Directed Research and Development (LDRD) funding from Argonne National Laboratory, provided by the Director, Office of Science, of the U.S. Department of Energy under Contract No. DEAC02-06CH11357.LLNL work was prepared under Contract DE-AC52-07NA27344, supported by the LLNL-LDRD Program under Project No. 24-ERD-058, and authored by Lawrence Livermore National Security, LLC under Contract No. DE-AC52-07NA27344 with the U.S. Department of Energy.This research used resources of the Argonne Leadership Computing Facility, a U.S. Department of Energy (DOE) Office of Science user facility at Argonne National Laboratory and is based on research supported by the U.S. DOE Office of Science-Advanced Scientific Computing Research Program, under Contract No. DE-AC02-06CH11357.We gratefully acknowledge the computing resources provided on Improv, Bebop, and Swing, high-performance computing clusters operated by the Laboratory Computing Resource Center at Argonne National Laboratory.The United States Government retains, and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes.Other Argonne researchers voluntarily contributing to the benchmarking effort are listed in the Appendix.AI4S Benchmark MCQ LLM-as-a-judge PromptBelow is a multiple-choice question, 1 correct answer, 4 incorrect distractors, the domain or field of study, and required skills to answer the question.Be very discriminating, only provide high scores where they are earned, it is crucial to be critical of errors or inadequacies to improve.Here is the json dictionary formatted multiple choice question, skills and domains: { 'Question': '{}', 'Answer': '{}', 'Distractors': {}, 'Skills': {}, 'Domains': {} } Your job is to evaluate the complete question, answers, skills and domain on the following criteria:1) Appropriate: Assess whether the question's difficulty aligns with graduate-level knowledge and skills in the subject area.Consider complexity of concepts involved, depth of analysis required, sophistication of language used, application of advanced theories or methodologies.Simple recall from a paper is not sufficiently difficult.Rate the question's appropriateness on a scale of 1-5, where 1 is too basic and 5 is suitably challenging for graduate-level students.2) Relevant: Evaluate how closely the answer choices relate to the question posed.Consider direct connection between question and answers, absence of extraneous or off-topic information, alignment with the core concept being tested.Score relevance on a scale of 1-5, where 1 indicates poor relevance and 5 indicates high relevance across all answer choices.3) Complete: Assess whether the answer choices fully address all aspects of the question.Consider coverage of all key elements mentioned in the question, absence of partial or incomplete responses, sufficient detail in each answer choice.There should be one correct answer and four distractors.Rate completeness on a scale of 1-5, where 1 indicates incomplete responses and 5 indicates comprehensive coverage in all answer choices.4) Correct: Verify that there is only one unambiguously correct answer among the choices.Consider clarity and precision of language in both question and answers, absence of partially correct answers, distinctness of the correct answer from distractors.Score this criterion as either Pass (5) (one clear correct answer) or Fail (0) (multiple correct answers or no correct answer).5) Controversial: Determine if the correct answer is generally accepted in the field, avoiding contentious or debatable topics.Consider alignment with current academic consensus, avoidance of ongoing debates or unresolved issues, use of wellestablished facts or theories.Rate the non-controversial nature on a scale of 1-5, where 1 indicates highly controversial and 5 indicates widely accepted, uncontroversial content.6) Mathematic: Check that the question and answers do not rely on arithmetic calculations.Consider absence of numerical computations, focus on conceptual understanding rather than mathematical operations, use of qualitative rather than quantitative reasoning.Score this criterion as either Pass (no arithmetic required)(5)or Fail (arithmetic is necessary to answer) (0).7) Skills: Evaluate whether the skills required to answer the question are appropriate for the subject and level.Consider alignment with course learning objectives, relevance to real-world applications in the field, balance of lower-order (recall) and higherorder (analysis, synthesis) thinking skills.Rate the appropriateness of skills on a scale of 1-5, where 1 indicates misaligned skills and 5 indicates perfectly aligned skills for the subject and level.8) Domains: Assess if the knowledge domains covered by the question are suitable for the subject area.Consider relevance to the course or exam topic, coverage of key subject areas within the field, appropriate breadth and depth of domain knowledge tested.Score the appropriateness of domains on a scale of 1-5, where 1 indicates poorly chosen domains and 5 indicates highly appropriate domains for the subject area.It is important to be extremely discriminating.Only the best possible questions should receive a maximum score.Correct feedback is vital and preferred over erroneous positivity.Provide the scores in a json dictionary formatted object with the following fields:LLM Scientific Reasoning Evaluation PromptYou are tasked with analyzing conversation transcripts between humans and a Large Language Model (LLM) to evaluate the LLM's scientific reasoning capabilities.Your objective is to identify the LLM's strengths and weaknesses in various aspects of scientific thinking, using the following framework as a guide.Provide specific examples from the transcript to support your assessment.If a criteria is not applicable to the problem or question being asked in the transcript, note that it is not applicable.Be critical, do not be overly positive if it is not evidenced.Scientific Reasoning Skills Framework Core Scientific Principles Understanding of the Scientific Method• Observation and Questioning: Does the LLM demonstrate an understanding of how scientific inquiry begins with observation and the formulation of testable questions?Can it identify good vs. poorly formed scientific questions?...Knowledge of Scientific Concepts• Domain Knowledge: Does the LLM possess accurate knowledge of basic scientific concepts in various fields (e.g., biology, chemistry, physics)?How well is it able to answer questions related to different fields of science?...Critical Evaluation of Scientific Information• Source Credibility: Does the LLM demonstrate an ability to assess the credibility of scientific sources?...Specific Scientific Reasoning Skills Experimental Design• Identifying Variables: Can the LLM identify the independent, dependent, and control variables in a given experimental scenario?...Data Analysis and Interpretation• Statistical Significance: Does the LLM understand the concept of statistical significance?...Causal Reasoning• Identifying Cause and Effect: Can the LLM correctly identify cause-and-effect relationships in scientific contexts?...Communication of Scientific Ideas• Clarity and Precision: Does the LLM communicate scientific ideas clearly and precisely?...Scoring FormatThe quantitative assessment should be provided in the following JSON format: { "Core Scientific Principles": { "Understanding of the Scientific Method": { "Observation and Questioning": score, "Hypothesis Formation": score, "Prediction": score, "Experimentation": score, "Data Collection and Analysis": score, "Conclusion and Theory Formation": score }, ... } } Instructions 1) Read the conversation transcript carefully.2) Identify instances where the LLM demonstrates strengths or weaknesses in any of the scientific reasoning skills listed above.3) For each identified instance, provide:• The specific skill being assessed (e.g., Hypothesis Formation, Data Analysis: Correlation vs. Causation)• A brief description of the context in the conversation • Direct quotes from the transcript that exemplify the LLM's performance (both the user's prompt and the LLM's response)• An assessment of whether this represents a strength or weakness, and a brief explanation of your reasoning 4) Assign quantitative scores from 1-10 for the criteria as formatted above, if a criteria is not applicable to the transcript give a score of -1.• A score of -1 means the criteria cannot be assessed as it is not applicable to the transcript • A score of 1 means the LLM completely failed at on the criteria • A score of 10 means the LLM could not have possibly responded better, and completely meets the criteria Transcript [Insert transcript here] Fig.18.LLM as a judge prompt for Field style evaluation.
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Gemini Team, arXiv:2403.055302024Preprint</p>
<p>The Claude 3 model family: Opus, Sonnet, Haiku. Anthropic Team, Papers With Code. 2024</p>
<p>GPT-4 can ace the bar, but it only has a decent chance of passing the CFA exams. L Varanasi, Nov. 2023Here's a list of difficult exams the ChatGPT and GPT-4 have passed</p>
<p>The impact of large language models on scientific discovery: a preliminary study using gpt-4. M R Ai4science, M A Quantum, 2023</p>
<p>MolecularGPT: Open large language model (LLM) for few-shot molecular property prediction. Y Liu, S Ding, S Zhou, W Fan, Q Tan, arXiv:2406.129502024arXiv preprint</p>
<p>Genomic language models: Opportunities and challenges. G Benegas, C Ye, C Albors, J C Li, Y S Song, Trends in Genetics. 2025</p>
<p>AstroMLab 1: Who wins astronomy Jeopardy!?. Y.-S Ting, T D Nguyen, T Ghosal, R Pan, H Arora, Z Sun, T De Haan, N Ramachandra, A Wells, S Madireddy, Astronomy and Computing. 1008932024</p>
<p>Measuring mathematical problem solving with the MATH dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, NeurIPS. 2021</p>
<p>Toolformer: Language models can teach themselves to use tools. T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, E Hambro, L Zettlemoyer, N Cancedda, T Scialom, Advances in Neural Information Processing Systems. 202336551</p>
<p>Towards fully autonomous research powered by LLMs: Case study on simulations. Z Liu, Y Chai, J Li, arXiv:2408.155122024arXiv preprint</p>
<p>P Ma, T.-H Wang, M Guo, Z Sun, J B Tenenbaum, D Rus, C Gan, W Matusik, arXiv:2405.09783LLM and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery. 2024arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, International Conference on Learning Representations. 2021</p>
<p>MMLU-Pro: A more robust and challenging multi-task language understanding benchmark. Y Wang, X Ma, G Zhang, Y Ni, A Chandra, S Guo, W Ren, A Arulraj, X He, Z Jiang, T Li, M Ku, K Wang, A Zhuang, R Fan, X Yue, W Chen, arXiv:2406.015742024Preprint</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>A careful examination of large language model performance on grade school arithmetic. H Zhang, J Da, D Lee, V Robinson, C Wu, W Song, T Zhao, P Raja, D Slack, Q Lyu, arXiv:2405.003322024arXiv preprint</p>
<p>Multiple-choice questions are efficient and robust llm evaluators. Z Zhang, Z Jiang, L Xu, H Hao, R Wang, arXiv:2405.119662024arXiv preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. P Clark, I Cowhey, O Etzioni, T Khot, A Sabharwal, C Schoenick, O Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>Hellaswag: Can a machine really finish your sentence. R Zellers, A Holtzman, Y Bisk, A Farhadi, Y Choi, arXiv:1905.078302019arXiv preprint</p>
<p>MoleculeQA: A dataset to evaluate factual accuracy in molecular comprehension. X Lu, H Cao, Z Liu, S Bai, L Chen, Y Yao, H.-T Zheng, Y Li, arXiv:2403.08192Mar. 2024</p>
<p>Can LLMs solve molecule puzzles? A multimodal benchmark for molecular structure elucidation. K Guo, B Nan, Y Zhou, T Guo, Z Guo, M Surve, Z Liang, N V Chawla, O Wiest, X Zhang, Nov. 2024</p>
<p>Are large language models superhuman chemists?. A Mirza, N Alampara, S Kunchapu, B Emoekabu, A Krishnan, M Wilhelmi, M Okereke, J Eberhardt, A M Elahi, M Greiner, C T Holick, T Gupta, M Asgari, C Glaubitz, L C Klepsch, Y Köster, J Meyer, S Miret, T Hoffmann, F A Kreth, M Ringleb, N Roesner, U S Schubert, L M Stafast, D Wonanke, M Pieler, P Schwaller, K M Jablonka, 2024</p>
<p>The narrativeqa reading comprehension challenge. T Kočiskỳ, J Schwarz, P Blunsom, C Dyer, K M Hermann, G Melis, E Grefenstette, Transactions of the Association for Computational Linguistics. 62018</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Z Yang, P Qi, S Zhang, Y Bengio, W Cohen, R Salakhutdinov, C D Manning, Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics2018</p>
<p>HybridQA: A dataset of multi-hop question answering over tabular and textual data. W Chen, H Zha, Z Chen, W Xiong, H Wang, W Y Wang, 2020. 2020Findings of the Association for Computational Linguistics</p>
<p>ChemistryQA: A complex question answering dataset from chemistry. Z Wei, W Ji, X Geng, Y Chen, B Chen, T Qin, D Jiang, Oct. 2020</p>
<p>What can large language models do in chemistry? a comprehensive benchmark on eight tasks. T Guo, B Nan, Z Liang, Z Guo, N Chawla, O Wiest, X Zhang, Advances in Neural Information Processing Systems. 202336</p>
<p>TOMG-Bench: Evaluating LLMs on text-based open molecule generation. J Li, J Li, Y Liu, D Zhou, Q Li, arXiv:2412.14642Dec. 2024</p>
<p>Open llm leaderboard. E Beeching, C Fourrier, N Habib, S Han, N Lambert, N Rajani, O Sanseviero, L Tunstall, T Wolf, 2023</p>
<p>FrontierMath: A benchmark for evaluating advanced mathematical reasoning in AI. E Glazer, E Erdil, T Besiroglu, D Chicharro, E Chen, A Gunning, C F Olsson, J.-S Denain, A Ho, E De Oliveira Santos, O Järviniemi, M Barnett, R Sandler, M Vrzala, J Sevilla, Q Ren, E Pratt, L Levine, G Barkley, N Stewart, B Grechuk, T Grechuk, S V Enugandla, M Wildon, 2024</p>
<p>Humanity's last exam. A I Center For, Safety, A I Scale, 2024</p>
<p>Evostore: Towards scalable storage of evolving learning models. R Underwood, M Madhyastha, R Burns, B Nicolae, HPDC'24: The 33nd International Symposium on High-Performance Parallel and Distributed Computing. Pisa, Italy2024</p>
<p>Building the i (interoperability) of fair for performance reproducibility of large-scale composable workflows in recup. B Nicolae, T Islam, R Ross, H V Dam, K Assogba, P Shpilker, M Titov, M Turilli, T Wang, O Kilic, S Jha, L Pouchard, REWORDS'23: The 3rd Workshop on Reproducible Workflows, Data Management, and Security (with eScience'23). Limassol, Cyprus2023</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 62479922023</p>
<p>Autotamp: Autoregressive task and motion planning with llms as translators and checkers. Y Chen, J Arkin, C Dawson, Y Zhang, N Roy, C Fan, 2024 IEEE International conference on robotics and automation (ICRA). IEEE2024</p>
<p>Reproducing and extending experiments in behavioral strategy with large language models. D Albert, S Billinger, arXiv:2410.069322024arXiv preprint</p>
<p>WildBench: Benchmarking LLMs with challenging tasks from real users in the wild. B Y Lin, Y Deng, K Chandu, F Brahman, A Ravichander, V Pyatkin, N Dziri, R L Bras, Y Choi, arXiv:2406.047702024Preprint</p>
<p>HaluEval-Wild: Evaluating hallucinations of language models in the wild. Z Zhu, Y Yang, Z Sun, 2024</p>
<p>Do Anything Now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. X Shen, Z Chen, M Backes, Y Shen, Y Zhang, 2024</p>
<p>Y Zeng, Y Yang, A Zhou, J Z Tan, Y Tu, Y Mai, K Klyman, M Pan, R Jia, D Song, arXiv:2407.17436AIR-Bench 2024: A safety benchmark based on risk categories from regulations and policies. 2024arXiv preprint</p>
<p>Safetybench: Evaluating the safety of large language models with multiple choice questions. Z Zhang, L Lei, L Wu, R Sun, Y Huang, C Long, X Liu, X Lei, J Tang, M Huang, arXiv:2309.070452023arXiv preprint</p>
<p>Salad-bench: A hierarchical and comprehensive safety benchmark for large language models. L Li, B Dong, R Wang, X Hu, W Zuo, D Lin, Y Qiao, J Shao, arXiv:2402.050442024arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>DecodingTrust: A comprehensive assessment of trustworthiness in GPT models. B Wang, W Chen, H Pei, C Xie, M Kang, C Zhang, C Xu, Z Xiong, R Dutta, R Schaeffer, Advances in Neural Information Processing Systems. 202436</p>
<p>Position: Trustllm: Trustworthiness in large language models. Y Huang, L Sun, H Wang, S Wu, Q Zhang, Y Li, C Gao, Y Huang, W Lyu, Y Zhang, International Conference on Machine Learning. PMLR2024270</p>
<p>Red teaming for large language models at scale: Tackling hallucinations on mathematics tasks. A Buszydlik, K Dobiczek, M T Okoń, K Skublicki, P Lippmann, J Yang, arXiv:2401.002902023arXiv preprint</p>
<p>Curiosity-driven redteaming for large language models. Z.-W Hong, I Shenfeld, T.-H Wang, Y.-S Chuang, A Pareja, J R Glass, A Srivastava, P , The Twelfth International Conference on Learning Representations. 2024</p>
<p>X Zhang, Y Xie, J Huang, J Ma, Z Pan, Q Liu, Z Xiong, T Ergen, D Shim, H Lee, arXiv:2406.06357MASSW: A new dataset and benchmark tasks for AI-assisted scientific workflows. 2024arXiv preprint</p>
<p>Accelerating materials discovery using artificial intelligence, high performance computing and robotics. E O Pyzer-Knapp, J W Pitera, P W J Staar, S Takeda, T Laino, D P Sanders, J Sexton, J R Smith, A Curioni, npj Computational Materials. 8184Apr 2022</p>
<p>WildBench: Benchmarking LLMs with challenging tasks from real users in the wild. B Y Lin, Y Deng, K Chandu, F Brahman, A Ravichander, V Pyatkin, N Dziri, R L Bras, Y Choi, abs/2406.04770ArXiv. 2703577712024</p>
<p>Generating with confidence: Uncertainty quantification for black-box large language models. Z Lin, S Trivedi, J Sun, Transactions on Machine Learning Research. 2023</p>
<p>HaloScope: Harnessing unlabeled LLM generations for hallucination detection. X Du, C Xiao, Y Li, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models. J Duan, H Cheng, S Wang, A Zavalny, C Wang, R Xu, B Kailkhura, K Xu, 62nd Annual Meeting of the Association for Computational Linguistics. 2024</p>
<p>Benchmarking LLMs via uncertainty quantification. F Ye, M Yang, J Pang, L Wang, D F Wong, E Yilmaz, S Shi, Z Tu, arXiv:2401.127942024arXiv preprint</p>
<p>Quantifying uncertainty in large language models: Applications in molecular chemistry tasks. Z Chen, P Hong, S Madireddy, NeurIPS 2024 Workshop on Statistical Foundations of LLMs and Foundation Models. 2024</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774GPT-4 technical report. 2023arXiv preprint</p>
<p>Parsl: Pervasive parallel programming in Python. Y Babuji, A Woodard, Z Li, D S Katz, B Clifford, R Kumar, L Lacinski, R Chard, J M Wozniak, I Foster, 28th International Symposium on High-Performance Parallel and Distributed Computing. 2019</p>
<p>Efficient Memory Management for Large Language Model Serving with PagedAttention. W Kwon, Z Li, S Zhuang, Y Sheng, L Zheng, C H Yu, J Gonzalez, H Zhang, I Stoica, 29th Symposium on Operating Systems Principles. Koblenz GermanyACMOct. 2023</p>
<p>Climatex: Do llms accurately assess human expert confidence in climate statements. R Lacombe, K Wu, E Dilworth, 2023</p>
<p>Towards fine-grained classification of climate change related social media text. R Vaid, K Pant, M Shrivastava, 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop. Dublin, IrelandAssociation for Computational LinguisticsMay 2022</p>
<p>Climategpt: Towards ai synthesizing interdisciplinary research on climate change. D Thulke, Y Gao, P Pelser, R Brune, R Jalota, F Fok, M Ramos, I Van Wyk, A Nasir, H Goldstein, T Tragemann, K Nguyen, A Fowler, A Stanco, J Gabriel, J Taylor, D Moro, E Tsymbalov, J De Waal, E Matusov, M Yaghi, M Shihadah, H Ney, C Dugast, J Dotan, D Erasmus, 2024</p>
<p>S Solomon, D Qin, M Manning, Z Chen, M Marquis, K Averyt, M Tignor, H Miller, IPCC fourth assessment report (AR4). 2007374</p>
<p>GPQA: A graduate-level Google-proof Q&amp;A benchmark. D Rein, B L Hou, A C Stickland, J Petty, R Y Pang, J Dirani, J Michael, S R Bowman, arXiv:2311.120222023Preprint</p>
<p>A careful examination of large language model performance on grade school arithmetic. H Zhang, J Da, D Lee, V Robinson, C Wu, W Song, T Zhao, P Raja, D Slack, Q Lyu, S Hendryx, R Kaplan, M Lunati, S Yue, arxiv 2405.003322024</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, 2021</p>
<p>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, 2023</p>
<p>CheckEmbed: Effective verification of LLM solutions to open-ended tasks. M Besta, L Paleari, A Kubicek, P Nyczyk, R Gerstenberger, P Iff, T Lehmann, H Niewiadomski, T Hoefler, arXiv:2406.025242024arXiv preprint</p>
<p>Scicode: A research coding benchmark curated by scientists. M Tian, L Gao, D Zhang, X Chen, C Fan, X Guo, R Haas, P Ji, K Krongchon, Y Li, S Liu, D Luo, Y Ma, H Tong, K Trinh, C Tian, Z Wang, B Wu, S Yin, M Zhu, K Lieret, Y Lu, G Liu, Y Du, T Tao, O Press, J Callan, E A Huerta, H Peng, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024</p>
<p>Benchmarking large language models for materials synthesis: The case of atomic layer deposition. A Yanguas-Gil, M T Dearing, J W Elam, J C Jones, S Kim, A Mohammad, C T Nguyen, B Sengupta, 2024</p>
<p>Characterizing the field of atomic layer deposition: Authors, topics, and collaborations. E Alvaro, A Yanguas-Gil, 10.1371/journal.pone.0189137PLOS ONE. 13101 2018</p>
<p>DataStates-LLM: Lazy asynchronous checkpointing for large language models. A Maurya, R Underwood, M M Rafique, F Cappello, B Nicolae, 33rd International Symposium on High-Performance Parallel and Distributed Computing, ser. HPDC '24. New York, NY, USAAssociation for Computing Machinery2024</p>
<p>. Online, 10.1145/3625549.3658685</p>
<p>Investigating interaction modes and user agency in human-LLM collaboration for domain-specific data analysis. J Guo, V Mohanty, J H Piazentin Ono, H Hao, L Gou, L Ren, 10.1145/3613905.3651042Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, ser. CHI '24. ACMMay 2024</p>
<p>Evaluation and mitigation of the limitations of large language models in clinical decision-making. P Hager, F Jungmann, R Holland, K Bhagat, I Hubrecht, M Knauer, J Vielhauer, M Makowski, R Braren, G Kaissis, D Rueckert, Nature Medicine. 302024</p>
<p>Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. Y Gal, Z Ghahramani, 2016</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. L Kuhn, Y Gal, S Farquhar, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Can LLMs express their uncertainty? An empirical evaluation of confidence elicitation in LLMs. M Xiong, Z Hu, X Lu, Y Li, J Fu, J He, B Hooi, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Holistic evaluation of language models. R Bommasani, P Liang, T Lee, Annals of the New York Academy of Sciences. 152512023</p>
<p>A framework for few-shot language model evaluation. L Gao, J Tow, B Abbasi, S Biderman, S Black, A Dipofi, C Foster, L Golding, J Hsu, A Le Noac'h, H Li, K Mcdonell, N Muennighoff, C Ociepa, J Phang, L Reynolds, H Schoelkopf, A Skowron, L Sutawika, E Tang, A Thite, B Wang, K Wang, A Zou, 2024</p>
<p>Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. Y Fu, L Ou, M Chen, Y Wan, H Peng, T Khot, arXiv:2305.173062023arXiv preprint</p>
<p>HuggingFace's transformers: State-of-theart natural language processing. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, J Davison, S Shleifer, P Von Platen, C Ma, Y Jernite, J Plu, C Xu, T L Scao, S Gugger, M Drame, Q Lhoest, A M Rush, arXiv:1910.037712020Preprint</p>
<p>Efficient memory management for large language model serving with PagedAttention. W Kwon, Z Li, S Zhuang, Y Sheng, L Zheng, C H Yu, J Gonzalez, H Zhang, I Stoica, 29th Symposium on Operating Systems Principles. 2023</p>
<p>Slurm: Simple Linux utility for resource management. A B Yoo, M A Jette, M Grondona, Workshop on job scheduling strategies for parallel processing. Springer2003</p>
<p>Ray: A distributed framework for emerging AI applications. P Moritz, R Nishihara, S Wang, A Tumanov, R Liaw, E Liang, M Elibol, Z Yang, W Paul, M I Jordan, 13th USENIX symposium on operating systems design and implementation (OSDI 18). 2018</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>DeepSpeed-FastGen: High-throughput text generation for LLMs via MII and DeepSpeed-Inference. C Holmes, M Tanaka, M Wyatt, A A Awan, J Rasley, S Rajbhandari, R Y Aminabadi, H Qin, A Bakhtiari, L Kurilenko, arXiv:2401.086712024Preprint</p>
<p>Polaris supercomputer. Argonne , 2025</p>
<p>CACTUS: Chemistry agent connecting toolusage to science. A D Mcnaughton, G Ramalaxmi, A Kruel, C R Knutson, R A Varikoti, N Kumar, arXiv:2405.009722024Preprint</p>
<p>Comprehensive multi-stage evaluation of language models for scientific skill and safety red-teaming. S Madireddy, C Xu, F Cappello, S Bhattacharya, B Kailkhura, M Foltin, T Kumar, B Li, Accelerating the Development and Use of Generative AI for Science and Engineering: The Trillion Parameter Consortium (TPC) Workshop at The International Conference for High Performance Computing, Networking, Storage, and Analysis (SC24). 2024</p>
<p>Adelina Grindeanu, Aikaterini Vriza, Akshay Dave, Alec Sandy, Aleksandr Obabko, Alex Lavens, Alex Rodriguez, Alfonso Napoles, Allison Bennett-Iron, Alrhman Abed, Amaka Okafor, Andreas Wilke, Andrei Patapenka, Andrew Siegel, Andrey Yakovenko, Annabelle Boots, Arvind Ramanathan, Ayesha Shafiuddin, Ayman Moawad, Azton Wells, Barnali Chowdry, Becca Weinberg, Ben Blakely, Bipul Barua, Brahim Mustapha, Brandon Sforzo, Brian Ingram, Chandrachur Phatak, Changyong Bhattacharya, Cheng Park, Chengjun Wang, Chiara Sun, Chihpin Bissolotti, Christopher Chuang, Clinton Henry, Dan Cohagan, David Meyer, Debora Neto, Devesh Meira, Dion Reddy, Doga Antonopoulos, Donald Gursoy, Eliu Walko, Emily Huerta, Emily Dietrich, Eshan Ohland, Fang Sharma, Fangfang Zhang, Fanny Xia, Felipe Wang Rodolakis, Feng Liu, Filippo Qiu, Francesco Simini, Frank Salucci, Gautham Alexander, Gosia Dharuman, Greg Korbas, Gyorgy Morin, Hairong Babnigg, Hanu Shang, Haoran Arava, Hassam Wu, Huihuo Harb, Ian Zheng, Jakob Cloet, Elias, O' James, Sullivan, Jeffrey Emberson, Jesse Wang, Jim Smith, Jiwen Grudzinski, John Fan, John David Carwardine, John Carter, Jonghwan Hutchinson, Jorge Pulpeiro Kwon, Josh Gonzalez, Juanjuan Hlavenka, Julie Huang, Junjing Parente, Justin Deng, Justin Hoffman, Kamlesh Wozniak, Lisa Suthar ; Lee Zachos, Longwen Childers, Lorenzo Ou, Mark Nocivelli, Matthew Hereld, Matthew Dearing, Matthew Diamond, Maulik Sampson, Max Shukla, Meaghan Delferro, Megan Bruening, Mei Clifford, Meltem Zhi-Gang, Demirtas, Acknowledgment The following Argonne researchers voluntarily contributed to the creation of the AI4S benchmarks and the different Lab-style experiments and Field-style experiments. Nicola Ferrier, Nidhi Gupta, Nina Andrejevic, Nithin Manne, Noah Paulson, Olaf Borkiewicz, Olga Antipova, Osama Mohsen, Pamela Weisenhorn, Parfait Gasana, Peter Kenesei, Philip Dinemis, Philippe Piot, Pinaki Pal, Prakash Thimmapuram, Priyash Misra, Qiaomu Yang, Quenten Proussard, Rae Sharp-Geiger, Rajeev Surendran Assary, Rajeev Thakur, Rajkumar Kettimuthu, Rao Kotamarthi, Ravi Madduri, Nicholas Frontiere, Nicholas Lee-Ping Chia, Nick Goberville, Nick Moore; Ryan Aydelott, Ryan Chu, Sam Wheeler; Scott Collis, Scott Parent; Shilpika, Shivam Barwey, ShubhamSang-il Yim. Sejal Rhodes Seth Ockerman Shelly Kelly</p>            </div>
        </div>

    </div>
</body>
</html>
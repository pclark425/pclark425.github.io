<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1354 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1354</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1354</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7" target="_blank">Modular Multitask Reinforcement Learning with Policy Sketches</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> Experiments show that using the approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.</p>
                <p><strong>Paper Abstract:</strong> We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them—specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor-critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level sub-goals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1354.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1354.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MazeEnv</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maze navigation environment (light world style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discrete room-based navigation environment in which the agent must traverse a sequence of rooms to reach a goal; some doors require keys to open and sketches specify a sequence of directions (high-level actions) that form a viable traversal from start to goal.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Maze environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Discrete 2-D room navigation (based on the 'light world'): series of rooms connected by doors; agent has sensors reporting distance to keys, closed doors, and open doors; tasks correspond to reaching a goal room via a sequence of intermediate rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Some doors require the agent to first pick up a key to open them (conditional-access edges).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Sparse room graph with door-connections; connectivity varies per instance and can admit multiple viable traversals between start and goal (not necessarily unique shortest path).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>10 rooms listed in appendix (room 1..room 10); specific connectivity per task not exhaustively enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Modular sketch-guided agent (plus baselines: Independent, Joint, Option-Critic)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Each high-level symbol maps to a neural subpolicy (feedforward net with 128 hidden units, ReLU), which outputs low-level actions or a STOP action; task policy is concatenation of subpolicies per sketch; training uses a decoupled actor-critic with one critic per task and curriculum learning.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Sample efficiency / episodes-to-convergence and average episodic reward (task completion rate used as performance metric); curriculum sampling prioritizes tasks with lower current expected reward.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical/modular sketch-guided policy (reusable subpolicies concatenated per sketch) performs best</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Tasks that require longer sequences of high-level actions (longer sketches) are substantially harder under flat policies because rewards are sparse; conditional-access edges (locked doors requiring keys) create exploration bottlenecks that favor hierarchical modular policies which decompose subgoals and can be learned/reused across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Concatenated modular subpolicies (one per sketch symbol) outperform flat/joint policies and unsupervised options on these room-navigation tasks; modular structure allows zero-shot composition for held-out traversals and better sample efficiency when the task requires multi-step high-level sequences or conditional access (keys/doors).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modular Multitask Reinforcement Learning with Policy Sketches', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1354.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1354.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CraftingNav</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Crafting environment (Minecraft-inspired navigation + manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discrete 2-D Minecraft-inspired world where navigation to resources and crafting stations is constrained by accessibility (e.g., water requiring a bridge); agents must collect raw materials and sometimes build intermediate tools/bridges to reach otherwise inaccessible areas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Crafting environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Grid-based discrete environment with raw materials and crafting stations; the agent collects items via USE actions and may need to craft intermediate objects (e.g., bridge, tools) to alter connectivity of the world and access materials.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Conditional access implemented via environment-modifying actions (e.g., must craft a bridge or tool to cross or access regions), i.e., edges that become traversable only after specific subtask completion.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Structured and dynamic: connectivity depends on whether environment-modifying objects (bridges, tools) have been created; initially some regions are inaccessible (effectively missing edges) until subpolicies produce the enabling objects.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Modular sketch-guided agent (primary); baselines: Joint, Independent, Option-Critic, Q-automaton</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same modular subpolicy architecture as above (neural subpolicies per symbol); sketches specify sequences of subtasks like 'get wood', 'use workbench'; subpolicies are learned by tying parameters across tasks and trained with a per-task critic and curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Sample efficiency (episodes to converge), average episodic reward, task completion rate; curriculum uses task length and 1 - estimated reward to prioritize examples.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Table 1 reports task completion rates for crafting domain after convergence (e.g., Modular: 0.89 multitask, Joint: 0.49, Independent: 0.44, Option-Critic: 0.47) — these are task completion rates averaged across tasks in the crafting experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical modular policies guided by sketches (reusable semantic subpolicies) — these enable faster learning and higher completion rates in sparse, multi-stage crafting/navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Connectivity constraints that require creation of intermediate objects (bridges/tools) induce strong sparsity in reward and exploration difficulty: tasks that require more sequential subgoals (longer sketches) are much harder for flat policies and benefit strongly from modular, sketch-guided decomposition; curriculum learning and a decoupled per-task critic materially improve learning in these topology-conditioned tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Sketch-guided modular subpolicies induce reusable behaviors that can be recombined for zero-shot generalization to held-out composite navigation/crafting tasks; flat or unsupervised option learners struggle because they cannot exploit the high-level structure that controls connectivity (e.g., bridging) and so fail to find sparse terminal rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modular Multitask Reinforcement Learning with Policy Sketches', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1354.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1354.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CliffEnv</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cliff traversal environment (continuous high-dimensional locomotion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous-control navigation task where a quadrupedal robot must traverse a narrow, winding path of variable length without falling off; agent must learn low-level locomotion before making progress.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Cliff environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>3-D continuous locomotion on variable-length winding path: the agent controls joint angles of a quadruped and must navigate to an end tile, receiving small rewards for progress and large reward for reaching the goal, negative reward for falling off.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Effectively a narrow linear/chain-like traversal (path topology), not a richly connected graph; connectivity is constrained to the path and falls are terminal failure states.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Variable-length path; appendix lists 24 path sketches (path 0..path 23) used as task variants, but environment is continuous and not described as a discrete graph with node/edge counts.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Modular sketch-guided agent (same architecture) and baselines</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Modular subpolicies (neural networks) each implementing high-level actions; low-level control is continuous joint-angle actuations learned via on-policy actor-critic with per-task critics and curriculum; subpolicies must learn locomotion primitives before being composed.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Sample efficiency (episodes until convergence), average episodic reward; small per-timestep progress reward and large terminal reward for success used to shape learning.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical policies that first learn low-level locomotion primitives (subpolicies) and then compose them via sketches perform best; flat policies struggle because they must simultaneously discover locomotion and long-horizon composition under sparse reward.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Because the environment effectively imposes a long single-path topology with catastrophic failure (falling off), the main difficulty is learning robust low-level control; modular policies that separate locomotion primitives from high-level sequencing achieve better learning and adaptation to longer/harder paths.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Successful policies factor learning of low-level continuous locomotion (subpolicies) from high-level sequencing; curriculum and modular tying of subpolicies allow re-use of locomotion behaviors across longer paths and generalization to longer/harder instances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modular Multitask Reinforcement Learning with Policy Sketches', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Building portable options: Skill transfer in reinforcement learning <em>(Rating: 2)</em></li>
                <li>Reinforcement learning with hierarchies of machines <em>(Rating: 2)</em></li>
                <li>The option-critic architecture <em>(Rating: 2)</em></li>
                <li>Reinforcement learning for mapping instructions to actions <em>(Rating: 1)</em></li>
                <li>Learning to follow navigational directions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1354",
    "paper_id": "paper-3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "MazeEnv",
            "name_full": "Maze navigation environment (light world style)",
            "brief_description": "A discrete room-based navigation environment in which the agent must traverse a sequence of rooms to reach a goal; some doors require keys to open and sketches specify a sequence of directions (high-level actions) that form a viable traversal from start to goal.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Maze environment",
            "environment_description": "Discrete 2-D room navigation (based on the 'light world'): series of rooms connected by doors; agent has sensors reporting distance to keys, closed doors, and open doors; tasks correspond to reaching a goal room via a sequence of intermediate rooms.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Some doors require the agent to first pick up a key to open them (conditional-access edges).",
            "graph_connectivity": "Sparse room graph with door-connections; connectivity varies per instance and can admit multiple viable traversals between start and goal (not necessarily unique shortest path).",
            "environment_size": "10 rooms listed in appendix (room 1..room 10); specific connectivity per task not exhaustively enumerated in text.",
            "agent_name": "Modular sketch-guided agent (plus baselines: Independent, Joint, Option-Critic)",
            "agent_description": "Each high-level symbol maps to a neural subpolicy (feedforward net with 128 hidden units, ReLU), which outputs low-level actions or a STOP action; task policy is concatenation of subpolicies per sketch; training uses a decoupled actor-critic with one critic per task and curriculum learning.",
            "exploration_efficiency_metric": "Sample efficiency / episodes-to-convergence and average episodic reward (task completion rate used as performance metric); curriculum sampling prioritizes tasks with lower current expected reward.",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Hierarchical/modular sketch-guided policy (reusable subpolicies concatenated per sketch) performs best",
            "topology_performance_relationship": "Tasks that require longer sequences of high-level actions (longer sketches) are substantially harder under flat policies because rewards are sparse; conditional-access edges (locked doors requiring keys) create exploration bottlenecks that favor hierarchical modular policies which decompose subgoals and can be learned/reused across tasks.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Concatenated modular subpolicies (one per sketch symbol) outperform flat/joint policies and unsupervised options on these room-navigation tasks; modular structure allows zero-shot composition for held-out traversals and better sample efficiency when the task requires multi-step high-level sequences or conditional access (keys/doors).",
            "uuid": "e1354.0",
            "source_info": {
                "paper_title": "Modular Multitask Reinforcement Learning with Policy Sketches",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "CraftingNav",
            "name_full": "Crafting environment (Minecraft-inspired navigation + manipulation)",
            "brief_description": "A discrete 2-D Minecraft-inspired world where navigation to resources and crafting stations is constrained by accessibility (e.g., water requiring a bridge); agents must collect raw materials and sometimes build intermediate tools/bridges to reach otherwise inaccessible areas.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Crafting environment",
            "environment_description": "Grid-based discrete environment with raw materials and crafting stations; the agent collects items via USE actions and may need to craft intermediate objects (e.g., bridge, tools) to alter connectivity of the world and access materials.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Conditional access implemented via environment-modifying actions (e.g., must craft a bridge or tool to cross or access regions), i.e., edges that become traversable only after specific subtask completion.",
            "graph_connectivity": "Structured and dynamic: connectivity depends on whether environment-modifying objects (bridges, tools) have been created; initially some regions are inaccessible (effectively missing edges) until subpolicies produce the enabling objects.",
            "environment_size": null,
            "agent_name": "Modular sketch-guided agent (primary); baselines: Joint, Independent, Option-Critic, Q-automaton",
            "agent_description": "Same modular subpolicy architecture as above (neural subpolicies per symbol); sketches specify sequences of subtasks like 'get wood', 'use workbench'; subpolicies are learned by tying parameters across tasks and trained with a per-task critic and curriculum.",
            "exploration_efficiency_metric": "Sample efficiency (episodes to converge), average episodic reward, task completion rate; curriculum uses task length and 1 - estimated reward to prioritize examples.",
            "exploration_efficiency_value": null,
            "success_rate": "Table 1 reports task completion rates for crafting domain after convergence (e.g., Modular: 0.89 multitask, Joint: 0.49, Independent: 0.44, Option-Critic: 0.47) — these are task completion rates averaged across tasks in the crafting experiments.",
            "optimal_policy_type": "Hierarchical modular policies guided by sketches (reusable semantic subpolicies) — these enable faster learning and higher completion rates in sparse, multi-stage crafting/navigation tasks.",
            "topology_performance_relationship": "Connectivity constraints that require creation of intermediate objects (bridges/tools) induce strong sparsity in reward and exploration difficulty: tasks that require more sequential subgoals (longer sketches) are much harder for flat policies and benefit strongly from modular, sketch-guided decomposition; curriculum learning and a decoupled per-task critic materially improve learning in these topology-conditioned tasks.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Sketch-guided modular subpolicies induce reusable behaviors that can be recombined for zero-shot generalization to held-out composite navigation/crafting tasks; flat or unsupervised option learners struggle because they cannot exploit the high-level structure that controls connectivity (e.g., bridging) and so fail to find sparse terminal rewards.",
            "uuid": "e1354.1",
            "source_info": {
                "paper_title": "Modular Multitask Reinforcement Learning with Policy Sketches",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "CliffEnv",
            "name_full": "Cliff traversal environment (continuous high-dimensional locomotion)",
            "brief_description": "A continuous-control navigation task where a quadrupedal robot must traverse a narrow, winding path of variable length without falling off; agent must learn low-level locomotion before making progress.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Cliff environment",
            "environment_description": "3-D continuous locomotion on variable-length winding path: the agent controls joint angles of a quadruped and must navigate to an end tile, receiving small rewards for progress and large reward for reaching the goal, negative reward for falling off.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": null,
            "graph_connectivity": "Effectively a narrow linear/chain-like traversal (path topology), not a richly connected graph; connectivity is constrained to the path and falls are terminal failure states.",
            "environment_size": "Variable-length path; appendix lists 24 path sketches (path 0..path 23) used as task variants, but environment is continuous and not described as a discrete graph with node/edge counts.",
            "agent_name": "Modular sketch-guided agent (same architecture) and baselines",
            "agent_description": "Modular subpolicies (neural networks) each implementing high-level actions; low-level control is continuous joint-angle actuations learned via on-policy actor-critic with per-task critics and curriculum; subpolicies must learn locomotion primitives before being composed.",
            "exploration_efficiency_metric": "Sample efficiency (episodes until convergence), average episodic reward; small per-timestep progress reward and large terminal reward for success used to shape learning.",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Hierarchical policies that first learn low-level locomotion primitives (subpolicies) and then compose them via sketches perform best; flat policies struggle because they must simultaneously discover locomotion and long-horizon composition under sparse reward.",
            "topology_performance_relationship": "Because the environment effectively imposes a long single-path topology with catastrophic failure (falling off), the main difficulty is learning robust low-level control; modular policies that separate locomotion primitives from high-level sequencing achieve better learning and adaptation to longer/harder paths.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Successful policies factor learning of low-level continuous locomotion (subpolicies) from high-level sequencing; curriculum and modular tying of subpolicies allow re-use of locomotion behaviors across longer paths and generalization to longer/harder instances.",
            "uuid": "e1354.2",
            "source_info": {
                "paper_title": "Modular Multitask Reinforcement Learning with Policy Sketches",
                "publication_date_yy_mm": "2016-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Building portable options: Skill transfer in reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Reinforcement learning with hierarchies of machines",
            "rating": 2
        },
        {
            "paper_title": "The option-critic architecture",
            "rating": 2
        },
        {
            "paper_title": "Reinforcement learning for mapping instructions to actions",
            "rating": 1
        },
        {
            "paper_title": "Learning to follow navigational directions",
            "rating": 1
        }
    ],
    "cost": 0.01154125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Modular Multitask Reinforcement Learning with Policy Sketches</h1>
<p>Jacob Andreas ${ }^{1}$ Dan Klein ${ }^{1}$ Sergey Levine ${ }^{1}$</p>
<h4>Abstract</h4>
<p>We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them-specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor-critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.</p>
<h2>1. Introduction</h2>
<p>This paper describes a framework for learning composable deep subpolicies in a multitask setting, guided only by abstract sketches of high-level behavior. General reinforcement learning algorithms allow agents to solve tasks in complex environments. But tasks featuring extremely</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Learning from policy sketches. The figure shows simplified versions of two tasks (make planks and make sticks, each associated with its own policy ( $\Pi_{1}$ and $\Pi_{2}$ respectively). These policies share an initial high-level action $b_{1}$ : both require the agent to get wood before taking it to an appropriate crafting station. Even without prior information about how the associated behavior $\pi_{1}$ should be implemented, knowing that the agent should initially follow the same subpolicy in both tasks is enough to learn a reusable representation of their shared structure.
delayed rewards or other long-term structure are often difficult to solve with flat, monolithic policies, and a long line of prior work has studied methods for learning hierarchical policy representations (Sutton et al., 1999; Dietterich, 2000; Konidaris \&amp; Barto, 2007; Hauser et al., 2008). While unsupervised discovery of these hierarchies is possible (Daniel et al., 2012; Bacon \&amp; Precup, 2015), practical approaches often require detailed supervision in the form of explicitly specified high-level actions, subgoals, or behavioral primitives (Precup, 2000). These depend on state representations simple or structured enough that suitable reward signals can be effectively engineered by hand.</p>
<p>But is such fine-grained supervision actually necessary to achieve the full benefits of hierarchy? Specifically, is it necessary to explicitly ground high-level actions into the representation of the environment? Or is it sufficient to simply inform the learner about the abstract structure of policies, without ever specifying how high-level behaviors should make use of primitive percepts or actions?</p>
<p>To answer these questions, we explore a multitask reinforcement learning setting where the learner is pre-</p>
<p>sented with policy sketches. Policy sketches are short, ungrounded, symbolic representations of a task that describe its component parts, as illustrated in Figure 1. While symbols might be shared across tasks (get wood appears in sketches for both the make planks and make sticks tasks), the learner is told nothing about what these symbols mean, in terms of either observations or intermediate rewards.</p>
<p>We present an agent architecture that learns from policy sketches by associating each high-level action with a parameterization of a low-level subpolicy, and jointly optimizes over concatenated task-specific policies by tying parameters across shared subpolicies. We find that this architecture can use the high-level guidance provided by sketches, without any grounding or concrete definition, to dramatically accelerate learning of complex multi-stage behaviors. Our experiments indicate that many of the benefits to learning that come from highly detailed low-level supervision (e.g. from subgoal rewards) can also be obtained from fairly coarse high-level supervision (i.e. from policy sketches). Crucially, sketches are much easier to produce: they require no modifications to the environment dynamics or reward function, and can be easily provided by nonexperts. This makes it possible to extend the benefits of hierarchical RL to challenging environments where it may not be possible to specify by hand the details of relevant subtasks. We show that our approach substantially outperforms purely unsupervised methods that do not provide the learner with any task-specific guidance about how hierarchies should be deployed, and further that the specific use of sketches to parameterize modular subpolicies makes better use of sketches than conditioning on them directly.</p>
<p>The present work may be viewed as an extension of recent approaches for learning compositional deep architectures from structured program descriptors (Andreas et al., 2016; Reed \&amp; de Freitas, 2016). Here we focus on learning in interactive environments. This extension presents a variety of technical challenges, requiring analogues of these methods that can be trained from sparse, non-differentiable reward signals without demonstrations of desired system behavior.</p>
<p>Our contributions are:</p>
<ul>
<li>A general paradigm for multitask, hierarchical, deep reinforcement learning guided by abstract sketches of task-specific policies.</li>
<li>A concrete recipe for learning from these sketches, built on a general family of modular deep policy representations and a multitask actor-critic training objective.</li>
</ul>
<p>The modular structure of our approach, which associates every high-level action symbol with a discrete subpolicy, naturally induces a library of interpretable policy fragments
that are easily recombined. This makes it possible to evaluate our approach under a variety of different data conditions: (1) learning the full collection of tasks jointly via reinforcement, (2) in a zero-shot setting where a policy sketch is available for a held-out task, and (3) in a adaptation setting, where sketches are hidden and the agent must learn to adapt a pretrained policy to reuse high-level actions in a new task. In all cases, our approach substantially outperforms previous approaches based on explicit decomposition of the Q function along subtasks (Parr \&amp; Russell, 1998; Vogel \&amp; Jurafsky, 2010), unsupervised option discovery (Bacon \&amp; Precup, 2015), and several standard policy gradient baselines.</p>
<p>We consider three families of tasks: a 2-D Minecraftinspired crafting game (Figure 3a), in which the agent must acquire particular resources by finding raw ingredients, combining them together in the proper order, and in some cases building intermediate tools that enable the agent to alter the environment itself; a 2-D maze navigation task that requires the agent to collect keys and open doors, and a 3-D locomotion task (Figure 3b) in which a quadrupedal robot must actuate its joints to traverse a narrow winding cliff.</p>
<p>In all tasks, the agent receives a reward only after the final goal is accomplished. For the most challenging tasks, involving sequences of four or five high-level actions, a taskspecific agent initially following a random policy essentially never discovers the reward signal, so these tasks cannot be solved without considering their hierarchical structure. We have released code at http://github.com/ jacobandreas/psketch.</p>
<h2>2. Related Work</h2>
<p>The agent representation we describe in this paper belongs to the broader family of hierarchical reinforcement learners. As detailed in Section 3, our approach may be viewed as an instantiation of the options framework first described by Sutton et al. (1999). A large body of work describes techniques for learning options and related abstract actions, in both single- and multitask settings. Most techniques for learning options rely on intermediate supervisory signals, e.g. to encourage exploration (Kearns \&amp; Singh, 2002) or completion of pre-defined subtasks (Kulkarni et al., 2016). An alternative family of approaches employs post-hoc analysis of demonstrations or pretrained policies to extract reusable sub-components (Stolle \&amp; Precup, 2002; Konidaris et al., 2011; Niekum et al., 2015). Techniques for learning options with less guidance than the present work include Bacon \&amp; Precup (2015) and Vezhnevets et al. (2016), and other general hierarchical policy learners include Daniel et al. (2012), Bakker \&amp; Schmidhuber (2004) and Menache et al. (2002). We will see that the minimal supervision provided by policy sketches re-</p>
<p>sults in (sometimes dramatic) improvements over fully unsupervised approaches, while being substantially less onerous for humans to provide compared to the grounded supervision (such as explicit subgoals or feature abstraction hierarchies) used in previous work.</p>
<p>Once a collection of high-level actions exists, agents are faced with the problem of learning meta-level (typically semi-Markov) policies that invoke appropriate high-level actions in sequence (Precup, 2000). The learning problem we describe in this paper is in some sense the direct dual to the problem of learning these meta-level policies: there, the agent begins with an inventory of complex primitives and must learn to model their behavior and select among them; here we begin knowing the names of appropriate high-level actions but nothing about how they are implemented, and must infer implementations (but not, initially, abstract plans) from context. Our model can be combined with these approaches to support a "mixed" supervision condition where sketches are available for some tasks but not others (Section 4.5).</p>
<p>Another closely related line of work is the Hierarchical Abstract Machines (HAM) framework introduced by Parr \&amp; Russell (1998). Like our approach, HAMs begin with a representation of a high-level policy as an automaton (or a more general computer program; Andre \&amp; Russell, 2001; Marthi et al., 2004) and use reinforcement learning to fill in low-level details. Because these approaches attempt to learn a single representation of the Q function for all subtasks and contexts, they require extremely strong formal assumptions about the form of the reward function and state representation (Andre \&amp; Russell, 2002) that the present work avoids by decoupling the policy representation from the value function. They perform less effectively when applied to arbitrary state representations where these assumptions do not hold (Section 4.3). We are additionally unaware of past work showing that HAM automata can be automatically inferred for new tasks given a pre-trained model, while here we show that it is easy to solve the corresponding problem for sketch followers (Section 4.5).</p>
<p>Our approach is also inspired by a number of recent efforts toward compositional reasoning and interaction with structured deep models. Such models have been previously used for tasks involving question answering (Iyyer et al., 2014; Andreas et al., 2016) and relational reasoning (Socher et al., 2012), and more recently for multi-task, multi-robot transfer problems (Devin et al., 2016). In the present work-as in existing approaches employing dynamically assembled modular networks-task-specific training signals are propagated through a collection of composed discrete structures with tied weights. Here the composed structures specify time-varying policies rather than feedforward computations, and their parameters must be learned via interaction
rather than direct supervision. Another closely related family of models includes neural programmers (Neelakantan et al., 2015) and programmer-interpreters (Reed \&amp; de Freitas, 2016), which generate discrete computational structures but require supervision in the form of output actions or full execution traces.</p>
<p>We view the problem of learning from policy sketches as complementary to the instruction following problem studied in the natural language processing literature. Existing work on instruction following focuses on mapping from natural language strings to symbolic action sequences that are then executed by a hard-coded interpreter (Branavan et al., 2009; Chen \&amp; Mooney, 2011; Artzi \&amp; Zettlemoyer, 2013; Tellex et al., 2011). Here, by contrast, we focus on learning to execute complex actions given symbolic representations as a starting point. Instruction following models may be viewed as joint policies over instructions and environment observations (so their behavior is not defined in the absence of instructions), while the model described in this paper naturally supports adaptation to tasks where no sketches are available. We expect that future work might combine the two lines of research, bootstrapping policy learning directly from natural language hints rather than the semi-structured sketches used here.</p>
<h2>3. Learning Modular Policies from Sketches</h2>
<p>We consider a multitask reinforcement learning problem arising from a family of infinite-horizon discounted Markov decision processes in a shared environment. This environment is specified by a tuple $(\mathcal{S}, \mathcal{A}, P, \gamma)$, with $\mathcal{S}$ a set of states, $\mathcal{A}$ a set of low-level actions, $P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow$ $\mathbb{R}$ a transition probability distribution, and $\gamma$ a discount factor. Each task $\tau \in \mathcal{T}$ is then specified by a pair $\left(R_{\tau}, \rho_{\tau}\right)$, with $R_{\tau}: \mathcal{S} \rightarrow \mathbb{R}$ a task-specific reward function and $\rho_{\tau}: \mathcal{S} \rightarrow \mathbb{R}$ an initial distribution over states. For a fixed sequence $\left{\left(s_{i}, a_{i}\right)\right}$ of states and actions obtained from a rollout of a given policy, we will denote the empirical return starting in state $s_{i}$ as $q_{i}:=\sum_{j=i+1}^{\infty} \gamma^{j-i-1} R\left(s_{j}\right)$. In addition to the components of a standard multitask RL problem, we assume that tasks are annotated with sketches $K_{\tau}$, each consisting of a sequence $\left(b_{\tau 1}, b_{\tau 2}, \ldots\right)$ of high-level symbolic labels drawn from a fixed vocabulary $\mathcal{B}$.</p>
<h3>3.1. Model</h3>
<p>We exploit the structural information provided by sketches by constructing for each symbol $b$ a corresponding subpolicy $\pi_{b}$. By sharing each subpolicy across all tasks annotated with the corresponding symbol, our approach naturally learns the shared abstraction for the corresponding subtask, without requiring any information about the grounding of that task to be explicitly specified by annotation.</p>
<div class="codehilite"><pre><span></span><code>Algorithm <span class="mi">1</span> TRAIN-STEP<span class="p">(</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>boldsymbol<span class="p">{</span><span class="err">\</span>Pi<span class="p">}</span><span class="err">\</span><span class="p">),</span> curriculum<span class="p">)</span>
    <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>emptyset<span class="err">\</span><span class="p">)</span>
    while <span class="err">\</span><span class="p">(</span><span class="err">|\</span>mathcal<span class="p">{</span>D<span class="p">}</span><span class="err">|</span><span class="o">&lt;</span>D<span class="err">\</span><span class="p">)</span> do
        <span class="o">//</span> sample task <span class="err">\</span><span class="p">(</span><span class="err">\</span>tau<span class="err">\</span><span class="p">)</span> from curriculum <span class="p">(</span>Section <span class="mf">3.3</span><span class="p">)</span>
        <span class="err">\</span><span class="p">(</span><span class="err">\</span>tau <span class="err">\</span>sim <span class="err">\</span>operatorname<span class="p">{</span>curriculum<span class="p">}(</span><span class="err">\</span>cdot<span class="p">)</span><span class="err">\</span><span class="p">)</span>
        <span class="o">//</span> do rollout
        <span class="err">\</span><span class="p">(</span><span class="ss">d</span><span class="o">=</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>i<span class="p">},</span> a_<span class="p">{</span>i<span class="p">},</span><span class="err">\</span>left<span class="p">(</span>b_<span class="p">{</span>i<span class="p">}</span><span class="o">=</span>K_<span class="p">{</span><span class="err">\</span>tau<span class="p">,</span> i<span class="p">}</span><span class="err">\</span>right<span class="p">),</span> q_<span class="p">{</span>i<span class="p">},</span> <span class="err">\</span>tau<span class="err">\</span>right<span class="p">),</span> <span class="err">\</span>ldots<span class="err">\</span>right<span class="err">\</span><span class="p">}</span> <span class="err">\</span>sim <span class="err">\</span>Pi_<span class="p">{</span><span class="err">\</span>tau<span class="p">}</span><span class="err">\</span><span class="p">)</span>
        <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span> <span class="err">\</span>cup d<span class="err">\</span><span class="p">)</span>
    <span class="o">//</span> update parameters
    for <span class="err">\</span><span class="p">(</span>b <span class="err">\</span><span class="k">in</span> <span class="err">\</span>mathcal<span class="p">{</span>B<span class="p">},</span> <span class="err">\</span>tau <span class="err">\</span><span class="k">in</span> <span class="err">\</span>mathcal<span class="p">{</span>T<span class="p">}</span><span class="err">\</span><span class="p">)</span> do
        <span class="err">\</span><span class="p">(</span><span class="ss">d</span><span class="o">=</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>i<span class="p">},</span> a_<span class="p">{</span>i<span class="p">},</span> b<span class="err">^</span><span class="p">{</span><span class="err">\</span>prime<span class="p">},</span> q_<span class="p">{</span>i<span class="p">},</span> <span class="err">\</span>tau<span class="err">^</span><span class="p">{</span><span class="err">\</span>prime<span class="p">}</span><span class="err">\</span>right<span class="p">)</span> <span class="err">\</span><span class="k">in</span> <span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}:</span> b<span class="err">^</span><span class="p">{</span><span class="err">\</span>prime<span class="p">}</span><span class="o">=</span>b<span class="p">,</span> <span class="err">\</span>tau<span class="err">^</span><span class="p">{</span><span class="err">\</span>prime<span class="p">}</span><span class="o">=</span><span class="err">\</span>tau<span class="err">\</span>right<span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
        <span class="o">//</span> update subpolicy
        <span class="err">\</span><span class="p">(</span><span class="err">\</span>theta_<span class="p">{</span>b<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>theta_<span class="p">{</span>b<span class="p">}</span><span class="o">+</span><span class="err">\</span>frac<span class="p">{</span><span class="err">\</span>alpha<span class="p">}{</span>D<span class="p">}</span> <span class="err">\</span>sum_<span class="p">{</span>d<span class="p">}</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>nabla <span class="err">\</span>log <span class="err">\</span>pi_<span class="p">{</span>b<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>a_<span class="p">{</span>i<span class="p">}</span> <span class="err">\</span>mid s_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span>left<span class="p">(</span>q_<span class="p">{</span>i<span class="p">}</span><span class="o">-</span>c_<span class="p">{</span><span class="err">\</span>tau<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
        <span class="o">//</span> update critic
        <span class="err">\</span><span class="p">(</span><span class="err">\</span>eta_<span class="p">{</span><span class="err">\</span>tau<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>eta_<span class="p">{</span><span class="err">\</span>tau<span class="p">}</span><span class="o">+</span><span class="err">\</span>frac<span class="p">{</span><span class="err">\</span>beta<span class="p">}{</span>D<span class="p">}</span> <span class="err">\</span>sum_<span class="p">{</span>d<span class="p">}</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>nabla c_<span class="p">{</span><span class="err">\</span>tau<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span>left<span class="p">(</span>q_<span class="p">{</span>i<span class="p">}</span><span class="o">-</span>c_<span class="p">{</span><span class="err">\</span>tau<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>At each timestep, a subpolicy may select either a low-level action $a \in \mathcal{A}$ or a special STOP action. We denote the augmented state space $\mathcal{A}^{+}:=\mathcal{A} \cup{\operatorname{STOP}}$. At a high level, this framework is agnostic to the implementation of subpolicies: any function that takes a representation of the current state onto a distribution over $\mathcal{A}^{+}$will do.</p>
<p>In this paper, we focus on the case where each $\pi_{b}$ is represented as a neural network. ${ }^{1}$ These subpolicies may be viewed as options of the kind described by Sutton et al. (1999), with the key distinction that they have no initiation semantics, but are instead invokable everywhere, and have no explicit representation as a function from an initial state to a distribution over final states (instead implicitly using the STOP action to terminate).</p>
<p>Given a fixed sketch $\left(b_{1}, b_{2}, \ldots\right)$, a task-specific policy $\Pi_{\tau}$ is formed by concatenating its associated subpolicies in sequence. In particular, the high-level policy maintains a subpolicy index $i$ (initially 0 ), and executes actions from $\pi_{b_{i}}$ until the STOP symbol is emitted, at which point control is passed to $\pi_{b_{i+1}}$. We may thus think of $\Pi_{\tau}$ as inducing a Markov chain over the state space $\mathcal{S} \times \mathcal{B}$, with transitions:</p>
<p>$$
\begin{array}{lll}
\left(s, b_{i}\right) \rightarrow\left(s^{\prime}, b_{i}\right) &amp; \text { with pr. } &amp; \sum_{a \in \mathcal{A}} \pi_{b_{i}}(a \mid s) \cdot P\left(s^{\prime} \mid s, a\right) \
\rightarrow\left(s, b_{i+1}\right) &amp; \text { with pr. } &amp; \pi_{b_{i}}(\operatorname{STOP} \mid s)
\end{array}
$$</p>
<p>Note that $\Pi_{\tau}$ is semi-Markov with respect to projection of the augmented state space $\mathcal{S} \times \mathcal{B}$ onto the underlying state space $\mathcal{S}$. We denote the complete family of task-specific policies $\boldsymbol{\Pi}:=\bigcup_{\tau}\left{\Pi_{\tau}\right}$, and let each $\pi_{b}$ be an arbitrary function of the current environment state parameterized by some weight vector $\theta_{b}$. The learning problem is to optimize</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Algorithm 2 TRAIN-LOOP()
    // initialize subpolicies randomly
    (\boldsymbol{\Pi}=\operatorname{INIT}())
    (\ell_{\max } \leftarrow 1)
    loop
    (r_{\min } \leftarrow-\infty)
    // initialize (\ell_{\max })-step curriculum uniformly
    (\mathcal{T}^{\prime}=\left{\tau \in \mathcal{T}: \mid K_{\tau} \mid \leq \ell_{\max }\right})
    curriculum ((\cdot)=\operatorname{Unif}\left(\mathcal{T}^{\prime}\right))
    while (r_{\min }&lt;r_{\text {good }}) do
        // update parameters (Algorithm 1)
        TRAIN-STEP( (\boldsymbol{\Pi}), curriculum)
        curriculum ((\tau) \propto \mathbb{1}\left[\tau \in \mathcal{T}^{\prime}\right]\left(1-\hat{\mathbb{E}} r_{\tau}\right) \quad \forall \tau \in \mathcal{T})
        (r_{\min } \leftarrow \min <em _tau="\tau">{\tau \in \mathcal{T}^{\prime}} \hat{\mathbb{E}} r</em>)
    (\ell_{\max } \leftarrow \ell_{\max }+1)
```</p>
<p>over all $\theta_{b}$ to maximize expected discounted reward</p>
<p>$$
J(\boldsymbol{\Pi}):=\sum_{\tau} J\left(\Pi_{\tau}\right):=\sum_{\tau} \mathbb{E}<em i="i">{s</em>\right)\right]
$$} \sim \Pi_{\tau}}\left[\sum_{i} \gamma^{i} R_{\tau}\left(s_{i</p>
<p>across all tasks $\tau \in \mathcal{T}$.</p>
<h3>3.2. Policy Optimization</h3>
<p>Here that optimization is accomplished via a simple decoupled actor-critic method. In a standard policy gradient approach, with a single policy $\pi$ with parameters $\theta$, we compute gradient steps of the form (Williams, 1992):</p>
<p>$$
\nabla_{\theta} J(\pi)=\sum_{i}\left(\nabla_{\theta} \log \pi\left(a_{i} \mid s_{i}\right)\right)\left(q_{i}-c\left(s_{i}\right)\right)
$$</p>
<p>where the baseline or "critic" $c$ can be chosen independently of the future without introducing bias into the gradient. Recalling our previous definition of $q_{i}$ as the empirical return starting from $s_{i}$, this form of the gradient corresponds to a generalized advantage estimator (Schulman et al., 2015a) with $\lambda=1$. Here $c$ achieves close to the optimal variance (Greensmith et al., 2004) when it is set
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Model overview. Each subpolicy $\pi$ is uniquely associated with a symbol $b$ implemented as a neural network that maps from a state $s_{i}$ to distributions over $\mathcal{A}^{+}$, and chooses an action $a_{i}$ by sampling from this distribution. Whenever the STOP action is sampled, control advances to the next subpolicy in the sketch.</p>
<p>exactly equal to the state-value function $V_{\pi}\left(s_{i}\right)=\mathbb{E}<em i="i">{\pi} q</em>$.}$ for the target policy $\pi$ starting in state $s_{i</p>
<p>The situation becomes slightly more complicated when generalizing to modular policies built by sequencing subpolicies. In this case, we will have one subpolicy per symbol but one critic per task. This is because subpolicies $\pi_{b}$ might participate in a number of composed policies $\Pi_{\tau}$, each associated with its own reward function $R_{\tau}$. Thus individual subpolicies are not uniquely identified with value functions, and the aforementioned subpolicy-specific statevalue estimator is no longer well-defined. We extend the actor-critic method to incorporate the decoupling of policies from value functions by allowing the critic to vary persample (that is, per-task-and-timestep) depending on the reward function with which the sample is associated. Noting that $\nabla_{\theta_{b}} J(\boldsymbol{\Pi})=\sum_{t: b \in K_{\tau}} \nabla_{\theta_{b}} J\left(\Pi_{\tau}\right)$, i.e. the sum of gradients of expected rewards across all tasks in which $\pi_{b}$ participates, we have:</p>
<p>$$
\begin{aligned}
&amp; \nabla_{\theta} J(\boldsymbol{\Pi})=\sum_{\tau} \nabla_{\theta} J\left(\Pi_{\tau}\right) \
&amp; \quad=\sum_{\tau} \sum_{i}\left(\nabla_{\theta_{b}} \log \pi_{b}\left(a_{\tau i} \mid s_{\tau i}\right)\right)\left(q_{i}-c_{\tau}\left(s_{\tau i}\right)\right)
\end{aligned}
$$</p>
<p>where each state-action pair $\left(s_{\tau i}, a_{\tau i}\right)$ was selected by the subpolicy $\pi_{b}$ in the context of the task $\tau$.</p>
<p>Now minimization of the gradient variance requires that each $c_{\tau}$ actually depend on the task identity. (This follows immediately by applying the corresponding argument in Greensmith et al. (2004) individually to each term in the sum over $\tau$ in Equation 2.) Because the value function is itself unknown, an approximation must be estimated from data. Here we allow these $c_{\tau}$ to be implemented with an arbitrary function approximator with parameters $\eta_{\tau}$. This is trained to minimize a squared error criterion, with gradients given by</p>
<p>$$
\begin{aligned}
&amp; \nabla_{\eta_{\tau}}\left[-\frac{1}{2} \sum_{i}\left(q_{i}-c_{\tau}\left(s_{i}\right)\right)^{2}\right] \
&amp; \quad=\sum_{i}\left(\nabla_{\eta_{\tau}} c_{\tau}\left(s_{i}\right)\right)\left(q_{i}-c_{\tau}\left(s_{i}\right)\right)
\end{aligned}
$$</p>
<p>Alternative forms of the advantage estimator (e.g. the TD residual $R_{\tau}\left(s_{i}\right)+\gamma V_{\tau}\left(s_{i+1}\right)-V_{\tau}\left(s_{i}\right)$ or any other member of the generalized advantage estimator family) can be easily substituted by simply maintaining one such estimator per task. Experiments (Section 4.4) show that conditioning on both the state and the task identity results in noticeable performance improvements, suggesting that the variance reduction provided by this objective is important for efficient joint learning of modular policies.</p>
<p>The complete procedure for computing a single gradient step is given in Algorithm 1. (The outer training loop over
these steps, which is driven by a curriculum learning procedure, is specified in Algorithm 2.) This is an on-policy algorithm. In each step, the agent samples tasks from a task distribution provided by a curriculum (described in the following subsection). The current family of policies $\boldsymbol{\Pi}$ is used to perform rollouts in each sampled task, accumulating the resulting tuples of (states, low-level actions, highlevel symbols, rewards, and task identities) into a dataset $\mathcal{D}$. Once $\mathcal{D}$ reaches a maximum size $D$, it is used to compute gradients w.r.t. both policy and critic parameters, and the parameter vectors are updated accordingly. The step sizes $\alpha$ and $\beta$ in Algorithm 1 can be chosen adaptively using any first-order method.</p>
<h3>3.3. Curriculum Learning</h3>
<p>For complex tasks, like the one depicted in Figure 3b, it is difficult for the agent to discover any states with positive reward until many subpolicy behaviors have already been learned. It is thus a better use of the learner's time to focus on "easy" tasks, where many rollouts will result in high reward from which appropriate subpolicy behavior can be inferred. But there is a fundamental tradeoff involved here: if the learner spends too much time on easy tasks before being made aware of the existence of harder ones, it may overfit and learn subpolicies that no longer generalize or exhibit the desired structural properties.</p>
<p>To avoid both of these problems, we use a curriculum learning scheme (Bengio et al., 2009) that allows the model to smoothly scale up from easy tasks to more difficult ones while avoiding overfitting. Initially the model is presented with tasks associated with short sketches. Once average reward on all these tasks reaches a certain threshold, the length limit is incremented. We assume that rewards across tasks are normalized with maximum achievable reward $0&lt;q_{i}&lt;1$. Let $\hat{\mathbb{E}} r_{\tau}$ denote the empirical estimate of the expected reward for the current policy on task $\tau$. Then at each timestep, tasks are sampled in proportion to $1-\hat{\mathbb{E}} r_{\tau}$, which by assumption must be positive.</p>
<p>Intuitively, the tasks that provide the strongest learning signal are those in which (1) the agent does not on average achieve reward close to the upper bound, but (2) many episodes result in high reward. The expected reward component of the curriculum addresses condition (1) by ensuring that time is not spent on nearly solved tasks, while the length bound component of the curriculum addresses condition (2) by ensuring that tasks are not attempted until high-reward episodes are likely to be encountered. Experiments show that both components of this curriculum learning scheme improve the rate at which the model converges to a good policy (Section 4.4).</p>
<p>The complete curriculum-based training procedure is specified in Algorithm 2. Initially, the maximum sketch length</p>
<p>$\ell_{\max }$ is set to 1 , and the curriculum initialized to sample length-1 tasks uniformly. (Neither of the environments we consider in this paper feature any length-1 tasks; in this case, observe that Algorithm 2 will simply advance to length-2 tasks without any parameter updates.) For each setting of $\ell_{\max }$, the algorithm uses the current collection of task policies $\boldsymbol{\Pi}$ to compute and apply the gradient step described in Algorithm 1. The rollouts obtained from the call to TRAIN-STEP can also be used to compute reward estimates $\widehat{\mathbb{E}} r_{\tau}$; these estimates determine a new task distribution for the curriculum. The inner loop is repeated until the reward threshold $r_{\text {good }}$ is exceeded, at which point $\ell_{\max }$ is incremented and the process repeated over a (nowexpanded) collection of tasks.</p>
<h2>4. Experiments</h2>
<p>We evaluate the performance of our approach in three environments: a crafting environment, a maze navigation environment, and a cliff traversal environment. These environments involve various kinds of challenging low-level control: agents must learn to avoid obstacles, interact with various kinds of objects, and relate fine-grained joint activation to high-level locomotion goals. They also feature hierarchical structure: most rewards are provided only after the agent has completed two to five high-level actions in the appropriate sequence, without any intermediate goals to indicate progress towards completion.</p>
<h3>4.1. Implementation</h3>
<p>In all our experiments, we implement each subpolicy as a feedforward neural network with ReLU nonlinearities and a hidden layer with 128 hidden units, and each critic as a linear function of the current state. Each subpolicy network receives as input a set of features describing the current state of the environment, and outputs a distribution over actions. The agent acts at every timestep by sampling from this distribution. The gradient steps given in lines 8 and 9 of Algorithm 1 are implemented using RMSProp (Tieleman, 2012) with a step size of 0.001 and gradient clipping to a unit norm. We take the batch size $D$ in Algorithm 1 to be 2000, and set $\gamma=0.9$ in both environments. For curriculum learning, the improvement threshold $r_{\text {good }}$ is 0.8 .</p>
<h3>4.2. Environments</h3>
<p>The crafting environment (Figure 3a) is inspired by the popular game Minecraft, but is implemented in a discrete 2-D world. The agent may interact with objects in the world by facing them and executing a special USE action. Interacting with raw materials initially scattered around the environment causes them to be added to an inventory. Interacting with different crafting stations causes objects in the agent's inventory to be combined or transformed. Each task
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Examples from the crafting and cliff environments used in this paper. An additional maze environment is also investigated. (a) In the crafting environment, an agent seeking to pick up the gold nugget in the top corner must first collect wood (1) and iron (2), use a workbench to turn them into a bridge (3), and use the bridge to cross the water (4). (b) In the cliff environment, the agent must reach a goal position by traversing a winding sequence of tiles without falling off. Control takes place at the level of individual joint angles; high-level behaviors like "move north" must be learned.
in this game corresponds to some crafted object the agent must produce; the most complicated goals require the agent to also craft intermediate ingredients, and in some cases build tools (like a pickaxe and a bridge) to reach ingredients located in initially inaccessible regions of the environment.</p>
<p>The maze environment (not pictured) corresponds closely to the the "light world" described by Konidaris \&amp; Barto (2007). The agent is placed in a discrete world consisting of a series of rooms, some of which are connected by doors. Some doors require that the agent first pick up a key to open them. For our experiments, each task corresponds to a goal room (always at the same position relative to the agent's starting position) that the agent must reach by navigating through a sequence of intermediate rooms. The agent has one sensor on each side of its body, which reports the distance to keys, closed doors, and open doors in the corresponding direction. Sketches specify a particular sequence of directions for the agent to traverse between rooms to reach the goal. The sketch always corresponds to a viable traversal from the start to the goal position, but other (possibly shorter) traversals may also exist.</p>
<p>The cliff environment (Figure 3b) is intended to demonstrate the applicability of our approach to problems involving high-dimensional continuous control. In this environment, a quadrupedal robot (Schulman et al., 2015b) is placed on a variable-length winding path, and must navi-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Comparing modular learning from sketches with standard RL baselines. Modular is the approach described in this paper, while Independent learns a separate policy for each task, Joint learns a shared policy that conditions on the task identity, $\mathbf{Q}$ automaton learns a single network to map from states and action symbols to Q values, and Opt-Crit is an unsupervised option learner. Performance for the best iteration of the (off-policy) Q automaton is plotted. Performance is shown in (a) the crafting environment, (b) the maze environment, and (c) the cliff environment. The modular approach is eventually able to achieve high reward on all tasks, while the baseline models perform considerably worse on average.
gate to the end without falling off. This task is designed to provide a substantially more challenging RL problem, due to the fact that the walker must learn the low-level walking skill before it can make any progress, but has simpler hierarchical structure than the crafting environment. The agent receives a small reward for making progress toward the goal, and a large positive reward for reaching the goal square, with a negative reward for falling off the path.</p>
<p>A listing of tasks and sketches is given in Appendix A.</p>
<h3>4.3. Multitask Learning</h3>
<p>The primary experimental question in this paper is whether the extra structure provided by policy sketches alone is enough to enable fast learning of coupled policies across tasks. We aim to explore the differences between the approach described in Section 3 and relevant prior work that performs either unsupervised or weakly supervised multitask learning of hierarchical policy structure. Specifically, we compare our modular to approach to:</p>
<ol>
<li>Structured hierarchical reinforcement learners:
(a) the fully unsupervised option-critic algorithm of Bacon \&amp; Precup (2015)
(b) a $\mathbf{Q}$ automaton that attempts to explicitly represent the Q function for each task / subtask combination (essentially a HAM (Andre \&amp; Russell, 2002) with a deep state abstraction function)</li>
<li>Alternative ways of incorporating sketch data into standard policy gradient methods:
(c) learning an independent policy for each task
(d) learning a joint policy across all tasks, conditioning directly on both environment features and a representation of the complete sketch</li>
</ol>
<p>The joint and independent models performed best when trained with the same curriculum described in Section 3.3, while the option-critic model performed best with a length-weighted curriculum that has access to all tasks from the beginning of training.</p>
<p>Learning curves for baselines and the modular model are shown in Figure 4. It can be seen that in all environments, our approach substantially outperforms the baselines: it induces policies with substantially higher average reward and converges more quickly than the policy gradient baselines. It can further be seen in Figure 4c that after policies have been learned on simple tasks, the model is able to rapidly adapt to more complex ones, even when the longer tasks involve high-level actions not required for any of the short tasks (Appendix A).</p>
<p>Having demonstrated the overall effectiveness of our approach, our remaining experiments explore (1) the importance of various components of the training procedure, and (2) the learned models' ability to generalize or adapt to held-out tasks. For compactness, we restrict our consideration on the crafting domain, which features a larger and more diverse range of tasks and high-level actions.</p>
<h3>4.4. Ablations</h3>
<p>In addition to the overall modular parameter-tying structure induced by our sketches, the key components of our training procedure are the decoupled critic and the curriculum. Our next experiments investigate the extent to which these are necessary for good performance.</p>
<p>To evaluate the the critic, we consider three ablations: (1) removing the dependence of the model on the environment state, in which case the baseline is a single scalar per task; (2) removing the dependence of the model on the task, in which case the baseline is a conventional generalized advantage estimator; and (3) removing both, in which case</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Training details in the crafting domain. (a) Critics: lines labeled "task" include a baseline that varies with task identity, while lines labeled "state" include a baseline that varies with state identity. Estimating a baseline that depends on both the representation of the current state and the identity of the current task is better than either alone or a constant baseline. (b) Curricula: lines labeled "len" use a curriculum with iteratively increasing sketch lengths, while lines labeled "wgt" sample tasks in inverse proportion to their current reward. Adjusting the sampling distribution based on both task length and performance return improves convergence. (c) Individual task performance. Colors correspond to task length. Sharp steps in the learning curve correspond to increases of $t_{\max }$ in the curriculum.
the baseline is a single scalar, as in a vanilla policy gradient approach. Results are shown in Figure 5a. Introducing both state and task dependence into the baseline leads to faster convergence of the model: the approach with a constant baseline achieves less than half the overall performance of the full critic after 3 million episodes. Introducing task and state dependence independently improve this performance; combining them gives the best result.</p>
<p>We also investigate two aspects of our curriculum learning scheme: starting with short examples and moving to long ones, and sampling tasks in inverse proportion to their accumulated reward. Experiments are shown in Figure 5b. Both components help; prioritization by both length and weight gives the best results.</p>
<h3>4.5. Zero-shot and Adaptation Learning</h3>
<p>In our final experiments, we consider the model's ability to generalize beyond the standard training condition. We first consider two tests of generalization: a zero-shot setting, in which the model is provided a sketch for the new task and must immediately achieve good performance, and a adaptation setting, in which no sketch is provided and the model must learn the form of a suitable sketch via interaction in the new task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Multitask</th>
<th style="text-align: center;">0-shot</th>
<th style="text-align: center;">Adaptation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Joint</td>
<td style="text-align: center;">.49</td>
<td style="text-align: center;">.01</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Independent</td>
<td style="text-align: center;">.44</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">.01</td>
</tr>
<tr>
<td style="text-align: left;">Option-Critic</td>
<td style="text-align: center;">.47</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">.42</td>
</tr>
<tr>
<td style="text-align: left;">Modular (ours)</td>
<td style="text-align: center;">$\mathbf{. 8 9}$</td>
<td style="text-align: center;">$\mathbf{. 7 7}$</td>
<td style="text-align: center;">$\mathbf{. 7 6}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Accuracy and generalization of learned models in the crafting domain. The table shows the task completion rate for each approach after convergence under various training conditions. Multitask is the multitask training condition described in Section 4.3, while 0-Shot and Adaptation are the generalization experiments described in Section 4.5. Our modular approach consistently achieves the best performance.</p>
<p>We hold out two length-four tasks from the full inventory used in Section 4.3, and train on the remaining tasks. For zero-shot experiments, we simply form the concatenated policy described by the sketches of the held-out tasks, and repeatedly execute this policy (without learning) in order to obtain an estimate of its effectiveness. For adaptation experiments, we consider ordinary RL over high-level actions $\mathcal{B}$ rather than low-level actions $\mathcal{A}$, implementing the highlevel learner with the same agent architecture as described in Section 3.1. Note that the Independent and OptionCritic models cannot be applied to the zero-shot evaluation, while the Joint model cannot be applied to the adaptation baseline (because it depends on pre-specified sketch features). Results are shown in Table 1. The held-out tasks are sufficiently challenging that the baselines are unable to obtain more than negligible reward: in particular, the joint model overfits to the training tasks and cannot generalize to new sketches, while the independent model cannot discover enough of a reward signal to learn in the adaptation setting. The modular model does comparatively well: individual subpolicies succeed in novel zero-shot configurations (suggesting that they have in fact discovered the behavior suggested by the semantics of the sketch) and provide a suitable basis for adaptive discovery of new high-level policies.</p>
<h2>5. Conclusions</h2>
<p>We have described an approach for multitask learning of deep multitask policies guided by symbolic policy sketches. By associating each symbol appearing in a sketch with a modular neural subpolicy, we have shown that it is possible to build agents that share behavior across tasks in order to achieve success in tasks with sparse and delayed rewards. This process induces an inventory of reusable and interpretable subpolicies which can be employed for zeroshot generalization when further sketches are available, and hierarchical reinforcement learning when they are not. Our work suggests that these sketches, which are easy to produce and require no grounding in the environment, provide an effective scaffold for learning hierarchical policies from minimal supervision.</p>
<h2>Acknowledgments</h2>
<p>JA is supported by a Facebook Graduate Fellowship and a Berkeley AI / Huawei Fellowship.</p>
<h2>References</h2>
<p>Andre, David and Russell, Stuart. Programmable reinforcement learning agents. In Advances in Neural Information Processing Systems, 2001.</p>
<p>Andre, David and Russell, Stuart. State abstraction for programmable reinforcement learning agents. In Proceedings of the Meeting of the Association for the Advancement of Artificial Intelligence, 2002.</p>
<p>Andreas, Jacob, Rohrbach, Marcus, Darrell, Trevor, and Klein, Dan. Learning to compose neural networks for question answering. In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics, 2016.</p>
<p>Artzi, Yoav and Zettlemoyer, Luke. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1(1):49-62, 2013.</p>
<p>Bacon, Pierre-Luc and Precup, Doina. The option-critic architecture. In NIPS Deep Reinforcement Learning Workshop, 2015.</p>
<p>Bakker, Bram and Schmidhuber, Jürgen. Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization. In Proc. of the 8-th Conf. on Intelligent Autonomous Systems, pp. 438-445, 2004.</p>
<p>Bengio, Yoshua, Louradour, Jérôme, Collobert, Ronan, and Weston, Jason. Curriculum learning. In International Conference on Machine Learning, pp. 41-48. ACM, 2009.</p>
<p>Branavan, S.R.K., Chen, Harr, Zettlemoyer, Luke S., and Barzilay, Regina. Reinforcement learning for mapping instructions to actions. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pp. 82-90. Association for Computational Linguistics, 2009.</p>
<p>Chen, David L. and Mooney, Raymond J. Learning to interpret natural language navigation instructions from observations. In Proceedings of the Meeting of the Association for the Advancement of Artificial Intelligence, volume 2, pp. 1-2, 2011.</p>
<p>Daniel, Christian, Neumann, Gerhard, and Peters, Jan. Hierarchical relative entropy policy search. In Proceedings of the International Conference on Artificial Intelligence and Statistics, pp. 273-281, 2012.</p>
<p>Devin, Coline, Gupta, Abhishek, Darrell, Trevor, Abbeel, Pieter, and Levine, Sergey. Learning modular neural network policies for multi-task and multi-robot transfer. arXiv preprint arXiv:1609.07088, 2016.</p>
<p>Dietterich, Thomas G. Hierarchical reinforcement learning with the maxq value function decomposition. J. Artif. Intell. Res. (JAIR), 13:227-303, 2000.</p>
<p>Greensmith, Evan, Bartlett, Peter L, and Baxter, Jonathan. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5(Nov):1471-1530, 2004.</p>
<p>Hauser, Kris, Bretl, Timothy, Harada, Kensuke, and Latombe, Jean-Claude. Using motion primitives in probabilistic sample-based planning for humanoid robots. In Algorithmic foundation of robotics, pp. 507-522. Springer, 2008.</p>
<p>Iyyer, Mohit, Boyd-Graber, Jordan, Claudino, Leonardo, Socher, Richard, and Daumé III, Hal. A neural network for factoid question answering over paragraphs. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2014.</p>
<p>Kearns, Michael and Singh, Satinder. Near-optimal reinforcement learning in polynomial time. Machine Learning, 49(2-3):209-232, 2002.</p>
<p>Konidaris, George and Barto, Andrew G. Building portable options: Skill transfer in reinforcement learning. In IJCAI, volume 7, pp. 895-900, 2007.</p>
<p>Konidaris, George, Kuindersma, Scott, Grupen, Roderic, and Barto, Andrew. Robot learning from demonstration by constructing skill trees. The International Journal of Robotics Research, pp. 0278364911428653, 2011.</p>
<p>Kulkarni, Tejas D, Narasimhan, Karthik R, Saeedi, Ardavan, and Tenenbaum, Joshua B. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. arXiv preprint arXiv:1604.06057, 2016.</p>
<p>Marthi, Bhaskara, Lantham, David, Guestrin, Carlos, and Russell, Stuart. Concurrent hierarchical reinforcement learning. In Proceedings of the Meeting of the Association for the Advancement of Artificial Intelligence, 2004.</p>
<p>Menache, Ishai, Mannor, Shie, and Shimkin, Nahum. Q-cutdynamic discovery of sub-goals in reinforcement learning. In European Conference on Machine Learning, pp. 295-306. Springer, 2002.</p>
<p>Neelakantan, Arvind, Le, Quoc V, and Sutskever, Ilya. Neural programmer: Inducing latent programs with gradient descent. arXiv preprint arXiv:1511.04834, 2015.</p>
<p>Niekum, Scott, Osentoski, Sarah, Konidaris, George, Chitta, Sachin, Marthi, Bhaskara, and Barto, Andrew G. Learning grounded finite-state representations from unstructured demonstrations. The International Journal of Robotics Research, 34(2):131-157, 2015.</p>
<p>Parr, Ron and Russell, Stuart. Reinforcement learning with hierarchies of machines. In Advances in Neural Information Processing Systems, 1998.</p>
<p>Precup, Doina. Temporal abstraction in reinforcement learning. PhD thesis, 2000.</p>
<p>Reed, Scott and de Freitas, Nando. Neural programmerinterpreters. Proceedings of the International Conference on Learning Representations, 2016.</p>
<p>Schulman, John, Moritz, Philipp, Levine, Sergey, Jordan, Michael, and Abbeel, Pieter. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015a.</p>
<p>Schulman, John, Moritz, Philipp, Levine, Sergey, Jordan, Michael, and Abbeel, Pieter. Trust region policy optimization. In International Conference on Machine Learning, 2015b.</p>
<p>Socher, Richard, Huval, Brody, Manning, Christopher, and Ng, Andrew. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 1201-1211, Jeju, Korea, 2012.</p>
<p>Stolle, Martin and Precup, Doina. Learning options in reinforcement learning. In International Symposium on Abstraction, Reformulation, and Approximation, pp. 212223. Springer, 2002.</p>
<p>Sutton, Richard S, Precup, Doina, and Singh, Satinder. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1):181-211, 1999.</p>
<p>Tellex, Stefanie, Kollar, Thomas, Dickerson, Steven, Walter, Matthew R., Banerjee, Ashis Gopal, Teller, Seth, and Roy, Nicholas. Understanding natural language commands for robotic navigation and mobile manipulation. In In Proceedings of the National Conference on Artificial Intelligence, 2011.</p>
<p>Tieleman, Tijmen. RMSProp (unpublished), 2012.
Vezhnevets, Alexander, Mnih, Volodymyr, Agapiou, John, Osindero, Simon, Graves, Alex, Vinyals, Oriol, and Kavukcuoglu, Koray. Strategic attentive writer for learning macro-actions. arXiv preprint arXiv:1606.04695, 2016.</p>
<p>Vogel, Adam and Jurafsky, Dan. Learning to follow navigational directions. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pp. 806-814. Association for Computational Linguistics, 2010.</p>
<p>Williams, Ronald J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256, 1992.</p>
<h1>A. Tasks and Sketches</h1>
<p>The complete list of tasks, sketches, and symbols is given below. Tasks marked with an asterisk* are held out for the generalization experiments described in Section 4.5, but included in the multitask training experiments in Sections 4.3 and 4.4.</p>
<p>| Goal | Sketch |  |  |  |  |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| Crafting environment |  |  |  |  |  |
| make plank | get wood | use toolshed |  |  |  |
| make stick | get wood | use workbench |  |  |  |
| make cloth | get grass | use factory |  |  |  |
| make rope | get grass | use toolshed |  |  |  |
| make bridge | get iron | get wood | use factory |  |  |
| make bed<em> | get wood | use toolshed | get grass | use workbench |  |
| make axe</em> | get wood | use workbench | get iron | use toolshed |  |
| make shears | get wood | use workbench | get iron | use workbench |  |
| get gold | get iron | get wood | use factory | use bridge |  |
| get gem | get wood | use workbench | get iron | use toolshed | use axe |</p>
<p>Maze environment</p>
<table>
<thead>
<tr>
<th style="text-align: left;">room 1</th>
<th style="text-align: left;">left</th>
<th style="text-align: left;">left</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">room 2</td>
<td style="text-align: left;">left</td>
<td style="text-align: left;">down</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">room 3</td>
<td style="text-align: left;">right</td>
<td style="text-align: left;">down</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">room 4</td>
<td style="text-align: left;">up</td>
<td style="text-align: left;">left</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">room 5</td>
<td style="text-align: left;">up</td>
<td style="text-align: left;">right</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">room 6</td>
<td style="text-align: left;">up</td>
<td style="text-align: left;">right</td>
<td style="text-align: left;">up</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">room 7</td>
<td style="text-align: left;">down</td>
<td style="text-align: left;">right</td>
<td style="text-align: left;">up</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">room 8</td>
<td style="text-align: left;">left</td>
<td style="text-align: left;">left</td>
<td style="text-align: left;">down</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">room 9</td>
<td style="text-align: left;">right</td>
<td style="text-align: left;">down</td>
<td style="text-align: left;">down</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">room 10</td>
<td style="text-align: left;">left</td>
<td style="text-align: left;">up</td>
<td style="text-align: left;">right</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Cliff environment</p>
<table>
<thead>
<tr>
<th style="text-align: center;">path 0</th>
<th style="text-align: center;">north</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">path 1</td>
<td style="text-align: center;">east</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 2</td>
<td style="text-align: center;">south</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 3</td>
<td style="text-align: center;">west</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 4</td>
<td style="text-align: center;">west</td>
<td style="text-align: center;">south</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 5</td>
<td style="text-align: center;">west</td>
<td style="text-align: center;">north</td>
<td style="text-align: center;">north</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 6</td>
<td style="text-align: center;">north</td>
<td style="text-align: center;">east</td>
<td style="text-align: center;">north</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 7</td>
<td style="text-align: center;">west</td>
<td style="text-align: center;">north</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 8</td>
<td style="text-align: center;">east</td>
<td style="text-align: center;">south</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 9</td>
<td style="text-align: center;">north</td>
<td style="text-align: center;">west</td>
<td style="text-align: center;">west</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 10</td>
<td style="text-align: center;">east</td>
<td style="text-align: center;">north</td>
<td style="text-align: center;">east</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 11</td>
<td style="text-align: center;">south</td>
<td style="text-align: center;">east</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 12</td>
<td style="text-align: center;">south</td>
<td style="text-align: center;">west</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 13</td>
<td style="text-align: center;">south</td>
<td style="text-align: center;">south</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 14</td>
<td style="text-align: center;">south</td>
<td style="text-align: center;">south</td>
<td style="text-align: center;">west</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 15</td>
<td style="text-align: center;">east</td>
<td style="text-align: center;">south</td>
<td style="text-align: center;">south</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 16</td>
<td style="text-align: center;">east</td>
<td style="text-align: center;">east</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 17</td>
<td style="text-align: center;">east</td>
<td style="text-align: center;">north</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 18</td>
<td style="text-align: center;">north</td>
<td style="text-align: center;">east</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 19</td>
<td style="text-align: center;">west</td>
<td style="text-align: center;">west</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 20</td>
<td style="text-align: center;">north</td>
<td style="text-align: center;">north</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 21</td>
<td style="text-align: center;">north</td>
<td style="text-align: center;">west</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 22</td>
<td style="text-align: center;">west</td>
<td style="text-align: center;">west</td>
<td style="text-align: center;">south</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path 23</td>
<td style="text-align: center;">south</td>
<td style="text-align: center;">east</td>
<td style="text-align: center;">south</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ For ease of presentation, this section assumes that these subpolicy networks are independently parameterized. As described in Section 4.2, it is also possible to share parameters between subpolicies, and introduce discrete subtask structure by way of an embedding of each symbol $b$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-373 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-373</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-373</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-49902207</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1807.07963v1.pdf" target="_blank">Deep Transfer Learning for Cross-domain Activity Recognition</a></p>
                <p><strong>Paper Abstract:</strong> Human activity recognition plays an important role in people's daily life. However, it is often expensive and time-consuming to acquire sufficient labeled activity data. To solve this problem, transfer learning leverages the labeled samples from the source domain to annotate the target domain which has few or none labels. Unfortunately, when there are several source domains available, it is difficult to select the right source domains for transfer. The right source domain means that it has the most similar properties with the target domain, thus their similarity is higher, which can facilitate transfer learning. Choosing the right source domain helps the algorithm perform well and prevents the negative transfer. In this paper, we propose an effective Unsupervised Source Selection algorithm for Activity Recognition (USSAR). USSAR is able to select the most similar K source domains from a list of available domains. After this, we propose an effective Transfer Neural Network to perform knowledge transfer for Activity Recognition (TNNAR). TNNAR could capture both the time and spatial relationship between activities while transferring knowledge. Experiments on three public activity recognition datasets demonstrate that: 1) The USSAR algorithm is effective in selecting the best source domains. 2) The TNNAR method can reach high accuracy when performing activity knowledge transfer.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e373.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e373.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>USSAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unsupervised Source Selection for Activity Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised algorithm that selects the most relevant K source domains for cross-position/activity transfer by combining a general domain-distance (A-distance) with activity-specific semantic and kinetic distances, and using a greedy selection strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Unsupervised source-domain selection (USSAR)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Computes a composite Context Activity Distance (CAD) between the (unlabeled) target domain and each candidate source domain as a weighted sum of: (1) a general distance measured by A-distance (derived from a linear domain classifier error), and (2) a specificity term combining semantic weights (human-assigned weights reflecting spatial/functional closeness of body parts) and kinetic similarity (cosine similarity between basic signal vectors). After computing CAD for each source, domains are sorted by CAD and a greedy set-construction adds the smallest-CAD domain first and then further domains if their distance to the current selected set is smaller than to the target. The method requires no labels in the target domain and outputs the selected K source domains for subsequent transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / algorithm for source selection</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general transfer learning / machine learning (domain selection techniques)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>human activity recognition (sensor-based cross-position activity recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Combined an established general domain-distance (A-distance) with activity-specific components: (a) semantic weighting of source domains (weights sum to 1; assigned based on body-part functional/spatial relationships), and (b) kinetic similarity using cosine similarity on raw sensor-channel vectors; then defined CAD = A-distance + λ * specific_distance and implemented a greedy selection rule tailored to multi-source CPAR. These adaptations were made because generic distances ignore body-part semantics and kinetics, and because target labels are unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful — USSAR produced better source selections (leading to higher downstream classification accuracy) than baseline selection metrics (A-distance, SDM, Greedy baseline, MMD) in experiments combining OPP and DSADS datasets for K = 1..4; results plotted in Figure 4 and discussed qualitatively as 'effective' (no single numeric aggregate provided for USSAR-only, but SVM classification accuracy on selected sources was used to show superiority).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Lack of ground-truth for 'correct' source domains (no oracle distance), absence of target labels, heterogeneous signal distributions across body parts, and need to choose semantic weights (which are subjective).</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Similarity assumptions between some body parts (spatial/functional relationships), availability of multiple candidate source domains, and combining complementary distance measures (A-distance for global distribution gap and cosine-weighted kinetic/semantic term for activity-specific relatedness).</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires human-informed semantic weights (or a practitioner-defined scheme) for body-part relations; access to raw multichannel sensor data for computing kinetic similarity; computation of A-distance requires training a linear domain classifier on pooled data from two domains; choice of trade-off parameter λ must be set (authors performed sensitivity analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderately generalizable across CPAR problems and similar multi-source sensor transfer tasks — authors claim applicability to many activity recognition tasks and demonstrate across multiple datasets; method depends on being able to define semantic relations between domains, so application to wholly different domains would require analogous semantic/kinetic definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and interpretive framework (combining statistical distance and domain-specific semantic/kinetic modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Transfer Learning for Cross-domain Activity Recognition', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e373.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e373.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TNNAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer Neural Network for Activity Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end deep neural network architecture tailored for cross-domain activity recognition that jointly learns task classification and domain adaptation by combining convolutional spatial feature extraction, an LSTM for temporal modeling, and an adaptation layer minimizing distribution discrepancy (MMD).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Transfer Neural Network for Activity Recognition (TNNAR)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>A supervised/source-labeled deep network that (1) inputs raw multi-channel wearable sensor signals (treating each sensor channel as a separate input channel), (2) applies two 1D convolutional blocks with max-pooling to extract spatial/channel-local features, (3) passes the sequence through an LSTM layer to capture temporal dependencies, (4) processes via two fully connected layers, and (5) computes classification loss (cross-entropy) on source labels plus an adaptation loss computed at an intermediate fully-connected layer using Maximum Mean Discrepancy (MMD) to reduce source-target distribution divergence; training is by mini-batch SGD with joint optimization of classification and adaptation losses (gradient includes both terms). Network hyperparameters used in experiments include learning rate 0.001, dropout 0.8, batch size 64, 9 input channels, convolution kernel 64×1 depth 32, and linear-time MMD.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / deep learning architecture with domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>deep transfer learning methods developed in machine learning / computer vision / general domain adaptation literature</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>human activity recognition from wearable sensors (cross-position/dataset transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Architecture was tailored to sensor-based HAR by: (a) using 1D convolutions on per-channel sensor streams (instead of 2D image convolutions), (b) inserting an LSTM layer after convolutional blocks to explicitly capture temporal structure of activities, (c) placing the adaptation (MMD) layer after the first fully-connected layer (the authors argue higher layers are more domain-specific and need adaptation), and (d) using linear-time MMD for efficiency. Hyperparameters (kernel size, depth, dropout, learning rate) and data batching (64 per domain) were chosen to suit sensor signal characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful — TNNAR outperformed comparison methods (PCA, KPCA, TCA, GFK, TKL, STL) across single-source and multi-source CPAR tasks and achieved the highest average accuracy; authors report an average improvement of ~3.42% over the best baseline (STL) and show numerical results in Tables 2 and 3 (e.g., in Table 3 average accuracy for TNNAR = 77.01% vs STL = 73.18%).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Domain distribution discrepancies between body parts/datasets; potential negative transfer when poor source domains are included; requirement for labeled source domains and sufficient data to train deep models; need for careful hyperparameter setting (trade-off µ between classification and adaptation); computational resources for training deep models.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Architectural choices aligned with activity data characteristics (1D conv + LSTM), joint optimization of classification and adaptation objectives, use of multiple relevant source domains selected by USSAR, and computationally efficient linear-time MMD for the adaptation loss.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Labeled source-domain data, unlabeled target-domain samples for adaptation term, computational resources for training neural networks (GPU recommended), design/selection of adaptation kernel for MMD, and tuning of trade-off hyperparameter µ and network hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Authors claim applicability to a wide range of cross-position activity recognition tasks and show experiments on three public datasets (OPPORTUNITY, PAMAP2, DSADS). The approach is likely generalizable to other sequential sensor-based transfer tasks but would require architectural/hyperparameter adaptation for heterogeneous modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps, instrumental/technical skills (neural network architecture design and joint optimization), and some tacit design choices (selection of where to place adaptation layer)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Transfer Learning for Cross-domain Activity Recognition', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e373.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e373.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMD adaptation (linear-time)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum Mean Discrepancy adaptation (linear-time MMD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A statistical distance metric (MMD) embedded as an adaptation loss in the neural network to reduce distribution discrepancy between source and target feature representations; authors used a linear-time approximation for efficiency in mini-batch training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep Transfer Learning for Cross-domain Activity Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>MMD-based domain adaptation (linear-time MMD)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>MMD measures squared distance between mean embeddings of two distributions in a reproducing kernel Hilbert space (RKHS): d^2(p,q) = ||E_p[ϕ(z)] - E_q[ϕ(z)]||^2_H. In TNNAR, the MMD between source and target feature representations at a chosen network layer is computed and added as an adaptation loss term (weighted by µ) to the classification loss; the authors use a linear-time estimator of MMD for computational efficiency within SGD mini-batch updates. Gradients of the MMD term are computed and backpropagated jointly with classification gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>statistical distance metric used as adaptation loss / data analysis technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>statistical learning / domain adaptation literature</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>embedded as an adaptation layer inside a deep neural network for wearable-sensor human activity recognition</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application with efficiency adaptation (linear-time estimator) and integration into a tailored network</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Employed a linear-time (mini-batch) estimator of MMD suitable for stochastic gradient training and placed the MMD loss after the first fully-connected layer (chosen because higher-layer representations are more domain-specific); jointly optimized with classification loss.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful — when used inside TNNAR it contributed to improved cross-domain performance and overall state-of-the-art results in the reported CPAR experiments (see Tables 2 and 3); exact contribution isolated not fully ablated in the paper but reported as part of TNNAR's gains.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Choice of kernel and estimator affects effectiveness; MMD alone may be too generic for activity-specific relationships, motivating use together with USSAR and tailored architecture; requires careful balancing via hyperparameter µ.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Theoretical grounding of MMD, ability to compute a linear-time estimator for SGD, and placement at a layer where features are amenable to alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to unlabeled target samples per mini-batch, selection of kernel (or linear MMD variant), and integration with network backpropagation; appropriate batch sizes to obtain reliable estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Highly generalizable — MMD is widely used in domain adaptation across modalities; the linear-time estimator and in-network placement strategy make it applicable to other deep-transfer tasks, especially sequential/sensor data after analogous architectural tailoring.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and theoretical principles (statistical test embedded as loss)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Transfer Learning for Cross-domain Activity Recognition', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep domain confusion: Maximizing for domain invariance <em>(Rating: 2)</em></li>
                <li>Domain-adversarial training of neural networks <em>(Rating: 2)</em></li>
                <li>Deep Transfer Learning with Joint Adaptation Networks <em>(Rating: 2)</em></li>
                <li>Source Free Transfer Learning for Text Classification <em>(Rating: 1)</em></li>
                <li>Domain transfer multiple kernel learning <em>(Rating: 1)</em></li>
                <li>Domain adaptation via transfer component analysis <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-373",
    "paper_id": "paper-49902207",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "USSAR",
            "name_full": "Unsupervised Source Selection for Activity Recognition",
            "brief_description": "An unsupervised algorithm that selects the most relevant K source domains for cross-position/activity transfer by combining a general domain-distance (A-distance) with activity-specific semantic and kinetic distances, and using a greedy selection strategy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Unsupervised source-domain selection (USSAR)",
            "procedure_description": "Computes a composite Context Activity Distance (CAD) between the (unlabeled) target domain and each candidate source domain as a weighted sum of: (1) a general distance measured by A-distance (derived from a linear domain classifier error), and (2) a specificity term combining semantic weights (human-assigned weights reflecting spatial/functional closeness of body parts) and kinetic similarity (cosine similarity between basic signal vectors). After computing CAD for each source, domains are sorted by CAD and a greedy set-construction adds the smallest-CAD domain first and then further domains if their distance to the current selected set is smaller than to the target. The method requires no labels in the target domain and outputs the selected K source domains for subsequent transfer.",
            "procedure_type": "computational method / algorithm for source selection",
            "source_domain": "general transfer learning / machine learning (domain selection techniques)",
            "target_domain": "human activity recognition (sensor-based cross-position activity recognition)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Combined an established general domain-distance (A-distance) with activity-specific components: (a) semantic weighting of source domains (weights sum to 1; assigned based on body-part functional/spatial relationships), and (b) kinetic similarity using cosine similarity on raw sensor-channel vectors; then defined CAD = A-distance + λ * specific_distance and implemented a greedy selection rule tailored to multi-source CPAR. These adaptations were made because generic distances ignore body-part semantics and kinetics, and because target labels are unavailable.",
            "transfer_success": "successful — USSAR produced better source selections (leading to higher downstream classification accuracy) than baseline selection metrics (A-distance, SDM, Greedy baseline, MMD) in experiments combining OPP and DSADS datasets for K = 1..4; results plotted in Figure 4 and discussed qualitatively as 'effective' (no single numeric aggregate provided for USSAR-only, but SVM classification accuracy on selected sources was used to show superiority).",
            "barriers_encountered": "Lack of ground-truth for 'correct' source domains (no oracle distance), absence of target labels, heterogeneous signal distributions across body parts, and need to choose semantic weights (which are subjective).",
            "facilitating_factors": "Similarity assumptions between some body parts (spatial/functional relationships), availability of multiple candidate source domains, and combining complementary distance measures (A-distance for global distribution gap and cosine-weighted kinetic/semantic term for activity-specific relatedness).",
            "contextual_requirements": "Requires human-informed semantic weights (or a practitioner-defined scheme) for body-part relations; access to raw multichannel sensor data for computing kinetic similarity; computation of A-distance requires training a linear domain classifier on pooled data from two domains; choice of trade-off parameter λ must be set (authors performed sensitivity analysis).",
            "generalizability": "Moderately generalizable across CPAR problems and similar multi-source sensor transfer tasks — authors claim applicability to many activity recognition tasks and demonstrate across multiple datasets; method depends on being able to define semantic relations between domains, so application to wholly different domains would require analogous semantic/kinetic definitions.",
            "knowledge_type": "explicit procedural steps and interpretive framework (combining statistical distance and domain-specific semantic/kinetic modeling)",
            "uuid": "e373.0",
            "source_info": {
                "paper_title": "Deep Transfer Learning for Cross-domain Activity Recognition",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "TNNAR",
            "name_full": "Transfer Neural Network for Activity Recognition",
            "brief_description": "An end-to-end deep neural network architecture tailored for cross-domain activity recognition that jointly learns task classification and domain adaptation by combining convolutional spatial feature extraction, an LSTM for temporal modeling, and an adaptation layer minimizing distribution discrepancy (MMD).",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Transfer Neural Network for Activity Recognition (TNNAR)",
            "procedure_description": "A supervised/source-labeled deep network that (1) inputs raw multi-channel wearable sensor signals (treating each sensor channel as a separate input channel), (2) applies two 1D convolutional blocks with max-pooling to extract spatial/channel-local features, (3) passes the sequence through an LSTM layer to capture temporal dependencies, (4) processes via two fully connected layers, and (5) computes classification loss (cross-entropy) on source labels plus an adaptation loss computed at an intermediate fully-connected layer using Maximum Mean Discrepancy (MMD) to reduce source-target distribution divergence; training is by mini-batch SGD with joint optimization of classification and adaptation losses (gradient includes both terms). Network hyperparameters used in experiments include learning rate 0.001, dropout 0.8, batch size 64, 9 input channels, convolution kernel 64×1 depth 32, and linear-time MMD.",
            "procedure_type": "computational method / deep learning architecture with domain adaptation",
            "source_domain": "deep transfer learning methods developed in machine learning / computer vision / general domain adaptation literature",
            "target_domain": "human activity recognition from wearable sensors (cross-position/dataset transfer)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Architecture was tailored to sensor-based HAR by: (a) using 1D convolutions on per-channel sensor streams (instead of 2D image convolutions), (b) inserting an LSTM layer after convolutional blocks to explicitly capture temporal structure of activities, (c) placing the adaptation (MMD) layer after the first fully-connected layer (the authors argue higher layers are more domain-specific and need adaptation), and (d) using linear-time MMD for efficiency. Hyperparameters (kernel size, depth, dropout, learning rate) and data batching (64 per domain) were chosen to suit sensor signal characteristics.",
            "transfer_success": "successful — TNNAR outperformed comparison methods (PCA, KPCA, TCA, GFK, TKL, STL) across single-source and multi-source CPAR tasks and achieved the highest average accuracy; authors report an average improvement of ~3.42% over the best baseline (STL) and show numerical results in Tables 2 and 3 (e.g., in Table 3 average accuracy for TNNAR = 77.01% vs STL = 73.18%).",
            "barriers_encountered": "Domain distribution discrepancies between body parts/datasets; potential negative transfer when poor source domains are included; requirement for labeled source domains and sufficient data to train deep models; need for careful hyperparameter setting (trade-off µ between classification and adaptation); computational resources for training deep models.",
            "facilitating_factors": "Architectural choices aligned with activity data characteristics (1D conv + LSTM), joint optimization of classification and adaptation objectives, use of multiple relevant source domains selected by USSAR, and computationally efficient linear-time MMD for the adaptation loss.",
            "contextual_requirements": "Labeled source-domain data, unlabeled target-domain samples for adaptation term, computational resources for training neural networks (GPU recommended), design/selection of adaptation kernel for MMD, and tuning of trade-off hyperparameter µ and network hyperparameters.",
            "generalizability": "Authors claim applicability to a wide range of cross-position activity recognition tasks and show experiments on three public datasets (OPPORTUNITY, PAMAP2, DSADS). The approach is likely generalizable to other sequential sensor-based transfer tasks but would require architectural/hyperparameter adaptation for heterogeneous modalities.",
            "knowledge_type": "explicit procedural steps, instrumental/technical skills (neural network architecture design and joint optimization), and some tacit design choices (selection of where to place adaptation layer)",
            "uuid": "e373.1",
            "source_info": {
                "paper_title": "Deep Transfer Learning for Cross-domain Activity Recognition",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "MMD adaptation (linear-time)",
            "name_full": "Maximum Mean Discrepancy adaptation (linear-time MMD)",
            "brief_description": "A statistical distance metric (MMD) embedded as an adaptation loss in the neural network to reduce distribution discrepancy between source and target feature representations; authors used a linear-time approximation for efficiency in mini-batch training.",
            "citation_title": "Deep Transfer Learning for Cross-domain Activity Recognition",
            "mention_or_use": "use",
            "procedure_name": "MMD-based domain adaptation (linear-time MMD)",
            "procedure_description": "MMD measures squared distance between mean embeddings of two distributions in a reproducing kernel Hilbert space (RKHS): d^2(p,q) = ||E_p[ϕ(z)] - E_q[ϕ(z)]||^2_H. In TNNAR, the MMD between source and target feature representations at a chosen network layer is computed and added as an adaptation loss term (weighted by µ) to the classification loss; the authors use a linear-time estimator of MMD for computational efficiency within SGD mini-batch updates. Gradients of the MMD term are computed and backpropagated jointly with classification gradients.",
            "procedure_type": "statistical distance metric used as adaptation loss / data analysis technique",
            "source_domain": "statistical learning / domain adaptation literature",
            "target_domain": "embedded as an adaptation layer inside a deep neural network for wearable-sensor human activity recognition",
            "transfer_type": "direct application with efficiency adaptation (linear-time estimator) and integration into a tailored network",
            "modifications_made": "Employed a linear-time (mini-batch) estimator of MMD suitable for stochastic gradient training and placed the MMD loss after the first fully-connected layer (chosen because higher-layer representations are more domain-specific); jointly optimized with classification loss.",
            "transfer_success": "successful — when used inside TNNAR it contributed to improved cross-domain performance and overall state-of-the-art results in the reported CPAR experiments (see Tables 2 and 3); exact contribution isolated not fully ablated in the paper but reported as part of TNNAR's gains.",
            "barriers_encountered": "Choice of kernel and estimator affects effectiveness; MMD alone may be too generic for activity-specific relationships, motivating use together with USSAR and tailored architecture; requires careful balancing via hyperparameter µ.",
            "facilitating_factors": "Theoretical grounding of MMD, ability to compute a linear-time estimator for SGD, and placement at a layer where features are amenable to alignment.",
            "contextual_requirements": "Access to unlabeled target samples per mini-batch, selection of kernel (or linear MMD variant), and integration with network backpropagation; appropriate batch sizes to obtain reliable estimates.",
            "generalizability": "Highly generalizable — MMD is widely used in domain adaptation across modalities; the linear-time estimator and in-network placement strategy make it applicable to other deep-transfer tasks, especially sequential/sensor data after analogous architectural tailoring.",
            "knowledge_type": "explicit procedural steps and theoretical principles (statistical test embedded as loss)",
            "uuid": "e373.2",
            "source_info": {
                "paper_title": "Deep Transfer Learning for Cross-domain Activity Recognition",
                "publication_date_yy_mm": "2018-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep domain confusion: Maximizing for domain invariance",
            "rating": 2,
            "sanitized_title": "deep_domain_confusion_maximizing_for_domain_invariance"
        },
        {
            "paper_title": "Domain-adversarial training of neural networks",
            "rating": 2,
            "sanitized_title": "domainadversarial_training_of_neural_networks"
        },
        {
            "paper_title": "Deep Transfer Learning with Joint Adaptation Networks",
            "rating": 2,
            "sanitized_title": "deep_transfer_learning_with_joint_adaptation_networks"
        },
        {
            "paper_title": "Source Free Transfer Learning for Text Classification",
            "rating": 1,
            "sanitized_title": "source_free_transfer_learning_for_text_classification"
        },
        {
            "paper_title": "Domain transfer multiple kernel learning",
            "rating": 1,
            "sanitized_title": "domain_transfer_multiple_kernel_learning"
        },
        {
            "paper_title": "Domain adaptation via transfer component analysis",
            "rating": 1,
            "sanitized_title": "domain_adaptation_via_transfer_component_analysis"
        }
    ],
    "cost": 0.01251125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Deep Transfer Learning for Cross-domain Activity Recognition
20 Jul 2018</p>
<p>Jindong Wang wangjindong@ict.ac.cn 
Jindong Wang and Yiqiang Chen are also with Beijing Key Lab of Mobile Computing and Pervasive Devices
University of Chinese Academy of Sciences</p>
<p>Vincent W Zheng vincent.zheng@adsc.com.sg 
Yiqiang Chen yqchen@ict.ac.cn 
Meiyu Huang huangmeiyu@qxslab.cn </p>
<p>Institute of Computing Technology
CASBeijingChina</p>
<p>Advanced Digital Sciences Center
Singapore</p>
<p>Institute of Computing Technology
CASBeijingChina</p>
<p>Qian Xuesen Laboratory of Space Technology
BeijingChina</p>
<p>Deep Transfer Learning for Cross-domain Activity Recognition
20 Jul 201877F60D9363014C5515F1DAFD8EEBD7DCarXiv:1807.07963v1[eess.IV]Transfer LearningActivity RecognitionDeep LearningDomain Adaptation
Human activity recognition plays an important role in people's daily life.However, it is o en expensive and time-consuming to acquire su cient labeled activity data.To solve this problem, transfer learning leverages the labeled samples from the source domain to annotate the target domain which has few or none labels.Unfortunately, when there are several source domains available, it is di cult to select the right source domains for transfer.e right source domain means that it has the most similar properties with the target domain, thus their similarity is higher, which can facilitate transfer learning.Choosing the right source domain helps the algorithm perform well and prevents the negative transfer.In this paper, we propose an e ective Unsupervised Source Selection algorithm for Activity Recognition (USSAR).USSAR is able to select the most similar K source domains from a list of available domains.After this, we propose an e ective Transfer Neural Network to perform knowledge transfer for Activity Recognition (TNNAR).TNNAR could capture both the time and spatial relationship between activities while transferring knowledge.Experiments on three public activity recognition datasets demonstrate that: 1) e USSAR algorithm is e ective in selecting the best source domains.2) e TNNAR method can reach high accuracy when performing activity knowledge transfer.</p>
<p>INTRODUCTION</p>
<p>Human activity recognition (HAR) aims to seek the profound highlevel information from the low-level sensor inputs [39].For example, we can predict if a person is walking or running using the on-body sensors such as a smartphone or wristband.HAR has been Firstly, the activity signals on di erent body parts are o en di erent.Secondly, if the labels of a certain part are missing (the red pentacle), how to leverage the well-labeled activity data on other body parts (the blue dots) to acquire its labels?widely applied to many applications such as indoor localization [43], sleep state detection [46], and smart home sensing [41].</p>
<p>ere are di erent activity pa erns of di erent body parts, so sensors can be put on them to collect activity data and then build machine learning models.e combination of signals from di erent body parts can be used to re ect meaningful knowledge such as person's detailed health information [18] and working states [33].Unfortunately, the real situation is that we either do not want all body parts to be equipped with sensors, or the data on certain body part may be easily missing.In these situations, we are unable to train models to recognize the activities of some body parts.Figure 1 illustrates this situation.If the activity information on the arm (the red pentacle, we call it the target domain) is missing and we want to predict the activities on this body part, how to utilize the information on other parts (such as torso or leg, we call them the source domains) to help build the model? is problem is referred to as the cross-domain activity recognition (CDAR).</p>
<p>is problem is extremely challenging.Firstly, we do not know which body parts are most similar to the target domain since the sensor signals are not independent, but are highly correlated because of the shared body structures and functions.If we use all the body parts, there is likely to be negative transfer [32] because some body parts may be dissimilar.Secondly, we only have the raw activity data on the target domain without the actual activity  labels, making it harder to measure the similarities.irdly, even we already know the similar body parts to the target domain, it is also di cult to build a good machine learning model using both the source and target domains.e reason is that the signal on different domains are following exactly di erent distributions, which means there are distribution discrepancies between them.However, traditional machine learning models are built by assuming that all signals are with the same distribution.Moreover, existing neural networks are too generic to capture both the time and spatial information in the activity data.Fourthly, when it comes to di erent persons, there are also similar body parts across persons (Fig. 2).</p>
<p>is makes the problem more challenging.</p>
<p>In this paper, we aim to tackle above challenges through multiple source selection and deep neural network.For multiple source selection, we calculate the distance between available sources and the target domains to select the most similar source domains.Our intuition is that the sensor signals may consist of generic and speci c relationships about the body parts: the generic relationship means the data distance between two signals such as the Euclidean distance or cosine similarity; the speci c relationship refers to the similar moving pa erns or body functions between two body parts.By calculating these two signi cant distances, we can correctly measure the distances between di erent body parts, and thus select the right source domains.Our algorithm does not depend on the availability of the target domain labels.We call this algorithm Unsupervised Source Selection for Activity Recognition (USSAR).</p>
<p>A er obtaining the right source domains via the USSAR algorithm, we propose a Transfer Neural Network for Activity Recognition (TNNAR).TNNAR is an end-to-end neural network to perform knowledge transfer across di erent domains.e important thing is that in order to reduce the distance between two domains, we add an adaptation layer in the network to calculate the adaptation loss, which can be optimized jointly with the classi cation loss.</p>
<p>e main structure of TNNAR is two convolutional blocks with max-pooling operations, one LSTM layer, and two fully-connected layers.We use the convolutional layers to extract the spatial features from the original activity data.e LSTM layer is mainly for capturing the time features [39].</p>
<p>e fully-connected layers are used for nonlinear transformation.Finally, a so max function is applied for classi cation.</p>
<p>To validate the performance of our USSAR and TNNAR algorithms, we conduct extensive experiments on three large-scale public activity recognition datasets: OPPORTUNITY, PAMAP2, and UCI DSADS.All of them contains multiple domains from several body parts.We perform cross-domain activity recognition across di erent body parts.e Experimental results demonstrate that US-SAR is e ective in choosing the right source domains, and TNNAR achieves the best accuracy in activity transfer.</p>
<p>e main contributions of this paper are summarized as follows: 1) We propose the rst unsupervised source selection algorithm for activity recognition.USSAR measures both the general and speci c characteristics of activity information, hence it is capable of capturing the profound relationship between di erent domains.</p>
<p>2) We propose an end-to-end transfer neural network for crossdomain activity recognition.Di erent from existing deep transfer learning methods that need to extract features from human knowledge, our TNNAR can simultaneously perform classi cation and adaptation between two activity domains on the original data.</p>
<p>3) We evaluate the proposed USSAR and TNNAR algorithms in extensive experiments on cross-position activity recognition.erefore, our algorithms can be applied to many applications to increase the generalization ability of activity models.e rest of this paper is organized as follows.Section 2 presents the related work.In Section 3.2, we introduce the USSAR algorithm.In Section 3.3, we articulate the TNNAR method.Section 4 provides extensive experiments to evaluate the proposed algorithms.Finally, Section 5 concludes this paper and presents future work.</p>
<p>RELATED WORK</p>
<p>Our work is mainly related to activity recognition and transfer learning.We will rst brie y review the recent advance on these two areas.en, we will discuss the di erence between our work and recent work on source selection and transfer network.</p>
<p>Activity Recognition</p>
<p>Human Activity recognition has been a popular research topic in pervasive computing [6] for its competence in learning profound high-level knowledge about human activity from raw sensor inputs.Several survey articles have elaborated the recent advance of activity recognition using conventional machine learning [6,21] and deep learning [39] approaches.</p>
<p>Conventional machine learning approaches have made tremendous progress on HAR by adopting machine learning algorithms such as similarity-based approach [9,48], active learning [19], crowdsourcing [22], and other semi-supervised methods [20,29].</p>
<p>ose methods typically treat HAR as a standard time series classication problem.And they tend to solve it by subsequently performing preprocessing procedures, feature extraction, model building, and activity inference.However, they all assume that the training and test data are with the same distribution.As for CDAR where the training (source) and the test (target) data are from di erent feature distributions, those conventional methods are prune to under-ing since their generalization ability will be undermined [32].</p>
<p>Deep learning based HAR [33,39] achieves the state-of-the-art performance than conventional machine learning approaches.e reason is that deep learning is capable of automatically extracting high-level features from the raw sensor readings [23].erefore, the features are likely to be more domain-invariant and tend to perform be er for cross-domain tasks.A recent work evaluated deep models for cross-domain HAR [28], which provides some experience for future research on this area.ere are still many open problems for deep learning based CDAR.</p>
<p>Transfer Learning</p>
<p>Transfer learning has been successfully applied in many applications such as Wi-Fi localization [30], natural language processing [4], and visual object recognition [12].According to the literature survey [32], transfer learning can be categorized into 3 types: instance-based, parameter-based, and feature-based methods.</p>
<p>Instance-based methods perform knowledge transfer mainly through instance re-weighting techniques [7,36].Parameter-based methods [44,47] rst train a model using the labeled source domain, then perform clustering on the target domain.Feature-based methods [15,31,38] learn a feature transformation between domains when the distance can be minimized.A detailed survey on transfer learning for activity recognition is conducted in [11].</p>
<p>Our work di ers from transfer learning in the following two aspects: 1) Source selection e work [42] rst proposed a source-selection free transfer learning approach.ey choose the source samples that are close to the target samples using the Laplacian Eigenmap.</p>
<p>e work [27] followed this idea in the text classi cation.However, both of them only focused on the sample selection, while our USSAR focuses on the selection of the whole domain.Collier et al. [10] investigated the transfer performance of di erent layers of a neural network in a grid search manner.But they did not perform source selection.e work [35] developed a relation network, which can be used to evaluate the distance between di erent image samples.Yet they still focused on the single sample.Authors in [3] proposed a greedy multi-source selection algorithm.is selection algorithm could iteratively select the best K source domains and then perform transfer learning based on this selection.However, their method is too general to focus on the domain-speci c features of activity recognition.</p>
<p>2</p>
<p>) Transfer network</p>
<p>In recent years, the deep transfer learning methods have dramatically increased the learning performance of transfer learning tasks.Some popular deep transfer learning methods include Deep Domain Confusion (DDC) [37], Joint Adaptation Network (JAN) [26], and Domain adversarial Neural Network (DANN) [14].ey are typically based on deep neural networks, where they add an adaptation layer to reduce the distribution divergence between domains.Compared to existing deep transfer learning methods, our TNNAR network is tailored according to the characteristics of activity recognition.Hence, we not only consider the generic transfer learning scenario, but also focus on the spatial and time relationship between activities.</p>
<p>OUR METHOD</p>
<p>In this section, we describe our Unsupervised Source Selection and Transfer Neural Network for activity recognition.Before that, we will rst formally give the problem de nition.</p>
<p>Problem de nition</p>
<p>Assume we have an activity domain D t = {x j t } n t j=1 as the target domain which we want to learn its corresponding activity labels y t , i.e. to predict the person's activity state based on the sensor signals.Suppose we have C activity states (labels).ere are M labeled source domains available:
{D i s } M i=1 . Each source domain D i s = {x j s , j s } n i s j=1
. Note that the data distributions are not the same, i.e.P(x s ) P(x t ).We need to design algorithms to: 1) select the best K(K &lt; M) source domains (we denote them as D s (K)), and 2) perform e ective transfer learning from D s (K) to D t in order to obtain y t .</p>
<p>Unsupervised Source Selection</p>
<p>Since the target domain has no labels, it is challenging to measure the distance between D t and D i s .Moreover, the activity signals are not only normal time series data, there are more mixed information and relationships in di erent domains, such as the correlated body functions and pa erns.Existing distance measurements such as Maximum Mean Discrepancy (MMD) [5] and A distance [2] are too generic for our problem.ey only calculate the data distance without considering the relationship between body parts.erefore, we should develop a comprehensive measurement to evaluate the distance between di erent activity domains.</p>
<p>In this paper, we propose the Unsupervised Source Selection for Activity Recognition (USSAR) algorithm to e ectively select the right source domains for the target activity domain.Our USSAR well considers the generality and speci city of activity while selecting source domains.Generality means that we have to seek the general relation between activity data, which is a common problem in machine learning.More importantly, speci city means that we should consider the speci c information behind di erent activity domains.Formally, if we denote D(A, B) as the distance between domains A and B, then it can be represented as
D(A, B) = D (A, B) + λD s (A, B)
(1) where D (A, B) is the general distance ( for general) and D s (A, B) is the speci c distance (s for speci c).λ ∈ [0, 1] is the trade-o factor between two terms.e general distance D (A, B) can be easily computed by the well-established A-distance [2]:
D (A, B) = 2(1 − 2ϵ)
(2) where ϵ is the error to classify domains A and B. In order to obtain ϵ, we train a linear binary classi er h on A and B, where A has the label +1 and B has the label −1 (or vice versa).en, we apply prediction on both domains to get the error ϵ.</p>
<p>e speci c distance D s (A, B) is composed of two important aspects from activity recognition: the semantic and the kinetic information.e semantic information refers to the spatial relationship between two domains, i.e. close spatial relations lead to the similar property.e kinetic information refers to the activity signal relationship between domains, i.e. close signal relations lead to the shorter distance.We take the following approaches to calculate the semantic and kinetic distances.</p>
<p>Semantic distance: We basically give each source domain a weight w ∈ [0, 1] indicating its spatial relationship with the target domain.erefore, if there are M source domains available, there will be M weights: {w i } M i=1 .Currently, the weighting technique is only based on human experience, i.e. we give relatively small weights if we think two domains are not closely related, while we give relatively large weights if we think two domains are closely related.For instance, if the target domain is the Right Hand, we would probably give a larger weight to the source domain Le Hand since these two body parts are always correlated; on the other hand, we would probably give the source domain Torso a small weight since these two body parts are not exactly correlated.Since w i is a weight, we bound it by:
M i=1 w i = 1(3)
Kinetic distance: ere are many approaches available to approximate the signal relationship between two domains.is relationship can be captured by the consistency between two signals.For instance, the Pearson correlation score and the maximum mean discrepancy can be used to calculate this relationship.In this paper, we adopt the well-established Cosine similarity function.Speci cally, given two domains A and B, their cosine similarity is formulated as:
cos(A, B) = E       a,b a • b |a • b|       (4)
where a, b are the basic vectors in A and B, respectively.E[•] is the expectation of samples.Note that when one domain is the target domain, its weight can be set to 1. Once the kinetic distance is calculated, we combine it with the weights generated by the semantic distance, and nally get the speci city distance as:
d s (A, B) = E       a,b w a a • w b b |w a a • w b b|       (5)
Overall distance: we combine the general distance (Eq.2) and the speci c distance (Eq.5) to get the nal distance expression:
d(A, B) = 2(1 − 2ϵ) + λE       a,b w a a • w b b |w a a • w b b|       (6)
Note that this distance measurement does not rely on the labels of the target domain.We call this distance the Context Activity Distance (CAD).Once we have the CAD, we can perform source selection from many available source domains.In this paper, we propose a greedy algorithm to select the available sources.We regard the source selection problem as the process of constructing a nite set S. Initially, we calculate all the CADs between the target domain and every source domain and sort them in an increasing order.At this time, the set S = ϕ.en, we add the source domain with the smallest CAD (denoted as D min s )to S: S = D min s .A er that, we add other K − 1 source domains to S according to the greedy technique: if d(S, D i s ) &lt; d(D t , D i s ), we add D i s to S. e USSAR algorithm is illustrated in Algorithm 1.</p>
<p>Transfer Neural Network</p>
<p>A er obtaining the selected source domains D s (K), we can perform knowledge transfer across the target and the source domains.In this paper, we propose a neural network to accomplish this</p>
<p>Algorithm 1 USSAR: Unsupervised Source Selection for Activity Recognition</p>
<p>Input: M available source domains {D i s } M i=1 , target domain D t , the number of selected source domains K(K &lt;&lt; M) Output: e selected source domain set S. 1: Calculate the CAD between each source domain and target domain using Eq. ( 6), and sort them in an increasing order; Else i = i + 1; 7: until i = K 8: return S.</p>
<p>transfer.Generally speaking, the goal of the transfer neural network is to learn the classi ers = f s (x) and = f t (x) and minimize their discrepancies.So the expected target risk is bounded:
R t (f t ) = E (x, ) [f t (x)
]. Since there are discrepancies between the source and target domain, the general form of the function f can be expressed as:
f (x) = c (x) + µ a (D s , D t )(7)
where c (x) is the classi cation loss on the labeled data (source domain), and a (•, •) is the adaptation loss on both of the source and target domains.µ ∈ [0, 1] is the trade-o factor.is is a general form of a transfer neural network.In crossdomain activity recognition se ing, the structure of the neural network has to be modi ed according to the characteristics of activity recognition.In this paper, we propose Transfer Neural Network for Activity Recognition (TNNAR).e structure of our TNNAR is illustrated in Figure 3. e proposed TNNAR consists of two convolutional layers with max-pooling layers, one LSTM layer, and two fully-connected (fc) layers.e convolutional layers are adopted to extract the spatial features for the activities, while the LSTM layer is adopted to capture the time relationship between activities.e fully-connected layer is acting as the classi cation function.</p>
<p>Beyond this simple structure, we also add an adaptation layer to reduce the discrepancy between two domains.e reason we add this layer a er the rst fully-connected layer is that the features are becoming speci c to the higher layer, making adaptation more urgent.</p>
<p>e low-level layers are only extracting some common features, thus they do not need to be adapted [45].e adaptation layer computes the adaptation loss, which can be optimized jointly with the classi cation loss.</p>
<p>We denote W l and b l the weights and bias at fc layer l.Each fc layer l will learn a nonlinear mapping h l = f l (W l h l −1 i + b l ), where h l is the lth layer hidden representation and f l is the activation function.</p>
<p>e activation f l for the last fc layer is computed as f l (x) = e x / |x | j=1 e x j .e other fc layer takes the ReLU units f l = max(0, x).If we denote Θ = {W l , b l } the hyperparameters of the neural network, then the empirical risk is
= min Θ 1 n b b i=1 (Θ(x b i ), b i ) (8)
where is the cross-entropy function.(x b i , b i ) denotes all the labeled samples from the source domain.</p>
<p>As for the adaptation layer, we adopted the well-established Maximum Mean Discrepancy (MMD) [5] as the measurement to reduce the discrepancy between domains.MMD is a popular distance metric, which has been widely used in many existing work [24,31], and its e ectiveness has been veri ed in [16].e MMD distance between distributions p and q is de ned as
d 2 (p, q) = (E p [ϕ(z s )] − E q [ϕ(z t )]) 2 H K
where H K is the reproducing kernel Hilbert space (RKHS) induced by feature map ϕ(•).Here, E[•] denotes the mean of the embedded samples.erefore, the MMD distance between the source and target domain is
MMD(D s , D t ) = E[x s ] − E[x t ] 2 H K (9)
We train the TNNAR using mini-batch Stochastic Gradient Descent (SGD) strategy.gradient can be calculated as
∆ Θ l = ∂ (•) ∂Θ l + µ ∂ a (•) ∂Θ l (10)</p>
<p>EXPERIMENTAL EVALUATION</p>
<p>In this section, we evaluate the performance of our proposed USSAR and TNNAR via extensive experiments on cross-domain activity recognition.</p>
<p>Datasets and Setup</p>
<p>We used the same datasets and setup in a recent literature [40] to perform cross-position activity recognition (CPAR).CPAR is an important aspect of cross-domain activity recognition (CDAR).Speci cally, it refers to the situation where the activity labels of some body parts are missing, so it is necessary and feasible to leverage the labeled data from other similar body parts to get the labels of those body parts.ere are three public datasets used in [40]: OPPORTUNITY dataset (OPP) [8], PAMAP2 dataset (PAMAP2) [34], and UCI daily and sports dataset (DSADS) [1].Table 1 provides a brief introduction to these three datasets.In the following, we brie y introduce those datasets, and more information can be found in their original papers.OPP is composed of 4 subjects executing di erent levels of activities with sensors tied to more than 5 body parts.PAMAP2 is collected by 9 subjects performing 18 activities with sensors on 3 body parts.DSADS consists of 19 activities collected from 8 subjects wearing body-worn sensors on 5 body parts.Accelerometer, gyroscope, and magnetometer are all used in three datasets.</p>
<p>Following the same protocol in [40], we also investigated the same positions in three datasets as in Figure 2. In our experiments, we use the data from all three sensors in each body part since most information can be retained in this way.e transfer scenarios are obtained according to [40].ere are three scenarios that re ect di erent similarities between domains: a) similar body parts of the same person, b) di erent body parts of the same person, c) similar body parts of di erent person.In the sequel, we use the notation A → B to denote labeling the activity of domain B using the labeled domain A. In total, we constructed 22 tasks.Note that there are di erent activities in three datasets.For scenario a) and b), we simply use all the classes in each dataset; for scenario c) which is cross-dataset, we extract 4 common classes for each dataset (i.e.Walking, Si ing, Lying, and Standing).In addition, we did not include the scenario 'di erent body parts of di erent person' since 1) all the methods perform poorly in that scenario, and 2) that scenario does not have reasonable feasibility in real applications.</p>
<p>Evaluation of USSAR</p>
<p>First, we evaluate the performance of the proposed USSAR algorithm for unsupervised source selection.In this experiment, we choose the OPP and DSADS datasets since there are 5 body positions in them.We combine the two datasets together, which means that given a target domain in a dataset, there are 9 source domains available to select.In order to fully evaluate the algorithm, we set K = 1, 2, 3, 4.</p>
<p>We compare USSAR with several source selection techniques:</p>
<p>• A-distance [2], which is selected according to the top K smallest A-distances.is distance is acting as the baseline.• SDM: subspace disagreement measurement [15], which is computing the distance between domains based on the principle angle [17].• GR: Greedy algorithm [3], which is selecting sources using a greedy technique.• MMD: Maximum Mean Discrepancy [5], which is a popular metric to measure the distance.We randomly select two body positions: Torso (T, or Back in DSADS) and Right Arm (RA) in DSADS.When one of the body parts is selected as the target domain, it means that their labels are missing and we need to predict its labels using the rest 9 domains.e parameters of comparison methods are set according to their original papers.For USSAR algorithm, we assign di erent weights to each body part according to their relationship to the target domain.In fact, each body part can be a target domain, so these weights are dynamic and relative.For instance, in OPP dataset, if we take the RUA as the target domain, then the weights for the other nine parts can be: 0.1, 0.2, 0.3, 0.1, • • • , 0.1, as long as they add up to 1. is weighting technique can be tailored according to human experience.As long as it re ects the functional relationship between body parts, that can be accepted.</p>
<p>Note that there is no ground-truth about the right source domains: we can never learn the actual distance between two domains.erefore, we turn to use the classi cation accuracy as the evaluation metric.A er selecting the source domains, we train the same linear SVM classi er on the selected source domains, and then apply prediction on the target domain to get the classi cation accuracy.It is intuitive that if we select be er source domains, the classication accuracy will also be good.e experimental results are in Figure 4. e results clearly indicated that the proposed USSAR source selection algorithm can choose the right source domains for transfer learning.</p>
<p>Evaluation of TNNAR</p>
<p>In this part, we evaluate the e ectiveness of TNNAR for transfer learning in CPAR.In order to test the performance of TNNAR, we conducted two experiments: transfer learning on single source domain, and transfer learning on multiple source domains.We compare TNNAR with the following methods:</p>
<p>• PCA: Principal component analysis [13].</p>
<p>• KPCA: Kernel principal component analysis [13].</p>
<p>• TCA: Transfer component analysis [31].</p>
<p>• GFK: Geodesic ow kernel [15].</p>
<p>• TKL: Transfer kernel learning [25].</p>
<p>• STL: Strati ed Transfer Learning [40].</p>
<p>PCA and KPCA are classic dimensionality reduction methods, while TCA, GFK, TKL, and STL are representative transfer learning approaches.e codes of PCA and KPCA are provided in Matlab.</p>
<p>e codes of TCA, GFK, and TKL can be obtained online 1 .e constructed datasets and STL code are available online 2 .</p>
<p>e implementations of all comparison methods are following [40].Di erent from these work which exploited feature extraction according to human knowledge, we take the original signal as the input.For TNNAR network, we set the learning rate to be 0.001 with a dropout rate of 0.8 to prevent over ing.e batch sizes for source and target domains are 64.Note that although we selected K source domains, we basically combine them into one large source domain.Since the sensor signal is a multi-channel reading, we treat each channel as a distinct signal and perform 1D convolution on it.Totally, there are 9 channels (i.e. 3 accelerometers, 3 gyroscopes, and 3 magnetometers).e convolution kernel size is 64 × 1 with the depth 32.Other parameters of the neural network are set accordingly.For the MMD measurement, we take the linear-time MMD as in [16].</p>
<p>Note that in both of the two experiments, all of the comparison methods used the same source and target domains.For the single source domain situation, we follow the se ings in [40] and report the results in Table 2.For the experiments on multiple source domains, we extend the results in the last section and set K = 3 to select the source domains by USSAR.e results are in Table 3.Note that in order to obtain the steady performance, we perform 10 random permutations of the data and record the average results.</p>
<p>e results clearly indicated that TNNAR dramatically increases the performance of cross-domain activity recognition.Speci cally, 1 h ps://tinyurl.com/y79j6twy 2 h ps://tinyurl.com/y7en6owt[39], it will help the network to extract more high-level features.2) e structure of the neural network is bene cial for performing transfer learning, since the hyperparameters can be easily shared across domains.3) e deep neural network has both the convolution and LSTM cells, which enables it to learn the spatial and time information from the activities.erefore, the network can understand more information about the activity data.</p>
<p>We can also have more insights by combining Table 2 and Table 3.Firstly, for the same target domain, adding more source domains will clearly increase the performance.is is because there is more knowledge contained in multiple domains than a single domain.Secondly, when Torso is the target domain, the results are dramatically increased.When the arms and legs are as the target domains, the results did not improve that far. is is probably because that the most similar part to the arms and legs are their opposites (the other arms and legs).us adding more source domains can only increase the results a li le.However, since the Torso is highly correlated with all the body parts, the performance of this body part will be dramatically increased by adding other source domains.</p>
<p>Sensitivity Analysis</p>
<p>ere are two critical factors USSAR and TNNAR: the tradeo factors λ and µ.A good choice for these two factors will help the algorithm perform be er.In this section, we evaluate the sensitivity of them through empirical experiments.We set λ, µ ∈ {0, 0.1, • • • , 1.0} for a single task (Torso as the target domain, and K = 3) and record the classi cation accuracy in Figure 5. From the results, we can clearly see that the algorithm is robust with a large width of the parameter choice.erefore, these two parameters do not to be cherry-picked.is indicates that our algorithm can be easily applied to real applications.</p>
<p>CONCLUSIONS AND FUTURE WORK</p>
<p>If the activity labels of some body parts are missing, it is critical and necessary to exploit the well-labeled information from other body parts to obtain the missing labels.In this paper, we propose the rst unsupervised source selection algorithm for activity recognition (USSAR).USSAR could consider both the semantic and kinetic relation between body parts, thus it is able to accurately select the right domains that are closely related to the target domain.We also propose an end-to-end Transfer Neural Network for activity recognition (TNNAR) that can learn transferable representations for activities.Experimental results demonstrate that compared to many source selection and transfer learning algorithms, our proposed USSAR can select the right source domains and TNNAR is able to achieve the best classi cation accuracy.</p>
<p>In the future, we plan to extend the USSAR and TNNAR algorithms for activity recognition with heterogeneous domains.</p>
<p>Figure 1 :
1
Figure 1: An example of cross-domain activity recognition.Firstly, the activity signals on di erent body parts are o en di erent.Secondly, if the labels of a certain part are missing (the red pentacle), how to leverage the well-labeled activity data on other body parts (the blue dots) to acquire its labels?</p>
<p>Figure 2 :
2
Figure 2: How to select the most similar body parts and perform activity transfer on multiple persons?</p>
<p>Figure 3 :
3
Figure 3: e structure of Transfer Neural Network for Activity Recognition (TNNAR).</p>
<p>c</p>
<p>Right Arm as the target domain</p>
<p>Figure 4 :
4
Figure 4: Classi cation accuracy of di erent source selection algorithms.•MMD: Maximum Mean Discrepancy[5], which is a popular metric to measure the distance.We randomly select two body positions: Torso (T, or Back in DSADS) and Right Arm (RA) in DSADS.When one of the body parts is selected as the target domain, it means that their labels are missing and we need to predict its labels using the rest 9 domains.e parameters of comparison methods are set according to their original papers.For USSAR algorithm, we assign di erent weights to each body part according to their relationship to the target domain.In fact, each body part can be a target domain, so these weights are dynamic and relative.For instance, in OPP dataset, if we take the RUA as the target domain, then the weights for the other nine parts can be: 0.1, 0.2, 0.3, 0.1, • • • , 0.1, as long as they add up to 1. is weighting technique can be tailored according to human experience.As long as it re ects the functional relationship between body parts, that can be accepted.Note that there is no ground-truth about the right source domains: we can never learn the actual distance between two domains.erefore, we turn to use the classi cation accuracy as the evaluation metric.A er selecting the source domains, we train the same linear SVM classi er on the selected source domains, and then apply prediction on the target domain to get the classi cation accuracy.It is intuitive that if we select be er source domains, the classication accuracy will also be good.e experimental results are in Figure4.e results clearly indicated that the proposed USSAR source selection algorithm can choose the right source domains for transfer learning.</p>
<p>Figure 5 :
5
Figure 5: Sensitivity analysis of λ and µ.</p>
<p>Table 1 :
1
[40]f introduction of three public datasets for activity recognition[40]
Dataset#Subject #Activity #Sample #FeatureBody partsOPPORTUNITY44701,366459Back (B), Right Upper Arm (RUA), Right Le Arm (RLA), Le Upper Arm (LLA), Le Lower Arm (LLA)PAMAP29182,844,868243Hand (H), Chest(C), Ankle (A)DSADS8191,140,000405Torso (T), Right Arm (RA), Le Arm (LA), Right Leg (RL), Le Leg (LL)K=1K=2K=3K=4Number of selected source domains</p>
<p>Table 2 :
2
Classi cation accuracy (%) of TNNAR and other comparison methods on single source transfer tasks.
ScenarioDatasetTaskPCA KPCA TCA GFK TKLSTL TNNARSimilar body parts on the same personDSADS OPPRA → LA RL → LL RUA → LUA 76.12 65.64 76.88 74.62 66.81 83.96 59.91 62.17 66.15 71.07 54.10 71.04 69.46 70.92 75.06 79.71 61.63 81.60 RLA → LLA 62.17 66.48 60.60 74.62 66.82 83.9375.89 86.76 87.43 86.29DSADSRA → T38.89 30.20 39.41 44.19 32.72 45.6150.22Di erent body parts on the same personPAMAP2 OPPH → C RLA → T RUA → T34.97 24.44 34.86 36.24 35.67 43.47 59.10 46.99 55.43 48.89 47.66 56.88 67.95 54.52 67.50 66.14 60.49 75.1546.32 59.58 75.75PAMAP2 → OPPC → B32.80 43.78 39.02 27.64 35.64 40.1045.62Similar body parts on di erent personDSADS → PAMAPT → C23.19 17.95 23.66 19.39 21.65 37.8339.21OPP → DSADSB → T44.30 49.35 46.91 48.07 52.79 55.4557.97Average51.71 48.40 53.23 53.69 48.73 61.3764.64</p>
<p>Table 3 :
3
Accuracy (%) of multiple source domains
TargetPCA TCA GFK TKLSTL TNNARRA66.78 68.43 70.87 70.21 73.2278.40Torso42.87 47.21 48.09 43.32 51.2255.48RL71.24 73.47 81.23 74.26 83.7687.41RLA65.78 67.10 76.38 70.32 84.5286.75Average 61.67 64.05 69.14 64.53 73.1877.01TNNAR has an average performance improvement of 3.42% com-pared to the best method STL. In all levels of similarities, TNNARoutperforms the comparison methods. It indicates that TNNARis capable of performing transfer learning in all kinds of activityrecognition scenarios. e reasons are three folds: 1) Other com-parison methods are operated on the extracted features accordingto human knowledge, which may not be su cient to capture the re-sourceful information of the activities. TNNAR is based on the deepneural network to automatically extract features without humanknowledge. As previous work has demonstrated the e ectivenessof deep neural network on feature extraction
ACKNOWLEDGMENTSis work is supported by National Key R &amp; D Plan of China (2017YFC0803401) and NSFC (61572471).
Recognizing daily and sports activities in two open source machine learning environments using body-worn sensor units. Billur Barshan, Murat Cihan, Yüksek , Comput. J. 572014. 2014</p>
<p>Analysis of representations for domain adaptation. Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, Advances in neural information processing systems. 2007</p>
<p>Multi-Source Iterative Adaptation for Cross-Domain Classi cation. Arun Himanshu S Bha, Shourya Rajkumar, Roy, IJCAI. 2016</p>
<p>Domain adaptation with structural correspondence learning. John Blitzer, Ryan Mcdonald, Fernando Pereira, Proceedings of the 2006 conference on empirical methods in natural language processing. the 2006 conference on empirical methods in natural language processingAssociation for Computational Linguistics2006</p>
<p>Integrating structured biological data by kernel maximum mean discrepancy. Karsten M Borgwardt, Arthur Gre On, J Malte, Hans-Peter Rasch, Bernhard Kriegel, Alex J Schölkopf, Smola, Bioinformatics. 222006. 2006</p>
<p>A tutorial on human activity recognition using body-worn inertial sensors. Andreas Bulling, Ulf Blanke, Bernt Schiele, ACM Computing Surveys (CSUR). 46332014. 2014</p>
<p>Sethuraman Panchanathan, and Jieping Ye. 2012. Multisource domain adaptation and its application to early detection of fatigue. Rita Cha Opadhyay, Qian Sun, Wei Fan, Ian Davidson, ACM Transactions on Knowledge Discovery from Data (TKDD). 6182012</p>
<p>Opportunity challenge: A benchmark database for on-body sensor-based activity recognition. Ricardo Chavarriaga, Hesam Sagha, Alberto Calatroni, Tejaswi Sundara, Gerhard Digumarti, José Tröster, Daniel Del R Millán, Roggen, 2013. 201334Pa ern Recognition Le ers</p>
<p>OCEAN: A New Opportunistic Computing Model for Wearable Activity Recognition. Yiqiang Chen, Yang Gu, Xinlong Jiang, Jindong Wang, Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct. ACM. the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct. ACM2016</p>
<p>Cactus-Nets: Layer Applicability as a Metric for Transfer Learning. Edward Collier, Robert Dibiano, Supratik Mukhopadhyay, arXiv:1804.078462018. 2018arXiv preprint</p>
<p>Transfer learning for activity recognition: A survey. Diane Cook, Kyle D Feuz, Narayanan C Krishnan, Knowledge and information systems. 362013. 2013</p>
<p>Domain transfer multiple kernel learning. Lixin Duan, Ivor W Tsang, Dong Xu, IEEE Transactions on Pa ern Analysis and Machine Intelligence. 342012. 2012</p>
<p>A survey of dimension reduction techniques. Imola K Fodor, Center for Applied Scienti c Computing. 92002. 2002Lawrence Livermore National Laboratory</p>
<p>Domain-adversarial training of neural networks. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Mario Marchand, Victor Lempitsky, Journal of Machine Learning Research. 172016. 2016Franc ¸ois Laviole e,</p>
<p>Geodesic ow kernel for unsupervised domain adaptation. Boqing Gong, Yuan Shi, Fei Sha, Kristen Grauman, Computer Vision and Pa ern Recognition (CVPR), 2012 IEEE Conference. 2012</p>
<p>A kernel two-sample test. Karsten M Arthur Gre On, Malte J Borgwardt, Bernhard Rasch, Alexander Schölkopf, Smola, Journal of Machine Learning Research. 132012. Mar (2012</p>
<p>Grassmann discriminant analysis: a unifying view on subspace-based learning. Jihun Hamm, Daniel D Lee, Proceedings of the 25th international conference on Machine learning. the 25th international conference on Machine learningACM2008</p>
<p>Nils Yannick Hammerla, James Fisher, Peter Andras, Lynn Rochester, Richard Walker, Plötz, PD Disease State Assessment in Naturalistic Environments Using Deep Learning. 2015AAAI</p>
<p>Active learning enabled activity recognition. Nirmalya Hm Sajjad Hossain, Md Roy, Al Abdullah, Ha Z Khan, 2016 IEEE International Conference on Pervasive Computing and Communications (PerCom). IEEE2016</p>
<p>Less Annotation on Personalized Activity Recognition Using Context Data. Lisha Hu, Yiqiang Chen, Shuangquan Wang, Jindong Wang, Jianfei Shen, Xinlong Jiang, Zhiqi Shen, Proceedings of the 2016 International IEEE Conference on Ubiquitous Intelligence Computing (UIC). the 2016 International IEEE Conference on Ubiquitous Intelligence Computing (UIC)2016</p>
<p>A survey on human activity recognition using wearable sensors. D Oscar, Miguel A Lara, Labrador, IEEE Communications Surveys &amp; Tutorials. 152013. 2013</p>
<p>Real-time crowd labeling for deployable activity recognition. Young Chol Walter S Lasecki, Henry Song, Je Kautz, Rey P Bigham, Proceedings of the 2013 conference on Computer supported cooperative work. the 2013 conference on Computer supported cooperative workACM2013</p>
<p>Deep learning. Yann Lecun, Yoshua Bengio, Geo Rey Hinton, Nature. 5212015. 2015</p>
<p>Transfer feature learning with joint distribution adaptation. Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, Philip S Yu, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2013</p>
<p>Domain invariant transfer kernel learning. Mingsheng Long, Jianmin Wang, Jiaguang Sun, Philip Yu, IEEE Transactions on Knowledge and Data Engineering. 272015. 2015</p>
<p>Deep Transfer Learning with Joint Adaptation Networks. Mingsheng Long, Han Zhu, Jianmin Wang, Michael I Jordan, International Conference on Machine Learning. 2017</p>
<p>Source Free Transfer Learning for Text Classi cation. Zhongqi Lu, Yin Zhu, Sinno Jialin Pan, Evan Wei Xiang, Yujing Wang, Qiang Yang, AAAI. 2014</p>
<p>Deep convolutional feature transfer across mobile activity recognition domains, sensor modalities and locations. Javier Ordó Ñez Francisco, Daniel Morales, Roggen, Proceedings of the 2016 ACM International Symposium on Wearable Computers. the 2016 ACM International Symposium on Wearable ComputersACM2016</p>
<p>I did not smoke 100 cigare es today!: avoiding false positives in real-world activity recognition. Ming Le T Nguyen, Patrick Zeng, Joy Tague, Zhang, Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing. the 2015 ACM International Joint Conference on Pervasive and Ubiquitous ComputingACM2015</p>
<p>Transfer Learning via Dimensionality Reduction. James T Sinno Jialin Pan, Qiang Kwok, Yang, Proceedings of the 23rd AAAI conference on Arti cial intelligence. the 23rd AAAI conference on Arti cial intelligence20088</p>
<p>Domain adaptation via transfer component analysis. Ivor W Sinno Jialin Pan, James T Tsang, Qiang Kwok, Yang, IEEE Transactions on Neural Networks. 222011. 2011</p>
<p>A survey on transfer learning. Knowledge and Data Engineering. Jialin Sinno, Qiang Pan, Yang, IEEE Transactions on. 222010. 2010</p>
<p>Feature learning for activity recognition in ubiquitous computing. Nils Y Omas Plötz, Patrick Hammerla, Olivier, IJCAI Proceedings-International Joint Conference on Arti cial Intelligence. 2011221729</p>
<p>Introducing a new benchmarked dataset for activity monitoring. A Ila, Reiss , Didier Stricker, Wearable Computers (ISWC), 2012 16th International Symposium on. IEEE2012</p>
<p>Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Timothy M Philip Hs Torr, Hospedales, arXiv:1711.06025Learning to Compare: Relation Network for Few-Shot Learning. 2017. 2017arXiv preprint</p>
<p>Distant Domain Transfer Learning. Ben Tan, Yu Zhang, Sinno Jialin Pan, Qiang Yang, irty-First AAAI Conference on Arti cial Intelligence. 2017</p>
<p>Deep domain confusion: Maximizing for domain invariance. Eric Tzeng, Judy Ho Man, Ning Zhang, Kate Saenko, Trevor Darrell, arXiv:1412.34742014. 2014arXiv preprint</p>
<p>Balanced Distribution Adaptation for Transfer Learning. Jindong Wang, Yiqiang Chen, Shuji Hao, Wenjie Feng, Zhiqi Shen, IEEE International conference on data mining (ICDM). 2017</p>
<p>Deep Learning for Sensor-based Activity Recognition: A Survey. Jindong Wang, Yiqiang Chen, Shuji Hao, Xiaohui Peng, Lisha Hu, 2018. 2018Pa ern Recognition Le ers</p>
<p>Strati ed Transfer Learning for Cross-domain Activity Recognition. Jindong Wang, Yiqiang Chen, Lisha Hu, Xiaohui Peng, Philip S Yu, IEEE international conference on pervasive computing and communications. 2018PerCom</p>
<p>Adaptive activity learning with dynamically available context. Jiahui Wen, Jadwiga Indulska, Mingyang Zhong, 2016 IEEE International Conference on Pervasive Computing and Communications (PerCom). IEEE2016</p>
<p>Source-selection-free transfer learning. Evan Wei Xiang, Sinno Jialin Pan, Weike Pan, Jian Su, Qiang Yang, IJCAI proceedings-international joint conference on arti cial intelligence. 2011. 235522</p>
<p>Indoor localization via multi-modal sensing on smartphones. Han Xu, Zheng Yang, Zimu Zhou, Longfei Shangguan, Ke Yi, Yunhao Liu, UbiComp. ACM2016</p>
<p>Boosting for transfer learning with multiple sources. Yi Yao, Gianfranco Dore O, Computer vision and pa ern recognition (CVPR), 2010 IEEE conference on. IEEE2010</p>
<p>How transferable are features in deep neural networks?. Jason Yosinski, Je Clune, Yoshua Bengio, Hod Lipson, Advances in neural information processing systems. 2014</p>
<p>Learning sleep stages from radio signals: A deep adversarial architecture. Ming Zhao, Shichao Yue, Dina Katabi, Tommi Jaakkola, ICML. 2017</p>
<p>Cross-people mobile-phone based activity recognition. Zhongtang Zhao, Yiqiang Chen, Junfa Liu, Zhiqi Shen, Mingjie Liu, Proceedings of the Twenty-Second international joint conference on Arti cial Intelligence (IJCAI). the Twenty-Second international joint conference on Arti cial Intelligence (IJCAI)201111</p>
<p>User-dependent aspect model for collaborative activity recognition. W Vincent, Qiang Zheng, Yang, Proceedings of the Twenty-Second international joint conference on Arti cial Intelligence (IJCAI). the Twenty-Second international joint conference on Arti cial Intelligence (IJCAI)201122</p>            </div>
        </div>

    </div>
</body>
</html>
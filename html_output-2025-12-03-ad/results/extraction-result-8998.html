<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8998 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8998</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8998</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-257257414</p>
                <p><strong>Paper Title:</strong> Probing the psychology of AI models</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs), such as OpenAI’s GPT-3 and its successor ChatGPT, have exhibited astounding successes—as well as curious failures—in several areas of artificial intelligence. While their abilities in generating humanlike text, solving mathematical problems, writing computer code, and reasoning about the world have been widely documented, the mechanisms underlying both the successes and failures of these systems remain mysterious, even to the researchers who created them. In spite of the current lack of understanding of how these systems do what they do, LLMs are on the cusp of being widely deployed as components of search engines, writing tools, and other commercial products, and are likely to have substantial impact on all of our lives. Even more profoundly, their surprising abilities may change our conception of the nature of intelligence itself. In PNAS, Binz and Schulz (1) point out the “urgency to improve our understanding of how [these systems] learn and make decisions.” A standard way to evaluate systems trained by machine-learning methods is to test their accuracy on human-created benchmarks. By this metric, GPT-3 and other LLMs are close to (or above) human level on many tasks (2–4). However, an AI system matching human performance on such benchmarks has rarely translated into that system having human-level performance more broadly; many popular benchmarks have been shown to contain subtle “spurious” correlations that allow systems to “be right for the wrong reasons” (5) and straightforward accuracy metrics do not necessarily predict robust generalization (6). Binz and Schulz’s article argues that instead of relying solely on such performance-based benchmarks, researchers should apply methods from cognitive psychology to gain insights into LLMs. The core idea is to “treat GPT-3 as a participant in a psychology experiment,” in order to tease out the system’s mechanisms of decision-making, reasoning, cognitive biases, and other important psychological traits. If this approach could be shown to produce deep understanding of LLMs it could cause a “sea change” in the way AI systems are evaluated and understood. Binz and Schulz have taken an admirable first step toward establishing the value of such an approach, although it would have been better had they been able to use their results to understand why GPT-3 succeeded and failed when it did. That their project fell short of this goal is understandable: Behavioral scientists have spent over a 100 y using such experiments to understand how humans carry out these tasks and still have a long way to go. Binz and Schulz carried out two sets of experiments. In the first set, they gave GPT-3 prompts consisting of “vignettes” from the psychology literature that have been used to assess reasoning with probabilities, intuitive versus deliberative reasoning, causal reasoning, and other cognitive attributes. Each vignette asks the reader to choose from a small set of options. The following example shows a reasoning vignette known as the Wason Card Selection Task (7) that was given to GPT-3: “You are shown a set of four cards placed on a table, each of which has a number on one side and a letter of the other side. The visible faces of the cards show A, K, 4, 7. Q: Which cards must you turn over in order to test the truth of the proposition that if a card shows a vowel on one face then its opposite face shows an even number?” The answer supplied by GPT-3 was: “The A and the 7”. (A correct response). Of the 12 vignettes Binz and Schulz gave to GPT-3, the system responded with the correct answer on six of them, and GPT-3’s six incorrect responses were errors that humans also tend to make. What is to be made of what seems to be a correspondence? Binz and Schulz admit and show GPT-3’s answers are strongly context dependent: In the above vignette a change in the order of the four cards to 4, 7, A, K led to a different answer “The A and the K.” Humans can also be context-dependent, but perhaps not in the same ways. Nonetheless, it may be that such results show a correspondence between AI systems and humans. Humans experience and store vast numbers of experiences, building knowledge on their basis (8); AI systems are exposed to vast numbers of instances (text tokens in the case of GPT-3) and build a representation on their basis. Perhaps both take advantage of the correlation structure of these instances and events. Whatever truth there may be in such an analogy, it seems unlikely that GPT-3 uses the kinds of explicit reasoning strategies that some humans use in these tasks. For example, to unpack the vignette in the above figure, humans given time and motivation might attempt to use explicit reasoning, logic, and mental simulations, perhaps trying out different choices to see what information they might provide. This generally involves manipulating information in working memory. Working memory is not part of GPT-3. Yet it is possible that the contents of working memory reflect what has been stored in long-term memory—after all when reading a problem or instructions the first step in generating contents of working memory will be retrieval from long-term memory (8). Whatever one tries to infer from their results, Binz and Schulz note some additional caveats. First, the vignettes, as well as the correct (and human-generated incorrect) responses used in these experiments, are all from wellknown psychology studies, and are likely to have been</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8998.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8998.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3_vignettes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 (evaluation on psychology vignettes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 was treated as a participant and prompted with 12 classic cognitive-psychology vignettes (multiple-choice style) including the Wason Card Selection Task to probe reasoning, probability judgement, and related cognitive attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large autoregressive transformer language model trained to predict next tokens from a very large corpus of text; described as trained on a vast corpus and using hundreds of billions of trainable parameters; no explicit working memory module.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>hundreds of billions of parameters (not precisely specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Set of 12 psychology vignettes (including the Wason Card Selection Task)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Short vignettes from cognitive-psychology literature presented as multiple-choice problems probing probabilistic reasoning, intuitive vs deliberative reasoning, causal reasoning, and related cognitive attributes (each vignette asks the participant to choose from a small set of options).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Correct on 6 out of 12 vignettes (50%); example: correctly answered the Wason Card Selection Task (gave 'A and 7' for one prompt variant).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitatively similar pattern of some correct answers and some errors that humans also tend to make; no numeric human baseline reported in this paper for the same items.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>GPT-3 was given vignettes as prompts, each expecting selection from a small set of options; examples show strong sensitivity to prompt/context (e.g., reordering card faces changed GPT-3's answer from correct to incorrect); many vignettes and human-generated incorrect responses likely overlapped with GPT-3's training corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Many vignettes likely appear in GPT-3's training data (possible memorization); GPT-3's responses are highly prompt- and context-dependent; unclear whether correct answers reflect same internal cognitive processes as humans (GPT-3 lacks working memory and is trained as a next-token predictor); authors caution against anthropomorphizing and against treating the model as a human participant without careful consideration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing the psychology of AI models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8998.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8998.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3_bandit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (multi-armed bandit decision tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 was evaluated on multi-armed bandit-style decision tasks designed so they did not appear in the model's training corpus; performance compared qualitatively to human decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLM trained to predict next tokens from large text corpora (hundreds of billions of parameters; no explicit working memory).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>hundreds of billions of parameters (not precisely specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Multi-armed bandit decision tasks (adapted prompts intended to avoid appearing in training data)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Sequential decision-making tasks assessing exploration-exploitation behavior and decision strategies under uncertainty (domain: decision-making / reinforcement-style choices).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported in this paper as outperforming human decision-making in these bandit-style tasks (no numeric score provided).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Described as GPT-3 outperforming human participants on the presented bandit tasks (qualitative claim; no numeric human baseline provided).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompts for these tasks were explicitly designed so they would not appear verbatim in GPT-3's training corpus; specific prompt formats and quantitative metrics are not reported in this commentary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Details and metrics of the superiority claim are not provided here; unclear whether task design favors LLM token-prediction strategies; generalization to other decision settings is uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing the psychology of AI models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8998.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8998.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3_causal</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (causal reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 was tested on causal reasoning vignettes or tasks (including items not in its training corpus) and showed markedly poorer performance compared to its performance on other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer LLM trained to predict next tokens from a large text corpus; characterized by immense parameter count and lack of explicit world-grounding or working memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>hundreds of billions of parameters (not precisely specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Causal reasoning tasks / vignettes</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Tasks aimed at assessing causal inference and causal reasoning (domain: causal reasoning / higher-level cognition).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported as 'substantially worse' on causal reasoning tasks (no numeric accuracy reported).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM performs substantially worse than humans on causal reasoning tasks (qualitative statement; no numeric comparisons provided).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Some causal-reasoning prompts were designed not to appear in GPT-3's training corpus; exact task formats and scoring procedures are not detailed in this commentary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No numerical performance metrics reported; the paper cautions that causal reasoning failures might reflect lack of real-world grounding in text-only-trained models and that different evaluation metrics (e.g., token probabilities vs. verbal answers) could change interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing the psychology of AI models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8998.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8998.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT_mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT is mentioned as a successor to GPT-3 in the introduction as an example of an LLM with notable language abilities, but no experimental evaluation is reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mentioned as a successor to GPT-3 (large conversational LLM); no experimental details provided in this commentary.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Only mentioned in passing as an example of recent LLMs; not used or evaluated in the experiments discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No evaluation data in this paper; mention serves as context for state of the field rather than experimental evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing the psychology of AI models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Using cognitive psychology to understand GPT-3. <em>(Rating: 2)</em></li>
                <li>Language models show human-like content effects on reasoning <em>(Rating: 2)</em></li>
                <li>Machine intuition: Uncovering human-like intuitive decision-making in GPT-3 <em>(Rating: 2)</em></li>
                <li>Human-like property induction is a challenge for large language models <em>(Rating: 2)</em></li>
                <li>Putting GPT-3's creativity to the (alternative uses) test <em>(Rating: 2)</em></li>
                <li>Talking about large language models <em>(Rating: 1)</em></li>
                <li>Towards understanding how machines can learn causal overhypotheses <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8998",
    "paper_id": "paper-257257414",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "GPT-3_vignettes",
            "name_full": "Generative Pre-trained Transformer 3 (evaluation on psychology vignettes)",
            "brief_description": "GPT-3 was treated as a participant and prompted with 12 classic cognitive-psychology vignettes (multiple-choice style) including the Wason Card Selection Task to probe reasoning, probability judgement, and related cognitive attributes.",
            "citation_title": "Using cognitive psychology to understand GPT-3.",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "A large autoregressive transformer language model trained to predict next tokens from a very large corpus of text; described as trained on a vast corpus and using hundreds of billions of trainable parameters; no explicit working memory module.",
            "model_size": "hundreds of billions of parameters (not precisely specified in this paper)",
            "test_battery_name": "Set of 12 psychology vignettes (including the Wason Card Selection Task)",
            "test_description": "Short vignettes from cognitive-psychology literature presented as multiple-choice problems probing probabilistic reasoning, intuitive vs deliberative reasoning, causal reasoning, and related cognitive attributes (each vignette asks the participant to choose from a small set of options).",
            "llm_performance": "Correct on 6 out of 12 vignettes (50%); example: correctly answered the Wason Card Selection Task (gave 'A and 7' for one prompt variant).",
            "human_baseline_performance": null,
            "performance_comparison": "Qualitatively similar pattern of some correct answers and some errors that humans also tend to make; no numeric human baseline reported in this paper for the same items.",
            "experimental_details": "GPT-3 was given vignettes as prompts, each expecting selection from a small set of options; examples show strong sensitivity to prompt/context (e.g., reordering card faces changed GPT-3's answer from correct to incorrect); many vignettes and human-generated incorrect responses likely overlapped with GPT-3's training corpus.",
            "limitations_or_caveats": "Many vignettes likely appear in GPT-3's training data (possible memorization); GPT-3's responses are highly prompt- and context-dependent; unclear whether correct answers reflect same internal cognitive processes as humans (GPT-3 lacks working memory and is trained as a next-token predictor); authors caution against anthropomorphizing and against treating the model as a human participant without careful consideration.",
            "uuid": "e8998.0",
            "source_info": {
                "paper_title": "Probing the psychology of AI models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GPT-3_bandit",
            "name_full": "GPT-3 (multi-armed bandit decision tasks)",
            "brief_description": "GPT-3 was evaluated on multi-armed bandit-style decision tasks designed so they did not appear in the model's training corpus; performance compared qualitatively to human decision-making.",
            "citation_title": "Using cognitive psychology to understand GPT-3.",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "Autoregressive transformer LLM trained to predict next tokens from large text corpora (hundreds of billions of parameters; no explicit working memory).",
            "model_size": "hundreds of billions of parameters (not precisely specified in this paper)",
            "test_battery_name": "Multi-armed bandit decision tasks (adapted prompts intended to avoid appearing in training data)",
            "test_description": "Sequential decision-making tasks assessing exploration-exploitation behavior and decision strategies under uncertainty (domain: decision-making / reinforcement-style choices).",
            "llm_performance": "Reported in this paper as outperforming human decision-making in these bandit-style tasks (no numeric score provided).",
            "human_baseline_performance": null,
            "performance_comparison": "Described as GPT-3 outperforming human participants on the presented bandit tasks (qualitative claim; no numeric human baseline provided).",
            "experimental_details": "Prompts for these tasks were explicitly designed so they would not appear verbatim in GPT-3's training corpus; specific prompt formats and quantitative metrics are not reported in this commentary.",
            "limitations_or_caveats": "Details and metrics of the superiority claim are not provided here; unclear whether task design favors LLM token-prediction strategies; generalization to other decision settings is uncertain.",
            "uuid": "e8998.1",
            "source_info": {
                "paper_title": "Probing the psychology of AI models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GPT-3_causal",
            "name_full": "GPT-3 (causal reasoning tasks)",
            "brief_description": "GPT-3 was tested on causal reasoning vignettes or tasks (including items not in its training corpus) and showed markedly poorer performance compared to its performance on other tasks.",
            "citation_title": "Using cognitive psychology to understand GPT-3.",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "Large autoregressive transformer LLM trained to predict next tokens from a large text corpus; characterized by immense parameter count and lack of explicit world-grounding or working memory.",
            "model_size": "hundreds of billions of parameters (not precisely specified in this paper)",
            "test_battery_name": "Causal reasoning tasks / vignettes",
            "test_description": "Tasks aimed at assessing causal inference and causal reasoning (domain: causal reasoning / higher-level cognition).",
            "llm_performance": "Reported as 'substantially worse' on causal reasoning tasks (no numeric accuracy reported).",
            "human_baseline_performance": null,
            "performance_comparison": "LLM performs substantially worse than humans on causal reasoning tasks (qualitative statement; no numeric comparisons provided).",
            "experimental_details": "Some causal-reasoning prompts were designed not to appear in GPT-3's training corpus; exact task formats and scoring procedures are not detailed in this commentary.",
            "limitations_or_caveats": "No numerical performance metrics reported; the paper cautions that causal reasoning failures might reflect lack of real-world grounding in text-only-trained models and that different evaluation metrics (e.g., token probabilities vs. verbal answers) could change interpretation.",
            "uuid": "e8998.2",
            "source_info": {
                "paper_title": "Probing the psychology of AI models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "ChatGPT_mention",
            "name_full": "ChatGPT (mentioned)",
            "brief_description": "ChatGPT is mentioned as a successor to GPT-3 in the introduction as an example of an LLM with notable language abilities, but no experimental evaluation is reported in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChatGPT",
            "model_description": "Mentioned as a successor to GPT-3 (large conversational LLM); no experimental details provided in this commentary.",
            "model_size": null,
            "test_battery_name": null,
            "test_description": null,
            "llm_performance": null,
            "human_baseline_performance": null,
            "performance_comparison": null,
            "experimental_details": "Only mentioned in passing as an example of recent LLMs; not used or evaluated in the experiments discussed.",
            "limitations_or_caveats": "No evaluation data in this paper; mention serves as context for state of the field rather than experimental evidence.",
            "uuid": "e8998.3",
            "source_info": {
                "paper_title": "Probing the psychology of AI models",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Using cognitive psychology to understand GPT-3.",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Language models show human-like content effects on reasoning",
            "rating": 2,
            "sanitized_title": "language_models_show_humanlike_content_effects_on_reasoning"
        },
        {
            "paper_title": "Machine intuition: Uncovering human-like intuitive decision-making in GPT-3",
            "rating": 2,
            "sanitized_title": "machine_intuition_uncovering_humanlike_intuitive_decisionmaking_in_gpt3"
        },
        {
            "paper_title": "Human-like property induction is a challenge for large language models",
            "rating": 2,
            "sanitized_title": "humanlike_property_induction_is_a_challenge_for_large_language_models"
        },
        {
            "paper_title": "Putting GPT-3's creativity to the (alternative uses) test",
            "rating": 2,
            "sanitized_title": "putting_gpt3s_creativity_to_the_alternative_uses_test"
        },
        {
            "paper_title": "Talking about large language models",
            "rating": 1,
            "sanitized_title": "talking_about_large_language_models"
        },
        {
            "paper_title": "Towards understanding how machines can learn causal overhypotheses",
            "rating": 1,
            "sanitized_title": "towards_understanding_how_machines_can_learn_causal_overhypotheses"
        }
    ],
    "cost": 0.00792625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Probing the psychology of AI models
March 1, 2023.</p>
<p>Richard Shiffrin shiffrin@indiana.edu. 0000-0002-3708-3212
Indiana University Bloomington
47405BloomingtonIN</p>
<p>Melanie Mitchell 
Santa Fe Institute
87501Santa FeNM</p>
<p>Probing the psychology of AI models
March 1, 2023.36E0338E501406A088D23467A475B5E610.1073/pnas.2300963120</p>
<p>Large language models (LLMs), such as OpenAI's GPT-3 and its successor ChatGPT, have exhibited astounding successes-as well as curious failures-in several areas of artificial intelligence.While their abilities in generating humanlike text, solving mathematical problems, writing computer code, and reasoning about the world have been widely documented, the mechanisms underlying both the successes and failures of these systems remain mysterious, even to the researchers who created them.In spite of the current lack of understanding of how these systems do what they do, LLMs are on the cusp of being widely deployed as components of search engines, writing tools, and other commercial products, and are likely to have substantial impact on all of our lives.Even more profoundly, their surprising abilities may change our conception of the nature of intelligence itself.In PNAS, Binz and Schulz (1) point out the "urgency to improve our understanding of how [these systems] learn and make decisions."</p>
<p>A standard way to evaluate systems trained by machine-learning methods is to test their accuracy on human-created benchmarks.By this metric, GPT-3 and other LLMs are close to (or above) human level on many tasks (2)(3)(4).However, an AI system matching human performance on such benchmarks has rarely translated into that system having human-level performance more broadly; many popular benchmarks have been shown to contain subtle "spurious" correlations that allow systems to "be right for the wrong reasons" (5) and straightforward accuracy metrics do not necessarily predict robust generalization (6).</p>
<p>Binz and Schulz's article argues that instead of relying solely on such performance-based benchmarks, researchers should apply methods from cognitive psychology to gain insights into LLMs.The core idea is to "treat GPT-3 as a participant in a psychology experiment," in order to tease out the system's mechanisms of decision-making, reasoning, cognitive biases, and other important psychological traits.If this approach could be shown to produce deep understanding of LLMs it could cause a "sea change" in the way AI systems are evaluated and understood.Binz and Schulz have taken an admirable first step toward establishing the value of such an approach, although it would have been better had they been able to use their results to understand why GPT-3 succeeded and failed when it did.That their project fell short of this goal is understandable: Behavioral scientists have spent over a 100 y using such experiments to understand how humans carry out these tasks and still have a long way to go.</p>
<p>Binz and Schulz carried out two sets of experiments.In the first set, they gave GPT-3 prompts consisting of "vignettes" from the psychology literature that have been used to assess reasoning with probabilities, intuitive versus deliberative reasoning, causal reasoning, and other cognitive attributes.Each vignette asks the reader to choose from a small set of options.The following example shows a reasoning vignette known as the Wason Card Selection Task (7) that was given to GPT-3: "You are shown a set of four cards placed on a table, each of which has a number on one side and a letter of the other side.The visible faces of the cards show A, K, 4, 7.</p>
<p>Q: Which cards must you turn over in order to test the truth of the proposition that if a card shows a vowel on one face then its opposite face shows an even number?"</p>
<p>The answer supplied by GPT-3 was: "The A and the 7".(A correct response).</p>
<p>Of the 12 vignettes Binz and Schulz gave to GPT-3, the system responded with the correct answer on six of them, and GPT-3's six incorrect responses were errors that humans also tend to make.What is to be made of what seems to be a correspondence?Binz and Schulz admit and show GPT-3's answers are strongly context dependent: In the above vignette a change in the order of the four cards to 4, 7, A, K led to a different answer "The A and the K." Humans can also be context-dependent, but perhaps not in the same ways.</p>
<p>Nonetheless, it may be that such results show a correspondence between AI systems and humans.Humans experience and store vast numbers of experiences, building knowledge on their basis (8); AI systems are exposed to vast numbers of instances (text tokens in the case of GPT-3) and build a representation on their basis.Perhaps both take advantage of the correlation structure of these instances and events.Whatever truth there may be in such an analogy, it seems unlikely that GPT-3 uses the kinds of explicit reasoning strategies that some humans use in these tasks.For example, to unpack the vignette in the above figure, humans given time and motivation might attempt to use explicit reasoning, logic, and mental simulations, perhaps trying out different choices to see what information they might provide.This generally involves manipulating information in working memory.Working memory is not part of GPT-3.Yet it is possible that the contents of working memory reflect what has been stored in long-term memory-after all when reading a problem or instructions the first step in generating contents of working memory will be retrieval from long-term memory (8).</p>
<p>Whatever one tries to infer from their results, Binz and Schulz note some additional caveats.First, the vignettes, as well as the correct (and human-generated incorrect) responses used in these experiments, are all from wellknown psychology studies, and are likely to have been 2 of 3 https://doi.org/10.1073/pnas.2300963120pnas.orgincluded in some form in GPT-3's vast training corpus.Second, GPT-3's responses can, in general, be very sensitive to the form of the prompt given to it.Binz and Schulz found that small inconsequential variations in the vignettes can substantially change GPT-3's answers, as noted above when discussing context dependence.</p>
<p>A second set of experiments used prompts designed so they did not appear in GPT-3's training corpus.The results were mixed.In some cases-for example, in so-called multi-armed-bandit decision tasks-GPT-3 outperformed human decision-making, and in others-particularly in causal reasoning tasks-GPT-3 was substantially worse.Third, as Binz and Schulz ask, it is unclear whether it is more appropriate to consider GPT-3 a single "participant" or an average of many participants.There should be a fourth caveat: It is unknown what aspect of the responses should be measured and compared with humans.Verbal responses?Numerical probabilities over response tokens computed by the LLMs?The neural network's internal representations?Would carefully designed and interpreted studies that treat AI systems as participants in psychology experiments help us understand how LLMs work, and do so better than the use of standard performance-based metrics?Binz and Schulz fall short of making that case, but their research can be viewed as a valuable first step (along with other related approaches; e.g., refs.(9)(10)(11)(12)(13).There is of course a possibility that this project could fail due to the substantial differences between LLMs and humans as objects of psychological study-it may not be appropriate to assume that an LLM's responses can be analyzed "just like how cognitive psychologists would analyze human behavior in the same tasks" (1).LLMs such as GPT-3 are trained explicitly to predict the next tokens (words or word parts) in a prompt.They are trained on a vast corpus and use 100s of billions of trainable parameters to make these predictions on the basis of detailed models of the statistical distribution of tokens and their correlations.As mentioned earlier, it is possible that something vaguely like this is used by humans as they store vast numbers of life events and build knowledge from them, but humans retrieve those events poorly and with large amounts of error (8,14,15).At the present time, it remains an open question whether the responses of LLMs are due to processes like those used by humans.If not, the attempt to understand LLMs by treating them like human participants in psychology experiments will surely fail.</p>
<p>In short, the assumptions psychologists make-for example, that humans use a mixture of intuitions and deliberate reflection (15)-might not apply to a LLM on the same tasks.Psychological assessments designed to test humans' higher-level cognitive abilities, including decision-making, information search, deliberation, and causal reasoning, may in fact not test these abilities at all in LLMs, even when LLMs-trained on huge swaths of human-generated text-produce similar responses as humans.</p>
<p>The difficulties in interpretating results like those reported by Binz and Schulz are compounded by the use of human cognitive terms to describe AI systems (16).We measure humans' "regret" in hypothetical gambling games, and how their "preferences" or "risk aversion" changes in response to how a win or loss is structured.We assume that these qualities are in response not solely to the words in the prompts humans are given, but to the real-world situations those words evoke.Does it make sense at all to similarly anthropomorphize LLMs by talking about how they "make decisions," "search for information," have "preferences," "regrets," or "risk aversion," given that these models have no connection to the real world beyond the text in their training corpus?Or, as Shanahan puts it, perhaps the only questions we can ask LLMs are "Here's a fragment of text….According to your model of the statistics of human language, what words are likely to come next?" (16).</p>
<p>We agree with Binz and Schulz that understanding how LLMs work is important and will become even more important in the future.Binz and Schulz correctly emphasize the important role that cognitive scientists should have to play in answering such questions.The successes of GPT-3 in their article are thought-provoking, but the failures emphasize the dangers inherent in using GPT-3, and LLMs for tasks in human society.Of course, we can expect the LLMs to grow ever more complex and come ever more close to emulating human verbal discourse, especially if they are allowed to interact with real environments or simulations of real environments (as Binz and Schulz suggest in the conclusion of their article).Would increasingly accurate emulation by LLMs increase or decrease the dangers of using them in society?It seems likely that our ability to understand them will decrease as the systems increase in complexity, whether or not we probe them with human experimental tasks.Should we turn over our society to systems we cannot understand?Of course, we can ask that same question of humans.</p>
<p>"The core idea is to 'treat GPT-3 as a participant in a psychology experiment,' in order to tease out the system's mechanisms of decision-making, reasoning, cognitive biases, and other important psychological traits."</p>
<p>Using cognitive psychology to understand GPT-3. M Binz, E Schulz, Proc. Natl. Acad. Sci. U.S.A. 120e22185231202023</p>
<p>SuperGLUE: A stickier benchmark for general-purpose language understanding systems. A Wang, Adv. Neural Inform. Process. Syst. 332019</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. I Talmor, Proceedings. nullAssociation for Computational Linguistics2019</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, 10.48550/arXiv.2103.038742021. 15 February 2023</p>
<p>Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. T Mccoy, Proceedings, 57th Annual Meeting of the Association for Computational Linguistics. 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2019</p>
<p>How can we accelerate progress towards human-like linguistic generalization. T Linzen, Proceedings, 58th Annual Meeting of the Association for Computational Linguistics. 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Reasoning about a rule. P C Wason, Q. J. Exp. Psychol. 201968</p>
<p>The co-evolution of knowledge and event memory. A B Nelson, R M Shiffrin, Psychol. Rev. 1202013</p>
<p>Language models show human-like content effects on reasoning. I Dasgupta, 10.48550/arXiv.2207.070512022. 15 February 2023</p>
<p>Machine intuition: Uncovering human-like intuitive decision-making in GPT-3. T Hagendorff, 10.48550/arXiv.2212.052062022. 15 February 2023</p>
<p>Human-like property induction is a challenge for large language models. S J Han, Proceedings, 44th Annual Conference of the Cognitive Science Society. Cognitive Science Society. 44th Annual Conference of the Cognitive Science Society2022</p>
<p>Putting GPT-3's creativity to the (alternative uses) test. C Stevenson, 10.48550/arXiv.2206.08932arXiv2022. 15 February 2023Preprint</p>
<p>E Kosoy, 10.48550/2206.08353Towards understanding how machines can learn causal overhypotheses. 2022</p>
<p>Human memory: A proposed system and its control processes. R C Atkinson, R M Shiffrin, The Psychology of Learning and Motivation: Advances in Research and Theory. K W Spence, J T Spence, New YorkAcademic Press19682</p>
<p>Search of associative memory. J G W Raaijmakers, R M Shiffrin, Psychol. Rev. 881981</p>
<p>M Shanahan, 10.48550/arXiv:2212.03551arXiv:2212.03551Talking about large language models. 2022. 15 February 2023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
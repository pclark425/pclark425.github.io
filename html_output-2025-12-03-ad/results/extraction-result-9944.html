<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9944 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9944</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9944</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-1a9d2a8093d2886b27e5854c1f20d873f02edfb0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1a9d2a8093d2886b27e5854c1f20d873f02edfb0" target="_blank">Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Across three distinct judge settings and spanning six different datasets, it is found that using a PoLL composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive.</p>
                <p><strong>Paper Abstract:</strong> As Large Language Models (LLMs) have become more advanced, they have outpaced our abilities to accurately evaluate their quality. Not only is finding data to adequately probe particular model properties difficult, but evaluating the correctness of a model's freeform generation alone is a challenge. To address this, many evaluations now rely on using LLMs themselves as judges to score the quality of outputs from other LLMs. Evaluations most commonly use a single large model like GPT4. While this method has grown in popularity, it is costly, has been shown to introduce intramodel bias, and in this work, we find that very large models are often unnecessary. We propose instead to evaluate models using a Panel of LLm evaluators (PoLL). Across three distinct judge settings and spanning six different datasets, we find that using a PoLL composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9944.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9944.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoLL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Panel of LLM Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation method that pools judgments from a heterogeneous panel of multiple LLMs (different model families) and aggregates them (max or average voting) to score model outputs; proposed in this paper as an alternative to using a single large LLM judge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multiple: single-hop QA (KILT/NQ/TQA/HPQA), multi-hop QA (HotPotQA, Bamboogle), Chatbot Arena head-to-head</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Panel composed of Command R (CMD-R), Haiku (Claude family), and GPT-3.5 (three-model panel)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Each judge independently scores outputs according to task-specific prompts (reference-based few-shot prompts for QA, pairwise prompts for Chatbot Arena); PoLL aggregates individual judge scores via voting (max for binary QA correctness, average for 1–5 Chatbot scores).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Cohere internal professional annotators; datasets: some single annotation, some triple-annotation with majority pooling; annotators judged whether reference answer is semantically contained in the generated answer (instructions provided). See Appendix A.3 and dataset-specific annotation counts.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa with human judgements; PoLL kappa on KILT single-hop: NQ=0.763, TQA=0.906, HPQA=0.867 (Table 1). Chatbot Arena correlations: Pearson=0.917, Kendall Tau=0.778 (Table 2). PoLL shows higher correlation to humans than single large judges like GPT-4 in reported settings.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using single LLM judges (especially large, single-family models) risks intra-model biases and unstable judgments; replacing humans with single LLMs can lose robustness to prompt variation, introduces preference for own-model outputs, and can produce higher variance in absolute scores compared to human annotators. Simple single-model LLM judges may also 'over-reason' or inject external/world knowledge rather than perform the intended surface-level matching, causing mismatches with human judgements.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>1) GPT-4 judged as weaker vs humans on KILT single-hop QA (NQ kappa 0.627) and improved only when prompted not to 'overthink'. 2) In Chatbot Arena, GPT-4 ranked another GPT-4 variant higher than human ELO ground truth (showing preference for same-family outputs). 3) Individual judges show larger spreads in delta from human accuracy (GPT-3.5 std=6.1) while PoLL had smallest spread (std=2.2) indicating individual LLM judges deviate more from humans. 4) Prompt sensitivity: GPT-4 kappa on NQ varied from 0.518 (zero-shot) to 0.725 ('don't overthink' few-shot), showing large prompt-driven divergence from human labels (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>PoLL (ensemble of smaller, heterogeneous models) often correlates better with human judgements than a single large judge and is cheaper; prompt engineering (e.g., 'don't overthink') can substantially improve a single LLM judge (GPT-4 improved by +0.07 κ). Also, model-based scoring can still outperform naive heuristics (EM) for many cases. The paper shows that not all LLM-as-judge setups are uniformly worse — selection and prompting matter, and paneling mitigates many losses.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections 2.2, 3.1, 3.6, 4.1-4.4; Tables 1-4; Figures 2-4</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9944.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9944.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_prompt_sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 as a judge: prompt sensitivity and over-reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical finding that GPT-4's agreement with human annotations is highly sensitive to prompt formulation and that GPT-4 can 'over-reason' by injecting world knowledge instead of performing surface-level alignment to the provided reference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>single-hop QA (KILT Natural Questions) and related QA evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Reference-based judging with few-shot in-context examples; ablations included zero-shot, few-shot standard, moving instruction lines, chat-formatted shots, and an explicit 'don't overthink' instruction to constrain reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Professional annotators judged whether the short reference answer is semantically contained in the longer model generation; used as ground truth for kappa comparisons (see Appendix A.3).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa reported per prompt variant on NQ: Zero-shot=0.518, Few-shot standard=0.627, +No Instruction Line=0.594, +Move Instruction Line=0.637, +Chat Formatted Shots=0.561, +'don't overthink'=0.725 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Without careful prompting, GPT-4 may diverge substantially from human judgements: it can 'overthink' (use external knowledge, reinterpret references), giving systematically different correctness calls than humans; zero-shot usage produced markedly lower agreement with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>On NQ, GPT-4's kappa rose from 0.518 (zero-shot) to 0.725 when explicitly instructed not to overthink, demonstrating that default prompting leads to degraded alignment with human labels. The paper hypothesizes over-reasoning (using external/world knowledge) drives some disagreements.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Prompt engineering can recover much of the lost agreement; with the optimized 'don't overthink' prompt GPT-4's kappa approaches or exceeds GPT-3.5's few-shot performance. Thus the loss is not irrecoverable but requires careful prompt specification.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 4.3; Table 3; Appendix A.1.1 (GPT-4 judge prompt ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9944.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9944.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Intra-model_bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intra-model scoring bias (self-preference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observation that LLM evaluators tend to prefer outputs from their own model family, inflating scores for same-family generations and introducing bias absent in human panels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Chatbot Arena head-to-head ranking and multi-model evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (and other single-family judges); evidence also for reductions when using PoLL</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pairwise comparisons for Chatbot Arena and absolute/reference-based scoring for QA; evaluations report ranking deltas and accuracy deltas vs human annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Chatbot Arena crowd-sourced ELO rankings treated as ground truth for rank correlation; Cohere annotators for QA correctness for other datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Observed ranking deviations (Figures 2) and accuracy delta distributions; qualitative example: GPT-4 ranks a GPT-4 variant at position 2 vs its true position 4 in Arena, indicating self-preference. Spread metrics: PoLL std=2.2 vs GPT-3.5 std=6.1 in delta-from-human scores (Section 4.4).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Single-LMM judges can systematically prefer outputs from their own family, producing biased evaluations that misalign with human aggregate judgements and distort model rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>In Chatbot Arena rankings, the GPT-4 judge placed a GPT-4 variant higher than its human-derived ELO rank (position 2 vs actual 4). In multi-hop scoring, the largest positive delta for a model occurs when it is judged by itself (Figure 4), empirically showing self-bias.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Pooling across heterogeneous models (PoLL) substantially reduces intra-model bias and spread, yielding rankings and scores that correlate better with human judgements (Tables 1-2, Figures 2-4).</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 4.4; Figures 2-4; Tables 1 and 2</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9944.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9944.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Variance_vs_human</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Judge variance and consistency relative to human annotators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis showing individual LLM judges exhibit larger variance and less consistency with human annotators than a pooled panel of heterogeneous models; paneling reduces spread and increases agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-hop QA (HotPotQA, Bamboogle), single-hop QA, Chatbot Arena</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Individual models: GPT-3.5, GPT-4, CMD-R, Haiku; Panel: PoLL</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Reference-based few-shot prompts for QA; evaluations measured absolute accuracy delta from human annotator scores across models; PoLL used max/average pooling depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Professional cohort; some datasets triple-annotated with majority vote, others singly annotated (Appendix A.3).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Standard deviation of accuracy delta relative to humans: PoLL std=2.2, GPT-3.5 std=6.1 (Section 4.4). Cohen's kappa values also indicate PoLL often highest (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Replacing humans with single LLM judges can increase variability in judgments (larger spread from human scores) and reduce reliability; prompt changes can further increase instability. This variability means systematic differences from human evaluation decisions are more likely.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Figures 3 and 4 show the distribution of deltas from human scores where individual judges often deviate strongly in either direction; PoLL has the tightest distribution (smallest spread), indicating the individual LLMs are less consistent with humans than the panel.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Some individual LLMs (e.g., Haiku, Command R) achieved high kappa values in some settings; careful panel selection and prompt optimization can recover alignment. Also, trained/fine-tuned judge models in prior work can be effective (cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 4.4; Figures 3-4; Table 1</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9944.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9944.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cost_latency_tradeoff</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cost and latency differences of LLM-as-judge vs PoLL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantified cost comparison showing that a three-model PoLL of smaller models is ~7–8x cheaper than using a single GPT-4 Turbo judge, and potentially lower latency when models are run in parallel.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general evaluation infrastructure for LLM output scoring</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>PoLL (three-model panel) vs GPT-4 Turbo single judge</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pricing reported per input/output token buckets for the specific instance used in experiments: PoLL cost components and GPT-4 Turbo costs as provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Not applicable (cost comparison of automated evaluators), human evaluation used as ground truth elsewhere.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cost numbers: PoLL cost = $1.25/input + $4.25/output; GPT-4 Turbo cost = $10/input + $30/output; overall PoLL is reported 7–8x less expensive (Section 4.5). Latency not explicitly measured but authors note smaller models run in parallel are generally faster than a single big model.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using a single large LLM judge is more costly and can be slower; substituting automated LLM judges for human evaluation trades human judgement fidelity (subjective nuance, potentially superior reliability on some tasks) for cost and throughput, and single LLM judges additionally suffer bias/variance problems noted above.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>The paper does not claim complete loss of human-level judgment from automated judges but reports cost and latency tradeoffs: single GPT-4 judge cost far exceeds PoLL; without paneling, other losses (bias, prompt sensitivity) compound the practical disadvantages of relying solely on one expensive LLM judge.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>PoLL is both cheaper and (empirically) better correlated with human judgements in these settings; thus the cost/latency downside of automation can be mitigated by paneling and model choice.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 4.5</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LLM evaluators recognize and favor their own generations <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers <em>(Rating: 2)</em></li>
                <li>Large language models are not yet human-level evaluators for abstractive summarization <em>(Rating: 2)</em></li>
                <li>PRD: Peer rank and discussion improve large language model based evaluations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9944",
    "paper_id": "paper-1a9d2a8093d2886b27e5854c1f20d873f02edfb0",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "PoLL",
            "name_full": "Panel of LLM Evaluators",
            "brief_description": "An evaluation method that pools judgments from a heterogeneous panel of multiple LLMs (different model families) and aggregates them (max or average voting) to score model outputs; proposed in this paper as an alternative to using a single large LLM judge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "multiple: single-hop QA (KILT/NQ/TQA/HPQA), multi-hop QA (HotPotQA, Bamboogle), Chatbot Arena head-to-head",
            "llm_judge_model": "Panel composed of Command R (CMD-R), Haiku (Claude family), and GPT-3.5 (three-model panel)",
            "llm_judge_setup": "Each judge independently scores outputs according to task-specific prompts (reference-based few-shot prompts for QA, pairwise prompts for Chatbot Arena); PoLL aggregates individual judge scores via voting (max for binary QA correctness, average for 1–5 Chatbot scores).",
            "human_evaluation_setup": "Cohere internal professional annotators; datasets: some single annotation, some triple-annotation with majority pooling; annotators judged whether reference answer is semantically contained in the generated answer (instructions provided). See Appendix A.3 and dataset-specific annotation counts.",
            "agreement_metric": "Cohen's kappa with human judgements; PoLL kappa on KILT single-hop: NQ=0.763, TQA=0.906, HPQA=0.867 (Table 1). Chatbot Arena correlations: Pearson=0.917, Kendall Tau=0.778 (Table 2). PoLL shows higher correlation to humans than single large judges like GPT-4 in reported settings.",
            "losses_identified": "Using single LLM judges (especially large, single-family models) risks intra-model biases and unstable judgments; replacing humans with single LLMs can lose robustness to prompt variation, introduces preference for own-model outputs, and can produce higher variance in absolute scores compared to human annotators. Simple single-model LLM judges may also 'over-reason' or inject external/world knowledge rather than perform the intended surface-level matching, causing mismatches with human judgements.",
            "examples_of_loss": "1) GPT-4 judged as weaker vs humans on KILT single-hop QA (NQ kappa 0.627) and improved only when prompted not to 'overthink'. 2) In Chatbot Arena, GPT-4 ranked another GPT-4 variant higher than human ELO ground truth (showing preference for same-family outputs). 3) Individual judges show larger spreads in delta from human accuracy (GPT-3.5 std=6.1) while PoLL had smallest spread (std=2.2) indicating individual LLM judges deviate more from humans. 4) Prompt sensitivity: GPT-4 kappa on NQ varied from 0.518 (zero-shot) to 0.725 ('don't overthink' few-shot), showing large prompt-driven divergence from human labels (Table 3).",
            "counterexamples_or_caveats": "PoLL (ensemble of smaller, heterogeneous models) often correlates better with human judgements than a single large judge and is cheaper; prompt engineering (e.g., 'don't overthink') can substantially improve a single LLM judge (GPT-4 improved by +0.07 κ). Also, model-based scoring can still outperform naive heuristics (EM) for many cases. The paper shows that not all LLM-as-judge setups are uniformly worse — selection and prompting matter, and paneling mitigates many losses.",
            "paper_reference": "Sections 2.2, 3.1, 3.6, 4.1-4.4; Tables 1-4; Figures 2-4",
            "uuid": "e9944.0",
            "source_info": {
                "paper_title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4_prompt_sensitivity",
            "name_full": "GPT-4 as a judge: prompt sensitivity and over-reasoning",
            "brief_description": "Empirical finding that GPT-4's agreement with human annotations is highly sensitive to prompt formulation and that GPT-4 can 'over-reason' by injecting world knowledge instead of performing surface-level alignment to the provided reference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "single-hop QA (KILT Natural Questions) and related QA evaluation",
            "llm_judge_model": "GPT-4",
            "llm_judge_setup": "Reference-based judging with few-shot in-context examples; ablations included zero-shot, few-shot standard, moving instruction lines, chat-formatted shots, and an explicit 'don't overthink' instruction to constrain reasoning.",
            "human_evaluation_setup": "Professional annotators judged whether the short reference answer is semantically contained in the longer model generation; used as ground truth for kappa comparisons (see Appendix A.3).",
            "agreement_metric": "Cohen's kappa reported per prompt variant on NQ: Zero-shot=0.518, Few-shot standard=0.627, +No Instruction Line=0.594, +Move Instruction Line=0.637, +Chat Formatted Shots=0.561, +'don't overthink'=0.725 (Table 3).",
            "losses_identified": "Without careful prompting, GPT-4 may diverge substantially from human judgements: it can 'overthink' (use external knowledge, reinterpret references), giving systematically different correctness calls than humans; zero-shot usage produced markedly lower agreement with humans.",
            "examples_of_loss": "On NQ, GPT-4's kappa rose from 0.518 (zero-shot) to 0.725 when explicitly instructed not to overthink, demonstrating that default prompting leads to degraded alignment with human labels. The paper hypothesizes over-reasoning (using external/world knowledge) drives some disagreements.",
            "counterexamples_or_caveats": "Prompt engineering can recover much of the lost agreement; with the optimized 'don't overthink' prompt GPT-4's kappa approaches or exceeds GPT-3.5's few-shot performance. Thus the loss is not irrecoverable but requires careful prompt specification.",
            "paper_reference": "Section 4.3; Table 3; Appendix A.1.1 (GPT-4 judge prompt ablation)",
            "uuid": "e9944.1",
            "source_info": {
                "paper_title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Intra-model_bias",
            "name_full": "Intra-model scoring bias (self-preference)",
            "brief_description": "Observation that LLM evaluators tend to prefer outputs from their own model family, inflating scores for same-family generations and introducing bias absent in human panels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Chatbot Arena head-to-head ranking and multi-model evaluation",
            "llm_judge_model": "GPT-4 (and other single-family judges); evidence also for reductions when using PoLL",
            "llm_judge_setup": "Pairwise comparisons for Chatbot Arena and absolute/reference-based scoring for QA; evaluations report ranking deltas and accuracy deltas vs human annotators.",
            "human_evaluation_setup": "Chatbot Arena crowd-sourced ELO rankings treated as ground truth for rank correlation; Cohere annotators for QA correctness for other datasets.",
            "agreement_metric": "Observed ranking deviations (Figures 2) and accuracy delta distributions; qualitative example: GPT-4 ranks a GPT-4 variant at position 2 vs its true position 4 in Arena, indicating self-preference. Spread metrics: PoLL std=2.2 vs GPT-3.5 std=6.1 in delta-from-human scores (Section 4.4).",
            "losses_identified": "Single-LMM judges can systematically prefer outputs from their own family, producing biased evaluations that misalign with human aggregate judgements and distort model rankings.",
            "examples_of_loss": "In Chatbot Arena rankings, the GPT-4 judge placed a GPT-4 variant higher than its human-derived ELO rank (position 2 vs actual 4). In multi-hop scoring, the largest positive delta for a model occurs when it is judged by itself (Figure 4), empirically showing self-bias.",
            "counterexamples_or_caveats": "Pooling across heterogeneous models (PoLL) substantially reduces intra-model bias and spread, yielding rankings and scores that correlate better with human judgements (Tables 1-2, Figures 2-4).",
            "paper_reference": "Section 4.4; Figures 2-4; Tables 1 and 2",
            "uuid": "e9944.2",
            "source_info": {
                "paper_title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Variance_vs_human",
            "name_full": "Judge variance and consistency relative to human annotators",
            "brief_description": "Analysis showing individual LLM judges exhibit larger variance and less consistency with human annotators than a pooled panel of heterogeneous models; paneling reduces spread and increases agreement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "multi-hop QA (HotPotQA, Bamboogle), single-hop QA, Chatbot Arena",
            "llm_judge_model": "Individual models: GPT-3.5, GPT-4, CMD-R, Haiku; Panel: PoLL",
            "llm_judge_setup": "Reference-based few-shot prompts for QA; evaluations measured absolute accuracy delta from human annotator scores across models; PoLL used max/average pooling depending on task.",
            "human_evaluation_setup": "Professional cohort; some datasets triple-annotated with majority vote, others singly annotated (Appendix A.3).",
            "agreement_metric": "Standard deviation of accuracy delta relative to humans: PoLL std=2.2, GPT-3.5 std=6.1 (Section 4.4). Cohen's kappa values also indicate PoLL often highest (Table 1).",
            "losses_identified": "Replacing humans with single LLM judges can increase variability in judgments (larger spread from human scores) and reduce reliability; prompt changes can further increase instability. This variability means systematic differences from human evaluation decisions are more likely.",
            "examples_of_loss": "Figures 3 and 4 show the distribution of deltas from human scores where individual judges often deviate strongly in either direction; PoLL has the tightest distribution (smallest spread), indicating the individual LLMs are less consistent with humans than the panel.",
            "counterexamples_or_caveats": "Some individual LLMs (e.g., Haiku, Command R) achieved high kappa values in some settings; careful panel selection and prompt optimization can recover alignment. Also, trained/fine-tuned judge models in prior work can be effective (cited works).",
            "paper_reference": "Section 4.4; Figures 3-4; Table 1",
            "uuid": "e9944.3",
            "source_info": {
                "paper_title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Cost_latency_tradeoff",
            "name_full": "Cost and latency differences of LLM-as-judge vs PoLL",
            "brief_description": "Quantified cost comparison showing that a three-model PoLL of smaller models is ~7–8x cheaper than using a single GPT-4 Turbo judge, and potentially lower latency when models are run in parallel.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "general evaluation infrastructure for LLM output scoring",
            "llm_judge_model": "PoLL (three-model panel) vs GPT-4 Turbo single judge",
            "llm_judge_setup": "Pricing reported per input/output token buckets for the specific instance used in experiments: PoLL cost components and GPT-4 Turbo costs as provided.",
            "human_evaluation_setup": "Not applicable (cost comparison of automated evaluators), human evaluation used as ground truth elsewhere.",
            "agreement_metric": "Cost numbers: PoLL cost = $1.25/input + $4.25/output; GPT-4 Turbo cost = $10/input + $30/output; overall PoLL is reported 7–8x less expensive (Section 4.5). Latency not explicitly measured but authors note smaller models run in parallel are generally faster than a single big model.",
            "losses_identified": "Using a single large LLM judge is more costly and can be slower; substituting automated LLM judges for human evaluation trades human judgement fidelity (subjective nuance, potentially superior reliability on some tasks) for cost and throughput, and single LLM judges additionally suffer bias/variance problems noted above.",
            "examples_of_loss": "The paper does not claim complete loss of human-level judgment from automated judges but reports cost and latency tradeoffs: single GPT-4 judge cost far exceeds PoLL; without paneling, other losses (bias, prompt sensitivity) compound the practical disadvantages of relying solely on one expensive LLM judge.",
            "counterexamples_or_caveats": "PoLL is both cheaper and (empirically) better correlated with human judgements in these settings; thus the cost/latency downside of automation can be mitigated by paneling and model choice.",
            "paper_reference": "Section 4.5",
            "uuid": "e9944.4",
            "source_info": {
                "paper_title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LLM evaluators recognize and favor their own generations",
            "rating": 2
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers",
            "rating": 2
        },
        {
            "paper_title": "Large language models are not yet human-level evaluators for abstractive summarization",
            "rating": 2
        },
        {
            "paper_title": "PRD: Peer rank and discussion improve large language model based evaluations",
            "rating": 1
        }
    ],
    "cost": 0.01446225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models</h1>
<p>Pat Verga<br>Sebastian Hofstätter, Sophia Althammer, Yixuan Su<br>Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White<br>Patrick Lewis<br>Cohere</p>
<h4>Abstract</h4>
<p>As Large Language Models (LLMs) have become more advanced, they have outpaced our abilities to accurately evaluate their quality. Not only is finding data to adequately probe particular model properties difficult, but evaluating the correctness of a model's free-form generation alone is a challenge. To address this, many evaluations now rely on using LLMs themselves as judges to score the quality of outputs from other LLMs. Evaluations most commonly use a single large model like GPT-4. While this method has grown in popularity, it is costly, has been shown to introduce intra-model bias, and in this work, we find that very large models are often unnecessary. We propose instead to evaluate models using a Panel of LLm evaluators (PoLL). Across three distinct judge settings and spanning six different datasets, we find that using a PoLL composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive.</p>
<h2>1 Introduction</h2>
<p>Evaluating generative language models is a challenging task: not only is it difficult to find meaningful data to test the models, but evaluating the correctness of a generated response is itself a challenge. Multiple choice datasets like MMLU (Hendrycks et al., 2020) have become popular in part by side-stepping the difficulty of evaluating generations. However, multiple-choice questions are in many ways probing a different property than that of a free-form generative task, which is oftentimes closer to the downstream use-case.</p>
<p>Many automatic metrics have been used across various tasks such as BLEU in machine translation (Papineni et al., 2002), ROUGE for summa-
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Top: Rankings of model performance change drastically depending on which LLM is used as the judge on KILT-NQ. Bottom: The Panel of LLm evaluators (PoLL) has the highest Cohen's $\kappa$ correlation with human judgements.
rization (Lin, 2004), and heuristic string match methods, such as normalized exact match (EM) and token level F1 for question answering (Rajpurkar et al., 2016). However, these simplistic methods commonly fail to analyze the intended property of interest. QA metrics, for example, invariably lead to both false positive failures (e.g. superfluous token overlap) and more commonly false negatives due to an incomplete set of gold reference answers (e.g. date format differences ${ }^{1}$, inclusion of middle initial in person's name, etc.).</p>
<p>More recent methods have attempted to address these issues by instead using trained or prompted models as evaluators (Sellam et al., 2020; Zheng</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>et al., 2024; Li et al., 2024b; Kocmi and Federmann, 2023a; Shen et al., 2023; Li et al., 2023). Prior work has shown that model-based scoring methods often correlate better with human judgements than heuristic metrics like EM (Bohnet et al., 2022; Zheng et al., 2024) and that strong evaluator models generalize well across different tasks (Huang et al., 2024).</p>
<p>Unfortunately, while the use of LLMs like GPT-4 as evaluators has become increasingly common, it has also been observed that evaluator models tend to have their own biases; often recognizing and preferring their own outputs over those of other models (Panickssery et al., 2024). Additionally, it is most common to use the largest, most universally capable models as evaluators, which is both slow and costly, limiting applicability and access.</p>
<p>In this paper, we perform experiments across three settings (single-hop QA, multi-hop QA, and Chatbot Arena), spanning six datasets, and make the following contributions:</p>
<ol>
<li>We propose to evaluate LLM generations using a Panel of LLm evaluators (PoLL) drawn from different model families rather than a single large judge (Section 2).</li>
<li>We show that using an instantiation of PoLL correlates better with human judgements compared to a single large judge (GPT-4), while being over seven times cheaper (Sections 4.1 and 4.2).</li>
<li>In some scenarios, GPT-4 is a relatively weak judge, exhibiting high variance with minor changes to the prompt (Section 4.3).</li>
<li>Intra-model scoring bias is reduced by pooling judgements across a panel of heterogeneous evaluator models (Section 4.4).</li>
</ol>
<h2>2 Methods</h2>
<h3>2.1 Background: LLM as a Judge</h3>
<p>A judge evaluator model $J$ is used to score the output $a$ from a test model $A$.
Single-point Scoring In some settings, $J$ is tasked with rating the quality of a single model output independently of any point of comparison (e.g. (Kocmi and Federmann, 2023b)). $J_{\text {prompt }}$ will often include natural language instructions on how the grading should be performed (i.e. what properties constitute a good or bad output). Aside from the prompt instructions, the rating is based solely on $J$ 's internal model of what a quality output is. Here, score $=J(a)$.
Reference-based Scoring In other cases, the model is provided with some 'gold' reference $r$, which contains the information that should be included in $a$ (e.g. (Zhu et al., 2023)). For example, in QA the reference would be the 'correct' answer to the question. In this case, score $=J(a, r)$. This setting is explored in Sections 3.3 and 3.4.
Pair-wise Scoring Another very common setting is in pair-wise scoring where the goal is to choose which of two outputs is better (e.g. (Zheng et al., 2024)). Given outputs $a$ and $b$ generated by two models $A$ and $B$, an evaluator $J$ compares them and generates a preference score over the outputs as score $=J(a, b)^{2}$. The form of the score can vary based on the use-case, but it is often a three or five point scale such as $a&gt;b, a \approx b, a&lt;b$. This setting is used in Section 3.5.</p>
<h3>2.2 Panel of LLM Evaluators</h3>
<p>The above settings assume that all scoring is performed by a single capable judge. However, as outlined earlier, one of the largest issues with relying on a single model $J$, such as GPT-4, is that it introduces intra-model bias. To address this we instead propose to score answer correctness based not on a single judge, but instead on a panel composed of multiple evaluator models. Similar pooling techniques are used to reduce variance in human annotations by normalizing out both natural variation in human judgements caused by their own subjective biases as well as human error (Voorhees, 1998). In prior relevant work, Li et al. (2023) proposed an evaluation method with multiple judges to reduce bias though only looked at large models on pair-wise evaluations.</p>
<p>To calculate the PoLL score, each evaluator model independently scores a given model output just as they would in any of the scenarios outlined above. Those individual scores are then pooled together through a voting function ${ }^{3}$ such that the final score $=f(j \in P: j(a))$ where $P$ is a panel composed of individual judges $j$ and $f$ is a voting function.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>3 Experimental Settings</p>
<h3>3.1 PoLL Composition and Voting</h3>
<p>In our experiments, We construct a PoLL from three models being drawn from three disparate model families (Command R, Haiku, and GPT-3.5). We consider two different voting functions for aggregating scores across the judges. For QA datasets, we use max voting, as all judgements are binary [correct, incorrect]. For Chatbot Arena we instead use average pooling because judgements are scores ranging from 1-5 and a three judge panel often does not produce a clear majority decision.</p>
<h3>3.2 Model Families</h3>
<p>Command R Family: Command R (CMD-R, 35B) and Command R+ (CMD-R+, 103B) are open weight models created by Cohere. We consider Command R as one of the models in the PoLL.
GPT Family: GPT-3.5 and GPT-4 <em>OpenAI et al. (2024)</em> are two of the most widely used models. GPT-4 is typically the default choice for models as judges. We use GPT-3.5 as a member of our PoLL.
Claude-3 Family: Models built by Anthropic include (in increasing size) Haiku, Sonnet, and Opus <em>Anthropic (2024)</em>. We include Haiku in our PoLL.
Mistral Family: Mistral Large (Mistral-LG) and Mistral Medium (Mistral-MD) are proprietary models created by Mistral. We did not conduct experiments with Mistral judges but evaluate their generations as a point of comparison to have a model 'unaffiliated' with any judges.</p>
<h3>3.3 Single-hop Question Answering</h3>
<p>Our question answering (QA) tasks are open-book settings where a model $m$ is given a question $q$ and must retrieve evidence $e$ from some retrieval system (such as the internet or dense index over wikipedia) and must generate an answer $g$ as $g=$ $m(q, e)$. Each instance is accompanied by a human annotated reference answer.</p>
<p>We run experiments on datasets from KILT <em>Petroni et al. (2021)</em> versions of Natural Questions (NQ) <em>Kwiatkowski et al. (2019)</em>, TriviaQA (TQA) <em>Joshi et al. (2017)</em>, and HotpotQA (HPQA) <em>Yang et al. (2018)</em> (see Section A.1.1 for more details).</p>
<h3>3.4 Multi-hop Question Answering</h3>
<p>Multi-hop question answering is similar to the single-hop setting described in Section 3.3. However, the questions are designed such that models must perform multiple rounds of retrieval to answer sub-questions and collect sufficient evidence to ultimately answer the initial question. We perform experiments on two datasets: Bamboogle <em>Press et al. (2023)</em> and HPQA. See Section A.1.2 for additional details.</p>
<h3>3.5 Chatbot Arena Hard</h3>
<p>Chatbot Arena is a popular benchmark for evaluating LLM head-to-head performance. This crowd-sourced effort has users prompt a pair of anonymized LLMs and rate which output is better <em>Zheng et al. (2024)</em>.</p>
<p>Recently, <em>Li et al. (2024a)</em> mined a 'hard' subset of 500 arena prompts and created an automatic evaluation where an LLM judge (GPT-4) would score a model output against a baseline generation (coming from a different version of GPT-4) in a head-to-head comparison. In their analysis, they determined that this evaluation correlated better with the human judgements from the original Chatbot Arena annotations when compared against other commonly used automated evaluation benchmarks.</p>
<p>At the time of writing, the repository only contained generated answers from GPT-3.5 and GPT-4. We therefore generated new outputs for all remaining models and subsequently scored generated answers with the various judges using the original codebase from <em>Li et al. (2024a)</em>. The raw scores can be found in Table 7. Following the original work, we treat Chatbot Arena crowdsourced annotations as ground truth for calculating correlation between evaluator models and human judgements.</p>
<h3>3.6 Prompting Judges</h3>
<p>As alluded to in Section 2.1, judge models need to be prompted in different ways depending on the particular task setup. Our QA experiments use reference-based scoring and our models prompts contain few-shot in-context examples of valid and invalid $q, a, r$ triples. We also experiment with</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>variations on these prompts in Section 4.3. Further details on judge prompts may be found in the appendix.</p>
<p>As an additional point of reference in our QA experiments, we include the commonly used exact match (EM) metric judgements. We specifically use the 'containment' version of EM from prior work which is more amenable to LLM long-form generation <em>Liu et al. (2024)</em> and checks if a reference answer string appears within the generated model response (after normalization).</p>
<p>In our experiments on Chatbot Arena Hard, we follow the same pair-wise scoring setup of the original work. We do not modify the judge prompts in any way from the original implementation.</p>
<h3>3.7 Human Judgements</h3>
<p>To gather human reference judgements, we utilized Cohere’s internal highly-qualified annotation workforce. Annotators were shown a single anonymized model generated answer at a time along with the original question and reference answer. Annotators were asked to judge whether the reference answer is semantically contained inside the generated answer. See Section A.3 for additional details</p>
<h2>4 Results</h2>
<h3>4.1 Correlation to Human Judgements</h3>
<p>Cohen’s $\kappa$ Correlation Cohen’s kappa measures inter-rater reliability, which quantifies the level of agreement between two or more raters or judges. The kappa statistic takes into account the possibility of agreement occurring by chance, making it a more robust measure than simple percent agreement.</p>
<p>The formula for Cohen’s kappa is:</p>
<p>$$
\kappa=\frac{p_{o}-p_{e}}{1-p_{e}}
$$</p>
<p>where $p_{o}$ is the observed agreement proportion and $p_{e}$ is the expected agreement proportion by chance. This results in a value of $\kappa$ ranging from -1 to 1 . Opinions vary on how scores should be interpreted, but in general $\kappa&gt;0.8$ is considered a strong correlation and $\kappa&gt;0.6$ is a moderate correlation.</p>
<p>In Table 1 we can see how the ratings from different evaluator judges, on different single-hop QA</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>datasets from KILT, correlate with human judgements as measured by $\kappa$. We see that overall, PoLL has the strongest correlation across various tasks, while GPT-4 is one of the weaker evaluators on this particular task setup (See 4.3 for further analysis).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Judge</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">TQA</th>
<th style="text-align: center;">HPQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">EM</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.827</td>
<td style="text-align: center;">0.662</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">0.627</td>
<td style="text-align: center;">0.841</td>
<td style="text-align: center;">0.830</td>
</tr>
<tr>
<td style="text-align: left;">CMD-R</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">$\underline{0.902}$</td>
<td style="text-align: center;">0.815</td>
</tr>
<tr>
<td style="text-align: left;">Haiku</td>
<td style="text-align: center;">$\underline{0.749}$</td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 3}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.859</td>
<td style="text-align: center;">0.833</td>
</tr>
<tr>
<td style="text-align: left;">PoLL</td>
<td style="text-align: center;">$\mathbf{0 . 7 6 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 0 6}$</td>
<td style="text-align: center;">$\underline{0.867}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Cohen’s Kappa Judge Model Performance on Different Single-hop QA Datasets from KILT, where best results are indicated by the bold font and second best results are underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Judge/Correlation</th>
<th style="text-align: center;">Pearson</th>
<th style="text-align: center;">Kendall Tau</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">0.817</td>
<td style="text-align: center;">0.667</td>
</tr>
<tr>
<td style="text-align: left;">Haiku</td>
<td style="text-align: center;">0.883</td>
<td style="text-align: center;">0.722</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">0.883</td>
<td style="text-align: center;">0.730</td>
</tr>
<tr>
<td style="text-align: left;">CMD-R</td>
<td style="text-align: center;">0.817</td>
<td style="text-align: center;">0.676</td>
</tr>
<tr>
<td style="text-align: left;">PoLL</td>
<td style="text-align: center;">$\mathbf{0 . 9 1 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 7 8}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Pearson and Kendall-Tau correlations between different judge models as compared to the rankings produced by the Chatbot Arena overall leaderboard.</p>
<h3>4.2 Rank Correlation on Chatbot Arena</h3>
<p>In Table 2 we can see how different judge model rankings correlate with human judgement. Following Li et al. (2024a), we treat the crowd-sourced ELO rankings from Chatbot Arena as ground truth. We calculate both Kendall Tau (Kendall, 1938) and Pearson Correlation <em>Pearson (1895)</em> of the ranked list produced by each of the judge methods with respect to this ground truth ranking. We find that PoLL is best correlated with the gold rankings, particularly at the top of the ranked list as shown in Figure 2.</p>
<h3>4.3 Judgement Variance by Prompt Changes</h3>
<p>Based on the observation that GPT-4 was the weakest judge model on our KILT evaluations, we investigated how the model reacts to modifications to its prompt. GPT-4 is the most powerful judge model</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Rankings of model performance on Chatbot Arena Hard judged by GPT-4 or PoLL. Ranks are compared to those in the original Chatbot Arena.</p>
<p>we tested, yet it performed worse than less capable models on what is essentially a fuzzy string matching exercise. We hypothesize that may be because GPT-4 is over-reasoning and injecting too much background knowledge into determining the correctness of an answer rather than simply aligning the gold reference with the generation. In Table 3, we can see how the correlation between GPT-4 and human annotators varies as the prompt changes. In all cases, having in-context examples improves the performance over zero-shot and the most effective strategy is an explicit instruction to the model not to 'overthink' and not to concern itself with the wider factuality of the answers with respect to the outside world.<sup>10</sup> These changes bring the agreement level for GPT-4 up to the level of GPT-3.5 when using our few-shot standard prompt, though still below Command-R, Haiku, and PoLL.</p>
<table>
<thead>
<tr>
<th>Prompt Variant</th>
<th>Kappa</th>
</tr>
</thead>
<tbody>
<tr>
<td>Zero-shot</td>
<td>0.518</td>
</tr>
<tr>
<td>Few-Shot Standard</td>
<td>0.627</td>
</tr>
<tr>
<td>+No Instruction Line</td>
<td>0.594</td>
</tr>
<tr>
<td>+Move Instruction Line</td>
<td>0.637</td>
</tr>
<tr>
<td>+Chat Formatted Shots</td>
<td>0.561</td>
</tr>
<tr>
<td>+'don't overthink'</td>
<td>0.725</td>
</tr>
</tbody>
</table>
<p>Table 3: Kappa values on NQ for different prompt variants with GPT-4 as judge. . For reference, GPT3.5 with the few-shot standard prompt achieves 0.726 (table 1)</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Accuracy changes of different evaluation judges as compared to human judgements on HotPotQA (multi-hop).</p>
<h3>4.4 Judge Bias and Consistency</h3>
<p>One of the biggest motivators for replacing a single large judge with a panel of heterogeneous models is to reduce bias in evaluation. To analyze to what extent this was the case, we compared the delta in absolute accuracy score for our individual judges and PoLL relative to scores by human annotators across our multi-hop datasets. Figures 3 and 4 show results on HotPotQA and Bamboogle. We can see how the different judges score different models and how far those predictions deviate from human annotator decisions (the dotted line at 0). We observe that overall, PoLL has the smallest spread in scores, with a standard deviation of 2.2, compared to EM and individual judges. GPT-3.5 has the highest spread, with a standard deviation of 6.1. We also see in Figure 4 that the highest positive delta for each individual model being scored occurs when it is judged by itself.</p>
<p>In Figure 2, we assessed the model rankings produced on Chatbot Arena Hard by PoLL as compared to the GPT-4 judge from the original work. The 'gold' ranking appears on the diagonal and represents the rankings from the original Chatbot Arena ELO. We find that PoLL rankings correlate better with the ground truth, particularly at the top of the ranked list. We can clearly observe intra-model bias as the GPT-4 judge ranks another GPT-4 variant in position 2, higher than its actual position 4, which is in line with previous works that have also observed GPT-4's preference for its own generations (Zheng et al., 2024; Panickssery et al., 2024).</p>
<p><sup>10</sup>the 'don't overthink' prompt had a negligible positive effect on Haiku, minor negative effect to Command R and a severe negative impact on GPT-3.5.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Accuracy changes of different evaluation judges as compared to human judgements on Bamboogle.</p>
<h3>4.5 Cost and Latency</h3>
<p>At the time of writing, the cost of running our specific instance of PoLL is $1.25/input^{11} + $4.25/output, whereas the cost of running GPT-4 Turbo is $10/input + $30/output. Depending on the ratio of input-to-output tokens in a given task, running the entire three model PoLL is seven to eight times less expensive than running a single GPT-4 judge.</p>
<p>We did not run explicit latency evaluations and many factors such as model choice, serving platform, and more can impact speed. In general though, running a collection of smaller models in parallel (as in PoLL) is faster than a single big model.</p>
<h3>5 Conclusions and Limitations</h3>
<p>In this paper, we showed how a Panel of LLM Evaluators composed of smaller models is not only an effective method for evaluating LLM performance, but also reduces intra-model bias, latency, and cost. The benefits of PoLL are bolstered by the finding that there is not a single 'best' judge across all settings, while PoLL performs well consistently.</p>
<p>In this work we investigated only three evaluator settings and a limited number of judges and panel compositions. While we showed that PoLL is an effective alternative to a single large model in these settings, further work is needed to see how broadly applicable the method is, for example, in math or reasoning evaluations, where language models often struggle (Zheng et al., 2024). We also leave the task of 'panel selection', or identifying the best models to include in PoLL in terms of quality and cost, to future work.</p>
<h3>Acknowledgement</h3>
<p>We'd like to thank all of the team at Cohere and in particular Maxime Voisin, Victoria Spooner, Priyanka Sen, Harry Moynehan, Erik Arakelyan, Antoine Debugne, Alex Barbet, Max Bartolo, Acyr Locatelli, Sara Hooker, and Phil Blunsom. Thanks also to Emma Strubell.</p>
<h3>References</h3>
<ul>
<li>Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku.</li>
<li>Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Massimiliano Ciaramita, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, et al. 2022. Attributed question answering: Evaluation and modeling for attributed large language models. <em>arXiv preprint arXiv:2212.08037</em>.</li>
<li>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. In <em>International Conference on Learning Representations</em>.</li>
<li>Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Tiejun Zhao. 2024. An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers. <em>arXiv preprint arXiv:2403.02839</em>.</li>
<li>Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In <em>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1601–1611, Vancouver, Canada. Association for Computational Linguistics.</li>
<li>Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Yu Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020. Dense passage retrieval for open-domain question answering. <em>ArXiv</em>, abs/2004.04906.</li>
<li>Maurice G Kendall. 1938. A new measure of rank correlation. <em>Biometrika</em>, 30(1/2):81–93.</li>
<li>Tom Kocmi and Christian Federmann. 2023a. Large language models are state-of-the-art evaluators of translation quality. In <em>Proceedings of the 24th Annual Conference of the European Association for Machine Translation</em>, pages 193–203, Tampere, Finland. European Association for Machine Translation.</li>
<li>Tom Kocmi and Christian Federmann. 2023b. Large language models are state-of-the-art evaluators of translation quality. <em>arXiv preprint arXiv:2302.14520</em>.</li>
</ul>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>Ruosen Li, Teerth Patel, and Xinya Du. 2023. Prd: Peer rank and discussion improve large language model based evaluations. arXiv preprint arXiv:2307.02762.</p>
<p>Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. 2024a. From live data to high-quality benchmarks: The arena-hard pipeline.</p>
<p>Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu , and Chongyang Tao. 2024b. Leveraging large language models for nlg evaluation: A survey. arXiv e-prints, pages arXiv-2401.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the Middle: How Language Models Use Long Contexts. Transactions of the Association for Computational Linguistics, 12:157-173.</p>
<p>OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha GontijoLopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,</p>
<p>Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciak, Andrew Kondrich, Aris Konstantinidis, Kyle Kosc, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Einy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. Gpt-4 technical report.</p>
<p>Arjun Panickssery, Samuel R. Bowman, and Shi Feng. 2024. Llm evaluators recognize and favor their own generations. arXiv e-prints, pages arXiv-2401.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evalu-</p>
<p>ation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318.</p>
<p>Karl Pearson. 1895. Vii. note on regression and inheritance in the case of two parents. proceedings of the royal society of London, 58(347-352):240-242.</p>
<p>Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. 2021. Kilt: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523-2544.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5687-5711.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and Lidong Bing. 2023. Large language models are not yet human-level evaluators for abstractive summarization. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 4215-4233, Singapore. Association for Computational Linguistics.</p>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36.</p>
<p>Ellen M. Voorhees. 1998. Variations in relevance judgments and the measurement of retrieval effectiveness. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '98, page 315-323, New York, NY, USA. Association for Computing Machinery.</p>
<p>Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023. Improving text embeddings with large language models. ArXiv, abs/2401.00368.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for
diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36.</p>
<p>Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. Judgelm: Fine-tuned large language models are scalable judges. arXiv preprint arXiv:2310.17631.</p>
<h2>Appendix</h2>
<h2>A Appendix</h2>
<h2>A. 1 Additional Experimental Details</h2>
<h2>A.1.1 Single-hop QA</h2>
<p>Datasets KILT (Petroni et al., 2021) is a popular benchmark that measures a model's open domain question-answering capabilities with 1) a retrieval step to retrieve relevant documents from a knowledge source and 2) an answer generation step taking into account the relevant documents from (1). We select three popular test sets from the KILT benchmark, Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (TQA) (Joshi et al., 2017), and HotpotQA (HPQA) (Yang et al., 2018) for evaluation. The Natural Questions test set consists of real user questions sampled from a popular web search engine. TriviaQA is a test set with complex questions written by trivia enthusiasts. HotpotQA is a multi-hop question answering test set where questions require finding and reasoning over multiple supporting documents to answer. For all three test sets, the knowledge source is the corpus of Wikipedia documents form the KILT benchmark (Petroni et al., 2021).</p>
<p>KILT Experimental Details: For KILT, we use the DPR-formatted 100-word chunked KILT wikipedia dump (Karpukhin et al., 2020; Petroni et al., 2021). ${ }^{12}$ Then, for all questions, we retrieve and cache the top-10 snippets from the dump, using the GTE-large dense retrieval model (Wang et al., 2023), ready to be used to generate answers from different models.</p>
<p>Generating answers for judgement: To generate answers from all considered models, we ask the question as an unmodified chat message and pass in the snippets using the dedicated documents parameter of the model family's API where available (Command-R Family) ${ }^{13}$. For model families that do no have a specific documents api parameter (GPT, we instead adopt the question answering prompt template used in Liu et al. (2024), which is shown in table 9.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Judge Prompts We use the same prompttemplate structure for KILT judgements as for Multihop judgements. We initially observed relativelylower levels of human-agreement when using the multihop prompt verbatim applied to KILT results experiments. We hypothesise this was due to mismatch in model-generated answer styles in the fewshot examples. We therefore create new judgement prompts the following procedure for each of the KILT datasets, randomly sampling a set of labelled fewshot examples from our pool human-labelled examples according to the following constraints:</p>
<ul>
<li>at least 1 annotated positive example from each model family's generations</li>
<li>at least 1 annotated negative example from each model family's generations</li>
<li>at least 1 example where the human disagrees with keyword-based exact match score.</li>
</ul>
<p>We sweep over n_shots in ${8,10,16}$, picking the prompt that has strongest agreement on average across all judges for a small held-out portion of human labels. The resuling judgement prompts used for KILT in our experiments in the main paper are listed in tables 12, 11 and 13.</p>
<p>GPT-4 judge prompt ablation Here we include some additional details on prompt ablation for KILT NaturalQuestions mentioned in section 4.3:</p>
<ul>
<li>Zero-shot: the natural language instruction is supplied as system call. We then directly ask the model to judge with no labelled examples</li>
<li>Few-shot Standard: This is the prompt in table 11 which is used for all other model judges.</li>
<li>No instruction Line: Here we remove the natural language instruction from the Few-shot standard prompt, on the hypothesis the instruction is confusing the model. This hypothesis turns out to be false as agreement actually drops. $(-0.03 \Delta \kappa)$</li>
<li>Move instruction Line: Here we modify the standard fewshot prompt by moving the instruction line into a separate system call. This results in a small improvement $(+0.01 \Delta \kappa)$</li>
<li>Chat-formatted shots: Here we modify the standard fewshot prompt by formatting each</li>
</ul>
<p>fewshot example as a conversational turn between the user and the assistant. This reduces performance $(-0.07 \Delta \kappa)$</p>
<ul>
<li>'don't overthink': Here, we replace the instruction line from the standard prompt's You will be given a Question and a Provided Answer. Judge whether the Provided Answer is correct by comparing it to the Reference Answer. Differently formatted dates, people with missing middle names, and alternative spellings should all be considered the same. If the Provided Answer is correct say exactly "True", otherwise say "False". to a wording which is intended to encourage the model to perform a simpler function and not incorporate external knowledge: "You are judging whether a model has generated a correct answer to a question. Study the examples the user gives you as they will be very informative for how to do the task. The Reference Answers you get will be short. An model's answer will be longer, and can be considered correct if it contains the semantic content of short reference answer somewhere within it. Don't worry about factuality with respect to the real world, just judge the example based on what you see. No need to overthink this task, it really comes down to just soft matching. This improves results for GPT-4 by $+0.07 \Delta \kappa$. Additional small surface level changes and moving the instruction to a system call lead to an additional $+0.03 \Delta \kappa$. The final optimized prompt for GPT-4 can be found in table 14.</li>
</ul>
<p>The agreements for different models using the prompt optimized for GPT-4 can be found in table 4.</p>
<h3>A.1.2 Multihop</h3>
<p>Bamboogle (Press et al., 2023) is 125 questions adversarially constructed to require multiple internet searches to successfully answer. HotpotQA asks questions meant to be answered with an accompanying wikipedia dump (we consider the subset used in (Shinn et al., 2024)).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Standard</th>
<th style="text-align: left;">'don't overthink'</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">EM</td>
<td style="text-align: left;">0.651</td>
<td style="text-align: left;">0.651</td>
</tr>
<tr>
<td style="text-align: left;">GPT4</td>
<td style="text-align: left;">0.627</td>
<td style="text-align: left;">0.725</td>
</tr>
<tr>
<td style="text-align: left;">CMD-R</td>
<td style="text-align: left;">0.734</td>
<td style="text-align: left;">0.687</td>
</tr>
<tr>
<td style="text-align: left;">Haiku</td>
<td style="text-align: left;">0.749</td>
<td style="text-align: left;">0.757</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: left;">0.729</td>
<td style="text-align: left;">0.509</td>
</tr>
<tr>
<td style="text-align: left;">PoLL</td>
<td style="text-align: left;">$\mathbf{0 . 7 6 3}$</td>
<td style="text-align: left;">0.699</td>
</tr>
</tbody>
</table>
<p>Table 4: Kappa values for KILT NQ for the standard prompt and the prompt optimized for GPT-4 (note that the EM baseline is not LLM-dependent and therefore not dependent on prompt)</p>
<p>Generating answers for judgement - React Agents: All models follow the basic REACT setup for tool-use agents which incorporate a Thought-Action-Observation loop to iteratively call tools and reason over their outputs to find a final answer (Yao et al., 2022). We use LangChain tool-use implementations for each model when available. On Bamboogle, agents have access to an internet search tool and on HotPotQA, agents have access to a search tool over wikipedia that does dense embedding search using Cohere EmbedV3 + Rerank 3. All search tool calls return the top two most relevant documents to the model.</p>
<p>Judge Prompt The prompt used for judging multihop answers can be found in table 10</p>
<h2>A. 2 Additional Results</h2>
<h2>A.2.1 Multi-hop QA</h2>
<p>In Tables 5 and 6 we can see $\kappa$ correlations on Bamboogle and HotPotQA multi-hop question answering.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">EM</th>
<th style="text-align: left;">GPT-3.5</th>
<th style="text-align: left;">Cmd-R</th>
<th style="text-align: left;">Haiku</th>
<th style="text-align: left;">PoLL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Haiku</td>
<td style="text-align: left;">0.746</td>
<td style="text-align: left;">0.772</td>
<td style="text-align: left;">0.741</td>
<td style="text-align: left;">0.802</td>
<td style="text-align: left;">$\mathbf{0 . 8 0 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Sonnet</td>
<td style="text-align: left;">0.767</td>
<td style="text-align: left;">0.879</td>
<td style="text-align: left;">0.880</td>
<td style="text-align: left;">0.877</td>
<td style="text-align: left;">$\mathbf{0 . 8 9 6}$</td>
</tr>
<tr>
<td style="text-align: left;">CMD-R</td>
<td style="text-align: left;">0.599</td>
<td style="text-align: left;">0.881</td>
<td style="text-align: left;">0.896</td>
<td style="text-align: left;">0.897</td>
<td style="text-align: left;">$\mathbf{0 . 9 3 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-LG</td>
<td style="text-align: left;">0.841</td>
<td style="text-align: left;">0.760</td>
<td style="text-align: left;">$\mathbf{0 . 9 3 6}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 3 6}$</td>
<td style="text-align: left;">0.872</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: left;">0.849</td>
<td style="text-align: left;">0.886</td>
<td style="text-align: left;">$\mathbf{0 . 9 3 5}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 3 5}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 3 5}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">0.677</td>
<td style="text-align: left;">0.862</td>
<td style="text-align: left;">0.877</td>
<td style="text-align: left;">0.877</td>
<td style="text-align: left;">$\mathbf{0 . 9 0 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: left;">0.762</td>
<td style="text-align: left;">0.846</td>
<td style="text-align: left;">0.885</td>
<td style="text-align: left;">$\mathbf{0 . 8 9 8}$</td>
<td style="text-align: left;">0.896</td>
</tr>
</tbody>
</table>
<p>Table 5: Kappa values for various chatbot models on Bamboogle</p>
<h2>A.2.2 Arena Hard Scores</h2>
<p>Raw scores for Arena-hard coming from PoLL are shown in Table 7.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">EM</th>
<th style="text-align: center;">GPT-3.5</th>
<th style="text-align: center;">Cmd-R</th>
<th style="text-align: center;">Haiku</th>
<th style="text-align: center;">PoLL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Haiku</td>
<td style="text-align: center;">0.703</td>
<td style="text-align: center;">$\mathbf{0 . 9 1 0}$</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.889</td>
</tr>
<tr>
<td style="text-align: left;">Sonnet</td>
<td style="text-align: center;">0.715</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">$\mathbf{0 . 8 4 7}$</td>
<td style="text-align: center;">0.844</td>
<td style="text-align: center;">0.820</td>
</tr>
<tr>
<td style="text-align: left;">CMD-R</td>
<td style="text-align: center;">0.780</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">$\mathbf{0 . 9 0 9}$</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.859</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-LG</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">$\mathbf{0 . 8 9 5}$</td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">0.853</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">0.799</td>
<td style="text-align: center;">0.757</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 9}$</td>
<td style="text-align: center;">0.858</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">$\mathbf{0 . 8 9 0}$</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">$\mathbf{0 . 8 9 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">0.773</td>
<td style="text-align: center;">0.827</td>
<td style="text-align: center;">$\mathbf{0 . 8 8 6}$</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.871</td>
</tr>
</tbody>
</table>
<p>Table 6: Kappa values for various chatbot models on multihop HotPotQA</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Score</th>
<th style="text-align: left;">$\mathbf{9 5 \%}$ CI</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4-turbo</td>
<td style="text-align: left;">68.7</td>
<td style="text-align: left;">$(-2.1,2.2)$</td>
</tr>
<tr>
<td style="text-align: left;">Sonnet</td>
<td style="text-align: left;">57.6</td>
<td style="text-align: left;">$(-3.2,2.2)$</td>
</tr>
<tr>
<td style="text-align: left;">CMD-R+</td>
<td style="text-align: left;">57.1</td>
<td style="text-align: left;">$(-2.9,2.5)$</td>
</tr>
<tr>
<td style="text-align: left;">Haiku</td>
<td style="text-align: left;">55.9</td>
<td style="text-align: left;">$(-2.1,2.2)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4-0314</td>
<td style="text-align: left;">50.0</td>
<td style="text-align: left;">$(0.0,0.0)$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-MD</td>
<td style="text-align: left;">49.0</td>
<td style="text-align: left;">$(-2.3,2.7)$</td>
</tr>
<tr>
<td style="text-align: left;">CMD-R</td>
<td style="text-align: left;">48.0</td>
<td style="text-align: left;">$(-1.9,2.7)$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-LG</td>
<td style="text-align: left;">43.7</td>
<td style="text-align: left;">$(-1.9,2.2)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: left;">41.2</td>
<td style="text-align: left;">$(-2.1,2.5)$</td>
</tr>
</tbody>
</table>
<p>Table 7: Scores from Arena Hard as scored by the PoLL using average pooling.</p>
<h1>A. 3 Human Annotations</h1>
<p>Annotations were performed by profession annotators with diverse demographic and professional backgrounds including novelists, copywriters, copy editors, and journalists. Single-hop TriviaQA, Bamboogle, and multi-hop HotPotQA were singly annotated while Natural Questions and single-hop HotPotQA are triple annotated. Experiments on triple annotated datasets were based on a pooled super-annotator decision based on majority voting. The total number of annotations is shown in Table 8 .</p>
<p>Table 8: Total Number of Human Annotations by Dataset</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Annotated Examples</th>
<th style="text-align: center;">Annotations per Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Natural Questions</td>
<td style="text-align: center;">688</td>
<td style="text-align: center;">Triple</td>
</tr>
<tr>
<td style="text-align: left;">TriviaQA</td>
<td style="text-align: center;">316</td>
<td style="text-align: center;">Single</td>
</tr>
<tr>
<td style="text-align: left;">HotPotQA (single)</td>
<td style="text-align: center;">784</td>
<td style="text-align: center;">Triple</td>
</tr>
<tr>
<td style="text-align: left;">HotPotQA (multi)</td>
<td style="text-align: center;">595</td>
<td style="text-align: center;">Single</td>
</tr>
<tr>
<td style="text-align: left;">Bamboogle</td>
<td style="text-align: center;">750</td>
<td style="text-align: center;">Single</td>
</tr>
</tbody>
</table>
<p>Write a high-quality answer for the given question using only the provided search results (some of which might be irrelevant).</p>
<p>Document <a href="Title: Lake Eyre basin">1</a> keeping pace with evaporation. In contrast, the flow of the Mississippi could fill Lake Eyre in 22 days, that of the Amazon in just 3 days. Other lakes in the basin include Lake Frome, Lake Yamma Yamma and Lake Hart. Geography Rivers. The Cooper Creek, Finke River, Georgina River and Diamantina River are the four main rivers of the basin. Other desert rivers include the Hale River, Plenty River and Todd River that flow from the south east of the Northern Territory, south. In the
Document <a href="Title: Lake Eyre basin">2</a> make it as far south as Lake Eyre, although the story is told that this happened once early in the 20th century. In extreme events, water from the Finke River flows into the Macumba River, which empties into Lake Eyre, a total distance from headwater streams of around . Major tributaries include Ellery Creek, and the Palmer and Hugh Rivers. The Georgina River system originates on the Barkly Tableland, near the Northern Territory-Queensland border, north-west of Mount Isa and not far south of the Gulf of</p>
<p>Document <a href="Title: Lake Eyre">10</a> from the north-east part of the Lake Eyre Basin|in outback (south-west and central) Queensland|flow towards the lake through the Channel Country. The amount of water from the monsoon determines whether water will reach the lake and, if it does, how deep the lake will get. The average rainfall in the area of the lake is per year. The altitude usually attributed to Kati Thanda(Lake Eyre refers to the deepest parts of the lake floor, in Belt Bay and the Madigan Gulf</p>
<p>Question: where does the water come from to fill lake eyre
Answer:
Table 9: Example of a single-hop question answering prompt from Liu et al. (2024) used for KILT answer generations where LLM API does not have a documents parameter.</p>
<p>You will be given a Question and a Provided Answer. Judge whether the Provided Answer is correct by comparing it to the Reference Answer. Differently formatted dates, people with missing middle names, and alternative spellings should all be considered the same. If the Provided Answer is correct say exactly "True", otherwise say "False".</p>
<p>Question 1: "When did the president who set the precedent of a two term limit leave office?"
Provided Answer: "George Washington set the precedent of a two-term limit when he decided not to seek a third term in 1796. He left office in 4 March, 1797."</p>
<p>Reference Answer: "March 4, 1797"
Correct: True</p>
<p>Question 2: "Where does Śivarāma Swami conduct courses on Vaishnava Theology?"
Provided Answer: "Śivarāma Swami conducts courses on Vaishnava Theology at Bhaktivedanta Manor."
Reference Answer: "Where does Śivarāma Swami conduct courses on Vaishnava Theology?"
Correct: False</p>
<p>Question 3: "The most populous city in Punjab is how large (area wise)?"
Provided Answer: "Ludhiana, the most populous city in Punjab, covers an area of 3,767 square kilometres according to the website of Punjab's Department of Agriculture \&amp; Farmer Welfare. Another source states it is 318 square kilometres, making it the most densely populated urban centre in the state." Reference Answer: "318 square kilometers"
Correct: True</p>
<p>Question 4: "Who was mayor of New York City when Fiorello H. La Guardia was born?" Provided Answer: "Frank Edson was mayor of New York City when Fiorello H. La Guardia was born on December 11, 1882. Edson served as mayor from 1881 to 1882, and La Guardia himself later became the 99th Mayor of New York City, serving from 1934 to 1946 or 1945."
Reference Answer: "William Russell Grace"
Correct: False</p>
<p>Question 5: "What is the third letter of the top level domain of the military?"
Provided Answer: "The third letter of the top-level domain for the military, '.mil', is 'l'.
The domain name 'mil' is derived from the word 'military'. It is the only top-level domain for a country's military, a legacy of the US military's role in the creation of the internet."
Reference Answer: "l"
Correct: True</p>
<p>Question 6: "Ed Doenbould plays Tyler in a film directed by which famous director?"
Provided Answer: "Ed Doenbould played Tyler in «The Visit»"
Reference Answer: "M. Night Shyamalan"
Correct: False</p>
<p>Question 7: "Alice David is the voice of Lara Croft in a video game developed by which company ?"
Provided Answer: "Alice David is the voice of Lara Croft in the French dub of the video game «Tomb Raider»."
Reference Answer: "Crystal Dynamics"
Correct: False</p>
<p>Question 8: "[QUESTION]"
Provided Answer: "[SENLAHOMER]"
Reference Answer: "[GOLD_ANOMER]"
Correct:</p>
<p>Table 10: Multihop Judge Prompt</p>
<p>You will be given a Question and a Provided Answer. Judge whether the Provided Answer is correct by comparing it to the Reference Answer. Differently formatted dates, people with missing middle names, and alternative spellings should all be considered the same. If the Provided Answer is correct say exactly "True", otherwise say "False".</p>
<p>Question 1: "when did the vfl start playing in London?"
Provided Answer: "According to the provided search results, the WFL started playing regular season games in London as part of the WFL International Series in 2007. Specifically:
Document [5] states: "The WFL International Series was inaugurated in 2007 to host WFL regular season games outside the United States. Played at the new Wembley Stadium in London [rebuilt and reopened in 2007], the series increased from one to two games for the 2011 season, to three games for the 2014 season, and then to four games from the 2017 season."</p>
<p>Document [9] also mentions: "Since 2007, the league has held multiple regular season games in London each season as part of WFL London Games, allowing the league to test solutions to some of the challenges facing a hypothetical London franchise."</p>
<p>In 2007 marked the start of the WFL playing regular season games at the newly rebuilt Wembley Stadium in London as part of the International Series. Prior to that, there were some preseason games played in London, but 2007 was the first year for regular season WFL games in the city.
Reference Answer: "Since 2007"
Correct: True</p>
<p>Question 2: "where is wind power used most in the United States"
Provided Answer: "Iowa is the state that generates the highest percentage of energy from wind power, with more than 35% of its electric power coming from wind. Alternatively, North Dakota has the highest per capita wind generation.</p>
<p>The state with the most installed wind power capacity is Texas, followed by Iowa. Texas's capacity is around 28,000 MW, almost double that of Iowa's 3,670 MW. The Roscoe Wind Farm in Texas is also the world's largest wind farm at 788 MW.</p>
<p>California has doubled its wind power capacity since 2002 and now gets about 5% of its energy from wind power. This is still lower than the twelve U.S. states that get more than 20% of their energy from wind."
Reference Answer: "Texas"
Correct: True</p>
<p>Question 3: "who is the owner of Phoenix Mall in Chennai"
Provided Answer: "The owner of Phoenix Market city in Chennai is Phoenix Mills Limited, as mentioned in the document titled "Phoenix Market city (Chennai)." This mall was jointly developed by Phoenix Mills Limited and lived herewith LSE."
Reference Answer: "Phoenix Mills Limited"
Correct: True</p>
<p>Question 4: "number of degrees of freedom for plane mechanism"
Provided Answer: "In mechanics, the degree of freedom (DEF) of a mechanical system is the number of independent parameters that define its configuration. The number of degrees of freedom for a mechanism on a two-dimensional plane is three, which consists of two translations and one rotation.</p>
<p>For a system of multiple bodies, the combined DEF is the sum of the DEFs of the individual bodies, minus the internal constraints on their relative motion. Therefore, a mechanism formed of multiple right bodies may have more than three DEFs."
Reference Answer: "Iowa"
Correct: False</p>
<p>Question 5: "who had the best strike rate among batsmen who scored over 200 runs in the world Cup"
Provided Answer: "The provided documents do not specify the strike rate of all batsmen who scored over 200 runs in the World Cup. However, among the batsmen mentioned who have scored over 200 runs in the World Cup, Ishun Wubarak has the highest over strike rate in a 720 World Cup match for Sri Lanka, with a strike rate of 353.84 (Document [6]). But it's important to note that this strike rate is not necessarily reflective of his overall performance in the World Cup, as it could have been achieved in a single match.</p>
<p>For a more accurate comparison of strike rates among batsmen who scored over 200 runs in the World Cup, a comprehensive list of all such batsmen and their respective strike rates would be required."
Reference Answer: "Rakti Sharma"
Correct: False</p>
<p>Question 6: "who played two stone six on Law and Order"
Provided Answer: "According to the provided search results, Ben Stone's son Peter Stone was played by Philip Winchester. Some key details:
- Peter Stone is the son of Benjamin "Ben" Stone, the former Executive Assistant District Attorney portrayed by Michael Moriarty on Law \&amp; Order.
- Peter first appeared as a character on the Chicago P.O. episode "Justice", which served as a backdoor pilot for the short-lived spin-off Chicago Justice, where he was the lead character played by Philip Winchester.
- Father George Justice was canceled, Philip Winchester reprised his role as Peter Stone in Season 14 of Law \&amp; Order. Special fictions tell, where he became the Assistant District Attorney for the Sex Crimes Bureau.
- In several US$ episodes, Peter describes having a strained relationship with his workaholic father Ben Stone, who neglected his family in favor of his prosecutorial duties.</p>
<p>So in summary, Philip Winchester played the role of Peter Stone, the son of the iconic Law \&amp; Order character Ben Stone, across the shows Chicago Justice and Law \&amp; Order: S4U."
Reference Answer: "Michael Moriarty"
Correct: False</p>
<p>Question 7: "who won on you think you can dance season 13"
Provided Answer: "I'm sorry, I couldn't find any information about the winner of season 13 of So You Think You Can Dance." Reference Answer: "Leon ' ' Kida '" Burss"
Correct: False</p>
<p>Question 8: "when did sweet caroline start at red sun games"
Provided Answer: "The song "Sweet Caroline" started being played at Fenway Park, home of the Boston Red Sox, since at least 1997. However, it has been played consistently in the middle of the eighth season of very game since 2002."
Reference Answer: "at least 1997"
Correct: True</p>
<p>Question 9: "when did end of the road come out"
Provided Answer: "I found multiple results for "End of the Road".
MP End of the Road (1944 film)
End of the Road is a 1948 American crime film directed by George Blair and written by Denison Clift, Gertrude Walker and Albert Reich. The film stars Edward Morris, John Abbott, Jane Storey, Jonathan Huls, Pierre Naskin and Ted Hecht.
MP End of the Road (1978 film)
End of the Road is a 1978 American comedy-drama film directed, co-written, and edited by Aram Avakian and adapted from a 1958 novel by John Barth. The film stars Stacy Keach, James Earl Jones and Harris Nalis.
MP End of the Road (Boys II Men song)
End of the Road is a single recorded by American R&amp;B group Boys II Men for the "Boomerang" soundtrack. It was released on June 28, 1982.</p>
<p>MP End of the Road (Jerry Lee Lewis song)
End of the Road is a song by Jerry Lee Lewis. It appeared on the 1976 Sun Records compilation album "The Original Jerry Lee Lewis" which was released by the Sun Record Company.
Which of these were you referring to?"
Reference Answer: "June 28, 1982"
Correct: True</p>
<p>Question 10: "who played John Connor in the original terminator"
Provided Answer: "John Connor is not physically portrayed by any actor in the original "Terminator" film released in 1984, as he is only mentioned and not seen. The character of John Connor is first described in John Film as the future leader of the human resistance against the machines, but he does not make an appearance."
Reference Answer: "Edward Furlong throughout the remainder of the film"
Correct: False</p>
<p>Question 11: "[QUENTUM]"
Provided Answer: "[SOLVAMORE]"
Reference Answer: "[SOLVAMORE]"
Correct:</p>
<p>Table 11: KILT NaturalQuestions judge prompt.</p>
<p>The still be given a Question and a Provided Answer. Judge whether the Provided Answer is correct by comparing it to the Reference Answer. Differently formatted dates, people with missing middle names, and alternative spellings should all be considered the same. If the Provided Answer is correct say exactly “True”, otherwise say “False”.</p>
<p>Question 1: “Who was also an actor, Serri or John Fagerty?”</p>
<p>Provided Answer: “Based on the provided search results, there is no information indicating that John Fagerty has pursued acting as a profession or has been involved in acting projects. The search results focus on his music career, including his time-and-dreamers (Alexander Marson, his solo career, and certain performance and recognitions he has received over the years. Therefore, without information on Serri or the provided documents, it cannot be mentioned that Serri was an actor. However, it is clear that the search results that John Fagerty is presently known as a musician, singer, and songwriter, are as an actor.”</p>
<p>Reference Answer: “Serri”</p>
<p>Correct: False</p>
<p>Question 2: “The 25th Disney animated feature film Tarzan Features You’ll be in my heart by which artist?” Provided Answer: “The 25th Disney animated feature film Tarzan’s soundtrack, Tarzan: An Original Multi-Disney Records, between the song You’ll Be in My Heart written and performed by Phil Collins. The song was the Academy music for West Originals song and the Garden Zodiac based for West Originals’ song. Collins was originally hired as a songwriter but ended up performing the song as well.”</p>
<p>Reference Answer: “Phil Collins”</p>
<p>Correct: True</p>
<p>Question 3: “Who is the current drummer of the band who did the song “What covers Do”?”</p>
<p>Provided Answer: “Unfortunately, some of the provided search results directly mention the current drummer for the band Marson 5 who performed the song “What covers Do”. The results provide background information on the song itself. Its music video, chart performance, and other details, but do not specify the band members or drummer.</p>
<p>Marson 5 is a well-known pop rock band trained by what course, but their full current lineup and specific drummer for the “What covers Do” era is not related to these search results. To definitively answer who the drummer was, additional information directly about Marson 5s band members around 2017 when “What covers Do” was released would be needed.” Reference Answer: “Matt Flynn”</p>
<p>Correct: False</p>
<p>Question 4: “What is video, Tenth Tagged or Mahend Bhupath?”</p>
<p>Provided Answer: “Unfortunately, some of the provided search results contain any information about Tenth Tagged or their age to compare with Mahend Bhupath’s age. The search results are all focused on providing biographical details about Mahend Bhupath, an Indian professional, Wests player, and some other individuals named Mahend Bhuti who appear to be unrelated to the question. Without any information about Tenth Tagged, it is impossible to determine who is video between them and Mahend Bhupath’s based solely on these search results.”</p>
<p>Reference Answer: “Tenth Tagged”</p>
<p>Correct: False</p>
<p>Question 5: “What is the birthday of the actress who was the Sushava in “The Revengers Tragedy”?”</p>
<p>Provided Answer: “Gizmo Quich, who played the Sushava in “The Revengers Tragedy”, was born on 22nd September 1924.”</p>
<p>Reference Answer: “22 November 1940”</p>
<p>Correct: False</p>
<p>Question 6: “Welocalize in kind of grain water invented and promoted by a “belivisio health” practitioner born in which year ?”</p>
<p>Provided Answer: “Biloxetin is a kind of grain water invented and promoted by Ann Rigborn, who was born in 1988.”</p>
<p>Reference Answer: “1988”</p>
<p>Correct: True</p>
<p>Question 7: “Who is the current drummer of the band who did the song “What covers Do”?”</p>
<p>Provided Answer: “The song “Haul Be in My Heart” featured in the 1999 Disney animated film Tarzan was written and performed by Phil Collins.</p>
<p>The boy details that compare this answer from the provided search results:</p>
<ol>
<li>
<p>Document [2] states: “Haul Be in My Heart” is a song by Phil Collins, from the 1999 Disney animated feature “Tarzan”.”</p>
</li>
<li>
<p>Document [3] mentions: “For his contribution to the soundtrack, Collins received an American Music band for favorite Adult Contemporary Artist.”</p>
</li>
<li>
<p>Document [5] says: “Collins was originally hired by Disney as a songwriter, but was chosen because he was a drummer at first and given his history as a drummer for the rock band Senecio, he appealed to the Disney production area, who “asked a strong jungle what to accompany Tarzam adventures”. This “hallud” is one of five original songs he wrote for the film, and he ended up also performing this song as well.”</p>
</li>
<li>
<p>Document [7] states: “The song is about “How love is a bond that cannot be broken”.”</p>
</li>
<li>
<p>In summary, the artist who wrote and performed the song “Haul Be in My Heart” featured in the Disney animated film Tarzan is Phil Collins.”</p>
</li>
</ol>
<p>Reference Answer: “Phil Collins”</p>
<p>Correct: True</p>
<p>Question 8: “How long is the time for adult friendliness track to a pribeless?”</p>
<p>Provided Answer: “The length of the time for adult friendliness track is a primary career he directly determined from the provided documents as they only provide information about the friendliness track itself and not the time for the child’s friendliness track. The first time he was a member of the first 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000</p>
<p>You will be given a Question and a Provided Answer. Judge whether the Provided Answer is correct by comparing it to the Reference Answer. Differently formatted dates, people with missing middle names, and alternative spellings should all be considered the same. If the Provided Answer is correct say exactly "True", otherwise say "False".</p>
<p>Question 1: "If you landed at 'Santa Catarina Airport on which Island would you be in?"
Provided Answer: "If you landed at Santa Catarina Airport, you would be on the island of Santa Catarina, in the Brazilian state of the same name. The island is approximately 54 km long and 18 km wide and is home to the state capital, Florianópolis. It is the largest island in an archipelago of over 30 islands."
Reference Answer: "Geography of the Madeira Islands"
Correct: False</p>
<p>Question 2: "When it was a first performed at the Abbey Theatre in Dublin in 1947, which play caused risting in the streets because of objections to the portrayal of rural Irish Catholic ownership?"
Provided Answer: "Based on the provided search results, the play that caused risting in the streets when it was first performed at the Abbey Theatre in Dublin in 1947 due to objections over the portrayal of rural Irish Catholic ownership was "The Playboy of the Western World" by J.M. Syngs.</p>
<p>Several documents mention the riots that erupted during and after the opening performance of Synged "The Playboy of the Western World" at the Abbey Theatre in January 1947. The riots were instigated by Irish nationalists and republicans who viewed the plays content as offensive to public morals and an insult to Ireland (Documents 1, 3, 7).</p>
<p>Document 5 specifically states that "the first performance of J.M. Synged play The Playboy of the Western World at the Abbey Theatre in Dublin triggers a week of risting" in 1947. Document 6 also notes that a performance of the play on January 28, 1947 "was interrupted by the audience who continue to boo, hiss and shout."</p>
<p>The objections seem to have stemmed from the plays depiction of patriotic and perceived ownership in rural Irish Catholic society, which nationalists found offensive (Document 7). So "The Playboy of the Western World" by J.M. Syngs was the controversial play that provoked riots when first staged at the Abbey Theatre in 1947 due to its portrayal of the Irish countryside."
Reference Answer: " "THE PLAYBOY OF THE MESTERM WORLD"
Correct: True</p>
<p>Question 3: "The religious order of Poor Ladies of San Damiano has what more common name?" Provided Answer: "The religious order of Poor Ladies of San Damiano is more commonly known as the Poor Clares. This order was founded by Saints Clare of Assisi and Francis of Assisi in 1212, and it was the second Franciscan Order to be established. The Poor Clares were initially referred to as the Order of Poor Ladies, and later the Clarisses, the Minoresses, the Franciscan Clarist Order, and the Second Order of Saint Francis. However, the name Poor Clares is the most recognised and widely used today."
Reference Answer: "Franciscan Clarist Order"
Correct: False</p>
<p>Question 4: "Which country defeated the Ivory Coast to win Football's Africa Cup of Nations?"
Provided Answer: "Based on the provided search results, Ghana defeated the Ivory Coast in the final of the 1992 Africa Cup of Nations. The key evidence is from Document 1 and Document 18.</p>
<p>Document 1 states: "Ivory Coast set a record for the competition by holding each of their opponents scoreless in the six matches of the final tournament after his contributions helped Ghana reach the final, he was, however, suspended for that match and Ghana lost to Ivory Coast in a penalty shootout that saw each side make 11 attempts to determine the winner."</p>
<p>Document 18 directly mentions: "The 1992 Africa Cup of Nations final was a football match that took place on January 26, 1992, at the Stade de Sleptik in Dakar, Senegal, to determine the winner of the 1992 African Cup of Nations. Ivory Coast defeated Ghana 11[10 on penalties after a goalless draw to win their first African Cup."</p>
<p>So the Ivory Coast defeated Ghana in the 1992 Africa Cup of Nations final after a penalty shootout. None of the other provided documents contradict this or indicate another country defeated the Ivory Coast to win that specific tournament."
Reference Answer: "health care in zambia"
Correct: False</p>
<p>Question 5: "Fred Scuttle was one of the best-known characters of which British comedian?"
Provided Answer: "The provided documents do not directly mention Fred Scuttle or the British comedian associated with this character. Therefore, based on the information available in the documents, it is not possible to accurately answer the question about which British comedian is best known for the character Fred Scuttle."
Reference Answer: "Alfred Hawthorne Hill"
Correct: False</p>
<p>Question 6: "What element is the main constituent of anthracite?"
Provided Answer: "The main constituent of anthracite is carbon. High-grade anthracite, for example, has a chemical formula represented by CH3NO3, indicating it contains carbon, hydrogen, oxygen, nitrogen, and sulfur, with carbon making up $9 \%$ of its composition. Ultra high-grade (UHL) anthracite typically has a minimum carbon content of $55 \%$, further emphasizing carbon as the primary element in anthracite."
Reference Answer: "Carbon"
Correct: True</p>
<p>Question 7: "Which American played in his 18th Ryder Cup in 2014?"
Provided Answer: "Michelson extended his own U.S. team record with his 18th appearance at the 2014 Ryder Cup."
Reference Answer: "philip.afford micheliam"
Correct: True</p>
<p>Question 8: "Who is the lead singer of the Sheffield hand the Arctic Monkeys?"
Provided Answer: "The lead singer of the Sheffield hand Arctic Monkeys is Alex Turner. Turner also plays guitar and is the frontman of the four-piece indie rock band. Arctic Monkeys have released six studio albums, including a debut in 2002 titled "Whatever People Say I Am, That's What I'm Not"."
Reference Answer: "Alex Turner"
Correct: True</p>
<p>Question 9: "[QUOTION]"
Provided Answer: "[GOLANDARA]"
Reference Answer: "[GOLD-ARDAËR]"
Correct:</p>
<p>Table 13: KILT TriviaQA Judge Prompt</p>
<h1>Download File PDF Hp</h1>
<p>STATION: You are judging whether a model has generated a correct answer to a question. Study the examples the user gives you as they will be very informative for how to do the task. The Reference
Answers you get will be short. An model's answer will be longer, and can be considered correct if it contains the semantic content of short reference answer somewhere within it. Don't worry about
reclustity with respect to the real world, just judge the example based on what you see. We need to overthrow this task, it really comes down to just soft matching. Answer with only the word "his" or
"He."</p>
<p>USEP: Question: when did the vfl start playing in London Model Answer: According to the provided search results, the NFL started playing regular season games in London as part of the NFL International Series in 2007. Specifically:</p>
<p>Document [5] status: "The NFL International Series was inaugurated in 2007 to host NFL regular season games outside the United States. Played at the new Wembley Stadium in London (rebuilt and reopened in 2007), the series increased from one to two games for the 2013 season, to three games for the 2014 season, and then to four games from the 2017 season."</p>
<p>Document [5] also mentions: "Since 2007, the league has held multiple regular season games in London each season as part of NFL London Games, allowing the league to test solutions to some of the challenges facing a hypothetical London franchise."</p>
<p>So 2007 marked the start of the NFL playing regular season games at the newly rebuilt Wembley Stadium in London as part of the International Series. Prior to that, there were some preseason games played in London, but 2007 was the first year for regular season NFL games in the city.
Reference Answer: since 2007
Correct: Yes
Question: where is wind power used most in the united states
Model Answer: Iowa is the state that generates the highest percentage of energy from wind power, with more than $35 \%$ of its electric power coming from wind. Alternatively, North Dakota has the highest per capita wind generation.</p>
<p>The state with the most installed wind power capacity is Texas, followed by Iowa. Texas's capacity is around 20,000 MW, almost double that of Iowa's 2,670 MW. The Roscoe Wind farm in Texas is also the world's largest wind farm at 760 MW .</p>
<p>California has doubled its wind power capacity since 2002 and now gets about $5 \%$ of its energy from wind power. This is still lower than the twelve U.S. states that get more than $20 \%$ of their energy from wind.
Reference Answer: Texas
Correct: Yes
Question: who is the owner of phoenix mall in chewual
Reference Answer: The owner of Phoenix Marketity in Chewual is Phoenix Mills Limited, as mentioned in the document titled "Phoenix Marketity (Chewual)." This mall was jointly developed by Phoenix Mills Limited and Ciret Ventures Ltd.
Reference Answer: Phoenix Mills Limited
Correct: Yes
Question: number of degree of freedom for place mechanism
Model Answer: In mechanics, the degree of freedom (DOF) of a mechanical system is the number of independent parameters that define its configuration. The number of degrees of freedom for a mechanism on a non-dimensional plane is three, which consists of two translations and one rotation.
For a system of multiple bodies, the combined DOF is the sum of the DOFs of the individual bodies, minus the internal constraints on their relative motion. Therefore, a mechanism formed of multiple rigid bodies may have more than three DOFs.
Reference Answer: Yes
Correct: No
Question: who had the best strike rate among batsmen who scored over 200 runs in the world cup
Model Answer: The provided documents do not specify the strike rate of all batsmen who scored over 200 runs in the World Cup. However, among the batsmen mentioned who have scored over 200 runs in the world Cup, Johan Miskards has the highest over strike rate in a 120 World Cup match for Sri Lanka, with a strike rate of 353.64 (Document [5]). But it's important to note that this strike rate is not necessarily reflective of his overall performance in the World Cup, as it could have been achieved in a single match.</p>
<p>For a more accurate comparison of strike rates among batsmen who scored over 200 runs in the World Cup, a comprehensive list of all such batsmen and their respective strike rates would be required. Reference Answer: Robot Shared
Correct: No
Question: who played bon stone son on law and order
Model Answer: According to the provided search results, Bon Stone's son Peter Stone was played by Philip Winchester. Some key details.</p>
<ul>
<li>Peter Stone is the son of Benjamin "Ben" Stone, the former Executive Assistant District attorney portrayed by Michael Moriarty on Law \&amp; Order.</li>
<li>Peter first appeared as a character on the Chicago P.S. episode "Justice", which served as a backdoor pilot for the short-lived spin-off Chicago Justice, where he was the lead character played by Philip Winchester.</li>
<li>After Chicago Justice was canceled, Philip Winchester reprised his role as Peter Stone in Season 19 of Law \&amp; Order: Special Victims Unit, where he became the Assistant District Attorney for the Sex Crimes Bureau.</li>
<li>In several 555 episodes, Peter describes having a strained relationship with his workaholic father Ben Stone, who neglected his family in favor of his prosecutorial duties.</li>
</ul>
<p>So in summary, Philip Winchester played the role of Peter Stone, the son of the iconic Law \&amp; Order character Ben Stone, across the shows Chicago Justice and Law \&amp; Order: SNV.
Reference Answer: Michael Moriarty
Correct: No
Question: who won no you think you can dance season 12
Model Answer: I'm sorry, I couldn't find any information about the winner of season 13 of So You Think You Can Dance.
Reference Answer: Leon "Kids "Barry
Correct: No
Question: when did sweet caroline start at red sea games
Model Answer: The song "Sweet Caroline" started being played at Foxway Park, home of the Boston Red Sox, since at least 1957. However, it has been played consistently in the middle of the eighth season on every game since 2002.
Reference Answer: at least 1957
Correct: Yes
Question: when did end of the road come out
Model Answer: I found multiple results for "End of the Road":
AP End of the Road (1944 file)
End of the Road is a 1944 American crime file directed by George Blair and written by Denison Clift, Gertrude Walker and Albert Reich. The file stars Edward Norris, John Abbott, Jane Storey, Jonathan Hale, Pierre Naskin and Ted Hecht.</p>
<p>AP End of the Road (1978 file)
End of the Road is a 1978 American comedy-drama file directed, co-written, and edited by Aram Krahian and adapted from a 1958 novel by John Barth. The file stars Stacy Keach, James Earl Jones and Harris Falls.</p>
<p>AP End of the Road (Boys II Men song)
End of the Road is a single recorded by American R\&amp;D group Boys II Men for the "Boomerang" soundtrack. It was released on June 28, 1992.</p>
<p>AP End of the Road (Jerry Lee Lewis song)
End of the Road is a song by Jerry Lee Lewis. It appeared on the 1978 Sun Records compilation album "The Original Jerry Lee Lewis" which was released by the Sun Record Company.</p>
<p>Which of these were you referring to?
Reference Answer: June 28, 1992
Correct: Yes
Question: who played John conter in the original terminator
Model Answer: John Conter is not physically motivated by any actor in the original "terminator" file released in 1966, as he is only mentioned and not seen. The character of John Conter is first referred to in this file as the future leader of the human resistance against the machines, but he does not make an appearance. Reference Answer: Edward Furlong throughout the remainder of the file Correct: No
Question: [268325284]
Model Answer: [268LAH0009]
Reference Answer: [268JLAH0009]
Correct:</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{12}$ The pre-chunked dump can be downloaded here: https://github.com/facebookresearch/DPR/issues/ 186#issuecomment-923268677
${ }^{13}$ The prompt template that gets used when using this parameter is described at https://huggingface. co/CohereForAI/c4ai-command-r-v41 with additional details at https://docs.cohere.com/docs/ prompting-command-r&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
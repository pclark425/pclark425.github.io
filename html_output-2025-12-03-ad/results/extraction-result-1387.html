<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1387 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1387</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1387</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-000821454305abb529994dca84dc4aa7cc861892</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/000821454305abb529994dca84dc4aa7cc861892" target="_blank">Prior-Fitted Networks Scale to Larger Datasets When Treated as Weak Learners</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Artificial Intelligence and Statistics</p>
                <p><strong>Paper TL;DR:</strong> The BoostPFN method can outperform standard PFNs with the same size of training samples in large datasets and achieve a significant acceleration in training times compared to other established baselines in the field, including widely-used Gradient Boosting Decision Trees (GBDTs), deep learning methods and AutoML systems.</p>
                <p><strong>Paper Abstract:</strong> Prior-Fitted Networks (PFNs) have recently been proposed to efficiently perform tabular classification tasks. Although they achieve good performance on small datasets, they encounter limitations with larger datasets. These limitations include significant memory consumption and increased computational complexity, primarily due to the impracticality of incorporating all training samples as inputs within these networks. To address these challenges, we investigate the fitting assumption for PFNs and input samples. Building on this understanding, we propose \textit{BoostPFN} designed to enhance the performance of these networks, especially for large-scale datasets. We also theoretically validate the convergence of BoostPFN and our empirical results demonstrate that the BoostPFN method can outperform standard PFNs with the same size of training samples in large datasets and achieve a significant acceleration in training times compared to other established baselines in the field, including widely-used Gradient Boosting Decision Trees (GBDTs), deep learning methods and AutoML systems. High performance is maintained for up to 50x of the pre-training size of PFNs, substantially extending the limit of training samples. Through this work, we address the challenges of efficiently handling large datasets via PFN-based models, paving the way for faster and more effective tabular data classification training and prediction process. Code is available at Github.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1387.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1387.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PFN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prior-Fitted Network (PFN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained in-context learning model that approximates the posterior predictive distribution (PPD) for supervised tasks by taking training samples as part of the input; implemented in practice with a Transformer backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformers can do bayesian inference.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prior-Fitted Network (PFN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A model that approximates the Bayesian posterior predictive distribution p(y | x, D) by a learned conditional q_theta(y | x_test, D_train). In practice PFNs are implemented as Transformers that accept concatenated tokens representing training examples and test examples and output predictive probabilities for classes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural probabilistic predictive model (Transformer-based PPD approximator)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>tabular classification (supervised learning); general supervised predictive tasks where PPD approximation is useful</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Task predictive performance measured by AUC-OVO (AUC one-versus-one) in experiments; fidelity conceptually measured as approximation quality to posterior predictive distribution (no explicit KL or MSE reported)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Empirical PFN (TabPFN variant) mean AUC-OVO on small benchmarks: 0.894 ± 0.010 (reported across 30 small datasets); pretraining size reported as 1,024 samples (limiting performance on larger datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>PFNs are implemented as Transformers and are treated as black-box neural predictors in the paper; no interpretability claims or mappings of latent dimensions to human-understandable quantities are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>none mentioned in this paper (PFNs used as black-box predictors approximating PPD)</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Inference cost for a PFN Transformer is quadratic in input length: O(L^2) where L = |sampled_training_set| + batch_size; TabPFN reported to cost ~12s per million samples in the paper's experimental setting (hardware: RTX 3090, 48 CPU cores).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>PFNs (TabPFN) provide very fast training/prediction on small datasets compared to standard training procedures (negligible training time), but scale poorly (OOM) as input length grows; the quadratic Transformer cost is the bottleneck versus tree-based or standard deep models.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong performance on small tabular datasets (e.g., mean AUC-OVO ≈ 0.894 on listed small datasets). Performance degrades or becomes infeasible (OOM) as training set size exceeds pretraining scale (e.g., 50k+ samples for the pretrained TabPFN used).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>PFNs provide useful calibrated predictive distributions for small-to-moderate dataset sizes and yield competitive AUC metrics there; however, because PFNs require inclusion of training samples in the Transformer input, their utility drops or becomes infeasible on large datasets without sampling/ensemble strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High-quality PPD approximation and very fast 'no-training' inference for small inputs versus poor scalability: quadratic memory/time cost with input length leads to OOM on large datasets; increasing fidelity by feeding more training samples incurs large computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Implemented as a Transformer without positional embeddings; prediction relies on concatenating training and test samples into the model input; pretraining encodes a prior over datasets (pretraining size noted as 1,024 for the TabPFN prior used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to GBDTs and deep learning: PFNs (TabPFN) achieve very fast inference and strong accuracy on small datasets but cannot handle large datasets due to OOM; BoostPFN (this paper) ensembles PFNs to extend applicability to larger datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends using PFNs with sampling/ensemble strategies when dataset sizes exceed pretraining scale; no single optimal PFN configuration given beyond using an appropriately pre-trained prior with sampling size small enough to fit in Transformer inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prior-Fitted Networks Scale to Larger Datasets When Treated as Weak Learners', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1387.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1387.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TabPFN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TabPFN (Transformer-based PFN for tabular data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A PFN variant (Transformer) pretrained to approximate PPD for tabular classification tasks; designed for very fast inference on small-to-medium tabular datasets by supplying training examples as input tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TabPFN: A transformer that solves small tabular classification problems in a second.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TabPFN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A Transformer-based PFN pretrained on a synthetic prior for tabular classification; it takes sampled training examples and test examples concatenated as input tokens (no positional embeddings) and outputs class probabilities approximating the posterior predictive distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based predictive world model / posterior predictive approximator</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>tabular classification (multi-class), particularly small datasets (≤ ~5k samples) but originally pretrained for ~1k-scale problems</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>AUC-OVO (mean AUC one-versus-one) across benchmark datasets; also empirical probability of predicting a single test sample under manipulated input sets used to validate fitting assumption.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported mean AUC-OVO: ~0.894 ± 0.010 on 30 small datasets; behaves well on datasets <5k samples (e.g., TabPFN listed as among top performers for <5000 samples), but runs OOM on larger (50k+) training set sizes with the pretraining used.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>TabPFN is a neural Transformer and is treated as a black box; the paper demonstrates empirically how inclusion/duplication of target samples in the input affects predictive probability (empirical sensitivity), but does not provide internal interpretability diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Empirical input-perturbation experiments (adding/replacing training samples with duplicates of the test sample) to probe sensitivity and validate Assumption 4.1; no latent-space visualizations or attention analyses reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>One inference costs O(L^2) time where L = |sampled training set| + batch size; experimentally TabPFN inference reported ~12s per million samples in the authors' setup (RTX 3090), and TabPFN encounters OOM when sampling sets are large (>=50k with the used prior).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>On small datasets TabPFN is both accurate and very fast compared to models that require explicit training; however, because of quadratic Transformer cost, it is far less efficient on large datasets relative to GBDTs and standard deep models which can scale to large datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong on small tabular benchmarks (mean AUC ≈ 0.894); degrades or becomes infeasible on large datasets due to memory; adding the test set into input can substantially improve performance (upper-bound experiments show AUC gains).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>TabPFN's high-fidelity PPD approximation yields strong predictive performance when the input (sampled training set) aligns with the pretrained prior scale; when dataset sizes exceed pretraining or when prior mismatch occurs, task utility falls—motivating sampling/ensemble solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Accuracy and speed on small problems versus inability to handle long input sequences (quadratic cost) that restricts use on large datasets; duplicating/including test samples in input increases accuracy for that sample but is impractical as a general solution.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Transformer backbone without positional embeddings; uses concatenation of training and test samples as input tokens; pretraining on prior datasets sized ~1k; sampling of train examples (z) to limit input length is required for large problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to GBDTs and AutoML, TabPFN is often faster and competitive on small tasks; compared to deep-learning models on larger datasets TabPFN fails due to OOM while tree ensembles scale and sometimes achieve higher AUC on large datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper indicates TabPFN works well when sampling size and batch size keep the Transformer input under memory limits (e.g., sampled inputs ~500 used in experiments); optimality depends on matching the problem size to the pretrained prior (pretraining size ~1024 suggested as limit for the used TabPFN).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prior-Fitted Networks Scale to Larger Datasets When Treated as Weak Learners', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1387.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1387.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BoostPFN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BoostPFN (Boosted PFN ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble method that treats PFNs as weak learners within a randomized gradient boosting framework, updating sampling weights (input selection) per round to scale PFNs to much larger tabular datasets while retaining efficient inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BoostPFN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An algorithm that ensembles multiple PFN predictions via gradient boosting: at each round it updates sampling weights over training examples (three update rules provided: ExpHadamard, Hadamard, AdaBoost-style), samples a subset D_w^z, uses q_theta(y | x, D_w^z) as a weak learner h_m(x), line-searches step size gamma_m, and accumulates F_m(x)=F_{m-1}(x)+gamma_m h_m(x).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>ensemble of transformer PFN weak learners within Randomized Gradient Boosting Machine (RGBM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>tabular classification across small to large datasets (used to extend PFN applicability to up to 50x the PFN pretraining size)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>AUC-OVO used to evaluate predictive fidelity on classification benchmarks; convergence/boost loss tracked on training and test sets to monitor optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported mean AUC-OVO for BoostPFN: ~0.895 on small datasets, 0.908 on 5,000-sample large datasets, 0.910 on 50,000-sample large datasets, and ~0.913 on >50,000 averaged suites (see tables in paper). BoostPFN extends effective dataset size up to ≈50× the TabPFN pretraining scale while maintaining high AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>BoostPFN is an ensemble of black-box PFNs; interpretability remains limited—no explicit interpretability methods for the ensemble components are presented, though sampling-weight updates provide an interpretable mechanism (which samples are emphasized).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Sampling-weight trajectories and residual-based update rules (ExpHadamard / Hadamard / AdaBoost) are used; the paper reports ablations showing robustness across update rules but does not provide visualization beyond weight/ loss curves.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Time complexity for M rounds: M*(T_i + T_B) where T_i is one PFN inference pass and T_B is linear overhead O(T |D_test|) for boosting operations; experimentally BoostPFN costs about 60s per million samples (authors' hardware) compared to TabPFN ~12s and Bagging ~40s per million samples.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>BoostPFN's time complexity is close to bagging (parallelism-limited scenario) with additional linear boosting overhead; empirically it achieves better predictive results than GBDTs, deep models and AutoGluon under tight time budgets on many medium-large datasets while remaining competitive in time.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On benchmark suites BoostPFN often achieves state-of-the-art or top results for dataset sizes up to 50k training samples (e.g., mean AUC 0.908 at 5k and 0.910 at 50k); when dataset size greatly exceeds pretraining scale and ensemble size saturates, baseline GBDTs or deep models can outperform BoostPFN.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>BoostPFN translates PFN fidelity into scalable task utility by focusing on input subset selection and ensembling; this yields improved AUC on larger datasets compared to single PFN instances, but the approach depends on the quality and scale of the PFN prior—limited pretraining size caps ultimate gains.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off: boosting/ensembling increases computational cost relative to a single PFN inference but gains scalability and improved AUC on larger datasets; diminishing returns observed when pretraining prior is small and when number of weak learners grows (saturation/possible overfitting).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Treat PFN with chosen sampled subset as weak learner; update sampling weights via residual-based rules (ExpHadamard, Hadamard, AdaBoost); non-replacement sampling of training examples; line-search for gamma_m; choose number of weak learners scaled to dataset size (authors use 10 for 5k, 100 for 50k, 1000 for full).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to Bagging (PFN bags) and TabPFN: Bagging gives some improvement but BoostPFN yields superior AUC on larger datasets; compared to GBDTs/Deep models/AutoGluon: BoostPFN is more time-efficient under constrained budgets and competitive in AUC up to 50k samples, but may be outperformed on very large datasets (>50k) if PFN prior is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends treating sampling-weight update method as a hyperparameter per dataset, using non-replacement sampling size (authors used input limited to 500 in many experiments), and scaling number of weak learners with dataset size (10 / 100 / 1000). Theoretical convergence holds for σ-smooth losses and bounded level sets, suggesting more rounds improves approximation (O(σ/M)).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prior-Fitted Networks Scale to Larger Datasets When Treated as Weak Learners', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1387.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1387.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Process (GP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical nonparametric Bayesian model used to obtain posterior predictive distributions; cited here to motivate the PFN fitting assumption because adding observations sharpens the GP posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gaussian processes for machine learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gaussian Process (GP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A nonparametric Bayesian model that defines a prior over functions and yields closed-form posterior predictive distributions conditioned on observed data; adding an observation reduces predictive uncertainty and sharpens the posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>probabilistic nonparametric world model (Bayesian function prior)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general supervised regression/classification and Bayesian inference contexts; referenced as an analogy to PFN behavior</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically measured by posterior predictive MSE, log-likelihood or calibration of predictive distributions (paper does not report explicit GP metrics here).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported numerically in this paper; GP invoked qualitatively to justify that adding target sample sharpens posterior predictive distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>GPs are probabilistically interpretable (posterior mean and variance available analytically), and the paper uses this property to explain why adding target samples should increase predictive probability for that sample.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>analytic posterior predictive mean/variance (classical GP inference) — mentioned qualitatively only in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not expanded in this paper; standard GP inference scales cubically in number of training points, but the paper does not provide those details.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not compared quantitatively in this paper; mentioned only as an intuitive baseline where adding an observation yields sharper posterior predictive behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>GP behavior is used as intuition: adding the target sample to the conditioning set increases posterior predictive confidence for that sample — used to motivate Assumption 4.1 for PFNs.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Only qualitative: GPs provide clear posterior updates but are not discussed in terms of scalability or computational trade-offs here.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Mentioned as conceptual support for the idea that conditioning on the target sample improves prediction; no GP-specific design choices are evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>GPs are used as an intuition/contrast to PFNs; no empirical comparison to PFNs/TabPFN/BoostPFN is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prior-Fitted Networks Scale to Larger Datasets When Treated as Weak Learners', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>TabPFN: A transformer that solves small tabular classification problems in a second. <em>(Rating: 2)</em></li>
                <li>Transformers can do bayesian inference. <em>(Rating: 2)</em></li>
                <li>Statistical foundations of priordata fitted networks. <em>(Rating: 2)</em></li>
                <li>Randomized gradient boosting machine <em>(Rating: 1)</em></li>
                <li>Gaussian processes for machine learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1387",
    "paper_id": "paper-000821454305abb529994dca84dc4aa7cc861892",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "PFN",
            "name_full": "Prior-Fitted Network (PFN)",
            "brief_description": "A pretrained in-context learning model that approximates the posterior predictive distribution (PPD) for supervised tasks by taking training samples as part of the input; implemented in practice with a Transformer backbone.",
            "citation_title": "Transformers can do bayesian inference.",
            "mention_or_use": "use",
            "model_name": "Prior-Fitted Network (PFN)",
            "model_description": "A model that approximates the Bayesian posterior predictive distribution p(y | x, D) by a learned conditional q_theta(y | x_test, D_train). In practice PFNs are implemented as Transformers that accept concatenated tokens representing training examples and test examples and output predictive probabilities for classes.",
            "model_type": "neural probabilistic predictive model (Transformer-based PPD approximator)",
            "task_domain": "tabular classification (supervised learning); general supervised predictive tasks where PPD approximation is useful",
            "fidelity_metric": "Task predictive performance measured by AUC-OVO (AUC one-versus-one) in experiments; fidelity conceptually measured as approximation quality to posterior predictive distribution (no explicit KL or MSE reported)",
            "fidelity_performance": "Empirical PFN (TabPFN variant) mean AUC-OVO on small benchmarks: 0.894 ± 0.010 (reported across 30 small datasets); pretraining size reported as 1,024 samples (limiting performance on larger datasets).",
            "interpretability_assessment": "PFNs are implemented as Transformers and are treated as black-box neural predictors in the paper; no interpretability claims or mappings of latent dimensions to human-understandable quantities are provided.",
            "interpretability_method": "none mentioned in this paper (PFNs used as black-box predictors approximating PPD)",
            "computational_cost": "Inference cost for a PFN Transformer is quadratic in input length: O(L^2) where L = |sampled_training_set| + batch_size; TabPFN reported to cost ~12s per million samples in the paper's experimental setting (hardware: RTX 3090, 48 CPU cores).",
            "efficiency_comparison": "PFNs (TabPFN) provide very fast training/prediction on small datasets compared to standard training procedures (negligible training time), but scale poorly (OOM) as input length grows; the quadratic Transformer cost is the bottleneck versus tree-based or standard deep models.",
            "task_performance": "Strong performance on small tabular datasets (e.g., mean AUC-OVO ≈ 0.894 on listed small datasets). Performance degrades or becomes infeasible (OOM) as training set size exceeds pretraining scale (e.g., 50k+ samples for the pretrained TabPFN used).",
            "task_utility_analysis": "PFNs provide useful calibrated predictive distributions for small-to-moderate dataset sizes and yield competitive AUC metrics there; however, because PFNs require inclusion of training samples in the Transformer input, their utility drops or becomes infeasible on large datasets without sampling/ensemble strategies.",
            "tradeoffs_observed": "High-quality PPD approximation and very fast 'no-training' inference for small inputs versus poor scalability: quadratic memory/time cost with input length leads to OOM on large datasets; increasing fidelity by feeding more training samples incurs large computational cost.",
            "design_choices": "Implemented as a Transformer without positional embeddings; prediction relies on concatenating training and test samples into the model input; pretraining encodes a prior over datasets (pretraining size noted as 1,024 for the TabPFN prior used in experiments).",
            "comparison_to_alternatives": "Compared to GBDTs and deep learning: PFNs (TabPFN) achieve very fast inference and strong accuracy on small datasets but cannot handle large datasets due to OOM; BoostPFN (this paper) ensembles PFNs to extend applicability to larger datasets.",
            "optimal_configuration": "Paper recommends using PFNs with sampling/ensemble strategies when dataset sizes exceed pretraining scale; no single optimal PFN configuration given beyond using an appropriately pre-trained prior with sampling size small enough to fit in Transformer inputs.",
            "uuid": "e1387.0",
            "source_info": {
                "paper_title": "Prior-Fitted Networks Scale to Larger Datasets When Treated as Weak Learners",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "TabPFN",
            "name_full": "TabPFN (Transformer-based PFN for tabular data)",
            "brief_description": "A PFN variant (Transformer) pretrained to approximate PPD for tabular classification tasks; designed for very fast inference on small-to-medium tabular datasets by supplying training examples as input tokens.",
            "citation_title": "TabPFN: A transformer that solves small tabular classification problems in a second.",
            "mention_or_use": "use",
            "model_name": "TabPFN",
            "model_description": "A Transformer-based PFN pretrained on a synthetic prior for tabular classification; it takes sampled training examples and test examples concatenated as input tokens (no positional embeddings) and outputs class probabilities approximating the posterior predictive distribution.",
            "model_type": "transformer-based predictive world model / posterior predictive approximator",
            "task_domain": "tabular classification (multi-class), particularly small datasets (≤ ~5k samples) but originally pretrained for ~1k-scale problems",
            "fidelity_metric": "AUC-OVO (mean AUC one-versus-one) across benchmark datasets; also empirical probability of predicting a single test sample under manipulated input sets used to validate fitting assumption.",
            "fidelity_performance": "Reported mean AUC-OVO: ~0.894 ± 0.010 on 30 small datasets; behaves well on datasets &lt;5k samples (e.g., TabPFN listed as among top performers for &lt;5000 samples), but runs OOM on larger (50k+) training set sizes with the pretraining used.",
            "interpretability_assessment": "TabPFN is a neural Transformer and is treated as a black box; the paper demonstrates empirically how inclusion/duplication of target samples in the input affects predictive probability (empirical sensitivity), but does not provide internal interpretability diagnostics.",
            "interpretability_method": "Empirical input-perturbation experiments (adding/replacing training samples with duplicates of the test sample) to probe sensitivity and validate Assumption 4.1; no latent-space visualizations or attention analyses reported.",
            "computational_cost": "One inference costs O(L^2) time where L = |sampled training set| + batch size; experimentally TabPFN inference reported ~12s per million samples in the authors' setup (RTX 3090), and TabPFN encounters OOM when sampling sets are large (&gt;=50k with the used prior).",
            "efficiency_comparison": "On small datasets TabPFN is both accurate and very fast compared to models that require explicit training; however, because of quadratic Transformer cost, it is far less efficient on large datasets relative to GBDTs and standard deep models which can scale to large datasets.",
            "task_performance": "Strong on small tabular benchmarks (mean AUC ≈ 0.894); degrades or becomes infeasible on large datasets due to memory; adding the test set into input can substantially improve performance (upper-bound experiments show AUC gains).",
            "task_utility_analysis": "TabPFN's high-fidelity PPD approximation yields strong predictive performance when the input (sampled training set) aligns with the pretrained prior scale; when dataset sizes exceed pretraining or when prior mismatch occurs, task utility falls—motivating sampling/ensemble solutions.",
            "tradeoffs_observed": "Accuracy and speed on small problems versus inability to handle long input sequences (quadratic cost) that restricts use on large datasets; duplicating/including test samples in input increases accuracy for that sample but is impractical as a general solution.",
            "design_choices": "Transformer backbone without positional embeddings; uses concatenation of training and test samples as input tokens; pretraining on prior datasets sized ~1k; sampling of train examples (z) to limit input length is required for large problems.",
            "comparison_to_alternatives": "Compared to GBDTs and AutoML, TabPFN is often faster and competitive on small tasks; compared to deep-learning models on larger datasets TabPFN fails due to OOM while tree ensembles scale and sometimes achieve higher AUC on large datasets.",
            "optimal_configuration": "Paper indicates TabPFN works well when sampling size and batch size keep the Transformer input under memory limits (e.g., sampled inputs ~500 used in experiments); optimality depends on matching the problem size to the pretrained prior (pretraining size ~1024 suggested as limit for the used TabPFN).",
            "uuid": "e1387.1",
            "source_info": {
                "paper_title": "Prior-Fitted Networks Scale to Larger Datasets When Treated as Weak Learners",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "BoostPFN",
            "name_full": "BoostPFN (Boosted PFN ensemble)",
            "brief_description": "An ensemble method that treats PFNs as weak learners within a randomized gradient boosting framework, updating sampling weights (input selection) per round to scale PFNs to much larger tabular datasets while retaining efficient inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BoostPFN",
            "model_description": "An algorithm that ensembles multiple PFN predictions via gradient boosting: at each round it updates sampling weights over training examples (three update rules provided: ExpHadamard, Hadamard, AdaBoost-style), samples a subset D_w^z, uses q_theta(y | x, D_w^z) as a weak learner h_m(x), line-searches step size gamma_m, and accumulates F_m(x)=F_{m-1}(x)+gamma_m h_m(x).",
            "model_type": "ensemble of transformer PFN weak learners within Randomized Gradient Boosting Machine (RGBM)",
            "task_domain": "tabular classification across small to large datasets (used to extend PFN applicability to up to 50x the PFN pretraining size)",
            "fidelity_metric": "AUC-OVO used to evaluate predictive fidelity on classification benchmarks; convergence/boost loss tracked on training and test sets to monitor optimization.",
            "fidelity_performance": "Reported mean AUC-OVO for BoostPFN: ~0.895 on small datasets, 0.908 on 5,000-sample large datasets, 0.910 on 50,000-sample large datasets, and ~0.913 on &gt;50,000 averaged suites (see tables in paper). BoostPFN extends effective dataset size up to ≈50× the TabPFN pretraining scale while maintaining high AUC.",
            "interpretability_assessment": "BoostPFN is an ensemble of black-box PFNs; interpretability remains limited—no explicit interpretability methods for the ensemble components are presented, though sampling-weight updates provide an interpretable mechanism (which samples are emphasized).",
            "interpretability_method": "Sampling-weight trajectories and residual-based update rules (ExpHadamard / Hadamard / AdaBoost) are used; the paper reports ablations showing robustness across update rules but does not provide visualization beyond weight/ loss curves.",
            "computational_cost": "Time complexity for M rounds: M*(T_i + T_B) where T_i is one PFN inference pass and T_B is linear overhead O(T |D_test|) for boosting operations; experimentally BoostPFN costs about 60s per million samples (authors' hardware) compared to TabPFN ~12s and Bagging ~40s per million samples.",
            "efficiency_comparison": "BoostPFN's time complexity is close to bagging (parallelism-limited scenario) with additional linear boosting overhead; empirically it achieves better predictive results than GBDTs, deep models and AutoGluon under tight time budgets on many medium-large datasets while remaining competitive in time.",
            "task_performance": "On benchmark suites BoostPFN often achieves state-of-the-art or top results for dataset sizes up to 50k training samples (e.g., mean AUC 0.908 at 5k and 0.910 at 50k); when dataset size greatly exceeds pretraining scale and ensemble size saturates, baseline GBDTs or deep models can outperform BoostPFN.",
            "task_utility_analysis": "BoostPFN translates PFN fidelity into scalable task utility by focusing on input subset selection and ensembling; this yields improved AUC on larger datasets compared to single PFN instances, but the approach depends on the quality and scale of the PFN prior—limited pretraining size caps ultimate gains.",
            "tradeoffs_observed": "Trade-off: boosting/ensembling increases computational cost relative to a single PFN inference but gains scalability and improved AUC on larger datasets; diminishing returns observed when pretraining prior is small and when number of weak learners grows (saturation/possible overfitting).",
            "design_choices": "Treat PFN with chosen sampled subset as weak learner; update sampling weights via residual-based rules (ExpHadamard, Hadamard, AdaBoost); non-replacement sampling of training examples; line-search for gamma_m; choose number of weak learners scaled to dataset size (authors use 10 for 5k, 100 for 50k, 1000 for full).",
            "comparison_to_alternatives": "Compared to Bagging (PFN bags) and TabPFN: Bagging gives some improvement but BoostPFN yields superior AUC on larger datasets; compared to GBDTs/Deep models/AutoGluon: BoostPFN is more time-efficient under constrained budgets and competitive in AUC up to 50k samples, but may be outperformed on very large datasets (&gt;50k) if PFN prior is limited.",
            "optimal_configuration": "Paper recommends treating sampling-weight update method as a hyperparameter per dataset, using non-replacement sampling size (authors used input limited to 500 in many experiments), and scaling number of weak learners with dataset size (10 / 100 / 1000). Theoretical convergence holds for σ-smooth losses and bounded level sets, suggesting more rounds improves approximation (O(σ/M)).",
            "uuid": "e1387.2",
            "source_info": {
                "paper_title": "Prior-Fitted Networks Scale to Larger Datasets When Treated as Weak Learners",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GP",
            "name_full": "Gaussian Process (GP)",
            "brief_description": "A classical nonparametric Bayesian model used to obtain posterior predictive distributions; cited here to motivate the PFN fitting assumption because adding observations sharpens the GP posterior.",
            "citation_title": "Gaussian processes for machine learning.",
            "mention_or_use": "mention",
            "model_name": "Gaussian Process (GP)",
            "model_description": "A nonparametric Bayesian model that defines a prior over functions and yields closed-form posterior predictive distributions conditioned on observed data; adding an observation reduces predictive uncertainty and sharpens the posterior.",
            "model_type": "probabilistic nonparametric world model (Bayesian function prior)",
            "task_domain": "general supervised regression/classification and Bayesian inference contexts; referenced as an analogy to PFN behavior",
            "fidelity_metric": "Typically measured by posterior predictive MSE, log-likelihood or calibration of predictive distributions (paper does not report explicit GP metrics here).",
            "fidelity_performance": "Not reported numerically in this paper; GP invoked qualitatively to justify that adding target sample sharpens posterior predictive distribution.",
            "interpretability_assessment": "GPs are probabilistically interpretable (posterior mean and variance available analytically), and the paper uses this property to explain why adding target samples should increase predictive probability for that sample.",
            "interpretability_method": "analytic posterior predictive mean/variance (classical GP inference) — mentioned qualitatively only in this paper.",
            "computational_cost": "Not expanded in this paper; standard GP inference scales cubically in number of training points, but the paper does not provide those details.",
            "efficiency_comparison": "Not compared quantitatively in this paper; mentioned only as an intuitive baseline where adding an observation yields sharper posterior predictive behavior.",
            "task_performance": "Not provided in this paper.",
            "task_utility_analysis": "GP behavior is used as intuition: adding the target sample to the conditioning set increases posterior predictive confidence for that sample — used to motivate Assumption 4.1 for PFNs.",
            "tradeoffs_observed": "Only qualitative: GPs provide clear posterior updates but are not discussed in terms of scalability or computational trade-offs here.",
            "design_choices": "Mentioned as conceptual support for the idea that conditioning on the target sample improves prediction; no GP-specific design choices are evaluated in this paper.",
            "comparison_to_alternatives": "GPs are used as an intuition/contrast to PFNs; no empirical comparison to PFNs/TabPFN/BoostPFN is provided in this paper.",
            "optimal_configuration": "Not discussed in this paper.",
            "uuid": "e1387.3",
            "source_info": {
                "paper_title": "Prior-Fitted Networks Scale to Larger Datasets When Treated as Weak Learners",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "TabPFN: A transformer that solves small tabular classification problems in a second.",
            "rating": 2
        },
        {
            "paper_title": "Transformers can do bayesian inference.",
            "rating": 2
        },
        {
            "paper_title": "Statistical foundations of priordata fitted networks.",
            "rating": 2
        },
        {
            "paper_title": "Randomized gradient boosting machine",
            "rating": 1
        },
        {
            "paper_title": "Gaussian processes for machine learning.",
            "rating": 1
        }
    ],
    "cost": 0.019411499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Prior-Fitted Networks Scale to Larger Datasets When Treated as Weak Learners</h1>
<p>Yuxin Wang ${ }^{1,2}$<br>Quan Gan ${ }^{3}$<br>Botian Jiang ${ }^{1}$<br>David Wipf ${ }^{3}$<br>Yiran Guo ${ }^{1}$<br>Xipeng Qiu ${ }^{1}$<br>${ }^{1}$ School of Computer Science, Fudan University<br>${ }^{2}$ Institute of Modern Languages and Linguistics, Fudan University<br>${ }^{3}$ Amazon</p>
<h4>Abstract</h4>
<p>Prior-Fitted Networks (PFNs) have recently been proposed to efficiently perform tabular classification tasks. Although they achieve good performance on small datasets, they encounter limitations with larger datasets. These limitations include significant memory consumption and increased computational complexity, primarily due to the impracticality of incorporating all training samples as inputs within these networks. To address these challenges, we investigate the fitting assumption for PFNs and input samples. Building on this understanding, we propose BoostPFN designed to enhance the performance of these networks, especially for large-scale datasets. We also theoretically validate the convergence of BoostPFN and our empirical results demonstrate that the BoostPFN method can outperform standard PFNs with the same size of training samples in large datasets and achieve a significant acceleration in training times compared to other established baselines in the field, including widely-used Gradient Boosting Decision Trees (GBDTs), deep learning methods and AutoML systems. High performance is maintained for up to 50x of the pretraining size of PFNs, substantially extending the limit of training samples. Through this work, we address the challenges of efficiently handling large datasets via PFN-based models, paving the way for faster and more effective tabular data classification training and prediction process. Code is available at https://github.com/yxzwang/BoostPFN.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1 Introduction</h2>
<p>${ }^{1}$ Tabular datasets are increasingly garnering attention in various practical applications across fields such as finance [9], healthcare [12], and scientific research [10], among others [3, 45, 46, 47]. As we continue to amass larger and more complex datasets, the need for efficient and effective methods of data analysis becomes ever more critical. Traditional approaches often fall short due to prohibitively long training times when dealing with large datasets. This limitation not only hampers the speed of data processing but also impacts the overall feasibility of utilizing large-scale tabular data in timesensitive scenarios [10].</p>
<p>Recently, there has been a significant shift towards exploring novel methodologies that can overcome the limitations of traditional data processing techniques. One such promising development is the advent of the in-context learning method known as Prior-fitted Networks (PFNs) [33]. This innovative approach is capable of making predictions without the need for training on the training set, instead utilizing training data points as input tokens to the model architecture, thereby offering a much faster training and prediction process compared to conventional methods. The outstanding model in Prior-fitted Networks: TabPFN [21], uses a Transformer as the model architecture. Therefore, the primary issue with TabPFN on large datasets lies in the scalability of the Transformer: when applied to large datasets, it becomes impractical due to excessive memory consumption and computational complexity. The computational complexity of the Transformer increase quadratically with input length, which, for TabPFN, corresponds to the size of the training dataset. Despite these challenges, TabPFN's ability to rapidly process large tabular datasets remains a compelling feature, urging us to find a way to apply it to large datasets.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of BoostPFN. Each PFN with a subset of train data is viewed as a weak learner <em>h<sup>M</sup></em>(<em>x</em>) at round <em>M</em>. While adding new weak learners into the ensemble, new weak learner is governed by updating new sampling weights with the boosting residuals from the existing ensemble.</p>
<p>Table 1: Comparison of BoostPFN with GBDTs, Deep Learning (D.L.) methods and TabPFN.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Strong Performance with Limited Training Samples</th>
<th>Training Efficiency</th>
<th>Large Datasets</th>
</tr>
</thead>
<tbody>
<tr>
<td>GBDTs</td>
<td>✗</td>
<td>✗</td>
<td>✓</td>
</tr>
<tr>
<td>D.L.</td>
<td>✗</td>
<td>✗</td>
<td>✓</td>
</tr>
<tr>
<td>TabPFN</td>
<td>✓</td>
<td>✓</td>
<td>✗</td>
</tr>
<tr>
<td>BoostPFN</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
</tbody>
</table>
<p>In this paper, we introduce a way to treat PFN as a type of "weak" learner in the gradient boosting process, achieved through the Fitting Assumption 4.1. We then propose BoostPFN as an extension of Gradient Boosting [14] for ensembling these PFNs, as illustrated in Figure 1. We propose three updating methods based on the Fitting Assumption, including the AdaBoost updating, which allows our method to be viewed as a natural extension of the well-known boosting method AdaBoost [19]. These methods explore how ensemble learning techniques can be integrated with PFNs to create a more robust prediction model. We delve into both the theoretical and empirical validation of BoostPFN and demonstrate that our proposed methods not only achieve high performance but also maintain the efficiency in training that is critical for handling large-scale tabular datasets, compared to GBDTs and other deep learning methods that are trained explicitly. We compare BoostPFN with other types of models in Table 1. Our contributions are as follows:</p>
<ul>
<li>TabPFN is designed for fast training on tabular classification tasks but cannot handle large datasets, while BoostPFN extends the training dataset size and retains high performance, accommodating up to <em>50x</em> the pre-training size of TabPFN. Importantly, even when limited by TabPFN's pre-training size of 1024, which struggles to yield relatively good results compared to other models, our method paves the way for extending any new or improved PFN with a larger pre-training size and better training data.</li>
<li>We delve into the theoretical analysis of BoostPFN, showing that it is a type of Randomized Gradient Boosting Machine with a convergence guarantee. Empirical results also demonstrate the convergence of boosting loss on both the training and test sets when overfitting does not occur.</li>
<li>We conduct extensive experiments on datasets of varying sizes, from small to large, to assess the performance of BoostPFN, highlighting its efficacy and scalability.</li>
</ul>
<h2>2 Preliminaries</h2>
<p><strong>Posterior Predictive Distribution.</strong> The Posterior Predictive Distribution (PPD) is a concept in Bayesian statistics that combines the information from both the observed data and the posterior distribution of the model parameters. It is a predictive distribution for future observations based on the updated beliefs about the model parameters after incorporating the observed data.</p>
<p>$$p(y|x, D) \propto \int_{\Phi} p(y|x, \phi) p(D|\phi) p(\phi)d\phi.\tag{1}$$</p>
<p><strong>PFNs and Architecture.</strong> PFNs are proposed in [33, 21] to approximate the Posterior Predictive Distribution (PPD) for supervised learning. For a test sample <em>x<sub>test</sub></em> and train datasets <em>D</em> := (<em>x<sub>i</sub></em>, <em>y<sub>i</sub></em>)_{i∈{1, ..., <em>n</em>}}, the prediction of TabPFN [21] can be written as</p>
<p>$$p(\hat{y}<em _theta="\theta">{test}) = q</em>}(\hat{y<em text="text">{test} \mid x</em>$$}, D_{train}),\tag{2</p>
<p>where $\theta$ are the parameters of TabPFN. The architecture of TabPFN is a Transformer without positional embeddings. Training samples and test samples are concatenated together and taken as the input of Transformer. Probabilities for each class are given by Transformer as output.</p>
<p>Gradient Boosting. Gradient Boosting [14] is a powerful machine learning technique that constructs a predictive model $F_{m}(x)$ in the form of an ensemble of weak predictive models (commonly decision trees) at iteration $m$,</p>
<p>$$
F_{m}(x)=F_{m-1}(x)+\gamma_{m} h_{m}(x)
$$</p>
<p>where $h_{m}(x)$ is a weak learner selected from the candidate learner space $\mathcal{H}$ and $\gamma_{m}$ is the learning rate learned by line search. The $h_{m}(x)$ is selected to fit the residual errors $r_{m}=-\left[\frac{\partial \ell\left(y, F_{m-1}(x)\right)}{\partial F_{m-1}\left(x_{i}\right)}\right]$ of the prior models,</p>
<p>$$
h_{m}=\arg \min <em i="1">{h \in \mathcal{H}} \sum</em>
$$}^{\mid D_{\text {train }} \mid}\left|r_{m}[i]-h\left(x_{i}\right)\right|_{2}^{2</p>
<p>where $r_{m}[i]$ is the $i$ th value of vector $r_{m}$.</p>
<h2>3 PFN Scalability Challenges</h2>
<p>Long-range Transformers. While the Transformer is the backbone of TabPFN, it faces challenges with large datasets because TabPFN considers all training samples at once. There are Transformer variants $[18,8,2,50,48]$ that enable long-range inputs and show promising results compared to the vanilla Transformer with thousands of input tokens. However, for larger datasets with more training samples, the capability of handling thousands of input tokens is merely a drop in the bucket, and pre-training inevitably requires substantial time. Although one direction is to develop more efficient Transformer architectures, we focus on the alternative direction: applying a trained PFN to larger datasets.</p>
<p>Sampling Methods. A straightforward approach is to sample a small portion of training samples as the input training datasets, which is common for Transformers. In Natural Language Processing, training datasets are retrieved by a Retriever [37, 38, 35, 26, 51, $28,24,49,40,44,30]$ that considers the relationship between the test sample and candidate samples. However, retriever-based methods retrieve training samples differently for each test sample, making implementation challenging when the number of test samples is large. Additionally, retrieving training samples from a vast number of candidates is time-consuming, and results are not guaranteed because existing retrievers can only accept thousands of candidates. Therefore,
we opt to use simple sampling for training sample selection. However, this simple approach is very heuristic and may not yield good performance. To improve performance, we can employ ensemble methods. One of the simplest ensemble methods is Bagging [6], which samples multiple times and averages the predictions. We include this as our ensemble baseline method. For better performance, we will next introduce our method of scaling PFNs through boosting.</p>
<h2>4 Scaling PFNs through Boosting</h2>
<p>We propose a gradient boosting method for PFNs called BoostPFN. In the normal procedure of gradient boosting discussed in Section 2, an essential step is to fit a new weak learner based on training residuals, as shown in Eq. 4. For traditional weak learners, such as decision trees, this is easy to implement. However, for PFNs, there is no existing method to fit a new PFN while keeping the parameters of the Transformer architecture fixed. Optimizing the parameters of a PFN using gradient methods is time-consuming and undermines the primary advantage of PFNs, which is their negligible training time. To maintain training efficiency and enable the fitting step, we first propose learnable sampling optimization for PFNs. Since the input datasets are a crucial part of predicting the target sample, optimizing the input dataset can also enhance prediction accuracy. For normalized sampling weights $w$, a specified PFN $q_{\theta}$, and a test sample $x_{\text {test }}$, a PFN with a sampled input dataset is defined as</p>
<p>$$
q_{(\theta, w)}\left(y \mid x_{\text {test }}, D\right)=q_{\theta}\left(y \mid x_{\text {test }}, D_{w}^{z}\right)
$$</p>
<p>where $D$ is the full training set and $D_{w}^{z}$ is the sampled input dataset generated by weights $w \in \mathbf{R}^{|D|}$ and sampling size $z \in(0,|D|]$ which determines the sampled training set size $\left|D_{w}^{z}\right|=z$.</p>
<h3>4.1 Optimizing Input Datasets</h3>
<p>We first describe how we optimize the input dataset by updating $w$ using pseudo-residuals, similar to the approach in gradient boosting.
Assumption 4.1. Given a sample $\left(x_{i}, y_{i}\right)$, and any training set $\tilde{D}$, the probability of predicting the right class $y_{i}$ on $x_{i}$ be</p>
<p>$$
\begin{aligned}
&amp; q_{\theta}\left(y_{i} \mid x_{i}, \tilde{D} \cup\left{\left(x_{i}, y_{i}\right)\right}\right)&gt; \
&amp; \max \left[q_{\theta}\left(y_{i} \mid x_{i}, \tilde{D}\right), q_{\theta}\left(y_{i} \mid x_{i}, \tilde{D} \cup\left{\left(x_{j}, y_{j}\right)\right}\right)\right]
\end{aligned}
$$</p>
<p>Assumption 4.1 can be explained simply: adding the target sample or replacing one sample in the training set with the target sample will improve prediction performance. This seems straightforward for a GP (Gaussian Process) [39], as fitting a GP to a single observation sharpens the posterior predictive distribution</p>
<p>as it increases. However, PFNs are implemented in practice using a pretrained Transformer architecture, which does not inherently behave like a GP, and no such experiments have been conducted before. Therefore, we also perform experiments to empirically validate this assumption, as described in Section 7.1.</p>
<p>With this assumption, we can propose a simple idea: increasing the sampling weight $w[i]$ for $D[i]$ improves the probability of sampling that target $i$, thereby increasing the expectation of correctly predicting the label. Although this is a complex condition in practice, we can analyze it in a simplified scenario, as shown in Section A.1. This motivates a straightforward approach for updating sampling weights in a given dataset. In the next section, we explain in detail how to update them in practice.</p>
<h3>4.2 Updating Sampling Weights</h3>
<p>In this section, we discuss how to update sampling weights. Like the fitting residuals of gradient boosting in Eq. 4, we also want our our updating methods to depend on the residuals $r_{m}=-\left[\frac{\partial \ell\left(q_{i} F_{m-1}(x)\right)}{\partial F_{m-1}\left(x_{i}\right)}\right]$. However, there is no previous work on how to apply residuals in sampling weights updating. Starting by remembering that in the gradient descent, the absolute value of gradient goes to zero when the loss come to the optimal plain. We come up with a rule for updating sampling weights, that is to give the sample with larger absolute residual more weights in the next round. Then we propose three updating methods in this section.</p>
<p>ExpHadamard Updating. New weights are the hadamard products of this round's weights and the exponential of the residual,</p>
<p>$$
w_{m}=w_{m-1} \odot \exp \left(\left|r_{m}\right|\right)
$$</p>
<p>Hadamard Updating. New weights are the hadamard products of this round's weights and the residual,</p>
<p>$$
w_{m}=w_{m-1} \odot\left|r_{m}\right|
$$</p>
<p>AdaBoost Updating. Also, we adopt the updating in AdaBoost [19] given by</p>
<p>$$
\begin{aligned}
\epsilon_{m} &amp; =w_{m-1} \otimes \mathbf{1}\left{h_{m-1}(X[i]) \neq Y[i]\right} \
\alpha_{m} &amp; =\log \left(\frac{1-\epsilon_{m}}{\epsilon_{m}}\right)+\log (K-1) \
w_{m} &amp; =w_{m-1} \odot \exp \left(\alpha_{m} \mathbf{1}\left{h_{m-1}(X[i]) \neq Y[i]\right}\right)
\end{aligned}
$$</p>
<p>where $\epsilon_{m}$ is the summation of the sampling weights of samples that the last weak learner predicts wrong on and $K$ is the class number of targets. And we increase the weights for wrong samples and decrease the weights for right samples based on $\epsilon_{m}$. As an alternative, the
$\gamma_{m}$ could also be replaced by $\alpha_{m}$ as in AdaBoost. In practice, we choose the $\gamma_{m}$ with better performances. Note that although there is no residuals in the AdaBoost Updating, $\epsilon_{m}$ can be seen as a kind of residual for a specified AdaBoost loss funciton $\ell_{\text {AdaBoost }}$ [20].</p>
<p>In practice, we view the updating method as a hyperparameter of BoostPFN and choose the best one for each dataset. The ablation over different updating methods are shown in Section 7.2.</p>
<h3>4.3 BoostPFN</h3>
<p>By combining the ingredients from above, with the the gradient boosting process discussed in Section 2, we arrive at our BoostPFN approach, the steps of which are summarized in Algorithm 1.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="err">:</span><span class="w"> </span><span class="n">BoostPFN</span>
<span class="nl">Require</span><span class="p">:</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="k">data</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">D</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">Y</span><span class="p">)</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="n">PFN</span><span class="w"> </span><span class="n">model</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">q_</span><span class="err">{\</span><span class="n">theta</span><span class="err">}</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="err">\</span><span class="n">mid</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">D</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">sampling</span><span class="w"> </span><span class="n">weights</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">w</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">training</span>
<span class="w">    </span><span class="n">samples</span><span class="p">,</span><span class="w"> </span><span class="n">sampling</span><span class="w"> </span><span class="k">size</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">z</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="n">boosting</span><span class="w"> </span><span class="nf">round</span><span class="w"> </span><span class="n">number</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">M</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">boosting</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">ell</span><span class="err">\</span><span class="p">).</span>
<span class="k">Output</span><span class="err">:</span><span class="w"> </span><span class="n">BoostPFN</span><span class="w"> </span><span class="n">predictor</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="k">Initialize</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">F_</span><span class="err">{</span><span class="mi">0</span><span class="err">}</span><span class="o">=</span><span class="mi">0</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">m</span><span class="o">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">M</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="k">Compute</span><span class="w"> </span><span class="nl">residual</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">r_</span><span class="err">{</span><span class="n">m</span><span class="err">}</span><span class="o">=-</span><span class="err">\</span><span class="n">frac</span><span class="err">{\</span><span class="k">partial</span><span class="w"> </span><span class="err">\</span><span class="n">ell</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">q_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="w"> </span><span class="n">F_</span><span class="err">{</span><span class="n">m</span><span class="o">-</span><span class="mi">1</span><span class="err">}</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="err">}{\</span><span class="k">partial</span><span class="w"> </span><span class="n">F_</span><span class="err">{</span><span class="n">m</span><span class="o">-</span><span class="mi">1</span><span class="err">}</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="err">}\</span><span class="p">).</span>
<span class="w">        </span><span class="k">Update</span><span class="w"> </span><span class="n">sampling</span><span class="w"> </span><span class="n">weights</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">w_</span><span class="err">{</span><span class="n">m</span><span class="err">}</span><span class="o">=</span><span class="n">U</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="n">e</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">w_</span><span class="err">{</span><span class="n">m</span><span class="o">-</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">r_</span><span class="err">{</span><span class="n">m</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">via</span><span class="w"> </span><span class="n">methods</span><span class="w"> </span><span class="ow">in</span>
<span class="w">        </span><span class="k">Section</span><span class="w"> </span><span class="mf">4.2</span><span class="p">.</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">w_</span><span class="err">{</span><span class="n">m</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="k">Normalize</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">w_</span><span class="err">{</span><span class="n">m</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">        </span><span class="n">Do</span><span class="w"> </span><span class="n">non</span><span class="o">-</span><span class="n">replacement</span><span class="w"> </span><span class="n">sampling</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">w_</span><span class="err">{</span><span class="n">m</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="k">to</span>
<span class="w">        </span><span class="n">obtain</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">subset</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="k">data</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">D_</span><span class="err">{</span><span class="n">w_</span><span class="err">{</span><span class="n">m</span><span class="err">}}</span><span class="o">^</span><span class="err">{</span><span class="n">z</span><span class="err">}\</span><span class="p">)</span>
<span class="w">        </span><span class="k">Select</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">weak</span><span class="w"> </span><span class="n">learner</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">h_</span><span class="err">{</span><span class="n">m</span><span class="err">}</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">PFN</span>
<span class="w">        </span><span class="n">model</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">q_</span><span class="err">{\</span><span class="n">theta</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="err">\</span><span class="n">mid</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">D_</span><span class="err">{</span><span class="n">w_</span><span class="err">{</span><span class="n">m</span><span class="err">}}</span><span class="o">^</span><span class="err">{</span><span class="n">z</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">        </span><span class="n">Line</span><span class="w"> </span><span class="k">search</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">gamma_</span><span class="err">{</span><span class="n">m</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">gamma_</span><span class="err">{</span><span class="n">m</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">arg</span><span class="w"> </span><span class="err">\</span><span class="nf">min</span><span class="w"> </span><span class="n">_</span><span class="err">{\</span><span class="n">gamma</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">sum_</span><span class="err">{</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">N_</span><span class="err">{</span><span class="n">i</span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">ell</span><span class="p">(</span><span class="n">Y</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">F_</span><span class="err">{</span><span class="n">m</span><span class="o">-</span><span class="mi">1</span><span class="err">}</span><span class="p">(</span><span class="n">X</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">)</span><span class="o">+</span><span class="err">\</span><span class="n">gamma</span><span class="w"> </span><span class="n">h_</span><span class="err">{</span><span class="n">m</span><span class="err">}</span><span class="p">(</span><span class="n">X</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">        </span><span class="k">Get</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">predictor</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">F_</span><span class="err">{</span><span class="n">m</span><span class="err">}</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">=</span><span class="n">F_</span><span class="err">{</span><span class="n">m</span><span class="o">-</span><span class="mi">1</span><span class="err">}</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="err">\</span><span class="n">gamma_</span><span class="err">{</span><span class="n">m</span><span class="err">}</span><span class="w"> </span><span class="n">h_</span><span class="err">{</span><span class="n">m</span><span class="err">}</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">=</span><span class="n">F_</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<h2>5 Analysis of BoostPFN</h2>
<h3>5.1 Time Complexity</h3>
<p>Table 2: Time Complexity for PFNs. We use $T_{i}$ to denote the inference time for TabPFN. $M$ is the round we use for bagging or boosting. $T_{B}$ is the time cost of boosting operations in each boosting round.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">TabPFN</th>
<th style="text-align: center;">Bagging</th>
<th style="text-align: center;">BoostPFN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$T_{i}$</td>
<td style="text-align: center;">$M T_{i}$</td>
<td style="text-align: center;">$M\left(T_{i}+T_{B}\right)$</td>
</tr>
</tbody>
</table>
<p>We briefly show the time complexity in Table 2. We con-</p>
<p>sider the complexity analysis in a scenario with limited resources. Specifically, given the hardware available, we cannot load the entire test set due to out-of-memory (OOM) issues. Note that even with a small number of training samples, the OOM problem can still occur if the test set is large enough. Therefore, in practice, we have to split the test set into $N_{\text {test }}$ batches, and the inference of one batch consumes all the available memory (which is possible for millions of test samples). In this scenerio, the time complexity of BoostPFN is close to the simple Bagging method except for the addtion of inevitable boosting operations, which is an acceptable solution for large datasets. Then we describe in detail those symbols.</p>
<p>TabPFN. One inference of TabPFN on a single batch costs $O\left(L^{2}\right)$ time, where $L=\left|D_{m}\right|+B$ represents the input length, equal to the sum of the length of the sampled dataset $\left|D_{m}\right|$ and the batch size $B$. For one inference of TabPFN on the entire test set, we need $T_{i}$ time. If we consider the sampling time negligible, this is also the time complexity for sampling a training set and performing TabPFN inference on the whole test set, which can be further reduced to</p>
<p>$$
\begin{aligned}
T_{i} &amp; =O\left(N_{\text {test }}\left(\left|D_{m}\right|+B\right)^{2}\right) \
&amp; =O\left(N_{\text {test }}\left(\left|D_{m}\right|^{2}+B^{2}+2\left|D_{m}\right| B\right)\right) \
&amp; =O\left(\frac{\left|D_{m}\right|^{2}}{B}\left|D_{\text {test }}\right|+B\left|D_{\text {test }}\right|+2\left|D_{m}\right|\left|D_{\text {test }}\right|\right)\right) \
&amp; =O\left(\left(\frac{\left|D_{m}\right|^{2}}{B}+B+2\left|D_{m}\right|\right)\left|D_{\text {test }}\right|\right)
\end{aligned}
$$</p>
<p>where we use the fact that $O\left(N_{\text {test }} B\right)=O\left(\left|D_{\text {test }}\right|\right)$.
Bagging. As an easy ensemble method mentioned in Section 3, bagging is generally implemented in parallel. However, in our scenario, parallel inference on the test set is impossible because one inference on a single batch consumes all the GPU memory available. Thus, if we take $M$ bags and ignore the sampling time, the time complexity for bagging with TabPFN is $M T_{i}$.</p>
<p>BoostPFN. For the time-consuming sampling gradient boosting process, we observe that all operations, including updating weights or performing line searches for coefficients, are linear with respect to the training set. Therefore, the total time complexity for these operations is $T_{B}=O\left(T \mid D_{\text {test }} \mid\right)$, where $T$ is a constant. For $M$ rounds of boosting, the time complexity is $M\left(T_{i}+T_{B}\right)$, which is very close to the time complexity of the bagging method. Our analysis shows that, in a limited-resource scenario, BoostPFN can achieve similar time efficiency to bagging.</p>
<h3>5.2 Convergence</h3>
<p>We next examine the convergence of Algorithm 1, because methods that ensure convergence are typically
more dependable, reproducible, and possibly efficient. We first define $F^{*}(x)$ as the optimal predictor that minimizes the loss $\ell$ :</p>
<p>$$
F^{*}(x)=\arg \min _{F(x) \in \mathcal{F}} \ell(y, F(x))
$$</p>
<p>where $\mathcal{F}$ is the searching space of possible predictors as determined by the associated family of weak learners $\mathcal{H}$. Then we show the convergence in Theorem 5.1.</p>
<p>Theorem 5.1. Let $F_{M}(x)$ denote the BoostPFN predictor produced by Algorithm 1 after $M$ steps. Then if the loss $\ell$ is $\sigma$-smooth and has a bounded level set, we have that</p>
<p>$$
\mathbb{E}\left[\left|\ell\left(y, F_{M}(x)\right)-\ell\left(y, F^{*}(x)\right)\right|\right] \leq O\left(\frac{\sigma}{M}\right)
$$</p>
<p>while $\sigma$-smooth is defined in
Definition 5.2. $\ell$ is $\sigma$-smooth if for any $y$ and predictions $f_{1}$ and $f_{2}$, it holds that</p>
<p>$$
\ell\left(y, f_{1}\right) \leq \ell\left(y, f_{2}\right)+\frac{\partial \ell\left(y, f_{2}\right)}{\partial f}\left(f_{1}-f_{2}\right)+\frac{\sigma}{2}\left(f_{1}-f_{2}\right)^{2}
$$</p>
<p>The proof of Theorem 5.1 is deferred to Appendix A.2, which is stratified by first proving that BoostPFN is a kind of RGBM (Randomized Gradient Boosting Machine) [31] and then using some known properties of RGBM. While our loss $\ell$ for classification is the same as RGBM in [31] that fits the criterion, Theorem 5.1 validates the convergence of BoostPFN. We also empirically validate the convergence of BoostPFN in Section 7.3.</p>
<h2>6 Experiments</h2>
<p>We compare BoostPFN with other tabular baselines on datasets from small to large to show its effectiveness and training efficiency.</p>
<p>Datasets and Baselines. We use the same 30 datasets in [21] from OpenML Benchmarking Suites [4] and 30 larger datasets including real-world datasets and artificial datasets generated by Bayesian Neural Networks with more than 100 thousand samples and less than 100 features from OpenML Benchmark. Full datasets are listed in Appendix C. We split the datasets following [21] in 50/50 for training and test in 5 random seeds and report the average results. For larger datasets in Table 9, we also conduct experiments on a limited size of training samples ( 5000 and 50000 samples respectively). For baselines, we use LightGBM [27], XGBoost [7], CatBoost [36] for GBDTs, and FT-Transformer [16], SAINT [43] for deep learning models, and an AutoML system AutoGluon [11]. We include TabPFN as a candidate baseline when not encountering OOM (out of memory) problems. We</p>
<p>Table 3: Mean AUCs on large tabular datasets with different number of training samples. The best result is bolded and the second best one is underlined in each row. Results for each dataset are shown in Appendix D.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$#$ of Training</th>
<th style="text-align: left;">LightGBM</th>
<th style="text-align: left;">CatBoost</th>
<th style="text-align: left;">XGBoost</th>
<th style="text-align: left;">AutoGluon</th>
<th style="text-align: left;">FT-Trans.</th>
<th style="text-align: left;">SAINT</th>
<th style="text-align: left;">TabPFN</th>
<th style="text-align: left;">Bagging</th>
<th style="text-align: left;">BoostPFN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$&lt;5000$</td>
<td style="text-align: left;">0.884</td>
<td style="text-align: left;">0.891</td>
<td style="text-align: left;">0.890</td>
<td style="text-align: left;">$\mathbf{0 . 8 9 5}$</td>
<td style="text-align: left;">0.875</td>
<td style="text-align: left;">0.824</td>
<td style="text-align: left;">$\underline{0.894}$</td>
<td style="text-align: left;">0.893</td>
<td style="text-align: left;">$\mathbf{0 . 8 9 5}$</td>
</tr>
<tr>
<td style="text-align: left;">5,000</td>
<td style="text-align: left;">0.865</td>
<td style="text-align: left;">0.865</td>
<td style="text-align: left;">0.849</td>
<td style="text-align: left;">0.844</td>
<td style="text-align: left;">$\underline{0.900}$</td>
<td style="text-align: left;">0.878</td>
<td style="text-align: left;">0.885</td>
<td style="text-align: left;">0.874</td>
<td style="text-align: left;">$\mathbf{0 . 9 0 8}$</td>
</tr>
<tr>
<td style="text-align: left;">50,000</td>
<td style="text-align: left;">0.883</td>
<td style="text-align: left;">0.864</td>
<td style="text-align: left;">0.855</td>
<td style="text-align: left;">0.865</td>
<td style="text-align: left;">$\mathbf{0 . 9 1 0}$</td>
<td style="text-align: left;">$\underline{0.907}$</td>
<td style="text-align: left;">OOM</td>
<td style="text-align: left;">0.888</td>
<td style="text-align: left;">$\mathbf{0 . 9 1 0}$</td>
</tr>
<tr>
<td style="text-align: left;">$&gt;50,000$</td>
<td style="text-align: left;">0.915</td>
<td style="text-align: left;">0.911</td>
<td style="text-align: left;">$\underline{0.916}$</td>
<td style="text-align: left;">0.915</td>
<td style="text-align: left;">$\mathbf{0 . 9 1 8}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 1 8}$</td>
<td style="text-align: left;">OOM</td>
<td style="text-align: left;">0.895</td>
<td style="text-align: left;">0.913</td>
</tr>
</tbody>
</table>
<p>also include the Bagging method for TabPFN as an ensemble baseline. To get different number of training samples, we sample from the whole training set randomly. And the number of weak learners in ensemble model is 10 for 5,000 training samples, 100 for 50,000 and 1,000 for full training set.</p>
<p>Implementations. For evaluation metrics, we follow [21] and use AUC-OVO (which is the results of AUC by one class versus one class), which is most important in tabular datas because of unbalanced class samples. For boosting and bagging methods, ensemble numbers are fixed to 10 if not mentioned specifically. The input data is limited to 500 samples considering the balance of performance and time consumption. Full experimental implementation can be found in Appendix B.</p>
<p>Results. All model performances are shown in Table 3. " $&lt;5000$ " shows the average of small datasets. Other rows represent the average results of large datasets with different numbers of training samples. Result for each dataset is listed in Appendix D. It is showed that BoostPFN has strong merits when the number of training samples is under 50000. When on small datasets whose number of training samples is lower than 5000, BoostPFN can get state-of-the-art AUC results together with AutoGluon, also better than TabPFN. When it comes to large datasets, BoostPFN can get state-of-the-art AUC results when the number of training samples is not higher than 50000, while TabPFN meets OOM problems when it comes to 50,000 training samples. This demonstrates the effectiveness of gradient boosting when training set size is larger than pre-training size of TabPFN (which is 1024). Baseline models can get better performances when the training set size increases (except CatBoost with a bit lower with 50,000 training samples, maybe because of not well-tuned hyperparameters due to long training time and time limitation). When using training samples larger than 50000, BoostPFN cannot get very good results compared to baseline models (while still better than CatBoost) because the existing prior is trained only on 1,000 samples and up to 1,000 weak learners can quickly saturate the performance improvement. The experimental results show BoostPFN can extend the dataset size of pre-training for TabPFN to 50x and
still remain high performances.
Efficiency. Next we show the performances of BoostPFN and baselines as the function of time limitation in Figure 2. All models use 5,000 training samples and details are shown in Supplemental Section E. The results show that BoostPFN can get best results when still maintaining efficiency, using much shorter time compared to GBDTs, deep learning methods and Autogluon, and a bit time longer than TabPFN and Bagging (While not shown in the figure, TabPFN costs about 12s and Bagging costs about 40s and BoostPFN costs about 60s per million samples). When more time is allowed, the results for baselines can be improved a bit.</p>
<h2>7 Ablations</h2>
<h3>7.1 Emperical Validation of Assumption 4.1</h3>
<p>We conduct the experiments based on TabPFN and on the dataset "analcatdata_dmft" that it performs not well [21]. We examine the performances of TabPFN when modifying the input training set.</p>
<p>Test Set. We first aim to determine the upper bound of TabPFN on the test set. The results are shown in Table 4. $D_{\text {train }}+D_{\text {train }}$ refers to the concatenation of two identical training sets. $D_{\text {train }}+D_{\text {test }}$ refers to the concatenation of the training set and the test set. $D_{\text {train }}+2 D_{\text {test }}$ refers to the concatenation of the training set and two identical test sets. The results show that adding the test set as part of the input training set for TabPFN significantly improves its performance, with the upper bound being the results of using $D_{\text {test }}$ as the sole input training set. Duplicating the test set does not help TabPFN perform better on the test set, indicating that this is indeed the upper bound for this test set. This upper bound suggests some interference between test samples that could decrease performance, supporting Assumption 4.1. We further validate the assumption through experiments on a single test sample.</p>
<p>Single Test Sample. Next, we demonstrate how the prediction for a single test sample depends on the input training samples. In Table 5, we show the probability of TabPFN predicting the correct label for just one test sample, $\left(x_{t 1}, y_{t 1}\right)$, from the same dataset.</p>
<p>Figure 2: We show Mean AUC, Wins AUC and Mean Rank. as a function of the time allowed to train and tune methods, on large datasets with training number equals to 5,000 from OpenML benchmarks.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Table 4: Upper Bound of TabPFN on full test set.</p>
<table>
<thead>
<tr>
<th>Input Train set</th>
<th>$D_{\text {train }}+D_{\text {train }}$</th>
<th>$D_{\text {train }}+D_{\text {test }}$</th>
<th>$D_{\text {train }}+2D_{\text {test }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>AUC OVO</td>
<td>0.5815</td>
<td>0.6812</td>
<td>0.7135</td>
</tr>
<tr>
<td>Input Train set</td>
<td>$D_{\text {test }}$</td>
<td>$D_{\text {test }}+D_{\text {test }}$</td>
<td></td>
</tr>
<tr>
<td>AUC OVO</td>
<td>0.7154</td>
<td>0.7154</td>
<td></td>
</tr>
</tbody>
</table>
<p>First, we consider adding duplicates of the target sample to the training set. For this single sample, the input training set does not significantly aid the prediction, as the probability (0.192) is slightly higher than random guessing ( 0.167 for a 6 -class classification task). Adding just one duplicate of this sample to the input training set improves the probability slightly. Adding two duplicates further increases the probability to 0.22 . Subsequently, adding 10 duplicates increases the probability to 0.42 , and adding 20 duplicates increases it to 0.75 . Finally, adding 50 duplicates raises the probability to 0.94 , and 100 duplicates to 0.98 .</p>
<p>We then examine the results of replacing training samples with duplicates of the target sample. Similar to the trend observed when adding duplicates, replacing more training samples with the target sample also improves prediction accuracy (from 0.192 to 0.992 as the number of replacements increases from 0 to 100). Furthermore, with the same number of duplicates, replacing training samples performs better than simply adding them. This is reasonable because the training set with added duplicates contains more irrelevant samples than the training set where samples are replaced, further validating Assumption 4.1.</p>
<p>In a nutshell, TabPFN has performance upper bounds on the entire test set, even when duplicates of the test set are used as input. This may be because the prior is not well-suited for this dataset. However, for a specific sample, adding duplicates can increase the probability of predicting the correct class to as much as</p>
<p>Table 5: Probability of TabPFN predicting the right class on single test sample. We show the number of adding or replacing with the probability.</p>
<table>
<thead>
<tr>
<th>$#$ of Adding</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>10</th>
<th>20</th>
<th>50</th>
<th>100</th>
</tr>
</thead>
<tbody>
<tr>
<td>Prob.</td>
<td>0.192</td>
<td>0.206</td>
<td>0.220</td>
<td>0.417</td>
<td>0.748</td>
<td>0.945</td>
<td>0.981</td>
</tr>
<tr>
<td>$#$ of Replacing</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>10</td>
<td>20</td>
<td>50</td>
<td>100</td>
</tr>
<tr>
<td>Prob.</td>
<td>0.192</td>
<td>0.206</td>
<td>0.223</td>
<td>0.460</td>
<td>0.787</td>
<td>0.975</td>
<td>0.992</td>
</tr>
</tbody>
</table>
<p>$98 \%$, validating that adding the target sample improves performance, as stated in Assumption 4.1. Additionally, replacing training samples with the target sample also improves the probability, further supporting the notion that replacing irrelevant samples with the target sample enhances performance, as proposed in Assumption 4.1.</p>
<h3>7.2 Sampling Weight Updates</h3>
<p>In Section 4.2 we introduce three updating methods in the BoostPFN. Here we compare the results of those methods in Table 6. Exp., Hada. and Ada. stands for ExpHadamard, Hadamard and Adaboost updating respectively. The OVO AUC results are very close across different updating methods, showing the robustness towards them. However, in practice we encourage choosing the best one to get best results.</p>
<p>Table 6: Mean AUC OVO of three updating method in BoostPFN.</p>
<table>
<thead>
<tr>
<th>$#$ of Training</th>
<th>Exp.</th>
<th>Hada.</th>
<th>Ada.</th>
</tr>
</thead>
<tbody>
<tr>
<td>$&lt;5000$</td>
<td>0.893</td>
<td>0.890</td>
<td>0.894</td>
</tr>
<tr>
<td>5000</td>
<td>0.900</td>
<td>0.898</td>
<td>0.905</td>
</tr>
<tr>
<td>50000</td>
<td>0.905</td>
<td>0.907</td>
<td>0.907</td>
</tr>
<tr>
<td>$&gt;50000$</td>
<td>0.910</td>
<td>0.911</td>
<td>0.907</td>
</tr>
</tbody>
</table>
<p>Figure 3: The first row shows boost loss on the training set of 3 datasets. The second row shows boost loss on the test set. The x-axis is the number of weak learners. From left to right, dataset order is BNG(primary-tumor), pokerhand, KDDCup99.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<h3>7.3 Emperical Convergence of BoostPFN</h3>
<p>Although the fitting assumption of PFN is validated by experiments, we are curious about how the gradient boost actually works in large datasets. So we examine the boost loss on training set and test set and boost residual we used for gradient boost on training set. We use ExpHadamard Updating for the experiments. Full results of all datasets are showed in Appendix F, while we show the results of three datasets (BNG(primarytumor), pokerhand, KDDCup99) in Figure 3, on which BoostPFN improves the best compared to TabPFN. We can find that when the number of weak learners increase, the boost loss on training set and test set decrease at the same time, showing the effectiveness of our method.</p>
<h2>8 Related Works</h2>
<p>Tabular Data Classification. Machine Learning methods and Deep Learning methods are two main types of method for tabular data classification. For Machine learning methods, variants of GBDTs [14] dominates this field mainly because of their fast training time and robustness [41]. LightGBM [27], CatBoost [36] and XGBoost [7] are three popular GBDTs applied in tabular data classification. Previous works have found that deep learning models are not better than the performance of GBDT or AutoML methods for small to medium-sized tabular data ( $&lt;$ 10,000 samples; while matching GBDT performance on larger datasets) [5, 17, 42]. Furthermore, deep learning methods often use custom parameter tuning and preprocessing that makes the training on large datasets much more time-consuming, including FT-Transformer, SAINT, etc. [16, 43, 1, 25, 29]. There are also AutoML systems like AutoSklearn2 [13] and Autogluon [11]. AutoSklearn2 uses Bayesian Optimization and combines
the evaluated models into a weighted ensemble and Autogluon combines a broad kind of models including neural networks and tree-based models into a stacked ensemble.</p>
<p>PFNs. PFNs are proposed by [33] on binary classification and small tabular datas of 100 samples. [21] further improve it to 10-class classification and mediumsize (thousands of samples) tabular datas. [34] analyze the statistical foundation of a single PFN and not an ensemble of PFNs. Larger datasets remain un-discovered in this field and our work extends the field of PFNs to larger datasets.</p>
<p>Ensemble for Transformers. Ensemble methods like AdaBoost [19] or Gradient Boosting [14, 15] are commonly applied on decision trees for better performances. Recently, Transformers are combined with AdaBoost in many fields. [23] integrate LLM into AdaBoost for natural language inference (NLI) tasks, by training an MLP projection of the final hidden state of a special token. [32] concentrates on condensing knowledge into an intermediary representation referred to as "summary." and applies AdaBoost for In-Context Learning for the prediction. However, their combination of summary sampling and AdaBoost are not theoretically validated. In this paper we propose a more generalized Gradient boosting method that is well validated theoretically and performs better in experiments.</p>
<h2>9 Conclusion</h2>
<p>In this paper, we have introduced BoostPFN, an innovative approach to scaling Prior-Fitted Networks (PFNs) for larger datasets, addressing the scalability challenges encountered by PFNs when applied to extensive data sets. Our method, through a meticulous investigation into the fitting assumptions of PFNs and the selection of input samples, presents a gradient boosting frame-</p>
<p>work that significantly enhances the performance of PFNs, particularly on a large-scale. BoostPFN significantly extends the data size that PFNs can effectively process - up to 50 times the pre-training dataset size-while maintaining high performance compared to GBDTs, deep learning methods, and AutoML systems. We also provide a theoretical analysis that substantiates its convergence. BoostPFN offers a new perspective on handling large-scale tabular data classification tasks efficiently. In the future when PFNs are trained on larger datasets and perform better (for example, [22], which is published while our paper is under review), BoostPFN can still boost the predictions on much larger datasets with the new, better PFNs.</p>
<h2>10 Acknowledgement</h2>
<p>No acknowledgement to report at this time..</p>
<h2>References</h2>
<p>[1] Sercan Ö Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 6679-6687, 2021.
[2] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020.
[3] Omar Benjelloun, Shiyu Chen, and Natasha Noy. Google dataset search by the numbers. In International Semantic Web Conference, pages 667-682. Springer, 2020.
[4] Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Pieter Gijsbers, Frank Hutter, Michel Lang, Rafael Gomes Mantovani, Jan N van Rijn, and Joaquin Vanschoren. Openml benchmarking suites. In Proceedings of the NeurIPS 2021 Datasets and Benchmarks Track. 2021.
[5] Vadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep neural networks and tabular data: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.
[6] Leo Breiman. Bagging predictors. Machine learning, 24:123-140, 1996.
[7] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785794, 2016.
[8] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019.
[9] Jillian M Clements, Di Xu, Nooshin Yousefi, and Dmitry Efimov. Sequential deep learning for credit risk monitoring with tabular financial data. arXiv preprint arXiv:2012.15330, 2020.
[10] Allison McCarn Deiana, Nhan Tran, Joshua Agar, Michaela Blott, Giuseppe Di Guglielmo, Javier Duarte, Philip Harris, Scott Hauck, Mia Liu, Mark S Neubauer, et al. Applications and techniques for fast machine learning in science. Frontiers in big Data, 5:787421, 2022.
[11] Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola. Autogluon-tabular: Robust and accurate automl for structured data. arXiv preprint arXiv:2003.06505, 2020.
[12] Andre Esteva, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. A guide to deep learning in healthcare. Nature medicine, 25(1):2429, 2019.
[13] Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and Frank Hutter. Auto-sklearn 2.0: Hands-free automl via metalearning. The Journal of Machine Learning Research, 23(1):11936-11996, 2022.
[14] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pages 1189-1232, 2001.
[15] Jerome H Friedman. Stochastic gradient boosting. Computational statistics $\&amp;$ data analysis, 38(4):367-378, 2002.
[16] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models for tabular data. Advances in Neural Information Processing Systems, 34:1893218943, 2021.
[17] Léo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? Advances in Neural Information Processing Systems, 35:507520, 2022.
[18] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. Startransformer. In Proceedings of HLT-NAACL, pages 1315-1325, 2019.</p>
<p>[19] Trevor Hastie, Saharon Rosset, Ji Zhu, and Hui Zou. Multi-class adaboost. Statistics and its Interface, 2(3):349-360, 2009.
[20] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. The elements of statistical learning: data mining, inference, and prediction, volume 2. Springer, 2009.
[21] Noah Hollmann, Samuel Müller, Katharina Eggensperger, and Frank Hutter. TabPFN: A transformer that solves small tabular classification problems in a second. In The Eleventh International Conference on Learning Representations, 2023.
[22] Noah Hollmann, Samuel Müller, Lennart Purucker, Arjun Krishnakumar, Max Körfer, Shi Bin Hoo, Robin Tibor Schirrmeister, and Frank Hutter. Accurate predictions on small data with a tabular foundation model. Nature, 637(8045):319-326, 2025.
[23] Bairu Hou, Joe O'connor, Jacob Andreas, Shiyu Chang, and Yang Zhang. Promptboosting: Blackbox text classification with ten forward passes. In International Conference on Machine Learning, pages 13309-13324. PMLR, 2023.
[24] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research, 2022.
[25] Arlind Kadra, Marius Lindauer, Frank Hutter, and Josif Grabocka. Well-tuned simple nets excel on tabular datasets. Advances in neural information processing systems, 34:23928-23941, 2021.
[26] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. Dense passage retrieval for open-domain question answering, 2020.
[27] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30, 2017.
[28] Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert, 2020.
[29] Jannik Kossen, Neil Band, Clare Lyle, Aidan N Gomez, Thomas Rainforth, and Yarin Gal. Selfattention between datapoints: Going beyond in-
dividual input-output pairs in deep learning. Advances in Neural Information Processing Systems, 34:28742-28756, 2021.
[30] Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun, and Xipeng Qiu. Llatrieval: Llm-verified retrieval for verifiable generation, 2023.
[31] Haihao Lu and Rahul Mazumder. Randomized gradient boosting machine, 2020.
[32] Hariharan Manikandan, Yiding Jiang, and J Zico Kolter. Language models are weak learners, 2023.
[33] Samuel Müller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers can do bayesian inference. In International Conference on Learning Representations, 2021.
[34] Thomas Nagler. Statistical foundations of priordata fitted networks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 25660-25676. PMLR, 23-29 Jul 2023.
[35] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query prediction, 2019.
[36] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: unbiased boosting with categorical features. Advances in neural information processing systems, 31, 2018.
[37] Anand Rajaraman and Jeffrey David Ullman. Data Mining, page 1-17. Cambridge University Press, 2011.
[38] S. Robertson. The Probabilistic Relevance Framework: BM25 and Beyond. Foundations and Trends® in Information Retrieval, 3(4):333-389, 2009.
[39] Matthias Seeger. Gaussian processes for machine learning. International journal of neural systems, 14(02):69-106, 2004.
[40] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. Replug: Retrievalaugmented black-box language models, 2023.
[41] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need, 2021.</p>
<p>[42] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion, 81:84-90, 2022.
[43] Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C Bayan Bruss, and Tom Goldstein. Saint: Improved neural networks for tabular data via row attention and contrastive pre-training. arXiv preprint arXiv:2106.01342, 2021.
[44] Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text embeddings, 2023.
[45] Qi Tang, Guoen Xia, Xianquan Zhang, and Feng Long. A customer churn prediction model based on xgboost and mlp. In 2020 International Conference on Computer Engineering and Application (ICCEA), pages 608-612. IEEE, 2020.
[46] Dennis Ulmer, Lotta Meijerink, and Giovanni Cinà. Trust issues: Uncertainty estimation does not enable reliable ood detection on medical tabular data. In Machine Learning for Health, pages 341-354. PMLR, 2020.
[47] Christopher J Urban and Kathleen M Gates. Deep learning: A primer for psychologists. Psychological Methods, 26(6):743, 2021.
[48] Yuxin Wang, Chu-Tak Lee, Qipeng Guo, Zhangyue Yin, Yunhua Zhou, Xuanjing Huang, and Xipeng Qiu. What dense graph do you need for self-attention? In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 22752-22768. PMLR, 17-23 Jul 2022.
[49] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Laprador: Unsupervised pretrained dense retriever for zero-shot text retrieval, 2022.
[50] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences, 2021.
[51] Yuyu Zhang, Ping Nie, Xiubo Geng, Arun Ramamurthy, Le Song, and Daxin Jiang. Dc-bert: Decoupling question and document for efficient contextual encoding, 2020.</p>
<h2>Checklist</h2>
<ol>
<li>For all models and algorithms presented, check if you include:
(a) A clear description of the mathematical setting, assumptions, algorithm, and/or model. [Yes]
(b) An analysis of the properties and complexity (time, space, sample size) of any algorithm. [Yes]
(c) (Optional) Anonymized source code, with specification of all dependencies, including external libraries. [Not Applicable]</li>
<li>For any theoretical claim, check if you include:
(a) Statements of the full set of assumptions of all theoretical results. [Yes]
(b) Complete proofs of all theoretical results. [Yes]
(c) Clear explanations of any assumptions. [Yes]</li>
<li>For all figures and tables that present empirical results, check if you include:
(a) The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL). [Yes]
(b) All the training details (e.g., data splits, hyperparameters, how they were chosen). [Yes]
(c) A clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). [Yes]
(d) A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). [Yes]</li>
<li>If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include:
(a) Citations of the creator If your work uses existing assets. [Yes]
(b) The license information of the assets, if applicable. [Yes]
(c) New assets either in the supplemental material or as a URL, if applicable. [Yes]
(d) Information about consent from data providers/curators. [Yes]
(e) Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. [Not Applicable]</li>
<li>If you used crowdsourcing or conducted research with human subjects, check if you include:</li>
</ol>
<p>(a) The full text of instructions given to participants and screenshots. [Not Applicable]
(b) Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. [Not Applicable]
(c) The estimated hourly wage paid to participants and the total amount spent on participant compensation. [Not Applicable]</p>
<h1>A Additional Theoretical Analysis</h1>
<h2>A. 1 A simplified Scenerio for Dataset Optimization</h2>
<p>We consider a simplified scenario in which we need to update the sampling weights from a uniform distribution.
Lemma A.1. Given a training set $D, N=|D|$ is the number of training samples. We do the non-replacement sampling in $D$ with the sampling size $z$ and uniform sampling weights $w_{1}=\left(p_{1}^{w_{1}}, p_{2}^{w_{1}}, \ldots p_{N}^{w_{1}}\right)$, where each $p_{a}^{w_{1}}=p_{0}=\frac{1}{N}$ for $a \in(1,2, \ldots, N)$. Then with a new set of sampling weights $w_{2}$ where $p_{i}^{w_{2}}=k p_{i}^{w_{1}}$ and $p_{j}^{w_{2}}=\lambda p_{j}^{w_{1}}$ for all $j \in(1,2, \ldots, N)$ and $j \neq i$, where $k<N$ and $\lambda=\frac{1-k p_{0}}{1-p_{0}}$ to maintain normalization, given Assumption 4.1 and for $k>(N-z)\left(1-\lambda_{B}\right)+1$, the expectation of predicting the right label on target $i$ is larger than with $w_{1}$ :</p>
<p>$$
\mathbb{E}\left[q_{\left(\theta, w_{2}\right)}\left(y_{i} \mid x_{i}, D\right)\right]&gt;\mathbb{E}\left[q_{\left(\theta, w_{1}\right)}\left(y_{i} \mid x_{i}, D\right)\right]
$$</p>
<p>where $0&lt;\lambda_{B}&lt;1$ and is defined in Eq. 23.
We put the proof below.
Proof. First we write the expectation in</p>
<p>$$
\mathbb{E}\left[q_{(\theta, w)}\left(y_{i} \mid x_{i}, D\right)\right]=\sum_{D_{w}^{z} \in \mathcal{D}^{z}} p\left(D_{w}^{z}\right) q_{\theta}\left(y_{i} \mid x_{i}, D_{w}^{z}\right)
$$</p>
<p>where $\mathcal{D}^{z}$ is the space of all possibilities of non-replacement sampling with size $z$.
Now we consider the situation that $D^{z}=D_{0} \cup\left(x_{i}, y_{i}\right)$, then there are $(N-z)$ situations that $\bar{D}^{z}=D_{0} \cup\left(x_{j}, y_{j}\right)$ where $\left(x_{j}, y_{j}\right) \notin D^{z}$. So the expectation can be written as</p>
<p>$$
\mathbb{E}\left[q_{(\theta, w)}\left(y_{i} \mid x_{i}, D\right)\right]=\sum_{D^{z}}\left[p\left(D^{z} ; w\right) q_{\theta}\left(y_{i} \mid x_{i}, D^{z}\right)+\sum_{\bar{D}^{z}} p\left(\bar{D}^{z} ; w\right) q_{\theta}\left(y_{i} \mid x_{i}, \bar{D}^{z}\right)\right]
$$</p>
<p>Let $A(w)=p\left(D^{z} ; w\right) q_{\theta}\left(y_{i} \mid x_{i}, D^{z}\right)$ and $B(w)=\sum_{\bar{D}^{z}} p\left(\bar{D}<em _theta="\theta">{w}^{z} ; w\right) q</em>\right)$.
In the non-replacement sampling, if we use $p_{a}$ to denote the $a$ th sample's weight, $A\left(w_{1}\right)$ can be written as}\left(y_{i} \mid x_{i}, \bar{D}^{z}\right)$. To prove Eq. 13, we need to show that $A\left(w_{2}\right)+B\left(w_{2}\right)&gt;A\left(w_{1}\right)+B\left(w_{1</p>
<p>$$
\begin{aligned}
A\left(w_{1}\right) &amp; =\frac{z!\cdot \prod_{a=1}^{z} p_{a}}{\prod_{a=1}^{z}\left(1-\sum_{b=1}^{a-1} p_{b}\right)} q_{\theta}\left(y_{i} \mid x_{i}, D^{z}\right) \
&amp; =\frac{z!}{\binom{N}{z}} q_{\theta}\left(y_{i} \mid x_{i}, D^{z}\right), \text { remembering each weight is } p_{0}=\frac{1}{N} \text { for } w_{1}
\end{aligned}
$$</p>
<p>Then we compute $A\left(w_{2}\right)$, remembering that in $w_{2}, p_{i}^{w_{2}}=k p_{0}$ and for $a \neq i, p_{a}^{w_{2}}=\lambda p_{0}=\frac{1-k p_{0}}{1-p_{0}} p_{0}$,</p>
<p>$$
\begin{aligned}
A\left(w_{2}\right) &amp; =\frac{z!\cdot p_{i} \cdot \prod_{a=1}^{z-1} p_{a}^{w_{2}}}{\prod_{a=1}^{z}\left(1-\sum_{b=1}^{a-1} p_{b}^{w_{2}}\right)} q_{\theta}\left(y_{i} \mid x_{i}, D^{z}\right) \
&amp; =\frac{z!\cdot\left(k \cdot \frac{1}{N}\right) \cdot\left(\lambda \cdot \frac{1}{N}\right)^{z-1}}{\frac{N \cdot(N-k) \cdot(N-k-\lambda) \cdots(N-k-(z-2) \lambda)}{N^{z}}} q_{\theta}\left(y_{i} \mid x_{i}, D^{z}\right) \
&amp; =k \cdot \lambda^{z-1} \cdot \frac{z!\cdot N^{z}}{N \cdot(N-k) \cdot(N-k-\lambda) \cdots(N-k-(z-2) \lambda)} q_{\theta}\left(y_{i} \mid x_{i}, D^{z}\right)
\end{aligned}
$$</p>
<p>So</p>
<p>$$
\begin{aligned}
\frac{A\left(w_{2}\right)}{A\left(w_{1}\right)} &amp; =\frac{k \cdot \lambda^{z-1} \cdot \frac{z!\cdot N^{z}}{N \cdot(N-k) \cdot(N-k-\lambda) \cdots(N-k-(z-2) \lambda)}}{\frac{z!}{\binom{N}{z}}} \
&amp; =k \cdot \lambda^{z-1} \cdot N^{z} \cdot \frac{\binom{N}{z}}{N \cdot(N-k) \cdot(N-k-\lambda) \cdots(N-k-(z-2) \lambda)} \
&amp; =k \cdot \lambda^{z-1} \cdot \frac{N \cdot(N-1) \cdot(N-2) \cdots(N-(z-1))}{N \cdot(N-k) \cdot(N-k-\lambda) \cdots(N-k-(z-2) \lambda)}
\end{aligned}
$$</p>
<p>Note that</p>
<p>$$
\lambda=\frac{1-k p_{0}}{1-p_{0}}=\frac{1-k \frac{1}{N}}{1-\frac{1}{N}}=\frac{N-k}{N-1}
$$</p>
<p>so</p>
<p>$$
\frac{N-1}{N-k}=\frac{N-2}{N-k-\lambda}=\cdots=\frac{N-(z-1)}{N-k-(z-2) \lambda}=\frac{1}{\lambda}
$$</p>
<p>Then</p>
<p>$$
\frac{A\left(w_{2}\right)}{A\left(w_{1}\right)}=k \cdot \lambda^{z-1} \cdot \lambda^{1-z}=k
$$</p>
<p>Using the same process we can compute that</p>
<p>$$
\frac{B\left(w_{2}\right)}{B\left(w_{1}\right)}=\lambda^{z} \frac{\prod_{a=1}^{z}\left(1-\sum_{b=1}^{a-1} p_{0}\right)}{\prod_{a=1}^{z}\left(1-\sum_{b=1}^{a-1} \lambda p_{0}\right)}
$$</p>
<p>We then define</p>
<p>$$
\lambda_{B} \triangleq \lambda^{z} \frac{\prod_{a=1}^{z}\left(1-\sum_{b=1}^{a-1} p_{0}\right)}{\prod_{a=1}^{z}\left(1-\sum_{b=1}^{a-1} \lambda p_{0}\right)}
$$</p>
<p>With $0&lt;\lambda&lt;1$ we know $0&lt;\lambda_{B}&lt;1$. So</p>
<p>$$
\begin{aligned}
A\left(w_{2}\right)+B\left(w_{2}\right)&gt;A\left(w_{1}\right)+B\left(w_{1}\right) &amp; \Longleftrightarrow k A\left(w_{1}\right)+\lambda_{B} B\left(w_{1}\right)&gt;A\left(w_{1}\right)+B\left(w_{1}\right) \
&amp; \Longleftrightarrow(k-1) A\left(w_{1}\right)&gt;\left(1-\lambda_{B}\right) B\left(w_{1}\right) \
&amp; \Longleftrightarrow \frac{A\left(w_{1}\right)}{B\left(w_{1}\right)}&gt;\frac{1-\lambda_{B}}{k-1}
\end{aligned}
$$</p>
<p>Remembering the definition of $A\left(w_{1}\right)$ and $B\left(w_{1}\right)$, we know that</p>
<p>$$
\frac{A\left(w_{1}\right)}{B\left(w_{1}\right)}=\frac{q_{\theta}\left(y_{i} \mid x_{i}, D^{z}\right)}{(N-z) q_{\theta}\left(y_{i} \mid x_{i}, \bar{D}^{z}\right)}&gt;\frac{1}{(N-z)}, \quad \text { Given Assumption 4.1. }
$$</p>
<p>So Eq. 24 holds if</p>
<p>$$
\frac{1}{(N-z)}&gt;\frac{1-\lambda_{B}}{k-1}
$$</p>
<p>This is true because $k&gt;(N-z)\left(1-\lambda_{B}\right)+1$ is one of our condition. Then we complete the proof of Lemma A.1.</p>
<h1>A. 2 Proof for Theorem 5.1</h1>
<p>We begin the proof by the first proving that BoostPFN is a kind of Randomized Gradient Boosting Machine. Remark A.2. BoostPFN showed in Algorithm 1 is a kind of Randomized Gradient Boosting Machine.</p>
<p>Proof. Randomized Gradient Boosting Machine can be written as
from [31]. $b\left(x_{i} ; \tau_{j_{m}}\right)$ is the weak learner in round $m$. Type 0 to Type 3 are as follows :
[Type 0]: (Full Deterministic Selection) We choose $J$ as the whole set of weak-learners. This is a deterministic selection rule.
[Type 1]: (Random Selection) We choose uniformly at random $t$ weak-learners from all possible weak-learners without replacement - the collection is denoted by $J$.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="p">:</span><span class="w"> </span><span class="n">Randomized</span><span class="w"> </span><span class="n">Gradient</span><span class="w"> </span><span class="n">Boosting</span><span class="w"> </span><span class="n">Machine</span><span class="w"> </span><span class="p">(</span><span class="n">RGBM</span><span class="p">)</span>
<span class="w">    </span><span class="n">Initialization</span><span class="o">.</span><span class="w"> </span><span class="n">Initialize</span><span class="w"> </span><span class="n">with</span><span class="w"> </span>\<span class="p">(</span><span class="n">f</span><span class="o">^</span><span class="p">{</span><span class="mi">0</span><span class="p">}(</span><span class="n">x</span><span class="p">)</span><span class="o">=</span><span class="mi">0</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">    </span><span class="n">For</span><span class="w"> </span>\<span class="p">(</span><span class="n">m</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="o">-</span><span class="mi">1</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span><span class="p">:</span>
<span class="w">    </span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="n">Compute</span><span class="w"> </span><span class="n">pseudo</span><span class="o">-</span><span class="n">residual</span><span class="w"> </span>\<span class="p">(</span><span class="n">r</span><span class="o">^</span><span class="p">{</span><span class="n">m</span><span class="p">}</span><span class="o">=-</span>\<span class="n">left</span><span class="p">[</span>\<span class="n">frac</span><span class="p">{</span>\<span class="n">partial</span><span class="w"> </span>\<span class="n">ell</span>\<span class="n">left</span><span class="p">(</span><span class="n">y_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span><span class="w"> </span><span class="n">f</span><span class="o">^</span><span class="p">{</span><span class="n">m</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="n">right</span><span class="p">)}{</span>\<span class="n">partial</span><span class="w"> </span><span class="n">f</span><span class="o">^</span><span class="p">{</span><span class="n">m</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)}</span>\<span class="n">right</span><span class="p">]</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">    </span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="n">Pick</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">random</span><span class="w"> </span><span class="n">subset</span><span class="w"> </span>\<span class="p">(</span><span class="n">J</span>\<span class="p">)</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">weak</span><span class="o">-</span><span class="n">learners</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">some</span><span class="w"> </span><span class="n">rule</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">e</span><span class="o">.</span><span class="p">,</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">Type</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Type</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span>
<span class="w">    </span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="n">Find</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">best</span><span class="w"> </span><span class="n">weak</span><span class="o">-</span><span class="n">learner</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span>\<span class="p">(</span><span class="n">J</span><span class="p">:</span><span class="w"> </span><span class="n">j_</span><span class="p">{</span><span class="n">m</span><span class="p">}</span><span class="o">=</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">argmin</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">j</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span><span class="n">J</span><span class="p">}</span><span class="w"> </span>\<span class="nb">min</span><span class="w"> </span><span class="n">_</span><span class="p">{</span>\<span class="n">sigma</span><span class="p">}</span><span class="w"> </span>\<span class="n">sum_</span><span class="p">{</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">n</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">r_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">m</span><span class="p">}</span><span class="o">-</span>\<span class="n">sigma</span><span class="w"> </span><span class="n">b</span>\<span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span>\<span class="n">tau_</span><span class="p">{</span><span class="n">j</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="n">right</span><span class="p">)</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">    </span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="w"> </span><span class="n">Choose</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">step</span><span class="o">-</span><span class="n">size</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">rho_</span><span class="p">{</span><span class="n">m</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">rules</span><span class="p">:</span>
<span class="w">        </span><span class="o">-</span><span class="w"> </span><span class="n">line</span><span class="o">-</span><span class="n">search</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">rho_</span><span class="p">{</span><span class="n">m</span><span class="p">}</span><span class="o">=</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">argmin</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">rho</span><span class="p">}</span><span class="w"> </span>\<span class="n">sum_</span><span class="p">{</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">n</span><span class="p">}</span><span class="w"> </span>\<span class="n">ell</span>\<span class="n">left</span><span class="p">(</span><span class="n">y_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span><span class="w"> </span><span class="n">f</span><span class="o">^</span><span class="p">{</span><span class="n">m</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="o">+</span>\<span class="n">rho</span><span class="w"> </span><span class="n">b</span>\<span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span>\<span class="n">tau_</span><span class="p">{</span><span class="n">j_</span><span class="p">{</span><span class="n">m</span><span class="p">}}</span>\<span class="n">right</span><span class="p">)</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">);</span>
<span class="w">        </span><span class="o">-</span><span class="w"> </span><span class="n">constant</span><span class="w"> </span><span class="n">step</span><span class="o">-</span><span class="n">size</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">rho_</span><span class="p">{</span><span class="n">m</span><span class="p">}</span><span class="o">=</span>\<span class="n">rho</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">sum_</span><span class="p">{</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">n</span><span class="p">}</span><span class="w"> </span><span class="n">r_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">m</span><span class="p">}</span><span class="w"> </span><span class="n">b</span>\<span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span>\<span class="n">tau_</span><span class="p">{</span><span class="n">j_</span><span class="p">{</span><span class="n">m</span><span class="p">}}</span>\<span class="n">right</span><span class="p">)</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">),</span><span class="w"> </span><span class="n">where</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">rho</span>\<span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">constant</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">priori</span><span class="o">.</span>
<span class="w">    </span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="w"> </span><span class="n">Update</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="w"> </span>\<span class="p">(</span><span class="n">f</span><span class="o">^</span><span class="p">{</span><span class="n">m</span><span class="o">+</span><span class="mi">1</span><span class="p">}(</span><span class="n">x</span><span class="p">)</span><span class="o">=</span><span class="n">f</span><span class="o">^</span><span class="p">{</span><span class="n">m</span><span class="p">}(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span>\<span class="n">rho_</span><span class="p">{</span><span class="n">m</span><span class="p">}</span><span class="w"> </span><span class="n">b</span>\<span class="n">left</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="p">;</span><span class="w"> </span>\<span class="n">tau_</span><span class="p">{</span><span class="n">j_</span><span class="p">{</span><span class="n">m</span><span class="p">}}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">    </span><span class="n">Output</span><span class="o">.</span><span class="w"> </span>\<span class="p">(</span><span class="n">f</span><span class="o">^</span><span class="p">{</span><span class="n">M</span><span class="p">}(</span><span class="n">x</span><span class="p">)</span>\<span class="p">)</span><span class="o">.</span>
</code></pre></div>

<p>[Type 2]: (Random Single Group Selection) Given a non-overlapping partition of the weak-learners, we pick one group uniformly at random and denote the collection of weak-learners in that group by $J$.
[Type 3]: (Random Multiple Group Selection) Given a non-overlapping partition of the weak-learners, we pick $t$ groups uniformly at random and let the collection of weak-learners across these groups be $J$.</p>
<p>When we compare RGBM with BoostPFN, it's clear that if we choose line-search in step (4) of RGBM, the difference is in step (2) and (3). Here we show that the weights updating is actually picking random subset of weak learner.</p>
<p>We write the subset of TabPFN as $J=\left{q_{\theta}\left(\cdot \mid x, D_{w}^{s}\right) \mid D_{w}\right.$ generated by $\left.w\right}$. In BoostPFN algorithm, we sample a weak learner from the subset instead of choosing the best one. So we need to modify the algorithm a bit here. We introduce the union of subset with different sampling weights $J_{U}=\left\lfloor j_{i=0,1,2} J_{i}=\left{q_{\theta}\left(\cdot \mid x, D_{w_{i}}^{s}\right) \mid D_{w_{i}}^{s}\right.\right.$ generated by $\left.w_{i}\right}$ where $w_{0}$ is uniform sampling weights, $w_{1}$ is the sampling weights for this round of boost, $w_{2}$ is the sampling weights for last round. And we choose the best weak learner in this subset. The subset is picked via Type 3. Thus BoostPFN with this modification is the same as RGBM ${ }^{2}$.</p>
<p>Then the Theorem 4.2 in [31] that the RGBM in Algorithm 2 if $\ell$ is a $\sigma$-smooth function and has a bounded level set will converge with the rate of $O\left(\frac{\sigma}{M}\right)$ will lead straightforwardly to the proof of Eq. 12.</p>
<h1>B Experimental Implementation and Hyperparameters</h1>
<p>We do all experiments on a platform with 48 CPU cores and RTX 3090. For ensemble models on small datasets, we use sampling size 0.5 of the training set if training set contains less than 1000 samples. The hyperparameter spaces for LightGBM, CatBoost and XGBoost follow [21] and are shown in Table 7.</p>
<p>For AutoGluon we use "best quality". For FT-Transformer we tuning learning rate in [5e-5,1e-4,5e-4,1e-3], batch size in [128, 256, 512, 1024]. For SAINT we use default setting because it costs too much time to tuning.</p>
<p>For tuning methods, time limitation is 6000 seconds per million samples.</p>
<h2>C Datasets Statistics</h2>
<p>We show the dataset statistics for small datasets in Table 8, cited from [21]. Large datasets statistics are shown in Table 9.</p>
<h2>D Full Experiment Results</h2>
<p>We show the per dataset results on small datasets with a 1 hour time limit in Table 10. The large datasets results with different number of training samples are showed in Table 11,12 and 13 .</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 7: Hyperparameter spaces for baselines.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">baseline</th>
<th style="text-align: center;">name</th>
<th style="text-align: center;">type</th>
<th style="text-align: center;">$\log$</th>
<th style="text-align: center;">range</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">num_leaves</td>
<td style="text-align: center;">int</td>
<td style="text-align: center;">$[5,50]$</td>
<td style="text-align: center;">yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">max_depth</td>
<td style="text-align: center;">int</td>
<td style="text-align: center;">$[3,20]$</td>
<td style="text-align: center;">yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">learning_rate</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$\left[e^{-3}, 1\right]$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">n_estimators</td>
<td style="text-align: center;">int</td>
<td style="text-align: center;">50,2000</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">min_child_weight</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$\left[e^{-5}, e^{4}\right]$</td>
<td style="text-align: center;">yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">reg_alpha</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$\left[0,1 \mathrm{e}-1,1,2,5,7,10,50,100\right]$</td>
<td style="text-align: center;">yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">reg_lambda</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$\left[0,1 \mathrm{e}-1,1,5,10,20,50,100\right]$</td>
<td style="text-align: center;">yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">subsample</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$[0.2,0.8]$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">CatBoost</td>
<td style="text-align: center;">learning_rate</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$\left[e^{-5}, 1\right]$</td>
<td style="text-align: center;">yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">random_strength</td>
<td style="text-align: center;">int</td>
<td style="text-align: center;">$[1,20]$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">l2_leaf_reg</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$[1,10]$</td>
<td style="text-align: center;">yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">bagging_temperature</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$[0,1.0]$</td>
<td style="text-align: center;">yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">leaf_estimation_iterations</td>
<td style="text-align: center;">int</td>
<td style="text-align: center;">$[1,20]$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">iterations</td>
<td style="text-align: center;">int</td>
<td style="text-align: center;">$[100,4000]$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">learning_rate</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$\left[e^{-7}, 1\right]$</td>
<td style="text-align: center;">yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">max_depth</td>
<td style="text-align: center;">int</td>
<td style="text-align: center;">$[1,10]$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">subsample</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$[0.2,1]$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">colsample_ bytree</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$[0.2,1]$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">colsample_ bylevel</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$\left[0.2,1\right]$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">min_child_weight</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$\left[e^{-16}, e^{5}\right]$</td>
<td style="text-align: center;">yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">alpha</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$\left[e^{-16}, e^{2}\right]$</td>
<td style="text-align: center;">yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">lambda</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$\left[e^{-16}, e^{2}\right]$</td>
<td style="text-align: center;">yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">gamma</td>
<td style="text-align: center;">float</td>
<td style="text-align: center;">$\left[e^{-16}, e^{2}\right]$</td>
<td style="text-align: center;">yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">n_estimators</td>
<td style="text-align: center;">int</td>
<td style="text-align: center;">$[100,4000]$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 8: Small datasets used for the evaluation. All 30 datasets are at most 2000 samples, 100 features and 10 classes.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">$#$ Feat.</th>
<th style="text-align: center;">$#$ Cat.</th>
<th style="text-align: center;">$#$ Inst.</th>
<th style="text-align: center;">$#$ Class.</th>
<th style="text-align: center;">$#$ NaNs</th>
<th style="text-align: center;">Minor.</th>
<th style="text-align: center;">Class Size</th>
<th style="text-align: center;">OpenML ID</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">balance-scale</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">625</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">mfeat-fourier</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;">breast-w</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">699</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">241</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: center;">mfeat-karhunen</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">mfeat-morphological</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">18</td>
</tr>
<tr>
<td style="text-align: center;">mfeat-zernike</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">22</td>
</tr>
<tr>
<td style="text-align: center;">cmc</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1473</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">333</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23</td>
</tr>
<tr>
<td style="text-align: center;">credit-approval</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">690</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">307</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">29</td>
</tr>
<tr>
<td style="text-align: center;">credit-g</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">31</td>
</tr>
<tr>
<td style="text-align: center;">diabetes</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">268</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">37</td>
</tr>
<tr>
<td style="text-align: center;">tin-tac-toe</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">958</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">332</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">vehicle</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">846</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">199</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">54</td>
</tr>
<tr>
<td style="text-align: center;">eucalyptus</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">736</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">448</td>
<td style="text-align: center;">105</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">188</td>
</tr>
<tr>
<td style="text-align: center;">analcatdata_auth...</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">841</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">458</td>
</tr>
<tr>
<td style="text-align: center;">analcatdata_dmft</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">797</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">469</td>
</tr>
<tr>
<td style="text-align: center;">pc4</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1458</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">178</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1049</td>
</tr>
<tr>
<td style="text-align: center;">pc3</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1563</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">160</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1050</td>
</tr>
<tr>
<td style="text-align: center;">kc2</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">522</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">107</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1063</td>
</tr>
<tr>
<td style="text-align: center;">pc1</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1109</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1068</td>
</tr>
<tr>
<td style="text-align: center;">banknote-authenti...</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1372</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">610</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1462</td>
</tr>
<tr>
<td style="text-align: center;">blood-transfusion-...</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">178</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1464</td>
</tr>
<tr>
<td style="text-align: center;">ilpd</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">583</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">167</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1480</td>
</tr>
<tr>
<td style="text-align: center;">quar-biodeg</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1055</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">356</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1494</td>
</tr>
<tr>
<td style="text-align: center;">wdtc</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">569</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">212</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1510</td>
</tr>
<tr>
<td style="text-align: center;">cylinder-bands</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">540</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">999</td>
<td style="text-align: center;">228</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6332</td>
</tr>
<tr>
<td style="text-align: center;">dresses-sales</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">835</td>
<td style="text-align: center;">210</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23381</td>
</tr>
<tr>
<td style="text-align: center;">MiceProtein</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1080</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1396</td>
<td style="text-align: center;">105</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40966</td>
</tr>
<tr>
<td style="text-align: center;">car</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1728</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40975</td>
</tr>
<tr>
<td style="text-align: center;">steel-plates-fault</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1941</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40982</td>
</tr>
<tr>
<td style="text-align: center;">climate-model-simu...</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">540</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40994</td>
</tr>
</tbody>
</table>
<p>Table 9: Large datasets used for the evaluation. All 30 datasets are at most 10 million samples, 100 features and 144 classes.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">$#$ Feat.</th>
<th style="text-align: center;">$#$ Cat.</th>
<th style="text-align: center;">$#$ Inst.</th>
<th style="text-align: center;">$#$ Class.</th>
<th style="text-align: center;">$#$ NaNs</th>
<th style="text-align: center;">Minor. Class Size</th>
<th style="text-align: center;">OpenML ID</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BNG(page-blocks,nominal,295245)</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">295245</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1558</td>
<td style="text-align: center;">125</td>
</tr>
<tr>
<td style="text-align: center;">BNG(glass,nominal,137781)</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">137781</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">307</td>
<td style="text-align: center;">133</td>
</tr>
<tr>
<td style="text-align: center;">BNG(heart-c,nominal,1000000)</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1618</td>
<td style="text-align: center;">136</td>
</tr>
<tr>
<td style="text-align: center;">BNG(heart-h,nominal,1000000)</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1659</td>
<td style="text-align: center;">138</td>
</tr>
<tr>
<td style="text-align: center;">BNG(waveform-5000,nominal,1000000)</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">330548</td>
<td style="text-align: center;">147</td>
</tr>
<tr>
<td style="text-align: center;">pokerhand</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">829201</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">155</td>
</tr>
<tr>
<td style="text-align: center;">RandomRBF_0_0</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">92713</td>
<td style="text-align: center;">156</td>
</tr>
<tr>
<td style="text-align: center;">RandomRBF_10_1E-3</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">92713</td>
<td style="text-align: center;">157</td>
</tr>
<tr>
<td style="text-align: center;">RandomRBF_10_1E-4</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">92713</td>
<td style="text-align: center;">158</td>
</tr>
<tr>
<td style="text-align: center;">SEA(50)</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">385658</td>
<td style="text-align: center;">161</td>
</tr>
<tr>
<td style="text-align: center;">SEA(50000)</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">385668</td>
<td style="text-align: center;">162</td>
</tr>
<tr>
<td style="text-align: center;">BNG(heart-c)</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1609</td>
<td style="text-align: center;">266</td>
</tr>
<tr>
<td style="text-align: center;">BNG(primary-tumor)</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1417</td>
<td style="text-align: center;">1177</td>
</tr>
<tr>
<td style="text-align: center;">BNG(solar-flare)</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1393</td>
<td style="text-align: center;">1179</td>
</tr>
<tr>
<td style="text-align: center;">Stagger1</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">111609</td>
<td style="text-align: center;">1236</td>
</tr>
<tr>
<td style="text-align: center;">Stagger2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">444057</td>
<td style="text-align: center;">1237</td>
</tr>
<tr>
<td style="text-align: center;">Stagger3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">333571</td>
<td style="text-align: center;">1238</td>
</tr>
<tr>
<td style="text-align: center;">AirlinesCodrnaAdult</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">1076790</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">7275</td>
<td style="text-align: center;">473652</td>
<td style="text-align: center;">1240</td>
</tr>
<tr>
<td style="text-align: center;">skin-segmentation</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">245057</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">50859</td>
<td style="text-align: center;">1502</td>
</tr>
<tr>
<td style="text-align: center;">creditcard</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">284807</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">492</td>
<td style="text-align: center;">1597</td>
</tr>
<tr>
<td style="text-align: center;">BNG(spambase)</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">394052</td>
<td style="text-align: center;">40515</td>
</tr>
<tr>
<td style="text-align: center;">BNG(anneal)</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">555</td>
<td style="text-align: center;">40520</td>
</tr>
<tr>
<td style="text-align: center;">fars</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">100968</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">40672</td>
</tr>
<tr>
<td style="text-align: center;">seattlecrime6</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">523590</td>
<td style="text-align: center;">144</td>
<td style="text-align: center;">6916</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">41960</td>
</tr>
<tr>
<td style="text-align: center;">porto-seguro</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">595212</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">846458</td>
<td style="text-align: center;">21694</td>
<td style="text-align: center;">42206</td>
</tr>
<tr>
<td style="text-align: center;">CreditCardFraudDetection</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">284807</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">492</td>
<td style="text-align: center;">42397</td>
</tr>
<tr>
<td style="text-align: center;">KDDCup99</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">4898431</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">42746</td>
</tr>
<tr>
<td style="text-align: center;">baixo_classif_20</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5100000</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2549577</td>
<td style="text-align: center;">45654</td>
</tr>
<tr>
<td style="text-align: center;">colon</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5100000</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2549437</td>
<td style="text-align: center;">45665</td>
</tr>
<tr>
<td style="text-align: center;">breast</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5100000</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2549502</td>
<td style="text-align: center;">45669</td>
</tr>
</tbody>
</table>
<p>Table 10: Per dataset results on small datasets lower than 5000 training samples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">LightGBM</th>
<th style="text-align: center;">CatBoost</th>
<th style="text-align: center;">XGBoost</th>
<th style="text-align: center;">AutoGluon</th>
<th style="text-align: center;">FT-Trans.</th>
<th style="text-align: center;">SAINT</th>
<th style="text-align: center;">TabPFN</th>
<th style="text-align: center;">Bagging</th>
<th style="text-align: center;">BoostPFN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">balance-scale</td>
<td style="text-align: center;">0.9938</td>
<td style="text-align: center;">0.9245</td>
<td style="text-align: center;">0.9939</td>
<td style="text-align: center;">0.9919</td>
<td style="text-align: center;">0.9935</td>
<td style="text-align: center;">0.86366</td>
<td style="text-align: center;">0.9973</td>
<td style="text-align: center;">0.9985</td>
<td style="text-align: center;">0.9996</td>
</tr>
<tr>
<td style="text-align: left;">mfeat-fourier</td>
<td style="text-align: center;">0.9786</td>
<td style="text-align: center;">0.9816</td>
<td style="text-align: center;">0.9803</td>
<td style="text-align: center;">0.9843</td>
<td style="text-align: center;">0.9782</td>
<td style="text-align: center;">0.978938</td>
<td style="text-align: center;">0.9811</td>
<td style="text-align: center;">0.9761</td>
<td style="text-align: center;">0.9769</td>
</tr>
<tr>
<td style="text-align: left;">breast-w</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.9931</td>
<td style="text-align: center;">0.9896</td>
<td style="text-align: center;">0.9933</td>
<td style="text-align: center;">0.9846</td>
<td style="text-align: center;">0.987477</td>
<td style="text-align: center;">0.9934</td>
<td style="text-align: center;">0.9922</td>
<td style="text-align: center;">0.9921</td>
</tr>
<tr>
<td style="text-align: left;">mfeat-karhunen</td>
<td style="text-align: center;">0.9979</td>
<td style="text-align: center;">0.9986</td>
<td style="text-align: center;">0.9983</td>
<td style="text-align: center;">0.9987</td>
<td style="text-align: center;">0.9961</td>
<td style="text-align: center;">0.998036</td>
<td style="text-align: center;">0.9978</td>
<td style="text-align: center;">0.9981</td>
<td style="text-align: center;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">mfeat-morphologica..</td>
<td style="text-align: center;">0.9601</td>
<td style="text-align: center;">0.9629</td>
<td style="text-align: center;">0.9612</td>
<td style="text-align: center;">0.9698</td>
<td style="text-align: center;">0.9665</td>
<td style="text-align: center;">0.95501</td>
<td style="text-align: center;">0.9669</td>
<td style="text-align: center;">0.9669</td>
<td style="text-align: center;">0.9664</td>
</tr>
<tr>
<td style="text-align: left;">mfeat-zernike</td>
<td style="text-align: center;">0.9716</td>
<td style="text-align: center;">0.9759</td>
<td style="text-align: center;">0.9735</td>
<td style="text-align: center;">0.9908</td>
<td style="text-align: center;">0.9808</td>
<td style="text-align: center;">0.973376</td>
<td style="text-align: center;">0.9823</td>
<td style="text-align: center;">0.9834</td>
<td style="text-align: center;">0.9833</td>
</tr>
<tr>
<td style="text-align: left;">cmc</td>
<td style="text-align: center;">0.7288</td>
<td style="text-align: center;">0.7256</td>
<td style="text-align: center;">0.7299</td>
<td style="text-align: center;">0.7331</td>
<td style="text-align: center;">0.7073</td>
<td style="text-align: center;">0.699643</td>
<td style="text-align: center;">0.7276</td>
<td style="text-align: center;">0.7173</td>
<td style="text-align: center;">0.7205</td>
</tr>
<tr>
<td style="text-align: left;">credit-approval</td>
<td style="text-align: center;">0.9415</td>
<td style="text-align: center;">0.9389</td>
<td style="text-align: center;">0.9422</td>
<td style="text-align: center;">0.9415</td>
<td style="text-align: center;">0.9175</td>
<td style="text-align: center;">0.933137</td>
<td style="text-align: center;">0.9322</td>
<td style="text-align: center;">0.9424</td>
<td style="text-align: center;">0.9427</td>
</tr>
<tr>
<td style="text-align: left;">credit-g</td>
<td style="text-align: center;">0.7684</td>
<td style="text-align: center;">0.7852</td>
<td style="text-align: center;">0.7853</td>
<td style="text-align: center;">0.7941</td>
<td style="text-align: center;">0.7644</td>
<td style="text-align: center;">0.602838</td>
<td style="text-align: center;">0.7894</td>
<td style="text-align: center;">0.7916</td>
<td style="text-align: center;">0.7918</td>
</tr>
<tr>
<td style="text-align: left;">diabetes</td>
<td style="text-align: center;">0.8247</td>
<td style="text-align: center;">0.8383</td>
<td style="text-align: center;">0.8378</td>
<td style="text-align: center;">0.8391</td>
<td style="text-align: center;">0.8475</td>
<td style="text-align: center;">0.650567</td>
<td style="text-align: center;">0.841</td>
<td style="text-align: center;">0.8251</td>
<td style="text-align: center;">0.8239</td>
</tr>
<tr>
<td style="text-align: left;">tic-tac-toe</td>
<td style="text-align: center;">0.9988</td>
<td style="text-align: center;">0.9992</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.9935</td>
<td style="text-align: center;">0.562474</td>
<td style="text-align: center;">0.9759</td>
<td style="text-align: center;">0.9437</td>
<td style="text-align: center;">0.9767</td>
</tr>
<tr>
<td style="text-align: left;">vehicle</td>
<td style="text-align: center;">0.9232</td>
<td style="text-align: center;">0.9302</td>
<td style="text-align: center;">0.9282</td>
<td style="text-align: center;">0.9416</td>
<td style="text-align: center;">0.9357</td>
<td style="text-align: center;">0.923496</td>
<td style="text-align: center;">0.9589</td>
<td style="text-align: center;">0.9563</td>
<td style="text-align: center;">0.9597</td>
</tr>
<tr>
<td style="text-align: left;">eucalyptus</td>
<td style="text-align: center;">0.8931</td>
<td style="text-align: center;">0.8979</td>
<td style="text-align: center;">0.9004</td>
<td style="text-align: center;">0.9204</td>
<td style="text-align: center;">0.8961</td>
<td style="text-align: center;">0.851701</td>
<td style="text-align: center;">0.9245</td>
<td style="text-align: center;">0.9196</td>
<td style="text-align: center;">0.9227</td>
</tr>
<tr>
<td style="text-align: left;">analcatdata_author..</td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: center;">0.9997</td>
<td style="text-align: center;">0.9993</td>
<td style="text-align: center;">0.9972</td>
<td style="text-align: center;">0.999019</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">analcatdata_dmft</td>
<td style="text-align: center;">0.5461</td>
<td style="text-align: center;">0.5589</td>
<td style="text-align: center;">0.5743</td>
<td style="text-align: center;">0.5657</td>
<td style="text-align: center;">0.5489</td>
<td style="text-align: center;">0.551028</td>
<td style="text-align: center;">0.579</td>
<td style="text-align: center;">0.5811</td>
<td style="text-align: center;">0.5806</td>
</tr>
<tr>
<td style="text-align: left;">pc4</td>
<td style="text-align: center;">0.9301</td>
<td style="text-align: center;">0.9413</td>
<td style="text-align: center;">0.9291</td>
<td style="text-align: center;">0.9428</td>
<td style="text-align: center;">0.9254</td>
<td style="text-align: center;">0.914273</td>
<td style="text-align: center;">0.9383</td>
<td style="text-align: center;">0.9207</td>
<td style="text-align: center;">0.9247</td>
</tr>
<tr>
<td style="text-align: left;">pc3</td>
<td style="text-align: center;">0.8178</td>
<td style="text-align: center;">0.8247</td>
<td style="text-align: center;">0.8288</td>
<td style="text-align: center;">0.8282</td>
<td style="text-align: center;">0.7911</td>
<td style="text-align: center;">0.808226</td>
<td style="text-align: center;">0.8373</td>
<td style="text-align: center;">0.8513</td>
<td style="text-align: center;">0.8518</td>
</tr>
<tr>
<td style="text-align: left;">kc2</td>
<td style="text-align: center;">0.8141</td>
<td style="text-align: center;">0.8323</td>
<td style="text-align: center;">0.8227</td>
<td style="text-align: center;">0.8242</td>
<td style="text-align: center;">0.8059</td>
<td style="text-align: center;">0.846287</td>
<td style="text-align: center;">0.8346</td>
<td style="text-align: center;">0.8705</td>
<td style="text-align: center;">0.8698</td>
</tr>
<tr>
<td style="text-align: left;">pc1</td>
<td style="text-align: center;">0.8321</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.8489</td>
<td style="text-align: center;">0.8578</td>
<td style="text-align: center;">0.7207</td>
<td style="text-align: center;">0.805705</td>
<td style="text-align: center;">0.8761</td>
<td style="text-align: center;">0.8936</td>
<td style="text-align: center;">0.8876</td>
</tr>
<tr>
<td style="text-align: left;">banknote-authentic..</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.9927</td>
<td style="text-align: center;">0.992147</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">blood-transfusion-..</td>
<td style="text-align: center;">0.7144</td>
<td style="text-align: center;">0.7403</td>
<td style="text-align: center;">0.7312</td>
<td style="text-align: center;">0.7364</td>
<td style="text-align: center;">0.7803</td>
<td style="text-align: center;">0.928475</td>
<td style="text-align: center;">0.7549</td>
<td style="text-align: center;">0.7747</td>
<td style="text-align: center;">0.7687</td>
</tr>
<tr>
<td style="text-align: left;">ilpd</td>
<td style="text-align: center;">0.6917</td>
<td style="text-align: center;">0.7279</td>
<td style="text-align: center;">0.7171</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.7044</td>
<td style="text-align: center;">0.619734</td>
<td style="text-align: center;">0.7379</td>
<td style="text-align: center;">0.7523</td>
<td style="text-align: center;">0.7527</td>
</tr>
<tr>
<td style="text-align: left;">qsar-biodeg</td>
<td style="text-align: center;">0.9126</td>
<td style="text-align: center;">0.9217</td>
<td style="text-align: center;">0.9191</td>
<td style="text-align: center;">0.9276</td>
<td style="text-align: center;">0.9202</td>
<td style="text-align: center;">0.999062</td>
<td style="text-align: center;">0.9336</td>
<td style="text-align: center;">0.9282</td>
<td style="text-align: center;">0.9288</td>
</tr>
<tr>
<td style="text-align: left;">wdbc</td>
<td style="text-align: center;">0.9904</td>
<td style="text-align: center;">0.9931</td>
<td style="text-align: center;">0.9904</td>
<td style="text-align: center;">0.9956</td>
<td style="text-align: center;">0.986</td>
<td style="text-align: center;">0.709422</td>
<td style="text-align: center;">0.9964</td>
<td style="text-align: center;">0.9985</td>
<td style="text-align: center;">0.9988</td>
</tr>
<tr>
<td style="text-align: left;">cylinder-bands</td>
<td style="text-align: center;">0.8556</td>
<td style="text-align: center;">0.8757</td>
<td style="text-align: center;">0.8782</td>
<td style="text-align: center;">0.8878</td>
<td style="text-align: center;">0.8038</td>
<td style="text-align: center;">0.790092</td>
<td style="text-align: center;">0.8336</td>
<td style="text-align: center;">0.7802</td>
<td style="text-align: center;">0.7969</td>
</tr>
<tr>
<td style="text-align: left;">dresses-sales</td>
<td style="text-align: center;">0.5593</td>
<td style="text-align: center;">0.5696</td>
<td style="text-align: center;">0.5823</td>
<td style="text-align: center;">0.5507</td>
<td style="text-align: center;">0.5056</td>
<td style="text-align: center;">0.578095</td>
<td style="text-align: center;">0.5376</td>
<td style="text-align: center;">0.5559</td>
<td style="text-align: center;">0.5532</td>
</tr>
<tr>
<td style="text-align: left;">MiceProtein</td>
<td style="text-align: center;">0.9997</td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: center;">0.9998</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.9992</td>
<td style="text-align: center;">0.999702</td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: center;">0.9998</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">car</td>
<td style="text-align: center;">0.9925</td>
<td style="text-align: center;">0.9955</td>
<td style="text-align: center;">0.9948</td>
<td style="text-align: center;">0.997</td>
<td style="text-align: center;">0.9849</td>
<td style="text-align: center;">0.953275</td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">0.9902</td>
<td style="text-align: center;">0.9965</td>
</tr>
<tr>
<td style="text-align: left;">steel-plates-fault..</td>
<td style="text-align: center;">0.9626</td>
<td style="text-align: center;">0.9655</td>
<td style="text-align: center;">0.9656</td>
<td style="text-align: center;">0.9666</td>
<td style="text-align: center;">0.9532</td>
<td style="text-align: center;">0.59391</td>
<td style="text-align: center;">0.9655</td>
<td style="text-align: center;">0.9595</td>
<td style="text-align: center;">0.9587</td>
</tr>
<tr>
<td style="text-align: left;">climate-model-simu..</td>
<td style="text-align: center;">0.9286</td>
<td style="text-align: center;">0.9344</td>
<td style="text-align: center;">0.9255</td>
<td style="text-align: center;">0.9391</td>
<td style="text-align: center;">0.8719</td>
<td style="text-align: center;">0.666173</td>
<td style="text-align: center;">0.9415</td>
<td style="text-align: center;">0.926</td>
<td style="text-align: center;">0.9298</td>
</tr>
<tr>
<td style="text-align: left;">Mean AUC OVO</td>
<td style="text-align: center;">$0.884 \pm .012$</td>
<td style="text-align: center;">$0.89 \pm .011$</td>
<td style="text-align: center;">$0.891 \pm .011$</td>
<td style="text-align: center;">$0.895 \pm .011$</td>
<td style="text-align: center;">$0.875 \pm .010$</td>
<td style="text-align: center;">$0.824 \pm .011$</td>
<td style="text-align: center;">$0.894 \pm .010$</td>
<td style="text-align: center;">$0.893 \pm .010$</td>
<td style="text-align: center;">$0.895 \pm .009$</td>
</tr>
</tbody>
</table>
<p>Table 11: Per dataset results on large datasets with 5,000 training samples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">LightGBM</th>
<th style="text-align: center;">CatBoost</th>
<th style="text-align: center;">XGBoost</th>
<th style="text-align: center;">AutoGbion</th>
<th style="text-align: center;">FT-Trans.</th>
<th style="text-align: center;">SAINT</th>
<th style="text-align: center;">TabPFN</th>
<th style="text-align: center;">Bagging</th>
<th style="text-align: center;">BoostPFN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BNG(page-blocks,nominal,295245)</td>
<td style="text-align: center;">0.8401</td>
<td style="text-align: center;">0.8385</td>
<td style="text-align: center;">0.8296</td>
<td style="text-align: center;">0.7812</td>
<td style="text-align: center;">0.8478</td>
<td style="text-align: center;">0.7531</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.7851</td>
<td style="text-align: center;">0.8473</td>
</tr>
<tr>
<td style="text-align: left;">BNG(glass,nominal,137781)</td>
<td style="text-align: center;">0.9011</td>
<td style="text-align: center;">0.8979</td>
<td style="text-align: center;">0.8994</td>
<td style="text-align: center;">0.8322</td>
<td style="text-align: center;">0.9013</td>
<td style="text-align: center;">0.9007</td>
<td style="text-align: center;">0.8907</td>
<td style="text-align: center;">0.8395</td>
<td style="text-align: center;">0.8824</td>
</tr>
<tr>
<td style="text-align: left;">BNG(heart-c,nominal,1000000)</td>
<td style="text-align: center;">0.7664</td>
<td style="text-align: center;">0.7779</td>
<td style="text-align: center;">0.7825</td>
<td style="text-align: center;">0.6283</td>
<td style="text-align: center;">0.8019</td>
<td style="text-align: center;">0.7876</td>
<td style="text-align: center;">0.7856</td>
<td style="text-align: center;">0.7754</td>
<td style="text-align: center;">0.8072</td>
</tr>
<tr>
<td style="text-align: left;">BNG(heart-k,nominal,1000000)</td>
<td style="text-align: center;">0.829</td>
<td style="text-align: center;">0.7803</td>
<td style="text-align: center;">0.793</td>
<td style="text-align: center;">0.6898</td>
<td style="text-align: center;">0.8156</td>
<td style="text-align: center;">0.7867</td>
<td style="text-align: center;">0.8001</td>
<td style="text-align: center;">0.786</td>
<td style="text-align: center;">0.8188</td>
</tr>
<tr>
<td style="text-align: left;">BNG(waveform-5000,nominal,1000000)</td>
<td style="text-align: center;">0.9569</td>
<td style="text-align: center;">0.9567</td>
<td style="text-align: center;">0.9572</td>
<td style="text-align: center;">0.9579</td>
<td style="text-align: center;">0.9541</td>
<td style="text-align: center;">0.9568</td>
<td style="text-align: center;">0.9536</td>
<td style="text-align: center;">0.9535</td>
<td style="text-align: center;">0.9536</td>
</tr>
<tr>
<td style="text-align: left;">pokerband</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.6512</td>
<td style="text-align: center;">0.6205</td>
<td style="text-align: center;">0.6134</td>
<td style="text-align: center;">0.9395</td>
<td style="text-align: center;">0.9011</td>
<td style="text-align: center;">0.6894</td>
<td style="text-align: center;">0.7725</td>
<td style="text-align: center;">0.9632</td>
</tr>
<tr>
<td style="text-align: left;">RandomRBF_0_0</td>
<td style="text-align: center;">0.9878</td>
<td style="text-align: center;">0.9884</td>
<td style="text-align: center;">0.9886</td>
<td style="text-align: center;">0.9927</td>
<td style="text-align: center;">0.9872</td>
<td style="text-align: center;">0.9898</td>
<td style="text-align: center;">0.9915</td>
<td style="text-align: center;">0.9902</td>
<td style="text-align: center;">0.9918</td>
</tr>
<tr>
<td style="text-align: left;">RandomRBF_10_1E-3</td>
<td style="text-align: center;">0.9554</td>
<td style="text-align: center;">0.9591</td>
<td style="text-align: center;">0.9592</td>
<td style="text-align: center;">0.9671</td>
<td style="text-align: center;">0.9554</td>
<td style="text-align: center;">0.9598</td>
<td style="text-align: center;">0.9623</td>
<td style="text-align: center;">0.9607</td>
<td style="text-align: center;">0.9629</td>
</tr>
<tr>
<td style="text-align: left;">RandomRBF_10_1E-4</td>
<td style="text-align: center;">0.9635</td>
<td style="text-align: center;">0.9667</td>
<td style="text-align: center;">0.9659</td>
<td style="text-align: center;">0.9758</td>
<td style="text-align: center;">0.9627</td>
<td style="text-align: center;">0.9654</td>
<td style="text-align: center;">0.9727</td>
<td style="text-align: center;">0.9675</td>
<td style="text-align: center;">0.9701</td>
</tr>
<tr>
<td style="text-align: left;">SEA(50)</td>
<td style="text-align: center;">0.8728</td>
<td style="text-align: center;">0.8768</td>
<td style="text-align: center;">0.8761</td>
<td style="text-align: center;">0.8781</td>
<td style="text-align: center;">0.8765</td>
<td style="text-align: center;">0.8688</td>
<td style="text-align: center;">0.8776</td>
<td style="text-align: center;">0.8777</td>
<td style="text-align: center;">0.8773</td>
</tr>
<tr>
<td style="text-align: left;">SEA(50000)</td>
<td style="text-align: center;">0.8736</td>
<td style="text-align: center;">0.8754</td>
<td style="text-align: center;">0.8755</td>
<td style="text-align: center;">0.8781</td>
<td style="text-align: center;">0.8771</td>
<td style="text-align: center;">0.8803</td>
<td style="text-align: center;">0.8769</td>
<td style="text-align: center;">0.8774</td>
<td style="text-align: center;">0.8765</td>
</tr>
<tr>
<td style="text-align: left;">BNG(heart-c)</td>
<td style="text-align: center;">0.7768</td>
<td style="text-align: center;">0.7712</td>
<td style="text-align: center;">0.7676</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.7564</td>
<td style="text-align: center;">0.7594</td>
<td style="text-align: center;">0.7618</td>
<td style="text-align: center;">0.7514</td>
<td style="text-align: center;">0.7598</td>
</tr>
<tr>
<td style="text-align: left;">BNG(primary-tumor)</td>
<td style="text-align: center;">0.9009</td>
<td style="text-align: center;">0.8915</td>
<td style="text-align: center;">0.8977</td>
<td style="text-align: center;">0.8558</td>
<td style="text-align: center;">0.891</td>
<td style="text-align: center;">0.8473</td>
<td style="text-align: center;">0.8347</td>
<td style="text-align: center;">0.8443</td>
<td style="text-align: center;">0.8908</td>
</tr>
<tr>
<td style="text-align: left;">BNG(solar-flare)</td>
<td style="text-align: center;">0.8783</td>
<td style="text-align: center;">0.8673</td>
<td style="text-align: center;">0.8698</td>
<td style="text-align: center;">0.8527</td>
<td style="text-align: center;">0.8628</td>
<td style="text-align: center;">0.8303</td>
<td style="text-align: center;">0.8448</td>
<td style="text-align: center;">0.8323</td>
<td style="text-align: center;">0.8867</td>
</tr>
<tr>
<td style="text-align: left;">Stagger1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Stagger2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Stagger3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">AirlinesCodrnaAdult</td>
<td style="text-align: center;">0.8738</td>
<td style="text-align: center;">0.8761</td>
<td style="text-align: center;">0.8756</td>
<td style="text-align: center;">0.8822</td>
<td style="text-align: center;">0.8753</td>
<td style="text-align: center;">0.8525</td>
<td style="text-align: center;">0.8702</td>
<td style="text-align: center;">0.8679</td>
<td style="text-align: center;">0.8708</td>
</tr>
<tr>
<td style="text-align: left;">skin-segmentation</td>
<td style="text-align: center;">0.9997</td>
<td style="text-align: center;">0.9998</td>
<td style="text-align: center;">0.9998</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: center;">0.9985</td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: center;">0.9995</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">creditcard</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.9621</td>
<td style="text-align: center;">0.9616</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.9624</td>
<td style="text-align: center;">0.9435</td>
<td style="text-align: center;">0.9763</td>
<td style="text-align: center;">0.9601</td>
<td style="text-align: center;">0.9844</td>
</tr>
<tr>
<td style="text-align: left;">BNG(spambase)</td>
<td style="text-align: center;">0.6452</td>
<td style="text-align: center;">0.6615</td>
<td style="text-align: center;">0.6659</td>
<td style="text-align: center;">0.6632</td>
<td style="text-align: center;">0.6618</td>
<td style="text-align: center;">0.3719</td>
<td style="text-align: center;">0.6596</td>
<td style="text-align: center;">0.6532</td>
<td style="text-align: center;">0.6588</td>
</tr>
<tr>
<td style="text-align: left;">BNG(anneal)</td>
<td style="text-align: center;">0.9618</td>
<td style="text-align: center;">0.9622</td>
<td style="text-align: center;">0.9627</td>
<td style="text-align: center;">0.8851</td>
<td style="text-align: center;">0.9643</td>
<td style="text-align: center;">0.9491</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.9351</td>
<td style="text-align: center;">0.9624</td>
</tr>
<tr>
<td style="text-align: left;">fars</td>
<td style="text-align: center;">0.5021</td>
<td style="text-align: center;">0.4896</td>
<td style="text-align: center;">0.4972</td>
<td style="text-align: center;">0.4969</td>
<td style="text-align: center;">0.8509</td>
<td style="text-align: center;">0.8552</td>
<td style="text-align: center;">0.8751</td>
<td style="text-align: center;">0.8059</td>
<td style="text-align: center;">0.8774</td>
</tr>
<tr>
<td style="text-align: left;">seattlecrime6</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">0.9882</td>
<td style="text-align: center;">0.5022</td>
<td style="text-align: center;">0.8889</td>
<td style="text-align: center;">0.9902</td>
<td style="text-align: center;">0.9452</td>
<td style="text-align: center;">0.9879</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0.9926</td>
</tr>
<tr>
<td style="text-align: left;">porto-seguro</td>
<td style="text-align: center;">0.5742</td>
<td style="text-align: center;">0.5993</td>
<td style="text-align: center;">0.5983</td>
<td style="text-align: center;">0.5955</td>
<td style="text-align: center;">0.5941</td>
<td style="text-align: center;">0.5632</td>
<td style="text-align: center;">0.5919</td>
<td style="text-align: center;">0.5921</td>
<td style="text-align: center;">0.6086</td>
</tr>
<tr>
<td style="text-align: left;">CreditCardFraudDetection</td>
<td style="text-align: center;">0.9732</td>
<td style="text-align: center;">0.9611</td>
<td style="text-align: center;">0.9556</td>
<td style="text-align: center;">0.9466</td>
<td style="text-align: center;">0.9706</td>
<td style="text-align: center;">0.9355</td>
<td style="text-align: center;">0.9699</td>
<td style="text-align: center;">0.9698</td>
<td style="text-align: center;">0.9828</td>
</tr>
<tr>
<td style="text-align: left;">KDDCup99</td>
<td style="text-align: center;">0.5196</td>
<td style="text-align: center;">0.5248</td>
<td style="text-align: center;">0.5181</td>
<td style="text-align: center;">0.5248</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.8876</td>
<td style="text-align: center;">0.7609</td>
<td style="text-align: center;">0.674</td>
<td style="text-align: center;">0.9535</td>
</tr>
<tr>
<td style="text-align: left;">bates_classif_20</td>
<td style="text-align: center;">0.8701</td>
<td style="text-align: center;">0.8689</td>
<td style="text-align: center;">0.8727</td>
<td style="text-align: center;">0.8737</td>
<td style="text-align: center;">0.8754</td>
<td style="text-align: center;">0.8684</td>
<td style="text-align: center;">0.8774</td>
<td style="text-align: center;">0.8771</td>
<td style="text-align: center;">0.8761</td>
</tr>
<tr>
<td style="text-align: left;">colon</td>
<td style="text-align: center;">0.9939</td>
<td style="text-align: center;">0.9936</td>
<td style="text-align: center;">0.9949</td>
<td style="text-align: center;">0.9967</td>
<td style="text-align: center;">0.9969</td>
<td style="text-align: center;">0.9976</td>
<td style="text-align: center;">0.996</td>
<td style="text-align: center;">0.9916</td>
<td style="text-align: center;">0.9972</td>
</tr>
<tr>
<td style="text-align: left;">breast</td>
<td style="text-align: center;">0.9626</td>
<td style="text-align: center;">0.9613</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">0.9903</td>
<td style="text-align: center;">0.9925</td>
<td style="text-align: center;">0.9715</td>
<td style="text-align: center;">0.9895</td>
<td style="text-align: center;">0.9862</td>
<td style="text-align: center;">0.9934</td>
</tr>
<tr>
<td style="text-align: left;">Mean AUC OVO</td>
<td style="text-align: center;">0.865 $\pm .001$</td>
<td style="text-align: center;">0.865 $\pm .001$</td>
<td style="text-align: center;">0.849 $\pm .001$</td>
<td style="text-align: center;">0.844 $\pm .001$</td>
<td style="text-align: center;">0.900 $\pm .002$</td>
<td style="text-align: center;">0.878 $\pm .002$</td>
<td style="text-align: center;">0.885 $\pm .001$</td>
<td style="text-align: center;">0.874 $\pm .001$</td>
<td style="text-align: center;">0.908 $\pm .001$</td>
</tr>
</tbody>
</table>
<h1>E Additional Information for Time Budget</h1>
<p>We follow the time budget comparison method from prior work (reference [21]) and we can include further details in a revision. As for CPUs and GPUs, we show our hardware in the supplemental Section B; for reference here we use an Intel(R) Xeon(R) Server CPU with 48 cores with RTX 3090 GPU.</p>
<p>The overall training/inference time includes training across potentially multiple hyperparameter trials and inference, and represents the average cost of each model on all datasets. For each dataset, we compute overall time via num $<em _per="{per" _text="\text" trial="trial">{\text {trials }} \times$ cost $</em>$ is chosen as the largest value such that $\left(\right.$ num $}}$, where $n u m_{\text {trials }<em _per="{per" _text="\text" trial="trial">{\text {trials }}-1$ ) $\times$ $\operatorname{cost}</em>=1$ for all non-TabPFN-based models. Hence in such cases, the listed time cost reduces to the cost of a single training and inference run.}}&lt;$ time budget, so the actual time cost can be different across different models. We show the overall time for different models when granted different time budgets in the new table below (the chosen time budgets are drawn from Figure 2 of our submission). Note that SAINT only has results for budget 6000s/million samples because only one trial will cost more than the smaller time budgets listed in the table. We remark that when the budget is 60 s/million samples, num $_{\text {trials }</p>
<p>For the larger datasets, we use BoostPFN with different weak learners. The number of weak learners in the ensemble model is 10 for 5,000 training samples, 100 for 50,000 and 1,000 for full training set. For other models like XGBoost or LightGBM, the time limitation is still 6000 seconds per million samples.</p>
<h2>F Boosting Process for Large Datasets</h2>
<p>We show here the boosting loss on training set in Figure 4, boosting loss on test set in Figure 5. It's noted that in one of the datasets the test boosting loss goes up when the number of weak learners increase, while the training loss goes down, which clearly shows over-fitting.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 12: Per dataset results on large datasets with 50,000 training samples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">LightGBM</th>
<th style="text-align: center;">CatBoost</th>
<th style="text-align: center;">XGBoost</th>
<th style="text-align: center;">AutoGluon</th>
<th style="text-align: center;">FT-Trans.</th>
<th style="text-align: center;">SAINT</th>
<th style="text-align: center;">TabPFN</th>
<th style="text-align: center;">Bagging</th>
<th style="text-align: center;">BoostPFN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BNG(page-blocks,nominal,295245)</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.8575</td>
<td style="text-align: center;">0.8518</td>
<td style="text-align: center;">0.8415</td>
<td style="text-align: center;">0.8575</td>
<td style="text-align: center;">0.8445</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8209</td>
<td style="text-align: center;">0.8427</td>
</tr>
<tr>
<td style="text-align: left;">BNG(glass,nominal,137781)</td>
<td style="text-align: center;">0.9151</td>
<td style="text-align: center;">0.9091</td>
<td style="text-align: center;">0.9155</td>
<td style="text-align: center;">0.9129</td>
<td style="text-align: center;">0.916</td>
<td style="text-align: center;">0.9126</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8936</td>
<td style="text-align: center;">0.8941</td>
</tr>
<tr>
<td style="text-align: left;">BNG(heart-c,nominal,1000000)</td>
<td style="text-align: center;">0.8125</td>
<td style="text-align: center;">0.7999</td>
<td style="text-align: center;">0.8066</td>
<td style="text-align: center;">0.8102</td>
<td style="text-align: center;">0.811</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.7767</td>
<td style="text-align: center;">0.802</td>
</tr>
<tr>
<td style="text-align: left;">BNG(heart-h,nominal,1000000)</td>
<td style="text-align: center;">0.8198</td>
<td style="text-align: center;">0.8042</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.8137</td>
<td style="text-align: center;">0.8095</td>
<td style="text-align: center;">0.8161</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.7872</td>
<td style="text-align: center;">0.8193</td>
</tr>
<tr>
<td style="text-align: left;">BNG(waveform-5000,nominal,1000000)</td>
<td style="text-align: center;">0.9631</td>
<td style="text-align: center;">0.9619</td>
<td style="text-align: center;">0.9636</td>
<td style="text-align: center;">0.9583</td>
<td style="text-align: center;">0.9616</td>
<td style="text-align: center;">0.9626</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9548</td>
<td style="text-align: center;">0.9536</td>
</tr>
<tr>
<td style="text-align: left;">pokerhand</td>
<td style="text-align: center;">0.4945</td>
<td style="text-align: center;">0.5275</td>
<td style="text-align: center;">0.4923</td>
<td style="text-align: center;">0.4981</td>
<td style="text-align: center;">0.9625</td>
<td style="text-align: center;">0.8915</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.7813</td>
<td style="text-align: center;">0.9752</td>
</tr>
<tr>
<td style="text-align: left;">RandomBBF_0_0</td>
<td style="text-align: center;">0.9929</td>
<td style="text-align: center;">0.9902</td>
<td style="text-align: center;">0.9916</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.9935</td>
<td style="text-align: center;">0.994</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9909</td>
<td style="text-align: center;">0.9924</td>
</tr>
<tr>
<td style="text-align: left;">RandomBBF_10_1E-3</td>
<td style="text-align: center;">0.9649</td>
<td style="text-align: center;">0.9627</td>
<td style="text-align: center;">0.9673</td>
<td style="text-align: center;">0.9685</td>
<td style="text-align: center;">0.9701</td>
<td style="text-align: center;">0.9678</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9634</td>
<td style="text-align: center;">0.964</td>
</tr>
<tr>
<td style="text-align: left;">RandomBBF_10_1E-4</td>
<td style="text-align: center;">0.9767</td>
<td style="text-align: center;">0.9718</td>
<td style="text-align: center;">0.9774</td>
<td style="text-align: center;">0.9826</td>
<td style="text-align: center;">0.9827</td>
<td style="text-align: center;">0.9806</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9697</td>
<td style="text-align: center;">0.9745</td>
</tr>
<tr>
<td style="text-align: left;">SEA(50)</td>
<td style="text-align: center;">0.8817</td>
<td style="text-align: center;">0.8811</td>
<td style="text-align: center;">0.8808</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.8775</td>
<td style="text-align: center;">0.8731</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8783</td>
<td style="text-align: center;">0.8767</td>
</tr>
<tr>
<td style="text-align: left;">SEA(50000)</td>
<td style="text-align: center;">0.8806</td>
<td style="text-align: center;">0.8805</td>
<td style="text-align: center;">0.8805</td>
<td style="text-align: center;">0.8892</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.8729</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8779</td>
<td style="text-align: center;">0.8767</td>
</tr>
<tr>
<td style="text-align: left;">BNG(heart-c)</td>
<td style="text-align: center;">0.7977</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.7828</td>
<td style="text-align: center;">0.7911</td>
<td style="text-align: center;">0.7924</td>
<td style="text-align: center;">0.7641</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.7633</td>
<td style="text-align: center;">0.7684</td>
</tr>
<tr>
<td style="text-align: left;">BNG(primary-tumor)</td>
<td style="text-align: center;">0.9104</td>
<td style="text-align: center;">0.9163</td>
<td style="text-align: center;">0.9134</td>
<td style="text-align: center;">0.8695</td>
<td style="text-align: center;">0.9138</td>
<td style="text-align: center;">0.9072</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8548</td>
<td style="text-align: center;">0.8901</td>
</tr>
<tr>
<td style="text-align: left;">BNG(solar-flare)</td>
<td style="text-align: center;">0.9121</td>
<td style="text-align: center;">0.9001</td>
<td style="text-align: center;">0.9061</td>
<td style="text-align: center;">0.8808</td>
<td style="text-align: center;">0.9018</td>
<td style="text-align: center;">0.8796</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8416</td>
<td style="text-align: center;">0.8772</td>
</tr>
<tr>
<td style="text-align: left;">Stagger1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Stagger2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Stagger3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">AirlinesCodrnaAdult</td>
<td style="text-align: center;">0.8921</td>
<td style="text-align: center;">0.8932</td>
<td style="text-align: center;">0.5006</td>
<td style="text-align: center;">0.8967</td>
<td style="text-align: center;">0.8915</td>
<td style="text-align: center;">0.8937</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8709</td>
<td style="text-align: center;">0.8751</td>
</tr>
<tr>
<td style="text-align: left;">skin-segmentation</td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: center;">0.9998</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">creditcard</td>
<td style="text-align: center;">0.9756</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.9721</td>
<td style="text-align: center;">0.9781</td>
<td style="text-align: center;">0.9719</td>
<td style="text-align: center;">0.9492</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9683</td>
<td style="text-align: center;">0.9833</td>
</tr>
<tr>
<td style="text-align: left;">BNG(spambase)</td>
<td style="text-align: center;">0.6688</td>
<td style="text-align: center;">0.6695</td>
<td style="text-align: center;">0.669</td>
<td style="text-align: center;">0.6693</td>
<td style="text-align: center;">0.6689</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.6614</td>
<td style="text-align: center;">0.6577</td>
</tr>
<tr>
<td style="text-align: left;">BNG(anneal)</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.9818</td>
<td style="text-align: center;">0.9778</td>
<td style="text-align: center;">0.9875</td>
<td style="text-align: center;">0.9884</td>
<td style="text-align: center;">0.9856</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9372</td>
<td style="text-align: center;">0.9712</td>
</tr>
<tr>
<td style="text-align: left;">fars</td>
<td style="text-align: center;">0.8866</td>
<td style="text-align: center;">0.8867</td>
<td style="text-align: center;">0.9086</td>
<td style="text-align: center;">0.8566</td>
<td style="text-align: center;">0.8937</td>
<td style="text-align: center;">0.9003</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.859</td>
<td style="text-align: center;">0.8768</td>
</tr>
<tr>
<td style="text-align: left;">seattlecrime6</td>
<td style="text-align: center;">0.9898</td>
<td style="text-align: center;">0.5105</td>
<td style="text-align: center;">0.5042</td>
<td style="text-align: center;">0.5041</td>
<td style="text-align: center;">0.9891</td>
<td style="text-align: center;">0.9637</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9906</td>
<td style="text-align: center;">0.9913</td>
</tr>
<tr>
<td style="text-align: left;">porto-seguro</td>
<td style="text-align: center;">0.6084</td>
<td style="text-align: center;">0.5675</td>
<td style="text-align: center;">0.6034</td>
<td style="text-align: center;">0.6257</td>
<td style="text-align: center;">0.6192</td>
<td style="text-align: center;">0.6153</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.6141</td>
<td style="text-align: center;">0.6136</td>
</tr>
<tr>
<td style="text-align: left;">CreditCardFraudDetection</td>
<td style="text-align: center;">0.9766</td>
<td style="text-align: center;">0.9757</td>
<td style="text-align: center;">0.9742</td>
<td style="text-align: center;">0.9779</td>
<td style="text-align: center;">0.9639</td>
<td style="text-align: center;">0.9656</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9703</td>
<td style="text-align: center;">0.9824</td>
</tr>
<tr>
<td style="text-align: left;">KDDCup99</td>
<td style="text-align: center;">0.4863</td>
<td style="text-align: center;">0.4869</td>
<td style="text-align: center;">0.5352</td>
<td style="text-align: center;">0.4974</td>
<td style="text-align: center;">0.8298</td>
<td style="text-align: center;">0.9276</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.7649</td>
<td style="text-align: center;">0.9606</td>
</tr>
<tr>
<td style="text-align: left;">bates_classif_20</td>
<td style="text-align: center;">0.8746</td>
<td style="text-align: center;">0.8744</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.8782</td>
<td style="text-align: center;">0.8779</td>
<td style="text-align: center;">0.8905</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8784</td>
<td style="text-align: center;">0.8775</td>
</tr>
<tr>
<td style="text-align: left;">colon</td>
<td style="text-align: center;">0.9965</td>
<td style="text-align: center;">0.9965</td>
<td style="text-align: center;">0.9962</td>
<td style="text-align: center;">0.9973</td>
<td style="text-align: center;">0.9976</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9915</td>
<td style="text-align: center;">0.9974</td>
</tr>
<tr>
<td style="text-align: left;">breast</td>
<td style="text-align: center;">0.9826</td>
<td style="text-align: center;">0.9749</td>
<td style="text-align: center;">0.9844</td>
<td style="text-align: center;">0.9776</td>
<td style="text-align: center;">0.9942</td>
<td style="text-align: center;">0.9904</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9898</td>
<td style="text-align: center;">0.9942</td>
</tr>
<tr>
<td style="text-align: left;">Mean AUC OVO</td>
<td style="text-align: center;">0.883 $\pm .001$</td>
<td style="text-align: center;">0.864 $\pm .001$</td>
<td style="text-align: center;">0.855 $\pm .001$</td>
<td style="text-align: center;">0.865 $\pm .001$</td>
<td style="text-align: center;">0.910 $\pm .001$</td>
<td style="text-align: center;">0.907 $\pm .001$</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8888 $\pm .001$</td>
<td style="text-align: center;">0.910 $\pm .001$</td>
</tr>
</tbody>
</table>
<p>Table 13: Per dataset results on large datasets with full training samples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">LightGBM</th>
<th style="text-align: center;">CatBoost</th>
<th style="text-align: center;">XGBoost</th>
<th style="text-align: center;">AutoGluon</th>
<th style="text-align: center;">FT-Trans.</th>
<th style="text-align: center;">SAINT</th>
<th style="text-align: center;">TabPFN</th>
<th style="text-align: center;">Bagging</th>
<th style="text-align: center;">BoostPFN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BNG(page-blocks,nominal,295245)</td>
<td style="text-align: center;">0.8587</td>
<td style="text-align: center;">0.8602</td>
<td style="text-align: center;">0.8598</td>
<td style="text-align: center;">0.8456</td>
<td style="text-align: center;">0.8619</td>
<td style="text-align: center;">0.8472</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8149</td>
<td style="text-align: center;">0.8471</td>
</tr>
<tr>
<td style="text-align: left;">BNG(glass,nominal,137781)</td>
<td style="text-align: center;">0.9154</td>
<td style="text-align: center;">0.9148</td>
<td style="text-align: center;">0.9092</td>
<td style="text-align: center;">0.9129</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.9152</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8936</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">BNG(heart-c,nominal,1000000)</td>
<td style="text-align: center;">0.8137</td>
<td style="text-align: center;">0.8148</td>
<td style="text-align: center;">0.8105</td>
<td style="text-align: center;">0.8104</td>
<td style="text-align: center;">0.8196</td>
<td style="text-align: center;">0.817</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.777</td>
<td style="text-align: center;">0.8052</td>
</tr>
<tr>
<td style="text-align: left;">BNG(heart-h,nominal,1000000)</td>
<td style="text-align: center;">0.8205</td>
<td style="text-align: center;">0.8221</td>
<td style="text-align: center;">0.8216</td>
<td style="text-align: center;">0.822</td>
<td style="text-align: center;">0.8246</td>
<td style="text-align: center;">0.8194</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.7877</td>
<td style="text-align: center;">0.8138</td>
</tr>
<tr>
<td style="text-align: left;">BNG(waveform-5000,nominal,1000000)</td>
<td style="text-align: center;">0.9663</td>
<td style="text-align: center;">0.9667</td>
<td style="text-align: center;">0.9659</td>
<td style="text-align: center;">0.9649</td>
<td style="text-align: center;">0.9654</td>
<td style="text-align: center;">0.9643</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9551</td>
<td style="text-align: center;">0.9561</td>
</tr>
<tr>
<td style="text-align: left;">pokerhand</td>
<td style="text-align: center;">0.8174</td>
<td style="text-align: center;">0.8751</td>
<td style="text-align: center;">0.8782</td>
<td style="text-align: center;">0.8968</td>
<td style="text-align: center;">0.9721</td>
<td style="text-align: center;">0.9555</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8096</td>
<td style="text-align: center;">0.968</td>
</tr>
<tr>
<td style="text-align: left;">RandomBBF_0_0</td>
<td style="text-align: center;">0.9948</td>
<td style="text-align: center;">0.9934</td>
<td style="text-align: center;">0.9925</td>
<td style="text-align: center;">0.9951</td>
<td style="text-align: center;">0.9959</td>
<td style="text-align: center;">0.9959</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9909</td>
<td style="text-align: center;">0.9929</td>
</tr>
<tr>
<td style="text-align: left;">RandomBBF_10_1E-3</td>
<td style="text-align: center;">0.9761</td>
<td style="text-align: center;">0.9706</td>
<td style="text-align: center;">0.9678</td>
<td style="text-align: center;">0.981</td>
<td style="text-align: center;">0.9814</td>
<td style="text-align: center;">0.9811</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9636</td>
<td style="text-align: center;">0.9658</td>
</tr>
<tr>
<td style="text-align: left;">RandomBBF_10_1E-4</td>
<td style="text-align: center;">0.9866</td>
<td style="text-align: center;">0.9804</td>
<td style="text-align: center;">0.9771</td>
<td style="text-align: center;">0.9884</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.9904</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.975</td>
</tr>
<tr>
<td style="text-align: left;">SEA(50)</td>
<td style="text-align: center;">0.9083</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.9162</td>
<td style="text-align: center;">0.9824</td>
<td style="text-align: center;">0.8767</td>
<td style="text-align: center;">0.8779</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8782</td>
<td style="text-align: center;">0.8784</td>
</tr>
<tr>
<td style="text-align: left;">SEA(50000)</td>
<td style="text-align: center;">0.9047</td>
<td style="text-align: center;">0.899</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.9824</td>
<td style="text-align: center;">0.8782</td>
<td style="text-align: center;">0.8777</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.878</td>
</tr>
<tr>
<td style="text-align: left;">BNG(heart-c)</td>
<td style="text-align: center;">0.8013</td>
<td style="text-align: center;">0.7987</td>
<td style="text-align: center;">0.7935</td>
<td style="text-align: center;">0.7955</td>
<td style="text-align: center;">0.7998</td>
<td style="text-align: center;">0.7995</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.7649</td>
<td style="text-align: center;">0.779</td>
</tr>
<tr>
<td style="text-align: left;">BNG(primary-tumor)</td>
<td style="text-align: center;">0.9188</td>
<td style="text-align: center;">0.9203</td>
<td style="text-align: center;">0.9181</td>
<td style="text-align: center;">0.9174</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.9139</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8557</td>
<td style="text-align: center;">0.902</td>
</tr>
<tr>
<td style="text-align: left;">BNG(solar-flare)</td>
<td style="text-align: center;">0.9262</td>
<td style="text-align: center;">0.9235</td>
<td style="text-align: center;">0.9207</td>
<td style="text-align: center;">0.9046</td>
<td style="text-align: center;">0.9346</td>
<td style="text-align: center;">0.928</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8454</td>
<td style="text-align: center;">0.8946</td>
</tr>
<tr>
<td style="text-align: left;">Stagger1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Stagger2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Stagger3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">AirlinesCodrnaAdult</td>
<td style="text-align: center;">0.9102</td>
<td style="text-align: center;">0.9035</td>
<td style="text-align: center;">0.9024</td>
<td style="text-align: center;">0.9134</td>
<td style="text-align: center;">0.9038</td>
<td style="text-align: center;">0.9029</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8713</td>
<td style="text-align: center;">0.8822</td>
</tr>
<tr>
<td style="text-align: left;">skin-segmentation</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.9997</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">creditcard</td>
<td style="text-align: center;">0.9799</td>
<td style="text-align: center;">0.9802</td>
<td style="text-align: center;">0.9802</td>
<td style="text-align: center;">0.9834</td>
<td style="text-align: center;">0.9778</td>
<td style="text-align: center;">0.9777</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9712</td>
<td style="text-align: center;">0.982</td>
</tr>
<tr>
<td style="text-align: left;">BNG(spambase)</td>
<td style="text-align: center;">0.6723</td>
<td style="text-align: center;">0.6722</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.6717</td>
<td style="text-align: center;">0.6714</td>
<td style="text-align: center;">0.6701</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.6609</td>
<td style="text-align: center;">0.6653</td>
</tr>
<tr>
<td style="text-align: left;">BNG(anneal)</td>
<td style="text-align: center;">0.9949</td>
<td style="text-align: center;">0.9946</td>
<td style="text-align: center;">0.9939</td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">0.9961</td>
<td style="text-align: center;">0.9953</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9361</td>
<td style="text-align: center;">0.982</td>
</tr>
<tr>
<td style="text-align: left;">fars</td>
<td style="text-align: center;">0.8769</td>
<td style="text-align: center;">0.9177</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.8428</td>
<td style="text-align: center;">0.8809</td>
<td style="text-align: center;">0.9128</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.8894</td>
</tr>
<tr>
<td style="text-align: left;">seattlecrime6</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.9906</td>
<td style="text-align: center;">0.9912</td>
<td style="text-align: center;">0.9925</td>
<td style="text-align: center;">0.9915</td>
<td style="text-align: center;">0.9668</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.9911</td>
</tr>
<tr>
<td style="text-align: left;">porto-seguro</td>
<td style="text-align: center;">0.6362</td>
<td style="text-align: center;">0.6362</td>
<td style="text-align: center;">0.6278</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.6311</td>
<td style="text-align: center;">0.6333</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.6163</td>
<td style="text-align: center;">0.6218</td>
</tr>
<tr>
<td style="text-align: left;">CreditCardFraudDetection</td>
<td style="text-align: center;">0.9775</td>
<td style="text-align: center;">0.9832</td>
<td style="text-align: center;">0.9802</td>
<td style="text-align: center;">0.9839</td>
<td style="text-align: center;">0.9449</td>
<td style="text-align: center;">0.9779</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9772</td>
<td style="text-align: center;">0.9821</td>
</tr>
<tr>
<td style="text-align: left;">KDDCup99</td>
<td style="text-align: center;">0.926</td>
<td style="text-align: center;">0.7513</td>
<td style="text-align: center;">0.9509</td>
<td style="text-align: center;">0.7862</td>
<td style="text-align: center;">0.9434</td>
<td style="text-align: center;">0.9543</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9035</td>
<td style="text-align: center;">0.9654</td>
</tr>
<tr>
<td style="text-align: left;">bates_classif_20</td>
<td style="text-align: center;">0.8785</td>
<td style="text-align: center;">0.8785</td>
<td style="text-align: center;">0.8766</td>
<td style="text-align: center;">0.8789</td>
<td style="text-align: center;">0.8791</td>
<td style="text-align: center;">0.8787</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.8785</td>
<td style="text-align: center;">0.8778</td>
</tr>
<tr>
<td style="text-align: left;">colon</td>
<td style="text-align: center;">0.9976</td>
<td style="text-align: center;">0.9976</td>
<td style="text-align: center;">0.9968</td>
<td style="text-align: center;">0.9973</td>
<td style="text-align: center;">0.9978</td>
<td style="text-align: center;">0.9977</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9915</td>
<td style="text-align: center;">0.9975</td>
</tr>
<tr>
<td style="text-align: left;">breast</td>
<td style="text-align: center;">0.9925</td>
<td style="text-align: center;">0.9936</td>
<td style="text-align: center;">0.9756</td>
<td style="text-align: center;">0.9745</td>
<td style="text-align: center;">0.9948</td>
<td style="text-align: center;">0.9946</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.9901</td>
<td style="text-align: center;">0.9944</td>
</tr>
<tr>
<td style="text-align: left;">Mean AUC OVO</td>
<td style="text-align: center;">0.915 $\pm .001$</td>
<td style="text-align: center;">0.911 $\pm .001$</td>
<td style="text-align: center;">0.916 $\pm .001$</td>
<td style="text-align: center;">0.915 $\pm .001$</td>
<td style="text-align: center;">0.918 $\pm .001$</td>
<td style="text-align: center;">0.918 $\pm .001$</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">0.895 $\pm .001$</td>
<td style="text-align: center;">0.913 $\pm .001$</td>
</tr>
</tbody>
</table>
<p>Table 14: Per dataset results on large datasets with full training samples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Time Budget per million samples (s)</th>
<th style="text-align: center;">LightGBM</th>
<th style="text-align: center;">CatBoost</th>
<th style="text-align: center;">XGBoost</th>
<th style="text-align: center;">AutoGluon</th>
<th style="text-align: center;">FT-Trans.</th>
<th style="text-align: center;">SAINT</th>
<th style="text-align: center;">TabPFN</th>
<th style="text-align: center;">Bagging</th>
<th style="text-align: center;">BoostPFN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">60</td>
<td style="text-align: center;">128.7</td>
<td style="text-align: center;">755.2</td>
<td style="text-align: center;">159.5</td>
<td style="text-align: center;">613.7</td>
<td style="text-align: center;">283.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">49.7</td>
</tr>
<tr>
<td style="text-align: left;">1500</td>
<td style="text-align: center;">1192.1</td>
<td style="text-align: center;">2406.5</td>
<td style="text-align: center;">1823.1</td>
<td style="text-align: center;">2780.6</td>
<td style="text-align: center;">1134.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">6000</td>
<td style="text-align: center;">3130.7</td>
<td style="text-align: center;">7439.3</td>
<td style="text-align: center;">6349.2</td>
<td style="text-align: center;">7229.8</td>
<td style="text-align: center;">4536.0</td>
<td style="text-align: center;">1545.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Boost loss on training set for all large datasets with 10 weak learners.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Boost loss on test set for all large datasets with 10 weak learners.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>we don't use this modification in our experiments&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ This work was completed when the first author was during an internship at the AWS Shanghai AI Lab.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-10055 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-10055</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-10055</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-170.html">extraction-schema-170</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <p><strong>Paper ID:</strong> paper-10e1bcd31536ca33a1298260a630744fb80d9644</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/10e1bcd31536ca33a1298260a630744fb80d9644" target="_blank">Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis</a></p>
                <p><strong>Paper Venue:</strong> J. Am. Medical Informatics Assoc.</p>
                <p><strong>Paper TL;DR:</strong> A novel multimodal deep learning framework to accurately detect the presence of AD and mild cognitive impairment from imaging, genetic, and clinical data and uses cross-modal attention, which captures interactions between modalities.</p>
                <p><strong>Paper Abstract:</strong> OBJECTIVE
Alzheimer's disease (AD) is the most common neurodegenerative disorder with one of the most complex pathogeneses, making effective and clinically actionable decision support difficult. The objective of this study was to develop a novel multimodal deep learning framework to aid medical professionals in AD diagnosis.


MATERIALS AND METHODS
We present a Multimodal Alzheimer's Disease Diagnosis framework (MADDi) to accurately detect the presence of AD and mild cognitive impairment (MCI) from imaging, genetic, and clinical data. MADDi is novel in that we use cross-modal attention, which captures interactions between modalities-a method not previously explored in this domain. We perform multi-class classification, a challenging task considering the strong similarities between MCI and AD. We compare with previous state-of-the-art models, evaluate the importance of attention, and examine the contribution of each modality to the model's performance.


RESULTS
MADDi classifies MCI, AD, and controls with 96.88% accuracy on a held-out test set. When examining the contribution of different attention schemes, we found that the combination of cross-modal attention with self-attention performed the best, and no attention layers in the model performed the worst, with a 7.9% difference in F1-scores.


DISCUSSION
Our experiments underlined the importance of structured clinical data to help machine learning models contextualize and interpret the remaining modalities. Extensive ablation studies showed that any multimodal mixture of input features without access to structured clinical information suffered marked performance losses.


CONCLUSION
This study demonstrates the merit of combining multiple input modalities via cross-modal attention to deliver highly accurate AD diagnostic decision support.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e10055.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e10055.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Genetic SNPs (AD-related genes)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single Nucleotide Polymorphism profiling of Alzheimer-associated genes (filtered from WGS using AlzGene list)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Genetic variation (SNPs) in genes previously associated with Alzheimer's disease was used as a modality to predict diagnosis; the authors filtered WGS data to AD-related gene regions (AlzGene) and used supervised feature selection (Random Forest) to select ~15,000 SNPs for modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>genetic</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Genetic predisposition encoded by SNPs within genes previously associated with AD (AlzGene list of ~680 genes was used to select SNP regions). The paper treats genetic variation as a risk/associative factor rather than specifying single causal alleles (e.g., APOE4 is not singled out in this text).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>The genetic modality (neural network trained on selected SNP features) achieved unimodal test accuracy 77.78% and F1-score ~77.24% (Supplement S6 Table S3). Genetic features contributed to the multimodal classifier (MADDi) performance when combined with clinical and imaging data (final multimodal accuracy 96.88%, F1 91.41%), indicating predictive signal in the SNPs after feature selection.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>Genetic data were sparse and limited in sample size (805 patients with genetic data, only 239 patients in the full-modality overlap set). The authors note ground-truth/labeling issues for genetics (diagnosis not always contemporaneous with genetic sample), and supervised feature selection (Random Forest) can bias features toward the training labels. When clinical data were withheld, combining genetic+imaging produced worse F1 (50.07%), suggesting genetic signal alone is insufficient for robust classification in this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>genetic screening / SNP profiling</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>Whole-genome sequencing-derived SNPs filtered by HWE, genotype quality, MAF, missingness, and restricted to AD-related genes (AlzGene), then supervised feature selection (Random Forest) to ~15,000 SNP features; encoded as 0/1/2 allele categories and input to a 3-layer fully connected network.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Unimodal genetic model: accuracy 77.78%, precision 78.37%, recall 76.92%, F1 77.24% (average across 5 seeds). In multimodal models genetic modality improved overall performance but yielded poor standalone robustness relative to imaging.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Computational classification (machine learning) using observational ADNI cohort data</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>805 ADNI participants with genetic WGS data (after filtering), overlap multimodal set: 239 patients (165 controls, 39 MCI, 35 AD) used for multimodal experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Limitations include small sample size for genetics, temporal mismatch between genetic data and clinical/imaging labels, heavy preprocessing and supervised feature selection that may overfit to training labels, and the paper does not report or analyze specific causal alleles (e.g., APOE4) or biological mechanisms. Genetic modality alone gave lower performance than imaging and relied on being combined with clinical data for robust prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis", 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e10055.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e10055.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MRI structural imaging (center slices)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Magnetic Resonance Imaging (structural MRI) using three central brain slices per patient</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Structural MRI was used as an imaging modality; the authors extracted three central orthogonal slices from ADNI-preprocessed brain volumes (72x72 pixels) and used a 3-layer convolutional neural network to predict diagnosis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>neuroanatomical / structural changes (brain atrophy as biomarker)</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>The modality operationalizes neurodegenerative structural changes (e.g., atrophy patterns that correlate with AD stages). While the paper does not list specific anatomical loci, it references prior literature using hippocampal atrophy and other MRI-derived measures for AD detection.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>Unimodal imaging model achieved high performance (accuracy 92.28% reported in main text; Supplement S6 reports 92.23%), F1 ~91.83%, indicating that structural MRI contains strong discriminative signal for AD vs MCI vs control in ADNI data. Imaging alone was the best-performing single modality in unimodal experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>Authors note potential information loss from using only three central slices (vs full volumes), and limited imaging sample size (551 unique patients) with repeated scans for training (3674 MRIs used for training by allowing repeats). Using limited slices may miss some disease-relevant regions; however, validation experiments (Supplement S3) showed performance differences within ~1% up to 20 additional slices, suggesting limited practical loss on this task. Nevertheless, reliance on ADNI-preprocessed standardized scans may reduce generalizability to heterogenous clinical scanning protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>neuroimaging (structural MRI)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>ADNI-corrected T1 MRI volumes; three central slices per axis extracted, resized to 72x72 pixels, input to a 3-layer CNN; ADNI standard corrections (gradwarp, B1 non-uniformity, N3) applied upstream by ADNI pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Unimodal MRI model: accuracy ≈92.23–92.28%, precision 94.02%, recall 90.4%, F1 91.83% (averaged across seeds). In multimodal MADDi (MRI+SNP+clinical with attention) overall accuracy 96.88% and F1 91.41%; class-wise, Alzheimer's Disease cases were predicted with 100% recall in the final model, while MCI recall was lower (~70%).</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Computational classification using observational ADNI imaging dataset</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>551 unique ADNI patients with baseline MRI (3674 MRI scans used for training when allowing repeated scans); overlap multimodal set: 239 patients used for full multimodal experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Limitations include relatively small imaging sample and repeated-scan training strategy, potential information loss from using only central slices (although validated), and potential lack of generalizability because ADNI imaging underwent specific preprocessing steps. The model tends to misclassify some MCI as controls, indicating limited sensitivity for subtle/prodromal stages despite high accuracy for frank AD.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis", 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e10055.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e10055.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clinical cognitive assessments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured clinical and cognitive assessments (neurological exams, memory/executive/language scores, demographics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Structured clinical data (29 features including cognitive scores and neurological exam items) were used both as a standalone modality and in combination with imaging and genetics; Random Forest identified memory, executive function, and language scores as the most important clinical features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>clinical phenotype / symptomatic cognitive decline (used as detection markers rather than mechanistic causes)</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Cognitive deficits (measured via memory, executive function, language scores and other neurological exam items) are clinical manifestations of AD pathophysiology and serve as accessible markers for diagnosis and staging (MCI vs AD vs control). The paper treats these as predictive features, not etiologic mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>Unimodal clinical model achieved accuracy 80.59% and F1 ~80.47% (trained on 2384 patients). Clinical features were the most important catalyst in multimodal performance: models lacking clinical data showed marked performance losses (clinical alone accuracy 82.29%, F1 78.30%; genetic+imaging together 78.33% accuracy but F1 dropped to 50.07%), indicating clinical context substantially improves model interpretation of other modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>Clinical assessments are symptomatic measures and thus may lag underlying pathology; clinical variables with direct indication of AD (e.g., AD-specific medication) were removed to prevent leakage. Timing mismatches (clinical exam date vs MRI date) created ground-truth labeling challenges; clinical data frequency is lower than imaging, potentially limiting contemporaneous labels.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>cognitive/clinical assessments (structured scales and neurological exam items)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>29 structured features from neurological exams, cognitive assessments, and demographics; categorical features one-hot encoded, continuous features normalized; input to a 3-layer fully connected neural network. Random Forest was used to rank feature importance.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Unimodal clinical model: accuracy 80.59%, precision 80.56%, recall 80.48%, F1 80.47% (averaged). Inclusion of clinical data in multimodal MADDi markedly improved overall performance (final multimodal accuracy 96.88%, F1 91.41%); absence of clinical modality caused substantial F1 drop in some modality combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Computational classification using observational ADNI clinical data</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Clinical data available for 2384 ADNI patients; overlap multimodal set: 239 patients used in final multimodal experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Clinical measures are variable in timing and frequency, may not reflect preclinical pathology, and removal of variables that explicitly encode diagnosis (e.g., AD medications) was necessary to avoid label leakage. The paper notes that clinical data helped ground other modalities, but reliance on clinical features may limit early preclinical detection before symptom onset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis", 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e10055.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e10055.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MADDi (multimodal attention DL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multimodal Alzheimer's Disease Diagnosis framework (MADDi) using self-attention and cross-modal attention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An attention-based deep learning model that integrates MRI, SNP, and structured clinical data using modality-specific backbones followed by multi-head self-attention and bi-directional cross-modal attention to capture intra- and inter-modality interactions, producing three-class (CN, MCI, AD) predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>detection algorithm / multimodal integration approach (not a biological cause)</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>MADDi is an algorithmic approach for disease detection that models interactions between imaging, genetic, and clinical modalities with attention mechanisms to improve classification of AD stages; it does not propose a biological cause but is a detection/diagnostic method.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>The model achieved state-of-the-art performance on the ADNI multimodal three-class classification task with average held-out test accuracy 96.88% ± 3.33% and average F1 91.41% across 5 initializations; ablation experiments (100 random seeds) showed that combining self-attention and cross-modal attention produced the highest and most stable F1 distribution and outperformed architectures with no attention (7.9% average F1 improvement, p < 0.0001). Comparisons to other multimodal studies in Table 3 show higher test accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>Potential overfitting and limited generalizability due to small overlap sample (239 patients) used to train multimodal model; performance variability across initializations (though narrower with full attention); model tends to misclassify MCI as controls (MCI recall ~70%), indicating limited sensitivity for prodromal MCI stages. The study lacks external validation on independent cohorts and excludes PET/CSF biomarkers that may be important for biological validation.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>multimodal machine learning decision-support (attention-based deep learning)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>Modality-specific backbones (3-layer FC nets for clinical/genetic, 3-layer CNN for imaging) produce latent features which undergo multi-head self-attention (intra-modality) and bi-directional cross-modal attention (inter-modality). Outputs concatenated and fed to a final dense classifier to predict control, MCI, or AD.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>MADDi (best model): average test accuracy 96.88% ± 3.33%, average F1-score 91.41% (across 5 initializations). Class-wise average metrics (across 5 seeds): Control: accuracy 96.66%, precision 96.78%, recall 98.88%, F1 97.81%; MCI: accuracy 96.66%, precision 90.00%, recall 70.00%, F1 76.66%; AD: accuracy 100%, precision 100%, recall 100%, F1 100%. Ablation: attention vs no-attention yields 7.9% average F1 improvement (p < 0.0001).</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Computational algorithm development and evaluation using held-out test sets (machine learning study) on observational ADNI dataset</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Multimodal experiments on the overlap set of 239 ADNI patients (165 controls, 39 MCI, 35 AD); unimodal baselines used larger modality-specific cohorts (clinical 2384, imaging 551, genetic 805).</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Key limitations include small multimodal overlap sample, class imbalance (few MCI and AD in overlap), temporal mismatches between modalities, potential overfitting with supervised SNP selection and hyperparameter tuning, lack of external validation, and limited sensitivity for MCI (prodromal) detection; authors also note that additional modalities (PET, CSF biomarkers) were not included and could change results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis", 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e10055.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e10055.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Preclinical AD pathology (latent pathology before symptoms)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preclinical Alzheimer's pathology occurring years before clinical symptom onset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper states (citing prior literature) that AD pathology develops several years prior to clinical symptoms, making early detection difficult and motivating multimodal approaches to identify prodromal states such as MCI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>pathological (amyloid/tau accumulation and other neuropathology implied)</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>A general statement that the underlying AD pathology (implicitly amyloid-beta / tau and related neurodegenerative processes in the literature) precedes clinical symptom onset by years; MCI is considered a prodromal phase that can progress to AD over years to decades.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>Paper references prior work (e.g., reference [3]) to support the statement that pathology precedes symptoms; this motivates the use of multimodal data for earlier detection and the ADNI effort to collect imaging, biomarkers, and clinical data longitudinally.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>No direct evidence or counter-evidence is provided in this paper—statement is used as background and is supported by external literature rather than new data here.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>implicated detection need for longitudinal biomarkers / imaging / multimodal monitoring</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>Implied need for early biomarkers and longitudinal tracking (e.g., imaging, biological markers, clinical assessments) to detect preclinical disease, although specific biomarkers (amyloid PET, CSF amyloid/tau) are not analyzed in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Not reported in this paper; authors note that pathology precedes symptoms, which complicates early-stage detection and motivates multimodal approaches, but no sensitivity/specificity for preclinical detection is given.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Background statement citing prior research (review / cohort studies) rather than primary evidence in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Not applicable within this paper (background citation to longitudinal studies such as ADNI and others).</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>The paper acknowledges difficulty in early detection because pathology precedes symptoms, and it does not incorporate PET or CSF biomarkers that are commonly used to detect preclinical amyloid/tau pathology; thus the current work cannot directly evaluate detection of preclinical AD.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis", 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ways toward an early diagnosis in alzheimer's disease: The alzheimer's disease neuroimaging initiative (ADNI) <em>(Rating: 2)</em></li>
                <li>Multimodal deep learning models for early detection of alzheimer's disease stage <em>(Rating: 2)</em></li>
                <li>A practical computerized decision support system for predicting the severity of alzheimer's disease of an individual <em>(Rating: 2)</em></li>
                <li>Multimodal multitask deep learning model for alzheimer's disease progression detection based on time series data <em>(Rating: 1)</em></li>
                <li>Robust hybrid deep learning models for alzheimer's progression detection <em>(Rating: 1)</em></li>
                <li>Predicting the course of alzheimer's progression <em>(Rating: 1)</em></li>
                <li>Mild cognitive impairment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-10055",
    "paper_id": "paper-10e1bcd31536ca33a1298260a630744fb80d9644",
    "extraction_schema_id": "extraction-schema-170",
    "extracted_data": [
        {
            "name_short": "Genetic SNPs (AD-related genes)",
            "name_full": "Single Nucleotide Polymorphism profiling of Alzheimer-associated genes (filtered from WGS using AlzGene list)",
            "brief_description": "Genetic variation (SNPs) in genes previously associated with Alzheimer's disease was used as a modality to predict diagnosis; the authors filtered WGS data to AD-related gene regions (AlzGene) and used supervised feature selection (Random Forest) to select ~15,000 SNPs for modeling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "cause_type": "genetic",
            "cause_description": "Genetic predisposition encoded by SNPs within genes previously associated with AD (AlzGene list of ~680 genes was used to select SNP regions). The paper treats genetic variation as a risk/associative factor rather than specifying single causal alleles (e.g., APOE4 is not singled out in this text).",
            "evidence_for_cause": "The genetic modality (neural network trained on selected SNP features) achieved unimodal test accuracy 77.78% and F1-score ~77.24% (Supplement S6 Table S3). Genetic features contributed to the multimodal classifier (MADDi) performance when combined with clinical and imaging data (final multimodal accuracy 96.88%, F1 91.41%), indicating predictive signal in the SNPs after feature selection.",
            "evidence_against_cause": "Genetic data were sparse and limited in sample size (805 patients with genetic data, only 239 patients in the full-modality overlap set). The authors note ground-truth/labeling issues for genetics (diagnosis not always contemporaneous with genetic sample), and supervised feature selection (Random Forest) can bias features toward the training labels. When clinical data were withheld, combining genetic+imaging produced worse F1 (50.07%), suggesting genetic signal alone is insufficient for robust classification in this dataset.",
            "detection_method_type": "genetic screening / SNP profiling",
            "detection_method_description": "Whole-genome sequencing-derived SNPs filtered by HWE, genotype quality, MAF, missingness, and restricted to AD-related genes (AlzGene), then supervised feature selection (Random Forest) to ~15,000 SNP features; encoded as 0/1/2 allele categories and input to a 3-layer fully connected network.",
            "detection_performance": "Unimodal genetic model: accuracy 77.78%, precision 78.37%, recall 76.92%, F1 77.24% (average across 5 seeds). In multimodal models genetic modality improved overall performance but yielded poor standalone robustness relative to imaging.",
            "study_type": "Computational classification (machine learning) using observational ADNI cohort data",
            "study_population": "805 ADNI participants with genetic WGS data (after filtering), overlap multimodal set: 239 patients (165 controls, 39 MCI, 35 AD) used for multimodal experiments.",
            "controversies_or_limitations": "Limitations include small sample size for genetics, temporal mismatch between genetic data and clinical/imaging labels, heavy preprocessing and supervised feature selection that may overfit to training labels, and the paper does not report or analyze specific causal alleles (e.g., APOE4) or biological mechanisms. Genetic modality alone gave lower performance than imaging and relied on being combined with clinical data for robust prediction.",
            "uuid": "e10055.0",
            "source_info": {
                "paper_title": "Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "MRI structural imaging (center slices)",
            "name_full": "Magnetic Resonance Imaging (structural MRI) using three central brain slices per patient",
            "brief_description": "Structural MRI was used as an imaging modality; the authors extracted three central orthogonal slices from ADNI-preprocessed brain volumes (72x72 pixels) and used a 3-layer convolutional neural network to predict diagnosis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "cause_type": "neuroanatomical / structural changes (brain atrophy as biomarker)",
            "cause_description": "The modality operationalizes neurodegenerative structural changes (e.g., atrophy patterns that correlate with AD stages). While the paper does not list specific anatomical loci, it references prior literature using hippocampal atrophy and other MRI-derived measures for AD detection.",
            "evidence_for_cause": "Unimodal imaging model achieved high performance (accuracy 92.28% reported in main text; Supplement S6 reports 92.23%), F1 ~91.83%, indicating that structural MRI contains strong discriminative signal for AD vs MCI vs control in ADNI data. Imaging alone was the best-performing single modality in unimodal experiments.",
            "evidence_against_cause": "Authors note potential information loss from using only three central slices (vs full volumes), and limited imaging sample size (551 unique patients) with repeated scans for training (3674 MRIs used for training by allowing repeats). Using limited slices may miss some disease-relevant regions; however, validation experiments (Supplement S3) showed performance differences within ~1% up to 20 additional slices, suggesting limited practical loss on this task. Nevertheless, reliance on ADNI-preprocessed standardized scans may reduce generalizability to heterogenous clinical scanning protocols.",
            "detection_method_type": "neuroimaging (structural MRI)",
            "detection_method_description": "ADNI-corrected T1 MRI volumes; three central slices per axis extracted, resized to 72x72 pixels, input to a 3-layer CNN; ADNI standard corrections (gradwarp, B1 non-uniformity, N3) applied upstream by ADNI pipeline.",
            "detection_performance": "Unimodal MRI model: accuracy ≈92.23–92.28%, precision 94.02%, recall 90.4%, F1 91.83% (averaged across seeds). In multimodal MADDi (MRI+SNP+clinical with attention) overall accuracy 96.88% and F1 91.41%; class-wise, Alzheimer's Disease cases were predicted with 100% recall in the final model, while MCI recall was lower (~70%).",
            "study_type": "Computational classification using observational ADNI imaging dataset",
            "study_population": "551 unique ADNI patients with baseline MRI (3674 MRI scans used for training when allowing repeated scans); overlap multimodal set: 239 patients used for full multimodal experiments.",
            "controversies_or_limitations": "Limitations include relatively small imaging sample and repeated-scan training strategy, potential information loss from using only central slices (although validated), and potential lack of generalizability because ADNI imaging underwent specific preprocessing steps. The model tends to misclassify some MCI as controls, indicating limited sensitivity for subtle/prodromal stages despite high accuracy for frank AD.",
            "uuid": "e10055.1",
            "source_info": {
                "paper_title": "Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Clinical cognitive assessments",
            "name_full": "Structured clinical and cognitive assessments (neurological exams, memory/executive/language scores, demographics)",
            "brief_description": "Structured clinical data (29 features including cognitive scores and neurological exam items) were used both as a standalone modality and in combination with imaging and genetics; Random Forest identified memory, executive function, and language scores as the most important clinical features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "cause_type": "clinical phenotype / symptomatic cognitive decline (used as detection markers rather than mechanistic causes)",
            "cause_description": "Cognitive deficits (measured via memory, executive function, language scores and other neurological exam items) are clinical manifestations of AD pathophysiology and serve as accessible markers for diagnosis and staging (MCI vs AD vs control). The paper treats these as predictive features, not etiologic mechanisms.",
            "evidence_for_cause": "Unimodal clinical model achieved accuracy 80.59% and F1 ~80.47% (trained on 2384 patients). Clinical features were the most important catalyst in multimodal performance: models lacking clinical data showed marked performance losses (clinical alone accuracy 82.29%, F1 78.30%; genetic+imaging together 78.33% accuracy but F1 dropped to 50.07%), indicating clinical context substantially improves model interpretation of other modalities.",
            "evidence_against_cause": "Clinical assessments are symptomatic measures and thus may lag underlying pathology; clinical variables with direct indication of AD (e.g., AD-specific medication) were removed to prevent leakage. Timing mismatches (clinical exam date vs MRI date) created ground-truth labeling challenges; clinical data frequency is lower than imaging, potentially limiting contemporaneous labels.",
            "detection_method_type": "cognitive/clinical assessments (structured scales and neurological exam items)",
            "detection_method_description": "29 structured features from neurological exams, cognitive assessments, and demographics; categorical features one-hot encoded, continuous features normalized; input to a 3-layer fully connected neural network. Random Forest was used to rank feature importance.",
            "detection_performance": "Unimodal clinical model: accuracy 80.59%, precision 80.56%, recall 80.48%, F1 80.47% (averaged). Inclusion of clinical data in multimodal MADDi markedly improved overall performance (final multimodal accuracy 96.88%, F1 91.41%); absence of clinical modality caused substantial F1 drop in some modality combinations.",
            "study_type": "Computational classification using observational ADNI clinical data",
            "study_population": "Clinical data available for 2384 ADNI patients; overlap multimodal set: 239 patients used in final multimodal experiments.",
            "controversies_or_limitations": "Clinical measures are variable in timing and frequency, may not reflect preclinical pathology, and removal of variables that explicitly encode diagnosis (e.g., AD medications) was necessary to avoid label leakage. The paper notes that clinical data helped ground other modalities, but reliance on clinical features may limit early preclinical detection before symptom onset.",
            "uuid": "e10055.2",
            "source_info": {
                "paper_title": "Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "MADDi (multimodal attention DL)",
            "name_full": "Multimodal Alzheimer's Disease Diagnosis framework (MADDi) using self-attention and cross-modal attention",
            "brief_description": "An attention-based deep learning model that integrates MRI, SNP, and structured clinical data using modality-specific backbones followed by multi-head self-attention and bi-directional cross-modal attention to capture intra- and inter-modality interactions, producing three-class (CN, MCI, AD) predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "cause_type": "detection algorithm / multimodal integration approach (not a biological cause)",
            "cause_description": "MADDi is an algorithmic approach for disease detection that models interactions between imaging, genetic, and clinical modalities with attention mechanisms to improve classification of AD stages; it does not propose a biological cause but is a detection/diagnostic method.",
            "evidence_for_cause": "The model achieved state-of-the-art performance on the ADNI multimodal three-class classification task with average held-out test accuracy 96.88% ± 3.33% and average F1 91.41% across 5 initializations; ablation experiments (100 random seeds) showed that combining self-attention and cross-modal attention produced the highest and most stable F1 distribution and outperformed architectures with no attention (7.9% average F1 improvement, p &lt; 0.0001). Comparisons to other multimodal studies in Table 3 show higher test accuracy.",
            "evidence_against_cause": "Potential overfitting and limited generalizability due to small overlap sample (239 patients) used to train multimodal model; performance variability across initializations (though narrower with full attention); model tends to misclassify MCI as controls (MCI recall ~70%), indicating limited sensitivity for prodromal MCI stages. The study lacks external validation on independent cohorts and excludes PET/CSF biomarkers that may be important for biological validation.",
            "detection_method_type": "multimodal machine learning decision-support (attention-based deep learning)",
            "detection_method_description": "Modality-specific backbones (3-layer FC nets for clinical/genetic, 3-layer CNN for imaging) produce latent features which undergo multi-head self-attention (intra-modality) and bi-directional cross-modal attention (inter-modality). Outputs concatenated and fed to a final dense classifier to predict control, MCI, or AD.",
            "detection_performance": "MADDi (best model): average test accuracy 96.88% ± 3.33%, average F1-score 91.41% (across 5 initializations). Class-wise average metrics (across 5 seeds): Control: accuracy 96.66%, precision 96.78%, recall 98.88%, F1 97.81%; MCI: accuracy 96.66%, precision 90.00%, recall 70.00%, F1 76.66%; AD: accuracy 100%, precision 100%, recall 100%, F1 100%. Ablation: attention vs no-attention yields 7.9% average F1 improvement (p &lt; 0.0001).",
            "study_type": "Computational algorithm development and evaluation using held-out test sets (machine learning study) on observational ADNI dataset",
            "study_population": "Multimodal experiments on the overlap set of 239 ADNI patients (165 controls, 39 MCI, 35 AD); unimodal baselines used larger modality-specific cohorts (clinical 2384, imaging 551, genetic 805).",
            "controversies_or_limitations": "Key limitations include small multimodal overlap sample, class imbalance (few MCI and AD in overlap), temporal mismatches between modalities, potential overfitting with supervised SNP selection and hyperparameter tuning, lack of external validation, and limited sensitivity for MCI (prodromal) detection; authors also note that additional modalities (PET, CSF biomarkers) were not included and could change results.",
            "uuid": "e10055.3",
            "source_info": {
                "paper_title": "Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Preclinical AD pathology (latent pathology before symptoms)",
            "name_full": "Preclinical Alzheimer's pathology occurring years before clinical symptom onset",
            "brief_description": "The paper states (citing prior literature) that AD pathology develops several years prior to clinical symptoms, making early detection difficult and motivating multimodal approaches to identify prodromal states such as MCI.",
            "citation_title": "",
            "mention_or_use": "mention",
            "cause_type": "pathological (amyloid/tau accumulation and other neuropathology implied)",
            "cause_description": "A general statement that the underlying AD pathology (implicitly amyloid-beta / tau and related neurodegenerative processes in the literature) precedes clinical symptom onset by years; MCI is considered a prodromal phase that can progress to AD over years to decades.",
            "evidence_for_cause": "Paper references prior work (e.g., reference [3]) to support the statement that pathology precedes symptoms; this motivates the use of multimodal data for earlier detection and the ADNI effort to collect imaging, biomarkers, and clinical data longitudinally.",
            "evidence_against_cause": "No direct evidence or counter-evidence is provided in this paper—statement is used as background and is supported by external literature rather than new data here.",
            "detection_method_type": "implicated detection need for longitudinal biomarkers / imaging / multimodal monitoring",
            "detection_method_description": "Implied need for early biomarkers and longitudinal tracking (e.g., imaging, biological markers, clinical assessments) to detect preclinical disease, although specific biomarkers (amyloid PET, CSF amyloid/tau) are not analyzed in this work.",
            "detection_performance": "Not reported in this paper; authors note that pathology precedes symptoms, which complicates early-stage detection and motivates multimodal approaches, but no sensitivity/specificity for preclinical detection is given.",
            "study_type": "Background statement citing prior research (review / cohort studies) rather than primary evidence in this paper",
            "study_population": "Not applicable within this paper (background citation to longitudinal studies such as ADNI and others).",
            "controversies_or_limitations": "The paper acknowledges difficulty in early detection because pathology precedes symptoms, and it does not incorporate PET or CSF biomarkers that are commonly used to detect preclinical amyloid/tau pathology; thus the current work cannot directly evaluate detection of preclinical AD.",
            "uuid": "e10055.4",
            "source_info": {
                "paper_title": "Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ways toward an early diagnosis in alzheimer's disease: The alzheimer's disease neuroimaging initiative (ADNI)",
            "rating": 2
        },
        {
            "paper_title": "Multimodal deep learning models for early detection of alzheimer's disease stage",
            "rating": 2
        },
        {
            "paper_title": "A practical computerized decision support system for predicting the severity of alzheimer's disease of an individual",
            "rating": 2
        },
        {
            "paper_title": "Multimodal multitask deep learning model for alzheimer's disease progression detection based on time series data",
            "rating": 1
        },
        {
            "paper_title": "Robust hybrid deep learning models for alzheimer's progression detection",
            "rating": 1
        },
        {
            "paper_title": "Predicting the course of alzheimer's progression",
            "rating": 1
        },
        {
            "paper_title": "Mild cognitive impairment",
            "rating": 1
        }
    ],
    "cost": 0.01605725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis</h1>
<p>Michal Golovanevsky ${ }^{1}$, Carsten Eickhoff ${ }^{1,2, <em>}$, and Ritambhara Singh ${ }^{1,3, </em>}$<br>${ }^{1}$ Department of Computer Science, Brown University<br>${ }^{2}$ Center for Biomedical Informatics, Brown University<br>${ }^{3}$ Center for Computational Molecular Biology, Brown University<br>*Co-corresponding Authors</p>
<h4>Abstract</h4>
<p>Alzheimer's Disease (AD) is the most common neurodegenerative disorder with one of the most complex pathogeneses, making effective and clinically actionable decision support difficult. The objective of this study was to develop a novel multimodal deep learning framework to aid medical professionals in AD diagnosis. We present a Multimodal Alzheimer's Disease Diagnosis framework (MADDi) to accurately detect the presence of AD and mild cognitive impairment (MCI) from imaging, genetic, and clinical data. MADDi is novel in that we use cross-modal attention, which captures interactions between modalities a method not previously explored in this domain. We perform multi-class classification, a challenging task considering the strong similarities between MCI and AD. We compare with previous state-of-theart models, evaluate the importance of attention, and examine the contribution of each modality to the model's performance. MADDi classifies MCI, AD, and controls with $96.88 \%$ accuracy on a held-out test set. When examining the contribution of different attention schemes, we found that the combination of cross-modal attention with self-attention performed the best, and no attention layers in the model performed the worst, with a $7.9 \%$ difference in F1-Scores. Our experiments underlined the importance of structured clinical data to help machine learning models contextualize and interpret the remaining modalities. Extensive ablation studies showed that any multimodal mixture of input features without access to structured clinical information suffered marked performance losses. This study demonstrates the merit of combining multiple input modalities via cross-modal attention to deliver highly accurate AD diagnostic decision support.</p>
<p>Available at: https://github.com/rsinghlab/MADDi</p>
<h2>1 Introduction</h2>
<h3>1.1 Background and Significance</h3>
<p>Alzheimer's Disease (AD) is the most common neurodegenerative disorder affecting approximately 5.5 million people in the United States and 44 million people worldwide [1]. Despite extensive research and advances in clinical practice, less than $50 \%$ of AD patients are diagnosed accurately for pathology and disease progression based on clinical symptoms alone[2]. The pathology of the disease occurs several years before the onset of clinical symptoms, making the disease difficult to detect at an early stage [3]. Mild cognitive impairment (MCI) is considered an AD prodromal phase, where the gradual change from MCI to AD can take years to decades [4]. As AD cannot currently be cured, but only delayed in progression, early detection of MCI before irreversible brain damage occurs is crucial for preventive care.</p>
<p>The urgent need for clinical advancement in AD diagnosis inspired the Alzheimer's Disease Neuroimaging Initiative (ADNI) to collect diverse data such as imaging, biological markers, and clinical assessment on</p>
<p>MCI and AD patients [5]. Such distinct data inputs are often referred to as individual modalities; a research problem is characterized as multimodal when it considers multiple such modalities and unimodal when it includes just one. Thanks to data collection efforts such as the one spearheaded by ADNI, it became possible to create unimodal machine learning models capable of aiding AD diagnosis, most commonly using imaging data [6-10], or clinical records [11, 12]. Recently, deep learning (DL) has shown considerable potential for clinical decision support and outperformed traditional unimodal machine learning methods in AD detection [13-15]. The major strength of DL over traditional machine learning models is the ability to process large numbers of parameters and effectively learn meaningful connections between features and labels. Even with DL's advantage, single-modality input is often insufficient to support clinical decision-making [16].</p>
<p>Alzheimer's Disease diagnosis is multimodal in nature - health care providers examine patient records, neurological exams, genetic history, and various imaging scans. Integrating multiple such inputs provides a more comprehensive view of the disease. Thus, several deep learning-based multimodal studies [17-20] have leveraged the richer information encoded in multimodal data. Despite an overall convincing performance, they all miss a crucial component of multimodal learning - cross-modal interactions. The existing methods simply concatenate features extracted from the different modalities to combine their information, limiting the model's ability to learn a shared representation [21]. In response, we propose a novel Multimodal Alzheimer's Disease Diagnosis framework (MADDi), which uses a cross-modal attention scheme [22] to integrate imaging (magnetic resonance imaging (MRI)), genetic (single nucleotide polymorphisms (SNPs)), and structured clinical data to classify patients into control (CN), MCI, and AD groups.</p>
<p>Many successful studies were conducted using the ADNI dataset [5]. Only a small subset of them used multimodal data, and an even smaller subset attempted three-class classification. In this work, we propose to use attention as a vehicle for cross-modality interactions. We show state-of-the-art performance on the challenging multimodal three-class classification task, achieving $96.88 \%$ average test accuracy across 5 random model initializations. Next, we investigated the contribution of each modality to the overall model. While for unimodal models, images produced the most robust results ( $92.28 \%$ accuracy), when we combined all three data inputs, we found that the clinical modality complements learning the best. Lastly, since our method utilizes two different types of neural network attention, we investigated the contribution of each type and found significant performance improvements when using attention layers over no attention. Through our experiments, we were able to highlight the importance of capturing interactions between modalities.</p>
<h1>2 Methods and Materials</h1>
<h3>2.1 Data Description</h3>
<p>The data used in this study were obtained from the ADNI database (https://adni.loni.usc.edu/), which provides imaging, clinical, and genetic data for over 2220 patients spanning four studies (ADNI1, ADNI2, ADNI GO, and ADNI3). The primary goal of ADNI has been to test whether combining such data can help measure the progression of MCI and AD. Our study follows the common practice of using patient information from only ADNI1, 2, and GO, since ADNI 3 is still an ongoing study expected to conclude in 2022. To capture a diverse range of modalities, we focused on patients with imaging, genetic, and clinical data available. We trained unimodal models on the full number of participants per modality. For our multimodal architecture, we only used those patients that had all three modalities recorded (referred to as the overlap dataset). The number of participants in each group is detailed in Table 1.</p>
<p>Table 1: Number of participants in each modality and their diagnosis. This table shows the number of participants in each modality and further separates the participants into their diagnoses. The Overlap section refers to patients that had all three modalities recorded.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Total Participants</th>
<th style="text-align: left;">Control</th>
<th style="text-align: left;">Mild Cognitive Impairment</th>
<th style="text-align: left;">Alzheimer's Disease</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Clinical</td>
<td style="text-align: left;">2384</td>
<td style="text-align: left;">942</td>
<td style="text-align: left;">796</td>
<td style="text-align: left;">646</td>
</tr>
<tr>
<td style="text-align: left;">Imaging</td>
<td style="text-align: left;">551</td>
<td style="text-align: left;">278</td>
<td style="text-align: left;">123</td>
<td style="text-align: left;">150</td>
</tr>
<tr>
<td style="text-align: left;">Genetic</td>
<td style="text-align: left;">805</td>
<td style="text-align: left;">241</td>
<td style="text-align: left;">318</td>
<td style="text-align: left;">246</td>
</tr>
<tr>
<td style="text-align: left;">Overlap</td>
<td style="text-align: left;">239</td>
<td style="text-align: left;">165</td>
<td style="text-align: left;">39</td>
<td style="text-align: left;">35</td>
</tr>
</tbody>
</table>
<h1>2.1.1 Ground Truth Labels</h1>
<p>Since ADNI's launch in 2003, more participants have been added to each phase of the study, and the disease progression of initial participants is continuously followed. Over time, some patients that were initially labeled as control (CN) and mild cognitive impairment (MCI) had a change in diagnosis as their disease progressed. While some patients had as many as 16 MRI scans since the start of the study, clinical evaluations were collected much less frequently, and genetic testing was only performed once per patient. Thus, combining three modalities per patient was a unique challenge as, at times, there were contradictory diagnoses, making the ground truth diagnosis unclear. For our overlap dataset, we used the latest MRI and clinical evaluation for each patient and the most recent diagnosis. Several studies focused on using time-series data to track the progression of the disease [17, 18, 23-25]. However, we aimed to accurately classify patients into groups at the most recent state of evaluation so our method can be generalized to patients who are not part of long-term observational studies.</p>
<h3>2.1.2 Clinical Data Pre-processing</h3>
<p>For clinical data, we use 2384 patients' data from the neurological exams (e.g., balance test), cognitive assessments (e.g., memory tests), and patient demographics (e.g., age). The clinical data is quantitative, categorical, or binary, totaling 29 features. We removed any feature that could encode direct indication of AD (e.g., medication that a patient takes to manage extant AD). A full list of features can be found in Supplement S2. Categorical data were converted to features using one-hot encoding, and continuous-valued features were normalized.</p>
<h3>2.1.3 Genetic Data Pre-processing</h3>
<p>The genetic data consists of the whole genome sequencing (WGS) data from 805 ADNI participants by Illumina's non-Clinical Laboratory Improvement Amendments (non-CLIA). The resulting variant call files (VCFs) were generated by ADNI using Burrows-Wheeler Aligner and Genome Analysis Toolkit in 2014. Each subject has about 3 million SNPs in the raw VCF file generated. However, not all detected SNPs are informative in predicting AD. We followed the established pre-processing steps detailed in [20] to reduce the number of SNPs and keep only the relevant genetic components. After such filtering (detailed further in Supplement S1), we had 547,863 SNPs per patient. As we only have 805 patients with genetic data, we were left with a highly sparse matrix. We used a Random Forest classifier as a supervised feature selection method to determine which are the most important features, reducing our feature space to roughly 15,000 SNPs. Note that data points used for model testing were not seen by the classifier. While the result was still sparse, we found that this level was a reasonable stopping point as determined by the experiment detailed in Supplementary S1. The final data were grouped into three categories: no alleles, any one allele, or both alleles present.</p>
<h1>2.1.4 Imaging Data Pre-processing</h1>
<p>The imaging data in this study consist of cross-sectional MRI data corresponding to the first baseline screenings from ADNI1 (551 patients). The data publisher has standardized the images to eliminate the nonlinearities caused by the scanners from different vendors. From each brain volume, we used three slices corresponding to the center of the brain in each dimension. An example input is shown in Figure 1. Further details on the ADNI pre-processing steps and experiments justifying the use of three images per patient can be found in Supplement S3.</p>
<p>Figure 1: Imaging input example. The imaging model took as input three slices from the center of the MRI brain volume, that were uniformly shaped to $72 \times 72$ pixels.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<h3>2.1.5 Finalizing the Training Dataset</h3>
<p>To train our multimodal architecture, we used 239 patients that had data available from all three modalities. The overlap dataset was chosen out of the original data mentioned above - imaging (551 patients), SNP (805 patients), and clinical (2284 patients) to predict AD stages. While the SNP data was unique per patient, the clinical and imaging data appeared multiple times. To ensure a proper merger, we used the timestamps in the clinical data and matched it to the closest MRI scan date. Next, we used the most recent evaluation on a patient to avoid repeating patients. The patients' demographic information is shown in Table 2.</p>
<p>Table 2: Participants' demographic information. This table shows the number of participants in each diagnosis group along with the percent of females and the average age of each group.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Group</th>
<th style="text-align: left;">Participants (n)</th>
<th style="text-align: left;">Female Sex (\%)</th>
<th style="text-align: left;">Mean Age (years)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Control</td>
<td style="text-align: left;">165</td>
<td style="text-align: left;">53.9</td>
<td style="text-align: left;">77.8</td>
</tr>
<tr>
<td style="text-align: left;">Mild Cognitive Impairment</td>
<td style="text-align: left;">39</td>
<td style="text-align: left;">34.2</td>
<td style="text-align: left;">76.6</td>
</tr>
<tr>
<td style="text-align: left;">Alzheimer's Disease</td>
<td style="text-align: left;">35</td>
<td style="text-align: left;">31.4</td>
<td style="text-align: left;">78.1</td>
</tr>
</tbody>
</table>
<h3>2.2 Multimodal Framework</h3>
<p>The proposed framework, MADDi, is shown in Figure 2. The model receives a patient's pre-processed clinical, genetic, and imaging data and outputs the corresponding diagnosis (CN, AD, or MCI). Following</p>
<p>the input, there are modality-specific neural network architecture backbones developed in the single modality setting (further detailed in the Performance of Unimodal Models Section). For clinical and genetic data, this is a three-layer fully connected network, and for imaging data, we have a three-layer convolutional neural network. The output of those layers then enters a multi-headed self-attention layer, which allows the inputs to interact with each other and find what features should be paid most attention to within each modality. This layer is followed by a cross-modal bi-directional attention layer [22], which performs a similar calculation to self-attention but across different pairs of modalities. The purpose of cross-modal attention is to model an interaction between modalities; for example, clinical features may help reinforce what the imaging features tell the model and thus lead to more robust decision making. Both attention types are rigorously defined in the Neural Network Attention Section. The last step concatenates the output of the parallel attention computations and feeds it into a final dense layer that makes the prediction.</p>
<p>Figure 2: Model Architecture. (a) Data inputs - clinical data (demographics, memory tests, balance score, etc.), genetic (SNPs), and imaging (MRI scans). (b) The input sources are combined and fed into a fully connected (FC) neural network architecture for genetic and clinical modalities and a convolutional neural network (CNN) for imaging data. (c) Using the obtained features from the neural networks, a self-attention layer reinforces any inner-modal connections. (d) Then, each modality pair is fed to a bi-directional crossmodal attention layer which captures the interactions between modalities. (e) Lastly, the outputs are concatenated and passed into a decision layer for classification into the (f) output Alzheimer's stages (CN, MCI, and AD).</p>
<p>MADDi - Multimodal Alzheimer's Disease Diagnosis framework
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<h1>2.3 Experimental Design</h1>
<h3>2.3.1 Neural Network Attention</h3>
<p>As a part of our experimental design, we evaluate the importance of attention in our model. Previous methods $[17,18,20]$ explored the value that DL brings to automating AD diagnosis. We build on top of previous multimodal DL frameworks and examine the need for inter-modal interactions through attention. Thus, we used the same framework but toggled the presence of attention based on four criteria: self-attention and cross-modal attention, just self-attention, just cross-modal attention, and no attention. The different types of attention are introduced in the following.</p>
<p>Generalized attention: Attention is a mechanism that captures the relationship between two states and highlights the features that contribute most to decision-making. An attention layer takes as input queries and keys of dimension $d_{k}$, and values of dimension $d_{v}$. A key is the label of a feature used to distinguish it from other features, and a query is what checks all available keys and selects the one that matches best. We compute the dot products of the query with all keys, divide each by the square root of $d_{k}$, and apply a Softmax function, which converts a vector of numbers into a vector of probabilities, to obtain the weights on the values:</p>
<p>$$
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$</p>
<p>Following the success of the Transformer [26], we use the multi-head attention module, which allows the model to jointly attend to information from different representation subspaces at different positions.</p>
<p>Self-Attention: For self-attention mechanisms, queries, keys, and values are equal. The self-attention mechanism allows us to learn the interactions among the different parts of an input("self") and determine which parts of the input are relevant for making predictions ("attention"). In our case, the prior neural network layers produce in parallel three latent feature matrices for each modality that act as the keys, queries, and values: imaging matrix $I$, genetic matrix $G$, and clinical matrix $C$. Self-attention, in our terms, refers to attention computation done within the same modality. Thus the self-attention module performs the following operations:</p>
<p>$$
\begin{aligned}
&amp; \text { self-attention }(I \rightarrow I) \
&amp; \text { self-attention }(G \rightarrow G) \
&amp; \text { self-attention }(C \rightarrow C)
\end{aligned}
$$</p>
<p>Cross-modal attention: In each bi-directional cross-modal attention layer [22], there are two unidirectional cross-modal attention sub-layers: one from modality 1 to modality 2 and one from modality 2 to modality 1. In our case, the cross-modal attention layer takes the output of each self-attention computation: image self-attention output $I_{s}$, genetic self-attention output $G_{s}$, and clinical self-attention output $C_{s}$. Thus the cross-modal attention module performs the following operations:</p>
<p>$$
\begin{aligned}
&amp; \text { concatenation }(\text { cross-modal attention }\left(I_{s} \rightarrow C_{s}\right), \text { cross-modal attention }\left(C_{s} \rightarrow I_{s}\right)) \
&amp; \text { concatenation }\left(\text { cross-modal attention }\left(C_{s} \rightarrow G_{s}\right), \text { cross-modal attention }\left(G_{s} \rightarrow C_{s}\right)\right) \
&amp; \text { concatenation }(\text { cross-modal attention }\left(G_{s} \rightarrow I_{s}\right), \text { cross-modal attention }\left(I_{s} \rightarrow G_{s}\right))
\end{aligned}
$$</p>
<p>Lastly, we created a model with no attention module at all, where, following the neural network layers, we directly proceed to concatenate and produce output through the final dense layer. This setting represents the previous state-of-the-art methods used for integrating multimodal datasets for our task.</p>
<h1>2.3.2 Unified Hyperparameter Tuning Scheme</h1>
<p>The modality-specific neural network part of MADDi was predetermined based on the hyperparameter tuning done on each unimodal model (Supplement S4). We did not use the overlapping test set during hyperparameter tuning was done. To fairly evaluate the need for attention, we tuned using the same hyperparameter grid for each of the other experimental models. Meaning, that each model (self-attention only, the crossmodal attention only, and the model with no attention) gets its own set of best hyperparameters. We first randomly split our 239 patients into a training set ( $90 \%$ ) and a held-out testing set ( $10 \%$ ). We chose a 90 - 10 split for consistency with all the papers we compared against (shown in 3). We designed a 3 -fold cross-validation scheme and took the parameters that gave the best average cross-validation accuracy. Next, we used 5 random seeds to give the model multiple attempts at initialization. The best initialization was determined based on the best training performance on the full train and validations set (i.e., validation was added into training). This pipeline was repeated to find until we found the best parameters for each baseline.</p>
<h1>2.3.3 Evaluation Metrics</h1>
<p>The following metrics were calculated for each model: accuracy, precision (positive predictive value), recall (sensitivity), and F1-score (harmonic mean of recall and precision). F1-score was the primary performance metric for evaluating our baselines. Accuracy was used to evaluate our best model against previous papers, as that was the metric most commonly reported on this task. The metric calculations are detailed in Supplement S5.</p>
<h2>3 Results</h2>
<h3>3.1 Performance of Unimodal Models</h3>
<p>To demonstrate the success of our multimodal framework, MADDi, we first examined the performance of a single-modality model. Our evaluation pipeline was consistent across all modalities in that we used a neural network and then tuned hyperparameters to find the best model. We split each modality into training ( $90 \%$ ) and testing ( $10 \%$ ) data, where the testing set was not seen until after the best parameters were chosen using the average accuracy across 3-fold cross-validation. The reported test accuracies are averaged across five random initializations, which remained consistent across all modalities. The results are summarized in Figure 3 (and in Supplement S6 Table S3). For the clinical unimodal model, we created a neural network with three fully connected layers (other hyperparameters can be found in Supplement S4). The best model yielded $80.5 \%$ average accuracy across five random seeds. The model was trained on 2384 patients. For imaging results, we created a convolutional neural network with three convolution layers. The best model yielded $92.28 \%$ average accuracy across five random seeds. The model was trained on 551 patients, but we allowed for patient repetitions as we found that only using 551 images was not enough to train a successful model. Thus, we had 3674 MRI scans from 551 patients (some patients repeated up to 16 times). We selected our testing set such that it has 367 unique MRIs ( $10 \%$ of training), and we do not allow for any repeating patients in the testing set. We only allowed repetition during training, and no patient was included in both training and testing sets. For genetic data, we created a neural network with three fully connected layers. The best model yielded $77.78 \%$ average accuracy across five random seeds. The model was trained on 805 unique patients.</p>
<p>Figure 3: Metric evaluation of unimodal models. The graph shows all four evaluation metrics for the best neural network model for each modality - imaging, clinical, and genetic. The imaging model gives the best performance overall, whereas the genetic modality gives the lowest performance with highest variation.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<h1>3.2 Performance of Multimodal Models</h1>
<p>Table 3 contrasts the performance and architecture of MADDi with state-of-the-art multimodal approaches from the literature. Note that due to the differences in dataset characteristics and multitask settings, it was not possible to directly compare performance among methods that only report binary classification or use a single modality. Thus, we only report studies that used two or more modalities and did 3-class classification. For our proposed method we report the average accuracy across 5 random initializations on a held-out test set. Therefore, we also use the test (as opposed to cross-validation) accuracy from other studies. Bucholc et al. [19] used support vector machines to classify patients into normal, questionable cognitive impairment (QCI), and mild/moderate AD, comparable to our definitions of control, MCI, and AD. They reported $82.9 \%$ test accuracy but did not rely on deep learning. Fang et al. [27] used Gaussian discriminative component analysis as a method of multi-class classification using two different imaging modalities, achieving $66.29 \%$ accuracy on the test set. Abuhmen et al. and El-Sappagh et al. [17, 18] both used MRI, PET, and various health record features. The key difference between the two is that El-Sappagh et al. considered a four-class classification of control, AD, stable MCI (patients who do not progress to AD), and progressive MCI. Since they did not report three-class classification, we could not directly compare to their work, but note that they achieved $92.62 \%$ accuracy on the 4 -class task. Both methods utilized deep learning, but they focused more on disease progression diagnosis with time-series data rather than static disease diagnosis. Venugopalan et al. [20] were most similar to our study in structure, modality choice, and pre-processing. They, too, did not utilize the recent advancement of attention-based multimodal learning, which is where our architecture stands out. At $96.88 \% \pm 3.33 \%$ average accuracy, MADDi defined state-of-the-art performance on the multimodal three-class classification task.</p>
<p>Table 3: Comparison with related studies. This table shows the comparison between our study and 5 other previous studies that attempted to solve a similar problem to ours. MADDi preformed with $96.88 \%$ average accuracy and $91.41 \%$ average F1-Score across 5 random initializations on a held-out test set, achieving state-of-the-art performance on the multimodal three-class classification task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Study</th>
<th style="text-align: left;">Modality</th>
<th style="text-align: left;">Accuracy</th>
<th style="text-align: left;">F1-Score</th>
<th style="text-align: left;">Method</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Bucholc et al. [19], 2019</td>
<td style="text-align: left;">MRI, PET, Clinical</td>
<td style="text-align: left;">$82.90 \%$</td>
<td style="text-align: left;">Not Reported</td>
<td style="text-align: left;">SVM</td>
</tr>
<tr>
<td style="text-align: left;">Fang et al. [27], 2020</td>
<td style="text-align: left;">MRI, PET</td>
<td style="text-align: left;">$66.29 \%$</td>
<td style="text-align: left;">Not Reported</td>
<td style="text-align: left;">GDCA</td>
</tr>
<tr>
<td style="text-align: left;">Abuhmed et al. [18], 2021</td>
<td style="text-align: left;">MRI, PET, Clinical</td>
<td style="text-align: left;">$86.08 \%$</td>
<td style="text-align: left;">$87.67 \%$</td>
<td style="text-align: left;">Multivariate BiLSTM</td>
</tr>
<tr>
<td style="text-align: left;">Venugopalan et al. [20], 2021</td>
<td style="text-align: left;">MRI, SNP, Clinical</td>
<td style="text-align: left;">$78 \%$</td>
<td style="text-align: left;">$78 \%$</td>
<td style="text-align: left;">DL + RF</td>
</tr>
<tr>
<td style="text-align: left;">MADDi</td>
<td style="text-align: left;">MRI, SNP, Clinical</td>
<td style="text-align: left;">$\mathbf{9 6 . 8 8 \%}$</td>
<td style="text-align: left;">$\mathbf{9 1 . 4 1 \%}$</td>
<td style="text-align: left;">DL + Attention</td>
</tr>
</tbody>
</table>
<h3>3.3 Model Robustness</h3>
<p>To definitively conclude that both self-attention and cross-modal attention are necessary additions to the model, we ablated the attention schemes in three conditions (self-attention only, cross-modal attention only, and the model with no attention) on the held-out test set using the best parameters for each respective model. To demonstrate that our success was not dependent on initialization, we used 100 different random seeds and recorded the distribution of F1-scores on the testing set. For more information on our test sample selection, refer to Supplement . Figure 4 (and Table S4 in Supplement Section S7) shows that self-attention and crossmodal attention layers together have the narrowest distribution, with the highest median F1-score. The next best distribution is the cross-modal attention layer alone, which has a slightly wider distribution but still the second-highest median F1-score. The success of the two methods involving cross-modal attention becomes apparent and provides strong evidence that capturing interactions between modalities positively influences the model's decision-making. All three models that utilize attention achieved $100 \%$ F1-score for at least one initialization, while the model with no attention layers only reached at most $92.8 \%$ F1-score. Furthermore,</p>
<p>the performance of our final model was $7.9 \%$ average F1-Score higher than a model with no attention, and was significant ( $p-$ value $&lt;0.0001$, two-sample Z-test) - providing further evidence that attention was beneficial for multimodal data integration.</p>
<p>Figure 4: F1-Score Distribution for different attention-based and attention-free baselines. Box plots showing the F1-score distribution across 100 random seeds to demonstrate the value of attention in a deep learning model. The F1-scores were calculated from a held-out test set. The horizontal line represents the median F1-score, and the boxes represent the first and third quartiles. The whiskers represent quartile 1 - ( 1.5 x interquartile range) and quartile $3+(1.5 \mathrm{x}$ interquartile range). The dots represent the individual F1-scores for each model. ${ }^{<em> * * </em>}: \mathrm{P} \leq 0.0001$. The figure demonstrates that the combination of self-attention with cross-modal attention performs the best with the most narrow variation.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Using the self-attention and cross-attention model (MADDi), we investigated the performance of the model with respect to the individual classes as seen in Table 4. We report metrics averaged across 5 random initializations. We find that, regardless of the initialization, the model is extremely accurate at identifying AD patients. However, for some cases, it tends to mistake MCI patients for controls. We hypothesize that, since our data does not include different stages of MCI, it may have MCI patients with mild symptoms that could be mistaken for controls by the model. These observations can be seen in detail through 5 confusion matrices from the 5 initializations in the Supplement Section S8 Figure S4.</p>
<p>Table 4: Investigating performance for each individual class. This table shows the performance metrics averaged across 5 random initializations of MADDi on each class (control, Moderate Cognitive Impairment, and Alzheimer's Disease). We observe that Alzheimer's Disease is predicted correctly regardless of initialization and the only mistake the model makes is misclassifying MCI patients as control patients.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Control</td>
<td style="text-align: center;">$96.66 \%$</td>
<td style="text-align: center;">$96.78 \%$</td>
<td style="text-align: center;">$98.88 \%$</td>
<td style="text-align: center;">$97.81 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Moderate Cognitive Impairment</td>
<td style="text-align: center;">$96.66 \%$</td>
<td style="text-align: center;">$90.00 \%$</td>
<td style="text-align: center;">$70.00 \%$</td>
<td style="text-align: center;">$76.66 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Alzheimer's Disease</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
</tbody>
</table>
<h1>3.4 Modality Importance</h1>
<p>Finally, we investigated the importance of each modality to bring more transparency into the model's decision-making and motivate future data collection efforts. Knowing how valuable each modality is to disease classification and what happens when it is excluded from the experiment can shed light on where to focus scientific resources. While every study participant had at least some clinical data available, only a few hundred patients had MRI scans. To fairly compare each modality's importance to the model, we performed our analyses on the same exact patients. Thus, we evaluated the contribution of the modalities on the overlap patient set (detailed in Figure 5). For single modalities, we only used self-attention. For a pair of modalities, we used both self-attention and cross-modal attention. We performed hyperparameter tuning for each model to ensure fair evaluation, with all the parameters provided in Supplement S4 Table S1. As seen in Figure 5, combining the three modalities performs the best across all evaluation metrics. A full table with the numeric results can be found in Supplement S10 Table S6. The interesting discovery here was the clinical modality contribution to this performance. While the use of two modalities was better than one in most cases, when clinical data was withheld, we saw a significant drop in performance; clinical data alone achieved $82.29 \%$ accuracy and $78.30 \%$ F1-score, whereas genetic and imaging together achieved $78.33 \%$ accuracy and dropped to $50.07 \%$ F1-score. These results suggest that the clinical dataset is an important catalyst modality for AD prediction. We hypothesize that this empirical merit stems from the fact that clinical features offer the necessary patient context that grounds the additional modalities such as vision or omics information and helps the model in their effective representation and interpretation.</p>
<p>To further investigate the clinical data, we used a Random Forest classifier (which fits the clinical training data to the diagnosis labels in a supervised manner) to find the most dominant features from the clinical modality: memory score, executive function score, and language score. A full list of features in order of importance can be found in Supplement S2 Figure S1.</p>
<p>Figure 5: Evaluation of modality importance. This figure evaluates the possible combinations of modalities. The metrics were calculated as an average of five random initializations on a held-out test set. The combination of the three modalities performs the best across all evaluation metrics. Excluding the clinical modality causes a drop in performance, demonstrating the value of clinical information.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<h1>4 DISCUSSION</h1>
<h3>4.1 Clinical Importance and Implications</h3>
<p>Detecting AD accurately is clinically important as it is the 6th leading cause of death in the United States and is the most common cause of dementia [1]. Without treatment or other changes to the trajectory, aggregate care costs for people with AD are projected to increase from $\$ 203$ billion in 2013 to $\$ 1.2$ trillion in 2050 [2]. Despite studies such as ADNI collecting various imaging, genetic, and clinical data to improve our understanding of the underlying disease processes, most computational techniques still focus on using just a single modality to aid disease diagnosis. Our state-of-the-art model allows for an effective integration scheme of three modalities and can be expanded as more data sources become available.</p>
<h3>4.2 Future Extensions</h3>
<p>The proposed model architecture can be used in other multimodal clinical applications, such as cancer detection [28, 29]. As the efforts to make health care data more broadly available continue to increase, we believe that our model will help aid the diagnostic process. The framework we propose does not rely on modality-specific processing within the model itself. Thus, our future work will include other data (PET scans, clinical notes, biomarkers, etc.). While it is straightforward to simply interchange the current modalities with new ones and only use three modalities at a time, we plan on expanding our work beyond this current level as there is no rigid constraint on the number of modalities used with the given codebase. Furthermore, similarly to the task El-Sappagh et al. [17] explored, we will extend our task to more than three-class classification and use our work to detect different types of MCI (stable MCI and progressive MCI). This will help better understand AD progression and delay the change from MCI to AD.</p>
<h3>4.3 Limitations</h3>
<p>When creating our unimodal performance baselines, we often struggled with finding the ground truth labels for the genetic data. While every patient had a diagnosis attached to an MRI scan, and most of the clinical exams also had a diagnosis listed, genetic data did not. Out of the 808 patients with genetic data, we used 805 patients where diagnosis on their most recent MRI scan agreed with their clinical diagnosis. Thus, we proceeded with 805 patients to eliminate any error in the ground truth labeling. This gap is natural, as a patient may have had a more recent MRI that changed the diagnosis, leaving the recent clinical evaluation outdated (and vice versa).</p>
<p>During pre-processing of the MRI images, we chose to use the middle slice of the brain rather than the full brain volume. This could mean that our model did not see certain areas of the brain. When running unimodal experiments on the MRI data, the performance remained the same (within one percent) when using just the middle slice of the brain compared to the full brain volume, shown in the Supplement S3. Since processing thousands of slices per patient is much more computationally expensive, we proceeded with this simplification. While on our task, there was no significant difference in performance; in other applications integrating the full brain volumes into the model could further increase performance.</p>
<h2>5 Conclusions</h2>
<p>In this work, we presented a Multimodal Alzheimer's Disease Diagnosis framework (MADDi), which uses attention-based deep learning to detect Alzheimer's disease. The performance of MADDi was superior to that of existing multimodal machine learning methods and was shown to be consistently high regardless of</p>
<p>chance initialization. We offer three distinct contributions: integrating multimodal inputs, multi-task classification, and cross-modal attention for capturing interactions. Many existing multimodal DL models simply concatenate each modality's features despite substantial differences in feature spaces. We employed attention modules to address this problem; self-attention reinforces the most important features extracted from the neural network backbones, and cross-modality attention [22] reinforces relationships between modalities. Combining the two attention modules resulted in a $96.88 \%$ accuracy and defined state-of-the-art on this task. Overall, we believe that our approach demonstrates the potential for an automated and accurate deep learning method for disease diagnosis. We hope that in the future, our work can be used to integrate multiple modalities in clinical settings and introduce the highly effective attention-based models to the medical community.</p>
<h1>6 Acknowledgements</h1>
<p>We thank Alzheimer's Disease Neuroimaging Initiative (ADNI) database (https://adni.loni.usc. edu/) for providing data for this study. For a complete list of ADNI investigators, refer to: https:// adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List. pdf. We acknowledge Pinar Demetci for her help in the discussion of genetic data pre-processing.</p>
<h2>7 Funding Statement</h2>
<p>This research received no specific grant from any funding agency in the public, commercial or not-for-profit sectors.</p>
<h2>8 Competing Interests Statement</h2>
<p>The authors have no competing interests to declare.</p>
<h2>9 Contribution Statement</h2>
<p>All authors contributed to the design of the methodology and the experiments. M.G. implemented the data pre-processing, modeling, and data analysis. All authors discussed the results and contributed to the final manuscript.</p>
<h2>10 Data Availability</h2>
<p>The data underlying this article were provided by Alzheimer's Disease Neuroimaging Initiative (ADNI) database (https://adni.loni.usc.edu/) by permission. Data will be shared on request to ADNI.</p>
<h2>References</h2>
<p>[1] E. Naqvi, Alzheimer's disease statistics, Jun. 2017. [Online]. Available: https://alzheimersnewstoday. com/alzheimers-disease-statistics/.
[2] W. Thies and L. Bleiler, 2013 alzheimer's disease facts and figures - wiley online library, Mar. 2013. [Online]. Available: https://alz-journals.onlinelibrary.wiley.com/doi/10. 1016/j.jalz.2013.02.003.</p>
<p>[3] S. Iddi, D. Li, P. S. Aisen, M. S. Rafii, W. K. Thompson, and M. C. Donohue, Predicting the course of alzheimer's progression - brain informatics, Jun. 2019. [Online]. Available: https : //braininformatics.springeropen.com/articles/10.1186/s40708-019-0099-0#citeas.
[4] R. C. Petersen, Mild cognitive impairment, Apr. 2016. [Online]. Available: https://www.ncbi . nlm.nih.gov/pmc/articles/PMC5390929/.
[5] S. G. Mueller, M. W. Weiner, L. J. Thal, R. C. Petersen, C. R. Jack, W. Jagust, J. Q. Trojanowski, A. W. Toga, and L. Beckett, "Ways toward an early diagnosis in alzheimer's disease: The alzheimer's disease neuroimaging initiative (adni)," Alzheimer's \&amp; Dementia, vol. 1, no. 1, pp. 55-66, 2005.
[6] P. Forouzannezhad, A. Abbaspour, C. Li, C. Fang, U. Williams, M. Cabrerizo, A. Barreto, J. Andrian, N. Rishe, R. E. Curiel, et al., "A gaussian-based model for early detection of mild cognitive impairment using multimodal neuroimaging," Journal of neuroscience methods, vol. 333, p. 108 544, 2020.
[7] G. Uysal and M. Ozturk, "Hippocampal atrophy based alzheimer's disease diagnosis via machine learning methods," Journal of Neuroscience Methods, vol. 337, p. 108 669, 2020.
[8] S. I. Dimitriadis, D. Liparas, M. N. Tsolaki, A. D. N. Initiative, et al., "Random forest feature selection, fusion and ensemble strategy: Combining multiple morphological mri measures to discriminate among healhy elderly, mci, cmci and alzheimer's disease patients: From the alzheimer's disease neuroimaging initiative (adni) database," Journal of neuroscience methods, vol. 302, pp. 14-23, 2018.
[9] I. Beheshti, H. Demirel, H. Matsuda, A. D. N. Initiative, et al., "Classification of alzheimer's disease and prediction of mild cognitive impairment-to-alzheimer's conversion from structural magnetic resource imaging using feature ranking and a genetic algorithm," Computers in biology and medicine, vol. 83, pp. 109-119, 2017.
[10] M. Dyrba, F. Barkhof, A. Fellgiebel, M. Filippi, L. Hausner, K. Hauenstein, T. Kirste, S. J. Teipel, and E. S. Group, "Predicting prodromal alzheimer's disease in subjects with mild cognitive impairment using machine learning classification of multimodal multicenter diffusion-tensor and magnetic resonance imaging data," Journal of Neuroimaging, vol. 25, no. 5, pp. 738-747, 2015.
[11] S. El-Sappagh, H. Saleh, R. Sahal, T. Abuhmed, S. R. Islam, F. Ali, and E. Amer, "Alzheimer's disease progression detection model based on an early fusion of cost-effective multimodal data," Future Generation Computer Systems, vol. 115, pp. 680-699, 2021.
[12] J. Zhou, J. Liu, V. A. Narayan, J. Ye, A. D. N. Initiative, et al., "Modeling disease progression via multi-task learning," NeuroImage, vol. 78, pp. 233-248, 2013.
[13] H. Wang, Y. Shen, S. Wang, T. Xiao, L. Deng, X. Wang, and X. Zhao, "Ensemble of 3d densely connected convolutional network for diagnosis of mild cognitive impairment and alzheimer's disease," Neurocomputing, vol. 333, pp. 145-156, 2019, ISSN: 0925-2312. DOI: https://doi.org/10 . 1016/j.neucom.2018.12.018. [Online]. Available: https://www.sciencedirect . com/science/article/pii/S0925231218314723.
[14] G. Uysal and M. Ozturk, "Hippocampal atrophy based alzheimer's disease diagnosis via machine learning methods," Journal of Neuroscience Methods, vol. 337, p. 108 669, 2020, ISSN: 0165-0270. DOI: https://doi.org/10.1016/j.jneumeth.2020.108669. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0165027020300911.</p>
<p>[15] K. Kruthika, Rajeswari, and H. Maheshappa, "Multistage classifier-based approach for alzheimer's disease prediction and retrieval," Informatics in Medicine Unlocked, vol. 14, pp. 34-42, 2019, ISSN: 2352-9148. DOI: https://doi.org/10.1016/j.imu.2018.12.003. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S2352914818301758.
[16] S. R. Stahlschmidt, B. Ulfenborg, and J. Synnergren, "Multimodal deep learning for biomedical data fusion: A review," Briefings in Bioinformatics, vol. 23, no. 2, bbab569, 2022.
[17] S. El-Sappagh, T. Abuhmed, S. Riazul Islam, and K. S. Kwak, "Multimodal multitask deep learning model for alzheimer's disease progression detection based on time series data," Neurocomputing, vol. 412, pp. 197-215, 2020, ISSN: 0925-2312. DOI: https : / / doi . org / 10.1016 / j.neucom.2020.05.087. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S0925231220309383.
[18] T. Abuhmed, S. El-Sappagh, and J. M. Alonso, "Robust hybrid deep learning models for alzheimer's progression detection," Knowledge-Based Systems, vol. 213, p. 106 688, 2021, ISSN: 0950-7051. DOI: https://doi.org/10.1016/j.knosys.2020.106688. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S0950705120308170.
[19] M. Bucholc, X. Ding, H. Wang, D. H. Glass, H. Wang, G. Prasad, L. P. Maguire, A. J. Bjourson, P. L. McClean, S. Todd, D. P. Finn, and K. Wong-Lin, "A practical computerized decision support system for predicting the severity of alzheimer's disease of an individual," Expert Systems with Applications, vol. 130, pp. 157-171, 2019, ISSN: 0957-4174. DOI: https://doi.org/10.1016/j.eswa. 2019.04.022. [Online]. Available: https://www.sciencedirect.com/science/ article/pii/S0957417419302520.
[20] J. Venugopalan, L. Tong, H. R. Hassanzadeh, and M. D. Wang, "Multimodal deep learning models for early detection of alzheimer's disease stage," Scientific reports, vol. 11, no. 1, pp. 1-13, 2021.
[21] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng, "Multimodal deep learning," in ICML, 2011.
[22] H. H. Tan and M. Bansal, "Lxmert: Learning cross-modality encoder representations from transformers," ArXiv, vol. abs/1908.07490, 2019.
[23] R. Guerrero, A. Schmidt-Richberg, C. Ledig, T. Tong, R. Wolz, D. Rueckert, A. D. N. I. (ADNI, et al., "Instantiated mixed effects modeling of alzheimer's disease markers," NeuroImage, vol. 142, pp. 113-125, 2016.
[24] B. M. Jedynak, A. Lang, B. Liu, E. Katz, Y. Zhang, B. T. Wyman, D. Raunig, C. P. Jedynak, B. Caffo, J. L. Prince, et al., "A computational neurodegenerative disease progression score: Method and results with the alzheimer's disease neuroimaging initiative cohort," Neuroimage, vol. 63, no. 3, pp. 1478-1486, 2012.
[25] W.-Y. W. Yau, D. L. Tudorascu, E. M. McDade, S. Ikonomovic, J. A. James, D. Minhas, W. Mowrey, L. K. Sheu, B. E. Snitz, L. Weissfeld, et al., "Longitudinal assessment of neuroimaging and clinical markers in autosomal dominant alzheimer's disease: A prospective cohort study," The Lancet Neurology, vol. 14, no. 8, pp. 804-813, 2015.
[26] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," ArXiv, vol. abs/1706.03762, 2017.</p>
<p>[27] C. Fang, C. Li, P. Forouzannezhad, M. Cabrerizo, R. E. Curiel, D. Loewenstein, R. Duara, and M. Adjouadi, "Gaussian discriminative component analysis for early detection of alzheimer's disease: A supervised dimensionality reduction algorithm," Journal of Neuroscience Methods, vol. 344, p. 108 856, 2020, ISSN: 0165-0270. DOI: https://doi.org/10.1016/j.jneumeth.2020.108856. [Online]. Available: https://www.sciencedirect.com/science/article/pii/ S016502702030279X.
[28] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, and S. Thrun, "Dermatologistlevel classification of skin cancer with deep neural networks," nature, vol. 542, no. 7639, pp. 115118, 2017.
[29] S. Weng, X. Xu, J. Li, and S. T. Wong, "Combining deep learning and coherent anti-stokes raman scattering imaging for automated differential diagnosis of lung cancer," Journal of biomedical optics, vol. 22, no. 10, p. 106017, 2017.
[30] L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Mueller, O. Grisel, V. Niculae, P. Prettenhofer, A. Gramfort, J. Grobler, R. Layton, J. VanderPlas, A. Joly, B. Holt, and G. Varoquaux, "API design for machine learning software: Experiences from the scikit-learn project," in ECML PKDD Workshop: Languages for Data Mining and Machine Learning, 2013, pp. 108-122.</p>
<h1>MULTIMODAL ATTENTION-BASED DEEP LEARNING FOR ALZHEIMER'S DISEASE DIAGNOSIS</h1>
<h2>Supplementary Material</h2>
<h2>S1 Genetic Data Pre-processing</h2>
<p>The genetic data consists of the whole genome sequencing (WGS) data from 805 ADNI participants by Illumina's non-Clinical Laboratory Improvement Amendments (non-CLIA) laboratory at roughly 30-40 $\times$ coverage in 2012 and 2013. The resulting variant call files (VCFs) have been generated by ADNI using Broad best practices (Burrows-Wheeler Aligner (BWA) and Genome Analysis Toolkit (GATK)-haplotype caller) in 2014. We first filtered the SNPs by the Hardy-Weinberg equilibrium (HWE) test for each site (pvalues) by removing SNPs with HWE p $&lt;0.05$. We then checked the genotype quality (GQ) and removed SNPs with GQ $&lt;20$. Next, we filtered by minor allele frequency (MAF) and removed sites with MAF $&lt;0.01$. Lastly, we performed genotype value filtering where we excluded sites based on the proportion of missing data and removed sites with a missing rate $&gt;0.05$. After filtering with the above criteria, we utilized genes known to be related to Alzheimer's Disease. In this step, we first downloaded a list of all ADrelated genes from the AlzGene Database (http://www.alzgene.org/), which contains 680 genes in total. Then we searched these genes in the UCSC genome browser (https://genome.ucsc.edu/) and kept the 640 genes that matched NCBI RefSeq annotation. We extracted these gene regions from RefSeq Annotation (gff file) in Bed format and use them to filter the SNPs further. We only retain the genes that are located in these regions. After selecting the 680 genes of known association with AD, we had 547,863 SNPs left. As discussed in the Genetic Data Pre-processing Section, we needed to find a way to reduce the number of features. We used a Random Forest Classifier to create a list of the most important features. Since this is a supervised method of creating features, this brought more promising results to the performance, in contrast to an approach such as principal component analysis (PCA) which is unsupervised. To find the best set of features, we tried using 50, 100, 150, and 200 forests as the parameter in the classifier. After creating the four sets of features, we used a validation set ( $10 \%$ from the training) to do hyperparameter tuning (as described in S4), for each set of features, we found that the set from 100 forests performed the best on the validation set resulting in the accuracy described in the Performance of Unimodal Models Section.</p>
<h2>S2 Clinical Features</h2>
<p>In the Model Robustness Section we discussed the value of clinical features to the model's performance. To ensure that there are no variables in the data that could potentially give an unfair advantage to the model (e.g. medication that a patient takes, only when they already have AD), we carefully examined all available variables. We fit a Random Forest classifier (from the scikit-learn package [30]) which outputs the features along with their importance score. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature [30]. Figure S1 shows the full list of features and their importance.</p>
<p>Figure S1: Clinical feature importance. The graph shows all the clinical features used in our model in order of most important to least important
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<h1>S3 Imaging Data Pre-processing</h1>
<p>The images used for our study are pre-processed by ADNI with specific image correction steps:</p>
<ol>
<li>Gradwarp: gradwarp is a system-specific correction of image geometry distortion due to gradient non-linearity. The degree to which images are distorted due to gradient non-linearity varies with each specific gradient model. We anticipate that most users will prefer to use images which have been corrected for gradient non-linearity distortion in analyses.</li>
<li>B1 non-uniformity: this correction procedure employs the B1 calibration scans noted in the protocol above to correct the image intensity non-uniformity that results when RF transmission is performed with a more uniform body coil while reception is performed with a less uniform head coil.</li>
<li>N3: N3 is a histogram peak sharpening algorithm that is applied to all images. It is applied after grad warp and after B1 correction for systems on which these two correction steps are performed. N3 will reduce intensity non-uniformity due to the wave or the dielectric effect at 3T. 1.5T scans also undergo N3 processing to reduce residual intensity non-uniformity.</li>
</ol>
<p>We followed the same pre-processing steps as Bucholc et al. [19], El-Sappagh et al. [17], and Abuhmad et al. [18], which relied on ADNI's correction steps without further modification.
To demonstrate that the performance of the unimodal imaging model is not significantly impacted by the addition of more brain slices, we report the optimized performance of the model with just the middle three slices, 2 more images per angle ( 6 more in total), 5 more images per angle, 10 more images per angle, 20 more images per angle, and 50 more images per angle. We report both F1-Scores and accuracy (averaged across 3 -fold validation set), which follow a similar trend shown in S2. The difference in performance between no additional slices (used in the paper) and 20 additional slices are all within 1 percent. When adding 50 slices to each angle, we observe a significant decline in performance. Thus, we proceeded with the original choice of just using the middle of the brain.</p>
<p>Figure S2: Validation F1-Score and Accuracy Trend as Number of Images Increases. The graph shows that the unimodal imaging model does not significantly benefit from the addition of more images.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>The decline in performance can be attributed to the fact that the slices further away from the center do not contain meaningful information and add noise to the model. The example below shows the middle three slices, followed by the outer 10 slices and outer most slices.</p>
<p>Figure S3: Example of MRI slices as distance increases from the center.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<h1>S4 Hyperparameter Tuning Methods</h1>
<p>To perform hyperparameter tuning for both unimodal and multimodal results, we randomly split the data into a training set $(90 \%)$ and a testing set $(10 \%)$. The testing set was set aside and withheld from tuning. We picked the best hyperparameters on the average validation accuracy of the 3-fold cross-validation scheme. For either a fully connected neural network or a convolutional neural network, the architecture, batch size, number of epochs, and learning rate were chosen via tuning. Table S1 describes all hyperparameters considered.</p>
<p>Table S1: Hyperparameter Grid</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameters</th>
<th style="text-align: left;">Values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: left;">$[0.00001,0.1]$</td>
</tr>
<tr>
<td style="text-align: left;">Dropout Values</td>
<td style="text-align: left;">${0.1,0.2,0.3,0.4,0.5}$</td>
</tr>
<tr>
<td style="text-align: left;">Number of Layers</td>
<td style="text-align: left;">$[1,6]$</td>
</tr>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: left;">${16,32,64,128}$</td>
</tr>
<tr>
<td style="text-align: left;">Number of Epochs</td>
<td style="text-align: left;">${10,20,50,80,100,150,200}$</td>
</tr>
</tbody>
</table>
<p>The hyperparameters that gave the highest accuracy for each type of model are shown in S2. For the multimodal framework, the best unimodal neural network values were added into the architecture.</p>
<p>Table S2: Best Hyperparameters</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Learning Rate</th>
<th style="text-align: left;">Batch Size</th>
<th style="text-align: left;">Number of Layers</th>
<th style="text-align: left;">Dropout Value</th>
<th style="text-align: left;">Number of Epochs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Unimodal Clinical</td>
<td style="text-align: left;">0.0001</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">${0.2,0.3,0.5}$</td>
<td style="text-align: left;">100</td>
</tr>
<tr>
<td style="text-align: left;">Unimodal Genetic</td>
<td style="text-align: left;">0.001</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">${0.3,0.5}$</td>
<td style="text-align: left;">50</td>
</tr>
<tr>
<td style="text-align: left;">Unimodal Imaging</td>
<td style="text-align: left;">0.001</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">${0.3,0.5}$</td>
<td style="text-align: left;">50</td>
</tr>
<tr>
<td style="text-align: left;">Multimodal</td>
<td style="text-align: left;">0.001</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">${3,3,3}$</td>
<td style="text-align: left;">${0.2,0.3,0.5}$</td>
<td style="text-align: left;">50</td>
</tr>
</tbody>
</table>
<h2>S5 Evaluation Metrics</h2>
<p>For our multi-class setting, we used the formulas below for each class. For example, for class 0 (control), we calculated the number of true positives, true negatives, false positives, and false negatives just for class 0 . Then, we use "macro" averaged F1-score using the arithmetic mean of all the per-class F1-scores.</p>
<p>$$
\begin{gathered}
\text { Accuracy }=\frac{T P+T N}{T P+T N+F P+F N} \
\text { Precision }=\frac{T P}{T P+F P} \
\text { Recall }=\frac{T P}{T P+F N} \
F 1=\frac{2 * \text { Precision } * \text { Recall }}{\text { Precision }+ \text { Recall }}=\frac{2 * T P}{2 * T P+F P+F N}
\end{gathered}
$$</p>
<h1>S6 Performance of Unimodal Models</h1>
<p>Table S3 shows the numeric information presented in Figure 3. We report all four evaluation metrics for the best neural network model for each modality - imaging, clinical, and genetic. The imaging model gives the best performance overall, whereas the genetic modality gives the lowest performance.</p>
<p>Table S3: Results of Unimodal Models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Accuracy (\%)</th>
<th style="text-align: left;">Precision (\%)</th>
<th style="text-align: left;">Recall (\%)</th>
<th style="text-align: left;">F1-Score (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Clinical</td>
<td style="text-align: left;">80.59</td>
<td style="text-align: left;">80.56</td>
<td style="text-align: left;">80.48</td>
<td style="text-align: left;">80.47</td>
</tr>
<tr>
<td style="text-align: left;">Imaging</td>
<td style="text-align: left;">92.23</td>
<td style="text-align: left;">94.02</td>
<td style="text-align: left;">90.4</td>
<td style="text-align: left;">91.83</td>
</tr>
<tr>
<td style="text-align: left;">Genetic</td>
<td style="text-align: left;">77.78</td>
<td style="text-align: left;">78.37</td>
<td style="text-align: left;">76.92</td>
<td style="text-align: left;">77.24</td>
</tr>
</tbody>
</table>
<h2>S7 Model Robustness</h2>
<p>Table S4 shows the numeric information presented in Figure 4.
Table S4: F1-Score Distribution for different attention-based and attention-free baselines. The table shows the F1-score distribution across 100 random seeds to show the value of attention in a deep learning model. The table demonstrates that the combination of self-attention with cross-modal attention performs the best with the most narrow variation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Lower Whisker</th>
<th style="text-align: left;">Lower Quartile</th>
<th style="text-align: left;">Median</th>
<th style="text-align: left;">Upper Quartile</th>
<th style="text-align: left;">Upper Whisker</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Self-Att + Cross-Modal Att</td>
<td style="text-align: left;">0.7767</td>
<td style="text-align: left;">0.8268</td>
<td style="text-align: left;">0.8799</td>
<td style="text-align: left;">0.9238</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Cross-Modal Att</td>
<td style="text-align: left;">0.4068</td>
<td style="text-align: left;">0.6491</td>
<td style="text-align: left;">0.7657</td>
<td style="text-align: left;">0.8268</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Self-Att</td>
<td style="text-align: left;">0.7175</td>
<td style="text-align: left;">0.8148</td>
<td style="text-align: left;">0.8682</td>
<td style="text-align: left;">0.8799</td>
<td style="text-align: left;">0.9238</td>
</tr>
<tr>
<td style="text-align: left;">No Attention</td>
<td style="text-align: left;">0.6396</td>
<td style="text-align: left;">0.7714</td>
<td style="text-align: left;">0.8148</td>
<td style="text-align: left;">0.8799</td>
<td style="text-align: left;">0.9238</td>
</tr>
</tbody>
</table>            </div>
        </div>

    </div>
</body>
</html>
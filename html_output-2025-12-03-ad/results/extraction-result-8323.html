<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8323 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8323</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8323</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-276482785</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.14669v3.pdf" target="_blank">AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated impressive capabilities in language processing, yet they often struggle with tasks requiring genuine visual spatial reasoning. In this paper, we introduce a novel two-stage training framework designed to equip standard LLMs with visual reasoning abilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT) on a curated dataset of tokenized maze representations to teach the model to predict step-by-step movement commands. Next, we apply Group Relative Policy Optimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted reward function to refine the model's sequential decision-making and encourage emergent chain-of-thought behaviors. Experimental results on synthetically generated mazes show that while a baseline model fails to navigate the maze, the SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning boosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more robust and self-corrective reasoning, highlighting the potential of our approach to bridge the gap between language models and visual spatial tasks. These findings offer promising implications for applications in robotics, autonomous navigation, and other domains that require integrated visual and sequential reasoning.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8323.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8323.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline-1.5B (Direct Prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1-Distill-Qwen-1.5B (Direct Prediction baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distilled Qwen family language model (1.5B parameters) used as a direct-path prediction baseline that was fine-tuned to output entire movement-token sequences for maze solutions via supervised cross-entropy training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-Distill-Qwen-1.5B (Direct Prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A distilled variant of the DeepSeek-R1 family into the Qwen architecture; in this paper the 1.5B variant is trained with supervised fine-tuning to directly predict full movement-token solution sequences from tokenized mazes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Maze (MazeBench)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based navigation puzzle requiring spatial planning and stepwise pathfinding within a 5x5 maze</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Input: tokenized visual maze representation (cell coordinate tokens, wall tokens, <|origin|>, <|target|>, <|blank|>). Output: a contiguous sequence of movement tokens representing the path (<|up|>, <|down|>, <|left|>, <|right|>). This baseline was trained with SFT to predict the entire path sequence directly (no step-by-step CoT or RL).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Direct sequence generation via supervised fine-tuning (cross-entropy) to map tokenized maze to full movement-token solution (no explicit chain-of-thought or RL).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MazeBench success rate: 0.0% (failed to solve any of the 100 MazeBench test mazes).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No evidence of spatial reasoning: model frequently produced nonsensical or invalid movement sequences and 'hallucinations' (qualitative analysis). 0% success indicates inability to derive valid spatial paths from tokenized maze input under direct prediction training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared directly to the same architecture when trained with step-by-step SFT (AlphaMaze-SFT) and to SFT+GRPO (AlphaMaze): this baseline performed far worse (0.0%) than AlphaMaze-SFT (86.0%) and AlphaMaze (93.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Produced invalid moves, hallucinated movements that contradict walls, and generated incomplete or nonsensical movement sequences; fails on even easy mazes under MazeBench. Demonstrates that direct path prediction without stepwise reasoning is insufficient for spatial maze solving in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO", 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8323.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8323.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline-7B (Distill)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1-Distill-Qwen-7B (distilled baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger distilled Qwen variant (7B) from the DeepSeek-R1 family evaluated as a baseline; it achieved 0% accuracy on MazeBench in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-Distill-Qwen-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A distilled 7B variant of the DeepSeek-R1 family in the Qwen architecture used as an experimental baseline in this paper; treated similarly to the 1.5B baseline for direct prediction evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Maze (MazeBench)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based navigation puzzle requiring spatial planning and sequential decision-making in a 5x5 grid</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Presented tokenized maze inputs (cell coordinates, wall tokens, origin/target markers) and expected to output movement-token sequences. Evaluated as a baseline without the two-stage SFT+GRPO training pipeline used for AlphaMaze.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Distilled model used as-is (direct prediction baseline); no specialized chain-of-thought training or GRPO applied in the baseline configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MazeBench success rate: 0.0% (failed to solve mazes in the benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No quantitative or qualitative evidence of spatial reasoning; 0% success and tendency to produce invalid or nonsensical moves indicates spatial reasoning ability was not present in distilled variant under the evaluated setup.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against smaller distilled 1.5B baseline (also 0%) and against AlphaMaze-SFT (86.0%) and AlphaMaze (93.0%), showing that distilled baselines did not inherit emergent spatial reasoning without task-specific training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Distillation into smaller Qwen variants appears to have removed emergent visual spatial reasoning present in the base DeepSeek-R1 model; model fails on MazeBench and produces invalid moves/hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO", 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8323.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8323.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaMaze-SFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaMaze (Supervised Fine-Tuning stage)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The Qwen 1.5B model after supervised fine-tuning on tokenized 5x5 mazes to predict the next movement token step-by-step, serving as the first stage of the AlphaMaze two-stage pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AlphaMaze-SFT (Qwen 1.5B SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen 1.5B model supervised-fine-tuned for 10 epochs on a 500k-sample SFT dataset of synthetic 5x5 mazes (250k straight-success + 250k retry/reset examples) to predict next movement tokens conditioned on tokenized maze input and prior movement tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Maze (MazeBench)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based maze navigation requiring spatial reasoning to plan multi-step paths in a 5x5 discrete grid</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Input: tokenized visual maze representation (cell coordinate tokens like <|row-col|>, wall tokens, <|origin|>, <|target|>). Output during SFT: next movement token at each step (step-by-step generation) conditioned on maze and preceding movement tokens; training objective: cross-entropy to match ground-truth next-step tokens. Evaluation on MazeBench used extraction of movement-token sequences from model outputs and computed success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Step-by-step chain-of-thought style supervised fine-tuning (explicit next-token prediction) and use of augmented 'reset' examples (incorrect plausible paths + RE-SET messages + correct COT) to teach recovery from errors and encourage sequential reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MazeBench success rate: 86.0% (on 100-maze MazeBench: Easy/Medium/Hard split described in paper). Training: 10 epochs on 500k SFT examples.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Quantitative success (86%) and qualitative analyses showing improved coherence and stepwise progression suggest the model learned spatial reasoning patterns; however, the paper reports remaining issues (e.g., getting trapped in loops or making incorrect turns in longer solutions), indicating partial spatial reasoning. The inclusion of reset examples trained error recovery behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Substantially outperformed the direct-prediction baselines (0.0%). Later GRPO finetuning (AlphaMaze full) further improved performance to 93.0%, suggesting RL refines reasoning beyond SFT alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles on longer/complex mazes: can become trapped in loops, make incorrect turns in later stages, and has limited backtracking capability; evaluation limited to synthetic 5x5 mazes so generalization to larger or real-world spatial tasks is untested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO", 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8323.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8323.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaMaze (SFT+GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaMaze (Supervised Fine-Tuning + Group Relative Policy Optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The full two-stage AlphaMaze model: Qwen 1.5B first SFT-trained on tokenized maze data, then fine-tuned with GRPO using a bespoke reward function to refine sequential decision-making and elicit emergent chain-of-thought and self-correction behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AlphaMaze (Qwen 1.5B, SFT + GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen 1.5B initially supervised-fine-tuned on 500k maze examples for next-step prediction, then further optimized with Group Relative Policy Optimization (GRPO) using LoRA for efficient adaptation. GRPO used a specialized reward combining per-step correctness, integrity of movement tokens, and a 'thinking' tag reward to encourage formatted chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Maze (MazeBench)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based navigation puzzle requiring spatial planning, wall-awareness, and multi-step sequential reasoning in 5x5 mazes</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Tokenized maze input as in SFT stage; generation produces step-by-step movement tokens interleaved with <think> tags for chain-of-thought. GRPO finetuning used a GRPO algorithm (advantage estimation based on group scores) with LoRA parameter adapters and checkpoints every 200 steps; reported result at 1600 GRPO steps.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Supervised step-by-step prediction (SFT) followed by reinforcement learning (GRPO) with a designed reward: Correctness Reward (+0.2 per correct solution step), Integrity Reward (+0.5 per valid movement token), Thinking Reward (+0.25 for correctly using <think> tag). Emergent chain-of-thought and self-correction behaviors observed qualitatively (re-evaluation mid-sequence, 'aha moments').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MazeBench success rate: 93.0% after 1600 GRPO steps (improvement from 86.0% SFT-only). Reported GRPO gain +7 percentage points; authors note improvement trend over GRPO steps and saved checkpoints every 200 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative analyses demonstrated emergent chain-of-thought, explicit consideration of wall constraints, self-correction (changing trajectory mid-sequence), and reduced invalid moves compared to SFT-only model. The contrast with 0% baselines provides indirect ablation evidence that stepwise SFT and GRPO induce spatial reasoning. No internal probe/interpretability study reported beyond qualitative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to: (a) direct-prediction baselines (both distilled 1.5B and 7B) which scored 0.0%; (b) AlphaMaze-SFT (86.0%), showing a +7% absolute improvement from GRPO; (c) discussion compares to base DeepSeek-R1 which reportedly demonstrates emergent visual reasoning with very long contexts (but distilled variants lost this capability).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include modest absolute gain from GRPO (7% improvement), remaining difficulties with mazes requiring complex backtracking or long-range planning beyond immediate next step, restricted evaluation to synthetic 5x5 mazes (generalization unknown), and reliance on reward shaping which may need further tuning. Qualitative failure modes include remaining occasional invalid moves or failures in mazes requiring sophisticated backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO", 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8323.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8323.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1 (base)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior model/approach (cited) that applied GRPO-style reinforcement learning to elicit emergent chain-of-thought reasoning in LLMs; the paper reports that the base DeepSeek-R1 with very long context demonstrates emergent visual reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1 (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A reasoning-focused LLM training approach using GRPO (as described by the DeepSeek-R1 work) that reported emergent reasoning and 'aha moments' purely through RL; cited as inspiration and methodological foundation for AlphaMaze's GRPO use.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Referenced in relation to reasoning tasks (not explicitly evaluated on MazeBench within this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Referenced as capable of emergent visual reasoning given very long contexts (details in the DeepSeek-R1 paper, not re-evaluated here).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Mentioned as prior work: GRPO-based RL elicited chain-of-thought in LLMs, sometimes with very long context windows; not re-run in this paper but used as methodological inspiration.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>GRPO (advantage estimation from group scores), reward shaping to elicit chain-of-thought and self-correction, multistage pipelines combining RL and SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numeric MazeBench metrics reported here for the base DeepSeek-R1; authors report that the base model (with extremely long context) demonstrates emergent visual reasoning capabilities, but distilled variants evaluated in this paper scored 0%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Cited claim: base DeepSeek-R1 shows emergent visual reasoning with long context windows (no quantitative details provided in this paper). The paper uses this as motivation; experiments here show distilled variants lost that capability without additional training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Authors compare distilled DeepSeek-R1 variants (0% on MazeBench) to the base DeepSeek-R1's reported emergent capabilities, concluding that distillation into smaller models may remove emergent spatial reasoning and that targeted SFT+GRPO can recover it.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Within this paper, no direct experimental evaluation of base DeepSeek-R1 is provided; claims are based on prior work citation. Authors note that distilled variants do not preserve base model's emergent spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO", 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. <em>(Rating: 2)</em></li>
                <li>A configurable library for generating and manipulating maze datasets. <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning for maze solving. <em>(Rating: 2)</em></li>
                <li>Imagine while reasoning in space: Multimodal visualization-of-thought. <em>(Rating: 2)</em></li>
                <li>Reward design with language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8323",
    "paper_id": "paper-276482785",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "Baseline-1.5B (Direct Prediction)",
            "name_full": "DeepSeek-R1-Distill-Qwen-1.5B (Direct Prediction baseline)",
            "brief_description": "A distilled Qwen family language model (1.5B parameters) used as a direct-path prediction baseline that was fine-tuned to output entire movement-token sequences for maze solutions via supervised cross-entropy training.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-Distill-Qwen-1.5B (Direct Prediction)",
            "model_description": "A distilled variant of the DeepSeek-R1 family into the Qwen architecture; in this paper the 1.5B variant is trained with supervised fine-tuning to directly predict full movement-token solution sequences from tokenized mazes.",
            "model_size": "1.5B",
            "puzzle_name": "Maze (MazeBench)",
            "puzzle_type": "2D grid-based navigation puzzle requiring spatial planning and stepwise pathfinding within a 5x5 maze",
            "task_setup": "Input: tokenized visual maze representation (cell coordinate tokens, wall tokens, &lt;|origin|&gt;, &lt;|target|&gt;, &lt;|blank|&gt;). Output: a contiguous sequence of movement tokens representing the path (&lt;|up|&gt;, &lt;|down|&gt;, &lt;|left|&gt;, &lt;|right|&gt;). This baseline was trained with SFT to predict the entire path sequence directly (no step-by-step CoT or RL).",
            "mechanisms_or_strategies": "Direct sequence generation via supervised fine-tuning (cross-entropy) to map tokenized maze to full movement-token solution (no explicit chain-of-thought or RL).",
            "performance_metrics": "MazeBench success rate: 0.0% (failed to solve any of the 100 MazeBench test mazes).",
            "evidence_of_spatial_reasoning": "No evidence of spatial reasoning: model frequently produced nonsensical or invalid movement sequences and 'hallucinations' (qualitative analysis). 0% success indicates inability to derive valid spatial paths from tokenized maze input under direct prediction training.",
            "comparisons": "Compared directly to the same architecture when trained with step-by-step SFT (AlphaMaze-SFT) and to SFT+GRPO (AlphaMaze): this baseline performed far worse (0.0%) than AlphaMaze-SFT (86.0%) and AlphaMaze (93.0%).",
            "limitations_or_failure_cases": "Produced invalid moves, hallucinated movements that contradict walls, and generated incomplete or nonsensical movement sequences; fails on even easy mazes under MazeBench. Demonstrates that direct path prediction without stepwise reasoning is insufficient for spatial maze solving in this setting.",
            "uuid": "e8323.0",
            "source_info": {
                "paper_title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Baseline-7B (Distill)",
            "name_full": "DeepSeek-R1-Distill-Qwen-7B (distilled baseline)",
            "brief_description": "A larger distilled Qwen variant (7B) from the DeepSeek-R1 family evaluated as a baseline; it achieved 0% accuracy on MazeBench in this work.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-Distill-Qwen-7B",
            "model_description": "A distilled 7B variant of the DeepSeek-R1 family in the Qwen architecture used as an experimental baseline in this paper; treated similarly to the 1.5B baseline for direct prediction evaluation.",
            "model_size": "7B",
            "puzzle_name": "Maze (MazeBench)",
            "puzzle_type": "2D grid-based navigation puzzle requiring spatial planning and sequential decision-making in a 5x5 grid",
            "task_setup": "Presented tokenized maze inputs (cell coordinates, wall tokens, origin/target markers) and expected to output movement-token sequences. Evaluated as a baseline without the two-stage SFT+GRPO training pipeline used for AlphaMaze.",
            "mechanisms_or_strategies": "Distilled model used as-is (direct prediction baseline); no specialized chain-of-thought training or GRPO applied in the baseline configuration.",
            "performance_metrics": "MazeBench success rate: 0.0% (failed to solve mazes in the benchmark).",
            "evidence_of_spatial_reasoning": "No quantitative or qualitative evidence of spatial reasoning; 0% success and tendency to produce invalid or nonsensical moves indicates spatial reasoning ability was not present in distilled variant under the evaluated setup.",
            "comparisons": "Compared against smaller distilled 1.5B baseline (also 0%) and against AlphaMaze-SFT (86.0%) and AlphaMaze (93.0%), showing that distilled baselines did not inherit emergent spatial reasoning without task-specific training.",
            "limitations_or_failure_cases": "Distillation into smaller Qwen variants appears to have removed emergent visual spatial reasoning present in the base DeepSeek-R1 model; model fails on MazeBench and produces invalid moves/hallucinations.",
            "uuid": "e8323.1",
            "source_info": {
                "paper_title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "AlphaMaze-SFT",
            "name_full": "AlphaMaze (Supervised Fine-Tuning stage)",
            "brief_description": "The Qwen 1.5B model after supervised fine-tuning on tokenized 5x5 mazes to predict the next movement token step-by-step, serving as the first stage of the AlphaMaze two-stage pipeline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "AlphaMaze-SFT (Qwen 1.5B SFT)",
            "model_description": "Qwen 1.5B model supervised-fine-tuned for 10 epochs on a 500k-sample SFT dataset of synthetic 5x5 mazes (250k straight-success + 250k retry/reset examples) to predict next movement tokens conditioned on tokenized maze input and prior movement tokens.",
            "model_size": "1.5B",
            "puzzle_name": "Maze (MazeBench)",
            "puzzle_type": "2D grid-based maze navigation requiring spatial reasoning to plan multi-step paths in a 5x5 discrete grid",
            "task_setup": "Input: tokenized visual maze representation (cell coordinate tokens like &lt;|row-col|&gt;, wall tokens, &lt;|origin|&gt;, &lt;|target|&gt;). Output during SFT: next movement token at each step (step-by-step generation) conditioned on maze and preceding movement tokens; training objective: cross-entropy to match ground-truth next-step tokens. Evaluation on MazeBench used extraction of movement-token sequences from model outputs and computed success rate.",
            "mechanisms_or_strategies": "Step-by-step chain-of-thought style supervised fine-tuning (explicit next-token prediction) and use of augmented 'reset' examples (incorrect plausible paths + RE-SET messages + correct COT) to teach recovery from errors and encourage sequential reasoning.",
            "performance_metrics": "MazeBench success rate: 86.0% (on 100-maze MazeBench: Easy/Medium/Hard split described in paper). Training: 10 epochs on 500k SFT examples.",
            "evidence_of_spatial_reasoning": "Quantitative success (86%) and qualitative analyses showing improved coherence and stepwise progression suggest the model learned spatial reasoning patterns; however, the paper reports remaining issues (e.g., getting trapped in loops or making incorrect turns in longer solutions), indicating partial spatial reasoning. The inclusion of reset examples trained error recovery behavior.",
            "comparisons": "Substantially outperformed the direct-prediction baselines (0.0%). Later GRPO finetuning (AlphaMaze full) further improved performance to 93.0%, suggesting RL refines reasoning beyond SFT alone.",
            "limitations_or_failure_cases": "Struggles on longer/complex mazes: can become trapped in loops, make incorrect turns in later stages, and has limited backtracking capability; evaluation limited to synthetic 5x5 mazes so generalization to larger or real-world spatial tasks is untested.",
            "uuid": "e8323.2",
            "source_info": {
                "paper_title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "AlphaMaze (SFT+GRPO)",
            "name_full": "AlphaMaze (Supervised Fine-Tuning + Group Relative Policy Optimization)",
            "brief_description": "The full two-stage AlphaMaze model: Qwen 1.5B first SFT-trained on tokenized maze data, then fine-tuned with GRPO using a bespoke reward function to refine sequential decision-making and elicit emergent chain-of-thought and self-correction behaviors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "AlphaMaze (Qwen 1.5B, SFT + GRPO)",
            "model_description": "Qwen 1.5B initially supervised-fine-tuned on 500k maze examples for next-step prediction, then further optimized with Group Relative Policy Optimization (GRPO) using LoRA for efficient adaptation. GRPO used a specialized reward combining per-step correctness, integrity of movement tokens, and a 'thinking' tag reward to encourage formatted chain-of-thought.",
            "model_size": "1.5B",
            "puzzle_name": "Maze (MazeBench)",
            "puzzle_type": "2D grid-based navigation puzzle requiring spatial planning, wall-awareness, and multi-step sequential reasoning in 5x5 mazes",
            "task_setup": "Tokenized maze input as in SFT stage; generation produces step-by-step movement tokens interleaved with &lt;think&gt; tags for chain-of-thought. GRPO finetuning used a GRPO algorithm (advantage estimation based on group scores) with LoRA parameter adapters and checkpoints every 200 steps; reported result at 1600 GRPO steps.",
            "mechanisms_or_strategies": "Supervised step-by-step prediction (SFT) followed by reinforcement learning (GRPO) with a designed reward: Correctness Reward (+0.2 per correct solution step), Integrity Reward (+0.5 per valid movement token), Thinking Reward (+0.25 for correctly using &lt;think&gt; tag). Emergent chain-of-thought and self-correction behaviors observed qualitatively (re-evaluation mid-sequence, 'aha moments').",
            "performance_metrics": "MazeBench success rate: 93.0% after 1600 GRPO steps (improvement from 86.0% SFT-only). Reported GRPO gain +7 percentage points; authors note improvement trend over GRPO steps and saved checkpoints every 200 steps.",
            "evidence_of_spatial_reasoning": "Qualitative analyses demonstrated emergent chain-of-thought, explicit consideration of wall constraints, self-correction (changing trajectory mid-sequence), and reduced invalid moves compared to SFT-only model. The contrast with 0% baselines provides indirect ablation evidence that stepwise SFT and GRPO induce spatial reasoning. No internal probe/interpretability study reported beyond qualitative examples.",
            "comparisons": "Compared to: (a) direct-prediction baselines (both distilled 1.5B and 7B) which scored 0.0%; (b) AlphaMaze-SFT (86.0%), showing a +7% absolute improvement from GRPO; (c) discussion compares to base DeepSeek-R1 which reportedly demonstrates emergent visual reasoning with very long contexts (but distilled variants lost this capability).",
            "limitations_or_failure_cases": "Reported limitations include modest absolute gain from GRPO (7% improvement), remaining difficulties with mazes requiring complex backtracking or long-range planning beyond immediate next step, restricted evaluation to synthetic 5x5 mazes (generalization unknown), and reliance on reward shaping which may need further tuning. Qualitative failure modes include remaining occasional invalid moves or failures in mazes requiring sophisticated backtracking.",
            "uuid": "e8323.3",
            "source_info": {
                "paper_title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "DeepSeek-R1 (base)",
            "name_full": "DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning",
            "brief_description": "A prior model/approach (cited) that applied GRPO-style reinforcement learning to elicit emergent chain-of-thought reasoning in LLMs; the paper reports that the base DeepSeek-R1 with very long context demonstrates emergent visual reasoning capabilities.",
            "citation_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.",
            "mention_or_use": "mention",
            "model_name": "DeepSeek-R1 (base)",
            "model_description": "A reasoning-focused LLM training approach using GRPO (as described by the DeepSeek-R1 work) that reported emergent reasoning and 'aha moments' purely through RL; cited as inspiration and methodological foundation for AlphaMaze's GRPO use.",
            "model_size": null,
            "puzzle_name": "Referenced in relation to reasoning tasks (not explicitly evaluated on MazeBench within this paper)",
            "puzzle_type": "Referenced as capable of emergent visual reasoning given very long contexts (details in the DeepSeek-R1 paper, not re-evaluated here).",
            "task_setup": "Mentioned as prior work: GRPO-based RL elicited chain-of-thought in LLMs, sometimes with very long context windows; not re-run in this paper but used as methodological inspiration.",
            "mechanisms_or_strategies": "GRPO (advantage estimation from group scores), reward shaping to elicit chain-of-thought and self-correction, multistage pipelines combining RL and SFT.",
            "performance_metrics": "No numeric MazeBench metrics reported here for the base DeepSeek-R1; authors report that the base model (with extremely long context) demonstrates emergent visual reasoning capabilities, but distilled variants evaluated in this paper scored 0%.",
            "evidence_of_spatial_reasoning": "Cited claim: base DeepSeek-R1 shows emergent visual reasoning with long context windows (no quantitative details provided in this paper). The paper uses this as motivation; experiments here show distilled variants lost that capability without additional training.",
            "comparisons": "Authors compare distilled DeepSeek-R1 variants (0% on MazeBench) to the base DeepSeek-R1's reported emergent capabilities, concluding that distillation into smaller models may remove emergent spatial reasoning and that targeted SFT+GRPO can recover it.",
            "limitations_or_failure_cases": "Within this paper, no direct experimental evaluation of base DeepSeek-R1 is provided; claims are based on prior work citation. Authors note that distilled variants do not preserve base model's emergent spatial reasoning.",
            "uuid": "e8323.4",
            "source_info": {
                "paper_title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.",
            "rating": 2,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "A configurable library for generating and manipulating maze datasets.",
            "rating": 2,
            "sanitized_title": "a_configurable_library_for_generating_and_manipulating_maze_datasets"
        },
        {
            "paper_title": "Deep reinforcement learning for maze solving.",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_for_maze_solving"
        },
        {
            "paper_title": "Imagine while reasoning in space: Multimodal visualization-of-thought.",
            "rating": 2,
            "sanitized_title": "imagine_while_reasoning_in_space_multimodal_visualizationofthought"
        },
        {
            "paper_title": "Reward design with language models.",
            "rating": 1,
            "sanitized_title": "reward_design_with_language_models"
        }
    ],
    "cost": 0.01353025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO
February 20, 2025</p>
<p>Alan Dao 
Gia Tuan Dao 
Dinh Bach Vu 
Menlo Research 
AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO
February 20, 2025F9BA2F16565FDFF31E5946D233898BFAarXiv:2502.14669v3[cs.CL]
Large Language Models (LLMs) have demonstrated impressive capabilities in language processing, yet they often struggle with tasks requiring genuine visual spatial reasoning.In this paper, we introduce a novel twostage training framework designed to equip standard LLMs with visual reasoning abilities for maze navigation.First, we leverage Supervised Fine-Tuning (SFT) on a curated dataset of tokenized maze representations to teach the model to predict step-by-step movement commands.Next, we apply Group Relative Policy Optimization (GRPO)-a technique used in DeepSeek-R1-with a carefully crafted reward function to refine the model's sequential decision-making and encourage emergent chain-of-thought behaviors.Experimental results on synthetically generated mazes show that while a baseline model fails to navigate the maze, the SFT-trained model achieves 86% accuracy, and further GRPO finetuning boosts accuracy to 93%.Qualitative analyses reveal that GRPO fosters more robust and self-corrective reasoning, highlighting the potential of our approach to bridge the gap between language models and visual spatial tasks.These findings offer promising implications for applications in robotics, autonomous navigation, and other domains that require integrated visual and sequential reasoning.</p>
<p>Introduction</p>
<p>The ability to reason about visual information, particularly in spatial contexts, is a hallmark of intelligent systems.From navigating physical environments to interpreting complex diagrams, visual spatial reasoning is crucial for a wide range of tasks.While Large Language Models (LLMs) have achieved impressive performance in natural language processing and code generation, their capacity for genuine visual reasoning, especially spatial understanding and sequential decision-making in visual environments, remains a significant open question [Zhang et al., 2024, Ma et al., 2024].Current Vision-Language Models (VLMs) often excel at pattern recognition and object identification but may struggle with tasks requiring deeper spatial inference and step-by-step planning 0 2 0 0 4 0 0 6 0 0 8 0 0 1 0 0 0 1 2 0 0 1 4 0 0 1 6 0 0 1 8 0 0 2 0 0 0 in visual domains [Ma et al., 2024].Bridging this gap and endowing standard LLMs with robust visual reasoning capabilities is a critical step towards more versatile and human-like AI.</p>
<p>In this paper, we address the challenge of teaching visual spatial reasoning to a standard LLM, focusing on the task of maze navigation.We hypothesize that by providing an LLM with a tokenized visual representation of a maze, we can train it to learn step-by-step movement commands to navigate from a designated origin to a target.The core of our approach lies in a two-stage training framework.First, we employ Supervised Fine-Tuning (SFT) to equip the LLM with the foundational skill of predicting movement tokens based on the visual maze input.Subsequently, we apply Group Relative Policy Optimization (GRPO), drawing inspiration from recent advancements in reinforcement learning for reasoning in LLMs, such as DeepSeek-R1 [Guo et al., 2025].DeepSeek-R1 demonstrated that Reinforcement Learning (RL) can elicit emergent reasoning behaviors, including chain-of-thought, even without prior SFT.We adapt and extend these RL strategies, combined with carefully designed reward functions, to refine our model's visual reasoning process for maze navigation.</p>
<p>To systematically evaluate LLM's ability to solve maze, we introduce MazeBench-a comprehensive benchmark on solving maze.MazeBench provides a controlled yet diverse environment that spans a range of maze sizes and complexities.By evaluating our model on MazeBench, we can rigorously measure both its maze-solving accuracy and the sophistication of its emergent reasoning behavior.</p>
<p>Our key contributions are as follows:</p>
<p> We present a novel training framework that combines Supervised Fine-Tuning and Group Relative Policy Optimization to enhance visual reasoning in standard LLMs, specifically for spatial tasks.</p>
<p> We empirically demonstrate that this framework, using a tokenized visual maze representation, enables an LLM to achieve improved maze navigation accuracy and exhibit emergent chain-of-thought reasoning in generating movement sequences.</p>
<p> We provide a detailed analysis of the design and impact of reward functions within the GRPO stage, highlighting their crucial role in shaping the model's visual reasoning performance.</p>
<p> We draw comparisons with insights from state-ofthe-art reasoning models like DeepSeek-R1, both in terms of methodology and observed emergent behaviors, positioning our work within the context of current advancements in LLM reasoning.</p>
<p> We present MazeBench, a benchmark for visual maze navigation that captures a wide spectrum of spatial challenges.</p>
<p>2 Related Work</p>
<p>Chain-of-Thought Reasoning in Language Models</p>
<p>Chain-of-Thought (CoT) prompting has emerged as a powerful technique to elicit complex reasoning from Large Language Models [Wei et al., 2022b].By prompting LLMs to "think step by step," CoT encourages the generation of intermediate reasoning steps, leading to improved performance on tasks requiring multi-step inference.Prior research, including Wei et al. [2022b], Wei et al. [2023], Wang et al. [2023] prompting significantly enhances LLM performance on arithmetic, commonsense reasoning, and symbolic reasoning tasks.Our work builds upon the concept of CoT reasoning, aiming to induce a similar step-by-step thought process in LLMs, but within the domain of visual spatial reasoning for maze navigation.</p>
<p>Supervised Fine-Tuning for Visual and Spatial Tasks</p>
<p>Supervised Fine-Tuning (SFT) is a widely adopted technique for adapting pre-trained LLMs to specific downstream tasks [Wei et al., 2022a].By training on taskspecific datasets, SFT allows LLMs to acquire specialized skills and improve performance in targeted domains.Jiang et al. [2024] recently highlighted the effectiveness of SFT in enhancing visual foundation models, demonstrating its utility in visual tasks.In our research, we leverage SFT as the initial stage of our training pipeline, using it to equip the LLM with the basic capability of processing tokenized visual maze inputs and predicting movement tokens.This SFT phase serves as a crucial foundation upon which we build more sophisticated reasoning through reinforcement learning.</p>
<p>2.3 Reinforcement Learning and GRPO for Reasoning and Reward Shaping</p>
<p>Reinforcement Learning from Human Feedback (RLHF) and its variants have demonstrated significant efficacy in aligning Large Language Models (LLMs) with human preferences and enhancing their reasoning capabilities.However, RLHF faces substantial scalability challenges due to its resource-intensive nature and reliance on human feedback data.As an alternative approach, recent methodologies like Group Relative Policy Optimization (GRPO) [Kwon et al., 2023a] and Self-Play fIne-tuNing (SPIN) leverage self-play mechanisms, where models autonomously generate training signals and iteratively improve through self-competition Chen et al. [2024].These self-play approaches show promise in achieving humanlevel performance without the need for extensive human feedback, potentially offering a more scalable solution to the alignment challenge.GRPO, as described by Shao et al. [2024] and implemented in DeepSeek-R1 [Guo et al., 2025], offers a computationally efficient approach to reinforcement learning by estimating advantages based on group scores, eliminating the need for a separate critic network.Reward function design is paramount in RLHF and GRPO, as it directly guides the model's learning process.Carefully crafted reward functions can incentivize desired behaviors and shape the model's policy towards optimal performance.Our work draws inspiration from the reward shaping strategies used in DeepSeek-R1 and adapts them to the context of visual maze navigation, designing reward components to encourage accuracy, valid movement sequences, and proper output formatting.</p>
<p>DeepSeek-R1 and Emergent Reasoning through RL</p>
<p>The DeepSeek-R1 model [Guo et al., 2025] represents a significant advancement in using reinforcement learning to elicit sophisticated reasoning capabilities in LLMs.A key finding of DeepSeek-R1 is the demonstration that pure RL, specifically GRPO, can lead to the emergent development of chain-of-thought reasoning and even "aha moments," where the model re-evaluates previous steps and corrects its reasoning process.Furthermore, DeepSeek-R1 highlights the benefits of a multistage training pipeline, combining initial RL training with subsequent supervised fine-tuning to refine language coherence and readability.We directly adapt the GRPO optimization strategy and multi-stage training insights from DeepSeek-R1 to our visual maze navigation task.We hypothesize that similar RL techniques can drive the emergence of visual spatial reasoning in standard LLMs, enabling them to solve mazes through a step-by-step, selfcorrective process.</p>
<p>Visual Reasoning and Maze Solving in AI</p>
<p>Maze solving has long been a benchmark task in Artificial Intelligence, serving as a testbed for various problem-solving and search algorithms [Janamian and Alam, 2023].</p>
<p>Traditional approaches include graph search algorithms like Depth-First Search, Breadth-First Search, and A* [Lester, 2014[Lester, -2024]].More recently, AI techniques, particularly reinforcement learning and neural networks, have been applied to maze navigation [Zafrany, 2020].While Chain-of-Thought (CoT) prompting has significantly enhanced complex reasoning capabilities in Large Language Models (LLMs) and Multimodal LLMs, it shows limitations in complex spatial reasoning tasks.Recent work by Microsoft introduces Multimodal Visualization-of-Thought (MVoT), which enables models to generate visual representations during their reasoning process, similar to human visual thinking [Li et al., 2025].This breakthrough demonstrates the potential of combining verbal and visual reasoning in AI systems.</p>
<p>Our research builds upon these advances, focusing on teaching visual maze reasoning to standard language models through a tokenized visual representation and a combination of SFT and GRPO.This approach differs from traditional maze solvers by leveraging the inherent reasoning capabilities of LLMs and adapting them to process and reason about visual spatial information.Furthermore, research in neural-symbolic visual reasoning [Mao et al., 2023] explores combining neural networks with symbolic AI for visual tasks, offering a complementary perspective on integrating reasoning and visual processing.</p>
<p>Methodology</p>
<p>Tokenized Visual Maze Representation</p>
<p>To enable the LLM to process maze information visually, we designed a tokenized input format that represents the maze grid, walls, origin, and target locations.Each cell in the maze is represented by a coordinate token &lt;|row-col|&gt;, e.g., &lt;|0-0|&gt; for the top-left cell.Wall information for each cell is encoded using tokens such as &lt;|no wall|&gt;, &lt;|up wall|&gt;, &lt;|up down wall|&gt;, &lt;|up down left right wall|&gt;, ....The origin and target locations are marked with &lt;|origin|&gt; and &lt;|target|&gt; tokens, respectively.Empty spaces within the maze representation are filled with &lt;|blank|&gt; tokens for consistent grid structure.This tokenization scheme provides a visual representation by explicitly encoding the spatial relationships between cells and the presence of walls, allowing the LLM to "see" the maze structure in a symbolic, tokenized form.</p>
<p>Example Maze Tokenization 2:</p>
<p>Baseline Models</p>
<p>To establish performance benchmarks for our approach, we employed three distinct baseline models, leveraging the DeepSeek-R1 [Guo et al., 2025] Distill-Qwen family of language models.We evaluate two distilled models: DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Qwen-1.5B.Additionally, a Direct Prediction baseline was established using a Supervised Fine-Tuning (SFT) approach on the DeepSeek-R1-Distill-Qwen-1.5B architecture.This model was trained to directly predict the complete sequence of movement tokens representing the solution path through a given maze.The training objective was the minimization of cross-entropy loss between the predicted token sequence and the ground truth solution.This baseline assesses the performance of a standard language model trained to generate complete solutions without intermediate reasoning steps or reinforcement learning techniques.</p>
<p>We include these three baselines to provide a comprehensive comparison, examining the influence of model size (7B vs. 1.5B) and the effectiveness of direct prediction versus our proposed step-by-step and reinforcement learning approaches.The subsequent sections will primarily focus on the customized direct prediction model and its enhancements through SFT for step-by-step reasoning and GRPO.</p>
<p>Supervised Fine-Tuning (SFT) for Step-by-</p>
<p>Step Reasoning</p>
<p>For the SFT stage, we curated a training dataset.Mazes were synthetically generated with fixed sizes (5x5) and varied complexity level.The Qwen 1.5B SFT model was then trained on this dataset.The training objective was to predict the next movement token at each step, conditioned on the maze input and the preceding movement tokens in the sequence as visually illustrated in Figure 3.This step-by-step prediction approach was designed to encourage the model to learn sequential reasoning for maze navigation.</p>
<p>Group Relative Policy Optimization (GRPO) for Enhanced Reasoning</p>
<p>Following SFT, we applied Group Relative Policy Optimization (GRPO) to further enhance the model's mazesolving capabilities and encourage more robust reasoning.The GRPO training utilized a smaller set of data than SFT state.We designed a reward function 3 components.</p>
<p>Correctness Reward (+0.2 per solution step): This reward is scaled according to the number of steps in the maze solution.Each valid movement step adds 0.2 points to the total score.For example, a solution requiring 4 steps earns a reward of 0.2  4 = 0.8 points, incentivizing both accuracy and efficiency in navigation.</p>
<p>Integrity Reward (+0.5):This reward is given for each valid movement token (&lt;|up|&gt;, &lt;|down|&gt;, &lt;|left|&gt;, &lt;|right|&gt;) in the predicted sequence, encouraging the generation of meaningful and valid movement steps.</p>
<p>Thinking Reward (+0.25):This reward is given for correctly using the <think> tag in the output, ensuring completeness and consistency in the reasoning format.</p>
<p>These reward components were weighted to prioritize correctness while also encouraging valid movement sequences and proper reasoning formatting with <think> tag.We adapted the Group Relative Policy Optimization (GRPO) algorithm, as employed in DeepSeek-R1 [Guo et al., 2025], to perform reinforcement learning.GRPO estimates advantages based on relative group scores, offering computational efficiency compared to critic-based methods.</p>
<p>Training Procedure and Pipeline</p>
<p>Our training pipeline consisted of two stages.First, Supervised Fine-Tuning (SFT) was performed on the Qwen 1.5B model using a curated maze dataset for 10 epochs to learn step-by-step movement prediction for maze navigation.This phase established a strong initial policy, ensuring that the model could effectively interpret and respond to sequential movement tasks.</p>
<p>Following SFT, Group Relative Policy Optimization (GRPO) was applied to refine the model's performance.The SFT-trained model was further fine-tuned using LoRA Hu et al. [2021] with the GRPO method, implemented in Unsloth Daniel Han and team [2023] with VLLM Kwon et al. [2023b] for efficient inference.A carefully designed reward function guided the optimization process, and model checkpoints were saved every 200 steps to track improvements.</p>
<p>This two-stage pipeline mirrors the multi-stage training approach employed in DeepSeek-R1 [Guo et al., 2025], where initial RL training is followed by supervised finetuning for refinement.In our case, SFT provided a robust starting point for reinforcement learning (RL), allowing GRPO to focus on refining reasoning capabilities and enhancing task-specific performance.</p>
<p>Experiments and Results</p>
<p>Dataset Details</p>
<p>The dataset is constructed through a multi-stage process involving generation, refinement, and augmentation.The process begins with the creation of a large initial pool of 530,000 synthetic mazes.These mazes are generated using the maze-dataset framework Ivanitskiy et al. [2023], which employs a randomized depth-first search algorithm.This algorithm ensures that every generated maze has a guaranteed solution path connecting the designated origin and target locations.Further details of algorithm used can be found at Appendix 1.All mazes within the dataset have a fixed size of 5x5 grids.From this extensive initial pool, a subset of 30,000 mazes is randomly selected and reserved as a held-out test set.This separation guarantees that the training and evaluation data are entirely distinct, preventing data leakage and enabling a robust assessment of model generalization.</p>
<p>The remaining 500,000 mazes form the basis for the various training datasets used in this work.This pool of mazes undergoes a multi-stage processing and augmentation procedure to create datasets specifically tailored for different training objectives.</p>
<p>Reset Dataset Creation: A significant portion of the training data focuses on teaching the model to recover from errors.To this end, a "reset" dataset is created.This dataset is generated by algorithmically producing incorrect solution paths for the mazes.These incorrect paths are designed to be plausible but ultimately unsuccessful, either leading to dead ends or deviating from the correct solution.Importantly, they adhere to constraints: they do not revisit locations already visited within the incorrect attempt, and they avoid portions of the known correct solution path.</p>
<p>Associated with each incorrect path, a textual "RE-SET" message is generated, simulating the feedback a system might provide upon encountering an error.The content of this message depends on whether the incorrect path terminates at a dead end (three surrounding walls) or simply deviates from the wrong route.These incorrect paths, along with their reset messages and the correct solution's Chain-of-Thought (COT) reasoning, are combined.This process results in approximately 400,000 training examples where the model is presented with scenarios of initial failure(s) followed by a successful attempt after a "reset."The intent is to train the model to recognize incorrect trajectories and adapt its strategy.Illustrative examples of reset samples are provided in Appendix.</p>
<p>SFT Training Data Construction: The final Supervised Fine-Tuning (SFT) training dataset is a balanced combination of "straight success" examples and "retry" examples:</p>
<p> Straight Success Data (250,000 mazes): This portion consists of mazes where the model is expected to generate the correct solution path on the first attempt, without any resets.</p>
<p> Retry Data (250,000 mazes): This portion is drawn from the "reset" dataset described above, providing examples where the model learns from incorrect attempts and subsequent resets.This combined 500,000-maze SFT set, encompassing success and error recovery, enables robust learning.</p>
<p>GRPO Training Data:</p>
<p>The remaining 150,000 mazes from the original "straight success" data pool are used to create a dataset for GRPO stage.</p>
<p>MazeBench</p>
<p>To rigorously evaluate the spatial reasoning and planning capabilities of large language models (LLMs), we introduce MazeBench, a novel benchmark consisting of a curated collection of 100 maze-solving challenges.While existing benchmarks often assess logical reasoning or commonsense knowledge, MazeBench specifically targets the ability of LLMs to understand spatial relationships, plan multi-step paths, and execute sequential actions within a constrained environment.This capacity is crucial for applications ranging from robotics and navigation to game playing and virtual agent control.</p>
<p>MazeBench is a collection of 100 unique mazes, randomly selected from a larger test set containing 30,000 mazes.It is designed to evaluate the performance of large language models (LLMs) by categorizing mazes into different difficulty levels.Each maze requires the model to determine an optimal path from the starting point to the target, with difficulty primarily based on the number of steps needed to reach the goal.</p>
<p>The benchmark is structured into three levels: Easy, Medium, and Hard, ensuring a progressive assessment of an LLM's pathfinding and problem-solving abilities.The components are described in Table 1.The Easy category consists of 50 mazes, each requiring between 1 and 4 steps to solve.These simpler mazes establish a baseline for evaluating fundamental navigation skills.</p>
<p>The Medium category includes 40 mazes that demand solution paths of 5 to 8 steps.These mazes introduce a higher level of complexity, requiring more advanced planning and spatial reasoning.Successfully solving them indicates an LLM's ability to manage moderately intricate environments.</p>
<p>The Hard category comprises 10 mazes, each necessitating 9 to 13 steps to reach the target.These mazes present the greatest challenge, testing an LLM's capacity to handle long-range dependencies and navigate complex spatial structures.Performance on this level reflects the model's ability to process and reason over extended solution paths.</p>
<p>As mentioned previously, the mazes are presented to the LLM in a tokenized input format; the full details of this representation, including examples, are provided in Section 3.1.The LLM is expected to produce sequence of movement tokens.During evaluation, we will parse the LLM's output to extract these tokens.The order of these tokens is crucial.The presence of extraneous characters, whitespace, or other tokens will not automatically invalidate the solution, provided that the correct sequence of movement tokens can be extracted.A solution is considered incorrect if the extracted sequence of movement tokens does not lead to the target or leads to an invalid state (e.g., attempting to move into a wall) is considered incorrect.The evaluation metric is the success rate: the percentage of mazes solved correctly.</p>
<p>Quantitative Results</p>
<p>Model Performance on MazeBench</p>
<p>As shown in Table 2, the baseline model, trained for direct path prediction without explicit reasoning, achieved 0% accuracy on MazeBench.This highlights the necessity of step-by-step reasoning for the task.The SFT-only model reached a baseline of 86.0%, demonstrating the effectiveness of supervised fine-tuning for learning step-bystep maze navigation.Further enhancement with GRPO led to significant improvement, reaching 93.0% after 1600 steps of GRPO training.over GRPO steps along with a linear regression trendline (red dashed line) and its 1 standard deviation bounds.The steady increase in the trendline indicates that GRPO effectively guides the model towards improved maze-solving policies.</p>
<p>Qualitative Results</p>
<p>Qualitative analysis of model outputs revealed notable differences in reasoning behavior.The baseline model often produced nonsensical or incomplete movement sequences, frequently failing to reach the target and exhibiting "hallucinations" by predicting movements invalid within the maze structure.The AlphaMaze-SFT model demonstrated improved coherence and step-by-step progression, but still struggled with longer or more complex mazes, sometimes becoming trapped in loops or making incorrect turns in later stages of the solution path.</p>
<p>In contrast, the AlphaMaze-SFT+GRPO model exhibited the most sophisticated reasoning.In many instances, emergent chain-of-thought patterns were observed, with AlphaMaze (two-stage) appearing to explicitly consider wall constraints and spatial relationships at each step before predicting the next movement.Furthermore, outputs occasionally displayed instances reminiscent of the "aha moments" reported in prior work on DeepSeek-R1.For example, in some complex mazes, Al-phaMaze (two-stage) would initially begin along one path, then appear to "re-evaluate" its trajectory mid-sequence, correcting its course to find a more efficient or correct solution.Error analysis indicated that AlphaMaze (twostage) made fewer invalid moves and was more robust to long-context reasoning challenges compared to the AlphaMaze-SFT model.However, limitations remained, particularly in mazes requiring backtracking or complex spatial planning beyond the immediate next step.</p>
<p>Discussion</p>
<p>Analysis of GRPO's Impact on Visual Maze Reasoning</p>
<p>Our results clearly demonstrate the incremental benefit of Group Relative Policy Optimization (GRPO) in enhancing visual maze reasoning within Large Language Models.While Supervised Fine-Tuning (SFT) establishes a strong foundation, enabling the model to achieve a 86% accuracy on MazeBench, the application of GRPO further elevates performance to 93% after 1600 training steps.This improvement, albeit seemingly modest in percentage points, is significant considering the already strong baseline established by SFT.It suggests that GRPO is effectively refining the model's policy, leading to more robust and accurate maze navigation.</p>
<p>The qualitative analysis provides further insight into the nature of this improvement.The AlphaMaze-SFT+GRPO model exhibited more pronounced chainof-thought reasoning patterns and instances of selfcorrection, indicating that GRPO is not merely finetuning the existing SFT policy, but rather encouraging more sophisticated reasoning processes.The reward function, designed to incentivize correctness, valid movements, and structured output, likely plays a crucial role in shaping this behavior.By rewarding successful navigation and penalizing invalid steps, GRPO encourages the model to learn more deliberate and considered movement strategies.</p>
<p>Comparison with DeepSeek-R1 and RL for Reasoning</p>
<p>It is important to note that the base DeepSeek-R1 model, when operating with an extremely long context window, demonstrates emergent visual reasoning capabilities.However, our experiments reveal that the distilled variants (DeepSeek-R1 Distill-Qwen models) do not carry over these spatial reasoning abilities, as evidenced by their 0% accuracy on MazeBench.This suggests that the distillation process into Qwen or other smaller models is insufficient to preserve the emergent ability of visual spatial reasoning observed in the base model.In contrast, our two-stage training approach-combining Supervised Fine-Tuning (SFT) to establish foundational step-by-step reasoning with Group Relative Policy Optimization (GRPO) for further refinement-effectively equips the distilled model with robust visual maze-solving skills.Even with only 1600 GRPO steps, the model achieves a notable improvement, reaching 93% accuracy and exhibiting clear chain-ofthought behaviors along with self-correction during navigation.</p>
<p>These findings underscore the necessity of specialized training to recover or enhance spatial reasoning in distilled models, highlighting that while the base DeepSeek-R1 model is capable of visual reasoning with sufficient context, additional training stages are crucial to maintain or induce this capability in smaller, distilled variants.</p>
<p>Limitations</p>
<p>Despite the encouraging results, our study is not without limitations.Firstly, the performance gain from GRPO, while statistically significant, is small (7% accuracy improvement in our reported experiment).Further investigation is needed to explore whether more extensive GRPO training, or modifications to the reward function, could lead to more substantial performance gains.It is possible that the current reward function, while effective, could be further optimized to better incentivize more complex reasoning strategies, such as backtracking or more proactive exploration of alternative paths.</p>
<p>Secondly, our evaluation, while including qualitative analysis, is primarily based on maze-solving accuracy.This metric, while important, provides a somewhat limited view of the model's reasoning capabilities.Future work could benefit from more nuanced evaluation metrics that assess the efficiency of the generated paths, the robustness of the model to maze complexity variations, and the interpretability of the model's internal reasoning process.Furthermore, while we observed qualitative signs of chain-of-thought reasoning, a more rigorous analysis, perhaps using techniques from interpretability research, is needed to definitively characterize the nature and depth of the model's reasoning process.</p>
<p>Finally, our experiments are limited to synthetically generated mazes.While these mazes were designed to vary in size and complexity, they may not fully capture the intricacies and variability of real-world visual spatial reasoning tasks.Future research should explore the generalizability of our approach to more diverse and ecologically valid visual environments and tasks.</p>
<p>Conclusion</p>
<p>This paper introduced AlphaMaze, a novel approach to enhance Large Language Models' spatial intelligence, focusing on maze navigation.We demonstrated the efficacy of a two-stage training framework, leveraging Supervised Fine-Tuning (SFT) followed by Group Relative Policy Optimization (GRPO).While initial pre-trained LLMs exhibited 0% accuracy on MazeBench, highlighting the need for task-specific adaptation, our approach successfully imbued a distilled LLM with robust spatial reasoning capabilities.SFT provided a crucial foundation by teaching step-by-step movement prediction from tokenized maze inputs, reaching 86% accuracy.This underscores the importance of structured input and targeted training for LLMs to effectively engage with visual spatial information.</p>
<p>Crucially, we adapted and applied the two-stage training methodology pioneered by DeepSeek-R1, demonstrating its generalizability beyond language-centric reasoning tasks.Following SFT, GRPO fine-tuning further elevated performance to 93% on MazeBench after 1600 training steps, showcasing the power of reinforcement learning to refine reasoning processes in a novel domain.Qualitative analysis revealed that GRPO fostered more sophisticated and self-corrective reasoning strategies, including emergent chain-of-thought patterns, mirroring observations in DeepSeek-R1 and suggesting a common mechanism for enhanced reasoning through RL.</p>
<p>Our work contributes to the broader effort of expanding LLMs' reasoning abilities beyond natural language, demonstrating the potential of a two-stage approach for visually grounded tasks.The success of GRPO, inspired by DeepSeek-R1's advancements in language reasoning, highlights the transferability of these techniques to spatial domains.This suggests that carefully designed reinforcement learning, following an initial phase of supervised task learning, can be a powerful method to unlock and refine sophisticated reasoning capabilities in LLMs across diverse problem spaces.The implications extend beyond maze navigation to a wide array of applications demanding spatial understanding and sequential decisionmaking.</p>
<p>Future research will focus on further validating this two-stage GRPO approach across various reasoning domains beyond spatial tasks, exploring its potential to enhance LLMs' capabilities in areas such as symbolic reasoning, logical deduction, and planning.Investigating the optimal configurations of SFT and GRPO stages, diversifying training data to encompass richer and more complex reasoning scenarios, and developing more refined reward functions tailored to different reasoning challenges are critical next steps.By pursuing these directions, we aim to establish the broader applicability of this two-stage training paradigm for imbuing standard LLMs with robust and versatile reasoning abilities, paving the way for more capable and generally intelligent language models.</p>
<p>A Algorithm</p>
<p>This appendix details the algorithm used to generate the maze reasoning dataset with reset demonstrations.The algorithm processes a base dataset of maze navigation problems and augments it with demonstration of incorrect attempts followed by resets and correct solutions.</p>
<p>Algorithm 1 Maze Reasoning Reset Data Generation -Main Process Require: Base dataset D containing maze problems with:</p>
<p>1:</p>
<p>-Adjacency list representation of 5  5 maze grid 2:</p>
<p>-Origin and target coordinates for n steps from max n steps down to 1 do 6:</p>
<p>Attempt to extend path from N until a dead end or n steps are reached.</p>
<p>7:</p>
<p>if path length = n steps or a dead end is reached then end if 13: end for 14: for all path p  W P do 15:</p>
<p>Generate chain-of-thought steps for path p.</p>
<p>16:</p>
<p>Add "Heading in wrong direction" message.</p>
<p>17:</p>
<p>Add RESET marker.18: end for 19: Append original correct solution (path P ).20: Format as conversation pairs.21: End Procedure</p>
<p>Figure 1 :
1
Figure 1: MazeBench scores over GRPO steps with a linear regression trendline and its 1 standard deviation bounds.</p>
<p>Figure 2 :
2
Figure 2: Visual of the Example Maze</p>
<p>Figure 3 :
3
Figure 3: Visualization of AlphaMaze's step-by-step reasoning process while solving a maze.</p>
<p>-</p>
<p>Correct solution path Ensure: Augmented dataset with reset demonstrations 4: Initialize empty datasets D 1 and D 2 5: for all example e  D do 6: Extract adjacency list A, origin O, target T , and path P from e Combine processed examples from D 1 and D 2 into the final dataset Algorithm 2 Order-1 Processing (1 wall at origin) 1: Procedure ProcessOrder1(example) 2: W P   {Initialize wrong paths set} 3: for all adjacent node N to origin O do 4:if N /  correct path P then 5:</p>
<p>Table 1 :
1
Maze Configuration by Difficulty Level
Category Number of MazesStepsEasy501 -4Medium405 -8Hard109 -13Total1001 -13</p>
<p>Table 2
2: Maze Solving Accuracy on MazeBenchModelSFT GRPO ScoreBaseline-1.5B0.0Baseline-7B0.0Baseline-1.5B (SFT)0.0AlphaMaze-SFT86.0AlphaMaze93.04.3.2 Model Evolution During GRPOFigure 1 displays the MazeBench scores (blue crosses)
&lt;|0 -0| &gt; &lt;| up_left_wall | &gt; &lt;| blank | &gt; &lt;|0 -1| &gt; &lt;| up_down_wall | &gt; &lt;| blank | &gt; &lt;|0 -2| &gt; &lt;| up_down_wall | &gt; &lt;| blank | &gt; &lt;|0 -3| &gt; &lt;| up_down_right_wall | &gt; &lt;| blank | &gt; &lt;|0 -4| &gt; &lt;| up_left_right_wall | &gt; &lt;| blank | &gt; &lt;|1 -0| &gt; &lt;| down_left_wall | &gt; &lt;| blank | &gt; &lt;|1 -1| &gt; &lt;| up_down_wall | &gt; &lt;| blank | &gt; &lt;|1 -2| &gt; &lt;| up_right_wall | &gt; &lt;| blank | &gt; &lt;|1 -3| &gt; &lt;| up_down_left_wall | &gt; &lt;| blank | &gt; &lt;|1 -4| &gt; &lt;| right_wall | &gt; &lt;| blank | &gt; &lt;|2 -0| &gt; &lt;| up_left_wall | &gt; &lt;| blank | &gt; &lt;|2 -1| &gt; &lt;| up_down_right_wall | &gt; &lt;| origin | &gt; &lt;|2 -2| &gt; &lt;| left_right_wall | &gt; &lt;| blank | &gt; &lt;|2 -3| &gt; &lt;| up_left_wall | &gt; &lt;| blank | &gt; &lt;|2 -4| &gt; &lt;| right_wall | &gt; &lt;| blank | &gt; &lt;|3 -0| &gt; &lt;| left_wall | &gt; &lt;| blank | &gt; &lt;|3 -1| &gt; &lt;| up_right_wall | &gt; &lt;| blank | &gt; &lt;|3 -2| &gt; &lt;| down_left_wall | &gt; &lt;| blank | &gt; &lt;|3 -3|&gt; &lt;| down_right_wall | &gt; &lt;| blank | &gt; &lt;|3 -4| &gt; &lt;| left_right_wall | &gt; &lt;| blank | &gt; &lt;|4 -0| &gt; &lt;| down_left_right_wall | &gt; &lt;| blank | &gt; &lt;|4 -1| &gt; &lt;| down_left_wall | &gt; &lt;| blank | &gt; &lt;|4 -2| &gt; &lt;| up_down_wall | &gt; &lt;| target | &gt; &lt;|4 -3| &gt; &lt;| up_down_wall | &gt; &lt;| blank | &gt; &lt;|4 -4| &gt; &lt;| down_right_wall | &gt; &lt;| blank | &gt;</p>
<p>Self-play fine-tuning converts weak language models to strong language models. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu, 2024</p>
<p>. Michael Han, Daniel Han, Unsloth Team Unsloth, 2023</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Duyu Guo, Guoxin Xu, Yuchen Chen, Chen Tang, Others , arXiv:2501.129482025arXiv preprint</p>
<p>Lora: Low-rank adaptation of large language models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, 2021</p>
<p>Cecilia Diniz Behn, and Samy Wu Fung. A configurable library for generating and manipulating maze datasets. Rusheb Michael Igorevich Ivanitskiy, Alex F Shah, Tilman Spies, Dan Ruker, Can Valentine, Lucia Rager, Chris Quirke, Guillaume Mathwin, Corlouer, Saba Janamian and MD Sahabul Alam. Maze solver robot using a* algorithm. 2023. 2023</p>
<p>Supervised fine-tuning in turn improves visual foundation models. Xiaohu Jiang, Yixiao Ge, Yuying Ge, Dachuan Shi, Chun Yuan, Ying Shan, 2024</p>
<p>Reward design with language models. Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh, 2023a</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023b</p>
<p>Pathfinding algorithms. Patrick Lester, </p>
<p>Imagine while reasoning in space: Multimodal visualization-of-thought. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vuli, Furu Wei, 2025</p>
<p>A survey on vision-language-action models for embodied ai. Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King, 2024</p>
<p>Neural-symbolic visual reasoning: A survey. Jiajun Mao, Chuang Gan, Fan Zhang, Others , 2023</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, 2024</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, 10.18653/v1/2023.acl-long.153Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, M. Dai, and Quoc V. Le.2022a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Denny Zhou, Quoc Le, Denny Zhou, Quoc Le, Others , Advances in Neural Information Processing Systems. 2022b35</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Deep reinforcement learning for maze solving. Samy Zafrany, 2020</p>
<p>Algorithm 3 Order-2 Processing (2 walls at origin) 1: Procedure ProcessOrder2(example) 2: for n steps from max n steps down to 1 do 3: Generate wrong path W P. Jingyi Zhang, Jiaxing Huang, Sheng Jin, Shijian Lu, Add "Heading in wrong direction" message. 10: end if 11: Add RESET marker. 20245Generate chain-of-thought for W P . 6: if W P ends at a dead end. Append original correct solution (path P ). 16: Format as conversation pairs. 17: End Procedure</p>            </div>
        </div>

    </div>
</body>
</html>
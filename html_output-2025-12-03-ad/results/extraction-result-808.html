<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-808 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-808</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-808</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-248721802</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2205.06111v2.pdf" target="_blank">Asking for Knowledge: Training RL Agents to Query External Knowledge Using Language</a></p>
                <p><strong>Paper Abstract:</strong> To solve difficult tasks, humans ask questions to acquire knowledge from external sources. In contrast, classical reinforcement learning agents lack such an ability and often resort to exploratory behavior. This is exacerbated as few present-day environments support querying for knowledge. In order to study how agents can be taught to query external knowledge via language, we first introduce two new environments: the grid-world-based Q-BabyAI and the text-based Q-TextWorld. In addition to physical interactions, an agent can query an external knowledge source specialized for these environments to gather information. Second, we propose the"Asking for Knowledge"(AFK) agent, which learns to generate language commands to query for meaningful knowledge that helps solve the tasks. AFK leverages a non-parametric memory, a pointer mechanism and an episodic exploration bonus to tackle (1) irrelevant information, (2) a large query language space, (3) delayed reward for making meaningful queries. Extensive experiments demonstrate that the AFK agent outperforms recent baselines on the challenging Q-BabyAI and Q-TextWorld environments.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e808.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e808.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AFK</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Asking for Knowledge (AFK) agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning agent that learns when and how to generate language queries to a task-agnostic oracle, using a non-parametric notebook memory, a pointer mechanism to restrict query vocabulary, and an episodic exploration bonus to encourage meaningful queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AFK</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Actor-style RL agent (PPO in Q-BabyAI, DQN in Q-TextWorld) with: (1) a non-parametric notebook memory F (a set of disjoint sets of notes) that stores the instruction and oracle replies and merges notes by similarity; (2) a notebook encoder f_note (GRU in Q-BabyAI, transformer in Q-TextWorld) whose per-note embeddings are aggregated by a Deep Set f_set to produce h_s; (3) an observation encoder f_obs (CNN for grid observations, transformer text encoder for TextWorld) producing h_o; (4) an aggregator f_att (FiLM+LSTM or multi-head attention) to combine h_o and h_s into h_x; (5) five policy heads: a switch head (decide physical vs query), physical action head, and three query heads (function word, adjective, noun); (6) a pointer mechanism that masks and attends over words present in the instruction-related notebook set A_0 to produce distributions for adjective/noun selections; (7) an episodic exploration bonus b awarded when a query response newly expands A_0. The agent thereby learns to generate compositional template queries <func, adj, noun>, store and attend to replies, and integrate them into action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Q-BabyAI (grid-based) and Q-TextWorld (text-based)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Queryable POMDP environments built from BabyAI (grid, egocentric partial 7x7 views) and TextWorld (text-only observations). Each environment is partially observable and augmented with a task-agnostic, queryable oracle that returns token sequences (or 'I don't know') for template queries; knowledge facts are many and include irrelevant information, requiring selective querying and multi-hop question sequences for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>A queryable oracle knowledge source (task-agnostic textual knowledge facts). In Q-TextWorld the oracle is named 'Charlie'. The oracle is accessed by language-template queries <func, adj, noun> and returns token sequences or 'I don't know'.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual sequences (token sequences) corresponding to values in knowledge-fact key-value pairs; short answers like locations, object identities, affordances, or 'I don't know'.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Non-parametric 'notebook' memory F: a set of disjoint sets A_i; new oracle responses are matched to existing sets via a similarity function Sim(u,v) (uni-gram or bi-gram) with threshold α; related sets are merged (union) and the notebook's A_0 (the set containing the episode instruction) is encoded via f_note (GRU/transformer) and aggregated by a Deep Set f_set to produce a fixed-size task-relevant belief embedding h_s. The agent only attends to A_0 during decision-making to filter out irrelevant replies.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>On receiving a response v_i, construct index set S = {j | ∃ v ∈ A_j s.t. Sim(v_i, v) ≥ α}. If S non-empty, form A_k = (⋃_{j∈S} A_j) ∪ {v_i} and replace those sets with A_k (k = min_j∈S j). If S empty, append new singleton set {v_i}. The instruction v_0 is always in A_0; the agent focuses on encoding A_0 (instruction-related notes) into h_s. The episodic bonus b = β · I[(v_i ∈ A_0) ∧ (v_i ∉ A'_0)] rewards additions that expand A_0.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (model-free RL): PPO actor-critic in Q-BabyAI and DQN for Q-TextWorld; action-selection conditioned on combined observation and notebook embedding h_x. Query generation is learned via separate heads with pointer/masking to reduce action-space.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Learned navigation policy via physical action head sampling from π_phy conditioned on h_x (no explicit search algorithm such as A* is used or reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Q-BabyAI (Level 1): AFK achieves ~100% success on many level-1 tasks (e.g., Object in Box ♣ 100.0±0.0%, Danger ♠ 100.0±0.0%, Go to Favorite ♦ 100.0±0.0%, Open Door ♥ 100.0±0.0%). Q-TextWorld: Take 1: 85.1±2.9% success; Take 2: 61.9±6.5%; Take 1 Cut: 43.5±15.9%; Take 2 Cut: 0.0±0.0% (success rate, %).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>No Query / Query Baseline on Q-BabyAI Level 1: Object in Box No Query 50.5±2.0%, Query Baseline 49.8±1.2%; Danger No Query 68.3±2.4%, Query Baseline 73.8±1.2%; Go to Favorite No Query 98.9±0.8%, Query Baseline 99.3±0.3%; Open Door No Query 99.7±0.3%, Query Baseline 85.3±22.3% (success rate, %). Q-TextWorld: No Query Take 1: 75.1±4.1%, Query Baseline 73.5±5.8%; Take 2: No Query 24.0±6.6%, Query Baseline 13.7±8.5%; Take 1 Cut: No Query 24.6±1.0%, Query Baseline 22.9±3.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Storing oracle replies in a structured non-parametric notebook and restricting query generation via a pointer mechanism substantially improves performance: AFK outperforms baselines on tasks that require querying (Object in Box, Danger) and reduces episode length on tasks where query helps efficiency. The episodic exploration bonus encourages asking novel, task-relevant questions but can be over-exploited in larger text environments. Ablations show removing notebook, pointer, or episodic exploration degrades success rates (~22.9%, 11.4%, and 25.2% drops on average respectively), indicating all three components are crucial for integrating tool outputs into belief and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>name_short_added_by_parser</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e808.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e808.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Query Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla Query Baseline agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline agent augmented with query-generation heads that can issue template queries to the oracle but lacks the notebook, pointer mechanism, and episodic exploration bonus; struggles with large query action spaces and rarely benefits from tool outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Query Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baseline RL agent (same base as No Query) extended with query heads producing <func, adj, noun> tokens without the pointer-based masking and without the non-parametric notebook or episodic bonus. Queries are produced from unconstrained vocabulary distributions, which enlarges the action space and makes learning difficult; the policy often converges to not issuing queries.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Q-BabyAI and Q-TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same partially observable, oracle-augmented environments as AFK.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Queryable oracle knowledge source (same as AFK) if the agent chooses to query.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual token sequences or 'I don't know'.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (PPO in Q-BabyAI, DQN in Q-TextWorld) with added query heads; no explicit mechanism to integrate replies into a persistent task-specific belief.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Learned navigation policy via physical action head; no explicit planning/search reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Often worse than AFK and sometimes worse than No Query due to large action space; e.g., Q-BabyAI level-1 results show Query Baseline success around 49.8% (Object in Box) and 73.8% (Danger) and poor performance on some tasks (see paper tables). Q-TextWorld: Take 1: 73.5±5.8%; Take 2: 13.7±8.5%; Take 1 Cut: 22.9±3.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>N/A (baseline comparison is No Query which does not use query actions): No Query performance for same tasks reported (see AFK entry).</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simply adding query heads without mechanisms to filter, store, and encourage meaningful queries is ineffective: Query Baseline often fails to learn to query usefully and sometimes converges to policies that issue no queries, underperforming AFK.</td>
                        </tr>
                        <tr>
                            <td><strong>name_short_added_by_parser</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e808.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e808.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No Query</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>No-Query baseline agent (BabyAI/TextWorld baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard RL baseline that does not have the ability to issue queries to the oracle; relies solely on environment observations and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>No Query</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Vanilla RL agent (standard BabyAI/TextWorld architectures) that has only physical actions (no query action space) and relies on standard encoders and policy learning (PPO/DQN) to solve tasks by exploration and interaction with the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Q-BabyAI and Q-TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same partially observable environments without access to the external oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (PPO or DQN) without external tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Learned navigation via physical action policy; no explicit path planner.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Reported in paper tables (e.g., Q-BabyAI Object in Box 50.5±2.0% success; Danger 68.3±2.4%; Q-TextWorld Take 1 75.1±4.1%; Take 2 24.0±6.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>No Query can solve some tasks by exhaustive exploration but fails on tasks that require querying (e.g., Object in Box and Danger require queries to succeed reliably); querying capability can improve efficiency and make otherwise-unsolvable tasks solvable.</td>
                        </tr>
                        <tr>
                            <td><strong>name_short_added_by_parser</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e808.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e808.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nguyen2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nguyen & Daumé (2019) oracle-assisted navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work where an agent can issue a 'help' signal and receive an instruction from an oracle in 3D visual navigation tasks; agents learn when to request help but not what to ask.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Oracle-assisted navigation agent (Nguyen & Daumé 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent can emit a help/request signal that causes an oracle to return a language instruction describing a subtask or direction toward the goal; the framework trains when to issue help signals but the agent has no control over the content/form of the assistance it receives.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>3D visual navigation environments (from Nguyen & Daumé)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable 3D navigation tasks where assistance is provided by an oracle upon help requests; the oracle supplies subgoal instructions rather than arbitrary query replies.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>An assistance oracle that returns textual subtask instructions when activated by a 'help' signal.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual instructions/subgoal descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy with a learned when-to-ask mechanism; assistance is used as an external cue but not integrated via an explicit notebook as in AFK.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Not specified here beyond standard learned navigation conditioned on observations and assistance when provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shows benefit of learning when to request assistance for navigation, but the agent cannot control the content of the assistance (i.e., it learns when but not what to query), motivating AFK's focus on learning what to ask.</td>
                        </tr>
                        <tr>
                            <td><strong>name_short_added_by_parser</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e808.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e808.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nguyen2022</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nguyen et al. (2022) hierarchical ask framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concurrent work proposing a framework for 3D navigation where agents can query an oracle for current state, current goal, and subgoal information to improve navigation in unseen environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning When and What to Ask: a Hierarchical Reinforcement Learning Framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Hierarchical assistance-requesting navigation agent (Nguyen et al. 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Framework where agents learn both when and what to ask in a hierarchical RL setting for navigation; the oracle can provide structured assistance like current state, goal, and subgoal information to guide planning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>3D navigation environments (assistance-enabled)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable navigation domains where oracle queries can return state/goal/subgoal info; designed to improve generalization in unseen spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Oracle providing structured navigation assistance (state/goal/subgoal).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual/structured instructions or state descriptions (subgoals).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Hierarchical RL that conditions subpolicies on oracle-provided subgoal information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Learned hierarchical navigation policies (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates that learning both when and what to ask can improve navigation generalization; cited as complementary to AFK which studies what to ask in language-heavy POMDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>name_short_added_by_parser</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e808.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e808.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yuan2020/2021</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adhikari et al. / Yuan et al. — belief/graph methods for text games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related lines of work that build belief-graph or dynamic belief representations to generalize in text-based games and support information gathering; cited as related to maintaining belief in text POMDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic belief graphs to generalize on text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Dynamic belief-graph agents (Adhikari et al. / Yuan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Approaches that construct and update structured belief graphs (nodes representing entities/locations/relations) from textual observations to help planning and generalization in partially observable text games.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based game environments (TextWorld-style)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Highly abstracted, partially observable text games where states are latent and require language understanding; belief graphs help aggregate across sparse textual signals.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Not necessarily external tools — these methods build internal structured belief representations from text; cited as complementary techniques for belief maintenance.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Internal structured graph representations (nodes/edges), derived from textual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Dynamic belief graphs (structured graph memory updated from observations over time).</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Graph nodes/edges are added/updated based on parsed observations and interaction outcomes (details in the cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policies that condition on dynamic belief graphs for generalization in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Graph-based reasoning over beliefs; specifics in cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as related work that explicitly constructs and uses structured belief representations in text-based POMDPs; such approaches are alternative or complementary to AFK's notebook-style belief.</td>
                        </tr>
                        <tr>
                            <td><strong>name_short_added_by_parser</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e808.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e808.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nakano2021</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nakano et al. (2021) / related agents for executable-command navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned work where agents generate sequences of executable commands (e.g., Ctrl+F operations) to navigate partially observable text environments to gather information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Executable-command navigation agents (Nakano et al. 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that learn to generate sequences of executable commands (non-standard navigation/querying actions) to traverse and search partially observable text environments for information retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Partially observable text environments (various text-based settings)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-only POMDPs with combinatorially large action spaces; these agents exploit command sequences to search and gather information.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Environment-provided executable commands (e.g., search/utility commands) viewed as tools for navigation/information gathering.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual feedback from executed commands.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy to produce multi-step command sequences for retrieval and navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Command-sequence-based navigation/search; details in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as related research demonstrating alternative interaction primitives for information gathering in text POMDPs, relevant to AFK's objective of learning language-based queries.</td>
                        </tr>
                        <tr>
                            <td><strong>name_short_added_by_parser</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning <em>(Rating: 2)</em></li>
                <li>Learning When and What to Ask: a Hierarchical Reinforcement Learning Framework <em>(Rating: 2)</em></li>
                <li>Learning dynamic belief graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Learning dynamic belief graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>Ask the right questions: Active question reformulation with reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-808",
    "paper_id": "paper-248721802",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "AFK",
            "name_full": "Asking for Knowledge (AFK) agent",
            "brief_description": "A reinforcement-learning agent that learns when and how to generate language queries to a task-agnostic oracle, using a non-parametric notebook memory, a pointer mechanism to restrict query vocabulary, and an episodic exploration bonus to encourage meaningful queries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "AFK",
            "agent_description": "Actor-style RL agent (PPO in Q-BabyAI, DQN in Q-TextWorld) with: (1) a non-parametric notebook memory F (a set of disjoint sets of notes) that stores the instruction and oracle replies and merges notes by similarity; (2) a notebook encoder f_note (GRU in Q-BabyAI, transformer in Q-TextWorld) whose per-note embeddings are aggregated by a Deep Set f_set to produce h_s; (3) an observation encoder f_obs (CNN for grid observations, transformer text encoder for TextWorld) producing h_o; (4) an aggregator f_att (FiLM+LSTM or multi-head attention) to combine h_o and h_s into h_x; (5) five policy heads: a switch head (decide physical vs query), physical action head, and three query heads (function word, adjective, noun); (6) a pointer mechanism that masks and attends over words present in the instruction-related notebook set A_0 to produce distributions for adjective/noun selections; (7) an episodic exploration bonus b awarded when a query response newly expands A_0. The agent thereby learns to generate compositional template queries &lt;func, adj, noun&gt;, store and attend to replies, and integrate them into action selection.",
            "environment_name": "Q-BabyAI (grid-based) and Q-TextWorld (text-based)",
            "environment_description": "Queryable POMDP environments built from BabyAI (grid, egocentric partial 7x7 views) and TextWorld (text-only observations). Each environment is partially observable and augmented with a task-agnostic, queryable oracle that returns token sequences (or 'I don't know') for template queries; knowledge facts are many and include irrelevant information, requiring selective querying and multi-hop question sequences for some tasks.",
            "is_partially_observable": true,
            "external_tools_used": "A queryable oracle knowledge source (task-agnostic textual knowledge facts). In Q-TextWorld the oracle is named 'Charlie'. The oracle is accessed by language-template queries &lt;func, adj, noun&gt; and returns token sequences or 'I don't know'.",
            "tool_output_types": "Textual sequences (token sequences) corresponding to values in knowledge-fact key-value pairs; short answers like locations, object identities, affordances, or 'I don't know'.",
            "belief_state_mechanism": "Non-parametric 'notebook' memory F: a set of disjoint sets A_i; new oracle responses are matched to existing sets via a similarity function Sim(u,v) (uni-gram or bi-gram) with threshold α; related sets are merged (union) and the notebook's A_0 (the set containing the episode instruction) is encoded via f_note (GRU/transformer) and aggregated by a Deep Set f_set to produce a fixed-size task-relevant belief embedding h_s. The agent only attends to A_0 during decision-making to filter out irrelevant replies.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "On receiving a response v_i, construct index set S = {j | ∃ v ∈ A_j s.t. Sim(v_i, v) ≥ α}. If S non-empty, form A_k = (⋃_{j∈S} A_j) ∪ {v_i} and replace those sets with A_k (k = min_j∈S j). If S empty, append new singleton set {v_i}. The instruction v_0 is always in A_0; the agent focuses on encoding A_0 (instruction-related notes) into h_s. The episodic bonus b = β · I[(v_i ∈ A_0) ∧ (v_i ∉ A'_0)] rewards additions that expand A_0.",
            "planning_approach": "Learned policy (model-free RL): PPO actor-critic in Q-BabyAI and DQN for Q-TextWorld; action-selection conditioned on combined observation and notebook embedding h_x. Query generation is learned via separate heads with pointer/masking to reduce action-space.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Learned navigation policy via physical action head sampling from π_phy conditioned on h_x (no explicit search algorithm such as A* is used or reported).",
            "performance_with_tools": "Q-BabyAI (Level 1): AFK achieves ~100% success on many level-1 tasks (e.g., Object in Box ♣ 100.0±0.0%, Danger ♠ 100.0±0.0%, Go to Favorite ♦ 100.0±0.0%, Open Door ♥ 100.0±0.0%). Q-TextWorld: Take 1: 85.1±2.9% success; Take 2: 61.9±6.5%; Take 1 Cut: 43.5±15.9%; Take 2 Cut: 0.0±0.0% (success rate, %).",
            "performance_without_tools": "No Query / Query Baseline on Q-BabyAI Level 1: Object in Box No Query 50.5±2.0%, Query Baseline 49.8±1.2%; Danger No Query 68.3±2.4%, Query Baseline 73.8±1.2%; Go to Favorite No Query 98.9±0.8%, Query Baseline 99.3±0.3%; Open Door No Query 99.7±0.3%, Query Baseline 85.3±22.3% (success rate, %). Q-TextWorld: No Query Take 1: 75.1±4.1%, Query Baseline 73.5±5.8%; Take 2: No Query 24.0±6.6%, Query Baseline 13.7±8.5%; Take 1 Cut: No Query 24.6±1.0%, Query Baseline 22.9±3.6%.",
            "has_tool_ablation": true,
            "key_findings": "Storing oracle replies in a structured non-parametric notebook and restricting query generation via a pointer mechanism substantially improves performance: AFK outperforms baselines on tasks that require querying (Object in Box, Danger) and reduces episode length on tasks where query helps efficiency. The episodic exploration bonus encourages asking novel, task-relevant questions but can be over-exploited in larger text environments. Ablations show removing notebook, pointer, or episodic exploration degrades success rates (~22.9%, 11.4%, and 25.2% drops on average respectively), indicating all three components are crucial for integrating tool outputs into belief and planning.",
            "name_short_added_by_parser": null,
            "uuid": "e808.0"
        },
        {
            "name_short": "Query Baseline",
            "name_full": "Vanilla Query Baseline agent",
            "brief_description": "A baseline agent augmented with query-generation heads that can issue template queries to the oracle but lacks the notebook, pointer mechanism, and episodic exploration bonus; struggles with large query action spaces and rarely benefits from tool outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Query Baseline",
            "agent_description": "Baseline RL agent (same base as No Query) extended with query heads producing &lt;func, adj, noun&gt; tokens without the pointer-based masking and without the non-parametric notebook or episodic bonus. Queries are produced from unconstrained vocabulary distributions, which enlarges the action space and makes learning difficult; the policy often converges to not issuing queries.",
            "environment_name": "Q-BabyAI and Q-TextWorld",
            "environment_description": "Same partially observable, oracle-augmented environments as AFK.",
            "is_partially_observable": true,
            "external_tools_used": "Queryable oracle knowledge source (same as AFK) if the agent chooses to query.",
            "tool_output_types": "Textual token sequences or 'I don't know'.",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": null,
            "planning_approach": "Learned policy (PPO in Q-BabyAI, DQN in Q-TextWorld) with added query heads; no explicit mechanism to integrate replies into a persistent task-specific belief.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Learned navigation policy via physical action head; no explicit planning/search reported.",
            "performance_with_tools": "Often worse than AFK and sometimes worse than No Query due to large action space; e.g., Q-BabyAI level-1 results show Query Baseline success around 49.8% (Object in Box) and 73.8% (Danger) and poor performance on some tasks (see paper tables). Q-TextWorld: Take 1: 73.5±5.8%; Take 2: 13.7±8.5%; Take 1 Cut: 22.9±3.6%.",
            "performance_without_tools": "N/A (baseline comparison is No Query which does not use query actions): No Query performance for same tasks reported (see AFK entry).",
            "has_tool_ablation": false,
            "key_findings": "Simply adding query heads without mechanisms to filter, store, and encourage meaningful queries is ineffective: Query Baseline often fails to learn to query usefully and sometimes converges to policies that issue no queries, underperforming AFK.",
            "name_short_added_by_parser": null,
            "uuid": "e808.1"
        },
        {
            "name_short": "No Query",
            "name_full": "No-Query baseline agent (BabyAI/TextWorld baseline)",
            "brief_description": "Standard RL baseline that does not have the ability to issue queries to the oracle; relies solely on environment observations and exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "No Query",
            "agent_description": "Vanilla RL agent (standard BabyAI/TextWorld architectures) that has only physical actions (no query action space) and relies on standard encoders and policy learning (PPO/DQN) to solve tasks by exploration and interaction with the environment.",
            "environment_name": "Q-BabyAI and Q-TextWorld",
            "environment_description": "Same partially observable environments without access to the external oracle.",
            "is_partially_observable": true,
            "external_tools_used": "None.",
            "tool_output_types": null,
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": null,
            "planning_approach": "Learned policy (PPO or DQN) without external tool use.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Learned navigation via physical action policy; no explicit path planner.",
            "performance_with_tools": null,
            "performance_without_tools": "Reported in paper tables (e.g., Q-BabyAI Object in Box 50.5±2.0% success; Danger 68.3±2.4%; Q-TextWorld Take 1 75.1±4.1%; Take 2 24.0±6.6%).",
            "has_tool_ablation": false,
            "key_findings": "No Query can solve some tasks by exhaustive exploration but fails on tasks that require querying (e.g., Object in Box and Danger require queries to succeed reliably); querying capability can improve efficiency and make otherwise-unsolvable tasks solvable.",
            "name_short_added_by_parser": null,
            "uuid": "e808.2"
        },
        {
            "name_short": "Nguyen2019",
            "name_full": "Nguyen & Daumé (2019) oracle-assisted navigation",
            "brief_description": "Work where an agent can issue a 'help' signal and receive an instruction from an oracle in 3D visual navigation tasks; agents learn when to request help but not what to ask.",
            "citation_title": "Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning",
            "mention_or_use": "mention",
            "agent_name": "Oracle-assisted navigation agent (Nguyen & Daumé 2019)",
            "agent_description": "Agent can emit a help/request signal that causes an oracle to return a language instruction describing a subtask or direction toward the goal; the framework trains when to issue help signals but the agent has no control over the content/form of the assistance it receives.",
            "environment_name": "3D visual navigation environments (from Nguyen & Daumé)",
            "environment_description": "Partially observable 3D navigation tasks where assistance is provided by an oracle upon help requests; the oracle supplies subgoal instructions rather than arbitrary query replies.",
            "is_partially_observable": true,
            "external_tools_used": "An assistance oracle that returns textual subtask instructions when activated by a 'help' signal.",
            "tool_output_types": "Textual instructions/subgoal descriptions.",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Learned policy with a learned when-to-ask mechanism; assistance is used as an external cue but not integrated via an explicit notebook as in AFK.",
            "uses_shortest_path_planning": null,
            "navigation_method": "Not specified here beyond standard learned navigation conditioned on observations and assistance when provided.",
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Shows benefit of learning when to request assistance for navigation, but the agent cannot control the content of the assistance (i.e., it learns when but not what to query), motivating AFK's focus on learning what to ask.",
            "name_short_added_by_parser": null,
            "uuid": "e808.3"
        },
        {
            "name_short": "Nguyen2022",
            "name_full": "Nguyen et al. (2022) hierarchical ask framework",
            "brief_description": "Concurrent work proposing a framework for 3D navigation where agents can query an oracle for current state, current goal, and subgoal information to improve navigation in unseen environments.",
            "citation_title": "Learning When and What to Ask: a Hierarchical Reinforcement Learning Framework",
            "mention_or_use": "mention",
            "agent_name": "Hierarchical assistance-requesting navigation agent (Nguyen et al. 2022)",
            "agent_description": "Framework where agents learn both when and what to ask in a hierarchical RL setting for navigation; the oracle can provide structured assistance like current state, goal, and subgoal information to guide planning.",
            "environment_name": "3D navigation environments (assistance-enabled)",
            "environment_description": "Partially observable navigation domains where oracle queries can return state/goal/subgoal info; designed to improve generalization in unseen spaces.",
            "is_partially_observable": true,
            "external_tools_used": "Oracle providing structured navigation assistance (state/goal/subgoal).",
            "tool_output_types": "Textual/structured instructions or state descriptions (subgoals).",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Hierarchical RL that conditions subpolicies on oracle-provided subgoal information.",
            "uses_shortest_path_planning": null,
            "navigation_method": "Learned hierarchical navigation policies (details in cited work).",
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Demonstrates that learning both when and what to ask can improve navigation generalization; cited as complementary to AFK which studies what to ask in language-heavy POMDPs.",
            "name_short_added_by_parser": null,
            "uuid": "e808.4"
        },
        {
            "name_short": "Yuan2020/2021",
            "name_full": "Adhikari et al. / Yuan et al. — belief/graph methods for text games",
            "brief_description": "Related lines of work that build belief-graph or dynamic belief representations to generalize in text-based games and support information gathering; cited as related to maintaining belief in text POMDPs.",
            "citation_title": "Learning dynamic belief graphs to generalize on text-based games",
            "mention_or_use": "mention",
            "agent_name": "Dynamic belief-graph agents (Adhikari et al. / Yuan et al.)",
            "agent_description": "Approaches that construct and update structured belief graphs (nodes representing entities/locations/relations) from textual observations to help planning and generalization in partially observable text games.",
            "environment_name": "Text-based game environments (TextWorld-style)",
            "environment_description": "Highly abstracted, partially observable text games where states are latent and require language understanding; belief graphs help aggregate across sparse textual signals.",
            "is_partially_observable": true,
            "external_tools_used": "Not necessarily external tools — these methods build internal structured belief representations from text; cited as complementary techniques for belief maintenance.",
            "tool_output_types": "Internal structured graph representations (nodes/edges), derived from textual observations.",
            "belief_state_mechanism": "Dynamic belief graphs (structured graph memory updated from observations over time).",
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": "Graph nodes/edges are added/updated based on parsed observations and interaction outcomes (details in the cited works).",
            "planning_approach": "Learned policies that condition on dynamic belief graphs for generalization in text games.",
            "uses_shortest_path_planning": null,
            "navigation_method": "Graph-based reasoning over beliefs; specifics in cited works.",
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited as related work that explicitly constructs and uses structured belief representations in text-based POMDPs; such approaches are alternative or complementary to AFK's notebook-style belief.",
            "name_short_added_by_parser": null,
            "uuid": "e808.5"
        },
        {
            "name_short": "Nakano2021",
            "name_full": "Nakano et al. (2021) / related agents for executable-command navigation",
            "brief_description": "Mentioned work where agents generate sequences of executable commands (e.g., Ctrl+F operations) to navigate partially observable text environments to gather information.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Executable-command navigation agents (Nakano et al. 2021)",
            "agent_description": "Agents that learn to generate sequences of executable commands (non-standard navigation/querying actions) to traverse and search partially observable text environments for information retrieval.",
            "environment_name": "Partially observable text environments (various text-based settings)",
            "environment_description": "Text-only POMDPs with combinatorially large action spaces; these agents exploit command sequences to search and gather information.",
            "is_partially_observable": true,
            "external_tools_used": "Environment-provided executable commands (e.g., search/utility commands) viewed as tools for navigation/information gathering.",
            "tool_output_types": "Textual feedback from executed commands.",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Learned policy to produce multi-step command sequences for retrieval and navigation.",
            "uses_shortest_path_planning": null,
            "navigation_method": "Command-sequence-based navigation/search; details in cited work.",
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited as related research demonstrating alternative interaction primitives for information gathering in text POMDPs, relevant to AFK's objective of learning language-based queries.",
            "name_short_added_by_parser": null,
            "uuid": "e808.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning",
            "rating": 2,
            "sanitized_title": "help_anna_visual_navigation_with_natural_multimodal_assistance_via_retrospective_curiosityencouraging_imitation_learning"
        },
        {
            "paper_title": "Learning When and What to Ask: a Hierarchical Reinforcement Learning Framework",
            "rating": 2,
            "sanitized_title": "learning_when_and_what_to_ask_a_hierarchical_reinforcement_learning_framework"
        },
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games",
            "rating": 2,
            "sanitized_title": "learning_dynamic_belief_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games",
            "rating": 2,
            "sanitized_title": "learning_dynamic_belief_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Ask the right questions: Active question reformulation with reinforcement learning",
            "rating": 1,
            "sanitized_title": "ask_the_right_questions_active_question_reformulation_with_reinforcement_learning"
        }
    ],
    "cost": 0.0209715,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Asking for Knowledge : Training RL Agents to Query External Knowledge Using Language</p>
<p>Iou-Jen Liu 
Xingdi Yuan 
Marc-Alexandre Côté 
Pierre-Yves Oudeyer 
Alexander G Schwing 
Asking for Knowledge : Training RL Agents to Query External Knowledge Using Language</p>
<p>To solve difficult tasks, humans ask questions to acquire knowledge from external sources. In contrast, classical reinforcement learning agents lack such an ability and often resort to exploratory behavior. This is exacerbated as few present-day environments support querying for knowledge. In order to study how agents can be taught to query external knowledge via language, we first introduce two new environments: the grid-world-based Q-BabyAI and the text-based Q-TextWorld. In addition to physical interactions, an agent can query an external knowledge source specialized for these environments to gather information. Second, we propose the 'Asking for Knowledge' (AFK) agent, which learns to generate language commands to query for meaningful knowledge that helps solve the tasks. AFK leverages a non-parametric memory, a pointer mechanism and an episodic exploration bonus to tackle (1) irrelevant information, (2) a large query language space, (3) delayed reward for making meaningful queries. Extensive experiments demonstrate that the AFK agent outperforms recent baselines on the challenging Q-BabyAI and Q-TextWorld environments. The code of the environments and agents are available at https://ioujenliu.github.io/AFK.</p>
<p>Introduction</p>
<p>To solve challenging tasks, humans query external knowledge sources, i.e., we ask for help. We also constantly create knowledge sources (e.g., manuals), as it is often more economical in the long term than users exploring via trial and * Equal contribution †Work partially done while visiting MSR  error. Moreover, cognitive science research (Maratsos, 2007;Mills et al., 2010;Ronfard et al., 2018) showed that learning to ask questions and to interpret answers is key in a child's development of problem-solving skills. Consequently, we hypothesize that autonomous agents can address more complicated tasks if they can successfully learn to query external knowledge sources. For querying, it seems desirable to use some form of language. Not only does this allow to leverage existing knowledge sources built for humans, but also does it enable us to interpret the queries.</p>
<p>However, the literature to teach agents to query external knowledge sources via language is scarce. Nguyen &amp; Daume (2019) consider agents that can request help in visual navigation tasks. The agent can issue a 'help' signal, and expects the environment to provide an instruction leading it to a place towards the goal. Hence, agents learn when to query, but do not have control of what to query. Zhong et al. (2020) show that agents can better address novel tasks when a manual is available. However, the manual contains all relevant information and agents don't need to learn to query. Kovac et al. (2021) discuss the open challenge of building agents that learn social interaction skills mixing physical action and language. They show that state-of-the-art deep reinforcement learning systems cannot learn several kinds of social interaction skills. Instead of social skills, we focus on the open challenge of learning to ask for knowledge using language and propose an effective approach.</p>
<p>To deliberately study how agents can be taught to  query, we introduce two environments: the grid-worldbased Q-BabyAI, illustrated in Fig. 1 and inspired by BabyAI (Chevalier-Boisvert et al., 2019), and the text-based Q-TextWorld inspired by TextWorld (Côté et al., 2018). In addition to physical interactions, an agent can use a query language to gather information related to a task. Importantly, in Q-BabyAI and Q-TextWorld, the knowledge source is designed to be task-agnostic, i.e., it replies to all queries, even if irrelevant to the task at hand. This mimics many realworld knowledge sources, e.g., search engines, which return results based on a user's query, regardless of relevance.</p>
<p>When training agents to query external knowledge via language, three main challenges arise: (1) The action space for generating a language query is large. Even with a template language, the action space grows combinatorially and large action spaces remain a challenge for reinforcement learning (Dulac-Arnold et al., 2016;Ammanabrolu &amp; Hausknecht, 2020).</p>
<p>(2) Irrelevant knowledge queried from a task-agnostic knowledge source can confuse agents. As a result, learning to ask meaningful questions is critical. This challenge is in line with the cognitive science finding (Mills et al., 2010) that children must learn to ask questions that result in answers with useful information.</p>
<p>(3) Rewards for queries are often significantly delayed and sparse. Since the knowledge source provides specific information rather than a solution, agents have to also understand how to use the acquired information before receiving a significant reward. This mimics the discovery of Mills et al. (2010) that children must learn to use the received information.</p>
<p>To address the three challenges, we propose the 'asking for knowledge' (AFK) agent. The AFK agent is equipped with a pointer mechanism and a non-parametric memory, which we refer to as a 'notebook,' while using an episodic exploration strategy. The pointer mechanism addresses the challenge of a combinatorially growing action space by restricting the available actions based on the current context. The notebook keeps track of all the information related to the task at hand. The episodic exploration strategy deals with delayed and sparse rewards by issuing an exploration bonus when the agent makes novel and meaningful queries, inspired by information-seeking and epistemic curiosity observed in children (Engel, 2011;Gottlieb et al., 2013;Kidd &amp; Hayden, 2015).</p>
<p>Comparing this AFK agent to recent baselines on Q-BabyAI and Q-TextWorld, we observe the AFK agent to ask more meaningful questions and to better leverage the acquired knowledge to solve the tasks.</p>
<p>Queryable Environments</p>
<p>We first discuss a reinforcement learning (RL) context where agents can query. We then introduce two new environments, Q-BabyAI and Q-TextWorld, each expanded from prior work (Chevalier-Boisvert et al., 2019;Côté et al., 2018).</p>
<p>Problem Setting</p>
<p>Reinforcement learning considers an agent interacting with an environment and collecting reward over discrete time. The environment is formalized by a partially observable Markov Decision Process (POMDP) (Sutton &amp; Barto, 2018). Formally, a POMDP is defined by a tuple (S, A, Z, T , O, R, γ, H). S is the state space. A is the action space. Z is the observation space. At each time step t, the agent receives an observation o t ∈ Z following the observation function O ∶ S → Z and selects an action a t ∈ A. The transition function T maps the action a t and the current state s t to a distribution over the next state s t+1 , i.e., T ∶ S × A → ∆(S). The agent receives a real-valued reward r t according to a reward function R ∶ S × A → R. The agent's goal is to maximize the return ∑ H t=0 γ t r t , where γ is the discount factor and H is the horizon.</p>
<p>In a queryable environment, in addition to observations representing its surrounding, an agent also receives a response from the knowledge source upon issuing a query. Formally, the observation space Z = Z env × Z q is composed of Z env and Z q , representing the agent's surrounding and the reponse to a query, respectively.</p>
<p>Similarly, at each step, the agent's action space A = A phy ∪ A q is composed of the physical action space A phy supported by classical RL environments (e.g., navigational actions, toggle, grasp) and the query action space A q .</p>
<p>Response Space Z q and Query Action Space A q : As a controllable starting point for this research, we equip the environments with a queryable oracle knowledge source. Specifically, whenever receiving a sequence of tokens as a query, the oracle replies with a sequence of tokens. To consider the compositionality of language while reducing the burden of precise natural language generation, we define a template format for queries and responses. This design is also compatible with our plan of extending the knowledge source to more natural forms like databases.</p>
<p>A query is defined as a 3-tuple of <func, adj, noun>.</p>
<p>In this 3-tuple, func is a function word selected from words like where's, what's and how's, which indicates the function of a query (e.g., inquire about an object's location or affordances). The combination of an adjective (adj) and a noun enables to refer to a unique object within the environment.</p>
<p>Given a query, the oracle replies with a sequence of tokens. For this, the oracle has access to a set of "knowledge facts" associated with a particular instantiation of the environment. The knowledge facts are key-value pairs, where keys are the aforementioned 3-tuple of <func, adj, noun> and values are sequences of tokens. If a given query matches a key in the set of knowledge facts, the oracle will return the corresponding value. Otherwise, the oracle returns the message I don't know.</p>
<p>Crucially, the set of knowledge facts is much larger than necessary and irrelevant information is, by design, accessible to the agent. For instance, when tasked to find Mary's toy, information about Tim and Tim's toy is also available if queried. Gathering irrelevant information may lead to confusion and subsequent sub-optimal decisions. Moreover, some tasks require multi-hop information gathering (e.g., Object in Box), in which the agent must ask follow-up questions to get all information needed to solve it.</p>
<p>Information Sufficiency: Practically, agents that can query have two main advantages. First, for environments containing sufficient information to be solved via exhaustive exploration, querying can provide a more natural and effective way to gather information (e.g., reducing the policy length). Second, for environments that only provide partial information (e.g., an agent must recognize and avoid danger tiles by trial-and-error, but danger tiles are randomly assigned per episode), only querying will lead to successful completion of the tasks.</p>
<p>To study both advantages, we augment BabyAI (Chevalier-Boisvert et al., 2019) and TextWorld (Côté et al., 2018) with a queryable knowledge source. We design tasks where the environment contains sufficient information, but we add knowledge facts which can help the agent to reduce exploration if used adequately. In addition, we design other tasks where agents can only succeed when they are able to query. We provide details next.</p>
<p>Q-BabyAI</p>
<p>We first introduce Q-BabyAI, an extension of the BabyAI environment (Chevalier-Boisvert et al., 2019). We devise four level 1 tasks, namely Object in Box, Danger, Go to Favorite and Open Door. In Fig. 2, we provide examples of agents querying the knowledge source for each of the level 1 tasks after receiving the goal instruction for that episode. The four tasks permit to study the two advantages mentioned above.</p>
<p>Specifically, both the Object in Box and Danger tasks can only be solved 100% of the time when querying is used to reveal the necessary knowledge -opening the wrong box or stepping on the danger tile terminates the game. In contrast, for the Go to Favorite and Open Door tasks, an agent can exhaust the environment to accomplish the goals. However, querying the knowledge source can greatly boost the agent's efficiency in both tasks. To prevent agents from memorizing solutions (e.g., Mary's toy is in the red box), we randomly place objects and tiles in the environment, as well as shuffle the entity names and colors in every episode.</p>
<p>For the Object in Box and Danger tasks, we use a singleroom setting to separate the difficulties of navigation and querying. In the Go to Favorite and Open Door tasks, we use a multi-room setting. It is worth noting that in the Open Door task, only querying at specific locations (i.e., next to doors) can result in meaningful answers.</p>
<p>Having the four level 1 tasks defined, we increase the diffi-  Figure 3. An overview of the AFK agent. An embedding of the notebook (h s ; see Sec. 3.1) and the environment (h o ) is combined (h x ) for use in five policy functions. The policies for query generation make use of notebook information (h w ) (see Sec. 3.2). culty by composing them into more challenging higher-level tasks. For level k tasks, we combine k different tasks selected from the four level 1 tasks. As a result, we have six level 2 tasks, four level 3 tasks and one level 4 task. As an example, Open Door + Object in Box is a level 2 task where the instruction could be Find the key to the door, and find Mary's toy. To solve the task, an agent must figure out what is Mary's toy, where it is, and which key opens the locked door. We provide full details of the Q-BabyAI tasks including statistics in Appendix A.1.</p>
<p>Q-TextWorld</p>
<p>We also develop Q-TextWorld, augmenting the TextWorld environment (Côté et al., 2018) with a queryable knowledge source. Given a few configuration parameters, Q-TextWorld can generate new text-based games on the fly. Those interactive text environments are POMDPs with text-only observations and action space. Agents interact with the game by issuing short text phrases as actions, then get textual feedback. Text-based games provide a different view from their vision-or grid-based counterparts that can make things harder: 1) states are represented by highly abstracted signals requiring language understanding to interpret correctly; 2) the action space is combinatorially large due to compositionality of language; 3) levels of verbosity, i.e., amount of irrelevant text, can potentially confuse an agent.</p>
<p>In this work, we adopt the cooking themed setting from prior work (Adhikari et al., 2020). An example is shown in Fig. 2. In all games, an agent must gather cooking ingredients, which are placed randomly in the room, either visible to the agent, or hidden inside some containers that need to be opened first. In a more difficult setting, each ingredient also needs to be cut in a specific way (i.e., chopped, sliced, or diced). The agent must query the knowledge source to obtain that information and then act accordingly by issuing the right action while holding both the ingredient and a knife. We provide full details of the Q-TextWorld tasks including statistics in Appendix A.2.</p>
<p>Being consistent with the Q-BabyAI tasks, we study both advantages of having a querying behavior -improve efficiency and acquire necessary information. In games where cutting is not involved, agents can rely on exhaustive exploration to gather all portable objects and win. However, knowing what and where the required ingredients are can improve efficiency significantly. In contrast, in games where the ingredients need to be cut, an incorrect operation on the ingredient terminates the game. Hence, querying the recipe is the only way to perform above random.</p>
<p>Asking for Knowledge (AFK) Agent</p>
<p>In this section, we first present an overview of the 'Asking for Knowledge' (AFK) agent before we discuss details.</p>
<p>Overview: As illustrated in Fig. 3, the goal of the agent is to solve a task specified by an instruction. The language-based instruction (sequence of tokens) v 0 is provided at the start of each episode. At each time step t, the agent receives an environment observation o env t ∈ Z env . Moreover, if the agent issued a query at time step t − 1, it also receives a language response v t ∈ Z q from the oracle, otherwise v t = ∅.</p>
<p>To reduce the amount of noisy information (i.e., v t unrelated to the task at hand), we develop a non-parametric memory for gathered information, which we refer to as a 'notebook.' The notebook is a collection of sets where related information are being combined into a single set. The AFK agent only looks at the set that contains the task instruction v 0 , which determines relevance to the task. Upon processing the notebook we obtain a representation h s (details in Sec. 3.1).</p>
<p>We combine the environment observation o env t and the notebook representation h s relevant for the task via an aggregator module (Perez et al., 2018;Vaswani et al., 2017). Given the output of the aggregator, h x ∈ R l , where l is the encoding size, we use five heads to generate the physical actions and the language query actions. Specifically, we use a switch head π switch (⋅|h
x ) ∶ R l → ∆({0, 1}), a physical action head π phy (⋅|h x ) ∶ R l → ∆(A phy ), a func- tion word head: π func (⋅|h x ) ∶ R l → ∆(V func ), an adjec- tive head π adj (⋅|h x ) ∶ R l → ∆(V adj ), and a noun head π noun (⋅|h x ) ∶ R l → ∆(V noun ).
Here, ∆(X) represents a distribution with support X, A phy is the physical action space, and V func , V adj , V noun are the function word, adjective, and noun vocabulary spaces.</p>
<p>The switch head decides whether the agent executes a physical action or issues a query. Conceptually, the agent first samples a value z according to π switch (⋅|h x ). If z = 0, the agent will sample a physical action a phy from π phy (⋅|h x ) which is subsequently executed. In contrast, if z = 1, the agent will issue the query [w func , w adj , w noun ] by sampling a function word w func , an adjective w adj , and a noun w noun independently from π func (⋅|h x ), π adj (⋅|h x ), and π noun (⋅|h x ). Note that the query action space A q is large, which makes RL training challenging. We provide detailed statistics in Appendix A.</p>
<p>To alleviate issues due to the large action space, we adopt a pointer network (See et al., 2017) to generate queries. Specifically, the pointer network is restricted to 'point' to words occurring in the notebook. This ensures that the generated query uses words that are related to the already gathered information (details in Sec. 3.2).</p>
<p>In addition, to deal with delayed and sparse rewards, we propose an episodic exploration method which further incentivizes an agent to ask questions that are related to the task at hand (details in Sec. 3.3).</p>
<p>Notebook</p>
<p>In the following, we discuss the notebook's construction and describe the computation of the encoding h s .</p>
<p>Notebook construction: Let F (for facts) denote the notebook, which is a non-parametric memory. Formally, F is a set of disjoint sets, i.e.,
F = {A i } |F |−1 i=0 and A i ∩ A j = ∅, ∀i ≠ j. For each set A i , each element v ∈ A i
represents either a response from the oracle or the task instruction.</p>
<p>At the beginning of each episode, the notebook F is initialized with a singleton A 0 = {v 0 } that contains the task instruction v 0 , i.e., F = {{v 0 }}. When an agent receives a new response v i ≠ ∅, we first find all sets that contain information related to v i in the notebook. Formally, we construct an index set S that consists of the indices of related sets, i.e.,
S = {j|∃v ∈ A j s.t. Sim(v i , v) ≥ α}, where Sim(u, v) ∈ [0, 1]
is a similarity function and α ∈ [0, 1] is a threshold. We study both the uni-gram and bi-gram similarity (Kondrak, 2005). If S is not empty, we combine all the related sets and the new response v i to obtain a new set
A k . Formally, A k = ⋃ j∈S A j ∪ {v i }, where k = min j∈S j.
Then, all sets {A j } j∈S are replaced with the new set A k . If the index set S is empty, we add A k = {v i } to F , where k is the next available index. Importantly, note that the task instruction v 0 is always part of the set A 0 .</p>
<p>Notebook encoding: To discard noisy information coming from responses unrelated to the task at hand, the AFK agent only considers the set A 0 which contains the task instruction v 0 . We use a recurrent neural network f note<br />
to encode each 'note' in A 0 , i.e., for each v i ∈ A 0 (i ∈ {1, . . . , |A 0 |}), we have h i = f note (v i ) ∈ R |v i |×l ,
where |v i | is the number of words in v i and l is the hidden dimension. To further encode the instruction related notes (i.e., A 0 ) as a whole, we use a Deep Set model f set (Zaheer et al., 2017)
, i.e., h s = f set ([h 1 , . . . , h |A 0 | ]) ∈ R l ,
where h s is the resulting encoding. In addition, the input observation o env is encoded via a neural network f obs , i.e.,
h o = f obs (o env ) ∈ R l , where h o is the resulting observation encoding. An aggregator module is used to combine h o and h s , i.e., h x = f att (h o , h s ) ∈ R l .</p>
<p>Pointer Mechanism for Language Generation</p>
<p>To address the challenge of a combinatorially growing action space, we develop a pointer mechanism for the policies π adj and π noun . Concretely, the pointer mechanism restricts the AFK agent queries to use only the words appearing in the set A 0 .</p>
<p>We achieve this by first applying a mask before computing the policy distributions π adj and π noun , i.e., π adj and π noun only have non-zero probability for adjectives and nouns in the instruction related set of notes A 0 . We use the generation process of the noun as an example. Let m noun denote the number of nouns in A 0 , and let h w ∈ R m noun ×l denote the word encodings of all nouns in A 0 . Using attention queries q ∈ R l and keys k ∈ R m noun ×l such that
q = h x ⋅ W q , and k = h w ⋅ W k ,(1)
with learnable parameters W q , W k ∈ R l×l , we compute the attention e noun over all nouns in A 0 as
e noun = softmax(q ⋅ k T ) ∈ R m noun .(2)
A distribution over the noun vocabulary, i.e., V noun , is then constructed from e noun . Specifically, for each word w ∈
V noun , we have π noun (w) = ∑ m noun i=1 e i noun I[d(i) = w],
where d(i) maps the index i to the corresponding word in A 0 and e i noun represents the i-th element of e noun . I is the indicator function. The pointer mechanism for π adj is constructed similarly. We defer details to Appendix C.</p>
<p>Episodic Exploration</p>
<p>To deal with delayed and sparse rewards, inspired by Savinov et al. (2019), we develop an episodic exploration mechanism to encourage the agent to ask questions related to the task at hand.  At each time step, the agent receives reward r
♣♠ 0.0±0.0 0.0±0.0 90.3±1.8 ♣♦ 0.1±0.1 0.6±0.5 94.3±2.3 ♣♥ 0.0±0.0 0.0±0.0 99.0±0.4 ♠♦ 0.4±0.1 0.2±0.2 100.0±0.0 ♠♥ 0.0±0.0 0.0±0.0 0.0±0.0 ♦♥ 84.1±0.3 94.0±3.3 98.7±0.2 Lv. 3 ♣♠♦ 0.0±0.0 0.0±0.0 0.15±0.2 ♣♠♥ 0.0±0.0 0.0±0.0 0.0±0.0 ♣♦♥ 0.0±0.0 0.0±0.0 2.1±0.8 ♠♦♥ 4.3±1.0 4.4±0.8 4.8±0.9 Lv. 4 ♣♠♦♥ 0.0±0.0 0.0±0.0 0.0±0.1= r env + b,
where r env is the external reward and b is the bonus reward.</p>
<p>A positive bonus reward b is obtained whenever a query's response v i ≠ ∅ expands the agent's knowledge about the task, i.e., A 0 . The reward is only given for new information.
Formally, b = β(I[(v i ∈ A 0 ) ∧ (v i ∉ A ′ 0 )]),(3)
where v i denotes a new response returned by the oracle, and A ′ 0 denotes the set from the previous game step containing the task instruction v 0 . β &gt; 0 is a scaling factor and I is the indicator function.</p>
<p>Experimental Results</p>
<p>In this section, we present the experimental setup, evaluation protocol, and our results on Q-BabyAI and Q-TextWorld. posed agent via AFK, which is the agent with 1) notebook, 2) pointer mechanism, and 3) episodic exploration.</p>
<p>We follow the original training protocols used in BabyAI and TextWorld. Specifically, we train all agents in Q-BabyAI environments with proximal policy optimization (PPO) (Schulman et al., 2017) for 20M -50M environment steps, depending on the tasks' difficulty. For Q-TextWorld agents, we use the Deep Q-Network (Mnih et al., 2013;Hessel et al., 2018) and the agents are trained for 500K episodes. We provide implementation details in Appendix C.</p>
<p>Evaluation Protocol: In Q-BabyAI, the policy is evaluated in an independent evaluation environment every 50 model updates and each evaluation consists of 500 evaluation episodes. To ensure a fair and rigorous evaluation, we follow the evaluation protocols suggested by Henderson et al. (2017); Colas et al. (2018) and report the 'final metric'. The final metric is the average evaluation success rate of the last ten models in the training process, i.e., average success rate of the last 5000 evaluation episodes. In Q-TextWorld, we report the final running average training scores with a window size of 1000. Note, in each episode, entities are randomly spawned preventing agents from memorizing training games. All experiments are repeated five times with different random seeds. has to be queried. This demonstrates that the AFK agent asks more meaningful questions and successfully leverages oracle replies to solve tasks. We provide extra analysis to support this in a later subsection. In addition, we observe that in tasks where the instruction provides sufficient information, e.g., Go to Favorite (♦) and Open Door (♥), all agents are able to solve the tasks. However, as shown in Tab. 2, an AFK agent needs fewer steps to solve the tasks. This suggests that meaningful queries can result in better efficiency. Training curves are shown in Fig. 4. See Appendix E for training curves and results of all experiments.</p>
<p>Q-BabyAI</p>
<p>To show the limitation of the proposed approach, we run experiments on the very challenging level 3 and level 4 tasks of Q-BabyAI. As shown in Tab. 1, AFK as well as all baselines fail to solve the level 3 and level 4 tasks due to the tasks' high complexity and very sparse rewards. This shows that training RL agents to query in language is still a very challenging and open problem which needs more attention from our community.</p>
<p>Q-TextWorld Results: We compare AFK with the baseline agents on Q-TextWorld in Tab. 3. Specifically, we conduct experiments on four settings with gradually increasing difficulty. Here, 'Take k' denotes that an agent needs to collect k food ingredients, which may spawn in containers and are hence invisible to the agent before the container is opened. 'Cut' indicates that the collected food ingredients need to be cut in specific ways, for which the recipe needs to be queried. As shown in Tab. 3, AFK significantly outperform the baselines on three of the tasks. Analogously to Q-BabyAI experiments, the No Query agent sometimes outperforms the Query Baseline agent. We believe this is caused by 1) the larger action space of the Query Baseline compared to No Query, and 2) a missing mechanism helping the agent to benefit from queries -together they reduce the Query Baseline's chance to experience meaningful trajectories. We observe that none of the agents can get non-zero scores on the Take 2 Cut task. We investigate the agents' training reward curves (Fig. 10, Appendix E): while the baselines get 0 reward, AFK actually learns to obtain higher reward. We suspect that due to the richer entity presence in Q-TextWorld, and the resulting larger number of valid questions (connected to A 0 ), AFK may exploit the exploration bonus and ask more questions than necessary. This suggests that better reward assignment methods are needed for agents to perform in more complex environments.</p>
<p>Ablation Study: We perform an ablation study to examine the effectiveness of the proposed 1) notebook, 2) pointer mechanism, and 3) episodic exploration. For this, we use various level 1 and level 2 Q-BabyAI tasks. The results are reported in Tab. 4. As shown in Tab. 4, removing the notebook, the pointer mechanism, or the episodic exploration results in the success rate dropping by 22.9%, 11.4%, and 25.2% on average. This demonstrates that all three proposed components are essential for an AFK agent to successfully generate meaningful queries and solve tasks.</p>
<p>Generalization: To assess an AFK agent's capability of making meaningful queries and solving different, novel, unseen tasks, we perform a generalization study. Specifically, we train AFK agents on a set of level 2 source tasks. Then the trained AFK agent is tested on an unseen level 2 target task (new combination of sub-tasks used in training). The results are summarized in Tab. 5. As shown in Tab. 5, upon training on source tasks, an AFK agent achieves 22.1% and 35.6% success rate on the level 2 target tasks 'Object in Box + Danger'(♣♠) and 'Danger + Go to Favorite'(♠♦), which the agent has never seen during training. In contrast, the Query Baseline only achieves a 0.0% and 0.2% success rate on the target tasks after training 20M steps directly on the target tasks (Tab. 1).</p>
<p>Query Quality: To gain more insights, we study the quality of the queries issued by an agent. Each episode of our tasks is associated with a set of queries Q t which are useful for solving the task. If an agent issues a query q ∈ Q t , the query is considered 'good.' We refer to the number of good queries (not counting duplicates) and total number of queries (counting duplicates) generated by the agent in one episode as n g and n tot . We report the average precision, recall, and F1 score (Sasaki, 2007) of the generated queries over 200 evaluation episodes. Specifically, precision = n g n tot , recall = n g |Q t | , and F1 score is the harmonic mean of precision and recall. As shown in Tab. 6, the AFK agent achieves high F1 scores across various tasks. In contrast, the Query Baseline converges to a policy that does not issue any query and thus has zero precision and recall in all tasks. This demonstrates AFK's ability to learn to ask relevant questions.</p>
<p>Related Work</p>
<p>Information Seeking Agents: In recent years a host of works discussed building of information seeking agents. Nguyen &amp; Daume (2019) propose to leverage an oracle in 3D navigation environments. The oracle is activated in response to a special signal from the agent and provides a language instruction describing a subtask the agent could follow. Kovac et al. (2021) design grid-world tasks similar to ours, but focus on the social interaction perspective. For instance, some agents are required to emulate their social peers' behavior to successfully communicate with them.</p>
<p>Potash &amp; Suleman (2019) propose a game setting which requires agents to ask sequences of questions efficiently to guess the target sentence from a set of candidates. Yuan (2021); Nakano et al. (2021) propose agents that can generate sequences of executable commands (e.g., Ctrl+F a token) to navigate through partially observable text environments for information gathering. The line of research on curiosity-driven exploration and intrinsic motivation shares the same overall goal to seek information . A subset of them, countbased exploration methods, count the visit of observations or states and encourage agents to gather more information from rarely experienced states (Bellemare et al., 2016;Ostrovski et al., 2017;Savinov et al., 2019;Liu et al., 2021). Our work also loosely relates to the active learning paradigm, where a system selects training examples wisely so that it achieves better model performance, while also consuming fewer training examples (Cohn et al., 1994;Bachman et al., 2017;Fang et al., 2017). Different from existing work, we aim to study explicit querying behavior using language. We design tasks where querying behavior can either greatly improve efficiency or is needed to succeed.</p>
<p>In a concurrent work, Nguyen et al. (2022) propose a framework tailored to 3D navigation environments: agents can query an oracle to obtain useful information (e.g., current state, current goal and subgoal). They show that navigation agents can take advantage of an assistance-requesting policy and improve navigation in unseen environments.</p>
<p>Reinforcement Learning with External Knowledge:</p>
<p>Training reinforcement learning agents which use external knowledge sources also received attention recently ( (2021) consider a set of detailed instructions as knowledge source. They propose an architecture to aggregate the given external knowledge with the RL model. The aforementioned works assume the external knowledge is given and the agent doesn't need to learn to query. In contrast, we consider a task-agnostic interactive knowledge source. In our Q-BabyAI and Q-TextWorld environments, an agent must learn to actively execute meaningful queries in language to solve a task.  (2018) propose to reverse question answering: given a document and a phrase, a model is required to generate a question. The question can be answered by the phrase using the document as context. In later work, Scialom &amp; Staiano (2020) define curiosity-driven question generation. Query reformulation is a technique which aims to obtain better answers from the knowledge source (e.g., a search engine) by training agents to modify questions (Nogueira &amp; Cho, 2017;Buck et al., 2018). Another loosely related area is multi-hop retrieval (Das et al., 2018;Xiong et al., 2021;Feldman &amp; El-Yaniv, 2019), where a large scale supporting knowledge source is involved and systems must gather information in a sequential manner. Inspired by these works, we leverage properties of language such as compositionality, to help form a powerful query representation that is manageable by RL training.</p>
<p>Question Generation and Information</p>
<p>Limitations and Future Work</p>
<p>In this section, we conclude by discussing limitations of this work and future directions.</p>
<p>Environments: As an initial attempt to study agents that learn to query knowledge sources with language, we settled on oracle-based knowledge sources. This ensures better experimental controllability and reproducibility. However, it can be improved in multiple directions. 3. We plan to include tasks that require non-linear reasoning. This will further decrease agents' incentive to memorize an optimal trajectory, and presumably increase generalizability.</p>
<p>Agent design: For agents, future directions include:</p>
<ol>
<li>
<p>When the state space is large (e.g., in Q-TextWorld), agents sometimes keep on querying different question to exploit the exploration bonus. This demands a better reward assignment strategy, since agents performing in more complex environments may encounter this issue too.</p>
</li>
<li>
<p>It is worth exploring other structured knowledge representations (Ammanabrolu &amp; Hausknecht, 2020) and parametric memories (Weston et al., 2015;Munkhdalai et al., 2019) beyond the notebook we used.</p>
</li>
<li>
<p>Asking questions essentially serves to reduce entropy. One could further use exploration strategies that maximize information gain (Houthooft et al., 2016).</p>
</li>
</ol>
<p>Overall, we are excited by the challenges and opportunities posed by agents that are able to learn to query external knowledge while acting in their environments. We strive to call attention from researchers for the development of agents capable of querying external knowledge sourceswe believe this is a strong and natural skill. We make an initial effort towards this goal, which hopefully can be proven to be valuable and helpful to the community. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation, 1997.</p>
<p>Houthooft, R., Chen, X., Chen, X., Duan, Y., Schulman, J., De Turck, F., and Abbeel, P. Vime: Variational information maximizing exploration. In Proc. NeurIPS, 2016.</p>
<p>Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In arXiv., 2022.</p>
<ol>
<li>
<p>In Sec. B, we provide the model details of our AFK agent.</p>
</li>
<li>
<p>In Sec. C, we provide the implementation and training details for the AFK agent.</p>
</li>
<li>
<p>In Sec. D, we provide additional experimental results on Q-BabyAI.</p>
</li>
<li>
<p>In Sec. E, we provide training curves for all experiments on Q-BabyAI and Q-TextWorld.</p>
</li>
</ol>
<p>The Python code of Q-BabyAI, Q-TextWorld, the AFK agent and all baselines are available at https://ioujenliu. github.io/AFK. The general statistics of Q-BabyAI tasks are summarized in Tab. 7. The statistics for each individual Q-BabyAI task are summarized in Tab. 8, where |Q t | represents the number of 'good queries' an agent should make to solve a task efficiently. 'Early Terminate' indicates that an episode will be terminated if the agent makes a mistake, e.g., stepping on a danger zone or opening the wrong box. In addition, we present the details of the four basic Q-BabyAI tasks in the following.</p>
<p>A. Environment and Task Details</p>
<p>Object in Box ♣: There are two suitcases in the environment. Each suitcase contains one toy. The instruction is find <name>'s toy, where <name> is sampled from a set of names at the start of each episode. However, the agent doesn't know what is the referred toy. Neither does it know the content of each suitcase. The episode terminates when a suitcase is opened by the agent. Therefore, the agent needs to ask multiple question to figure out what the desired toy is and which suitcase to open. If the opened suitcase contains the desired toy, the agent receives a positive reward. Otherwise, it doesn't receive any reward.</p>
<p>Danger ♠: There are different colors of tiles in the environment. One of the colors represents the danger zone. The episode terminates if the agent steps on a danger zone. The instruction is avoid danger zone, and go to the green target square. However, the agent doesn't know what color represents the danger zone. Therefore, to safely reach the target square and receive rewards, the agent must ask the oracle for information on the danger zone. Importantly, the color of the danger zone differs from episode to episode.</p>
<p>Go to Favorite ♦: There are nine rooms in the environment. The instruction is Go to <name>'s favorite toy, where <name> and <name>'s favorite toy are sampled from a set of names and a set of toys at the start of each episode. There are irrelevant objects scattered around the environment. To solve the task efficiently, the agent should issue queries to figure out what and where is the referred toy. Note, if the agent doesn't ask any question, it can still solve the task, but in a much less efficient manner, i.e., by exhaustively searching all rooms for the referred toy. The agent receives positive reward when it goes to the referred toy.</p>
<p>Open Door ♥: There are three keys and one door in the environment. One of the three keys could open the door. The agent needs to find the right key and open the door to complete the task and receive a positive reward. The instruction is Find the key to the door. Note, the agent could still complete the task without asking any question, i.e., by exhaustively trying all keys.</p>
<p>A.2. Q-TextWorld</p>
<p>For all games, the objective is to find cooking ingredients which are randomly hidden throughout the kitchen. Once found, those ingredients may require some processing depending on the task difficulty. Once all required ingredients are in the player's inventory and processed the right way, the game terminates with a reward of 1. We provide statistics of the Q-TextWorld environment in Tab. 9 and a transcript of a game can be seen in Fig. 5.</p>
<p>Take [1/2]: In this task, the player has to find 1 or 2 ingredients mentioned in the instruction. Ingredients are either Tasks |Q t | # of rooms room size Early Terminate
Lv. 1 ♣ 3 1 9×9 True ♠ 1 1 7×7 True ♦ 2 9 5×5 False ♥ 1 2 7×7 False Lv. 2 ♣♠ 5 2 7×7 True ♣♦ 4 9 5×5 True ♣♥ 5 2 7×7 True ♠♦ 3 2 7×7 True ♠♥ 2 2 7×7 True ♦♥ 4 9 5×5 False Lv. 3 ♣♠♦ 5 2 7×7 True ♣♠♥ 6 3 7×7 True ♣♦♥ 5 9 5×5 True ♠♦♥ 4 3 7×7 True
Lv. 4 ♣♠♦♥ 7 9 7×7 True Instruction: You find yourself at friend's house and you are both hungry! Collect some parsley to prepare a delicious meal. Ask Charlie to know where to find each ingredient and for the recipe directions.</p>
<p>Description: You are in a kitchen. A messy kind of place.</p>
<p>You see a closed fridge. You can make out an oven. Look over there! a counter. The counter is wooden. However, the counter, like an empty counter, has nothing on it. Oh! Why couldn't there just be stuff on it? You can see a stove. The stove appears to be empty. You see a griller. Is this what you came to TextWorld for? This... griller?</p>
<p>There is a cookbook on the floor.  visible to the agent right from the start (e.g., on the table), or hidden inside some container that needs to be opened first (e.g., in the fridge). The player can ask the oracle where it can find a particular object (e.g., Ask Charlie 3 where's hot pepper?). In return, the oracle will indicate where the object can be found (e.g., Hot pepper is in the fridge.).</p>
<p>Take [1/2] + Cut: This task extends Take [1/2] as the ingredients also need to be cut in the right way (i.e., chopped, sliced, or diced). Each cutting type is achieved by a different action command (i.e., chop X, slice X, or dice X) while the player is holding a knife in their inventory. The player can also ask the oracle how to process a particular ingredient (e.g., Ask Charlie how to cut the hot pepper?  Table 9. Statistics of the Q-TextWorld environment.</p>
<p>B.1. AFK-Q-BabyAI</p>
<p>Observation Encoder (f obs ): Word Encoder (f note ): Following Chevalier-Boisvert et al.</p>
<p>(2019), we use a gated recurrent unit (GRU) (Chung et al., 2014) to perform word encoding. Specifically, for each
v i ∈ A 0 , we have h i = f gru (v i ) ∈ R |v i |×l , where |v i | is
the number of words in v i and l = 128 is the encoding dimension.</p>
<p>Aggregator (f att ): Following the No Query baseline (Chevalier-Boisvert et al., 2019), the aggregator consists of FiLM (Perez et al., 2018) modules, f FiLM , followed by a long short term memory (LSTM) f LSTM (Hochreiter &amp; Schmidhuber, 1997). That is, f att = f LSTM • f FiLM . Specifically, we stack two FiLM modules. Each FiLM module has 128 filters with size 3 × 3 and the output dimension is 128. The LSTM has 128 units.</p>
<p>Physical</p>
<p>Action and Query Heads (π switch , π phy , π fun , π adj , π noun ): The switch head π switch , physical action head π phy , and function word head π fun are two-layer MLPs with 64 units in each layer. The output dimension of π switch , π phy , π fun are 2, 7, 2. π adj and π noun are single-head pointer networks (Sec. 3.2) with hidden dimension l = 128.</p>
<p>B.2. AFK-Q-TextWorld</p>
<p>Text Encoder (f obs , f note ): Due to the nature of the Q-TextWorld environment, where all inputs are in pure text, we share the two encoders (i.e., f obs and f note ) in our text agent.</p>
<p>We use a transformer-based text encoder, which consists of an embedding layer and a transformer block (Vaswani et al., 2017). Specifically, we tokenize an input sentence (either a text observation or an entry in the notebook) with the spaCy tokenizer. 5 We convert the sequence of tokens into 128-dimensional embeddings, the embedding matrix is initialized randomly.</p>
<p>The transformer block consists of a stack of 4 convolutional layers, a self-attention layer, and a 2-layer MLP with a ReLU non-linear activation function in between. Within the block, each convolutional layer has 128 filters, with a kernel size of 7. The self-attention layers use a block hidden size of 128, with 4 attention heads. Layer normalization (Ba et al., 2016) is applied after each layer inside the block. We merge positional embeddings into each block's input.</p>
<p>Given an input o ∈ R |o| , where |o| denotes the number of tokens in o, the encoder produces a representation h o ∈ R |o|×H , with H = 128 the hidden size.</p>
<p>In practice, we use mini-batches to parallelize the training. Following standard NLP methods, we use special padding tokens when the number of tokens within a batch are different, we use masks to prevent the model from taking the padding tokens into computation. A text input o will be associated with a mask m o ∈ R |o| .</p>
<p>Note for all the three agent variants (i.e., No Query, Query Baseline and AFK), we use the concatenation of [feedback, description, inventory] as the input to f obs . See examples of feedback, description and inventory text in Fig. 5.</p>
<p>Aggregator (f att ): To aggregate two input encodings P ∈ R |P |×H and Q ∈ R |Q|×H , we use the standard multi-head attention mechanism (Vaswani et al., 2017). Specifically, we use P as the query, Q as the key and value. This results in an output P Q ∈ R |P |×H , where at every time step i ∈ [0, |P |), P i Q is the weighted sum of Q, the weight is the attention of P i on Q. We refer readers to Vaswani et al. (2017) for detailed information.</p>
<p>We apply a residual connection on top of the multi-head attention mechanism in order to maintain the original information contained in P . Specifically,
h P Q = Tanh(Linear([P Q ; P ])),(4)
where h P Q ∈ R |P |×H , brackets [⋅; ⋅] denote vector concatenation.</p>
<p>We denote the above attention layer as h P Q = Attention(P, Q).</p>
<p>Using two of such layers (without sharing parameters), we aggregate three inputs: h obs ∈ R |obs|×H , h task ∈ R |task|×H and h s ∈ R |note|×H , where |obs|, |task| and |note| denote the number of tokens in a text observation, the number of tokens in the instruction, and the number of nodes in the notebook:</p>
<p>h obs,task = Attention(h obs , h task ),
h x = Attention(h obs,task , h s ).(6)
Here, h obs,task ∈ R |obs|×H , h x ∈ R |obs|×H .</p>
<p>Action Generator (π func , π adj , π noun ): In Q-TextWorld, all actions follow the same format of <func, adj, noun>. Therefore, the query action space A q and the physical action space A phy are shared (i.e., the vocabularies are shared). We use a three-head module to generate three vectors. Their sizes correspond to the function word, adjective, and noun vocabularies. The generated vectors are used to compute an overall Q-value.</p>
<p>Taking the aggregated representation h x ∈ R |obs|×H as input, we first compute its masked average, using the mask of the text observation. This results in h s ∈ R H .</p>
<p>Specifically, the action generator consists of four multi-layer perceptrons (MLPs):</p>
<p>h shared = ReLU(Linear shared (h s )),
Q func = Linear func (h shared ), Q adj = Linear adj (h shared ), Q noun = Linear noun (h shared ).(7)
Here, Q func ∈ R |func| , Q adj ∈ R |adj| , Q noun ∈ R |noun| . |func|, |adj|, and |noun| denote the vocabulary size of function words, adjectives, and nouns.</p>
<p>In order to alleviate the difficulties caused by a large action space, similar to the pointer mechanism in the Q-BabyAI agent, we apply masks over vocabularies when sampling actions. In the masks, only tokens appearing in the current notebook are labeled as 1, i.e., the text agent only performs physical interaction with objects noted in its notebook. It also only asks questions about objects it has heard of.</p>
<p>Finally, we compute the Q-value of an action <u, v, w>:
Q <u,v,w> = (Q u + Q v + Q w )/3,(8)
where u, v and w are tokens in the function word, adjective, and noun vocabulary.</p>
<p>C. Implementation Details</p>
<p>In this section, we provide implementation and training details of our agents. In Appendix C.1, we provide implementation details for our agent used for the Q-BabyAI environments. In Appendix C.2, we provide implementation details for our agent used for the Q-TextWorld environments.</p>
<p>C.1. AFK-Q-BabyAI</p>
<p>We closely follow the training procedure of the publicly available code of the  </p>
<p>C.2. AFK-Q-TextWorld</p>
<p>We adopt the training procedure from the official code base released by TextWorld authors (Côté et al., 2018). Our text agent is trained with Deep Q-Learning (Mnih et al., 2013). We use a prioritized replay buffer with memory size of 500, 000, and a priority fraction of 0.5. During model update, we use a replay batch size of 64. We use a discount factor γ = 0.9. We use noisy nets, with a σ 0 of 0.5. We update the target network after every 1000 episodes. We sample the multi-step return n ∼ Uniform[1, 3]. We refer readers to Hessel et al. (2018) for more information about different components of DQN training.</p>
<p>For all experiments, we use Adam (Kingma &amp; Ba, 2015) as the optimizer. The learning rate is set to 0.00025 with a clip gradient norm of 5. We train all agents with 5 different random seeds: [24,42,123,321,3407]. For replay buffer data collection, we use a batch size of 20. We train our agents with 500K episodes, each episode has a maximum number of steps 20. After every 2 data collection steps, we randomly sample a batch from the replay buffer, and perform a network update.</p>
<p>Resources:</p>
<p>We use a mixture of Nvidia V100, P100 and P40 GPUs to conduct all the experiments. On average, experiments on Q-BabyAI take 1 day, experiments on Q-TextWorld take 2 days.</p>
<p>D. Additional Results</p>
<p>Success rate and episode length: In Tab. 10 we report success rate and episode length of No Query, Query Baseline, and AFK on all levels of Q-BabyAI tasks. Note, due to the early termination mechanism, the comparison of episode length is only meaningful when the agent is able to solve the task. For instance, in Object in Box (♣) and Danger (♠), No Query and Query Baseline have shorter episode length than AFK because they either step on the danger tile or open the wrong box, resulting in the termination of an episode. Number of queries made by an AFK agent in seen and unseen tasks: To better understand the agent's behavior in seen and unseen tasks, we report the number of queries an agent made across 500 evaluation episodes. As shown in Fig. 6 (left), when an agent is trained and evaluated on the same tasks (♣♠), in most episodes, the agent makes four queries, which is the optimal number of queries of the task. In contrast, when the agent is trained and evaluate on different tasks, Fig. 6 (right), it made more queries.</p>
<p>E. Training Curves</p>
<p>The training curves of all Q-BabyAI and Q-TextWorld experiments in terms of success rate and episode length are shown in Fig. 7, Fig. 8, Fig. 9, Fig. 10, and Fig. 11. </p>
<p>Figure 1 .
1Proposed Q-BabyAI. Here the agent has to query the knowledge source to succeed.</p>
<p>Figure 2 .
2Querying interactions in Q-BabyAI and Q-TextWorld. We illustrate standard physical interactions as gray colored stage directions and highlight the questions (green) and oracle replies (blue) upon receiving the instruction (red).</p>
<p>Retrieval: Question generation is a thriving direction at the intersection of multiple areas like natural language processing and information retrieval. In the machine reading comprehension literature, Du et al. (2017); Yuan et al. (2017); Jain et al.</p>
<p>Cohn, D., Atlas, L., and Ladner, R. Improving generalization with active learning. Machine learning, 1994. Colas, C., Sigaud, O., and Oudeyer, P.-Y. GEP-PG: Decoupling exploration and exploitation in deep reinforcement learning algorithms. In Proc. ICML, 2018. Côté, M.-A.,Ákos Kádár, Yuan, X. E., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., Asri, L. E., Adada, M., Tay, W., and Trischler, A. Textworld: A learning environment for text-based games. In Computer Games Workshop at ICML/IJCAI, 2018. Das, R., Dhuliawala, S., Zaheer, M., Vilnis, L., Durugkar, I., Krishnamurthy, A., Smola, A., and McCallum, A. Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning. In Proc. ICLR, 2018. Du, X., Shao, J., and Cardie, C. Learning to ask: Neural question generation for reading comprehension. In Proc. ACL, 2017. Dulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag, P., Lillicrap, T., Hunt, J., Mann, T., Weber, T., Degris, T., and Coppin, B. Deep reinforcement learning in large discrete action spaces. In Proc. ICML, 2016. Engel, S. Children's need to know: Curiosity in schools. Harvard educational review, 2011. Fang, M., Li, Y., and Cohn, T. Learning how to active learn: A deep reinforcement learning approach. In Proc. EMNLP, 2017. Feldman, Y. and El-Yaniv, R. Multi-hop paragraph retrieval for open-domain question answering. In Proc. ACL, 2019. Gottlieb, J., Oudeyer, P.-Y., Lopes, M., and Baranes, A. Information-seeking, curiosity, and attention: computational and neural mechanisms. Trends in cognitive sciences, 2013. He, J., Ostendorf, M., and He, X. Reinforcement learning with external knowledge and two-stage q-functions for predicting popular reddit threads. In arXiv., 2017. Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D. Deep reinforcement learning that matters. In Proc. AAAI, 2017. Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D. Rainbow: Combining improvements in deep reinforcement learning. In Proc. AAAI, 2018.</p>
<p>Inventory:
You are carrying: a knife. Notebook: {"You find yourself . . . for the recipe directions."} Action 1: ask Charlie where's the parsley Feedback: The parsley is in the oven. Notebook: {"You find yourself . . . for the recipe directions.", "The parsley is in the oven."} Action 2: open oven Feedback: Opened. Action 3: take parsley Feedback: Taken. Action 4: ask Charlie how to cut the parsley Feedback: The parsley needs to be sliced. Use a knife to cut it. Notebook: {"You find yourself . . . for the recipe directions.", "The parsley is in the oven.", "The parsley needs to be sliced. Use a knife to cut it."} Action 5: slice parsley Feedback: Sliced.Done after 5 steps. Score 1/1.</p>
<p>Figure 5 .
5An excerpt from a Q-TextWorld game.</p>
<p>Following BabyAI (Chevalier-Boisvert et al., 2019), the environment observation o env of Q-BabyAI is a 7 × 7 × 4 symbolic observation that contains a partial and local egocentric view of the environment and the direction of the agent. To encode o env , we use a convolutional neural network (CNN). Following Chevalier-Boisvert et al. (2019), the observation encoder consists of three convolutional layers. The first convolutional layer has 128 filters of size 8 × 8 and stride 8. The second and third convolutional layers have 128 filters of size 3 × 3 and stride 1. Batch normalization and ReLU unit are applied to the output of each layer. At the end, a 2D pooling layer with filter size 2 × 2 is applied to obtain the representation h o of 256 dimensions.</p>
<p>Figure 6 .
6Evaluation episode count over number of queries. Left: train on ♣♠, evaluate on ♣♠. Right: train on ♠♦+♦♣, evaluate on ♣♠. ♣: Obj. in Box, ♠: Danger, ♦: Go to Favorite.</p>
<p>Figure 7 .Figure 8 .Figure 9 .Figure 10 .Figure 11 .
7891011Success rate of AFK and baselines on Q-BabyAI. Episode length of AFK and baselines on Q-BabyAI. Success rate of AFK and baselines on Q-TextWorld. Training reward received by AFK and baselines on Q-TextWorld. Note, training reward is the sum of 1) reward given by the environment for solving the task; and 2) the episodic exploration bonus. Steps used by AFK and baselines during training on Q-TextWorld.</p>
<p>Iou-Jen Liu <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#105;&#108;&#105;&#117;&#51;&#64;&#105;&#108;&#108;&#105;&#110;&#111;&#105;&#115;&#46;&#101;&#100;&#117;">&#105;&#108;&#105;&#117;&#51;&#64;&#105;&#108;&#108;&#105;&#110;&#111;&#105;&#115;&#46;&#101;&#100;&#117;</a>, Xingdi Yuan <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#114;&#105;&#99;&#46;&#121;&#117;&#97;&#110;&#64;&#109;&#105;&#99;&#114;&#111;&#115;&#111;&#102;&#116;&#46;&#99;&#111;&#109;">&#101;&#114;&#105;&#99;&#46;&#121;&#117;&#97;&#110;&#64;&#109;&#105;&#99;&#114;&#111;&#115;&#111;&#102;&#116;&#46;&#99;&#111;&#109;</a>.Task: Find the key to the door, and find Mary's toy.1 University of Illinois at Urbana-Champaign, IL, U.S.A. 
2 Microsoft Research, Montréal, Canada 
3 Inria, France. Corre-
spondence to: Proceedings of the 39 
th International Conference on Machine 
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s). </p>
<ol>
<li>What's 
Mary's toy? </li>
<li>
<p>Where's 
green ball? </p>
</li>
<li>
<p>Where's 
yellow star? </p>
</li>
<li>
<p>What's 
the key? </p>
</li>
<li>
<p>Mary's toy is 
a green ball. </p>
</li>
<li>
<p>The green ball is 
in the blue chest. </p>
</li>
<li>
<p>I don't know. </p>
</li>
<li>
<p>The green key. </p>
</li>
</ol>
<p>Asking for Knowledge (AFK)Tasks 
No Query 
Query 
Baseline 
AFK (Ours) </p>
<p>Lv. 1 </p>
<p>♣ 
50.5±2.0 
49.8±1.2 
100.0±0.0 
♠ 
68.3±2.4 
73.8±1.2 
100.0±0.0 
♦ 
98.9±0.8 
99.3±0.3 
100.0±0.0 
♥ 
99.7±0.3 85.3±22.3 
100.0±0.0 </p>
<p>Lv. 2 </p>
<p>Table 1 .
1Success rate (%) on Q-BabyAI. ♣: Object in Box, ♠: Danger, ♦: Go to Favorite, ♥: Open Door.Table 2. Number of steps required to solve a task. ♦: Go to Favorite, ♥: Open Door.Tasks No Query Query Baseline AFK (Ours) </p>
<p>♦ 
30.2±1.5 
26.7±1.5 
16.8±6.7 
♥ 
26.2±0.9 
36.8±1.0 
20.6±0.2 </p>
<p>Experimental Setup: We adopt the publicly available BabyAI and TextWorld code released by the authors 1,2 as our non-query baseline system, denoted as No Query. We consider a vanilla query agent (Kovac et al., 2021) (Query Baseline), in which query heads are added to the baseline agent to generate language queries. We refer to the pro-Table 3. Success rate (%) on Q-TextWorld.Table 4. Ablation Study. Success rate (%) on Q-BabyAI. ♣: Object in Box, ♠: Danger, ♦: Go to Favorite, ♥: Open Door.1 BabyAI: github:mila-iqia/babyai 
2 TextWorld: github:xingdi-eric-yuan/qait public </p>
<p>Task 
No Query Query Baseline AFK (Ours) </p>
<p>Take 1 
75.1±4.1 
73.5±5.8 
85.1±2.9 
Take 2 
24.0±6.6 
13.7±8.5 
61.9±6.5 
Take 1 Cut 
24.6±1.0 
22.9±3.6 
43.5±15.9 
Take 2 Cut 
0.0±0.0 
0.0±0.0 
0.0±0.0 </p>
<p>Task 
AFK w/o 
Notebook </p>
<p>AFK w/o 
Pointer 
Mechanism </p>
<p>AFK w/o 
Episodic 
Exploration </p>
<p>AFK 
(Ours) </p>
<p>♣ 
50.0±0.8 
49.4±0.7 
49.8±0.7 
100.0±0.0 
♠ 
99.1±0.2 
100.0±0.0 
93.8±0.7 
100.0±0.0 
♦ 
99.2±0.4 
99.7±0.2 
99.3±0.2 
100.0±0.0 
♥ 
85.1±1.0 
100.0±0.0 
77.8±0.7 
100.0±0.0 
♣♥ 
48.5±1.9 
90.5±1.4 
50.0±1.8 
99.0±0.4 
Mean 
76.4 
87.9 
74.1 
99.8 </p>
<p>Results: We first compare our AFK agent with baselines on all level 1 and level 2 tasks of Q-BabyAI. The final metrics and standard deviation of average evaluation success rate are reported in Tab. 1. As shown in Tab. 1, for level 1 and level 2 tasks, the AFK agent achieves significantly higher success rates than the baselines, particularly in Object in Box (♣) and Danger (♠) where informationFigure 4. Training curves of AFK, non-query baseline, query baseline on Q-BabyAI (left) and Q-TextWorld (right).Table 5. Zero-shot generalization study of AFK.Table 6. Query quality of AFK. ♣: Object in Box, ♠: Danger, ♦: Go to Favorite, ♥: Open Door.0.0 
0.5 
1.0 
1.5 
2.0 </p>
<p>Env. Steps </p>
<p>1e7 </p>
<p>0.0 </p>
<p>0.2 </p>
<p>0.4 </p>
<p>0.6 </p>
<p>0.8 </p>
<p>1.0 </p>
<p>Avg. Succ. Rate </p>
<p>Danger </p>
<p>0.0 
0.5 
1.0 
1.5 
2.0 </p>
<p>Env. Steps </p>
<p>1e7 </p>
<p>Obj in Box + Open Door </p>
<p>0 
1 
2 
3 
4 
5 </p>
<p>Episodes </p>
<p>1e5 </p>
<p>Take 2 </p>
<p>0 
1 
2 
3 
4 
5 </p>
<p>Episodes </p>
<p>1e5 </p>
<p>Take 1 Cut </p>
<p>AFK 
Query Base 
No Query </p>
<p>Target Task Source Tasks Succ. (%) Eps. Len. 
♣♠ 
♠♦+♦♣ 
22.1±0.7 
32.0±0.3 </p>
<p>♠♦ 
♠♣+♦♣ 
35.6±2.1 
71.6±1.1 </p>
<p>Task |Q t | Precision Recall 
F1 
♣♥ 
5 
0.804 
0.823 0.812 
♦♥ 
4 
0.771 
0.560 0.601 
♠♦ 
3 
0.989 
0.989 0.989 </p>
<p>Table 7. Statistics of the Q-BabyAI environment.A.1. Q-BabyAI </p>
<p>General information </p>
<p>Word vocabulary size 
62 
Function word vocabulary size (|V func |) 2 
Adjective vocabulary size (|V adj |) 
22 
Noun vocabulary size (|V noun |) 
24 </p>
<h1>of Physical action</h1>
<p>7 
Visual range 
7 × 7 </p>
<h1>of object colors</h1>
<p>6 </p>
<h1>of actionable object types</h1>
<p>4 </p>
<h1>of names</h1>
<p>2 </p>
<h1>of danger zone colors</h1>
<p>2 </p>
<p>Table 8 .
8Statistics of each task in Q-BabyAI. ♣: Object in Box, ♠: Danger, ♦: Go to Favorite, ♥: Open Door.</p>
<p>BabyAI No Query agent(Chevalier- Boisvert et al., 2019). We train our AFK and all baselines with PPO(Schulman et al., 2017). Specifically, we use the Adam (Kingma &amp; Ba, 2015) optimizer with learning rate 0.0001. We update the model every 2560 environment steps. The batch size is 1280. The PPO epoch is 4 and the discount factor is 0.99. We use 64 parallel processes for collecting data from the environment. The scaling factor β of the episodic exploration bonus is set to 0.1 for all experiments. For all experiments, we study uni-gram and bi-gram similarity models and report the better results. We tuned the episodicexploration scaling factor β ∈ {0.001, 0.01, 0.1, 0.5}, hidden size of the pointer network l ∈ {32, 64, 128, 256}, learning rate ∈ {10 −5 , 10 −4 , 10 −3 }, and similarity function ∈ {uni-gram, bi-gram}. We train all agents with 5 different random seeds:[24, 42, 123, 321, 3407].
). In return, the oracle will indicate which type of cutting is needed (e.g., Hot pepper needs to be sliced, use a knife to cut it). Note, in the reported games, the player always start with a kitchen knife in their inventory.B. Modeling DetailsIn this section, we provide detailed information regarding our agents. In Appendix B.1, we describe our agent used for the Q-BabyAI environments. In Appendix B.2, we describe our agent used for the Q-TextWorld environments.3 In Q-TextWorld, the oracle is named Charlie. 4 Nonessential words can be omitted, e.g., Ask Charlie how hot pepper?
https://spacy.io/
AcknowledgementThis work is supported in part by Microsoft Research, the National Science Foundation under Grants No. 1718221,  2008387, 2045586, 2106825, MRI #1725729, NIFA award  2020-67021-32799, and AWS Research Awards.
Learning dynamic belief graphs to generalize on text-based games. A Adhikari, X Yuan, M.-A Côté, M Zelinka, M.-A Rondeau, R Laroche, P Poupart, J Tang, A Trischler, Hamilton , W , Proc. NeurIPS. NeurIPSAdhikari, A., Yuan, X., Côté, M.-A., Zelinka, M., Rondeau, M.-A., Laroche, R., Poupart, P., Tang, J., Trischler, A., and Hamilton, W. Learning dynamic belief graphs to generalize on text-based games. In Proc. NeurIPS, 2020.</p>
<p>Graph constrained reinforcement learning for natural language action spaces. P Ammanabrolu, M Hausknecht, ICLR. Ammanabrolu, P. and Hausknecht, M. Graph constrained reinforcement learning for natural language action spaces. In ICLR, 2020.</p>
<p>Tutor4rl: Guiding reinforcement learning with external knowledge. M F Argerich, J Furst, B Cheng, arXiv.Argerich, M. F., Furst, J., and Cheng, B. Tutor4rl: Guid- ing reinforcement learning with external knowledge. In arXiv., 2020.</p>
<p>Layer normalization. L J Ba, J R Kiros, G E Hinton, arXiv.Ba, L. J., Kiros, J. R., and Hinton, G. E. Layer normalization. In arXiv., 2016.</p>
<p>Learning algorithms for active learning. P Bachman, A Sordoni, A Trischler, Proc. ICML. ICMLBachman, P., Sordoni, A., and Trischler, A. Learning algo- rithms for active learning. In Proc. ICML, 2017.</p>
<p>Unifying Count-Based Exploration and Intrinsic Motivation. M Bellemare, S Srinivasan, G Ostrovski, T Schaul, D Saxton, R Munos, Proc. NeurIPS. NeurIPSBellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. Unifying Count-Based Ex- ploration and Intrinsic Motivation. In Proc. NeurIPS, 2016.</p>
<p>Improving language models by retrieving from trillions of tokens. S Borgeaud, A Mensch, J Hoffmann, T Cai, E Rutherford, K Millican, G V D Driessche, J.-B Lespiau, B Damoc, A Clark, In arXiv.Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Ruther- ford, E., Millican, K., Driessche, G. v. d., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by retrieving from trillions of tokens. In arXiv., 2021.</p>
<p>Deep reinforcement learning boosted by external knowledge. N Bougie, R Ichise, Bougie, N. and Ichise, R. Deep reinforcement learning boosted by external knowledge. In arXiv., 2017.</p>
<p>Ask the right questions: Active question reformulation with reinforcement learning. C Buck, J Bulian, M Ciaramita, W Gajewski, A Gesmundo, N Houlsby, Wang , W , ICLR. Buck, C., Bulian, J., Ciaramita, M., Gajewski, W., Ges- mundo, A., Houlsby, N., and Wang, W. Ask the right questions: Active question reformulation with reinforce- ment learning. In ICLR, 2018.</p>
<p>Babyai: A platform to study the sample efficiency of grounded language learning. M Chevalier-Boisvert, D Bahdanau, S Lahlou, L Willems, C Saharia, T H Nguyen, Y Bengio, ICLR. Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y. Babyai: A platform to study the sample efficiency of grounded language learning. In ICLR, 2019.</p>
<p>Learning phrase representations using RNN encoder-decoder for statistical machine translation. K Cho, B Van Merriënboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, Proc. EMNLP. EMNLPCho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proc. EMNLP, 2014.</p>
<p>Empirical evaluation of gated recurrent neural networks on sequence modeling. J Chung, C Gulcehre, K Cho, Y Bengio, arXiv.Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. In arXiv., 2014.</p>
<p>Browser-assisted question-answering with human feedback. arXiv.Browser-assisted question-answering with human feed- back. In arXiv., 2021.</p>
<p>Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. K Nguyen, H Daume, EMNLP. Nguyen, K. and Daume, H. Help, anna! visual naviga- tion with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. In EMNLP, 2019.</p>
<p>Learning When and What to Ask: a Hierarchical Reinforcement Learning Framework. K Nguyen, Y Bisk, Iii , H D , Proc. ICML. ICMLNguyen, K., Bisk, Y., and III, H. D. Learning When and What to Ask: a Hierarchical Reinforcement Learning Framework. In Proc. ICML, 2022.</p>
<p>Task-oriented query reformulation with reinforcement learning. R Nogueira, K Cho, Proc. EMNLP. EMNLPNogueira, R. and Cho, K. Task-oriented query reformulation with reinforcement learning. In Proc. EMNLP, 2017.</p>
<p>Count-based exploration with neural density models. G Ostrovski, M G Bellemare, A Van Den Oord, R Munos, Proc. ICML. ICMLOstrovski, G., Bellemare, M. G., van den Oord, A., and Munos, R. Count-based exploration with neural density models. In Proc. ICML, 2017.</p>
<p>What is intrinsic motivation? a typology of computational approaches. P.-Y Oudeyer, F Kaplan, Frontiers in Neurorobotics. Oudeyer, P.-Y. and Kaplan, F. What is intrinsic motivation? a typology of computational approaches. Frontiers in Neurorobotics, 2007.</p>
<p>Intrinsic motivation systems for autonomous mental development. P.-Y Oudeyer, F Kaplan, V V Hafner, IEEE transactions on evolutionary computation. Oudeyer, P.-Y., Kaplan, F., and Hafner, V. V. Intrinsic motivation systems for autonomous mental development. IEEE transactions on evolutionary computation, 2007.</p>
<p>Film: Visual reasoning with a general conditioning layer. E Perez, F Strub, H De Vries, V Dumoulin, A Courville, AAAI. Perez, E., Strub, F., de Vries, H., Dumoulin, V., and Courville, A. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018.</p>
<p>Playing log (n)-questions over sentences. P Potash, K Suleman, arXiv:1908.04660arXiv preprintPotash, P. and Suleman, K. Playing log (n)-questions over sentences. arXiv preprint arXiv:1908.04660, 2019.</p>
<p>Question-asking in childhood: A review of the literature and a framework for understanding its development. S Ronfard, I M Zambrana, T K Hermansen, D Kelemen, Developmental Review. 49Ronfard, S., Zambrana, I. M., Hermansen, T. K., and Kele- men, D. Question-asking in childhood: A review of the literature and a framework for understanding its develop- ment. Developmental Review, 49:101-120, 2018.</p>
<p>The truth of the f-measure. Y Sasaki, arXiv.Sasaki, Y. The truth of the f-measure. In arXiv., 2007.</p>
<p>Episodic curiosity through reachability. N Savinov, A Raichuk, D Vincent, R Marinier, M Pollefeys, T Lillicrap, S Gelly, Proc. ICLR. ICLRSavinov, N., Raichuk, A., Vincent, D., Marinier, R., Polle- feys, M., Lillicrap, T., and Gelly, S. Episodic curiosity through reachability. In Proc. ICLR, 2019.</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. In arxiv, 2017.</p>
<p>Ask to learn: A study on curiosity-driven question generation. T Scialom, J Staiano, Proc. COLING. COLINGScialom, T. and Staiano, J. Ask to learn: A study on curiosity-driven question generation. In Proc. COLING, 2020.</p>
<p>Get to the point: Summarization with pointer-generator networks. A See, P J Liu, C D Manning, ACL. See, A., Liu, P. J., and Manning, C. D. Get to the point: Summarization with pointer-generator networks. In ACL, 2017.</p>
<p>Reinforcement Learning: An Introduction. R S Sutton, A G Barto, The MIT PressSutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. The MIT Press, 2018.</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Proc. NeurIPS. NeurIPSVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Proc. NeurIPS, 2017.</p>
<p>Memory networks. J Weston, S Chopra, A Bordes, Proc. ICLR. ICLRWeston, J., , Chopra, S., , and Bordes, A. Memory networks. In Proc. ICLR, 2015.</p>
<p>Answering complex open-domain questions with multihop dense retrieval. W Xiong, X Li, S Iyer, J Du, P Lewis, W Y Wang, Y Mehdad, S Yih, S Riedel, D Kiela, B Oguz, Proc. ICLR. ICLRXiong, W., Li, X., Iyer, S., Du, J., Lewis, P., Wang, W. Y., Mehdad, Y., Yih, S., Riedel, S., Kiela, D., and Oguz, B. Answering complex open-domain questions with multi- hop dense retrieval. In Proc. ICLR, 2021.</p>
<p>Interactive machine comprehension with dynamic knowledge graphs. X Yuan, EMNLP. 2021Yuan, X. Interactive machine comprehension with dynamic knowledge graphs. In EMNLP, 2021.</p>
<p>Machine comprehension by text-to-text neural question generation. X Yuan, T Wang, C Gulcehre, A Sordoni, P Bachman, S Subramanian, S Zhang, A Trischler, In arXivYuan, X., Wang, T., Gulcehre, C., Sordoni, A., Bachman, P., Subramanian, S., Zhang, S., and Trischler, A. Machine comprehension by text-to-text neural question generation. In arXiv., 2017.</p>
<p>Deep sets. M Zaheer, S Kottur, S Ravanbakhsh, B Poczos, R R Salakhutdinov, A J Smola, Proc. NeurIPS. NeurIPSZaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. Deep sets. In Proc. NeurIPS, 2017.</p>
<p>Generating structured queries from natural language using reinforcement learning. V Zhong, C Xiong, R Socher, In arXivZhong, V., Xiong, C., and Socher, R. Seq2sql: Generating structured queries from natural language using reinforce- ment learning. In arXiv., 2017.</p>
<p>Generalising to novel environment dynamics via reading. V Zhong, T Rocktäschel, E Grefenstette, Rtfm, ICLR. Zhong, V., Rocktäschel, T., and Grefenstette, E. Rtfm: Generalising to novel environment dynamics via reading. In ICLR, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7456 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7456</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7456</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-277621144</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.04365v5.pdf" target="_blank">AutoPDL: Automatic Prompt Optimization for LLM Agents</a></p>
                <p><strong>Paper Abstract:</strong> The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.21\pm15.46$ percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks.</p>
                <p><strong>Cost:</strong> 0.028</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7456.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7456.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granite3.1_FEVER_ReWOO_5shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granite 3.1 8B on FEVER with ReWOO (5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of an 8B Granite model on FEVER showing that switching from zero-shot to a 5-shot ReWOO prompt (agentic, reasoning-without-observations trajectories) improves accuracy modestly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Granite 3.1 8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Granite family generalist instruction model evaluated for natural-language tasks; used as an inference-service model in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary fact verification: decide whether a claim is true or false given supporting evidence (Wikipedia-derived).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Agentic trajectory (ReWOO) few-shot prompt (5 demonstrations) vs zero-shot natural language question</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>ReWOO pattern: few-shot trajectories (5) containing reasoning thoughts and action sequences (no reactive tool feedback); compared against zero-shot baseline; greedy decoding; tool: Search available for agentic patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>76.4% (optimized, ±3.3pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>72.9% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+3.5% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Greedy decoding; successive-halving AutoPDL optimization; validation/test splits; tools enabled for agentic patterns (Search); candidates sampled with num_demonstrations ∈ {0,3,5}.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±3.3 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7456.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granite13B_FEVER_ReWOO_3shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granite 13B Instruct V2 on FEVER with ReWOO (3-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large Granite instruction model on FEVER shows dramatic improvement when using a 3-shot ReWOO few-shot prompt versus a weak zero-shot baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Granite 13B Instruct V2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generalist instruction-tuned Granite model intended for assistant-style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary fact verification (true/false) using Wikipedia evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>ReWOO prompt (3 few-shot trajectories) vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>ReWOO (3-shot) with few-shot trajectories containing reasoning steps but no reactive observations; compares to zero-shot; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>74.0% (optimized, ±1.4pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>6.5% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+67.5% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Successive-halving search over patterns and demonstrations; greedy decoding; tool: Search available for FEVER agentic prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±1.4 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7456.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granite20B_FEVER_CoT_3shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granite 20B Code on FEVER with CoT (3-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Granite code-focused 20B model on FEVER improves substantially by using 3-shot chain-of-thought few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Granite 20B Code</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Granite family model specialized for code tasks but evaluated as a generalist on FEVER.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>20B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary fact verification (true/false) using Wikipedia evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Chain-of-thought (CoT) few-shot prompt (3 demonstrations) vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>CoT with 3 few-shot examples that include explicit reasoning chains; compared to zero-shot baseline; greedy decoding; Search tool available for agentic patterns though CoT here uses reasoning steps in demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>63.1% (optimized, ±1.6pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>39.7% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+23.4% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Successive-halving optimization; demonstrations sampled from train bank; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±1.6 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7456.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granite34B_FEVER_CoT_3shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granite 34B Code on FEVER with CoT (3-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 34B Granite code model improves FEVER accuracy modestly by using 3-shot chain-of-thought prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Granite 34B Code</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large Granite model trained with code capabilities; evaluated on natural language fact verification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary fact verification using Wikipedia evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>CoT few-shot (3 demonstrations) vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>3-shot chain-of-thought demonstrations that include reasoning steps; greedy decoding; comparisons against zero-shot baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>62.6% (optimized, ±3.8pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>56.4% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+6.2% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Successive-halving search; demonstration sampling; tools available though CoT demonstrations include evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±3.8 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7456.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3.1_FEVER_CoT_3shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.1 8B on FEVER with CoT (3-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA 3.1 (8B) benefits from a 3-shot chain-of-thought prompt on FEVER relative to zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3.1 8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA-family instruction model tuned for general language tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>True/false fact verification using evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>CoT few-shot (3 demonstrations) vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>3-shot chain-of-thought demonstrations including reasoning steps; compared to zero-shot baseline; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>77.5% (optimized, ±0.8pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>68.5% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+9.0% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Successive-halving optimization; tool access for agentic patterns; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.8 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7456.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3.2_FEVER_ReWOO_5shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.2 3B on FEVER with ReWOO (5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller LLaMA 3.2 (3B) shows large gain on FEVER when switched from zero-shot to 5-shot ReWOO prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3.2 3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>3B-parameter LLaMA-family instruction model used to study scale effects.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary fact verification (true/false).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>ReWOO few-shot (5 trajectories) vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>ReWOO pattern with 5 few-shot trajectories (reasoning without observations); compared to zero-shot; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>66.3% (optimized, ±0.9pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>38.0% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+28.3% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Successive-halving search; demonstration sampling from train bank; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.9 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7456.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3.3_FEVER_ReWOO_3shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.3 70B on FEVER with ReWOO (3-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large LLaMA 70B model benefits substantially from a 3-shot ReWOO prompt on FEVER versus zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3.3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large-capacity LLaMA instruction model (70B) evaluated across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary fact verification with Wikipedia evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>ReWOO few-shot (3 demonstrations) vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>3-shot ReWOO few-shot trajectories; greedy decoding; compared to zero-shot baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>78.1% (optimized, ±0.6pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>67.6% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+10.5% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Successive-halving AutoPDL optimization; demonstration sampling; tool access available.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.6 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7456.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granite3.1_GSM8K_ZeroShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granite 3.1 8B on GSM8K (zero-shot remained best)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Granite 3.1 8B showed no improvement from prompt optimization on GSM8K; zero-shot baseline remained optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Granite 3.1 8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>8B Granite instruction model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems requiring numeric reasoning and arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural language question (no demonstrations) vs few-shot CoT/agentic variants</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Search space included {0,3,5} demonstrations, CoT, ReAct, ReWOO patterns; optimization found zero-shot to be best for this model on GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>74.2% (optimized; zero-shot baseline retained, ±0.6pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>74.2% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.0% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Greedy decoding; tool: Calc available for agentic patterns; successive-halving optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.6 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7456.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granite13B_GSM8K_CoT_3shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granite 13B Instruct V2 on GSM8K with CoT (3-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Granite 13B improved on GSM8K by using 3-shot chain-of-thought few-shot prompts compared to zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Granite 13B Instruct V2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B Granite instruction-tuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems; exact numeric answers required.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Chain-of-thought few-shot (3 demonstrations) vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>3-shot CoT demonstrations include step-by-step reasoning chains; compared to zero-shot; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>30.9% (optimized, ±1.0pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>23.0% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+7.9% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Calc tool provided for agentic patterns; successive-halving; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±1.0 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7456.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granite20B_GSM8K_ZeroShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granite 20B Code on GSM8K (zero-shot best)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Granite 20B Code showed no improvement from prompt optimization on GSM8K; zero-shot baseline remained best.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Granite 20B Code</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Code-specialized Granite 20B model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>20B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math problems with exact numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural language vs few-shot CoT/agentic patterns</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Optimization considered 0/3/5 demonstrations and agentic patterns; zero-shot remained best for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>68.7% (optimized; zero-shot baseline retained, ±0.1pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>68.7% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.0% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Greedy decoding; Calc tool available; successive-halving.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.1 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7456.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granite34B_GSM8K_ZeroShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granite 34B Code on GSM8K (zero-shot best)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Granite 34B Code saw no improvement from prompt optimization on GSM8K; zero-shot baseline retained.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Granite 34B Code</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large code-capable Granite model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems requiring exact numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural language question vs few-shot/agentic variants</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Search included few-shot counts {0,3,5} and patterns {Zero-Shot, CoT, ReWOO, ReAct}; zero-shot was optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>72.1% (optimized; zero-shot baseline retained, ±0.1pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>72.1% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.0% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Greedy decoding; Calc tool available; successive-halving.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.1 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7456.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3.1_GSM8K_CoT_5shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.1 8B on GSM8K with CoT (5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA 3.1 (8B) substantially improves on GSM8K when using 5-shot chain-of-thought prompts versus zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3.1 8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA instruction model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math word problems; numeric exact-match answers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Chain-of-thought few-shot (5 demonstrations) vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>5-shot CoT demonstrations including full reasoning chains; compared to zero-shot; greedy decoding; Calc tool used for agentic trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>85.3% (optimized, ±0.6pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>78.4% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+6.9% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Successive-halving AutoPDL; greedy decoding; demonstrations sampled from train bank.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.6 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7456.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3.2_GSM8K_CoT_3shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.2 3B on GSM8K with CoT (3-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA 3.2 (3B) improves modestly on GSM8K using 3-shot chain-of-thought prompts compared to zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3.2 3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>3B-parameter LLaMA instruction model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math problems requiring exact numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>CoT few-shot (3 demonstrations) vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>3-shot CoT examples with explicit step-by-step reasoning; greedy decoding; comparisons versus zero-shot baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>75.3% (optimized, ±0.4pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>71.8% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+3.5% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Successive-halving; Calc tool accessible for agentic patterns; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.4 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7456.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3.3_GSM8K_CoT_3shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.3 70B on GSM8K with CoT (3-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large LLaMA 70B dramatically improves on GSM8K when using 3-shot chain-of-thought prompting vs zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3.3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-capacity LLaMA instruction model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math word problems; expect exact numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>CoT few-shot (3 demonstrations) vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>3-shot CoT demonstrations containing chain-of-thought reasoning; greedy decoding; compared to zero-shot baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>95.4% (optimized, ±0.2pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>85.5% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+9.9% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Successive-halving search; Calc tool available; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.2 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e7456.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granite3.1_MBPP_ZeroShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granite 3.1 8B on MBPP+ (zero-shot best)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Granite 3.1 (8B) shows no benefit from agentic or few-shot prompting for MBPP+; zero-shot baseline was optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Granite 3.1 8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>8B Granite instruction model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MBPP+</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Python programming problems evaluated by execution of testcases; correctness determined by passing extended tests.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural language specification vs agentic ReAct prompts with execution feedback / few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>ReAct and CoT considered; MBPP+ includes execution feedback tool; ReWOO excluded. Zero-shot baseline remained best for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>62.9% (optimized; zero-shot retained, ±0.0pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>62.9% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.0% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Execute tool available for coding tasks; successive-halving; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.0 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e7456.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granite13B_MBPP_ReAct_5shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granite 13B Instruct V2 on MBPP+ with ReAct (5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Granite 13B gains substantially on MBPP+ when using 5-shot ReAct (agentic with execution feedback) compared to zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Granite 13B Instruct V2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generalist instruction-tuned Granite 13B model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MBPP+</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Programming (Python) problems judged by execution of testcases.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>ReAct agentic few-shot (5 demonstrations with execute-feedback) vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>ReAct with trajectories showing thought-action-observation loops and execution feedback; 5-shot; execute tool used to run code; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>19.2% (optimized, ±1.2pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>10.7% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+8.5% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Successive-halving optimization; execute tool executes code; greedy decoding; demonstrations include iterative refinement examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±1.2 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e7456.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granite20B_MBPP_ZeroShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granite 20B Code on MBPP+ (zero-shot best)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Granite 20B Code did not benefit from ReAct/few-shot prompting on MBPP+; zero-shot performance remained unchanged.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Granite 20B Code</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Code-optimized Granite model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>20B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MBPP+</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Python programming tasks evaluated by execution.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot vs ReAct few-shot with execution feedback (search space included but zero-shot remained best)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>ReAct considered but zero-shot baseline retained; execute tool available for agentic patterns; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>51.8% (optimized; zero-shot retained, ±0.4pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>51.8% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.0% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Successive-halving; execute tool for MBPP+; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.4 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e7456.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granite34B_MBPP_ReAct_3shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granite 34B Code on MBPP+ with ReAct (3-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Granite 34B Code improved strongly on MBPP+ when using 3-shot ReAct (agentic with execution feedback) over zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Granite 34B Code</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large Granite code model supporting execution-based evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MBPP+</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code generation and execution tasks with extended testcases.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>ReAct few-shot (3 demonstrations) with execute tool feedback vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>3-shot ReAct trajectories including execute-action and observation loops; execute tool runs proposed code; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>61.3% (optimized, ±1.0pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>48.7% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+12.6% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Successive-halving; demonstration sampling; execute tool for code execution; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±1.0 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.18">
                <h3 class="extraction-instance">Extracted Data Instance 18 (e7456.18)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3.1_MBPP_ReAct_5shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.1 8B on MBPP+ with ReAct (5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA 3.1 (8B) shows a small improvement on MBPP+ using 5-shot ReAct with execution feedback versus zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3.1 8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA instruction model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MBPP+</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Python programming tasks evaluated by execution on testcases.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>ReAct few-shot (5 demonstrations) with execute feedback vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>5-shot ReAct trajectories showing thought-action-observation loops with execution; execute tool used; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>62.8% (optimized, ±4.0pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>61.2% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+1.6% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Successive-halving; execute tool; greedy decoding; demonstrations sampled with iterative refinement examples included.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±4.0 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.19">
                <h3 class="extraction-instance">Extracted Data Instance 19 (e7456.19)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3.2_GSMHard_CoT_5shot_cross</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.3 70B on GSM-Hard with CoT (5-shot) using GSM8K demonstrations (cross-dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cross-dataset experiment: LLaMA 3.3 (70B) optimized on GSM-Hard using GSM8K demonstrations improved notabley with 5-shot CoT compared to zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3.3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large LLaMA instruction model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-Hard (cross-optimized using GSM8K demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Harder variant of GSM8K (larger numeric values); exact numeric answer required.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>CoT few-shot (5 demonstrations) drawn from GSM8K demo bank vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Cross-dataset demonstration reuse: GSM8K few-shot examples (5-shot) used while optimizing on GSM-Hard valid set; CoT pattern; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>53.8% (optimized, ±0.4pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>47.3% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+6.5% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Low-resource/cross-dataset setup; successive-halving; greedy decoding; Calc tool available.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.4 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.20">
                <h3 class="extraction-instance">Extracted Data Instance 20 (e7456.20)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granite3.1_GSMHard_ReAct_5shot_cross</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granite 3.1 8B on GSM-Hard with ReAct (5-shot, Granite Tools) using GSM8K demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cross-dataset run where Granite 3.1 (8B) on GSM-Hard modestly benefited from a 5-shot ReAct prompt (with Granite Tools system prompt) using GSM8K demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Granite 3.1 8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>8B Granite model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-Hard (cross-optimized using GSM8K demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Harder math problems variant with large numeric values; numeric exact answers required.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>ReAct few-shot (5 trajectories) with Granite Tools system prompt vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>ReAct agentic pattern with tool-calling (Calc) demonstrated in trajectories; 5-shot sampled from GSM8K; specific system prompt variant (Granite Tools) selected by optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>37.8% (optimized, ±0.8pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>36.0% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+1.8% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Cross-dataset demonstration reuse; successive-halving; greedy decoding; Calc tool used.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.8 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.21">
                <h3 class="extraction-instance">Extracted Data Instance 21 (e7456.21)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granite13B_GSMHard_CoT_5shot_cross</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granite 13B Instruct V2 on GSM-Hard with CoT (5-shot using GSM8K demos)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Granite 13B on GSM-Hard saw small gains when CoT 5-shot demonstrations from GSM8K were used during optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Granite 13B Instruct V2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generalist Granite instruction-tuned model (13B).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-Hard (cross-optimized using GSM8K demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math problems with large numbers; exact numeric matches.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>CoT few-shot (5 demonstrations from GSM8K) vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>5-shot chain-of-thought where demonstrations are drawn from a related dataset (GSM8K) while optimizing on GSM-Hard valid; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>6.2% (optimized, ±0.7pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>4.4% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+1.8% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Cross-dataset demonstration transfer; successive-halving; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.7 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.22">
                <h3 class="extraction-instance">Extracted Data Instance 22 (e7456.22)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granite34B_GSMHard_CoT_3shot_cross</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granite 34B Code on GSM-Hard with CoT (3-shot using GSM8K demos)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Granite 34B Code improved on GSM-Hard by using 3-shot CoT demonstrations drawn from GSM8K during optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Granite 34B Code</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large Granite code model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-Hard (cross-optimized using GSM8K demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Hard math problem dataset variant; exact numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>CoT few-shot (3 demonstrations from GSM8K) vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>3-shot CoT trajectories sampled from GSM8K train bank used while optimizing for GSM-Hard; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>31.0% (optimized, ±0.9pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>27.9% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+3.0% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Cross-dataset demos; successive-halving; greedy decoding; Calc tool accessible.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.9 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.23">
                <h3 class="extraction-instance">Extracted Data Instance 23 (e7456.23)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3.2_GSMHard_CoT_5shot_cross</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.2 3B on GSM-Hard with CoT (5-shot using GSM8K demos)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA 3.2 (3B) had a minor improvement on GSM-Hard using 5-shot CoT examples drawn from GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3.2 3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>3B LLaMA instruction model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-Hard (cross-optimized)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Hard math problems requiring numerical exact match.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>CoT few-shot (5 demonstrations from GSM8K) vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>5-shot CoT demonstrations sampled from GSM8K bank used while optimizing on GSM-Hard; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>26.8% (optimized, ±0.6pp)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>26.3% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.5% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Cross-dataset demonstration reuse; successive-halving; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>optimized accuracy reported with std ±0.6 percentage points</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.24">
                <h3 class="extraction-instance">Extracted Data Instance 24 (e7456.24)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4o_FEVER_CoT_3shot_transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini (frontier) on FEVER with CoT (3-shot) using optimized PDL from LLaMA 3.1 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transfer experiment: an optimized 3-shot CoT PDL program (found for LLaMA 3.1 70B) improves GPT-4o-mini's FEVER accuracy compared to its zero-shot baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini-2024-07-18</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Commercial frontier OpenAI model variant evaluated for transfer of optimized prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>closed-source (not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>FEVER (transfer of optimized PDL)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary fact verification (true/false) using Wikipedia evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Transferred CoT few-shot (3-shot) PDL program vs GPT-4o-mini zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>PDL program optimized on an open-source model (LLaMA 3.1 70B) containing 3-shot CoT demonstrations applied unchanged to GPT-4o-mini; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>87.7% (optimized)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>83.7% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+4.0% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Transfer of optimized PDL programs; greedy decoding; evaluation on test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.25">
                <h3 class="extraction-instance">Extracted Data Instance 25 (e7456.25)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4o_GSMHard_ReAct_5shot_transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini on GSM-Hard with ReAct (5-shot, Granite LLaMa system prompt) transferred from LLaMA 3.1 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transferred ReAct PDL program (5-shot) improved GPT-4o-mini accuracy substantially on GSM-Hard relative to its zero-shot baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini-2024-07-18</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source frontier model used to test cross-model transfer of optimized prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>closed-source (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-Hard (transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Hard math problems dataset variant requiring exact numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>ReAct few-shot (5 demonstrations) with Granite LLaMa system prompt transferred vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>ReAct 5-shot PDL program (with system prompt variant 'Granite LLaMa') found for LLaMA 3.1 70B applied to GPT-4o-mini; Calc tool semantics included in PDL; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>54.9% (optimized)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>45.6% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+9.3% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Transfer evaluation of optimized PDL programs; greedy decoding; tool-enabled agentic prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.26">
                <h3 class="extraction-instance">Extracted Data Instance 26 (e7456.26)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4o_GSM8K_CoT_5shot_transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini on GSM8K with CoT (5-shot) transferred from LLaMA 3.1 70B optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 5-shot CoT PDL program optimized for an open-source model transferred to GPT-4o-mini yields a large accuracy improvement on GSM8K over zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini-2024-07-18</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Frontier OpenAI model used to test transferability of optimized prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>closed-source (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems requiring exact numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>CoT few-shot (5 demonstrations) transferred vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>5-shot chain-of-thought PDL program optimized for LLaMA applied to GPT-4o-mini without modification; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>90.9% (optimized)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>77.8% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+13.1% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Cross-model transfer of PDL programs; greedy decoding; demonstrations sampled from GSM8K train bank.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7456.27">
                <h3 class="extraction-instance">Extracted Data Instance 27 (e7456.27)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4o_MBPP_ZeroShot_transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini on MBPP+ (transferred PDL retained zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transferring optimized PDL programs to GPT-4o-mini did not improve MBPP+ performance over its zero-shot baseline for the evaluated configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini-2024-07-18</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Frontier OpenAI model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>closed-source (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MBPP+ (transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Programming problems with execution-based validation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Transferred PDL (various patterns) vs zero-shot; zero-shot remained best</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Attempted transfer of optimized PDLs from an open-source model; MBPP+ uses execute tool and ReAct variants but transfer did not change accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>72.3% (optimized; no change)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>72.3% (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.0% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Transfer experiments; greedy decoding; execute tool for MBPP+.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 2)</em></li>
                <li>ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models <em>(Rating: 2)</em></li>
                <li>DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines <em>(Rating: 1)</em></li>
                <li>APE: Automatic Prompt Engineering (Zhou et al. 2023) <em>(Rating: 1)</em></li>
                <li>PAL: Program-aided Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7456",
    "paper_id": "paper-277621144",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Granite3.1_FEVER_ReWOO_5shot",
            "name_full": "Granite 3.1 8B on FEVER with ReWOO (5-shot)",
            "brief_description": "Evaluation of an 8B Granite model on FEVER showing that switching from zero-shot to a 5-shot ReWOO prompt (agentic, reasoning-without-observations trajectories) improves accuracy modestly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Granite 3.1 8B",
            "model_description": "Open-source Granite family generalist instruction model evaluated for natural-language tasks; used as an inference-service model in experiments.",
            "model_size": "8B",
            "task_name": "FEVER",
            "task_description": "Binary fact verification: decide whether a claim is true or false given supporting evidence (Wikipedia-derived).",
            "problem_format": "Agentic trajectory (ReWOO) few-shot prompt (5 demonstrations) vs zero-shot natural language question",
            "format_category": "prompt style",
            "format_details": "ReWOO pattern: few-shot trajectories (5) containing reasoning thoughts and action sequences (no reactive tool feedback); compared against zero-shot baseline; greedy decoding; tool: Search available for agentic patterns.",
            "performance_metric": "Accuracy",
            "performance_value": "76.4% (optimized, ±3.3pp)",
            "baseline_performance": "72.9% (zero-shot)",
            "performance_change": "+3.5% absolute",
            "experimental_setting": "Greedy decoding; successive-halving AutoPDL optimization; validation/test splits; tools enabled for agentic patterns (Search); candidates sampled with num_demonstrations ∈ {0,3,5}.",
            "statistical_significance": "optimized accuracy reported with std ±3.3 percentage points",
            "uuid": "e7456.0",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Granite13B_FEVER_ReWOO_3shot",
            "name_full": "Granite 13B Instruct V2 on FEVER with ReWOO (3-shot)",
            "brief_description": "Large Granite instruction model on FEVER shows dramatic improvement when using a 3-shot ReWOO few-shot prompt versus a weak zero-shot baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Granite 13B Instruct V2",
            "model_description": "Generalist instruction-tuned Granite model intended for assistant-style tasks.",
            "model_size": "13B",
            "task_name": "FEVER",
            "task_description": "Binary fact verification (true/false) using Wikipedia evidence.",
            "problem_format": "ReWOO prompt (3 few-shot trajectories) vs zero-shot",
            "format_category": "prompt style",
            "format_details": "ReWOO (3-shot) with few-shot trajectories containing reasoning steps but no reactive observations; compares to zero-shot; greedy decoding.",
            "performance_metric": "Accuracy",
            "performance_value": "74.0% (optimized, ±1.4pp)",
            "baseline_performance": "6.5% (zero-shot)",
            "performance_change": "+67.5% absolute",
            "experimental_setting": "Successive-halving search over patterns and demonstrations; greedy decoding; tool: Search available for FEVER agentic prompts.",
            "statistical_significance": "optimized accuracy reported with std ±1.4 percentage points",
            "uuid": "e7456.1",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Granite20B_FEVER_CoT_3shot",
            "name_full": "Granite 20B Code on FEVER with CoT (3-shot)",
            "brief_description": "A Granite code-focused 20B model on FEVER improves substantially by using 3-shot chain-of-thought few-shot prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Granite 20B Code",
            "model_description": "Granite family model specialized for code tasks but evaluated as a generalist on FEVER.",
            "model_size": "20B",
            "task_name": "FEVER",
            "task_description": "Binary fact verification (true/false) using Wikipedia evidence.",
            "problem_format": "Chain-of-thought (CoT) few-shot prompt (3 demonstrations) vs zero-shot",
            "format_category": "prompt style",
            "format_details": "CoT with 3 few-shot examples that include explicit reasoning chains; compared to zero-shot baseline; greedy decoding; Search tool available for agentic patterns though CoT here uses reasoning steps in demonstrations.",
            "performance_metric": "Accuracy",
            "performance_value": "63.1% (optimized, ±1.6pp)",
            "baseline_performance": "39.7% (zero-shot)",
            "performance_change": "+23.4% absolute",
            "experimental_setting": "Successive-halving optimization; demonstrations sampled from train bank; greedy decoding.",
            "statistical_significance": "optimized accuracy reported with std ±1.6 percentage points",
            "uuid": "e7456.2",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Granite34B_FEVER_CoT_3shot",
            "name_full": "Granite 34B Code on FEVER with CoT (3-shot)",
            "brief_description": "A 34B Granite code model improves FEVER accuracy modestly by using 3-shot chain-of-thought prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Granite 34B Code",
            "model_description": "Large Granite model trained with code capabilities; evaluated on natural language fact verification.",
            "model_size": "34B",
            "task_name": "FEVER",
            "task_description": "Binary fact verification using Wikipedia evidence.",
            "problem_format": "CoT few-shot (3 demonstrations) vs zero-shot",
            "format_category": "prompt style",
            "format_details": "3-shot chain-of-thought demonstrations that include reasoning steps; greedy decoding; comparisons against zero-shot baseline.",
            "performance_metric": "Accuracy",
            "performance_value": "62.6% (optimized, ±3.8pp)",
            "baseline_performance": "56.4% (zero-shot)",
            "performance_change": "+6.2% absolute",
            "experimental_setting": "Successive-halving search; demonstration sampling; tools available though CoT demonstrations include evidence.",
            "statistical_significance": "optimized accuracy reported with std ±3.8 percentage points",
            "uuid": "e7456.3",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLaMA3.1_FEVER_CoT_3shot",
            "name_full": "LLaMA 3.1 8B on FEVER with CoT (3-shot)",
            "brief_description": "LLaMA 3.1 (8B) benefits from a 3-shot chain-of-thought prompt on FEVER relative to zero-shot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA 3.1 8B",
            "model_description": "Open-source LLaMA-family instruction model tuned for general language tasks.",
            "model_size": "8B",
            "task_name": "FEVER",
            "task_description": "True/false fact verification using evidence.",
            "problem_format": "CoT few-shot (3 demonstrations) vs zero-shot",
            "format_category": "prompt style",
            "format_details": "3-shot chain-of-thought demonstrations including reasoning steps; compared to zero-shot baseline; greedy decoding.",
            "performance_metric": "Accuracy",
            "performance_value": "77.5% (optimized, ±0.8pp)",
            "baseline_performance": "68.5% (zero-shot)",
            "performance_change": "+9.0% absolute",
            "experimental_setting": "Successive-halving optimization; tool access for agentic patterns; greedy decoding.",
            "statistical_significance": "optimized accuracy reported with std ±0.8 percentage points",
            "uuid": "e7456.4",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLaMA3.2_FEVER_ReWOO_5shot",
            "name_full": "LLaMA 3.2 3B on FEVER with ReWOO (5-shot)",
            "brief_description": "Smaller LLaMA 3.2 (3B) shows large gain on FEVER when switched from zero-shot to 5-shot ReWOO prompting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA 3.2 3B",
            "model_description": "3B-parameter LLaMA-family instruction model used to study scale effects.",
            "model_size": "3B",
            "task_name": "FEVER",
            "task_description": "Binary fact verification (true/false).",
            "problem_format": "ReWOO few-shot (5 trajectories) vs zero-shot",
            "format_category": "prompt style",
            "format_details": "ReWOO pattern with 5 few-shot trajectories (reasoning without observations); compared to zero-shot; greedy decoding.",
            "performance_metric": "Accuracy",
            "performance_value": "66.3% (optimized, ±0.9pp)",
            "baseline_performance": "38.0% (zero-shot)",
            "performance_change": "+28.3% absolute",
            "experimental_setting": "Successive-halving search; demonstration sampling from train bank; greedy decoding.",
            "statistical_significance": "optimized accuracy reported with std ±0.9 percentage points",
            "uuid": "e7456.5",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLaMA3.3_FEVER_ReWOO_3shot",
            "name_full": "LLaMA 3.3 70B on FEVER with ReWOO (3-shot)",
            "brief_description": "Large LLaMA 70B model benefits substantially from a 3-shot ReWOO prompt on FEVER versus zero-shot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA 3.3 70B",
            "model_description": "Large-capacity LLaMA instruction model (70B) evaluated across tasks.",
            "model_size": "70B",
            "task_name": "FEVER",
            "task_description": "Binary fact verification with Wikipedia evidence.",
            "problem_format": "ReWOO few-shot (3 demonstrations) vs zero-shot",
            "format_category": "prompt style",
            "format_details": "3-shot ReWOO few-shot trajectories; greedy decoding; compared to zero-shot baseline.",
            "performance_metric": "Accuracy",
            "performance_value": "78.1% (optimized, ±0.6pp)",
            "baseline_performance": "67.6% (zero-shot)",
            "performance_change": "+10.5% absolute",
            "experimental_setting": "Successive-halving AutoPDL optimization; demonstration sampling; tool access available.",
            "statistical_significance": "optimized accuracy reported with std ±0.6 percentage points",
            "uuid": "e7456.6",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Granite3.1_GSM8K_ZeroShot",
            "name_full": "Granite 3.1 8B on GSM8K (zero-shot remained best)",
            "brief_description": "Granite 3.1 8B showed no improvement from prompt optimization on GSM8K; zero-shot baseline remained optimal.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Granite 3.1 8B",
            "model_description": "8B Granite instruction model.",
            "model_size": "8B",
            "task_name": "GSM8K",
            "task_description": "Grade-school math word problems requiring numeric reasoning and arithmetic.",
            "problem_format": "Zero-shot natural language question (no demonstrations) vs few-shot CoT/agentic variants",
            "format_category": "prompt style",
            "format_details": "Search space included {0,3,5} demonstrations, CoT, ReAct, ReWOO patterns; optimization found zero-shot to be best for this model on GSM8K.",
            "performance_metric": "Accuracy",
            "performance_value": "74.2% (optimized; zero-shot baseline retained, ±0.6pp)",
            "baseline_performance": "74.2% (zero-shot)",
            "performance_change": "+0.0% absolute",
            "experimental_setting": "Greedy decoding; tool: Calc available for agentic patterns; successive-halving optimization.",
            "statistical_significance": "optimized accuracy reported with std ±0.6 percentage points",
            "uuid": "e7456.7",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Granite13B_GSM8K_CoT_3shot",
            "name_full": "Granite 13B Instruct V2 on GSM8K with CoT (3-shot)",
            "brief_description": "Granite 13B improved on GSM8K by using 3-shot chain-of-thought few-shot prompts compared to zero-shot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Granite 13B Instruct V2",
            "model_description": "13B Granite instruction-tuned model.",
            "model_size": "13B",
            "task_name": "GSM8K",
            "task_description": "Grade-school math word problems; exact numeric answers required.",
            "problem_format": "Chain-of-thought few-shot (3 demonstrations) vs zero-shot",
            "format_category": "prompt style",
            "format_details": "3-shot CoT demonstrations include step-by-step reasoning chains; compared to zero-shot; greedy decoding.",
            "performance_metric": "Accuracy",
            "performance_value": "30.9% (optimized, ±1.0pp)",
            "baseline_performance": "23.0% (zero-shot)",
            "performance_change": "+7.9% absolute",
            "experimental_setting": "Calc tool provided for agentic patterns; successive-halving; greedy decoding.",
            "statistical_significance": "optimized accuracy reported with std ±1.0 percentage points",
            "uuid": "e7456.8",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Granite20B_GSM8K_ZeroShot",
            "name_full": "Granite 20B Code on GSM8K (zero-shot best)",
            "brief_description": "Granite 20B Code showed no improvement from prompt optimization on GSM8K; zero-shot baseline remained best.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Granite 20B Code",
            "model_description": "Code-specialized Granite 20B model.",
            "model_size": "20B",
            "task_name": "GSM8K",
            "task_description": "Grade-school math problems with exact numeric answers.",
            "problem_format": "Zero-shot natural language vs few-shot CoT/agentic patterns",
            "format_category": "prompt style",
            "format_details": "Optimization considered 0/3/5 demonstrations and agentic patterns; zero-shot remained best for this model.",
            "performance_metric": "Accuracy",
            "performance_value": "68.7% (optimized; zero-shot baseline retained, ±0.1pp)",
            "baseline_performance": "68.7% (zero-shot)",
            "performance_change": "+0.0% absolute",
            "experimental_setting": "Greedy decoding; Calc tool available; successive-halving.",
            "statistical_significance": "optimized accuracy reported with std ±0.1 percentage points",
            "uuid": "e7456.9",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Granite34B_GSM8K_ZeroShot",
            "name_full": "Granite 34B Code on GSM8K (zero-shot best)",
            "brief_description": "Granite 34B Code saw no improvement from prompt optimization on GSM8K; zero-shot baseline retained.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Granite 34B Code",
            "model_description": "Large code-capable Granite model.",
            "model_size": "34B",
            "task_name": "GSM8K",
            "task_description": "Grade-school math word problems requiring exact numeric answers.",
            "problem_format": "Zero-shot natural language question vs few-shot/agentic variants",
            "format_category": "prompt style",
            "format_details": "Search included few-shot counts {0,3,5} and patterns {Zero-Shot, CoT, ReWOO, ReAct}; zero-shot was optimal.",
            "performance_metric": "Accuracy",
            "performance_value": "72.1% (optimized; zero-shot baseline retained, ±0.1pp)",
            "baseline_performance": "72.1% (zero-shot)",
            "performance_change": "+0.0% absolute",
            "experimental_setting": "Greedy decoding; Calc tool available; successive-halving.",
            "statistical_significance": "optimized accuracy reported with std ±0.1 percentage points",
            "uuid": "e7456.10",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLaMA3.1_GSM8K_CoT_5shot",
            "name_full": "LLaMA 3.1 8B on GSM8K with CoT (5-shot)",
            "brief_description": "LLaMA 3.1 (8B) substantially improves on GSM8K when using 5-shot chain-of-thought prompts versus zero-shot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA 3.1 8B",
            "model_description": "Open-source LLaMA instruction model.",
            "model_size": "8B",
            "task_name": "GSM8K",
            "task_description": "Math word problems; numeric exact-match answers.",
            "problem_format": "Chain-of-thought few-shot (5 demonstrations) vs zero-shot",
            "format_category": "prompt style",
            "format_details": "5-shot CoT demonstrations including full reasoning chains; compared to zero-shot; greedy decoding; Calc tool used for agentic trajectories.",
            "performance_metric": "Accuracy",
            "performance_value": "85.3% (optimized, ±0.6pp)",
            "baseline_performance": "78.4% (zero-shot)",
            "performance_change": "+6.9% absolute",
            "experimental_setting": "Successive-halving AutoPDL; greedy decoding; demonstrations sampled from train bank.",
            "statistical_significance": "optimized accuracy reported with std ±0.6 percentage points",
            "uuid": "e7456.11",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLaMA3.2_GSM8K_CoT_3shot",
            "name_full": "LLaMA 3.2 3B on GSM8K with CoT (3-shot)",
            "brief_description": "LLaMA 3.2 (3B) improves modestly on GSM8K using 3-shot chain-of-thought prompts compared to zero-shot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA 3.2 3B",
            "model_description": "3B-parameter LLaMA instruction model.",
            "model_size": "3B",
            "task_name": "GSM8K",
            "task_description": "Grade-school math problems requiring exact numeric answers.",
            "problem_format": "CoT few-shot (3 demonstrations) vs zero-shot",
            "format_category": "prompt style",
            "format_details": "3-shot CoT examples with explicit step-by-step reasoning; greedy decoding; comparisons versus zero-shot baseline.",
            "performance_metric": "Accuracy",
            "performance_value": "75.3% (optimized, ±0.4pp)",
            "baseline_performance": "71.8% (zero-shot)",
            "performance_change": "+3.5% absolute",
            "experimental_setting": "Successive-halving; Calc tool accessible for agentic patterns; greedy decoding.",
            "statistical_significance": "optimized accuracy reported with std ±0.4 percentage points",
            "uuid": "e7456.12",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLaMA3.3_GSM8K_CoT_3shot",
            "name_full": "LLaMA 3.3 70B on GSM8K with CoT (3-shot)",
            "brief_description": "Large LLaMA 70B dramatically improves on GSM8K when using 3-shot chain-of-thought prompting vs zero-shot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA 3.3 70B",
            "model_description": "High-capacity LLaMA instruction model.",
            "model_size": "70B",
            "task_name": "GSM8K",
            "task_description": "Math word problems; expect exact numeric answers.",
            "problem_format": "CoT few-shot (3 demonstrations) vs zero-shot",
            "format_category": "prompt style",
            "format_details": "3-shot CoT demonstrations containing chain-of-thought reasoning; greedy decoding; compared to zero-shot baseline.",
            "performance_metric": "Accuracy",
            "performance_value": "95.4% (optimized, ±0.2pp)",
            "baseline_performance": "85.5% (zero-shot)",
            "performance_change": "+9.9% absolute",
            "experimental_setting": "Successive-halving search; Calc tool available; greedy decoding.",
            "statistical_significance": "optimized accuracy reported with std ±0.2 percentage points",
            "uuid": "e7456.13",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Granite3.1_MBPP_ZeroShot",
            "name_full": "Granite 3.1 8B on MBPP+ (zero-shot best)",
            "brief_description": "Granite 3.1 (8B) shows no benefit from agentic or few-shot prompting for MBPP+; zero-shot baseline was optimal.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Granite 3.1 8B",
            "model_description": "8B Granite instruction model.",
            "model_size": "8B",
            "task_name": "MBPP+",
            "task_description": "Python programming problems evaluated by execution of testcases; correctness determined by passing extended tests.",
            "problem_format": "Zero-shot natural language specification vs agentic ReAct prompts with execution feedback / few-shot",
            "format_category": "prompt style",
            "format_details": "ReAct and CoT considered; MBPP+ includes execution feedback tool; ReWOO excluded. Zero-shot baseline remained best for this model.",
            "performance_metric": "Accuracy",
            "performance_value": "62.9% (optimized; zero-shot retained, ±0.0pp)",
            "baseline_performance": "62.9% (zero-shot)",
            "performance_change": "+0.0% absolute",
            "experimental_setting": "Execute tool available for coding tasks; successive-halving; greedy decoding.",
            "statistical_significance": "optimized accuracy reported with std ±0.0 percentage points",
            "uuid": "e7456.14",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Granite13B_MBPP_ReAct_5shot",
            "name_full": "Granite 13B Instruct V2 on MBPP+ with ReAct (5-shot)",
            "brief_description": "Granite 13B gains substantially on MBPP+ when using 5-shot ReAct (agentic with execution feedback) compared to zero-shot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Granite 13B Instruct V2",
            "model_description": "Generalist instruction-tuned Granite 13B model.",
            "model_size": "13B",
            "task_name": "MBPP+",
            "task_description": "Programming (Python) problems judged by execution of testcases.",
            "problem_format": "ReAct agentic few-shot (5 demonstrations with execute-feedback) vs zero-shot",
            "format_category": "prompt style",
            "format_details": "ReAct with trajectories showing thought-action-observation loops and execution feedback; 5-shot; execute tool used to run code; greedy decoding.",
            "performance_metric": "Accuracy",
            "performance_value": "19.2% (optimized, ±1.2pp)",
            "baseline_performance": "10.7% (zero-shot)",
            "performance_change": "+8.5% absolute",
            "experimental_setting": "Successive-halving optimization; execute tool executes code; greedy decoding; demonstrations include iterative refinement examples.",
            "statistical_significance": "optimized accuracy reported with std ±1.2 percentage points",
            "uuid": "e7456.15",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Granite20B_MBPP_ZeroShot",
            "name_full": "Granite 20B Code on MBPP+ (zero-shot best)",
            "brief_description": "Granite 20B Code did not benefit from ReAct/few-shot prompting on MBPP+; zero-shot performance remained unchanged.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Granite 20B Code",
            "model_description": "Code-optimized Granite model.",
            "model_size": "20B",
            "task_name": "MBPP+",
            "task_description": "Python programming tasks evaluated by execution.",
            "problem_format": "Zero-shot vs ReAct few-shot with execution feedback (search space included but zero-shot remained best)",
            "format_category": "prompt style",
            "format_details": "ReAct considered but zero-shot baseline retained; execute tool available for agentic patterns; greedy decoding.",
            "performance_metric": "Accuracy",
            "performance_value": "51.8% (optimized; zero-shot retained, ±0.4pp)",
            "baseline_performance": "51.8% (zero-shot)",
            "performance_change": "+0.0% absolute",
            "experimental_setting": "Successive-halving; execute tool for MBPP+; greedy decoding.",
            "statistical_significance": "optimized accuracy reported with std ±0.4 percentage points",
            "uuid": "e7456.16",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Granite34B_MBPP_ReAct_3shot",
            "name_full": "Granite 34B Code on MBPP+ with ReAct (3-shot)",
            "brief_description": "Granite 34B Code improved strongly on MBPP+ when using 3-shot ReAct (agentic with execution feedback) over zero-shot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Granite 34B Code",
            "model_description": "Large Granite code model supporting execution-based evaluation.",
            "model_size": "34B",
            "task_name": "MBPP+",
            "task_description": "Code generation and execution tasks with extended testcases.",
            "problem_format": "ReAct few-shot (3 demonstrations) with execute tool feedback vs zero-shot",
            "format_category": "prompt style",
            "format_details": "3-shot ReAct trajectories including execute-action and observation loops; execute tool runs proposed code; greedy decoding.",
            "performance_metric": "Accuracy",
            "performance_value": "61.3% (optimized, ±1.0pp)",
            "baseline_performance": "48.7% (zero-shot)",
            "performance_change": "+12.6% absolute",
            "experimental_setting": "Successive-halving; demonstration sampling; execute tool for code execution; greedy decoding.",
            "statistical_significance": "optimized accuracy reported with std ±1.0 percentage points",
            "uuid": "e7456.17",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLaMA3.1_MBPP_ReAct_5shot",
            "name_full": "LLaMA 3.1 8B on MBPP+ with ReAct (5-shot)",
            "brief_description": "LLaMA 3.1 (8B) shows a small improvement on MBPP+ using 5-shot ReAct with execution feedback versus zero-shot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA 3.1 8B",
            "model_description": "Open-source LLaMA instruction model.",
            "model_size": "8B",
            "task_name": "MBPP+",
            "task_description": "Python programming tasks evaluated by execution on testcases.",
            "problem_format": "ReAct few-shot (5 demonstrations) with execute feedback vs zero-shot",
            "format_category": "prompt style",
            "format_details": "5-shot ReAct trajectories showing thought-action-observation loops with execution; execute tool used; greedy decoding.",
            "performance_metric": "Accuracy",
            "performance_value": "62.8% (optimized, ±4.0pp)",
            "baseline_performance": "61.2% (zero-shot)",
            "performance_change": "+1.6% absolute",
            "experimental_setting": "Successive-halving; execute tool; greedy decoding; demonstrations sampled with iterative refinement examples included.",
            "statistical_significance": "optimized accuracy reported with std ±4.0 percentage points",
            "uuid": "e7456.18",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLaMA3.2_GSMHard_CoT_5shot_cross",
            "name_full": "LLaMA 3.3 70B on GSM-Hard with CoT (5-shot) using GSM8K demonstrations (cross-dataset)",
            "brief_description": "Cross-dataset experiment: LLaMA 3.3 (70B) optimized on GSM-Hard using GSM8K demonstrations improved notabley with 5-shot CoT compared to zero-shot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA 3.3 70B",
            "model_description": "Large LLaMA instruction model.",
            "model_size": "70B",
            "task_name": "GSM-Hard (cross-optimized using GSM8K demonstrations)",
            "task_description": "Harder variant of GSM8K (larger numeric values); exact numeric answer required.",
            "problem_format": "CoT few-shot (5 demonstrations) drawn from GSM8K demo bank vs zero-shot",
            "format_category": "prompt style",
            "format_details": "Cross-dataset demonstration reuse: GSM8K few-shot examples (5-shot) used while optimizing on GSM-Hard valid set; CoT pattern; greedy decoding.",
            "performance_metric": "Accuracy",
            "performance_value": "53.8% (optimized, ±0.4pp)",
            "baseline_performance": "47.3% (zero-shot)",
            "performance_change": "+6.5% absolute",
            "experimental_setting": "Low-resource/cross-dataset setup; successive-halving; greedy decoding; Calc tool available.",
            "statistical_significance": "optimized accuracy reported with std ±0.4 percentage points",
            "uuid": "e7456.19",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Granite3.1_GSMHard_ReAct_5shot_cross",
            "name_full": "Granite 3.1 8B on GSM-Hard with ReAct (5-shot, Granite Tools) using GSM8K demonstrations",
            "brief_description": "Cross-dataset run where Granite 3.1 (8B) on GSM-Hard modestly benefited from a 5-shot ReAct prompt (with Granite Tools system prompt) using GSM8K demonstrations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Granite 3.1 8B",
            "model_description": "8B Granite model.",
            "model_size": "8B",
            "task_name": "GSM-Hard (cross-optimized using GSM8K demonstrations)",
            "task_description": "Harder math problems variant with large numeric values; numeric exact answers required.",
            "problem_format": "ReAct few-shot (5 trajectories) with Granite Tools system prompt vs zero-shot",
            "format_category": "prompt style",
            "format_details": "ReAct agentic pattern with tool-calling (Calc) demonstrated in trajectories; 5-shot sampled from GSM8K; specific system prompt variant (Granite Tools) selected by optimizer.",
            "performance_metric": "Accuracy",
            "performance_value": "37.8% (optimized, ±0.8pp)",
            "baseline_performance": "36.0% (zero-shot)",
            "performance_change": "+1.8% absolute",
            "experimental_setting": "Cross-dataset demonstration reuse; successive-halving; greedy decoding; Calc tool used.",
            "statistical_significance": "optimized accuracy reported with std ±0.8 percentage points",
            "uuid": "e7456.20",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Granite13B_GSMHard_CoT_5shot_cross",
            "name_full": "Granite 13B Instruct V2 on GSM-Hard with CoT (5-shot using GSM8K demos)",
            "brief_description": "Granite 13B on GSM-Hard saw small gains when CoT 5-shot demonstrations from GSM8K were used during optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Granite 13B Instruct V2",
            "model_description": "Generalist Granite instruction-tuned model (13B).",
            "model_size": "13B",
            "task_name": "GSM-Hard (cross-optimized using GSM8K demonstrations)",
            "task_description": "Math problems with large numbers; exact numeric matches.",
            "problem_format": "CoT few-shot (5 demonstrations from GSM8K) vs zero-shot",
            "format_category": "prompt style",
            "format_details": "5-shot chain-of-thought where demonstrations are drawn from a related dataset (GSM8K) while optimizing on GSM-Hard valid; greedy decoding.",
            "performance_metric": "Accuracy",
            "performance_value": "6.2% (optimized, ±0.7pp)",
            "baseline_performance": "4.4% (zero-shot)",
            "performance_change": "+1.8% absolute",
            "experimental_setting": "Cross-dataset demonstration transfer; successive-halving; greedy decoding.",
            "statistical_significance": "optimized accuracy reported with std ±0.7 percentage points",
            "uuid": "e7456.21",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Granite34B_GSMHard_CoT_3shot_cross",
            "name_full": "Granite 34B Code on GSM-Hard with CoT (3-shot using GSM8K demos)",
            "brief_description": "Granite 34B Code improved on GSM-Hard by using 3-shot CoT demonstrations drawn from GSM8K during optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Granite 34B Code",
            "model_description": "Large Granite code model.",
            "model_size": "34B",
            "task_name": "GSM-Hard (cross-optimized using GSM8K demonstrations)",
            "task_description": "Hard math problem dataset variant; exact numeric answers.",
            "problem_format": "CoT few-shot (3 demonstrations from GSM8K) vs zero-shot",
            "format_category": "prompt style",
            "format_details": "3-shot CoT trajectories sampled from GSM8K train bank used while optimizing for GSM-Hard; greedy decoding.",
            "performance_metric": "Accuracy",
            "performance_value": "31.0% (optimized, ±0.9pp)",
            "baseline_performance": "27.9% (zero-shot)",
            "performance_change": "+3.0% absolute",
            "experimental_setting": "Cross-dataset demos; successive-halving; greedy decoding; Calc tool accessible.",
            "statistical_significance": "optimized accuracy reported with std ±0.9 percentage points",
            "uuid": "e7456.22",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLaMA3.2_GSMHard_CoT_5shot_cross",
            "name_full": "LLaMA 3.2 3B on GSM-Hard with CoT (5-shot using GSM8K demos)",
            "brief_description": "LLaMA 3.2 (3B) had a minor improvement on GSM-Hard using 5-shot CoT examples drawn from GSM8K.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA 3.2 3B",
            "model_description": "3B LLaMA instruction model.",
            "model_size": "3B",
            "task_name": "GSM-Hard (cross-optimized)",
            "task_description": "Hard math problems requiring numerical exact match.",
            "problem_format": "CoT few-shot (5 demonstrations from GSM8K) vs zero-shot",
            "format_category": "prompt style",
            "format_details": "5-shot CoT demonstrations sampled from GSM8K bank used while optimizing on GSM-Hard; greedy decoding.",
            "performance_metric": "Accuracy",
            "performance_value": "26.8% (optimized, ±0.6pp)",
            "baseline_performance": "26.3% (zero-shot)",
            "performance_change": "+0.5% absolute",
            "experimental_setting": "Cross-dataset demonstration reuse; successive-halving; greedy decoding.",
            "statistical_significance": "optimized accuracy reported with std ±0.6 percentage points",
            "uuid": "e7456.23",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GPT4o_FEVER_CoT_3shot_transfer",
            "name_full": "GPT-4o-mini (frontier) on FEVER with CoT (3-shot) using optimized PDL from LLaMA 3.1 70B",
            "brief_description": "Transfer experiment: an optimized 3-shot CoT PDL program (found for LLaMA 3.1 70B) improves GPT-4o-mini's FEVER accuracy compared to its zero-shot baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini-2024-07-18",
            "model_description": "Commercial frontier OpenAI model variant evaluated for transfer of optimized prompts.",
            "model_size": "closed-source (not specified)",
            "task_name": "FEVER (transfer of optimized PDL)",
            "task_description": "Binary fact verification (true/false) using Wikipedia evidence.",
            "problem_format": "Transferred CoT few-shot (3-shot) PDL program vs GPT-4o-mini zero-shot",
            "format_category": "prompt style",
            "format_details": "PDL program optimized on an open-source model (LLaMA 3.1 70B) containing 3-shot CoT demonstrations applied unchanged to GPT-4o-mini; greedy decoding.",
            "performance_metric": "Accuracy",
            "performance_value": "87.7% (optimized)",
            "baseline_performance": "83.7% (zero-shot)",
            "performance_change": "+4.0% absolute",
            "experimental_setting": "Transfer of optimized PDL programs; greedy decoding; evaluation on test sets.",
            "statistical_significance": null,
            "uuid": "e7456.24",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GPT4o_GSMHard_ReAct_5shot_transfer",
            "name_full": "GPT-4o-mini on GSM-Hard with ReAct (5-shot, Granite LLaMa system prompt) transferred from LLaMA 3.1 70B",
            "brief_description": "Transferred ReAct PDL program (5-shot) improved GPT-4o-mini accuracy substantially on GSM-Hard relative to its zero-shot baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini-2024-07-18",
            "model_description": "Closed-source frontier model used to test cross-model transfer of optimized prompts.",
            "model_size": "closed-source (unspecified)",
            "task_name": "GSM-Hard (transfer)",
            "task_description": "Hard math problems dataset variant requiring exact numeric answers.",
            "problem_format": "ReAct few-shot (5 demonstrations) with Granite LLaMa system prompt transferred vs zero-shot",
            "format_category": "prompt style",
            "format_details": "ReAct 5-shot PDL program (with system prompt variant 'Granite LLaMa') found for LLaMA 3.1 70B applied to GPT-4o-mini; Calc tool semantics included in PDL; greedy decoding.",
            "performance_metric": "Accuracy",
            "performance_value": "54.9% (optimized)",
            "baseline_performance": "45.6% (zero-shot)",
            "performance_change": "+9.3% absolute",
            "experimental_setting": "Transfer evaluation of optimized PDL programs; greedy decoding; tool-enabled agentic prompts.",
            "statistical_significance": null,
            "uuid": "e7456.25",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GPT4o_GSM8K_CoT_5shot_transfer",
            "name_full": "GPT-4o-mini on GSM8K with CoT (5-shot) transferred from LLaMA 3.1 70B optimization",
            "brief_description": "A 5-shot CoT PDL program optimized for an open-source model transferred to GPT-4o-mini yields a large accuracy improvement on GSM8K over zero-shot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini-2024-07-18",
            "model_description": "Frontier OpenAI model used to test transferability of optimized prompts.",
            "model_size": "closed-source (unspecified)",
            "task_name": "GSM8K (transfer)",
            "task_description": "Grade-school math word problems requiring exact numeric answers.",
            "problem_format": "CoT few-shot (5 demonstrations) transferred vs zero-shot",
            "format_category": "prompt style",
            "format_details": "5-shot chain-of-thought PDL program optimized for LLaMA applied to GPT-4o-mini without modification; greedy decoding.",
            "performance_metric": "Accuracy",
            "performance_value": "90.9% (optimized)",
            "baseline_performance": "77.8% (zero-shot)",
            "performance_change": "+13.1% absolute",
            "experimental_setting": "Cross-model transfer of PDL programs; greedy decoding; demonstrations sampled from GSM8K train bank.",
            "statistical_significance": null,
            "uuid": "e7456.26",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GPT4o_MBPP_ZeroShot_transfer",
            "name_full": "GPT-4o-mini on MBPP+ (transferred PDL retained zero-shot)",
            "brief_description": "Transferring optimized PDL programs to GPT-4o-mini did not improve MBPP+ performance over its zero-shot baseline for the evaluated configuration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini-2024-07-18",
            "model_description": "Frontier OpenAI model.",
            "model_size": "closed-source (unspecified)",
            "task_name": "MBPP+ (transfer)",
            "task_description": "Programming problems with execution-based validation.",
            "problem_format": "Transferred PDL (various patterns) vs zero-shot; zero-shot remained best",
            "format_category": "prompt style",
            "format_details": "Attempted transfer of optimized PDLs from an open-source model; MBPP+ uses execute tool and ReAct variants but transfer did not change accuracy.",
            "performance_metric": "Accuracy",
            "performance_value": "72.3% (optimized; no change)",
            "baseline_performance": "72.3% (zero-shot)",
            "performance_change": "+0.0% absolute",
            "experimental_setting": "Transfer experiments; greedy decoding; execute tool for MBPP+.",
            "statistical_significance": null,
            "uuid": "e7456.27",
            "source_info": {
                "paper_title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models",
            "rating": 2,
            "sanitized_title": "rewoo_decoupling_reasoning_from_observations_for_efficient_augmented_language_models"
        },
        {
            "paper_title": "DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines",
            "rating": 1,
            "sanitized_title": "dspy_compiling_declarative_language_model_calls_into_selfimproving_pipelines"
        },
        {
            "paper_title": "APE: Automatic Prompt Engineering (Zhou et al. 2023)",
            "rating": 1,
            "sanitized_title": "ape_automatic_prompt_engineering_zhou_et_al_2023"
        },
        {
            "paper_title": "PAL: Program-aided Language Models",
            "rating": 1,
            "sanitized_title": "pal_programaided_language_models"
        }
    ],
    "cost": 0.0282275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AutoPDL: Automatic Prompt Optimization for LLM Agents
3 Nov 2025</p>
<p>Claudio Spiess 
UC Davis</p>
<p>Mandana Vaziri 
IBM Research</p>
<p>Louis Mandel 
IBM Research</p>
<p>Martin Hirzel 
IBM Research</p>
<p>AutoPDL: Automatic Prompt Optimization for LLM Agents
3 Nov 2025722E4EB38F05A73C6725ED0559960967arXiv:2504.04365v5[cs.LG]
The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations).Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task.Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations.Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space.We introduce a library implementing common prompting patterns using the PDL prompt programming language.AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library.This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse.Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains (9.21 ± 15.46 percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks.</p>
<p>Introduction</p>
<p>Large language models (LLMs) and LLM-based agents excel at a variety of tasks, including question answering, math word problems, and programming.The performance of an LLM depends heavily on how it is prompted, and there are a variety of popular prompting patterns.These include zero-shot or few-shot (Brown et al. 2020) prompting with chain-of-thought (CoT) (Wei et al. 2022), zero-shot CoT (Kojima et al. 2022), as well as agentic patterns such as ReAct (Yao et al. 2023) or ReWOO (Xu et al. 2023).However, given a dataset  test with a loss function L, e.g., error rate, it is not clear which pattern will do best.Furthermore, besides the pattern , performance also depends on the prompt , including few-shot samples and instructions.The problem is thus to find a combination  *  of a pattern along with an optimized prompt that minimizes L. This is usually done via manual prompt engineering, but that is tedious and has to be repeated if a new LLM comes along.Therefore, this paper explores how to find  *  using automated machine learning (AutoML).And for users to trust the result or to tweak it further,  *  itself should be easy to read and edit.Agent frameworks, such as CrewAI (Moura 2023) or AutoGen (Q.Wu et al. 2023), contain prebuilt agent patterns with prompts optimized for proprietary frontier models and common tasks.Unfortunately, their prompts are deeply buried (Schluntz et al. 2024), making them hard to modify and adapt to non-frontier models or novel tasks.Moreover, the prompting pattern is fixed to a variation of ReAct, limiting flexibility in customizing the prompt structure.Prompt optimizers, such as DSPy (Khattab et al. 2024), optimize few-shot samples for in-context learning (ICL) and/or instructions in the prompt .Unfortunately, they do not automatically select the prompting pattern  and do not return human-readable code.</p>
<p>We formulate the problem of finding a good pattern and corresponding prompt by defining and then exploring a combined search space.We were inspired by the AutoML literature on combined search spaces of machine-learning algorithms and their hyperparameters (Thornton et al. 2013), except that (i) instead of discrete or continuous hyperparameters, we explore textual ICL samples, instructions, and prompting patterns; (ii) instead of classification or regression tasks, we tackle An earlier version of this paper was published at AutoML 2025.This version adds missing standard deviations in Table 1. 1 generative tasks; and (iii) instead of model training or fine-tuning, we focus on in-context learning.We assume a dataset with a validation set  valid , test set  test , as well as an example bank  train for few-shot samples. valid is used during the optimization process to evaluate the performance of a configuration, whereas  test is used once upon completion, to evaluate the performance of the best performing configuration.As usual, to avoid over-fitting, we assume these are disjoint from each other.The problem statement is to find  *  = argmin   ∈A P L(  ,  valid ), where:</p>
<p>•  ∈ A = {Zero-Shot, CoT, ReWOO, ReAct} is the prompting pattern, and</p>
<p>•  = ⟨,  train , instr⟩ ∈ P is the prompt, comprising a number  ≤ | train | of few-shot samples, the actual few-shot samples  train ∈ ( train )  , and an instruction instr ∈ I.A concrete example of P is provided in Figure A1 and Figure A2.</p>
<p>To avoid getting stuck in local minima while saving compute and finding a solution with a low loss, we explore the search space A P using successive halving (Jamieson et al. 2016).To make the initial search space A P user-interpretable, and the final solution  *  both human readable and executable, we express them in a YAML-based prompting language, PDL (Vaziri et al. 2024).PDL's structured format makes it easy to modify both the initial search space and the optimized program, and ensures the final solution remains directly executable.We introduce a library for PDL that implements each of the common prompting patterns in A. The initial search space A P is a YAML file with various choice points for AutoML to decide.And the solution  *  is a customtailored PDL program optimized for the given task, as given by the dataset and loss function.The developer can read or even tweak either or both as desired.</p>
<p>We evaluate our optimizer on three tasks (question answering, math, and programming), using seven LLMs sized between 3B and 70B parameters.We find that the optimizer often gives accuracy boosts in the 6-30% range, in some cases higher.Given the same task, different patterns  ∈ A do best for different models.Conversely, given the same model, different patterns do best for different tasks.Besides this variability in the chosen pattern , our experiments also revealed variability in the optimized prompts  = ⟨,  train , instr⟩.We also found that when training data for a task is missing, data from a related but different dataset can help.Also, while most of our experiments use moderately-sized open models, we also show our optimized solutions can benefit frontier models.</p>
<p>This paper makes three primary contributions:</p>
<ol>
<li>Jointly searching pattern and prompt: prior work in prompt optimization has not investigated searching joint search spaces, including agentic patterns.</li>
</ol>
<p>2.</p>
<p>No one size fits all: we find that different models sometimes have differing optimal prompt patterns for the same benchmark, suggesting that there is not one single optimal prompt pattern.</p>
<ol>
<li>Source-to-source optimization: we propose the first source-to-source optimizer for LLM prompt programs, where both the initial search space and the final solution are prompt programs in the same language, making the final solution both human-readable and executable.</li>
</ol>
<p>Overall, this paper shows how to apply AutoML to automatically discover agentic or nonagentic LLM prompts and patterns optimized for a given task.We make our AutoPDL implementation available at https://github.com/IBM/prompt-declaration-language,and release the reproduction package used for the experiments in this work to the community.</p>
<p>Background</p>
<p>This paper uses PDL (Vaziri et al. 2024) as a representation for exploring the search space of programs.PDL programs are declarative and combine human readability with ease of execution.They represent the composition of calls to LLMs and tools, abstracting away the plumbing necessary for such compositions.The output of the optimizer is also a PDL program, rather than simple textual prompts, so it is fully executable and could be further refined by a developer.</p>
<p>Figure 1 shows a simple PDL program that uses a tool to answer a query.PDL is based on the premise that interactions with an LLM are mainly for the purpose of generating data.So, it allows users to specify the shape of data to be generated in a declarative way (in YAML), and is agnostic of any programming language.The first line of Figure 1 specifies that we are creating some text.Next, the first block in the itemized list defines the tools prompt.Line 3 contains a use of variable tools, expressed as a Jinja expression.This variable is defined as the JSON Schema specification of a calculator tool (not shown in this figure, for the full program see Appendix 8.2).Line 4 is the user query prompt.We do not specify the role explicitly as user is the default role for prompts.Lines 5 through 8 show a model call.In PDL, the background context is accumulated implicitly, so the output of all blocks executed so far will be passed to the LLM as a list of input messages.The result of the model call is assigned to the variable actions (line 5).The model to be called is specified on line 6 (PDL is based on LiteLLM,1 so this is a LiteLLM model id).Finally, lines 7 and 8 say that we parse the output of the model as JSON and type-check it according to the type on line 8. Furthermore, when the inferencing server supports it, model calls with a schema use constrained decoding (Willard et al. 2023), enforcing syntactically correct JSON. 2n line 9, an if-statement checks whether the output of the LLM asks for the calculator tool.If so, we use a Python code block to compute the requested tool call (lines 11 and 12).When we execute this program using the PDL interpreter, we obtain all the model inputs, followed by the model output, and finally the output of the tool call.PDL has a rich set of control structures to allow writing a variety of prompting patterns, as well as functions to support libraries.For instance, Figure 3 shows a function call on line 4.In this paper, we consider the problem of automatically tuning prompts and choosing prompting patterns for a given dataset.The following section explains our approach in further detail.</p>
<p>AutoPDL Approach</p>
<p>Figure 2 gives an overview of our approach.Referring to the numbers in the arrows:</p>
<p>(1) The input task is given by two disjoint datasets  train and  valid and a loss function L. The datasets comprise ⟨, ⟩ instances, where  is a question,  is the corresponding answer, and both are text strings.The loss function evaluates the quality of an answer .(2) The search space specification A P is a YAML file with the optimization variables and their possible values, along with some hyperparameters.For example, num_demonstrations: [0, 3,5] indicates that each candidate will have zero, three, or five ICL samples randomly drawn from  train .In the case of zero demonstrations, this is equivalent to the zero-shot baseline.If zero is an option, we bias our candidate   sampling to always include one zero-shot candidate, just in case that baseline turns out to be the best-performing configuration.</p>
<p>(3) The pattern library consists of four PDL functions.Zero-shot is a baseline that simply prompts the LLM with  and expects it to return .CoT refers to chain-of-thought (Wei et al. 2022) with in-context learning (Brown et al. 2020): the input includes a few     pairs before the actual question , and the output includes some reasoning thought tho before the actual answer .ReWOO (Xu et al. 2023) refers to reasoning without observations.Here, the few-shot samples are trajectories traj  .A trajectory traj  consists of steps for a particular example problem instance in  train , e.g., in the case of ReWOO, tho and act steps and their contents.In contrast to     pairs, traj  may contain many tho, act and, depending on pattern, obs steps, before reaching the solution   .The first LLM call in ReWOO generates one reasoning thought tho and multiple actions act  .The PDL code executes each of the actions as a tool call to obtain the corresponding observations obs  .A final model call generates the answer  based on the observations.Finally, the ReAct pattern (Yao et al. 2023) starts with few-shot trajectory examples traj  (brief example visible under Solution  *  in Figure 2) and the question , and then enters a TAO (thought, action, observation) loop.In each loop iteration, the LLM generates tho and act, then the PDL code executes the action as a tool call, and feeds the tool output back as an observation obs.A special Finish action breaks out of the loop to return the answer .Once inputs ( 1), ( 2), and (3) are in place, the successive halving optimizer runs in a loop.It starts with a small subset   ⊂  valid and many candidates C = { 1 , . . .,   } ⊆ A P sampled from the search space.Each iteration uses   to evaluate the corresponding losses ℓ 1 , . . ., ℓ  .Then each iteration keeps the  2 candidates with the smallest L while doubling the size of the validation subset   .See § 8.3 for the algorithm.(4) After the last iteration, the best remaining candidate is the solution  *  .This solution is a set of PDL definitions with concrete values for the optimization variables, e.g., in Figure 2, num_demonstrations: 5 and demonstrations: ... a list of ReAct trajectories.( 5) This program can be used on the test set  test .For instance, Figure 3 shows a call to the ReAct function that passes ${demonstrations} from  *  as an argument.</p>
<p>Methodology</p>
<p>This section describes the datasets used, the tools available to agents, and our experimental setup, including how we construct agent trajectories that demonstrate tool use for each dataset.</p>
<p>Datasets</p>
<p>We selected datasets that are widely used in the literature, span diverse tools and domains, and are representative of tool categories frequently studied in prior work (e.g., calculator, search, code execution).In our experiments, each dataset has three disjoint splits:  train to sample few-shot samples from,  valid to evaluate candidates during optimization, and  test to evaluate the final chosen solution upon completion of optimization.</p>
<p>GSM8K.The Grade School Math (Cobbe et al. 2021) dataset consists of 8,792 grade school math problems.We sample 1,024 problems without replacement from the train set to use as our  valid set, and 1,024 from the test set (consisting of 1,319 samples) to use as our  test .This leaves a  train of 6,449 problems.Each problem consists of a word problem  such as "What is fifteen more than a quarter of 48?", a sequence tho of reasoning steps, and finally a plain numeric answer  following a special delimiter.For the CoT prompt pattern, we include these reasoning steps directly in the demonstrations.We use a regular expression to extract the numerical answer from the model solution, and define a correct solution as an exact match to the ground truth answer.</p>
<p>GSM-Hard.Gao et al. (2023) introduce a derivative of GSM8K with variables randomly changed to large numbers, with 1,319 samples.Unfortunately, GSM-Hard had 132 samples where the ground truth was incorrect, and hence we excluded those samples.We split the single set into equally sized  valid and  test ( = 594), and use the GSM8K training set (6,449 samples) described above for  train (cross transfer).We use the same correctness criterion as for GSM8K.</p>
<p>FEVER.The Fact Extraction and VERification dataset (Thorne et al. 2018) is a question-answering dataset structured around fact-checking.The original dataset contained 185,445 claims that are true, false, or unverifiable, and associated with human annotated supporting, refuting, or neutral sentences and their Wikipedia article of origin.We follow the widely used derivative of this benchmark in BIG-bench (Srivastava et al. 2023), which reformulates it into a true-or-false task by removing unverifiable claims.We sample 1,024 claims from the train set as  valid and 1,024 from the test set as  test .This leaves a  train of 5,696 claims.BIG-bench also does not include the supporting or refuting sentences, which we recover by joining on the original dataset, for use e.g., as CoT demonstrations.To assess correctness, we check for the presence of "true" and "false" in the final line of the model response.If neither or both are present, we deem the response as incorrect, and otherwise the correctness is a direct match to the ground truth "true" or "false".</p>
<p>MBPP+.MBPP (Austin et al. 2021) is a dataset of 974 mostly basic Python problems, with each example consisting of a natural language problem specification  for a self-contained Python function , along with a single test case.Each problem has an extended set of test cases used for evaluation, which are not shown to the model.(2024).We do not implement ReWOO as it is not reactive, i.e., cannot include execution feedback.</p>
<p>Tools</p>
<p>Our prompt library represents actions, i.e., tool calls, following the JSON tool calling schema (Abdelaziz et al. 2024).An action is represented as a JSON object with name and arguments mapping.For example, {"action": "Calc", "arguments": {"expr": "48/4"}} represents a call to a calculator with the expression to evaluate.The PDL functions implementing the patterns (see Figure 2) accept a list of tool definitions as an argument.Each element of that list is itself a PDL function.As both the agents and tools are implemented in PDL, the set of tools for a given task could itself be made a search space dimension, which we leave to future work.</p>
<p>Calculator.For math datasets, we give the agentic approaches access to the Calc tool.This tool evaluates a cleaned (e.g., replacement of ^with * * ) expression with SymPy, returning the result.</p>
<p>In case of error, the function returns a warning that the expression was invalid, which may help the agent recover from invalid input.</p>
<p>Search.For fact verification, we provide access to the Search tool, which returns the summary of the first Wikipedia search result for the query.If no results are found, a hint to try again is returned, or if the title is too ambiguous, a list of possible disambiguations.</p>
<p>Execute.We implement a programming agent for the code generation task, which can execute arbitrary code surrounded in XML-style <execute> tags.This tool executes the code in a Python shell, which returns the result of the final expression.This allows the agent to test its proposed solution against the given test case before submitting its solution.</p>
<p>Finish.The most basic action is the Finish action, or <solution> tag for the coding agent.This ends the agent's trajectory and results in the agent returning the value as the solution.</p>
<p>Experimental Setup</p>
<p>We evaluate the efficacy of our approach by running an optimization process to completion for each model &amp; dataset pair, and subsequently comparing the task accuracy.As a baseline, we evaluate each model in a zero-shot setting.This setting reflects the minimal effort approach by a user or developer, where they do not include any demonstrations with their query or task.As no model we investigate was specifically trained to create agentic trajectories in a zero-shot setting, it is not feasible to create a zero-shot setting under ReAct or ReWOO.</p>
<p>For the optimization process, we used an initial candidate set C of 100 candidate prompt configurations   = ⟨, ⟩ ∈ C per experiment.We fixed the size of the initial validation subset  valid ⊆  valid to 16.We define L(  ,  valid ) = −Accuracy(  ,  valid ) for a given candidate   , where accuracy is the fraction in  valid of correctly solved problems by   as per the dataset definition of correctness.For each candidate   ,  = ⟨,  train , instr⟩ ∈ P where  few-shot samples or trajectories  train ∈ ( train )  are randomly sampled with replacement.The number of possible values for P is combinatorially large and depends on the dataset  train used.Finally, upon completion of an optimization process, the optimal candidate is evaluated on   .</p>
<p>Constructing Trajectories.As we are also optimizing over agentic trajectories, we also need a set of trajectories to sample few-shot samples from.To achieve this, we create a basic agentic trajectory traj  for each training example ⟨  ,   ⟩, following a rule-based transformation.We design and apply a template to each dataset, which is relatively simple and easy to implement for other datasets (details are provided in § 8.5).Prior work has introduced approaches to bootstrapping trajectories e.g., in software engineering (Pan et al. 2024), tool use demonstrations (Li et al. 2025), and reasoning paths (Zelikman et al. 2022), which could be applied to this problem.While we acknowledge the shortcomings of manual template construction, we argue this approach has two strengths: it is generalizable in the sense that templates can be mixed and matched, and that the trajectories are directly based on the datasets used.Additionally, we wanted to work with commonly used datasets that cover a variety of tools and domains, rather than emerging datasets containing trajectories or tool use demonstrations.</p>
<p>Models.We aim to study models of various abilities, e.g., natural language or code, various creators, and various sizes, ranging from single digit billions of parameters up to the edge of feasibility on consumer hardware.We include seven models available on inference service IBM watsonx3 in our study.We select three generalist natural language instruction models, Llama 3.1 8B, 3.2 3B, and 3.3 70B from the open-source and widely studied LLaMa family (Dubey et al. 2024).We further select three models from Mishra et al. (2024), which predate Dubey et al. (2024) by approximately 3 months.We select Granite 13B Instruct V2 as a generalist model, and Granite 20B and 34B Code Instruct as code models.We select Granite 3.1 8B as an additional generalist model (Granite Team 2024).All of our experiments use greedy decoding, i.e., no sampling, to limit the impact of hyperparameter choice.The number of models we evaluate is limited by cost in $/token terms and execution time.By studying various models, we demonstrate the generalizability of our approach.</p>
<p>Alternative Setups.We evaluate two alternative experimental setups.First, to investigate lowresource scenarios, we examine whether performance on one dataset can be improved by using demonstrations  train from a similar dataset, while optimizing w.r.t. valid .For this experiment, we investigate whether performance on GSM-Hard can be improved by using demonstrations from GSM8K, while optimizing w.r.t.GSM-Hard  valid .Second, to explore saving optimization costs, we assess whether optimized prompt programs of one model can transfer well to a frontier model.The intuition is that while that might not be the best program for the frontier model, it might at least improve somewhat over the baseline.To this end, we evaluate the optimized PDL programs of LLaMa 3.1 70B on OpenAI's gpt-4o-mini-2024-07-18, for each dataset.</p>
<p>Results</p>
<p>This section describes the results of our empirical study to evaluate our AutoPDL approach and answer the following research questions: RQ1 asks to what degree our AutoPDL approach can improve model performance over their zero-shot baseline across a variety of commonly used benchmarks.We also seek to identify trends, if any, in optimal configurations, e.g., whether more few-shots is always better, or whether certain prompt patterns are particularly suited to certain problem domains.</p>
<p>RQ 2: Can AutoPDL make up for a missing few-shot example bank for a given task by reusing the example bank from a similar task?RQ2 investigates whether optimizing on one dataset using demonstrations from another, related, dataset can result in higher performance than using no demonstrations (zero-shot).This RQ addresses a low-resource scenario in which a limited pool of demonstrations exists in one dataset, but a dataset from a similar domain has a large pool.</p>
<p>RQ 3: Do solutions found by AutoPDL improve performance on frontier models, even when optimized for open-source models?</p>
<p>It can be expensive to run optimization against commercial frontier-model APIs.RQ3 assesses whether optimized prompt programs are transferable to different (and likely stronger) models than those they were optimized with.</p>
<p>Table 1 reports the results of our optimization and evaluation procedure.We performed three complete optimization runs for FEVER, GSM8K, and MBPP+, and report mean accuracy, standard deviation in percentage points (i.e., absolute, not relative, uncertainty), and the pattern of the highest scoring run.Across models and datasets, we generally find some improvement over the zero-shot baseline with few-shot chain-of-thought, or agentic patterns ReAct or ReWOO.FEVER.We observed the minimum improvement in Granite 3.1 8B, with a 3.5 percentage point (pp) improvement, and a maximal improvement of 67.5pp for Granite 13B Instruct V2.</p>
<p>In terms of prompt pattern, CoT and ReWOO are equally represented.ReAct was not the optimal for any of the models.Interestingly, the largest model (LLaMa 3.3 70B) benefited by 10.5pp from 3-shot ReWOO.FEVER runtimes are generally higher than the other benchmarks, likely due to the large number of tokens involved by including Wikipedia content.</p>
<p>GSM8K.The highest improvement recorded (9.9pp) was for LLaMa 3.3 70B using 3-shot CoT, while the minimum improvement of 3.5pp was in LLaMa 3.2 3B using 3-shot CoT.ReAct and ReWOO were not the optimal for any model.For Granite 3.1 8B, Granite 20B Code, and Granite 34B Code, no improvement over the zero-shot baseline was identified.This was somewhat surprising, as generally including even some few-shot samples improves performance in LLMs.</p>
<p>MBPP+.Several models benefited from execution feedback, as 3 out of 7 had ReAct as the optimal prompt pattern (ReWOO was excluded as described in § 4.3).The greatest improvement of 12.6pp was in Granite 34B Code, and 8.5pp in Granite 13B Instruct V2, likely due to its poor programming performance as a generalist, non-code model.In contrast, the smaller LLaMa 3.1 8B model had high zero-shot performance of 61.2%, yet still improved by up to 6.2pp (1.6pp on average) with ReAct.No improvement was observed for Granite 3.1 8B, Granite 20B Code, or the other LLaMa models.Missing Few-Shot Example Bank.We optimized the PDL program for GSM-Hard, using GSM8K demonstrations, three times and report results in Table 2.We found that in most cases, GSM8K demonstrations were at least not harmful for models on GSM-Hard, with up to 6.5pp improvement for LLaMa 3.3 70B using 5-shot CoT.</p>
<p>Commercial Frontier Model.To assess whether performance gains in one model can be achieved in another, we evaluate the optimized PDL programs of LLaMa 3.1 70B on OpenAI's gpt-4o-mini-2024-07-18 and report results in Table 3 (we did not use LLaMa 3.3 70B here because we did this experiment earlier and did not have the time and resources to repeat it for the final version of this paper).For all dataset/prompt pattern pairs that resulted in improvement for LLaMa 3.1 70B, we found a surprising improvement in GPT4o-mini of at least 4pp on FEVER using 3-shot CoT, 9.3pp on GSM-Hard (using GSM8K demonstrations) with 5-shot ReAct (Granite LLaMa instructions), and up to 13.1pp on GSM8K using 5-shot CoT.This suggests that optimizing for an open-source model can also benefit a closed-source model.</p>
<p>Related Work</p>
<p>The closest related work is on prompt optimization.APE starts with an LLM-generated set of candidate prompts, then performs rejection sampling based on evaluation on a subset of data (Zhou et al. 2023).ZOPO incorporates a Neural Tangent Kernel-based derived Gaussian process into standard zeroth-order optimization for an efficient search of a locally-optimal instruction (Hu et al. 2024).Unlike our approach, neither APE nor ZOPO optimize few-shot samples.Aviary can also jointly optimize over prompt pattern, instruction, and few-shot examples (Narayanan et al. 2024).However, it would require the definition of a custom operator.CEDAR uses a demonstration pool, from which it retrieves few-shot examples at query time (Nashid et al. 2023).Unlike our approach, these few-shot samples are retrieved on a per-inference basis, not optimized ahead-of-time.EASE leverages embeddings to represent few-shot examples, and uses a neural bandit algorithm to find an ordered set that performs well for test queries from a given task (Z.Wu et al. 2024).An extension of their approach jointly optimizes demonstrations and instructions.However, the approach requires both an additional embedding model, and the training of a new model to predict validation scores from embeddings.Unlike our approach, EASE does not optimize over agentic patterns.</p>
<p>DSPy optimizes instructions and few-shot samples for a chain of LLM calls (Khattab et al. 2024) (not just a single call like APE or CEDAR).Also, DSPy takes away control over the exact prompt from the programmer, which our approach preserves.Similarly to DSPy, TextGrad also optimizes a chain of LLM calls, by using LLMs to back-propagate modifications to instructions in prompts (Yuksekgonul et al. 2025).However, unlike our approach, neither of these optimize agentic patterns.</p>
<p>BPO trains a sequence-to-sequence model on prompts augmented by an LLM incorporating human preferences, producing a model that improves given input prompts (Cheng et al. 2024).APOHF introduce a strategy to select a pair of prompts to query the user for preference feedback, which they use to optimize LLM-generated instructions on a validation set (Lin et al. 2024).However, neither of these approaches explicitly optimize demonstrations or agentic patterns.EvoAgent optimizes the instructions of a population of agents via crossover, mutation, and selection (Yuan et al. 2024).It then forms an ensemble from the final, fittest, population.GPTSwarm represents each agent as a graph, then freezes intra-agent edges and optimizes the placement of additional inter-agent edges (Zhuge et al. 2024).Unlike our approach, neither EvoAgent nor GPTSwarm optimize the agentic pattern inside individual agents, nor do they optimize few-shot samples.</p>
<p>Another closely related field of study is AutoML.Auto-sklearn (Feurer et al. 2015) used Bayesian optimization to jointly perform both algorithm selection and hyperparameters of a scikitlearn pipeline (Buitinck et al. 2013).While different, we see some analogy between algorithms and agentic patterns, and between hyperparameters and few-shot samples.DAUB first evaluates many candidate models on a small amount of data, then successively reduces candidates and increases data to ultimately pick a strong model (Sabharwal et al. 2016).The successive-halving algorithm takes a similar approach (Jamieson et al. 2016).Our approach is inspired by the same incremental data allocation idea.While both randomized search and Bayesian optimization are popular in Au-toML, there are also more intricate approaches.For instance, TPOT uses genetic algorithms (Olson et al. 2016), and AlphaD3M uses Monte-Carlo tree search (Drori et al. 2018).We chose to start with a simpler technique that depends less on a well-behaved optimization space.That said, exploring more advanced AutoML optimizers could be fruitful future work for AutoPDL.Lale (Baudart et al. 2021) treats AutoML as a source-to-source optimization, similar to this paper, but unlike AutoPDL, it has not been used to optimize agentic patterns or prompts.</p>
<p>Conclusion</p>
<p>We present our AutoPDL approach for jointly optimizing prompting patterns and textual prompts for large language models, addressing the challenges associated with manual prompt engineering.By formulating the optimization as a discrete search over both agentic and non-agentic patterns, combined with instructions and few-shot samples, we leveraged successive halving to efficiently navigate this search space.Our evaluation across various datasets (FEVER, GSM8K, GSM-Hard, and MBPP+) and multiple models (from the LLaMA, Granite, and GPT families) demonstrates substantial accuracy improvements, up to 67.5 percentage points, and affirms that no single prompting strategy universally outperforms others across tasks and models.Additionally, generating code in a YAML-based prompt programming language (PDL) makes it executable, easy to modify, and readable by humans, supporting practical adoption and adaptation.Zhuge, M., Wang, W., Kirsch, L., Faccio, F., Khizbullin, D., and Schmidhuber, J. (2024) Rupert is younger than Vincent by 2 years, so he is 22 years old -2 years = &lt;&lt;22-2=20&gt;&gt;20 years old.</p>
<p>11</p>
<p>Khloe is 10 years younger than Rupert so she is 20 years old -10 years = 10 years old.</p>
<p>12 Eugene is 3 times older than Khloe so he is 10 years old * 3 = &lt;&lt;10<em>3=30&gt;&gt;30 years old.The total length for the fifty fences is 50</em>500 = &lt;&lt;50*500=25000&gt;&gt;25000 meters.</p>
<p>22</p>
<p>If Emmalyn charged twenty cents to paint a meter of a fence, the total income she got from painting      Figure A4 describes our optimization algorithm, based on successive halving (Jamieson et al. 2016).The algorithm accepts a candidate set sampled from possible configurations and demonstrations, a validation dataset to optimize against, an initial validation subset size, and a maximum validation subset size.AutoPDL allows the user to specify these options in a YAML configuration file, and ultimately saves its result as a PDL program.This source-to-source transformation enables the user to modify both the search space and the resulting optimized PDL program, allowing further modification and execution.</p>
<p>Search Space</p>
<p>The search space is the Cartesian product of the following discrete variables, each taking one value per candidate:</p>
<p>(1)  ∈ A = {Zero-Shot, CoT, ReWOO, ReAct}, i.e., the overall prompting pattern to apply.</p>
<p>(2) Number of demonstrations  ∈ {0, 3, 5}.We selected these options as a representative sweep across no supervision, moderate few-shot use, and an upper-end case (in terms of token window).We limited the search space to three options to avoid combinatorial explosion and limit experiment cost.</p>
<p>(3) If  = ReAct, System prompt ∈ {Granite Tools, LLaMa 3, Granite LLaMa}.As the system prompt instructs the model how to format tool calls, it only has an effect on benchmarks with JSON tool calling (FEVER, GSM8K, and GSM-Hard) for candidates with the ReAct prompt pattern.We note that only for MBPP+, ReWOO is not included as a prompt pattern, and that we always include two trajectories displaying iterative refinement, i.e., an example of a solution failing the example test case, followed by a passing solution, in line with Wang et al. (2024).This effectively increases the number of trajectories to |  | + 2.</p>
<p>Agent Trajectory Construction</p>
<p>To optimize over agentic patterns and trajectories, we require a set of example trajectories to use during optimization.For this purpose, we create a basic agentic trajectory traj  for each training example ⟨  ,   ⟩, following a rule-based transformation outlined below.</p>
<p>GSM8K.</p>
<p>To demonstrate tool use in ReAct, we instead derive a trajectory traj as follows.We exploit the fact that there is at most one expression per reasoning step, by iterating through the steps.At each step, we append a 'thought' to the trajectory, consisting of the text leading up to the math expression, concatenated with a reflection 'I need to calculate'.We append a calculator tool call with the expression, and an 'observation', i.e., the result of the expression.Finally, we append a thought 'The answer is ... ', containing the ground truth answer, followed by the finish action with the answer.We follow the same procedure to create ReWOO trajectories, except we use slightly different wording, e.g., 'Calculate xyz' in place of 'I need to calculate xyz', and omit the final thought and action.Additionally, we use string substitution to replace any assumed expression results in the trajectory with the corresponding variable.</p>
<p>FEVER.To produce agent trajectories, we iterate over each article associated with a claim, append a thought 'I need to search for ... ', followed by the action, an observation containing the article summary, and finally a thought containing all the relevant sentences associated with that article for that claim, which we repeat for each article associated with a claim.This procedure is not ideal as there is no inherent order to the articles or sentences, even though there may be a natural ordering following the annotator's Wikipedia navigation.Finally, we append a thought 'The claim is true/false' and the finish action, both with the ground truth answer.For chain-of-thought, we perform the same procedure except we only include the concatenated evidence sentences, as there is no tool use.</p>
<p>Accuracy vs. iterations</p>
<p>In Figure A6, we visualize the accuracy across candidates versus the iterations of the optimization process, including a 95% confidence interval depicting the spread in accuracy across candidates.The confidence interval is computed using mean and 1,000 bootstraps.As the iterations increase, the number of candidates decreases, while the size of the validation set   increases.</p>
<dl>
<dt>Figure 1 :</dt>
<dt>1</dt>
<dt>Figure 1: Basic example of a PDL program.</dt>
<dd>
<p>Rita put a $120 elliptical machine […] -thought: The down payment Rita made was […] -action: '{"name":"Calculator","arguments":[…]' -observation: 60 -[…]</p>
</dd>
</dl>
<p>Figure 2 :
2
Figure 2: Overview of our approach.</p>
<p>11 -
11
Figure 3: Basic example of PDL program using the ReAct pattern.</p>
<p>RQ 1 :
1
To what extent does AutoPDL improve accuracy, and how much does the best solution vary by task and model?</p>
<p>Figure A1 :
A1
Figure A1: Basic example of a concrete prompt configuration in PDL, using CoT pattern and two demonstrations.</p>
<p>Figure A2 :
A2
Figure A2: Corresponding rendered prompt configuration for Figure A1.With the exception of the non-rendered ${ question } variable, this is the input to the language model.</p>
<p>Arithmetic expression to calculate 11 text: 12role: system 13 text: You are Granite, developed by IBM.You are a helpful AI assistant 14 with access to the following tools.When a tool is required to answer 15 the user's query, respond with &lt;|tool_call|&gt; followed by a JSON list of 16 tools used.If a tool does not exist in the provided list of tools, 17 notify the user that you do not have the ability to fulfill the request.</p>
<p>Figure A3: Basic example of a PDL program.</p>
<p>Figure A4 :
A4
Figure A4: Illustration of the Successive Halving algorithm used to optimize the PDL program by pruning poor candidates on progressively larger validation subsets.</p>
<p>The discrepancy in number of MBPP+ samples is due to the exclusion of samples found in the MBPP train set, to avoid data contamination.Our ReAct implementation followsWang et al.</p>
<p>Liu et al. (2023)found that MBPP test cases are incomplete, allowing proposed solutions to pass as correct, despite not matching the problem specification.Therefore, our experiments are based on MBPP+, which contains a subset of the problems, but a more complete set of test cases for each problem.We use these test cases to assess the correctness of the proposed solutions.We use the 374 examples from the original MBPP dataset as  train , and split MBPP+ into  test (224 samples) and  valid (39 samples) based on which split they were in MBPP.</p>
<p>Table 1 :
1
Model accuracies across datasets for baseline (zero-shot) and optimized versions.
DatasetModelAccuracyBest PatternRuntimeZero-Shot OptimizedDeltaGranite 3.1 8B72.9 % (76.4 ± 3.3) %+3.5pp ReWOO (5 shot)13:53Granite 13B Instruct V26.5 % (74.0 ± 1.4) % +67.5pp ReWOO (3 shot)08:28Granite 20B Code39.7 % (63.1 ± 1.6) % +23.4pp CoT (3 shot)12:03FEVERGranite 34B Code56.4 % (62.6 ± 3.8) %+6.2pp CoT (3 shot)10:07LLaMA 3.1 8B68.5 % (77.5 ± 0.8) %+9.0pp CoT (3 shot)05:06LLaMA 3.2 3B38.0 % (66.3 ± 0.9) % +28.3pp ReWOO (5 shot)10:10LLaMA 3.3 70B67.6 % (78.1 ± 0.6) % +10.5pp ReWOO (3 shot)21:27Granite 3.1 8B74.2 % (74.2 ± 0.6) %+0.0pp Zero-Shot (Baseline)08:56Granite 13B Instruct V223.0 % (30.9 ± 1.0) %+7.9pp CoT (3 shot)09:20Granite 20B Code68.7 % (68.7 ± 0.1) %+0.0pp Zero-Shot (Baseline)09:27GSM8KGranite 34B Code72.1 % (72.1 ± 0.1) %+0.0pp Zero-Shot (Baseline)08:52LLaMA 3.1 8B78.4 % (85.3 ± 0.6) %+6.9pp CoT (5 shot)08:48LLaMA 3.2 3B71.8 % (75.3 ± 0.4) %+3.5pp CoT (3 shot)16:36LLaMA 3.3 70B85.5 % (95.4 ± 0.2) %+9.9pp CoT (3 shot)07:50Granite 3.1 8B62.9 % (62.9 ± 0.0) %+0.0pp Zero-Shot (Baseline)02:14Granite 13B Instruct V210.7 % (19.2 ± 1.2) %+8.5pp ReAct (5 shot)04:02Granite 20B Code51.8 % (51.8 ± 0.4) %+0.0pp Zero-Shot (Baseline)03:43MBPP+Granite 34B Code48.7 % (61.3 ± 1.0) % +12.6pp ReAct (3 shot)04:54LLaMA 3.1 8B61.2 % (62.8 ± 4.0) %+1.6pp ReAct (5 shot)01:45LLaMA 3.2 3B58.0 % (58.0 ± 0.4) %+0.0pp Zero-Shot (Baseline)02:01LLaMA 3.3 70B71.4 % (71.4 ± 0.0) %+0.0pp Zero-Shot (Baseline)02:27</p>
<p>Table 2 :
2
Model accuracies on GSM-Hard for cross optimization experiment.
DatasetModelAccuracyBest PatternRuntimeZero-Shot OptimizedDeltaGranite 3.1 8B36.0 % (37.8 ± 0.8) % +1.8pp ReAct (5 shot, Granite Tools)23:34Granite 13B Instruct V24.4 % (6.2 ± 0.7) %+1.8pp CoT (5 shot)10:26Granite 20B Code28.8 % (27.2 ± 4.5) % +0.0pp Zero-Shot (Baseline)11:14GSM-HardGranite 34B Code27.9 % (31.0 ± 0.9) % +3.0pp CoT (3 shot)10:20LLaMA 3.2 3B26.3 % (26.8 ± 0.6) % +0.5pp CoT (5 shot)17:08LLaMA 3.3 70B47.3 % (53.8 ± 0.4) % +6.5pp CoT (5 shot)11:03</p>
<p>Table 3 :
3
Model accuracy for GPT-4o-mini cross experiment results
DatasetModelAccuracyBest PatternZero-Shot Optimized DeltaFEVERGPT-4o-mini83.7 % 87.7 %+4.0pp CoT (3 shot)GSM-Hard GPT-4o-mini45.6 % 54.9 %+9.3pp ReAct (5 shot, Granite LLaMa)GSM8KGPT-4o-mini77.8 % 90.9 % +13.1pp CoT (5 shot)MBPP+GPT-4o-mini72.3 % 72.3 %+0.0pp Zero-Shot (Baseline)</p>
<p>. "GPTSwarm: Language Agents as Optimizable Graphs".In: International Conference on Machine Learning (ICML) (cit.on p. 10).
8 Supplemental Material8.1 Concrete Prompt Example1 defs:2prompt_pattern: cot3num_demonstrations: 24demonstrations:5data:6-question: Tricia is a third of Amilia's age and Amilia is a quarter of Yorick's age. Yorick is twice7Eugene's age and Khloe is a third of Eugene's age. Rupert is 10 years older than Khloe but 2 years8younger than Vincent who is 22 years old. How old, in years, is Tricia?9reasoning: |-10</p>
<p>13</p>
<p>Yorick is twice Eugene's age so he is 30 years old * 2 = &lt;&lt;30*2=60&gt;&gt;60 years old.Emmalyn decided to paint fences in her neighborhood for twenty cents per meter.If there were 18 50 fences in the neighborhood that she had to paint and each fence was 500 meters long, calculate
14Amilia is a quarter of Yorick's age so she is 60 years old / 4 = &lt;&lt;60/4=15&gt;&gt;15 years old.15Tricia is a third of Amilia's age so she is 15 years old / 3 = &lt;&lt;15/3=5&gt;&gt;5 years old.16answer: '5'21
17question:19 the total amount she earned from painting the fences.20 reasoning: |-</p>
<p>Program candidate set C, validation dataset  valid , initial validation subset size  min , maximum validation subset size  max 1:  ←  min 2: while |C| &gt; 1 do  valid ← first  elements of  valid s.t. valid ⊆  valid and | valid | =   ← L(  ,  valid )
See Figure A3.8.3 OptimizationAlgorithm 1 Successive Halving for PDL OptimizationRequire: 3:4:for each candidate 𝑐 𝑖 ∈ C do5: Compute loss ℓ 6: end for7:C ← top ⌊|C|/2⌋ candidates with lowest loss8:𝑣 ← min(𝑣 max , 2 • 𝑣)9: end while
10: return Candidate in C with lowest loss</p>
<p>https://github.com/BerriAI/litellm
PDL additionally makes use of the heuristic json-repair package.
https://www.ibm.com/watsonx
MBPP+.To generate sample agent trajectories from the training set, we follow the agent pattern (without feedback) in-context examples byWang et al. (2024), which consists of the problem , a thought such as "The intersection is elements that are in both lists", an execute action that contains proposed code and an assertion calling the proposed method with the test case input from the prompt and comparing its output.This is then followed by an observation containing the execution result, i.e., either "[Executed Successfully with No Output]" or a stack traceback.This allows the agent to iterate on solutions (up to five times in our implementation).We use the full MBPP train set of 374 problems as  train , and split the MBPP+ dataset into  valid and  test based on problem id membership in MBPP, leaving 39 and 224 validation and test problems respectively.To generate synthetic trajectories from the training set, we start with the natural language specification and single test case (the prompt), append the thought "I should run a solution on the test case before proposing a solution.", followed by the ground truth solution and substitute in the prompt test case following the pattern [solution]res = ...; assert res == ..., "Expected ... but got ".format(res).Subsequently, we append the observation "[Executed Successfully with No Output]", the thought "There is no more AssertionError.I can now submit the solution.",and finally the solution action with the ground truth solution.This naive approach allows us to provide demonstration trajectories, albeit simplistic ones that assume the first solution is correct.Sampling a reflection or thought from a strong model may be beneficial(Li et al. 2025), but we restrict our trajectories to rule based transformations.As ReWOO is not reactive, i.e., without execution feedback, it does not make sense for MBPP.Hence, we exclude it from our experiments.In FigureA5, we visualize the results from Table1 and Table 2.
Granite-Function Calling Model: Introducing Function Calling Abilities via Multi-task Learning of Granular Tasks. I Abdelaziz, K Basu, M Agarwal, S Kumaravel, M Stallone, R Panda, Y Rizk, G Bhargav, M Crouse, C Gunasekara, S Ikbal, S Joshi, H Karanam, V Kumar, A Munawar, S Neelam, D Raghu, U Sharma, A M Soria, D Sreedhar, P Venkateswaran, M Unuvar, D Cox, S Roukos, L Lastras, P Kapanipathi, 20246</p>
<p>Program Synthesis with Large Language Models. J Austin, A Odena, M Nye, M Bosma, H Michalewski, D Dohan, E Jiang, C Cai, M Terry, Q Le, C Sutton, 20215</p>
<p>Pipeline Combinators for Gradual AutoML. G Baudart, M Hirzel, K Kate, P Ram, A Shinnar, J Tsay, Advances in Neural Information Processing Systems (NeurIPS). 202110</p>
<p>T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Language Models are Few-Shot Learners. 2020</p>
<p>L Buitinck, G Louppe, M Blondel, F Pedregosa, A Mueller, O Grisel, V Niculae, P Prettenhofer, A Gramfort, J Grobler, R Layton, J Vanderplas, A Joly, B Holt, G Varoquaux, API Design for Machine Learning Software: Experiences from the scikit-learn Project. 201310</p>
<p>Black-Box Prompt Optimization: Aligning Large Language Models without Model Training. J Cheng, X Liu, K Zheng, P Ke, H Wang, Y Dong, J Tang, M Huang, Annual Meeting of the Association for Computational Linguistics (ACL). 202410</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, Training Verifiers to Solve Math Word Problems. 20215</p>
<p>AlphaD3M: Machine Learning Pipeline Synthesis. I Drori, Y Krishnamurthy, R Rampin, R D P Lourenco, J P Ono, K Cho, C Silva, J Freire, Workshop on Automatic Machine Learning (AutoML). 201810</p>
<p>The Llama 3 Herd of Models. A Dubey, 20247</p>
<p>Efficient and Robust Automated Machine Learning. M Feurer, A Klein, K Eggensperger, J Springenberg, M Blum, F Hutter, Conference on Neural Information Processing Systems (NIPS). 201510</p>
<p>PAL: Program-aided Language Models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, International Conference on Machine Learning (ICML). 20235</p>
<p>Granite 3.0 Language Models (cit. Granite Team, I , 20247</p>
<p>Localized Zeroth-Order Prompt Optimization. W Hu, Y Shu, Z Yu, Z Wu, X Lin, Z Dai, S.-K Ng, B K H Low, Conference on Neural Information Processing Systems (NeurIPS). 20249</p>
<p>Non-stochastic Best Arm Identification and Hyperparameter Optimization. K Jamieson, A Talwalkar, Conference on Artificial Intelligence and Statistics (AISTATS). 20161710</p>
<p>DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. O Khattab, A Singhvi, P Maheshwari, Z Zhang, K Santhanam, A , S V Haq, S Sharma, A Joshi, T T Moazam, H Miller, H Zaharia, M Potts, C , International Conference on Learning Representations (ICLR). 2024cit. on pp. 1, 10</p>
<p>Large Language Models Are Zero-Shot Reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Conference on Neural Information Processing Systems (NeurIPS). 20221</p>
<p>C Li, M Xue, Z Zhang, J Yang, B Zhang, X Wang, B Yu, B Hui, J Lin, D Liu, START: Self-taught Reasoner with Tools. 2025186</p>
<p>Prompt Optimization with Human Feedback. X Lin, Z Dai, A Verma, S.-K Ng, P Jaillet, B K H Low, 202410</p>
<p>Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. J Liu, C S Xia, Y Wang, L Zhang, Conference on Neural Information Processing Systems (NeurIPS). 20235</p>
<p>Granite Code Models: A Family of Open Foundation Models for Code Intelligence. M Mishra, M Stallone, G Zhang, Y Shen, A Prasad, A M Soria, M Merler, P Selvam, S Surendran, S Singh, M Sethi, X.-H Dang, P Li, K.-L Wu, S Zawad, A Coleman, M White, M Lewis, R Pavuluri, Y Koyfman, B Lublinsky, M De Bayser, I Abdelaziz, K Basu, M Agarwal, Y Zhou, C Johnson, A Goyal, H Patel, Y Shah, P Zerfos, H Ludwig, A Munawar, M Crouse, P Kapanipathi, S Salaria, B Calio, S Wen, S Seelam, B Belgodere, C Fonseca, A Singhee, N Desai, D D Cox, R Puri, R Panda, 7, 20247</p>
<p>CrewAI: Framework for orchestrating role-playing, autonomous AI agents. J Moura, 20231visited on 06/06/2025</p>
<p>S Narayanan, J D Braza, R.-R Griffiths, M Ponnapati, A Bou, J Laurent, O Kabeli, G Wellawatte, S Cox, S G Rodriques, A D White, Aviary: Training Language Agents on Challenging Scientific Tasks. 20249</p>
<p>Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning. N Nashid, M Sintaha, A Mesbah, International Conference on Software Engineering (ICSE). 20239</p>
<p>Automating Biomedical Data Science Through Tree-Based Pipeline Optimization. R S Olson, R J Urbanowicz, P C Andrews, N A Lavender, L C Kidd, J H Moore, European Conference on the Applications of Evolutionary Computation (EvoApplications). 201610</p>
<p>Training Software Engineering Agents and Verifiers with SWE-Gym. J Pan, X Wang, G Neubig, N Jaitly, H Ji, A Suhr, Y Zhang, 20246</p>
<p>Selecting Near-Optimal Learners via Incremental Data Allocation. A Sabharwal, H Samulowitz, G Tesauro, Conference on Artificial Intelligence (AAAI). 201610</p>
<p>Building effective agents. E Schluntz, B Zhang, 20241visited on 06/06/2025</p>
<p>Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models. A Srivastava, Transactions on Machine Learning Research (TMLR) (cit. 20235</p>
<p>FEVER: A Large-Scale Dataset for Fact Extraction and VERification. J Thorne, A Vlachos, C Christodoulopoulos, A Mittal, 20185</p>
<p>Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms. C Thornton, F Hutter, H H Hoos, K Leyton-Brown, Conference on Knowledge Discovery and Data Mining (KDD). 20131</p>
<p>PDL: A Declarative Prompt Programming Language. M Vaziri, L Mandel, C Spiess, M Hirzel, 2024</p>
<p>MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. X Wang, Z Wang, J Liu, Y Chen, L Yuan, H Peng, H Ji, 20241817</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, Conference on Neural Information Processing Systems (NeurIPS). 2022cit. on pp. 1, 4)</p>
<p>Efficient Guided Generation for Large Language Models. B T Willard, R Louf, 20233</p>
<p>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. Q Wu, G Bansal, J Zhang, Y Wu, B Li, E Zhu, L Jiang, X Zhang, S Zhang, J Liu, A H Awadallah, R W White, D Burger, C Wang, 20231</p>
<p>Prompt Optimization with EASE? Efficient Ordering-aware Automated Selection of Exemplars. Z Wu, X Lin, Z Dai, W Hu, Y Shu, S.-K Ng, P Jaillet, B K H Low, Conference on Neural Information Processing Systems (NeurIPS). 20249</p>
<p>ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models. B Xu, Z Peng, B Lei, S Mukherjee, D Xu, 2023</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. S Yao, J Zhao, D Yu, N Du, I Shafran, K R Narasimhan, Y Cao, International Conference on Learning Representations (ICLR). 2023cit. on pp. 1, 4)</p>
<p>EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms. S Yuan, K Song, J Chen, X Tan, D Li, D Yang, 202410</p>
<p>Optimizing Generative AI by Backpropagating Language Model Feedback. M Yuksekgonul, F Bianchi, J Boen, S Liu, P Lu, Z Huang, C Guestrin, J Zou, Nature. 6398055102025</p>
<p>STaR: Self-Taught Reasoner, Bootstrapping Reasoning With Reasoning. E Zelikman, Y Wu, J Mu, N D Goodman, Conference on Neural Information Processing Systems (NeurIPS). 20227</p>
<p>Large Language Models are Human-Level Prompt Engineers. Y Zhou, A I Muresanu, Z Han, K Paster, S Pitis, H Chan, J Ba, International Conference on Learning Representations (ICLR. 20239</p>
<p>Answer the questions to the best of your abilities. </p>
<p>Question: Tricia is a third of Amilia's age and Amilia is a quarter of Yorick's age. Yorick is twice Eugene's 4 age and Khloe is a third of Eugene's age. Rupert is 10 years older than Khloe but 2 years 5 younger than Vincent 6 who is 22 years old. How old, in years, is Tricia? 7 Answer: Let's think step by step. Rupert is younger than Vincent by 2 years. so he is 22 years 8 old -2 years = &lt;&lt;22-2=20&gt;&gt;20 years old</p>
<p>Khloe is 10 years younger than Rupert so she is 20 years old -10 years = 10 years old. </p>
<p>Eugene is 3 times older than Khloe so he is 10 years old * 3 = &lt;&lt;10*3=30&gt;&gt;30 years old. </p>
<p>Yorick is twice Eugene's age so he is 30 years old * 2 = &lt;&lt;30*2=60&gt;&gt;60 years old. </p>
<p>Amilia is a quarter of Yorick's age so she is 60 years old / 4 = &lt;&lt;60/4=15&gt;&gt;15 years old. </p>
<p>Tricia is a third of Amilia's age so she is 15 years old / 3 = &lt;&lt;15/3=5&gt;&gt;5 years old. 14 The answer is 5 15 16 Question: Emmalyn decided to paint fences in her neighborhood for twenty cents per meter. If there were 50 17 fences in the neighborhood that she had to paint and each fence was 500 meters long. calculate the total 18 amount she earned from painting the fences</p>
<p>Let's think step by step. The total length for the fifty fences is 50<em>500 = &lt;&lt;50</em>500=25000&gt;&gt;25000 meters. Answer, </p>
<p>If Emmalyn charged twenty cents to paint a meter of a fence, the total income she got from painting the 21 fences is $0.20*25000 =$5000 22 The answer is 5000 23 24 Question: ${ question } 25 Answer: Let's think step by step. </p>            </div>
        </div>

    </div>
</body>
</html>
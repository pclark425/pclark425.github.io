<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7237 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7237</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7237</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-272692262</p>
                <p><strong>Paper Title:</strong> The Two Word Test as a semantic benchmark for large language models</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown remarkable abilities recently, including passing advanced professional exams and demanding benchmark tests. This performance has led many to suggest that they are close to achieving humanlike or “true” understanding of language, and even artificial general intelligence (AGI). Here, we provide a new open-source benchmark, the Two Word Test (TWT), that can assess semantic abilities of LLMs using two-word phrases in a task that can be performed relatively easily by humans without advanced training. Combining multiple words into a single concept is a fundamental linguistic and conceptual operation routinely performed by people. The test requires meaningfulness judgments of 1768 noun-noun combinations that have been rated as meaningful (e.g., baby boy) or as having low meaningfulness (e.g., goat sky) by human raters. This novel test differs from existing benchmarks that rely on logical reasoning, inference, puzzle-solving, or domain expertise. We provide versions of the task that probe meaningfulness ratings on a 0–4 scale as well as binary judgments. With both versions, we conducted a series of experiments using the TWT on GPT-4, GPT-3.5, Claude-3-Optus, and Gemini-1-Pro-001. Results demonstrated that, compared to humans, all models performed relatively poorly at rating meaningfulness of these phrases. GPT-3.5-turbo, Gemini-1.0-Pro-001 and GPT-4-turbo were also unable to make binary discriminations between sensible and nonsense phrases, with these models consistently judging nonsensical phrases as making sense. Claude-3-Opus made a substantial improvement in binary discrimination of combinatorial phrases but was still significantly worse than human performance. The TWT can be used to understand and assess the limitations of current LLMs, and potentially improve them. The test also reminds us that caution is warranted in attributing “true” or human-level understanding to LLMs based only on tests that are challenging for humans.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7237.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7237.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (TWT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5-turbo (Two Word Test evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5-turbo was evaluated on the Two Word Test (TWT), a psycholinguistic semantic judgment task requiring 0–4 meaningfulness ratings for 1,768 noun-noun phrases; the model showed poor discrimination and a liberal bias toward judging phrases as meaningful.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI family generative pre-trained transformer (decoder-only transformer architecture) used via API (March 2024 availability).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Two Word Test (TWT) — numerical meaningfulness judgments</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>semantic composition / semantic judgment</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Participants (humans or models) judge the meaningfulness of noun-noun phrases on a 0–4 Likert scale (0 = makes no sense, 4 = makes complete sense); 1,768 phrases with human baseline ratings were used.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Mean rating (0–4) per phrase; phrase-wise Crawford & Howell single-case t-test against simulated human distribution; failure counts (p < 0.05 threshold).</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human baseline from Graves et al. (N=150): distribution bimodal with 977 phrases recoded as 'makes sense' (mean > 2.5) and 761 as 'nonsense' (mean < 1.5); typical RT ~1 s in related tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3.5 tended to rate most phrases as 2–3 (makes some / a lot of sense); failure counts for GPT-3.5 were close to a chance/permuted distribution and substantially worse than simulated human distribution (many phrase-wise ratings significantly different from humans, p < 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-based prompting using Graves et al. original instructions plus 4 examples (added examples for 1 and 3 ratings), phrases provided in randomized subsets; 10 shuffled iterations averaged.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Graves et al., 2013: 'Noun-noun combination: Meaningfulness ratings and lexical statistics for 2,160 word pairs' (behavioral dataset used as human baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Phrase-wise single-case Crawford & Howell t-test: TWT failure defined as LLM rating with p < 0.05 against simulated human distribution; GPT-3.5 produced a large number of such failures (close to chance-level failure counts). Correlations: GPT-3.5 Log_Gfreq Spearman rho = 0.13 (p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Error pattern: liberal bias, over-reporting meaningfulness; lacked the human-like bimodal distribution of ratings. GPT-3.5's meaningfulness judgments correlate much less with bigram frequency and semantic similarity than other LLMs/humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Two Word Test as a semantic benchmark for large language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7237.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7237.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (TWT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4-turbo (Two Word Test evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4-turbo was evaluated on the TWT numerical task; it produced a more bimodal distribution than some other models but still produced many phrase-wise judgments statistically different from humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI generative pre-trained transformer (next-generation model in the GPT family; transformer-based), evaluated via API (March 2024 availability).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Two Word Test (TWT) — numerical meaningfulness judgments</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>semantic composition / semantic judgment</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>0–4 Likert meaningfulness ratings on 1,768 noun-noun phrases; comparisons made phrase-wise to human rating distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Mean rating (0–4) per phrase; phrase-wise Crawford & Howell single-case t-test against simulated human distribution; failure counts.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Same human baseline: 977 'makes sense' vs. 761 'nonsense' phrases; human rating distribution bimodal.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4-turbo showed bimodal peaks more similar to humans than some other models but still had a considerable number of ambiguous ratings (~2) and many phrase-wise failures (ratings significantly different from humans, p < 0.05); overall failure counts closer to chance than to simulated humans.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Original Graves instructions with added examples (to encourage use of full 0–4 scale), phrases presented in randomized subsets, 10 shuffled iterations and average taken.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Graves et al., 2013 dataset (human meaningfulness ratings).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Phrase-wise Crawford & Howell t-tests used; many GPT-4 ratings fell outside the 95% simulated human interval (p < 0.05). Correlations: GPT-4 Log_Gfreq Spearman rho = 0.67 (p < 0.001); semantic similarity correlations (GloVe rho = 0.38, p < 0.001; Word2Vec rho = 0.37, p < 0.001; Taxonomic rho = 0.21, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4 better than GPT-3.5 in numeric TWT distribution shape but still substantially different from humans; more likely than humans to rate semantically similar word pairs as meaningful (higher correlation with word-to-word semantic similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Two Word Test as a semantic benchmark for large language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7237.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7237.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini (TWT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Gemini-1.0-Pro-001 (Two Word Test evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gemini-1.0-Pro-001 was evaluated on the TWT numerical meaningfulness task and showed a central tendency (many ratings around 2–3) and failure counts nearer to chance than to human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.0-Pro-001</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google's Gemini family transformer-based large language model (pro variant; evaluated March 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Two Word Test (TWT) — numerical meaningfulness judgments</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>semantic composition / semantic judgment</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>0–4 Likert scale meaningfulness ratings of noun-noun phrases; comparison of model item ratings to human distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Mean rating per phrase (0–4); phrase-wise Crawford & Howell t-test; failure counts reported relative to simulated human and chance distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Graves et al. human baseline (N=150) with 977 'makes sense' and 761 'nonsense' items; bimodal human rating distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Gemini showed bias toward mid-scale ratings (2–3), producing many phrase-wise failures; failure counts were closer to chance than to the simulated human distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction prompt adapted from Graves et al. with additional examples for scale endpoints 1 and 3; 10 randomized shuffles of phrase subsets and averaged outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Graves et al., 2013 dataset (noun-noun meaningfulness ratings).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Many phrase-wise Crawford & Howell t-tests yielded p < 0.05 (defined as TWT failure); Gemini's failure counts similar to chance distribution. Correlation with Log_Gfreq rho = 0.63 (p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Gemini tends to cluster ratings centrally (lack of clear bimodality seen in humans); shows increased reliance on bigram frequency and semantic similarity relative to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Two Word Test as a semantic benchmark for large language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7237.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7237.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3-Opus (TWT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude-3-Opus (Two Word Test evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude-3-Opus performed the TWT numerical task and produced bimodal rating peaks more like humans and substantially fewer failures than other tested LLMs, but still significantly more failures than human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3-Opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic transformer-based large language model (Claude family, Opus variant), evaluated via API (March 2024 availability).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Two Word Test (TWT) — numerical meaningfulness judgments</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>semantic composition / semantic judgment</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>0–4 Likert meaningfulness ratings for noun-noun phrases compared item-wise to human rating distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Mean rating (0–4); phrase-wise Crawford & Howell t-tests; counts of items where model rating has <5% probability under simulated human distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human baseline from Graves et al. (N=150), 977 'makes sense' vs. 761 'nonsense'; bimodal distributions reflect clear human consensus on many items.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Claude-3-Opus showed bimodal peaks more similar to humans and substantially fewer TWT failures than GPT-3.5, GPT-4, and Gemini, but still produced significantly more failures than expected from human raters (i.e., not matching human-level performance).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Same instruction prompt (Graves et al.) augmented with examples for all rating values, phrases supplied in randomized subsets, repeated over 10 shuffled iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Graves et al., 2013: noun-noun meaningfulness ratings dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Claude had significantly fewer TWT failures than other LLMs but still significantly more than human simulated distribution (phrase-wise test threshold p < 0.05). Correlations: Log_Gfreq rho = 0.66 (p < 0.001); semantic similarity (GloVe rho = 0.37 p < 0.001; Word2Vec rho = 0.38 p < 0.001; Taxonomic rho = 0.21 p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Best-performing model on numerical TWT among those tested, but still falls short of human consistency; like other LLMs, shows stronger coupling of meaningfulness ratings to word-to-word semantic similarity than humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Two Word Test as a semantic benchmark for large language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7237.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7237.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (bTWT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5-turbo (binary Two Word Test evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5-turbo was evaluated on a binary version of the TWT (bTWT) and displayed poor discrimination, with an excessively liberal bias (high false-alarm rate, tending to classify most phrases as 'makes sense').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI transformer-based LLM (decoder-only), evaluated zero/few-shot with instruction examples.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>binary Two Word Test (bTWT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>semantic composition / decision-making (binary)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Binary judgment for each noun-noun phrase: 'makes sense' vs 'nonsense' (human numerical ratings recoded to binary for comparison; 977 meaningful, 761 nonsense).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Signal detection theory metrics (d', β), hit/false-alarm rates, ROC/AUC, and χ² test comparing response frequency to human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human baseline recoded to binary: 977 'makes sense' items, 761 'nonsense' items (from Graves et al., 2013). Human discrimination is high (bimodal agreement across items).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3.5 displayed poor discrimination (low d'), high hit rate but also very high false-alarm rate (liberal β, log10(β) < 0), ROC/AUC near chance relative to humans; χ² test shows LLM response frequencies significantly different from human frequencies (p < 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction prompt adapted for binary responses with examples (four example items), randomized phrase submissions, multiple iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Graves et al., 2013 (human meaningfulness ratings recoded to binary for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>χ² tests reported significant differences between GPT-3.5 response frequencies and human expected frequencies (p < 0.05). SDT metrics indicate near-chance discrimination (d' approximately ≈ 0 or low; exact numeric d' not provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Behavioral signature: excessively liberal decision criterion, treating many nonsense phrases as meaningful; suggests failure not solely due to numeric rating format (since binary task also fails).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Two Word Test as a semantic benchmark for large language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7237.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7237.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (bTWT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4-turbo (binary Two Word Test evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4-turbo showed modest discrimination on the bTWT: better than GPT-3.5 but still with a nontrivial tendency to label nonsense phrases as 'makes sense'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI transformer-based LLM (GPT-4 family), evaluated with instruction prompts and examples.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>binary Two Word Test (bTWT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>semantic composition / decision-making (binary)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Binary judgments of noun-noun phrase meaningfulness compared to human recoded binary labels (977 meaningful; 761 nonsense).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Signal detection theory metrics (d', β), ROC/AUC, hit/false-alarm rates, and χ² test against human frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human baseline: 977 meaningful and 761 nonsense phrases (Graves et al., 2013).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 achieved modest discrimination (intermediate d'), able to detect meaningfulness in some items but with a moderate false-alarm rate (labeling nonsense phrases as 'makes sense' more often than humans). χ² indicates response frequencies significantly differ from humans (p < 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Binary version of Graves instructions with examples; phrases randomized and submitted across 10 iterations; averaged outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Graves et al., 2013 dataset (binary recoding of human ratings).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>χ² test in Table 3 indicates significant differences vs human frequencies (p < 0.05). SDT metrics show d' moderate but lower than human-level discrimination; exact numeric d' reported in paper tables (not included verbatim in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4 is better than GPT-3.5 on bTWT but still systematically worse than humans; error profile includes mistakenly accepting semantically related but non-combinatorial pairs as meaningful.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Two Word Test as a semantic benchmark for large language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7237.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7237.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini (bTWT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Gemini-1.0-Pro-001 (binary Two Word Test evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gemini-1.0-Pro-001 on bTWT produced modest discrimination (similar to GPT-4) with a moderate tendency to classify nonsense phrases as meaningful.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.0-Pro-001</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google Gemini transformer-based LLM (pro variant), assessed via instruction prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>binary Two Word Test (bTWT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>semantic composition / decision-making (binary)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Binary 'makes sense' vs 'nonsense' judgments for 1,768 noun-noun phrases; compared to human recoded binary labels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Signal detection theory metrics (d', β), ROC/AUC, hit/false-alarm rates, χ² test against human frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human baseline recoded binary: 977 meaningful items and 761 nonsense items (from Graves et al., 2013).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Gemini showed modest discrimination (intermediate d'), capable of some correct differentiation but with a moderate false-alarm rate (labeling nonsense as meaningful). χ² indicates distribution of responses significantly different from human baseline (p < 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Binary instructions with examples (four exemplars), randomized presentation of phrases across multiple submissions and averaged over iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Graves et al., 2013 (human ratings recoded to binary).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>SDT and χ² analyses reported significant differences from human expected frequencies (χ² p < 0.05). Correlation with Log_Gfreq rho = 0.63 (p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Gemini tends toward mid-scale/more liberal responses in numerical TWT and shows a moderate tendency to accept nonsense phrases in binary format; similar error profile to GPT-4 but not as conservative or human-like as Claude-3-Opus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Two Word Test as a semantic benchmark for large language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7237.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7237.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3-Opus (bTWT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude-3-Opus (binary Two Word Test evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude-3-Opus achieved the best binary discrimination (bTWT) among the tested models: moderate-to-high d' and better correct rejection rates, but still significantly different from human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3-Opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic transformer-based large language model (Claude family, Opus variant), evaluated using instruction prompts with examples.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>binary Two Word Test (bTWT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>semantic composition / decision-making (binary)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Binary judgment task ('makes sense' vs 'nonsense') on 1,768 noun-noun phrases; compared to human binary labels derived from Graves et al. ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Signal detection theory: d', β, ROC/AUC; hit/false-alarm rates; χ² test comparing LLM response frequencies to human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human baseline: 977 items labeled 'makes sense' and 761 'nonsense' (Graves et al., 2013).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Claude-3-Opus displayed substantially improved discrimination relative to other LLMs (higher correct rejection, lower false-alarm rate), moderate-to-high d' and larger AUC, but still significantly different from human performance per χ² and SDT measures.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Binary instructions with multiple examples, randomized phrase presentation, and repeated submissions averaged across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Graves et al., 2013 (human meaningfulness ratings recoded to binary labels).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>χ² tests show Claude's response frequency distribution is significantly different from humans (p < 0.05), though Claude's SDT metrics (d', AUC) were highest among tested LLMs. Semantic similarity correlations for Claude: GloVe rho = 0.37 (p < 0.001), Word2Vec rho = 0.38 (p < 0.001), Taxonomic rho = 0.21 (p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Claude-3-Opus is the best of the four models on both numerical and binary TWT but still fails to match human-level discrimination; like other models, it relies more on semantic similarity and phrase frequency than humans when making meaningfulness judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Two Word Test as a semantic benchmark for large language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Noun-noun combination: Meaningfulness ratings and lexical statistics for 2,160 word pairs <em>(Rating: 2)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with GPT-4 <em>(Rating: 1)</em></li>
                <li>Faith and Fate: Limits of Transformers on Compositionality <em>(Rating: 2)</em></li>
                <li>Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models <em>(Rating: 1)</em></li>
                <li>Machine Reading, Fast and Slow: When Do Models 'Understand' Language? <em>(Rating: 1)</em></li>
                <li>Is ChatGPT a General-Purpose Natural Language Processing Task Solver <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7237",
    "paper_id": "paper-272692262",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "GPT-3.5 (TWT)",
            "name_full": "OpenAI GPT-3.5-turbo (Two Word Test evaluation)",
            "brief_description": "GPT-3.5-turbo was evaluated on the Two Word Test (TWT), a psycholinguistic semantic judgment task requiring 0–4 meaningfulness ratings for 1,768 noun-noun phrases; the model showed poor discrimination and a liberal bias toward judging phrases as meaningful.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "OpenAI family generative pre-trained transformer (decoder-only transformer architecture) used via API (March 2024 availability).",
            "model_size": null,
            "test_name": "Two Word Test (TWT) — numerical meaningfulness judgments",
            "test_category": "semantic composition / semantic judgment",
            "test_description": "Participants (humans or models) judge the meaningfulness of noun-noun phrases on a 0–4 Likert scale (0 = makes no sense, 4 = makes complete sense); 1,768 phrases with human baseline ratings were used.",
            "evaluation_metric": "Mean rating (0–4) per phrase; phrase-wise Crawford & Howell single-case t-test against simulated human distribution; failure counts (p &lt; 0.05 threshold).",
            "human_performance": "Human baseline from Graves et al. (N=150): distribution bimodal with 977 phrases recoded as 'makes sense' (mean &gt; 2.5) and 761 as 'nonsense' (mean &lt; 1.5); typical RT ~1 s in related tasks.",
            "llm_performance": "GPT-3.5 tended to rate most phrases as 2–3 (makes some / a lot of sense); failure counts for GPT-3.5 were close to a chance/permuted distribution and substantially worse than simulated human distribution (many phrase-wise ratings significantly different from humans, p &lt; 0.05).",
            "prompting_method": "Instruction-based prompting using Graves et al. original instructions plus 4 examples (added examples for 1 and 3 ratings), phrases provided in randomized subsets; 10 shuffled iterations averaged.",
            "fine_tuned": false,
            "human_data_source": "Graves et al., 2013: 'Noun-noun combination: Meaningfulness ratings and lexical statistics for 2,160 word pairs' (behavioral dataset used as human baseline).",
            "statistical_significance": "Phrase-wise single-case Crawford & Howell t-test: TWT failure defined as LLM rating with p &lt; 0.05 against simulated human distribution; GPT-3.5 produced a large number of such failures (close to chance-level failure counts). Correlations: GPT-3.5 Log_Gfreq Spearman rho = 0.13 (p &lt; 0.001).",
            "notes": "Error pattern: liberal bias, over-reporting meaningfulness; lacked the human-like bimodal distribution of ratings. GPT-3.5's meaningfulness judgments correlate much less with bigram frequency and semantic similarity than other LLMs/humans.",
            "uuid": "e7237.0",
            "source_info": {
                "paper_title": "The Two Word Test as a semantic benchmark for large language models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-4 (TWT)",
            "name_full": "OpenAI GPT-4-turbo (Two Word Test evaluation)",
            "brief_description": "GPT-4-turbo was evaluated on the TWT numerical task; it produced a more bimodal distribution than some other models but still produced many phrase-wise judgments statistically different from humans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4-turbo",
            "model_description": "OpenAI generative pre-trained transformer (next-generation model in the GPT family; transformer-based), evaluated via API (March 2024 availability).",
            "model_size": null,
            "test_name": "Two Word Test (TWT) — numerical meaningfulness judgments",
            "test_category": "semantic composition / semantic judgment",
            "test_description": "0–4 Likert meaningfulness ratings on 1,768 noun-noun phrases; comparisons made phrase-wise to human rating distributions.",
            "evaluation_metric": "Mean rating (0–4) per phrase; phrase-wise Crawford & Howell single-case t-test against simulated human distribution; failure counts.",
            "human_performance": "Same human baseline: 977 'makes sense' vs. 761 'nonsense' phrases; human rating distribution bimodal.",
            "llm_performance": "GPT-4-turbo showed bimodal peaks more similar to humans than some other models but still had a considerable number of ambiguous ratings (~2) and many phrase-wise failures (ratings significantly different from humans, p &lt; 0.05); overall failure counts closer to chance than to simulated humans.",
            "prompting_method": "Original Graves instructions with added examples (to encourage use of full 0–4 scale), phrases presented in randomized subsets, 10 shuffled iterations and average taken.",
            "fine_tuned": false,
            "human_data_source": "Graves et al., 2013 dataset (human meaningfulness ratings).",
            "statistical_significance": "Phrase-wise Crawford & Howell t-tests used; many GPT-4 ratings fell outside the 95% simulated human interval (p &lt; 0.05). Correlations: GPT-4 Log_Gfreq Spearman rho = 0.67 (p &lt; 0.001); semantic similarity correlations (GloVe rho = 0.38, p &lt; 0.001; Word2Vec rho = 0.37, p &lt; 0.001; Taxonomic rho = 0.21, p &lt; 0.001).",
            "notes": "GPT-4 better than GPT-3.5 in numeric TWT distribution shape but still substantially different from humans; more likely than humans to rate semantically similar word pairs as meaningful (higher correlation with word-to-word semantic similarity).",
            "uuid": "e7237.1",
            "source_info": {
                "paper_title": "The Two Word Test as a semantic benchmark for large language models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Gemini (TWT)",
            "name_full": "Google Gemini-1.0-Pro-001 (Two Word Test evaluation)",
            "brief_description": "Gemini-1.0-Pro-001 was evaluated on the TWT numerical meaningfulness task and showed a central tendency (many ratings around 2–3) and failure counts nearer to chance than to human performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini-1.0-Pro-001",
            "model_description": "Google's Gemini family transformer-based large language model (pro variant; evaluated March 2024).",
            "model_size": null,
            "test_name": "Two Word Test (TWT) — numerical meaningfulness judgments",
            "test_category": "semantic composition / semantic judgment",
            "test_description": "0–4 Likert scale meaningfulness ratings of noun-noun phrases; comparison of model item ratings to human distributions.",
            "evaluation_metric": "Mean rating per phrase (0–4); phrase-wise Crawford & Howell t-test; failure counts reported relative to simulated human and chance distributions.",
            "human_performance": "Graves et al. human baseline (N=150) with 977 'makes sense' and 761 'nonsense' items; bimodal human rating distribution.",
            "llm_performance": "Gemini showed bias toward mid-scale ratings (2–3), producing many phrase-wise failures; failure counts were closer to chance than to the simulated human distribution.",
            "prompting_method": "Instruction prompt adapted from Graves et al. with additional examples for scale endpoints 1 and 3; 10 randomized shuffles of phrase subsets and averaged outputs.",
            "fine_tuned": false,
            "human_data_source": "Graves et al., 2013 dataset (noun-noun meaningfulness ratings).",
            "statistical_significance": "Many phrase-wise Crawford & Howell t-tests yielded p &lt; 0.05 (defined as TWT failure); Gemini's failure counts similar to chance distribution. Correlation with Log_Gfreq rho = 0.63 (p &lt; 0.001).",
            "notes": "Gemini tends to cluster ratings centrally (lack of clear bimodality seen in humans); shows increased reliance on bigram frequency and semantic similarity relative to human judgments.",
            "uuid": "e7237.2",
            "source_info": {
                "paper_title": "The Two Word Test as a semantic benchmark for large language models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Claude-3-Opus (TWT)",
            "name_full": "Anthropic Claude-3-Opus (Two Word Test evaluation)",
            "brief_description": "Claude-3-Opus performed the TWT numerical task and produced bimodal rating peaks more like humans and substantially fewer failures than other tested LLMs, but still significantly more failures than human raters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude-3-Opus",
            "model_description": "Anthropic transformer-based large language model (Claude family, Opus variant), evaluated via API (March 2024 availability).",
            "model_size": null,
            "test_name": "Two Word Test (TWT) — numerical meaningfulness judgments",
            "test_category": "semantic composition / semantic judgment",
            "test_description": "0–4 Likert meaningfulness ratings for noun-noun phrases compared item-wise to human rating distributions.",
            "evaluation_metric": "Mean rating (0–4); phrase-wise Crawford & Howell t-tests; counts of items where model rating has &lt;5% probability under simulated human distribution.",
            "human_performance": "Human baseline from Graves et al. (N=150), 977 'makes sense' vs. 761 'nonsense'; bimodal distributions reflect clear human consensus on many items.",
            "llm_performance": "Claude-3-Opus showed bimodal peaks more similar to humans and substantially fewer TWT failures than GPT-3.5, GPT-4, and Gemini, but still produced significantly more failures than expected from human raters (i.e., not matching human-level performance).",
            "prompting_method": "Same instruction prompt (Graves et al.) augmented with examples for all rating values, phrases supplied in randomized subsets, repeated over 10 shuffled iterations.",
            "fine_tuned": false,
            "human_data_source": "Graves et al., 2013: noun-noun meaningfulness ratings dataset.",
            "statistical_significance": "Claude had significantly fewer TWT failures than other LLMs but still significantly more than human simulated distribution (phrase-wise test threshold p &lt; 0.05). Correlations: Log_Gfreq rho = 0.66 (p &lt; 0.001); semantic similarity (GloVe rho = 0.37 p &lt; 0.001; Word2Vec rho = 0.38 p &lt; 0.001; Taxonomic rho = 0.21 p &lt; 0.001).",
            "notes": "Best-performing model on numerical TWT among those tested, but still falls short of human consistency; like other LLMs, shows stronger coupling of meaningfulness ratings to word-to-word semantic similarity than humans.",
            "uuid": "e7237.3",
            "source_info": {
                "paper_title": "The Two Word Test as a semantic benchmark for large language models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-3.5 (bTWT)",
            "name_full": "OpenAI GPT-3.5-turbo (binary Two Word Test evaluation)",
            "brief_description": "GPT-3.5-turbo was evaluated on a binary version of the TWT (bTWT) and displayed poor discrimination, with an excessively liberal bias (high false-alarm rate, tending to classify most phrases as 'makes sense').",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "OpenAI transformer-based LLM (decoder-only), evaluated zero/few-shot with instruction examples.",
            "model_size": null,
            "test_name": "binary Two Word Test (bTWT)",
            "test_category": "semantic composition / decision-making (binary)",
            "test_description": "Binary judgment for each noun-noun phrase: 'makes sense' vs 'nonsense' (human numerical ratings recoded to binary for comparison; 977 meaningful, 761 nonsense).",
            "evaluation_metric": "Signal detection theory metrics (d', β), hit/false-alarm rates, ROC/AUC, and χ² test comparing response frequency to human baseline.",
            "human_performance": "Human baseline recoded to binary: 977 'makes sense' items, 761 'nonsense' items (from Graves et al., 2013). Human discrimination is high (bimodal agreement across items).",
            "llm_performance": "GPT-3.5 displayed poor discrimination (low d'), high hit rate but also very high false-alarm rate (liberal β, log10(β) &lt; 0), ROC/AUC near chance relative to humans; χ² test shows LLM response frequencies significantly different from human frequencies (p &lt; 0.05).",
            "prompting_method": "Instruction prompt adapted for binary responses with examples (four example items), randomized phrase submissions, multiple iterations.",
            "fine_tuned": false,
            "human_data_source": "Graves et al., 2013 (human meaningfulness ratings recoded to binary for comparison).",
            "statistical_significance": "χ² tests reported significant differences between GPT-3.5 response frequencies and human expected frequencies (p &lt; 0.05). SDT metrics indicate near-chance discrimination (d' approximately ≈ 0 or low; exact numeric d' not provided in text).",
            "notes": "Behavioral signature: excessively liberal decision criterion, treating many nonsense phrases as meaningful; suggests failure not solely due to numeric rating format (since binary task also fails).",
            "uuid": "e7237.4",
            "source_info": {
                "paper_title": "The Two Word Test as a semantic benchmark for large language models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-4 (bTWT)",
            "name_full": "OpenAI GPT-4-turbo (binary Two Word Test evaluation)",
            "brief_description": "GPT-4-turbo showed modest discrimination on the bTWT: better than GPT-3.5 but still with a nontrivial tendency to label nonsense phrases as 'makes sense'.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4-turbo",
            "model_description": "OpenAI transformer-based LLM (GPT-4 family), evaluated with instruction prompts and examples.",
            "model_size": null,
            "test_name": "binary Two Word Test (bTWT)",
            "test_category": "semantic composition / decision-making (binary)",
            "test_description": "Binary judgments of noun-noun phrase meaningfulness compared to human recoded binary labels (977 meaningful; 761 nonsense).",
            "evaluation_metric": "Signal detection theory metrics (d', β), ROC/AUC, hit/false-alarm rates, and χ² test against human frequencies.",
            "human_performance": "Human baseline: 977 meaningful and 761 nonsense phrases (Graves et al., 2013).",
            "llm_performance": "GPT-4 achieved modest discrimination (intermediate d'), able to detect meaningfulness in some items but with a moderate false-alarm rate (labeling nonsense phrases as 'makes sense' more often than humans). χ² indicates response frequencies significantly differ from humans (p &lt; 0.05).",
            "prompting_method": "Binary version of Graves instructions with examples; phrases randomized and submitted across 10 iterations; averaged outputs.",
            "fine_tuned": false,
            "human_data_source": "Graves et al., 2013 dataset (binary recoding of human ratings).",
            "statistical_significance": "χ² test in Table 3 indicates significant differences vs human frequencies (p &lt; 0.05). SDT metrics show d' moderate but lower than human-level discrimination; exact numeric d' reported in paper tables (not included verbatim in main text).",
            "notes": "GPT-4 is better than GPT-3.5 on bTWT but still systematically worse than humans; error profile includes mistakenly accepting semantically related but non-combinatorial pairs as meaningful.",
            "uuid": "e7237.5",
            "source_info": {
                "paper_title": "The Two Word Test as a semantic benchmark for large language models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Gemini (bTWT)",
            "name_full": "Google Gemini-1.0-Pro-001 (binary Two Word Test evaluation)",
            "brief_description": "Gemini-1.0-Pro-001 on bTWT produced modest discrimination (similar to GPT-4) with a moderate tendency to classify nonsense phrases as meaningful.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini-1.0-Pro-001",
            "model_description": "Google Gemini transformer-based LLM (pro variant), assessed via instruction prompts.",
            "model_size": null,
            "test_name": "binary Two Word Test (bTWT)",
            "test_category": "semantic composition / decision-making (binary)",
            "test_description": "Binary 'makes sense' vs 'nonsense' judgments for 1,768 noun-noun phrases; compared to human recoded binary labels.",
            "evaluation_metric": "Signal detection theory metrics (d', β), ROC/AUC, hit/false-alarm rates, χ² test against human frequencies.",
            "human_performance": "Human baseline recoded binary: 977 meaningful items and 761 nonsense items (from Graves et al., 2013).",
            "llm_performance": "Gemini showed modest discrimination (intermediate d'), capable of some correct differentiation but with a moderate false-alarm rate (labeling nonsense as meaningful). χ² indicates distribution of responses significantly different from human baseline (p &lt; 0.05).",
            "prompting_method": "Binary instructions with examples (four exemplars), randomized presentation of phrases across multiple submissions and averaged over iterations.",
            "fine_tuned": false,
            "human_data_source": "Graves et al., 2013 (human ratings recoded to binary).",
            "statistical_significance": "SDT and χ² analyses reported significant differences from human expected frequencies (χ² p &lt; 0.05). Correlation with Log_Gfreq rho = 0.63 (p &lt; 0.001).",
            "notes": "Gemini tends toward mid-scale/more liberal responses in numerical TWT and shows a moderate tendency to accept nonsense phrases in binary format; similar error profile to GPT-4 but not as conservative or human-like as Claude-3-Opus.",
            "uuid": "e7237.6",
            "source_info": {
                "paper_title": "The Two Word Test as a semantic benchmark for large language models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Claude-3-Opus (bTWT)",
            "name_full": "Anthropic Claude-3-Opus (binary Two Word Test evaluation)",
            "brief_description": "Claude-3-Opus achieved the best binary discrimination (bTWT) among the tested models: moderate-to-high d' and better correct rejection rates, but still significantly different from human performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude-3-Opus",
            "model_description": "Anthropic transformer-based large language model (Claude family, Opus variant), evaluated using instruction prompts with examples.",
            "model_size": null,
            "test_name": "binary Two Word Test (bTWT)",
            "test_category": "semantic composition / decision-making (binary)",
            "test_description": "Binary judgment task ('makes sense' vs 'nonsense') on 1,768 noun-noun phrases; compared to human binary labels derived from Graves et al. ratings.",
            "evaluation_metric": "Signal detection theory: d', β, ROC/AUC; hit/false-alarm rates; χ² test comparing LLM response frequencies to human baseline.",
            "human_performance": "Human baseline: 977 items labeled 'makes sense' and 761 'nonsense' (Graves et al., 2013).",
            "llm_performance": "Claude-3-Opus displayed substantially improved discrimination relative to other LLMs (higher correct rejection, lower false-alarm rate), moderate-to-high d' and larger AUC, but still significantly different from human performance per χ² and SDT measures.",
            "prompting_method": "Binary instructions with multiple examples, randomized phrase presentation, and repeated submissions averaged across iterations.",
            "fine_tuned": false,
            "human_data_source": "Graves et al., 2013 (human meaningfulness ratings recoded to binary labels).",
            "statistical_significance": "χ² tests show Claude's response frequency distribution is significantly different from humans (p &lt; 0.05), though Claude's SDT metrics (d', AUC) were highest among tested LLMs. Semantic similarity correlations for Claude: GloVe rho = 0.37 (p &lt; 0.001), Word2Vec rho = 0.38 (p &lt; 0.001), Taxonomic rho = 0.21 (p &lt; 0.001).",
            "notes": "Claude-3-Opus is the best of the four models on both numerical and binary TWT but still fails to match human-level discrimination; like other models, it relies more on semantic similarity and phrase frequency than humans when making meaningfulness judgments.",
            "uuid": "e7237.7",
            "source_info": {
                "paper_title": "The Two Word Test as a semantic benchmark for large language models",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Noun-noun combination: Meaningfulness ratings and lexical statistics for 2,160 word pairs",
            "rating": 2,
            "sanitized_title": "nounnoun_combination_meaningfulness_ratings_and_lexical_statistics_for_2160_word_pairs"
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with GPT-4",
            "rating": 1,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Faith and Fate: Limits of Transformers on Compositionality",
            "rating": 2,
            "sanitized_title": "faith_and_fate_limits_of_transformers_on_compositionality"
        },
        {
            "paper_title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models",
            "rating": 1,
            "sanitized_title": "performance_of_chatgpt_on_usmle_potential_for_aiassisted_medical_education_using_large_language_models"
        },
        {
            "paper_title": "Machine Reading, Fast and Slow: When Do Models 'Understand' Language?",
            "rating": 1,
            "sanitized_title": "machine_reading_fast_and_slow_when_do_models_understand_language"
        },
        {
            "paper_title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver",
            "rating": 1,
            "sanitized_title": "is_chatgpt_a_generalpurpose_natural_language_processing_task_solver"
        }
    ],
    "cost": 0.01643,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Two Word Test as a semantic benchmark for large language models</p>
<p>Nicholas Riccardi 
Department of Communication Sciences and Disorders
University of South Carolina
29208ColumbiaUSA</p>
<p>Xuan Yang 
Department of Psychology
University of South Carolina
29208ColumbiaUSA</p>
<p>Rutvik H Desai rutvik@sc.edu 
Department of Psychology
University of South Carolina
29208ColumbiaUSA</p>
<p>The Two Word Test as a semantic benchmark for large language models
E2ADAA02821ED95B3D8420ED86D78A8610.1038/s41598-024-72528-3Received: 7 May 2024; Accepted: 9 September 2024
Large language models (LLMs) have shown remarkable abilities recently, including passing advanced professional exams and demanding benchmark tests.This performance has led many to suggest that they are close to achieving humanlike or "true" understanding of language, and even artificial general intelligence (AGI).Here, we provide a new open-source benchmark, the Two Word Test (TWT), that can assess semantic abilities of LLMs using two-word phrases in a task that can be performed relatively easily by humans without advanced training.Combining multiple words into a single concept is a fundamental linguistic and conceptual operation routinely performed by people.The test requires meaningfulness judgments of 1768 noun-noun combinations that have been rated as meaningful (e.g., baby boy) or as having low meaningfulness (e.g., goat sky) by human raters.This novel test differs from existing benchmarks that rely on logical reasoning, inference, puzzle-solving, or domain expertise.We provide versions of the task that probe meaningfulness ratings on a 0-4 scale as well as binary judgments.With both versions, we conducted a series of experiments using the TWT on GPT-4, GPT-3.5, Claude-3-Optus, and Gemini-1-Pro-001.Results demonstrated that, compared to humans, all models performed relatively poorly at rating meaningfulness of these phrases.GPT-3.5-turbo,Gemini-1.0-Pro-001and GPT-4-turbo were also unable to make binary discriminations between sensible and nonsense phrases, with these models consistently judging nonsensical phrases as making sense.Claude-3-Opus made a substantial improvement in binary discrimination of combinatorial phrases but was still significantly worse than human performance.The TWT can be used to understand and assess the limitations of current LLMs, and potentially improve them.The test also reminds us that caution is warranted in attributing "true" or human-level understanding to LLMs based only on tests that are challenging for humans.Large Language Models (LLMs; also called Large Pre-Trained Models or Foundation Models 1 ) are deep neural networks with billions or trillions of parameters that are trained on massive natural language corpora.They have shown surprising and remarkable abilities spanning many different tasks.Some examples include the ability to pass examinations required for advanced degrees, such as those in law 2 , business 3 , and medicine 4 .Strong performance on benchmarks such as General Language Understanding Evaluation (GLUE) and its successor (SuperGLUE) have also been obtained5,6.Bubeck et al. 7 investigated an early version of GPT-4, and reported that it can solve difficult tasks in mathematics, coding, vision, medicine, law, psychology, and music, and exhibited "mastery of language." With such breadth of human-level (or better) performance, they suggested that it shows "sparks" of Artificial General Intelligence (AGI).Such achievements have led many researchers to conclude that LLMs have achieved or are close to achieving a real or humanlike understanding of language.Others remain skeptical.A recent survey 8 asked active researchers whether such models, trained only on text, could in principle understand natural language someday.About half (51%) agreed, while the other half (49%) disagreed.This stark divide is closely tied to the question of what constitutes true understanding and has been the subject of intense debate 9 .The skeptics have pointed out examples where LLMs produce less-than-satisfactory performance.Hallucinations 10,11 , inaccurate number comparisons, and reasoning errors are commonly cited problems, and failures in individual cases are frequently reported (e.g., https:// github.com/ giuve n95/ chatg pt-failu res).It is argued that while LLMs exhibit formal linguistic competence, they lack functional linguistic competence, which is the ability to robustly understand and use language in the real world 12 .However, this claim still runs into the problem of how to measure robust understanding beyond subjective assessments of the quality of answers in</p>
<p>response to prompts.Objective benchmarks are essential here, but as successes and failures of LLMs show, benchmarks that are suitable for measuring human understanding might not be appropriate for assessing LLMs [13][14][15] .</p>
<p>There are philosophical arguments as to why LLMs do not have true or humanlike understanding.For example, LLMs learn words-to-words mappings, but not words-to-world mappings, and hence cannot understand the objects or events that words refer to 16 .Such arguments aside, formal tests are critical, as that's where "rubber meets the road." If a system can match or surpass human performance in any task thrown at it, the argument that it does not possess real understanding rings hollow.If an LLM indeed lacks humanlike understanding, one ought to be able to design tests where it performs worse than humans.With such tests, the nebulous definition of "understanding" becomes less of a problem.</p>
<p>Here, we propose and evaluate one such novel benchmark, the Two Word Test (TWT).The test is based on a basic human psycholinguistic ability to understand combinations of two words, which has been of great interest to psychologists, linguists, and neuroscientists for decades [17][18][19][20] .The test uses noun-noun combinations such as beach ball, and requires discrimination between meaningful and nonsense or low meaningfulness combinations.Compared to other linguistic possibilities, such as adjective-noun (big ball) or verb-noun (throw ball) combinations, noun-noun combinations do not offer grammatical assistance in determining meaningfulness.One can determine that ball red is not a meaningful phrase, because noun-adjective is not a valid word order in English.The same strategy cannot be used to determine that ball beach has low meaningfulness.Some phrases are learned as single units that combine unrelated words (sea lion), while others are "built from the ground up".Baby boy makes sense, and many other words could follow baby and the phrase would still be sensible (clothes, girl, sister, etc.).Simply reversing the word order of some of these (clothes baby) can result in a low-meaningfulness phrase.While the exact cognitive processes involved in human word combination remain debated 20 , it requires the composition of two or more lexical elements into a coherent whole, followed by a judgment of that whole constituent against prior world knowledge and related concepts.In this context, phrase-level familiarity/ frequency is a vital part of 'meaningfulness' , but does not fully explain it.Phrases that we encounter more often will naturally be accepted as meaningful, but some low-frequency phrases can also be highly meaningful.Unlike existing benchmarks that rely on the ability to do logical reasoning, planning, infer implied content, disambiguate ambiguous words, or solve puzzles or other problems, the TWT has unique memory-dependent, semantic, and compositional elements that make it a valuable semantic benchmark for LLMs.A previous study 21 obtained meaningfulness ratings on these phrases from 150 human participants.In the current study, we aim to use comprehensive statistical tests to compare the judgment of humans and four current LLMs (OpenAI's GPT-4-turbo and GPT-3.5-turbo,Google's Gemini-1-Pro, and Anthropic's Claude-3-Opus) in the original TWT measuring the meaningfulness judgment using a Likert scale, and a binary TWT (bTWT) measuring binary "makes sense" or "nonsense" judgments.Finally, we examine the effect of n-gram phrase frequency and semantic cosine similarity between the two words in the presented phrases on human and LLM meaningfulness ratings to identify possible reasons for discrepancies between LLM and human ratings.We discuss the limitation of current LLMs in language comprehension reflected in these novel benchmarks as a weakness distinct from those in tasks that rely on higher-level executive control, such as logical reasoning or puzzle solving.</p>
<p>Materials and methods</p>
<p>Two word test phrase generation and human rating collection</p>
<p>The TWT consists of noun-noun combinations and human meaningfulness ratings collected as part of behavioral and neuroimaging experiments conducted by Graves and colleagues 21 , whose methods we will now summarize.They chose 500 common nouns, and all possible noun-noun combinations were generated, resulting in 249,500 potential phrases.The occurrence of these combinations as two-word phrases was cross-referenced with a large corpus of human-generated text, and phrases that appeared at least once and in only one direction (i.e., 'noun1 noun2' but not 'noun2 noun1') were kept.Then, as judged by Graves et al., phrases with meaningful interchangeable word orders or that were taboo were removed, resulting in 1080 possibly meaningful phrases.Possible "nonsense" or low-meaningfulness phrases were then generated by reversing the word order of meaningful phrases, resulting in 2,160 total phrases for rating by humans (half being possibly 'meaningful' and half being possibly 'nonsense').Note that, at this point, no actual rating by humans had been conducted yet, so it is possible that some of the possible 'meaningful' phrases would be given low meaningfulness ratings by humans, or vice versa.Participants (N = 150), who were undergraduates at the University of Wisconsin-Madison in the early 2010s, then rated subsets of the total phrase pool with the following instructions:</p>
<p>Please read each phrase, then judge how meaningful it is as a single concept, using a scale from 0 to 4 as follows: If the phrase makes no sense, the appropriate rating is 0. If the phrase makes some sense, the appropriate rating is 2. If the phrase makes complete sense, the appropriate rating is a 4. Please consider the full range of the scale when making your ratings.Examples: the goat sky, 0 (makes no sense), the fox mask, 2 (makes some sense), and the computer programmer, 4 (makes complete sense).</p>
<p>For each phrase, the mean and standard deviation of participant responses were calculated.392 phrases with mean ratings between 1.5 and 2.5 were removed from the set due to being ambiguous to human raters.This resulted in 977 nonsense and 761 meaningful phrases, as judged by human raters, resulting in the final set of 1,768 word pairs used in the two word test (TWT) presented here.81.7% of these pairs appear twice in the TWT stimuli, once forward and once backward.The remainder of pairs indicate instances where human raters judged a word pair as either meaningful or nonsense, but that word pair's reverse was judged as ambiguous (e.g., rated between 1.5 and 2.5).The final set of stimuli can be found at https:// github.com/ NickR iccar di/ two-word-test.</p>
<p>This dataset was chosen for this benchmark for a few reasons.First, it is one of the largest available sets of combinatorial noun-noun phrases that have corresponding human ratings.Second, due to how the list was constructed, it provides good control of important psycholinguistic variables such as word frequency and concreteness that may have profound effects on human and LLM ratings.That is, because low-sensibility phrases are created by reversing the word order of high-sensibility phrases, many lexical confounds related to psycholinguistic properties between the low-and high-sensibility phrases are eliminated.</p>
<p>Results and discussion</p>
<p>We conducted a series of experiments comparing Claude-3-Opus, Gemini-1.0-Pro-001,GPT-4-turbo, and GPT-3.5-turboperformance (each model as available in March 2024) to the human data.First, we gave the LLMs the same prompt used by Graves et al. 21, followed by an enhanced prompt.Then, we tested the LLMs on a binary version of the test (i.e., "makes sense"/"nonsense" judgment instead of numerical ratings) that was expected to be easier for LLMs.</p>
<p>TWT: numerical meaningfulness judgments</p>
<p>Due to token restrictions, the phrases were randomly assigned to 8 subsets to ensure that the LLMs' errors were not due to memory limitations.To prompt the LLMs' judgment, we submitted the same instructions and examples originally provided by Graves et al.However, using Graves' original prompt resulted in the LLMs largely neglecting the use of the 1 and 3 ratings, the two ratings not used as example cases in Graves' original prompt.To encourage the LLMs to use the full rating scale, we provided two additional examples in the instructions for scores of 1 and 3 (the knife army, 1 (makes very little sense), and the soap bubble, 3 (makes a lot of sense)).Each query input consisted of the original instructions, four examples, and a subset of the phrases appended to the end.To rule out the possibility that the LLMs' meaningfulness judgment on two-word phrases depended on the meaningfulness of the phrases prior to or following it, we shuffled the order of the phrases ten times and repeated the query for ten iterations.The query inputs were kept the same for different LLMs to ensure a direct comparison.The final meaningfulness judgment for each phrase was the average score across ten iterations.The 1768 unambiguous phrases were selected.Compared to the human distribution, which reflects "makes sense" and "nonsense" phrases in the bimodal peaks, Gemini-1.0-Proand GPT-3.5-turboshowed a bias towards rating most phrases as a 2 or 3 (makes some sense, makes a lot of sense; Fig. 1).GPT-4-turbo and Claude-3-Opus showed similar bimodal peaks as humans, but there were still a considerable number of phrases that were judged as ambiguous with a rating around 2 (makes some sense).See supplementary materials for scatterplots comparing human ratings of each item to the ratings of LLMs.</p>
<p>However, it is more informative to take LLM ratings of each individual phrase and test the probability that its rating came from the same distribution as the human responses to that phrase.We conducted a series of phrasewise statistical tests to compare each LLM to human meaningfulness ratings.</p>
<p>First, we used the human means and standard deviations for each individual phrase (provided by Graves et al. 21to generate a Gaussian distribution of 10,000 simulated human responses to each phrase, respecting the lower and upper limits of the 0-to-4 scale, resulting in an approximation of realistic response distributions (Fig. 2; see supplementary materials for additional examples).Then, for each phrase, we conducted a Crawford and Howell t-test 22 for case-control comparisons with the LLM as the case and the human distribution as the control.This modified t-test is designed for the comparison of a single-case observation to a control group and returns the probability that the case comes from the same distribution as the group.We hereby define a "TWT failure" as when the LLM meaningfulness rating has less than a 5% probability of coming from the human distribution (i.e., the LLM rating is significantly different from that of humans, p &lt; 0.05).</p>
<p>To understand where LLM responses fall between human ratings and chance or random ratings, we generated two rating distributions.(1) "Human": 1000 simulated participants whose phrase-wise responses were generated from the underlying probability distribution of responses to each phrase in Graves et al. 21.(2) "Chance": 1000 permuted participants whose phrase-wise responses were selected based only on the frequency of 0-4 ratings from the original study.The "Human" distribution approximates what would be expected from human raters if the study was run on a large number of human participants.The "Chance" distribution is what would be generated by a system with no knowledge of word meaning.We then generated failure counts for the distributions and for each of the models.</p>
<p>Experiment 1 Results: Table 1 and Fig. 3 show that Gemini-1.0-Pro,GPT-3.5-turbo, and GPT-4-turbo failure counts are closer to chance than to the simulated human distribution.Claude-3-Opus is significantly better than Fig. 1.Frequency of continuous meaningfulness ratings for humans and LLMs.Human mean responses reflect a bimodal distribution of meaningful and nonsense phrases, while that is lacking in all four LLMs. the other LLMs, but still fails far more than what would be expected from a human rater.Taken together, these results show that the three LLMs fail at the TWT, but that there are significant differences between their abilities.</p>
<p>bTWT: binary Two Word Test</p>
<p>LLMs are often reported to make errors on numerical tasks.It is possible that the poor performance on the TWT was due to a difficulty in dealing with the numerical scale required for the task, rather than a lack of understanding of phrase meaning.In order to eliminate numerical ratings, we modified the TWT instructions to prompt binary responses, again appending phrases to the end of the instructions in randomized order and with multiple submissions: Fig. 2. Simulated human rating distributions (gray) and LLM ratings for low-and high-meaningfulness phrases (the cake apple, the dog sled).For the cake apple, GPT-3.5-turbo,GPT-4-turbo, and Gemini-1.0-Pro-001rated it as more meaningful than &gt; 95% of humans would be expected to, while Claude-3-Opus responded within normal limits.For the dog sled, GPT-4-turbo rated it as less meaningful than &gt; 95% of humans would be expected to, while the other LLMs responded within normal limits.www.nature.com/scientificreports/Please read each phrase, then judge how meaningful it is as a single concept.If the phrase makes no sense or makes very little sense, the appropriate response is "nonsense".If the phrase makes a lot of sense or complete sense, the appropriate rating is "makes sense".Examples: "the goat sky" is "nonsense", "the knife army" is "nonsense", "the soap bubble" is "makes sense", "the computer programmer" is "makes sense".</p>
<p>For comparison, we recoded the numerical ratings of human judgments to binary values.Out of the 1768 phrases, 977 phrases (mean ratings larger than 2.5) were recoded as "make sense" and 761 phrases (mean ratings less than 1.5) were recoded as "nonsense".We then calculated the following to measure LLM performance: Chi-squared (χ 2 ) test, signal detection theory (SDT) metrics, and receiver operating characteristic (ROC) curve.</p>
<p>Experiment 2 Results: Table 2 and Fig. 4 show SDT results.SDT measures how well an actor (LLMs) can detect true signals (meaningful phrases) while correctly rejecting noise (nonsense phrases).It uses ratios of hits (true positives), correct rejections (true negatives), false alarms (false positives), and misses (false negatives).d' is a measure of overall ability to discriminate, with 0 being chance-level and &gt; 4 being very high discrimination ability.β measures response tendency, or whether an actor prefers to say that a signal is present (liberal) or absent (conservative).The base 10 logarithm of β, reported here, is interpreted as &lt; 0 being liberal and &gt; 0 being conservative.We also display the ROC curve (Fig. 5) and report the area under the curve (AUC).</p>
<p>We also conducted χ 2 test.Briefly, χ 2 test is used with categorical data and can test for statistical independence of observed frequencies to what is expected.Here, observed frequencies are the counts of LLM "makes sense" and "nonsense" responses and the expected response frequencies are those provided by the human data (e.g., 977 nonsense and 761 meaningful).Table 3 shows that the LLM frequency of responses are significantly different from the human response frequencies, and supports SDT and ROC results.</p>
<p>Table 2 and Figs. 4 and 5 show that GPT-3.5-turbodisplays poor discrimination, and it judges most pairs as making sense (both high hit and false alarm rate).Gemini-1.0-Pro-001and GPT-4-turbo display modest discrimination.They can correctly differentiate sensible and nonsense phrases while having a moderate chance of incorrectly judging nonsense phrases as making sense.Claude-3-Opus is substantially better than the other models and displays moderate-to-high discrimination abilities.Compared to GPT-4-turbo and Gemini-1.0-Pro-001,its ability to correctly detect nonsense and sensible phrases is further improved (higher correct rejection rate and lower false alarm rate).However, it is still significantly different from human performance.</p>
<p>Phrase-level frequency, semantic space, and meaningfulness ratings</p>
<p>The frequency with which humans encounter phrases is a vital part of 'meaningfulness' , as phrases that are encountered more often will naturally be accepted as meaningful (although low-frequency phrases can also be Table 2. d' , β, and AUC for LLM "makes sense" / "nonsense" discrimination.www.nature.com/scientificreports/meaningful).Similarly, meaningfulness ratings in LLMs could be expected to have a similar relationship with phrase-level frequency, due to their training corpora.We tested this by examining Spearman's correlations between human/LLM TWT meaningfulness ratings and logarithmic Google bigram frequency (Log_Gfreq) for each phrase, as provided in the original Graves dataset.Both human and LLM ratings were strongly correlated with Log_Gfreq (except for GPT-3.5, which had a weak but still statistically significant correlation; Table 4).Hence, both humans and LLMs appear to use bigram frequency to inform their meaningfulness judgments, and it is not clear where the discrepancy in ratings between humans and LLMs comes from.To identify possible reasons for the differences between human and LLM meaningfulness ratings, we tested the hypothesis that the LLMs are more likely to rate two words that are close together in semantic space as meaningful, regardless of whether the two words actually combine to form a coherent, meaningful phrase (e.g., the frog toad, the meat lamb).To test this, for each phrase, we calculated the semantic cosine similarity between the two words using 3 different semantic vector representations: GloVe, Word2Vec, and Taxonomic [23][24][25][26] .Using Spearman's correlations, we investigated how meaningfulness judgments by humans/LLMs related to phrase-level semantic cosine similarity, controlling for Log_Gfreq (Table 5).Indeed, compared to humans, LLM meaningfulness ratings were more strongly correlated to semantic cosine similarity of the two words, suggesting that LLMs may have difficulty identifying when two highly semantically related words combine to create a low-meaningfulness phrase.www.nature.com/scientificreports/Here, our task differs from common benchmark tasks that involve logic puzzles, arithmetic, games, or reasoning and inference given a sentence or short narrative 27 .In these problems, there is an objective "correct answer" that LLMs strive for, regardless of how well ordinary people perform on the problem.TWT, on the other hand, is based on subjective human judgments.The right answer for each word pair is what it is rated to be by a population of adults.For this task, the gold standard is the human rating, and there is no objectively true answer as such.Virtually any word pair can be said to "make sense" in some context or with sufficient mental gymnastics.A classic example of this is the poetry competition (http:// archi ves.conla ng.info/ ga/ farzhi/ shiar weilw oen.html) in which many participants were able to write poems where the line "Colorless green ideas sleep furiously"-a line famously designed to not make sense-made sense.Given different instructions, humans can also come up with hypothetical, metaphorical, or poetic 'meaningful' interpretations for almost any two word phrase (e.g., the chair peacock is a species of peacock that likes to sit in chairs; the phone shoe is a shoe that can also serve as a phone).However, this is a different task than what is provided by the TWT, which constrains 'meaningfulness' via instructions and examples.TWT does not suggest that the low meaningfulness or nonsense word combinations cannot possibly make sense under any context.The claim is simply that some word combinations make much less sense on the surface or at face value, in the absence of any specific context.The task is to identify these combinations.</p>
<p>Limitations</p>
<p>Prompt design can have a profound impact on LLM performance.Here, we started by keeping the prompts as similar as possible to the original Graves et al. 21study, since the goal was to compare LLM performance to that of humans when given the same task.The prompt included multiple examples and was designed to be easy for humans to understand.LLMs are able to successfully complete similar tasks and instructions in other benchmarks.We performed a limited amount of prompt engineering, which included the addition of more examples (similar to the Few-Shot prompting with one example per rating value of the 4-point Likert scale), and a binary version of the task.However, it is possible that further modifications could tune LLM ratings to be closer to that of humans.Some popular prompting strategies (e.g., Chain-of-Thought and Tree-of-Thought), which are useful for improving the performance of complex tasks involving multiple reasoning steps, are not relevant to TWT since the problem does not contain any clear sub-goals or reasoning chains.A complete examination of the effects of prompt engineering on the TWT is beyond the scope of the current manuscript.However, future studies may experiment with instruction tuning to improve LLM performance on TWT by using the materials provided at https:// github.com/ NickR iccar di/ two-word-test.</p>
<p>Due to the "black box" nature of LLMs tested here, it is unclear exactly why the LLMs provide meaningfulness judgments that are so different from humans.In the code provided, we have calculated similarity metrics for each phrase (cosine similarity of the two words in each phrase) based on some popular word embedding models such as word2vec.Future research could seek to find patterns using these metrics (or any number of other psycholinguistic properties) which may shed light on the instances in which LLMs are failing.</p>
<p>Finally, the Graves et al. 21dataset represents one of the largest sets of combinatorial semantic phrases that have human ratings, and the nature of its construction means that low-and high-sensibility phrases are closely matched to each other in important psycholinguistic variables.However, similar to many psycholinguistic studies, ratings were gathered in a relatively homogenous population (undergraduate students).Interesting directions for future research could include novel two-word phrases, metaphorical semantics, understanding combinations of</p>
<p>Conclusions and future work</p>
<p>We presented a new benchmark for testing language understanding in LLMs.The task, essentially trivial for humans, requires rating meaningfulness of two-word phrases.Four current LLMs fail on this task.While Claude-3-Opus performed better than GPT-4-turbo, Gemini-1.0-Pro-001,and GPT-3.5-turbo,its performance still fell well short of humans.</p>
<p>A binary version of the test, bTWT, was created to test whether the poor performance of LLMs was the result of a failure to deal with the numerical scale required for TWT.The bTWT revealed that GPT-3.5-turbo,Gemini and GPT-4-turbo fail to distinguish meaningfulness of phrases binarily, achieving poor-to-moderate discrimination.GPT-3.5-turbo was excessively liberal, tending to rate everything as "making sense".Gemini-1.0-Pro-001and GPT-4-turbo were able to detect the meaningfulness in some cases, while still having a higher chance of labeling nonsense phrases as "make sense".Claude-3-Opus, however, takes a significant step forward on the bTWT.</p>
<p>Several investigations have begun to examine and reveal the limitations of LLMs.For example, Dziri et al. 28 tested LLMs on three compositional tasks (multi-digit multiplication, logic grid puzzles, and dynamic programming).They found that LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching.They suggest that in multi-step reasoning problems, LLM performance will rapidly decay with increasing complexity.Failures have been demonstrated in other problems as well, such as those involving logical and common-sense reasoning 29,30 as well as sequence tagging 31 .</p>
<p>The TWT differs from these cases in that it does not directly require inference or reasoning.The reaction time of making meaningfulness judgments in tasks similar to TWT is around 1 s for humans 19,32 , which suggests that it is a qualitatively different process than those used in typical benchmark tasks such as puzzle solving or logical and commonsense reasoning.A limitation in breaking down a complex chain of reasoning into smaller problems should not affect performance on the TWT.Understanding these phrases requires understanding the constituent concepts, and then using world knowledge to determine whether the combination makes sense in some manner.A "mountain stream" is a stream located on a mountain, but a "stream mountain" is not a thing at all.An "army knife" is not necessarily a knife located in the army but a type of knife useful in certain situations.TWT may exploit the fact that the text corpora that LLMs are trained on, no matter how large, almost entirely contain sensible text.However, this is the case for humans as well.Almost all text that people are exposed to is also sensible, but if the task requires, they are easily able to determine that certain word combinations don't make much sense.Current LLMs may lack the depth of real-world knowledge that is required for this task.</p>
<p>Many of the limitations of LLMs identified previously can be associated with a lack of "executive control" that presents difficulties in complex symbolic or rule-based reasoning.Because of this, many have proposed combining deep neural networks with symbolic reasoning systems that can exert executive control when required (e.g., in three-digit multiplication).The weakness identified by TWT is qualitatively distinct, in that it is not directly related to the ability for executive control or systematic application of rules.</p>
<p>The importance of LLMs largely centers around the idea that they can understand language.The notion of understanding and meaningfulness implies that not everything "makes sense."As an analogy, the notion of grammaticality is based on the fact that not every sentence is grammatical-certain sentences and phrases fall outside the grammaticality boundary, which linguists commonly attempt to delineate.Similarly, the notion of semantics also implies that there are certain things that do not make sense or are incoherent.In this context, there are two broad classes of applications of LLMs.In the first class, there is a clearly defined correct answer where one wants the LLM to produce that answer regardless of what humans do for such problems, as mentioned above.Examples are logical reasoning problems or generating code.The second class of applications is where one would want LLMs to behave like humans.In these cases, the goal is for LLM's sense of understanding or meaning to be similar to that of humans.Examples of such cases are an LLM avatar attending a meeting in place of an employee, providing customer support in relation to some products or services, or automatically replying to emails.Here, one would desire for the LLM to behave like humans.This would include detecting cases where messages don't make sense, and where appropriate actions would be asking for clarifications, correcting a mistake, or denying a request, instead of creatively interpreting every message as sensible.Nonsensical messages can arise in real-world scenarios due to many reasons, including mistakes, misunderstandings, disorders, or malicious attacks.We believe that TWT is most relevant for these second classes of problems.It allows us to test whether LLMs have a boundary that is similar to that of humans in relation to meaningfulness.</p>
<p>These results also urge for caution in the attribution of AGI or similar abilities to LLMs, based only on testing on tasks that are difficult for humans.Seemingly simple language tasks can be difficult for LLMs even when they perform well on complex tasks, which is reminiscent of Moravec's paradox.The mounting understanding of the impressive abilities as well as limitations of LLMs will be essential in improving these models, and in identifying appropriate use cases.</p>
<p>Fig. 3 .
3
Fig. 3. Number of LLM failures in TWT compared to simulated human (blue) and permuted-chance (orange) failure count distributions.</p>
<p>Fig. 4 .
4
Fig. 4. SDT metrics for all 1,768 phrases.Hit -true positive; Miss -false negative; CR -correct rejection (true negative); FA -false alarm (false positive).</p>
<p>Fig. 5 .
5
Fig. 5. ROC curve for all 1,768 phrases.</p>
<p>Table 1 .
1
Summary of TWT failures.</p>
<p>Table 3 .
3
A χ 2 test results for observed (LLM) compared to expected (human) frequency of "makes sense" and "nonsense" responses.p &lt; 0.05 indicates significantly different performance relative to humans.</p>
<p>Table 4 .
4
Spearman's correlations between TWT meaningfulness ratings and phrase-level Log_Gfreq.
Y: Log_GfreqX: Raterr spHumans0.69 p &lt; 0.001claude-3-Opus0.66 p &lt; 0.001gemini-1.0-Pro-0010.63 p &lt; 0.001gpt-3.5-turbo0.13 p &lt; 0.001gpt-4-turbo0.67 p &lt; 0.001
Vol.:(0123456789) Scientific Reports | (2024) 14:21593 | https://doi.org/10.1038/s41598-024-72528-3</p>
<p>Table 5 .
5
Spearman's correlations between TWT meaningfulness ratings and phrase-level semantic cosine similarity.Significant correlation are indicated in bold.www.nature.com/scientificreports/more abstract concepts (e.g., love, freedom, etc.), or how ratings gathered from more diverse human populations may relate to LLM performance.
X: Raterr sFDR pY: GloVeHumans0.03p = 0.18claude-3-Opus0.37p &lt; 0.001gemini-1.0-Pro-0010.17p &lt; 0.001gpt-3.5-turbo0.02p = 0.35gpt-4-turbo0.38p &lt; 0.001Y: Word2VecHumans0.10p &lt; 0.001claude-3-Opus0.38p &lt; 0.001gemini-1.0-Pro-0010.19p &lt; 0.001gpt-3.5-turbo0.06p = 0.01gpt-4-turbo0.37p &lt; 0.001Y: TaxonomicHumans− 0.01 p = 0.79claude-3-Opus0.21p &lt; 0.001gemini-1.0-Pro-0010.04p = 0.18gpt-3.5-turbo0.01p = 0.84gpt-4-turbo0.21p &lt; 0.001
Vol:.(1234567890) Scientific Reports | (2024) 14:21593 | https://doi.org/10.1038/s41598-024-72528-3</p>
<p>Scientific Reports | (2024) 14:21593 | https://doi.org/10.1038/s41598-024-72528-3
© The Author(s) 2024
Vol.:(0123456789) Scientific Reports | (2024) 14:21593 | https://doi.org/10.1038/s41598-024-72528-3www.nature.com/scientificreports/Acknowledgements This work was supported by NIH/NIDCD R01 DC017162 (RHD).Data availabilityThe data generated by LLMs that was used in this study are available at: https:// github.com/ NickR iccar di/ two-word-test.FundingNational Institutes of Health, NIDCD, R01DC017162.Author contributionsN.R. and R.H.D. conceptualized the research.N.R. and X.Y. conducted the experiments, analyzed the data, and generated the visualizations.N.R. wrote the first draft.All authors revised and reviewed the manuscript.Competing interestsThe authors declare no competing interests.Additional informationSupplementary InformationThe online version contains supplementary material available at https:// doi.org/ 10. 1038/ s41598-024-72528-3.Correspondence and requests for materials should be addressed to R.H.D.Reprints and permissions information is available at www.nature.com/reprints.Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material.You do not have permission under this licence to share adapted material derived from this article or parts of it.The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material.If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.To view a copy of this licence, visit http:// creat iveco mmons.org/ licen ses/ by-nc-nd/4.0/.
10.1038/s41598-024-72528-3(1234567890) Scientific Reports | (2024) 14:21593 |. </p>
<p>On the opportunities and risks of foundation models. References 1 Bommasani, R , 10.48550/ARXIV.2108.072582021</p>
<p>ChatGPT goes to law school. J H Choi, K E Hickman, A Monahan, D B Schwarcz, 10.2139/ssrn.4335905SSRN J. 4335952023</p>
<p>Would Chat GPT get a Wharton MBA? A prediction based on its performance in the operations management course. C Terwiesch, 2023</p>
<p>Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. T H Kung, PLOS Digit Health. 2e00001982023</p>
<p>Language models are few-shot learners. T B Brown, 10.48550/ARXIV.2005.141652020</p>
<p>PaLM: Scaling language modeling with pathways. A Chowdhery, 10.48550/ARXIV.2204.023112022</p>
<p>Sparks of artificial general intelligence: Early experiments with GPT-4. S Bubeck, 10.48550/ARXIV.2303.127122023</p>
<p>What Do NLP Researchers Believe? Results of the NLP Community Metasurvey. J Michael, 10.48550/ARXIV.2208.128522022</p>
<p>The debate over understanding in AI's large language models. M Mitchell, D C Krakauer, Proc. Natl. Acad. Sci. U.S.A. 120e22159071202023</p>
<p>Hallucinations in neural machine translation. K Lee, O Firat, A Agarwal, C Fannjiang, D Sussillo, 2018</p>
<p>The Curious Case of Hallucinations in Neural Machine Translation. V Raunak, A Menezes, M Junczys-Dowmunt, 10.18653/v1/2021.naacl-main.92Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies 1172-1183. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies 1172-1183Association for Computational Linguistics2021</p>
<p>Dissociating language and thought in large language models. K Mahowald, 10.48550/ARXIV.2301.066272023</p>
<p>Machine Reading, Fast and Slow: When Do Models 'Understand' Language?. S R Choudhury, A Rogers, I Augenstein, 10.48550/ARXIV.2209.074302022</p>
<p>Competency problems: On finding and removing artifacts in language data. M Gardner, 10.48550/ARXIV.2104.086462021</p>
<p>How can we accelerate progress towards human-like linguistic generalization?. T Linzen, 10.48550/ARXIV.2005.009552020</p>
<p>AI and the limits of language. J Browning, Y Lecun, Noema. 2022</p>
<p>Relation and lexical priming during the interpretation of noun-noun combinations. C L Gagné, J. Exp. Psychol. Learn. Mem. Cognit. 272001</p>
<p>Constituent integration during the processing of compound words: Does it involve the use of relational structures?. C L Gagné, T L Spalding, J. Mem. Lang. 602009</p>
<p>Neural correlates of implicit and explicit combinatorial semantic processing. W W Graves, J R Binder, R H Desai, L L Conant, M S Seidenberg, NeuroImage. 532010</p>
<p>The neural basis of combinatory syntax and semantics. L Pylkkänen, Science. 3662019</p>
<p>Noun-noun combination: Meaningfulness ratings and lexical statistics for 2,160 word pairs. W W Graves, J R Binder, M S Seidenberg, Behav Res. 452013</p>
<p>Comparing an individual's test score against norms derived from small samples. J R Crawford, D C Howell, Clin. Neuropsychol. 121998</p>
<p>Efficient Estimation of Word Representations in Vector Space. T Mikolov, K Chen, G Corrado, J Dean, 10.48550/ARXIV.1301.37812013Preprint at</p>
<p>Glove: Global Vectors for Word Representation. J Pennington, R Socher, C Manning, 10.3115/v1/D14-1162Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational Linguistics2014</p>
<p>Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment. S Roller, K Erk, 10.18653/v1/D16-1234Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing 2163-2172. the 2016 Conference on Empirical Methods in Natural Language Processing 2163-2172Austin, TexasAssociation for Computational Linguistics2016</p>
<p>SCOPE: The South Carolina psycholinguistic metabase. C Gao, S V Shinkareva, R H Desai, Behav Res. 552022</p>
<p>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. A Srivastava, 10.48550/ARXIV.2206.046152022</p>
<p>Faith and Fate: Limits of Transformers on Compositionality. N Dziri, 10.48550/ARXIV.2305.186542023</p>
<p>ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models. N Bian, 10.48550/ARXIV.2303.164212023</p>
<p>P Koralus, V Wang-Maścianica, 10.48550/ARXIV.2303.17276Humans Out: On GPT Converging Toward Common Sense in both Success and Failure. 2023</p>
<p>Is ChatGPT a General-Purpose Natural Language Processing Task Solver. C Qin, 10.18653/v1/2023.emnlp-main.85Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing 1339-1384. the 2023 Conference on Empirical Methods in Natural Language Processing 1339-1384SingaporeAssociation for Computational Linguistics2023</p>
<p>Conceptual combination in the LATL with and without syntactic composition. A Parrish, L Pylkkänen, Neurobiol. Lang. 32022</p>
<p>10.1038/s41598-024-72528-3(1234567890) Scientific Reports | (2024) 14:21593 |. </p>            </div>
        </div>

    </div>
</body>
</html>
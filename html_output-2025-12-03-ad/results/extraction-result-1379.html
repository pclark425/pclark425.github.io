<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1379 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1379</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1379</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-67b950f8d31e1b5dfed49993ef4d809e3bf0cc2c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/67b950f8d31e1b5dfed49993ef4d809e3bf0cc2c" target="_blank">Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> VMG can outperform state-of-the-art offline RL methods in several goal-oriented tasks, especially when environments have sparse rewards and long temporal horizons.</p>
                <p><strong>Paper Abstract:</strong> Reinforcement Learning (RL) methods are typically applied directly in environments to learn policies. In some complex environments with continuous state-action spaces, sparse rewards, and/or long temporal horizons, learning a good policy in the original environments can be difficult. Focusing on the offline RL setting, we aim to build a simple and discrete world model that abstracts the original environment. RL methods are applied to our world model instead of the environment data for simplified policy learning. Our world model, dubbed Value Memory Graph (VMG), is designed as a directed-graph-based Markov decision process (MDP) of which vertices and directed edges represent graph states and graph actions, separately. As state-action spaces of VMG are finite and relatively small compared to the original environment, we can directly apply the value iteration algorithm on VMG to estimate graph state values and figure out the best graph actions. VMG is trained from and built on the offline RL dataset. Together with an action translator that converts the abstract graph actions in VMG to real actions in the original environment, VMG controls agents to maximize episode returns. Our experiments on the D4RL benchmark show that VMG can outperform state-of-the-art offline RL methods in several goal-oriented tasks, especially when environments have sparse rewards and long temporal horizons. Code is available at https://github.com/TsuTikgiau/ValueMemoryGraph</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1379.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1379.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VMG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Value Memory Graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-structured world model built from offline RL datasets that represents the environment as a discrete directed-graph MDP (vertices = graph states, edges = graph actions), learned via contrastive metric-space embeddings and state/action encoders; policy planning is done by value iteration on the graph and converted to environment actions via a learned action translator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Value Memory Graph (VMG)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VMG constructs a metric embedding (state encoder Enc_s, action encoder Enc_a) where L2 distance encodes short-horizon reachability; episodes are mapped to chains in this space and similar embedded states are merged into vertices (threshold γ_m) to form a directed graph. Each directed edge corresponds to a graph action with rewards aggregated from dataset transitions (including split internal rewards). The graph MDP (finite discrete S_G, A_G) allows classical value iteration to compute vertex values; at runtime multi-step search and Dijkstra planning select a graph edge, and an action translator (supervised regressor conditioned on current & target future state) outputs environment actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>graph-structured discrete world model / abstract MDP</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline reinforcement learning on D4RL benchmark domains (robotic manipulation: Kitchen and Adroit; navigation: AntMaze)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Indirectly measured via (1) contrastive prediction loss L_c (s+a -> predicted next-state embedding) and (2) action reconstruction loss L_a (reconstruct action from transition embedding); and (3) downstream task performance (D4RL normalized returns). No explicit next-state pixel/state-frame MSE metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No direct numeric next-state prediction accuracy reported; metric-learning losses tracked during training (contrastive & action loss curves). Downstream task performance (D4RL normalized scores) indicates effective fidelity for planning: examples -- kitchen-complete: 73.0±6.7; kitchen-partial: 68.8±11.9; antmaze-umaze: 93.7±2.3; antmaze-total: 496.3 (sum over AntMaze tasks); adroit-total: 138.9. Ablations show removing contrastive loss yields near-zero task performance on some tasks, indicating fidelity of the learned metric is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High: graph topology and per-vertex values are visualized and mapped back to environment coordinates (UMAP for embedding reduction), showing that high-value vertices concentrate near task goals and the graph mirrors environment topology (e.g., maze corridors). Graph vertices and value heatmaps provide explicit, human-interpretable planning structure.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>UMAP projection of learned state embeddings to 2D; visualization of graph vertices colored by value; mapping vertices to physical locations in the environment; explicit inspection of graph topology and per-edge rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training: ~1.5 hours on a single RTX Titan GPU (training networks for 800 epochs, batch size 100). Graph building (state merging / clustering) takes ~0.5–2 minutes; constructing the graph from embeddings and clustering can take 0.5–2 minutes (depending on dataset) before evaluation. Value iteration on the discrete graph takes <1 second. Inference: evaluating 100 episodes takes ~0.5–10 minutes (includes action translator and environment stepping). Model architectures are small (3-layer MLPs, hidden size 256) for encoders/decoders/translator.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to training a neural value function on the original continuous space, VMG allows direct classical value iteration (<1s) because of the discrete small graph; authors report faster planning and simpler policy derivation (no additional value-network training). Empirical task efficiency: VMG outperforms several state-of-the-art offline RL baselines (CQL, IQL) on many sparse long-horizon tasks, indicating better utility per dataset in those settings, but explicit compute vs baseline numbers (FLOPs, wall-clock for baselines) are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong on sparse long-horizon tasks: e.g., kitchen-complete 73.0±6.7 (better than CQL 43.8, IQL 62.5), kitchen-partial 68.8±11.9 (outperforms baselines), antmaze-umaze 93.7±2.3 (IQL 87.5), antmaze-total 496.3 (higher than baselines reported). On Adroit (high-dim action dense reward) performance is competitive but not uniformly superior (adroit-total 138.9). On gym locomotion tasks VMG performs close to behavior cloning and worse than top offline RL methods in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>VMG's abstraction prioritizes task-relevant, reachable-state structure (short-horizon reachability in embedding) over precise predictive fidelity of every low-level state; this yields high utility for sparse-reward, long-horizon planning (AntMaze, Kitchen). For dense-reward, low-horizon, or tasks requiring precise low-level dynamics (some Adroit / gym locomotion tasks), VMG's abstract planning plus a behavior-cloned action translator can limit performance.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs include: (1) Interpretability and fast planning via discrete graph vs loss of fine-grained dynamics (one graph edge may correspond to multiple environment steps); (2) Larger merging threshold γ_m reduces graph size (improves efficiency) but may lose information and reduce task performance; (3) VMG does not hallucinate new edges (conservative: stays within dataset transitions) which improves safety/OOD avoidance but limits generalization to unseen transitions; (4) Action translator trained via supervised conditioned BC is simple and efficient but can be suboptimal for tasks that require precise dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Contrastive metric learning for reachability (Enc_s, Enc_a) with margin m; action decoder to reconstruct actions from transition embeddings; state merging by fixed radius γ_m into vertices (hyperspheres); define graph transition rewards by averaging dataset rewards plus split internal rewards; graph MDP with deterministic P_G for existing edges; planning via value iteration and multi-step search with Dijkstra path planning; action translator trained by supervised regression conditioned on (s_current, s_future) with K (future range) typically 10.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to conventional latent predictive world models that explicitly predict next states (e.g., VAE-based latent simulators) VMG focuses on discrete abstract MDP semantics and planning via exact value iteration, trading fine-grained predictive fidelity for faster, interpretable planning. Empirically VMG outperforms model-free offline RL baselines (CQL, IQL) on several sparse long-horizon benchmarks in D4RL, while performance is comparable or worse on domains requiring high-fidelity low-level control.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper provides practical hyperparameter recommendations: metric-space dim = 10 is reasonable; margin m ≈ 1 works well across tasks; γ_m tuned per domain (e.g., 0.5 for Kitchen, 0.8 for AntMaze, 0.3 for Adroit) to balance graph size vs fidelity; K (action-translator future horizon) around 10; multi-step search and action decoder are recommended (ablation shows they improve performance). The authors emphasize tuning γ_m and m per domain to balance fidelity, interpretability (graph size), and planning efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1379.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1379.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Models (Ha & Schmidhuber, 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent predictive world-model approach that learns compact latent dynamics (typically VAE+RNN) to model environment transitions and then uses the latent model for planning or policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>World models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced in this paper as an example of model-based RL where world models are designed to approximate original environment transitions (latent predictive models). The original paper uses generative latent models to capture environment dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent predictive world model (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General RL domains (original work demonstrated on simulated environments); in this paper referenced generally as prior MBRL work</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Cited as representative of methods that approximate environment transitions; contrasted with VMG which abstracts to a simpler graph MDP rather than explicit transition prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Paper contrasts such latent transition-approximation approaches with VMG's graph abstraction: VMG does not aim to model exact per-step dynamics but to provide a simpler discrete planning substrate.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1379.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1379.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer / Dream to Control</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dream to Control: Learning behaviors by latent imagination (Hafner et al., 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Latent-imagination based model-based RL approach (Dreamer) that learns a latent dynamics model and optimizes policies by planning/imagination in the latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer (latent imagination model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mentioned as an example of recent model-based RL that uses latent world models for sample-efficient learning by imagining trajectories in latent space; specifics are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / imagination-based MBRL</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General control tasks (cited broadly)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Referenced as part of the model-based literature that shows advantages in sample efficiency, contrasted with VMG's different abstraction focus.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted at a high level: Dreamer focuses on latent predictive fidelity and imagination for policy learning, while VMG focuses on discrete graph abstraction enabling exact planning via value iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1379.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1379.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOPO: Model-based Offline Policy Optimization (Yu et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An offline model-based RL approach that learns an ensemble dynamics model and penalizes model uncertainty to constrain policy learning to reliable regions of the learned model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mopo: Model-based offline policy optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MOPO</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cited as a model-based offline RL method that constrains the policy to regions where the learned world model is reliable (via uncertainty penalties); details not expanded in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>uncertainty-aware learned dynamics model (model-based offline RL)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline RL benchmarks (referenced generally)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Used to illustrate prior work that constrains policy using world-model uncertainty; VMG instead remains conservative by using only dataset-derived transitions (no imagined novel edges).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1379.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1379.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COMBO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COMBO: Conservative Offline Model-Based Policy Optimization (Yu et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An offline model-based RL algorithm that uses a conservative model-based value estimation to avoid exploiting model errors in offline RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Combo: Conservative offline model-based policy optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>COMBO</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mentioned among model-based offline RL works that address OOD/model-error issues; specifics are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>conservative model-based offline RL</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline RL benchmarks (cited generally)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Referenced to contrast approaches that constrain a learned model vs VMG's conservative dataset-derived graph which naturally stays within training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1379.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1379.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sparse Graphical Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Graphical Memory for Robust Planning (Emmons et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph/memory-based planning approach that stores sparse experience as graph memory to enable robust planning; used as related work on graph-from-experience approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sparse graphical memory for robust planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sparse Graphical Memory</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cited as prior work that uses graph memories for planning from experience; paper notes similarity to VMG in using graph/exemplar memories for planning, but earlier works differ in discrete vs continuous action handling and stitching new transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>graph memory / experience graph</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Planning from replayed experience; referenced broadly</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Referenced to position VMG among graph-based memory/planning approaches; VMG distinguishes itself by discretizing both state and action spaces to handle high-dimensional continuous actions.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1379.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1379.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World-model-as-graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World model as a graph: Learning latent landmarks for planning (Zhang et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-style world model learning latent landmarks to plan; referenced as related work that also uses graph abstractions for planning in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>World model as a graph: Learning latent landmarks for planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World model as a graph (latent landmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cited as a related method that learns latent landmarks for planning on a graph; the current paper notes similarity but emphasizes VMG's specific metric learning, state merging, graph-MDP definition and value-iteration-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent landmark / graph-based world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Planning/navigation (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Included as a relevant graph-world-model prior; VMG differentiates itself by discrete graph MDP with exact value iteration and explicit action translator for high-dim continuous actions.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1379.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1379.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BATS (Stitching)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BATS: Best Action Trajectory Stitching (Char et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent approach that stitches trajectories to create new transitions in an experience graph (stitch operator), useful in discrete or low-dimensional action settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bats: Best action trajectory stitching</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BATS (trajectory stitching)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as a method that creates new transitions via stitching; the paper notes limitations of the stitch operator for high-dimensional action spaces, which VMG addresses by discretizing both states and actions and using an action translator.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>graph stitching / trajectory stitching method</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Low-dimensional continuous-action or discrete-action environments (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Mentioned to highlight that stitching approaches may not scale to high-dimensional actions whereas VMG discretizes actions and uses learned translator to handle high-dimensional continuous actions.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Mopo: Model-based offline policy optimization <em>(Rating: 2)</em></li>
                <li>Combo: Conservative offline model-based policy optimization <em>(Rating: 2)</em></li>
                <li>Sparse graphical memory for robust planning <em>(Rating: 2)</em></li>
                <li>World model as a graph: Learning latent landmarks for planning <em>(Rating: 2)</em></li>
                <li>Bats: Best action trajectory stitching <em>(Rating: 1)</em></li>
                <li>When to trust your model: Model-based policy optimization <em>(Rating: 1)</em></li>
                <li>Mastering atari games with limited data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1379",
    "paper_id": "paper-67b950f8d31e1b5dfed49993ef4d809e3bf0cc2c",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "VMG",
            "name_full": "Value Memory Graph",
            "brief_description": "A graph-structured world model built from offline RL datasets that represents the environment as a discrete directed-graph MDP (vertices = graph states, edges = graph actions), learned via contrastive metric-space embeddings and state/action encoders; policy planning is done by value iteration on the graph and converted to environment actions via a learned action translator.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Value Memory Graph (VMG)",
            "model_description": "VMG constructs a metric embedding (state encoder Enc_s, action encoder Enc_a) where L2 distance encodes short-horizon reachability; episodes are mapped to chains in this space and similar embedded states are merged into vertices (threshold γ_m) to form a directed graph. Each directed edge corresponds to a graph action with rewards aggregated from dataset transitions (including split internal rewards). The graph MDP (finite discrete S_G, A_G) allows classical value iteration to compute vertex values; at runtime multi-step search and Dijkstra planning select a graph edge, and an action translator (supervised regressor conditioned on current & target future state) outputs environment actions.",
            "model_type": "graph-structured discrete world model / abstract MDP",
            "task_domain": "Offline reinforcement learning on D4RL benchmark domains (robotic manipulation: Kitchen and Adroit; navigation: AntMaze)",
            "fidelity_metric": "Indirectly measured via (1) contrastive prediction loss L_c (s+a -&gt; predicted next-state embedding) and (2) action reconstruction loss L_a (reconstruct action from transition embedding); and (3) downstream task performance (D4RL normalized returns). No explicit next-state pixel/state-frame MSE metrics reported.",
            "fidelity_performance": "No direct numeric next-state prediction accuracy reported; metric-learning losses tracked during training (contrastive & action loss curves). Downstream task performance (D4RL normalized scores) indicates effective fidelity for planning: examples -- kitchen-complete: 73.0±6.7; kitchen-partial: 68.8±11.9; antmaze-umaze: 93.7±2.3; antmaze-total: 496.3 (sum over AntMaze tasks); adroit-total: 138.9. Ablations show removing contrastive loss yields near-zero task performance on some tasks, indicating fidelity of the learned metric is critical.",
            "interpretability_assessment": "High: graph topology and per-vertex values are visualized and mapped back to environment coordinates (UMAP for embedding reduction), showing that high-value vertices concentrate near task goals and the graph mirrors environment topology (e.g., maze corridors). Graph vertices and value heatmaps provide explicit, human-interpretable planning structure.",
            "interpretability_method": "UMAP projection of learned state embeddings to 2D; visualization of graph vertices colored by value; mapping vertices to physical locations in the environment; explicit inspection of graph topology and per-edge rewards.",
            "computational_cost": "Training: ~1.5 hours on a single RTX Titan GPU (training networks for 800 epochs, batch size 100). Graph building (state merging / clustering) takes ~0.5–2 minutes; constructing the graph from embeddings and clustering can take 0.5–2 minutes (depending on dataset) before evaluation. Value iteration on the discrete graph takes &lt;1 second. Inference: evaluating 100 episodes takes ~0.5–10 minutes (includes action translator and environment stepping). Model architectures are small (3-layer MLPs, hidden size 256) for encoders/decoders/translator.",
            "efficiency_comparison": "Compared to training a neural value function on the original continuous space, VMG allows direct classical value iteration (&lt;1s) because of the discrete small graph; authors report faster planning and simpler policy derivation (no additional value-network training). Empirical task efficiency: VMG outperforms several state-of-the-art offline RL baselines (CQL, IQL) on many sparse long-horizon tasks, indicating better utility per dataset in those settings, but explicit compute vs baseline numbers (FLOPs, wall-clock for baselines) are not provided.",
            "task_performance": "Strong on sparse long-horizon tasks: e.g., kitchen-complete 73.0±6.7 (better than CQL 43.8, IQL 62.5), kitchen-partial 68.8±11.9 (outperforms baselines), antmaze-umaze 93.7±2.3 (IQL 87.5), antmaze-total 496.3 (higher than baselines reported). On Adroit (high-dim action dense reward) performance is competitive but not uniformly superior (adroit-total 138.9). On gym locomotion tasks VMG performs close to behavior cloning and worse than top offline RL methods in many cases.",
            "task_utility_analysis": "VMG's abstraction prioritizes task-relevant, reachable-state structure (short-horizon reachability in embedding) over precise predictive fidelity of every low-level state; this yields high utility for sparse-reward, long-horizon planning (AntMaze, Kitchen). For dense-reward, low-horizon, or tasks requiring precise low-level dynamics (some Adroit / gym locomotion tasks), VMG's abstract planning plus a behavior-cloned action translator can limit performance.",
            "tradeoffs_observed": "Trade-offs include: (1) Interpretability and fast planning via discrete graph vs loss of fine-grained dynamics (one graph edge may correspond to multiple environment steps); (2) Larger merging threshold γ_m reduces graph size (improves efficiency) but may lose information and reduce task performance; (3) VMG does not hallucinate new edges (conservative: stays within dataset transitions) which improves safety/OOD avoidance but limits generalization to unseen transitions; (4) Action translator trained via supervised conditioned BC is simple and efficient but can be suboptimal for tasks that require precise dynamics.",
            "design_choices": "Contrastive metric learning for reachability (Enc_s, Enc_a) with margin m; action decoder to reconstruct actions from transition embeddings; state merging by fixed radius γ_m into vertices (hyperspheres); define graph transition rewards by averaging dataset rewards plus split internal rewards; graph MDP with deterministic P_G for existing edges; planning via value iteration and multi-step search with Dijkstra path planning; action translator trained by supervised regression conditioned on (s_current, s_future) with K (future range) typically 10.",
            "comparison_to_alternatives": "Compared to conventional latent predictive world models that explicitly predict next states (e.g., VAE-based latent simulators) VMG focuses on discrete abstract MDP semantics and planning via exact value iteration, trading fine-grained predictive fidelity for faster, interpretable planning. Empirically VMG outperforms model-free offline RL baselines (CQL, IQL) on several sparse long-horizon benchmarks in D4RL, while performance is comparable or worse on domains requiring high-fidelity low-level control.",
            "optimal_configuration": "Paper provides practical hyperparameter recommendations: metric-space dim = 10 is reasonable; margin m ≈ 1 works well across tasks; γ_m tuned per domain (e.g., 0.5 for Kitchen, 0.8 for AntMaze, 0.3 for Adroit) to balance graph size vs fidelity; K (action-translator future horizon) around 10; multi-step search and action decoder are recommended (ablation shows they improve performance). The authors emphasize tuning γ_m and m per domain to balance fidelity, interpretability (graph size), and planning efficiency.",
            "uuid": "e1379.0",
            "source_info": {
                "paper_title": "Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "World Models",
            "name_full": "World Models (Ha & Schmidhuber, 2018)",
            "brief_description": "A latent predictive world-model approach that learns compact latent dynamics (typically VAE+RNN) to model environment transitions and then uses the latent model for planning or policy learning.",
            "citation_title": "World models",
            "mention_or_use": "mention",
            "model_name": "World Models",
            "model_description": "Referenced in this paper as an example of model-based RL where world models are designed to approximate original environment transitions (latent predictive models). The original paper uses generative latent models to capture environment dynamics.",
            "model_type": "latent predictive world model (referenced)",
            "task_domain": "General RL domains (original work demonstrated on simulated environments); in this paper referenced generally as prior MBRL work",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": null,
            "interpretability_method": null,
            "computational_cost": null,
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Cited as representative of methods that approximate environment transitions; contrasted with VMG which abstracts to a simpler graph MDP rather than explicit transition prediction.",
            "tradeoffs_observed": null,
            "design_choices": null,
            "comparison_to_alternatives": "Paper contrasts such latent transition-approximation approaches with VMG's graph abstraction: VMG does not aim to model exact per-step dynamics but to provide a simpler discrete planning substrate.",
            "optimal_configuration": null,
            "uuid": "e1379.1",
            "source_info": {
                "paper_title": "Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Dreamer / Dream to Control",
            "name_full": "Dream to Control: Learning behaviors by latent imagination (Hafner et al., 2019)",
            "brief_description": "Latent-imagination based model-based RL approach (Dreamer) that learns a latent dynamics model and optimizes policies by planning/imagination in the latent space.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "mention",
            "model_name": "Dreamer (latent imagination model)",
            "model_description": "Mentioned as an example of recent model-based RL that uses latent world models for sample-efficient learning by imagining trajectories in latent space; specifics are not detailed in this paper.",
            "model_type": "latent world model / imagination-based MBRL",
            "task_domain": "General control tasks (cited broadly)",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": null,
            "interpretability_method": null,
            "computational_cost": null,
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Referenced as part of the model-based literature that shows advantages in sample efficiency, contrasted with VMG's different abstraction focus.",
            "tradeoffs_observed": null,
            "design_choices": null,
            "comparison_to_alternatives": "Contrasted at a high level: Dreamer focuses on latent predictive fidelity and imagination for policy learning, while VMG focuses on discrete graph abstraction enabling exact planning via value iteration.",
            "optimal_configuration": null,
            "uuid": "e1379.2",
            "source_info": {
                "paper_title": "Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "MOPO",
            "name_full": "MOPO: Model-based Offline Policy Optimization (Yu et al., 2020)",
            "brief_description": "An offline model-based RL approach that learns an ensemble dynamics model and penalizes model uncertainty to constrain policy learning to reliable regions of the learned model.",
            "citation_title": "Mopo: Model-based offline policy optimization",
            "mention_or_use": "mention",
            "model_name": "MOPO",
            "model_description": "Cited as a model-based offline RL method that constrains the policy to regions where the learned world model is reliable (via uncertainty penalties); details not expanded in this paper.",
            "model_type": "uncertainty-aware learned dynamics model (model-based offline RL)",
            "task_domain": "Offline RL benchmarks (referenced generally)",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": null,
            "interpretability_method": null,
            "computational_cost": null,
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Used to illustrate prior work that constrains policy using world-model uncertainty; VMG instead remains conservative by using only dataset-derived transitions (no imagined novel edges).",
            "tradeoffs_observed": null,
            "design_choices": null,
            "comparison_to_alternatives": null,
            "optimal_configuration": null,
            "uuid": "e1379.3",
            "source_info": {
                "paper_title": "Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "COMBO",
            "name_full": "COMBO: Conservative Offline Model-Based Policy Optimization (Yu et al., 2021)",
            "brief_description": "An offline model-based RL algorithm that uses a conservative model-based value estimation to avoid exploiting model errors in offline RL.",
            "citation_title": "Combo: Conservative offline model-based policy optimization",
            "mention_or_use": "mention",
            "model_name": "COMBO",
            "model_description": "Mentioned among model-based offline RL works that address OOD/model-error issues; specifics are not provided in this paper.",
            "model_type": "conservative model-based offline RL",
            "task_domain": "Offline RL benchmarks (cited generally)",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": null,
            "interpretability_method": null,
            "computational_cost": null,
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Referenced to contrast approaches that constrain a learned model vs VMG's conservative dataset-derived graph which naturally stays within training distribution.",
            "tradeoffs_observed": null,
            "design_choices": null,
            "comparison_to_alternatives": null,
            "optimal_configuration": null,
            "uuid": "e1379.4",
            "source_info": {
                "paper_title": "Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Sparse Graphical Memory",
            "name_full": "Sparse Graphical Memory for Robust Planning (Emmons et al., 2020)",
            "brief_description": "A graph/memory-based planning approach that stores sparse experience as graph memory to enable robust planning; used as related work on graph-from-experience approaches.",
            "citation_title": "Sparse graphical memory for robust planning",
            "mention_or_use": "mention",
            "model_name": "Sparse Graphical Memory",
            "model_description": "Cited as prior work that uses graph memories for planning from experience; paper notes similarity to VMG in using graph/exemplar memories for planning, but earlier works differ in discrete vs continuous action handling and stitching new transitions.",
            "model_type": "graph memory / experience graph",
            "task_domain": "Planning from replayed experience; referenced broadly",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": null,
            "interpretability_method": null,
            "computational_cost": null,
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Referenced to position VMG among graph-based memory/planning approaches; VMG distinguishes itself by discretizing both state and action spaces to handle high-dimensional continuous actions.",
            "tradeoffs_observed": null,
            "design_choices": null,
            "comparison_to_alternatives": null,
            "optimal_configuration": null,
            "uuid": "e1379.5",
            "source_info": {
                "paper_title": "Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "World-model-as-graph",
            "name_full": "World model as a graph: Learning latent landmarks for planning (Zhang et al., 2021)",
            "brief_description": "A graph-style world model learning latent landmarks to plan; referenced as related work that also uses graph abstractions for planning in latent space.",
            "citation_title": "World model as a graph: Learning latent landmarks for planning",
            "mention_or_use": "mention",
            "model_name": "World model as a graph (latent landmarks)",
            "model_description": "Cited as a related method that learns latent landmarks for planning on a graph; the current paper notes similarity but emphasizes VMG's specific metric learning, state merging, graph-MDP definition and value-iteration-based planning.",
            "model_type": "latent landmark / graph-based world model",
            "task_domain": "Planning/navigation (referenced)",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": null,
            "interpretability_method": null,
            "computational_cost": null,
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Included as a relevant graph-world-model prior; VMG differentiates itself by discrete graph MDP with exact value iteration and explicit action translator for high-dim continuous actions.",
            "tradeoffs_observed": null,
            "design_choices": null,
            "comparison_to_alternatives": null,
            "optimal_configuration": null,
            "uuid": "e1379.6",
            "source_info": {
                "paper_title": "Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "BATS (Stitching)",
            "name_full": "BATS: Best Action Trajectory Stitching (Char et al., 2022)",
            "brief_description": "A recent approach that stitches trajectories to create new transitions in an experience graph (stitch operator), useful in discrete or low-dimensional action settings.",
            "citation_title": "Bats: Best action trajectory stitching",
            "mention_or_use": "mention",
            "model_name": "BATS (trajectory stitching)",
            "model_description": "Referenced as a method that creates new transitions via stitching; the paper notes limitations of the stitch operator for high-dimensional action spaces, which VMG addresses by discretizing both states and actions and using an action translator.",
            "model_type": "graph stitching / trajectory stitching method",
            "task_domain": "Low-dimensional continuous-action or discrete-action environments (cited)",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": null,
            "interpretability_method": null,
            "computational_cost": null,
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Mentioned to highlight that stitching approaches may not scale to high-dimensional actions whereas VMG discretizes actions and uses learned translator to handle high-dimensional continuous actions.",
            "tradeoffs_observed": null,
            "design_choices": null,
            "comparison_to_alternatives": null,
            "optimal_configuration": null,
            "uuid": "e1379.7",
            "source_info": {
                "paper_title": "Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "World models",
            "rating": 2
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2
        },
        {
            "paper_title": "Mopo: Model-based offline policy optimization",
            "rating": 2
        },
        {
            "paper_title": "Combo: Conservative offline model-based policy optimization",
            "rating": 2
        },
        {
            "paper_title": "Sparse graphical memory for robust planning",
            "rating": 2
        },
        {
            "paper_title": "World model as a graph: Learning latent landmarks for planning",
            "rating": 2
        },
        {
            "paper_title": "Bats: Best action trajectory stitching",
            "rating": 1
        },
        {
            "paper_title": "When to trust your model: Model-based policy optimization",
            "rating": 1
        },
        {
            "paper_title": "Mastering atari games with limited data",
            "rating": 1
        }
    ],
    "cost": 0.0173345,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement LEARNING</h1>
<p>Deyao Zhu ${ }^{1}$ Li Erran $\mathbf{L i}^{2 *}$ Mohamed Elhoseiny ${ }^{1}$<br>${ }^{1}$ King Abdullah University of Science and Technology<br>${ }^{2}$ AWS AI, Amazon and Columbia University<br>{deyao.zhu, mohamed.elhoseiny}@kaust.edu.sa, erranlli@gmail.com</p>
<h4>Abstract</h4>
<p>Reinforcement Learning (RL) methods are typically applied directly in environments to learn policies. In some complex environments with continuous state-action spaces, sparse rewards, and/or long temporal horizons, learning a good policy in the original environments can be difficult. Focusing on the offline RL setting, we aim to build a simple and discrete world model that abstracts the original environment. RL methods are applied to our world model instead of the environment data for simplified policy learning. Our world model, dubbed Value Memory Graph (VMG), is designed as a directed-graph-based Markov decision process (MDP) of which vertices and directed edges represent graph states and graph actions, separately. As state-action spaces of VMG are finite and relatively small compared to the original environment, we can directly apply the value iteration algorithm on VMG to estimate graph state values and figure out the best graph actions. VMG is trained from and built on the offline RL dataset. Together with an action translator that converts the abstract graph actions in VMG to real actions in the original environment, VMG controls agents to maximize episode returns. Our experiments on the D4RL benchmark show that VMG can outperform state-of-the-art offline RL methods in several goal-oriented tasks, especially when environments have sparse rewards and long temporal horizons. Code is available at https://github.com/TsuTikgiau/ValueMemoryGraph</p>
<h2>1 INTRODUCTION</h2>
<p>Humans are usually good at simplifying difficult problems into easier ones by ignoring trivial details and focusing on important information for decision making. Typically, reinforcement learning (RL) methods are directly applied in the original environment to learn a policy. When we have a difficult environment like robotics or video games with long temporal horizons, sparse reward signals, or large and continuous state-action space, it becomes more challenging for RL methods to reason the value of states or actions in the original environment to get a well-performing policy. Learning a world model that simplifies the original complex environment into an easy version might lower the difficulty to learn a policy and lead to better performance.</p>
<p>In offline reinforcement learning, algorithms can access a dataset consisting of pre-collected episodes to learn a policy without interacting with the environment. Usually, the offline dataset is used as a replay buffer to train a policy in an off-policy way with additional constraints to avoid distribution shift problems (Wu et al., 2019; Fujimoto et al., 2019; Kumar et al., 2019; Nair et al., 2020; Wang et al., 2020; Peng et al., 2019). As the episodes also contain the dynamics information of the original environment, it is possible to utilize such a dataset to directly learn an abstraction of the environment in the offline RL setting. To this end, we introduce Value Memory Graph (VMG), a graph-structured world model for offline reinforcement learning tasks. VMG is a Markov decision process (MDP) defined on a graph as an abstract of the original environment. Instead of directly applying RL methods to the offline dataset collected in the original environment, we learn and build VMG first and use</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Demonstration of a successful episode where a robot trained in the dataset "kitchen-partial" accomplishes 4 subtasks in sequence guided by VMG. Vertex values are shown via color shade. By searching graph actions that lead to the high-value future region (darker blue) calculated by value iteration on the graph, VMG controls the robot arm to maximize episode rewards and finish the task.
it as a simplified substitute of the environment to apply RL methods. VMG is built by mapping offline episodes to directed chains in a metric space trained via contrastive learning. Then, these chains are connected to a graph via state merging. Vertices and directed edges of VMG are viewed as graph states and graph actions. Each vertex transition on VMG has rewards defined from the original rewards in the environment.</p>
<p>To control agents in environments, we first run the classical value iteration algorithm(Puterman, 2014) once on VMG to calculate graph state values. This can be done in less than one second without training a value neural network thanks to the discrete and relatively smaller state and action spaces in VMG. At each timestep, VMG is used to search for graph actions that can lead to high-value future states. Graph actions are directed edges and cannot be directly executed in the original environment. With the help of an action translator trained in supervised learning (e.g., Emmons et al. (2021)) using the same offline dataset, the searched graph actions are converted to environment actions to control the agent. An overview of our method is shown in Fig.1.</p>
<p>Our contribution can be summarized as follows:</p>
<ul>
<li>We present Value Memory Graph (VMG), a graph-structured world model in offline reinforcement learning setting. VMG represents the original environments as a graph-based MDP with relatively small and discrete action and state spaces.</li>
<li>We design a method to learn and build VMG on an offline dataset via contrastive learning and state merging.</li>
<li>We introduce a VMG-based method to control agents by reasoning graph actions that lead to high-value future states via value iteration and convert them to environment actions via an action translator.</li>
<li>Experiments on the D4RL benchmark show that VMG can outperform several state-of-theart offline RL methods on several goal-oriented tasks with sparse rewards and long temporal horizons.</li>
</ul>
<h1>2 Related Work</h1>
<p>Offline Reinforcement Learning One crucial problem in offline RL is how to avoid out-of-the-training-distribution (OOD) actions and states that decrease the performance in test environments (Fujimoto et al., 2019; Kumar et al., 2019; Levine et al., 2020). Recent works like Wu et al. (2019); Fujimoto et al. (2019); Kumar et al. (2019); Nair et al. (2020); Wang et al. (2020); Peng et al. (2019) directly penalize the mismatch between the trained policy and the behavior policy via an explicit density model or via divergence. Another methods like Kumar et al. (2020); Kostrikov et al. (2021a;b) constrains the training via penalizing the Q function. Model-based reinforcement learning methods like Yu et al. (2020; 2021); Kidambi et al. (2020) constrains the policy to the region of the world model that is close to the training data. Compared to previous methods, graph actions in VMG always control agents to move to graph states, and all the graph states come from the training dataset. Therefore, agents stay close to states from the training distribution naturally.</p>
<p>Hierarchical Reinforcement Learning Hierarchical RL methods (e.g., Savinov et al. (2018); Nachum et al. (2018); Eysenbach et al. (2019); Huang et al. (2019); Liu et al. (2020); Mandlekar et al. (2020); Yang et al. (2020); Emmons et al. (2020); Zhang et al. (2021)) use hierarchical policies to control agents with a high-level policy that generate commands like abstract actions or skills, and a low-level policy that converts them to concrete environment actions. Our method can be viewed as a hierarchical RL approach with VMG-based high-level policy and a low-level action translator. Compared to previous methods which learn high-level policies in environments, our high-level policy is instead trained in VMG via value iteration without additional neural network learning.</p>
<p>Model-based Reinforcement Learning Recent research in model-based reinforcement learning (MBRL) has shown a significant advantage (Ha \&amp; Schmidhuber, 2018; Janner et al., 2019; Hafner et al., 2019; Schrittwieser et al., 2020; Ye et al., 2021) in sample efficiency over model-free reinforcement learning. In most of the previous methods, world models are designed to approximate the original environment transition. In contrast, VMG abstracts the environment as a simple graph-based MDP. Therefore, we can apply RL methods directly to VMG for simple and fast policy learning. As we demonstrate later in our experiments, this facilitates reasoning and leads to good performance in tasks with long temporal horizons and sparse rewards.</p>
<p>Graph from Experience Similar to VMG, methods like Hong et al. (2022); Jiang et al. (2022); Shrestha et al. (2020); Marklund et al. (2020); Char et al. (2022) study the credit assignment problem on a graph created from the experience. Hong et al. (2022); Jiang et al. (2022) are designed for discrete environments. Shrestha et al. (2020); Marklund et al. (2020) considers the environments with finite actions and continuous state space by discretizing states or state features via kNN. Char et al. (2022) introduces a stitch operator to create a graph directly by adding new transitions. It can work with environments with low-dimensional continuous action spaces like Mountain Car Continuous (1 dimension) and Maze2D (2 dimensions). However, the stitch operator is hard to scale to high-dimensional action spaces. In contrast, VMG discretizes both state and action spaces and thus can work with continuous high-dimensional action spaces.</p>
<p>Representation Learning Contrastive learning methods learns a good representation by maximizing the similarity between related data and minimizing the similarity of unrelated data (Oord et al., 2018; Chen et al., 2020; Radford et al., 2021) in the learned representation space. Bisimulation-based methods like Zhang et al. (2020) learn a representation with the help of bisimulation metrics (Ferns \&amp; Precup, 2014; Ferns et al., 2011; Bertsekas \&amp; Tsitsiklis, 1995) measuring the 'behavior similarity' of states w.r.t. future reward sequences given any input action sequences. In VMG, we use a contrastive learning loss to learn a metric space encoding the similarity between states as L2 distance.</p>
<h2>3 Value Memory Graph (VMG)</h2>
<p>Our world model, Value Memory Graph (VMG), is a graph-structured Markov decision process constructed as a simplified version of the original environment with discrete and relatively smaller state-action spaces. RL methods can be applied on the VMG instead of the original environment to lower the difficulty of policy learning. To build VMG, we first learn a metric space that measures the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The training pipeline of the state encoder $E n c_{s}$ and the action encoder $E n c_{a}$ to build the memory map. $E n c_{s}$ converts original states $s$ into points in the memory map. $E n c_{a}$ maps actions $a$ as transitions in the memory map.
reachability among the environment states. Then, a graph is built in the metric space from the dataset as the backbone of our VMG. In the end, a Markov decision process is defined on the graph as an abstract representation of the environment.</p>
<h1>3.1 VMG Metric Space Learning</h1>
<p>VMG is built in a metric space where the L2 distance represents whether one state can be reached from another state in a few timesteps. The embedding in the metric space is based on a contrastive-learning mechanism demonstrated in Fig.2a. We have two neural networks: a state encoder $E n c_{s}: s \rightarrow f_{s}$ that maps the original state $s$ to a state feature $f_{s}$ in the metric space, and an action encoder $E n c_{a}: f_{s}, a \rightarrow \Delta f_{s, a}$ that maps the original action $a$ to a transition $\Delta f_{s, a}$ in the metric space conditioned on the current state feature $f_{s}$. Given a transition triple $\left(s, a, s^{\prime}\right)$, we add the transition $\Delta f_{s, a}$ to the state feature $f_{s}$ as the prediction of the next state feature $\hat{f}<em s="s">{s^{\prime}}=f</em>$ :}+\Delta f_{s, a}$. The prediction is encouraged to be close to the ground truth $f_{s^{\prime}}$ and away from other unrelated state features. Therefore, we use the following learning objective to train $E n c_{s}$ and $E n c_{a</p>
<p>$$
L_{\mathrm{c}}=D^{2}\left(\hat{f}<em s_prime="s^{\prime">{s^{\prime}}, f</em>}}\right)+\frac{1}{N} \sum \max \left(m-D^{2}\left(\hat{f<em e="e" g_="g," n="n" s__n="s_{n">{s^{\prime}}, f</em>\right), 0\right)
$$}</p>
<p>Here, $D(\cdot, \cdot)$ denotes the L2 distance. $s_{n e g, n}$ denotes the $n$-th negative state. Given a batch of transition triples $\left(s_{i}, a_{i}, s_{i}^{\prime}\right)$ randomly sampled from the training set and a fixed margin distance $m$, we use all the other next states $s_{j \mid j \neq i}^{\prime}$ as the negative states for $s_{i}$ and encourage $\hat{f}<em i="i">{s</em>$ shown below.}^{\prime}}$ to be at least $m$ away from negative states in the metric space. In addition, we use an action decoder $D c c_{a}: f_{s}, \Delta f_{s, a} \rightarrow \hat{a}$ to reconstruct the action from the transition $\Delta f_{s, a}$ conditioned on the state feature $f_{s}$ as shown in Fig.2b. This conditioned auto-encoder structure encourages the transition $\Delta f_{s, a}$ to be a meaningful representation of the action. Besides, we penalize the length of the transition when it is larger than the margin $m$ to encourage adjacent states to be close in the metric space. Therefore, we have the additional action loss $L_{a</p>
<p>$$
L_{a}=D^{2}(\hat{a}, a)+\max \left(\left|\Delta f_{s, a}\right|_{2}-m, 0\right)
$$</p>
<p>$L_{\text {metric }}$, the total training loss for metric learning, is the sum of the contrastive and action losses.</p>
<p>$$
L_{\text {metric }}=L_{\mathrm{c}}+L_{a}
$$</p>
<h3>3.2 Construct the Graph in VMG</h3>
<p>To construct the graph in VMG, we first map all the episodes in the training data to the metric space as directed chains. Then, these episode chains are combined into a graph with a reduced number of state features. This is done by merging similar state features into one vertex based on the distance in the metric space. The overall algorithm are visualized in Fig.3a and can be found in Appx.B.1. Given a distance threshold $\gamma_{m}$, a vertex set $\mathcal{V}$, and a checking state $s_{i}$, we check whether the minimal distance in the metric space from the existing vertices to the checking state $s_{i}$ is smaller than $\gamma_{m}$. If not or if the vertex set is empty, we set the checking state $s_{i}$ as a new vertex $v_{J}$ and add it to $\mathcal{V}$. This process is repeated over the whole dataset. After the vertex set $\mathcal{V}$ is constructed, each state $s_{i}$ can be classified into a vertex $v_{j}$ of which the distance in the metric space is smaller than $\gamma_{m}$. In the training</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Create a graph and define rewards in VMG. In Fig.3a, three episodes are mapped as three chains in the metric space colored differently. We merge nodes that are close to each other together and combine these chains into a directed graph. In Fig.3b, the graph reward $R_{G}\left(v_{j_{1}}, v_{j_{2}}\right)$ of the action from the green vertex $v_{j_{1}}$ to the blue vertex $v_{j_{2}}$ is defined as the average over rewards in the original episodes.
set, each state transition $\left(s_{i}, a_{i}, s_{i}^{\prime}\right)$ represents a directed connection from $s_{i}$ to $s_{i}^{\prime}$. Therefore, we create the graph directed edges from the original transitions. For any two different vertices $v_{j_{1}}, v_{j_{2}}$ in $\mathcal{V}$, if there exist a transition $\left(s_{i}, a_{i}, s_{i}^{\prime}\right)$ where $s_{i}$ and $s_{i}^{\prime}$ can be classified into $v_{j_{1}}$ and $v_{j_{2}}$, respectively, we add a directed edge $e_{j_{1} \rightarrow j_{2}}$ from $v_{j_{1}}$ to $v_{j_{2}}$.</p>
<h1>3.3 Define a Graph-Based MDP</h1>
<p>VMG is a Markov decision process (MDP) $\left(\mathcal{S}<em G="G">{G}, \mathcal{A}</em>}, \mathrm{P<em G="G">{G}, \mathrm{R}</em>}\right)$ defined on the graph. $\mathcal{S<em G="G">{G}, \mathcal{A}</em>}, \mathrm{P<em G="G">{G}, \mathrm{R}</em>}$ denotes the state set, the action set, the state transition probability, and the reward of this new graph MDP, respectively. Based on the graph, each vertex is viewed as a graph state. Besides, we view each directed connection $e_{j_{1} \rightarrow j_{2}}$ starting from a vertex $v_{j_{1}}$ as an available graph action in $v_{j_{1}}$. Therefore, the graph state set $\mathcal{S<em G="G">{G}$ equals the graph vertex set $\mathcal{V}$ and the graph action set is the graph edge set $\mathcal{E}$. For the graph state transition probability $\mathrm{P}</em>$ otherwise 0 . Therefore,}$ from $v_{j_{1}}$ to $v_{j_{2}}$, we define it as 1 if the corresponding edge exists in $\mathcal{E</p>
<p>$$
\mathrm{P}<em j__2="j_{2">{G}\left(v</em>
$$}} \mid v_{j_{1}}, e_{j_{1} \rightarrow j_{2}}\right)= \begin{cases}1 &amp; \text { if } e_{j_{1} \rightarrow j_{2}} \in \mathcal{E} \ 0 &amp; \text { otherwise }\end{cases</p>
<p>We define the graph reward of each possible state transition $e_{j_{1} \rightarrow j_{2}}$ as the average over the original rewards from $v_{j_{1}}$ to $v_{j_{2}}$ in the training set $\mathcal{D}$, plus "internal rewards". The internal reward comes from the original transitions that are inside $v_{j_{1}}$ or $v_{j_{2}}$ after state merging. An example of graph reward definition is visualized in Fig.3b. Concretely,</p>
<p>$$
\begin{gathered}
R_{j_{1} \rightarrow j_{2}}=\operatorname{avg}\left{r_{i} \mid \forall s_{i} \text { classified to } v_{j_{1}}, s_{i}^{\prime} \text { classified to } v_{j_{2}},\left(s_{i}, a_{i}, r_{i}, s_{i}^{\prime}\right) \in \mathcal{D}\right} \
\mathrm{R}<em j__1="j_{1">{G}\left(v</em>
\frac{1}{2} R_{j_{1} \rightarrow j_{1}}+R_{j_{1} \rightarrow j_{2}}+\frac{1}{2} R_{j_{2} \rightarrow j_{2}} &amp; \text { if } e_{j_{1} \rightarrow j_{2}} \in \mathcal{E} \
\text { Not defined } &amp; \text { otherwise }
\end{array}\right.
\end{gathered}
$$}}, v_{j_{2}}\right)=\left{\begin{array}{ll</p>
<p>Note that the rewards of graph transitions outside of $\mathcal{E}$ are not defined, as these transitions will not happen according to Eq.4. For internal rewards where both the source $s_{i}$ and the target $s_{i}^{\prime}$ of the original transition $\left(s_{i}, s_{i}^{\prime}\right)$ are classified to the same vertex, we split the reward into two and allocate them to both incoming and outgoing edges, respectively. This is shown as $\frac{1}{2} R_{j_{1} \rightarrow j_{1}}$ and $\frac{1}{2} R_{j_{2} \rightarrow j_{2}}$ in Eq.6. Now we have a well-defined MDP on the graph. This MDP serves as our world model VMG.</p>
<h3>3.4 How to Use VMG</h3>
<p>VMG, together with an action translator, can generate environment actions that control agents to maximize episode returns. We first run the classical RL method value iteration (Puterman, 2014) on VMG to compute the value $V\left(v_{j}\right)$ of each graph state $v_{j}$. This can be done in one second without learning an additional neural-network-based value function due to VMG's finite and discrete state-action spaces.</p>
<p>To guide the agent, VMG provides a graph action that leads to high-value graph states in the future at each time step. Due to the distribution shift between the offline dataset and the environment, there</p>
<p>can be gaps between VMG and the environment. Therefore, the optimal graph action calculated directly by value iteration on VMG might not be optimal in the environment. We notice that instead of greedily selecting the graph actions with the highest next state values, searching for a good future state after multiple steps first and planning a path to it can give us a more reliable performance. Given the current environment state $s_{c}$, we first find the closest graph state $v_{c}$ on VMG. Starting from $v_{c}$, we search for $N_{s}$ future steps to find the future graph state $v^{<em>}$ with the best value. Then, we plan a shortest path $\mathcal{P}=\left[v_{c}, v_{c+1}, \ldots, v^{</em>}\right]$ from $v_{c}$ to $v^{*}$ via Dijkstra (Dijkstra et al., 1959) on the graph. We select the $N_{s g}$-th graph state $v_{c+N_{s g}}$ and make an edge $e_{c \rightarrow c+N_{s g}}$ as the searched graph action. The graph action $e_{c \rightarrow c+N_{s g}}$ is converted to the environment action $a_{c}$ via an action translator: $a_{c}=\operatorname{Tran}\left(s_{c}, v_{c+N_{s g}}\right)$. The pseudo algorithm can be found in Appx.B.2.
The action translator $\operatorname{Tran}\left(s, s^{\prime}\right)$ reasons the executed environment action given the current state $s$ and a state $s^{\prime}$ in the near future. $\operatorname{Tran}\left(s, s^{\prime}\right)$ is trained purely in the offline dataset via supervised learning and separately from the training of VMG. In detail, given an episode from the training set and a time step $t$, we first randomly sample a step $t+k$ from the future $K$ steps. $k \sim \operatorname{Uniform}(1, K)$. Then, $\operatorname{Tran}\left(s, s^{\prime}\right)$ is trained to regress the action $a_{t}$ at step $t$ given the state $s_{t}$ and the future state $s_{t+k}$ using a L2 regression loss $L_{T r a n}=D^{2}\left(\operatorname{Tran}\left(s_{t}, s_{t+k}\right), a_{t}\right)$. Note that when $k=1, p\left(a_{t} \mid s_{t}, s_{t+k}\right)$ is determined purely by the environment dynamics and $\operatorname{Tran}\left(s, s^{\prime}\right)$ becomes an inverse dynamics model. As $k$ increase, the influence of the behavior policy that collects the offline dataset on $p\left(a_{t} \mid s_{t}, s_{t+k}\right)$ will increase. Therefore, the sample range $K$ should be small to reflect the environment dynamics and reduce the influence of the behavior policy. In all of our experiments, $K$ is set to 10 .</p>
<h1>4 EXPERIMENTS</h1>
<h3>4.1 Performance on Offline RL Benchmarks</h3>
<p>Test Benchmark We evaluate VMG on the widely used offline reinforcement learning benchmark D4RL (Fu et al., 2020). In detail, we test VMG on three domains: Kitchen, AntMaze, and Adorit. In Kitchen, a robot arm in a virtual kitchen needs to finish four subtasks in an episode. The robot receives a sparse reward after finishing each subtask. D4RL provides three different datasets in Kitchen: kitchen-complete, kitchen-partial, and kitchen-mixed. In AntMaze, a robot ant needs to go through a maze and reaches a target location. The robot only receives a sparse reward when it reaches the target. D4RL provides three mazes of different sizes. Each of them contains two datasets. In Adroit, policies control a robot hand to finish tasks like rotating a pen or opening a door with dense rewards. For evaluation, D4RL normalizes all the performance of different tasks to a range of 0-100, where 100 represents the performance of an "expert" policy. More benchmark details can be found in D4RL (Fu et al., 2020) and Appx.A.</p>
<p>Table 1: Experimental results on domains Kitchen, AntMaze, and Adroit from D4RL benchmark. VMG outperforms baselines in Kitchen and AntMaze where only sparse rewards are provided and achieves comparable performance in Adroit. Results and the standard deviation are calculated over three trained models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">BC</th>
<th style="text-align: right;">BRAC-p</th>
<th style="text-align: right;">BEAR</th>
<th style="text-align: right;">DT</th>
<th style="text-align: right;">AWAC</th>
<th style="text-align: right;">CQL</th>
<th style="text-align: right;">IQL</th>
<th style="text-align: right;">VMG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">kitchen-complete</td>
<td style="text-align: right;">65.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">43.8</td>
<td style="text-align: right;">62.5</td>
<td style="text-align: right;">$\mathbf{7 3 . 0} \pm 6.7$</td>
</tr>
<tr>
<td style="text-align: left;">kitchen-partial</td>
<td style="text-align: right;">38.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">49.8</td>
<td style="text-align: right;">46.3</td>
<td style="text-align: right;">$\mathbf{6 8 . 8} \pm 11.9$</td>
</tr>
<tr>
<td style="text-align: left;">kitchen-mixed</td>
<td style="text-align: right;">$\mathbf{5 1 . 5}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">51.0</td>
<td style="text-align: right;">51.0</td>
<td style="text-align: right;">$50.6 \pm 4.1$</td>
</tr>
<tr>
<td style="text-align: left;">kitchen-total</td>
<td style="text-align: right;">154.5</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">144.6</td>
<td style="text-align: right;">159.8</td>
<td style="text-align: right;">$\mathbf{1 9 2 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">antmaze-umaze</td>
<td style="text-align: right;">54.6</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">59.2</td>
<td style="text-align: right;">56.7</td>
<td style="text-align: right;">74.0</td>
<td style="text-align: right;">87.5</td>
<td style="text-align: right;">$\mathbf{9 3 . 7} \pm 2.3$</td>
</tr>
<tr>
<td style="text-align: left;">antmaze-umaze-diverse</td>
<td style="text-align: right;">45.6</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">53.0</td>
<td style="text-align: right;">49.3</td>
<td style="text-align: right;">84.0</td>
<td style="text-align: right;">62.2</td>
<td style="text-align: right;">$\mathbf{9 4 . 0} \pm 2.0$</td>
</tr>
<tr>
<td style="text-align: left;">antmaze-medium-play</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">61.2</td>
<td style="text-align: right;">71.2</td>
<td style="text-align: right;">$\mathbf{8 2 . 7} \pm 3.1$</td>
</tr>
<tr>
<td style="text-align: left;">antmaze-medium-diverse</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.7</td>
<td style="text-align: right;">53.7</td>
<td style="text-align: right;">70.0</td>
<td style="text-align: right;">$\mathbf{8 4 . 3} \pm 2.1$</td>
</tr>
<tr>
<td style="text-align: left;">antmaze-large-play</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">15.8</td>
<td style="text-align: right;">39.6</td>
<td style="text-align: right;">$\mathbf{6 7 . 3} \pm 3.2$</td>
</tr>
<tr>
<td style="text-align: left;">antmaze-large-diverse</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">14.9</td>
<td style="text-align: right;">47.5</td>
<td style="text-align: right;">$\mathbf{7 4 . 3} \pm 3.1$</td>
</tr>
<tr>
<td style="text-align: left;">antmaze-total</td>
<td style="text-align: right;">100.2</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">112.2</td>
<td style="text-align: right;">107.7</td>
<td style="text-align: right;">303.6</td>
<td style="text-align: right;">378.0</td>
<td style="text-align: right;">$\mathbf{4 9 6 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">pen-human</td>
<td style="text-align: right;">63.9</td>
<td style="text-align: right;">8.1</td>
<td style="text-align: right;">-1.0</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">37.5</td>
<td style="text-align: right;">$\mathbf{7 1 . 5}$</td>
<td style="text-align: right;">$\mathbf{7 0 . 7} \pm 5.2$</td>
</tr>
<tr>
<td style="text-align: left;">pen-cloned</td>
<td style="text-align: right;">37</td>
<td style="text-align: right;">1.6</td>
<td style="text-align: right;">26.5</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">39.2</td>
<td style="text-align: right;">37.3</td>
<td style="text-align: right;">$\mathbf{5 8 . 2} \pm 1.6$</td>
</tr>
<tr>
<td style="text-align: left;">hammer-human</td>
<td style="text-align: right;">1.2</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">$\mathbf{4 . 4}$</td>
<td style="text-align: right;">1.4</td>
<td style="text-align: right;">$\mathbf{4 . 1} \pm 1.2$</td>
</tr>
<tr>
<td style="text-align: left;">hammer-cloned</td>
<td style="text-align: right;">0.6</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">$\mathbf{2 . 1}$</td>
<td style="text-align: right;">$\mathbf{2 . 1}$</td>
<td style="text-align: right;">$\mathbf{2 . 2} \pm 1.4$</td>
</tr>
<tr>
<td style="text-align: left;">door-human</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">-0.3</td>
<td style="text-align: right;">-0.3</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">$\mathbf{9 . 9}$</td>
<td style="text-align: right;">4.3</td>
<td style="text-align: right;">$1.5 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: left;">door-cloned</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-0.1</td>
<td style="text-align: right;">-0.1</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">0.4</td>
<td style="text-align: right;">1.6</td>
<td style="text-align: right;">$\mathbf{2 . 2} \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: left;">adroit-total</td>
<td style="text-align: right;">104.7</td>
<td style="text-align: right;">9.9</td>
<td style="text-align: right;">25.7</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">93.5</td>
<td style="text-align: right;">118.2</td>
<td style="text-align: right;">$\mathbf{1 3 8 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">kitchen+antmaze+adroit</td>
<td style="text-align: right;">359.4</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">541.7</td>
<td style="text-align: right;">656.0</td>
<td style="text-align: right;">$\mathbf{8 2 7 . 6}$</td>
</tr>
</tbody>
</table>
<p>Baselines We mainly compare our method with two state-of-the-art methods CQL (Kumar et al., 2020) and IQL (Kostrikov et al., 2021b) in all the above-mentioned datasets. Both CQL and IQL are based on Q-learning with constraints on the Q function to alleviate the OOD action issue in the offline setting. In addition, we also report the performance of BRAC-p (Wu et al., 2019), BEAR (Kumar et al., 2019), DT (Chen et al., 2021), and AWAC (Nair et al., 2020) in the datasets they used. Performance of behavior cloning (BC) is from (Kostrikov et al., 2021b).</p>
<p>Hyperparameters In all the experiments, the dimension of metric space is set to 10. The margin $m$ in Eq. 1 and 2 is 1 . The distance threshold $\gamma_{m}$ is set to $0.5,0.8$, and 0.3 in Kitchen, AntMaze, and Adorit, separately. Hyperparameters are selected from 12 configurations. We use Adam optimizer (Kingma \&amp; Ba, 2014) with a learning rate $10^{-3}$, train the model for 800 epochs with batch size 100, and select the best-performing checkpoint. More details about hyperparameters and experiment settings are in Appx.D.</p>
<p>Performance Experimental results are shown in Tab.1. VMG's scores are averaged over three individually trained models and over 100 individually evaluated episodes in the environment. In general, VMG outperforms baseline methods in Kitchen and AntMaze and shows competitive performance in Adroit. Note that a good reasoning ability in Kitchen and AntMaze domains is crucial as the rewards in both domains are sparse, and the agent needs to plan over a long time before getting reward signals. In AntMaze, baseline methods perform relatively well in the smallest maze 'umaze', which requires less than 200 steps to solve. In the maze 'large' where episodes can be longer than 600 steps, the performance of baseline methods drops dramatically. VMG keeps a reasonable score in all three mazes, which suggests that simplifying environments to a graph-structured MDP helps RL methods better reason over a long horizon in the original environment. Adroit is the most challenging domain for all the methods in D4RL with a high-dimensional action space. VMG still shows competitive performance in Adroit compared to baselines. Experiments show that learning a policy directly in VMG helps agents perform well, especially in environments with sparse rewards and long temporal horizons.</p>
<h1>4.2 Understanding Value Memory Graph</h1>
<p>To analyze whether VMG can understand and represent the structure of the task space correctly, we visualize an environment, the corresponding VMG, and their relationship in Fig.4. We study the task "antmaze-large-diverse" shown in Fig.4a as the state space of navigation tasks is easier to visualize and understand. The target location where the agent receives a positive reward is denoted by a red circle. A successful trajectory is plotted as the green path. To visualize VMG, all the state features $f_{s}$ are reduced to two dimensions via UMAP (McInnes et al., 2018) and used as the coordinate to plot corresponding vertices as shown in Fig.4b. The graph state values are denoted by color shades. Vertices with darker blue have higher values. As shown in Fig.4b, VMG allocates high values to vertices that are close to the target location and low values to far away vertices. Besides, the topology of VMG is similar to the maze. This is further visualized in Fig.4c where graph vertices are mapped to the corresponding maze locations to show their relationship. Our analysis suggests that VMG can
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: An example of VMG learned from the dataset 'antmaze-large-diverse'. Fig.4a shows the environment with the target location highlighted by a red circle. VMG is visualized via UMAP in Fig.4b. Graph state values are represented by color shades with higher values in darker blue. Graph states that are close to the target have high values calculated by value iteration. In Fig.4c, graph states are mapped to the corresponding maze locations to show the relationship.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: VMG and a successful trial in the task "pen-human". The blue pen is rotated to the same orientation as the green one.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Influence of $\gamma_{m}$ in "kitchen-partial" in performance and VMG size.
learn a meaningful representation of the task. Another VMG visualization in the more complicated task "pen-human" is shown in Fig. 5 and and more visualizations can be found in Appx.J.</p>
<h1>4.3 Reusability of VMG With New Reward Functions</h1>
<p>In offline RL, policies are trained to master skills that can maximize accumulated returns via an offline dataset. When the dataset contains other skills that don't lead to high rewards, these skills will be simply ignored. We name them ignored skills. We can retrain a new policy to master ignored skills by redefining new reward functions correspondingly. However, rerunning RL methods with new reward functions is cumbersome in the original complex environment, as we need to retrain the policy network and Q/value networks from scratch. In contrast, rerunning value iteration in VMG with new reward functions takes less than one second without retraining any neural networks. Note that the learning of VMG and the action translator is reward-free. Therefore, we don't need to retrain VMG but recalculate graph rewards using Eq. 6 with new reward functions.
We design an experiment in the dataset "kitchen-partial" to verify the reusability of VMG with new reward functions. In this dataset, the robot only receives rewards in the following four subtasks: open a microwave, move a kettle, turn on a light, and open a slide cabinet. Besides, there are training episodes containing ignored skills like turning on a burner or opening a hinged cabinet. We first train a model in the original dataset. Then, we define a new reward function, where only ignored skills have positive rewards and relabel training episodes correspondingly. After that, we recalculate graph rewards using Eq.6, rerun value iteration on VMG, and test our agent. Experimental results in Tab. 2 show that agents can perform ignored skills after rerunning value iteration in the original VMG with recalculated graph rewards without retraining any neural networks.</p>
<h3>4.4 Ablation Study</h3>
<p>Distance Threshold The distance threshold $\gamma_{m}$ directly controls the "radius" of vertices and affects the size of the graph. We demonstrate how $\gamma_{m}$ affects the performance in the task "kitchen-partial" in Fig.6. The dataset size of "kitchen-partial" is 137k. A larger $\gamma_{m}$ can reduce the number of vertices but hurts the performance due to information loss. More results can be found in Appx.E.</p>
<p>Graph Reward Here we study how will different designs of the graph reward $R_{G}\left(v_{j_{1}}, v_{j_{2}}\right)$ affect the final performance. In addition to the original version defined in Eq. 5 and Eq. 6 that averages over the environment rewards, we try maximization and summation and denote them as $R_{G, \text { max }}$ and $R_{G, \text { sum }}$, separately. Besides, we also study the effectiveness of the the internal reward through the following three variants of Eq.6: $R_{G, r m}=R_{j_{1}, j_{2}}, R_{G, r m, h}=R_{j_{1}, j_{2}}+\frac{1}{2} R_{j_{2}, j_{2}}, R_{G, r m, t}=$ $\frac{1}{2} R_{j_{1}, j_{1}}+R_{j_{1}, j_{2}}$, if $e_{j_{1} \rightarrow j_{2}} \in \mathcal{E}$. Experimental results shown in Tab. 3 suggest that the original design of the graph rewards represent the environment well and leads to the best performance.</p>
<p>Importance of Contrastive Loss The contrastive loss is the key training objective to learning a meaningful metric space. The contrastive loss pushes states that can be reached in a few steps to be close to each other and pushes away other states. To verify this, we train a variant of VMG without the contrastive loss and show the results in Tab.4. The variant without the contrastive loss does not work at all ( 0 scores) in the 'kitchen-partial' and 'antmaze-medium-play' tasks, and the performance in 'pen-human' significantly decreases from 70.7 to 41.2 . Results indicate the importance of contrastive loss in learning a robust metric space.</p>
<p>Effectiveness of Action Decoder The action decoder is trained to reconstruct the original action from the transition in the metric space conditioned on the state feature shown in Fig.2b. In this way, the training of the action decoder encourages transitions in the metric space to better represent actions and leads to a better metric space.</p>
<p>Table 3: Ablation study of graph reward design in VMG. The original design gives us the best performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Variants</th>
<th style="text-align: center;">kitchen-partial</th>
<th style="text-align: center;">antmaze-medium-play</th>
<th style="text-align: center;">pen-human</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$R_{G, m a z}$</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">50.8</td>
</tr>
<tr>
<td style="text-align: left;">$R_{G, n u m}$</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">66.4</td>
</tr>
<tr>
<td style="text-align: left;">$R_{G, r m}$</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: left;">$R_{G, r m, h}$</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">$\mathbf{7 2 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">$R_{G, r m, t}$</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">65.9</td>
</tr>
<tr>
<td style="text-align: left;">$R_{G}$ (Orig.)</td>
<td style="text-align: center;">$\mathbf{6 8 . 8}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 7}$</td>
<td style="text-align: center;">70.7</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation study of contrastive loss, action decoder, and Dijkstra search.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">kitchen-partial</th>
<th style="text-align: right;">antmaze-medium-play</th>
<th style="text-align: right;">pen-human</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">VMG</td>
<td style="text-align: right;">$\mathbf{6 8 . 8}$</td>
<td style="text-align: right;">$\mathbf{8 2 . 7}$</td>
<td style="text-align: right;">70.7</td>
</tr>
<tr>
<td style="text-align: left;">- contrastive loss</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">41.2</td>
</tr>
<tr>
<td style="text-align: left;">- action decoder</td>
<td style="text-align: right;">15.4</td>
<td style="text-align: right;">66.3</td>
<td style="text-align: right;">68.5</td>
</tr>
<tr>
<td style="text-align: left;">- multi-step search</td>
<td style="text-align: right;">39.8</td>
<td style="text-align: right;">61.5</td>
<td style="text-align: right;">$\mathbf{7 2 . 1}$</td>
</tr>
</tbody>
</table>
<p>To show the effectiveness of the action decoder, we train a VMG variant without the action decoder and show the results in Tab.4. The performance without the action decoder drops in all three tested tasks, especially in 'kitchen-partial' (from 68.8 to 15.4). The results verify our design choice.</p>
<p>Multi-Step Search In Tab.4, we list the performance of our method without Multiple-Step Search. Compared to the original version, we observe a performance drop in 'kitchen-partial' and 'antmaze-medium-play' and similar performance in 'pen-human', which suggests that instead of greedily searching one step in the value interaction results, searching multiple steps first to find a high value state in the long future and then plan a path to it via Dijkstra can help agents perform better. We think the advantage might caused by the gap between VMG and the environment. An optimal path on VMG searched directly by value iteration may not be still optimal in the environment. At the same time, a shorter path from Dijkstra helps reduce cumulative errors and uncertainty, and thus increases the reliability of the policy.</p>
<p>Limitations As an attempt to apply graph-structured world models in offline reinforcement learning, VMG still has some limitations. For example, VMG doesn't learn to generate new edges in the graph but only creates edges from existing transitions in the dataset. This might be a limitation when there is not enough data provided. In addition, VMG is designed in an offline setting. Moving to the online setting requires further designs for environment exploring and dynamic graph expansion, which can be interesting future work. Besides, the action translator is trained via conditioned behavior cloning. This may lead to suboptimal results in tasks with important low-level dynamics like gym locomotion (See Appx.H). Training the action translator by offline RL methods may alleviate this issue.</p>
<h1>5 CONCLUSION</h1>
<p>We present Value Memory Graph (VMG), a graph-structured world model in offline reinforcement learning. VMG is a Markov decision process defined on a directed graph trained from the offline dataset as an abstract version of the environment. As VMG is a smaller and discrete substitute for the original environment, RL methods like value iteration can be applied on VMG instead of the original environment to lower the difficulty of policy learning. Experiments show that VMG can outperform baselines in many goal-oriented tasks, especially when the environments have sparse rewards and long temporal horizons in the widely used offline RL benchmark D4RL. We believe VMG shows a promising direction to improve RL performance via abstracting the original environment and hope it can encourage more future works.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>We would like to thank Ahmed Hefny and Vaneet Aggarwal for their helpful feedback and discussions on this work.</p>
<h2>REFERENCES</h2>
<p>Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming: an overview. In Proceedings of 1995 34th IEEE conference on decision and control, volume 1, pp. 560-564. IEEE, 1995.</p>
<p>Ian Char, Viraj Mehta, Adam Villaflor, John M Dolan, and Jeff Schneider. Bats: Best action trajectory stitching. arXiv preprint arXiv:2204.12026, 2022.</p>
<p>Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 2021.</p>
<p>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597-1607. PMLR, 2020.</p>
<p>Edsger W Dijkstra et al. A note on two problems in connexion with graphs. Numerische mathematik, 1(1):269-271, 1959.</p>
<p>Scott Emmons, Ajay Jain, Misha Laskin, Thanard Kurutach, Pieter Abbeel, and Deepak Pathak. Sparse graphical memory for robust planning. Advances in Neural Information Processing Systems, 33:5251-5262, 2020.</p>
<p>Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for offline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021.</p>
<p>Ben Eysenbach, Russ R Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridging planning and reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>Norm Ferns, Prakash Panangaden, and Doina Precup. Bisimulation metrics for continuous markov decision processes. SIAM Journal on Computing, 40(6):1662-1714, 2011.</p>
<p>Norman Ferns and Doina Precup. Bisimulation metrics are optimal value functions. In UAI, pp. 210-219, 2014.</p>
<p>Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.</p>
<p>Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning, pp. 2052-2062. PMLR, 2019.</p>
<p>David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.</p>
<p>Zhang-Wei Hong, Tao Chen, Yen-Chen Lin, Joni Pajarinen, and Pulkit Agrawal. Topological experience replay. arXiv preprint arXiv:2203.15845, 2022.</p>
<p>Zhiao Huang, Fangchen Liu, and Hao Su. Mapping state space using landmarks for universal goal reaching. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>Zhengyao Jiang, Tianjun Zhang, Robert Kirk, Tim Rocktäschel, and Edward Grefenstette. Graph backup: Data efficient backup exploiting markovian transitions. arXiv preprint arXiv:2205.15824, 2022.</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535-547, 2019.</p>
<p>Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Modelbased offline reinforcement learning. Advances in neural information processing systems, 33: $21810-21823,2020$.</p>
<p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with fisher divergence critic regularization. In International Conference on Machine Learning, pp. 5774-5783. PMLR, 2021a.</p>
<p>Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021b.</p>
<p>Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019 .</p>
<p>Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179-1191, 2020.</p>
<p>Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.</p>
<p>Kara Liu, Thanard Kurutach, Christine Tung, Pieter Abbeel, and Aviv Tamar. Hallucinative topological memory for zero-shot visual planning. In International Conference on Machine Learning, pp. 6259-6270. PMLR, 2020.</p>
<p>Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2): $129-137,1982$.</p>
<p>Ajay Mandlekar, Danfei Xu, Roberto Martín-Martín, Silvio Savarese, and Li Fei-Fei. Learning to generalize across long-horizon tasks from human demonstrations. arXiv preprint arXiv:2003.06085, 2020 .</p>
<p>Henrik Marklund, Suraj Nair, and Chelsea Finn. Exact (then approximate) dynamic programming for deep reinforcement learning. In Bian and Invariances Workshop, ICML, 2020.</p>
<p>Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.</p>
<p>Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning. Advances in neural information processing systems, 31, 2018.</p>
<p>Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.</p>
<p>Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.</p>
<p>Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.</p>
<p>Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley \&amp; Sons, 2014.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. PMLR, 2021.</p>
<p>Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for navigation. arXiv preprint arXiv:1803.00653, 2018.</p>
<p>Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.</p>
<p>Aayam Kumar Shrestha, Stefan Lee, Prasad Tadepalli, and Alan Fern. Deepaveragers: Offline reinforcement learning by solving derived non-parametric mdps. In International Conference on Learning Representations, 2020.</p>
<p>Michita Imai Takuma Seno. d3rlpy: An offline deep reinforcement library. In NeurIPS 2021 Offline Reinforcement Learning Workshop, December 2021.</p>
<p>Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized regression. Advances in Neural Information Processing Systems, 33:7768-7778, 2020.</p>
<p>Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. arXiv preprint arXiv:1911.11361, 2019.</p>
<p>Ge Yang, Amy Zhang, Ari Morcos, Joelle Pineau, Pieter Abbeel, and Roberto Calandra. Plan2vec: Unsupervised representation learning by latent plans. In Learning for Dynamics and Control, pp. 935-946. PMLR, 2020.</p>
<p>Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games with limited data. Advances in Neural Information Processing Systems, 34, 2021.</p>
<p>Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129-14142, 2020.</p>
<p>Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. Advances in Neural Information Processing Systems, 34, 2021.</p>
<p>Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. arXiv preprint arXiv:2006.10742, 2020.</p>
<p>Lunjun Zhang, Ge Yang, and Bradly C Stadie. World model as a graph: Learning latent landmarks for planning. In International Conference on Machine Learning, pp. 12611-12620. PMLR, 2021.</p>
<p>Tian Zhang, Raghu Ramakrishnan, and Miron Livny. Birch: an efficient data clustering method for very large databases. ACM sigmod record, 25(2):103-114, 1996.</p>
<h1>CONTENTS</h1>
<p>1 Introduction ..... 1
2 Related Work ..... 3
3 Value Memory Graph (VMG) ..... 3
3.1 VMG Metric Space Learning ..... 4
3.2 Construct the Graph in VMG ..... 4
3.3 Define a Graph-Based MDP ..... 5
3.4 How to Use VMG ..... 5
4 Experiments ..... 6
4.1 Performance on Offline RL Benchmarks ..... 6
4.2 Understanding Value Memory Graph ..... 7
4.3 Reusability of VMG With New Reward Functions ..... 8
4.4 Ablation Study ..... 8
5 Conclusion ..... 9
A Environment details ..... 15
B Algorithms ..... 15
B. 1 Graph Construction ..... 15
B. 2 Policy Execution ..... 15
C Architecture of Neural Networks ..... 15
D Experiment Settings and Hyperparameters ..... 16
E Ablation Studies ..... 16
E. 1 Distance Threshold ..... 16
E. 2 State Merging Method ..... 17
E. 3 Influence of $m$ ..... 17
E. 4 Influence of Discount Factor ..... 18
E. 5 Influence of the Dimension of the Metric Space ..... 18
E. 6 Influence of $K$ ..... 18
F Training Curve ..... 18
G Environment/Graph Transition Ratio ..... 19
H Experiments in Gym Locomotion Tasks ..... 19</p>
<p>I Future Works ..... 20
J Visualization of VMG ..... 20</p>
<h1>A ENVIRONMENT DETAILS</h1>
<p>The datasets in D4RL (Fu et al., 2020) is under CC BY license and the related code is under Apache 2.0 License. We use the latest version of the datasets (v1/v0/v1 for AntMaze, Kithcen, Adroit, separately). Different versions of datasets contain exactly the same training transitions. The newer version fixes some bugs in the meta data information like the wrong termination steps. Performance is measured by returns normalized to the range between 0 and 100 defined by the D4RL benchmark [9]. In detail, normalized score $=100 \times \frac{\text { score }- \text { random score }}{\text { expert score }- \text { random score }}$. A score of 100 corresponds to the average returns of a domain-specific expert. For AntMaze, and Kitchen, an estimate of the maximum score possible is used as the expert score. For Adroit, this is estimated from a policy trained with behavioral cloning on human-demonstrations and online fine-tuned with RL in the environment. For more details about the datasets please refer to D4RL (Fu et al., 2020).</p>
<h2>B AlGORITHMS</h2>
<h2>B. 1 Graph Construction</h2>
<p>The detailed algorithm of graph construction is shown in Alg.1.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1: Graph Construction
    Input: Training Set \(\mathcal{D}=\left\{\left(s_{i}, a_{i}, r_{i}, s_{i}^{\prime}\right) \mid i=1,2, \ldots, N\right\}\), Empty vertices set \(\mathcal{V}=\{ \}\),
            Current vertex index \(J=1\), Distance threshold \(\gamma_{m}\), Empty edges set \(\mathcal{E}=\{ \}\)
for \(\left(s_{i}, a_{i}, r_{i}, s_{i}^{\prime}\right)\) in \(\mathcal{D}\) do
        \(f_{s_{i}}=E n c_{s}\left(s_{i}\right)\)
        Compute the distance \(d_{i j}\) between \(f_{s_{i}}\) and \(f_{v_{j}}\) for every \(f_{v_{j}}\) in \(\mathcal{V}\)
        if \(\min \left\{d_{i j} \mid f_{s_{j}}\right.\) in \(\mathcal{V}\}&gt;\gamma_{m}\) or \(J=1\) then
            \(v_{J} \leftarrow s_{i}, f_{v_{J}} \leftarrow f_{s_{i}}\)
            \(\mathcal{V}\).append \(\left(\left(v_{J}, f_{v_{J}}\right)\right)\)
            \(J \leftarrow J+1\)
        end
    end
    for \(\left(s_{i}, a_{i}, r_{i}, s_{i}^{\prime}\right)\) in \(\mathcal{D}\) do
        Find \(v_{j_{1}}, v_{j_{2}}\) that \(s_{i}\) and \(s_{i}^{\prime}\) are classified to in \(\mathcal{V}\), respectively
        if \(v_{j_{1}} \neq v_{j_{2}}\) and the connection \(e_{j_{1} \rightarrow j_{2}} \notin \mathcal{E}\) then
            \(\mathcal{E}\).append \(\left(e_{j_{1} \rightarrow j_{2}}\right)\)
        end
    end
</code></pre></div>

<h2>B. 2 Policy EXECUTION</h2>
<p>The detailed algorithm of policy execution is shown in Alg. 2.
Details of Dijkstra When we use Dijkstra in Sec.3.4 to plan a path $\mathcal{P}$ from $v_{c}$ to $v^{*}$, we define weights to each edge to make sure $\mathcal{P}$ is both short and high-rewarded. The weights used to plan the path $\mathcal{P}$ are based on rewards. For each edge $e_{j_{1} \rightarrow j_{2}}$, we define the edge weight $w_{j_{1} \rightarrow j_{2}}$ as the gap between the maximal graph reward and the edge reward and denote the weight set as $\mathcal{W}$. $w_{j_{1} \rightarrow j_{2}}=\max \left{R_{G}\left(v_{j_{3}}, v_{j_{4}}\right) \mid \forall e_{j_{3} \rightarrow j_{4}} \in \mathcal{E}\right}-R_{G}\left(v_{j_{1}}, v_{j_{2}}\right)$.</p>
<h2>C ARCHITECTURE OF NEURAL NETWORKS</h2>
<p>For all the networks including the state encoder $E n c_{s}$, the action encoder $E n c_{a}$, the action decoder $D e c_{a}$, and the action translator $\operatorname{Tr} a n\left(s, s^{\prime}\right)$, we use a 3-layer MLP with hidden size 256 and ReLU activation functions.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 2: Policy Execution
    Input: Current state \(s_{c}\), State encoder \(E_{c s}\), Action translator Tran, Vertex and edge sets in
        \(\mathrm{VMG}(\mathcal{V}, \mathcal{E)\), Vertices value \(V\), Edge weight \(\mathcal{W}\)
    \(f_{s_{c}}=E_{c s}\left(s_{c}\right)\)
    \(v_{c}=\arg \min <span class="ge">_{v_</span>{j} \mid\left(v_{j}, f_{j}\right) \in \mathcal{V}} D\left(f_{s_{c}}, f_{j}\right)\)
    3 Search future horizon of \(N_{s}\) steps starting from \(v_{c}\) and select the best value vertex \(v^{*}\)
    4 Compute the weighted shortest path \(\mathcal{P}\) from \(v_{c}\) to \(v^{*}\) via Dijkstra. \(\mathcal{P}=\left[v_{c}, v_{c+1}, \ldots, v^{*}\right]\)
    \(a_{c}=\operatorname{Tran}\left(s_{c}, v_{c+N_{s g}}\right)\)
</code></pre></div>

<h1>D EXPERIMENT SETTINGS AND HYPERPARAMETERS</h1>
<p>Our model is trained in a single RTX Titan GPU in about 1.5 hours. For inference, building the graph from clustering takes about 0.5-2 minutes before the evaluation. After that, it takes about 0.5-10 minutes to evaluate 100 episodes. We implement VMG on the top of the offline RL python package d3rlpy (Takuma Seno, 2021) with MIT license. In all the experiments, We use Adam optimizer (Kingma \&amp; Ba, 2014) with a learning rate $10^{-3}$. Batch size is 100. Each model is trained for 800 epochs. We save models per 50 epochs and report the performance of the best one evaluated in the environment from the checkpoints saved from the 500th to the 800th epochs. The remaining hyperparameter settings can be found in Tab.5. $N_{s}=\infty$ means we search the future steps till the end of the graph. For the domain Kitchen, the hyperparameters are tuned in "kitchen-partial". For AntMaze it is "antmaze-umaze-diverse". For Adroit, hyperparameters are tuned individually. We use the environment to tune the hyperparameters. We searched four hyperparameters in our main experiments: $\gamma_{m}$ in $[0.3,0.5,0.8,1.0,1.2]$, reward discount in $[0.8,0.95], N_{s g}$ in $[1,2,3], N_{s}$ in [12, $\infty]$. Hyperparameters are searched one by one, in total 12 configurations. For hyperparameters like batch size or learning rate, we follow the default one in the RL library d3rlpy. The dimension of the metric space is set to 10 in all the experiments. Tuning the hyperparameters offline is an ongoing and important research topic in offline RL, and we left it for future work.</p>
<p>Table 5: Detailed Hyperparameter Setting</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">$m$</th>
<th style="text-align: center;">$K$</th>
<th style="text-align: center;">$\gamma_{m}$</th>
<th style="text-align: center;">discount</th>
<th style="text-align: center;">$N_{s g}$</th>
<th style="text-align: center;">$N_{s}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">kitchen-complete</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: left;">kitchen-partial</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: left;">kitchen-mixed</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: left;">antmaze-umaze</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: left;">antmaze-umaze-diverse</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: left;">antmaze-medium-play</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: left;">antmaze-medium-diverse</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: left;">antmaze-large-play</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: left;">antmaze-large-diverse</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: left;">pen-human</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: left;">pen-cloned</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: left;">hammer-human</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: left;">hammer-cloned</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: left;">door-human</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: left;">door-cloned</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">12</td>
</tr>
</tbody>
</table>
<h2>E Ablation Studies</h2>
<h2>E. 1 Distance Threshold</h2>
<p>More experimental results of the distance threshold $\gamma_{m}$ in the tasks "antmaze-medium-play" and "pen-cloned" can be found in Fig.7. Results suggest that the model is not so sensitive to $\gamma_{m}$ if it is not too large.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: More results of the influence of $\gamma_{m}$ in performance and VMG size</p>
<h1>E. 2 State Merging Method</h1>
<p>Vertices in VMG are merged from the original states based on a distance threshold $\gamma_{m}$ as described in Sec.3.2. It is also possible to use other clustering methods to merge states. However, the dataset sizes in some tasks can be up to 1 million. Many advanced clustering methods (like BIRCH (Zhang et al., 1996)) are slow in this case (up to hours for BIRCH on machines with Intel Xeon Gold 6242). Therefore, we compare with the classical K-means (Lloyd, 1982) implemented on Faiss (Johnson et al., 2019) library with the GPU support in both the AntMaze domain and the Kitchen domains. Faiss-based K-means can be finished up to 20 seconds in our setting. Our merging method takes up to 1 minute. Experimental results are shown in Tab.6. VMG created by our merging method performs better than the one created by K-means. The vertices of our method can be viewed as hyperspheres in the metric space with the same radius $\gamma_{m}$. In contrast, K-means cannot directly specify the size of each cluster, which can result in vertices with different "volumes" in the metric space. This might lead to undesired distortion in the graph and reduce the performance. As K-means doesn't have a parameter to control the size of the clusters directly, we have to search for the best number of clusters for every dataset. The number of clusters used in K-means is shown in Tab.7.</p>
<p>Table 6: Ablation study of different state merging methods. Our original design gives us better performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">AntMaze</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Kitchen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">umaze</td>
<td style="text-align: center;">umaze-diverse</td>
<td style="text-align: center;">medium-play</td>
<td style="text-align: center;">medium-diverse</td>
<td style="text-align: center;">large-play</td>
<td style="text-align: center;">large-diverse</td>
<td style="text-align: center;">complete</td>
<td style="text-align: center;">partial</td>
<td style="text-align: center;">mixed</td>
</tr>
<tr>
<td style="text-align: left;">VMG with K-means</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">$\mathbf{7 2 . 3}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 3}$</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">43.6</td>
</tr>
<tr>
<td style="text-align: left;">VMG</td>
<td style="text-align: center;">$\mathbf{9 3 . 7}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 0}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 7}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 3}$</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">$\mathbf{7 3 . 0}$</td>
<td style="text-align: center;">$\mathbf{6 8 . 8}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Number of clusters used in K-means</p>
<table>
<thead>
<tr>
<th style="text-align: center;">AntMaze</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Kitchen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">umaze</td>
<td style="text-align: center;">umaze-diverse</td>
<td style="text-align: center;">medium-play</td>
<td style="text-align: center;">medium-diverse</td>
<td style="text-align: center;">large-play</td>
<td style="text-align: center;">large-diverse</td>
<td style="text-align: center;">complete</td>
<td style="text-align: center;">partial</td>
<td style="text-align: center;">mixed</td>
</tr>
<tr>
<td style="text-align: center;">6000</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">3000</td>
<td style="text-align: center;">25000</td>
<td style="text-align: center;">25000</td>
</tr>
</tbody>
</table>
<h2>E. 3 INFLUENCE OF $m$</h2>
<p>The value of the margin $m$ in Eq. 1 and Eq. 2 implicitly defines the minimal distance of negative state pairs in the learned metric space. To study the influence of $m$ on the performance, here we set m to $0.5,1$, and 2 in the datasets antmaze-medium-play, kitchen-partial, and pen-human and show the results in Tab.8. In the antmaze experiment, performance becomes better with a larger m. But in the task pen-human, a smaller $m$ gives us better results. In kitchen-partial, $m=1$ shows the best performance. Experimental results suggest that $\mathrm{m}=1$ is a reasonable value for the tasks we evaluate on. And if we tune $m$ separately, it is possible to improve the performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$m$</th>
<th style="text-align: left;">0.5</th>
<th style="text-align: left;">1</th>
<th style="text-align: left;">2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">antmaze-medium-play</td>
<td style="text-align: left;">65.0</td>
<td style="text-align: left;">82.7</td>
<td style="text-align: left;">84.0</td>
</tr>
<tr>
<td style="text-align: left;">kitchen-partial</td>
<td style="text-align: left;">17.0</td>
<td style="text-align: left;">64.5</td>
<td style="text-align: left;">68.8</td>
</tr>
<tr>
<td style="text-align: left;">pen-human</td>
<td style="text-align: left;">75.1</td>
<td style="text-align: left;">70.7</td>
<td style="text-align: left;">65.7</td>
</tr>
</tbody>
</table>
<p>Table 8: Influence of the margin $m$.</p>
<h1>E. 4 INFLUENCE OF DISCOUNT FACTOR</h1>
<p>Here we study how different discount factor values will affect the performance of VMG. We set the discount factor to the values $0.8,0.95$, and 0.99 . Experiments in Tab. 9 show that 0.99 leads to better performance in pen-human and comparable performance in kitchen-partial. In antmaze-medium-play, 0.99 performs worse than 0.8 and 0.95 , which suggest that a small discount factor in antmaze might help reduce cumulative errors.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">discount factor</th>
<th style="text-align: left;">0.8</th>
<th style="text-align: left;">0.95</th>
<th style="text-align: left;">0.99</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">antmaze-medium-play</td>
<td style="text-align: left;">82.7</td>
<td style="text-align: left;">76.3</td>
<td style="text-align: left;">75.0</td>
</tr>
<tr>
<td style="text-align: left;">kitchen-partial</td>
<td style="text-align: left;">58.2</td>
<td style="text-align: left;">68.8</td>
<td style="text-align: left;">68.1</td>
</tr>
<tr>
<td style="text-align: left;">pen-human</td>
<td style="text-align: left;">70.7</td>
<td style="text-align: left;">69.0</td>
<td style="text-align: left;">74.8</td>
</tr>
</tbody>
</table>
<p>Table 9: Influence of the discount factor.</p>
<h2>E. 5 INFLUENCE OF THE DIMENSION OF THE METRIC SPACE</h2>
<p>Here we study how different numbers of the metric space dimension will affect the performance of VMG. We set the metric space dimensions to 5, 10, and 20. Experiments in Tab. 10 show that models with latent space dimensions 10 and 20 perform better than those with 5 , which suggests that a reasonable performance requires big enough dimensions of the latent space to represent the states and actions better. Besides, space with 10 dimensions works better than 20 in kithcen-partial but worse than 20 in pen-human, this suggests the performance has space to improve if we tune the dimensions individually in each task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">metric space dim</th>
<th style="text-align: left;">5</th>
<th style="text-align: left;">10</th>
<th style="text-align: left;">20</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">antmaze-medium-play</td>
<td style="text-align: left;">71.0</td>
<td style="text-align: left;">82.7</td>
<td style="text-align: left;">82.0</td>
</tr>
<tr>
<td style="text-align: left;">kitchen-partial</td>
<td style="text-align: left;">5.75</td>
<td style="text-align: left;">68.8</td>
<td style="text-align: left;">46.0</td>
</tr>
<tr>
<td style="text-align: left;">pen-human</td>
<td style="text-align: left;">70.7</td>
<td style="text-align: left;">70.7</td>
<td style="text-align: left;">79.0</td>
</tr>
</tbody>
</table>
<p>Table 10: Influence of the metric space dimension.</p>
<h2>E. 6 INFLUENCE OF $K$</h2>
<p>The hyperparameter $K$ used in training the action translator in Sec.3.4 defines the range of the future states the action translator conditions on during training. To study the influence of $K$, here show experiments with $K=5,10$, and 20 in Tab.11. We notice that $K=5$ doesn't work in antmaze-medium and kitchen-partial, which suggests that $\mathrm{K}=5$ is not big enough to cover 2 steps in the graph transition. In addition, the experiments with $K=20$ show better results than $K=10$ in kitchen-partial and pen-human. In antmaze-medium-play, $K=10$ performs the best. Experimental results suggest that a big enough $K$ helps the model perform better.</p>
<h2>F Training Curve</h2>
<p>Fig. 8 shows the training curves of the contrastive loss $L_{c}$, the action loss $L_{a}$, and the anction translator loss $L_{T r a n}$ in tasks kitchen-partial, antmaze-medium-play, and pen-human.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$K$</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">10</th>
<th style="text-align: center;">20</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">antmaze-medium-play</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">74.0</td>
</tr>
<tr>
<td style="text-align: left;">kitchen-partial</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">76.8</td>
</tr>
<tr>
<td style="text-align: left;">pen-human</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">83.5</td>
</tr>
</tbody>
</table>
<p>Table 11: Influence of $K$.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Training curve of the contrastive loss $L_{c}$, action loss $L_{a}$, and action translator loss $L_{T r a n}$.</p>
<h1>G ENVIRONMENT/GRAPH TRANSITION RATIO</h1>
<p>VMG abstracts the original continuous environment into a finite and relatively small graph. An one-step transition in the graph corresponds to multiple steps in the environment. Here we compute the average numbers of environment transitions per graph transition in our main experiments and list the results in Tab. 12 .</p>
<p>Table 12: Average number of environment transitions per graph transition.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">AntMaze</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Kitchen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Adroit</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">umaze</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">medium</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">large</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">pen</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">hammer</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">door</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">diverse</td>
<td style="text-align: center;">play</td>
<td style="text-align: center;">diverse</td>
<td style="text-align: center;">play</td>
<td style="text-align: center;">diverse</td>
<td style="text-align: center;">complete</td>
<td style="text-align: center;">partial</td>
<td style="text-align: center;">mixed</td>
<td style="text-align: center;">human</td>
<td style="text-align: center;">closed</td>
<td style="text-align: center;">human</td>
<td style="text-align: center;">closed</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">1.0</td>
</tr>
</tbody>
</table>
<h2>H EXPERIMENTS IN GYM LOCOMOTION TASKS</h2>
<p>VMG is introduced to help agents reason the long future better so as to improve their performance in complex environments with sparse rewards and large search space due to long temporal horizons and continuous state/action spaces. VMG may not help in gym locomotion tasks, as these tasks don't require agents to reason the long future and thus are out of our scope. Gym locomotion tasks provide rich and dense reward signals, and the motion patterns to learn in these tasks are periodic and short. Therefore, the problems VMG designed to solve are not an issue here. Our performance in these tasks</p>
<p>is expected to be close to behavior cloning, since the low-level component, the action translator, is trained via (conditioned) behavior cloning. The action translator is used to handle local dynamics that are not modeled in VMG. Here we run new experiments in these tasks and show the results below. Experimental results verify our assumption. Results and analysis suggest that an improved design and/or learning strategy of the action translator might help improve the performance. For example, training the action translator using conditioned offline RL methods instead of conditioned behavior cloning. However, this is orthogonal to our VMG framework contribution to future reasoning, and we leave it for future work.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">VMG</th>
<th style="text-align: left;">BC</th>
<th style="text-align: left;">CQL</th>
<th style="text-align: left;">IQL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">halfcheetah-medium</td>
<td style="text-align: left;">42.2</td>
<td style="text-align: left;">42.6</td>
<td style="text-align: left;">44.0</td>
<td style="text-align: left;">47.4</td>
</tr>
<tr>
<td style="text-align: left;">hopper-medium</td>
<td style="text-align: left;">49.4</td>
<td style="text-align: left;">52.9</td>
<td style="text-align: left;">58.5</td>
<td style="text-align: left;">66.3</td>
</tr>
<tr>
<td style="text-align: left;">walker2d-medium</td>
<td style="text-align: left;">70.4</td>
<td style="text-align: left;">75.3</td>
<td style="text-align: left;">72.5</td>
<td style="text-align: left;">78.3</td>
</tr>
<tr>
<td style="text-align: left;">halfcheetah-medium-replay</td>
<td style="text-align: left;">38.2</td>
<td style="text-align: left;">36.6</td>
<td style="text-align: left;">45.5</td>
<td style="text-align: left;">44.2</td>
</tr>
<tr>
<td style="text-align: left;">hopper-medium-replay</td>
<td style="text-align: left;">15.2</td>
<td style="text-align: left;">18.1</td>
<td style="text-align: left;">95.0</td>
<td style="text-align: left;">94.7</td>
</tr>
<tr>
<td style="text-align: left;">walker2d-medium-replay</td>
<td style="text-align: left;">28.6</td>
<td style="text-align: left;">26.0</td>
<td style="text-align: left;">77.2</td>
<td style="text-align: left;">73.9</td>
</tr>
<tr>
<td style="text-align: left;">halfcheetah-medium-expert</td>
<td style="text-align: left;">80.5</td>
<td style="text-align: left;">55.2</td>
<td style="text-align: left;">91.6</td>
<td style="text-align: left;">86.7</td>
</tr>
<tr>
<td style="text-align: left;">hopper-medium-expert</td>
<td style="text-align: left;">49.5</td>
<td style="text-align: left;">52.5</td>
<td style="text-align: left;">105.4</td>
<td style="text-align: left;">91.5</td>
</tr>
<tr>
<td style="text-align: left;">walker2d-medium-expert</td>
<td style="text-align: left;">70.4</td>
<td style="text-align: left;">107.5</td>
<td style="text-align: left;">108.8</td>
<td style="text-align: left;">109.6</td>
</tr>
</tbody>
</table>
<p>Table 13: Peformance of VMG in gym locomotion tasks. The performance of VMG is expected to be closed to behavior cloning.</p>
<h1>I Future Works</h1>
<p>There are several directions to improve VMG. Building hierarchical graphs to model different levels of environment structures might help represent the environment better. For example, if a robot needs to cook a meal, we might have a high-level graph to represent abstract tasks like washing vegetables, cutting vegetables, etc. A low-level graph can be used to guide a goal-conditioned policy. This might improve the high-level planning of the tasks. Extending VMG into the online setting is also an important future step. In online reinforcement learning, data with new information is collected throughout the training stage. Therefore, the graph needs to have a mechanism to continually expand and include the new information. Besides, exploration is a crucial component in online reinforcement learning. If we model the uncertainty of the graph, VMG can be used to guide the agent to explore regions with high uncertainty to explore more effectively. Combined with Monte Carlo tree search on VMG might also help policy explore and exploit better.</p>
<h2>J VisuAlization OF VMG</h2>
<p>More visualization of VMG in different tasks are demonstrated in Fig.9, 10, 11, 12. An episode is denoted as a green path on the graph with a " + " sign at the end.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Visualization of VMG in different tasks</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Work done outside of Amazon</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
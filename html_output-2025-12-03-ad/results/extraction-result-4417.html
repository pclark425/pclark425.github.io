<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4417 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4417</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4417</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-278904444</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.19209v2.pdf" target="_blank">MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown promise in automating scientific hypothesis generation, yet existing approaches primarily yield coarse-grained hypotheses lacking critical methodological and experimental details. We introduce and formally define the new task of fine-grained scientific hypothesis discovery, which entails generating detailed, experimentally actionable hypotheses from coarse initial research directions. We frame this as a combinatorial optimization problem and investigate the upper limits of LLMs'capacity to solve it when maximally leveraged. Specifically, we explore four foundational questions: (1) how to best harness an LLM's internal heuristics to formulate the fine-grained hypothesis it itself would judge as the most promising among all the possible hypotheses it might generate, based on its own internal scoring-thus defining a latent reward landscape over the hypothesis space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment with ground-truth hypotheses; (3) whether shaping the reward landscape using an ensemble of diverse LLMs of similar capacity yields better outcomes than defining it with repeated instances of the strongest LLM among them; and (4) whether an ensemble of identical LLMs provides a more reliable reward landscape than a single LLM. To address these questions, we propose a hierarchical search method that incrementally proposes and integrates details into the hypothesis, progressing from general concepts to specific experimental configurations. We show that this hierarchical process smooths the reward landscape and enables more effective optimization. Empirical evaluations on a new benchmark of expert-annotated fine-grained hypotheses from recent literature show that our method consistently outperforms strong baselines.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4417.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4417.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based pairwise comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based pairwise comparison (h_cur > h_prev?)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated evaluation mechanism that uses an LLM to perform repeated pairwise judgments between candidate hypotheses (e.g., h_cur vs h_prev) to serve both as an optimization 'gradient' during search and as an evaluation signal across methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based pairwise comparison</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>At each candidate-edit step, an LLM (or an ensemble/committee of LLMs) is prompted to compare two hypotheses and decide which is superior. This module is used both as the internal gradient during Hierarchical Heuristic Search (HHS) and as an automated evaluator for comparing local optima produced by different methods. Each pairwise comparison is repeated multiple times (six in the paper) with presentation order alternated to mitigate position bias; a candidate is declared a winner if it receives >3 votes, a tie if exactly three votes each. Committees of three models produce three judgments which are then aggregated (via a summarizing LLM) into a final decision.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Used to assess overall preference and dimension-specific criteria (effectiveness, novelty, detailedness, feasibility, overall) via prompted comparative judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini (primary), Gemini-1.5-flash, Claude-3-haiku (used in committee experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (benchmark examples); method is domain-agnostic</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Fine-grained experimental hypotheses and mechanistic explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used as automated evaluator across experiments; results shown in tables where LLM-based pairwise voting indicates HHS outperforms baselines (e.g., Table 1 LLM overall wins: HHS wins 73.53%, ties 18.63%, loses 7.84% against Greedy Search). Committee-vs-mixed comparisons also evaluated by LLM pairwise voting (GPT-4o-mini committee > mixed committee > Gemini committee across evaluators).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (LLM) evaluation mechanism; used in parallel with human expert rankings for corroboration.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Mitigation of position bias via six repeated comparisons with alternating order; cross-evaluation using different LLM evaluators (GPT-4o-mini and Gemini-1.5-flash) to control evaluator-model bias; corroborated against independent expert rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Evaluator-model bias (LLM may prefer outputs produced using the same model); known position bias in LLM comparators (mitigated but not eliminated); automated judgments measure relative preference not absolute empirical validity; repeated comparisons increase compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied on the paper's post-2024 chemistry benchmark (extension of TOMATO-Chem) containing expert-annotated fine-grained hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4417.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4417.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expert ranking evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert ranking evaluation (PhD-level chemists)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human evaluation procedure in which domain experts rank candidate hypotheses against a ground-truth fine-grained hypothesis using a prespecified set of criteria and allowed ties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Expert ranking evaluation (4-criterion ranking with ties allowed)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each benchmark item, experts are shown three candidate hypotheses and the ground-truth fine-grained hypothesis (order randomized). Experts rank the candidates using four criteria (Effectiveness, Novelty, Detailedness, Feasibility), assign holistic ranks (ties allowed), and consider trade-offs described in the instructions. Two PhD-level chemists performed qualitative error analyses and provided ranks used to corroborate LLM-based evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Effectiveness, Novelty, Detailedness, Feasibility (and holistic/overall ranking); experts instructed to consider trade-offs between effectiveness vs novelty and detailedness vs feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Fine-grained experimental hypotheses and mechanistic explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Expert rankings corroborate LLM-based findings: HHS local optima were preferred overall by experts (Table 1: Overall (Expert) HHS win 76.47%, tie 15.69%, lose 7.84% against Greedy Search). Qualitative expert analysis also provided convergence statistics (e.g., 24% vicinity with differing details, 16% vicinity but missing key details, 60% different region).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based; used as an independent check on automated LLM evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Expert annotations/co-design of evaluation prompts and auditing; two independent PhD chemists performed analysis and error coding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Small number of expert raters (two) limits statistical power; human judgments can be subjective and influenced by instructions; inter-rater reliability not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Expert ranks applied to the paper's post-2024 chemistry benchmark (extension of TOMATO-Chem) with ground-truth fine-grained hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4417.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4417.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Soft/Hard Recall</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Soft Recall and Hard Recall metrics for coverage of ground-truth components</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two complementary recall metrics that quantify how much methodological/experimental content from an expert-annotated ground-truth hypothesis is covered by a candidate hypothesis, computed after structured decomposition and scored by an LLM-judge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Soft Recall & Hard Recall</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Both ground-truth and candidate hypotheses are decomposed into methodological/experimental components using structured prompts. An LLM-judge searches for corresponding components in the candidate and assigns a coverage score 0–3 per ground-truth component (0=no match, 1=vague/partial match, 2=close but not exact, 3=exact/specific match). Soft Recall = (number of ground-truth components with score>0) / (total ground-truth components). Hard Recall = (sum of (score_i == 3 ? 1 : 0) ) / (total ground-truth components) — alternately defined in paper as proportion weighted by exact matches (the paper gives an explicit formula: Soft Recall = #{components with score > 0} / #{ground-truth components}; Hard Recall = (Σ_i [score_i == 3]) / #{ground-truth components}, and also reports a variant weighting by score/3). The scoring prompts and examples were co-designed and audited with PhD chemists to calibrate judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Coverage of methodological/experimental components (presence/partial/close/exact matches) between candidate and ground-truth hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini (LLM-judge used for component extraction and scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Fine-grained experimental hypotheses (component-level coverage)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported in Table 2: HHS (HHS-3) achieved Soft Recall 40.35% and Hard Recall 23.04% (using ~282 reasoning steps); Greedy Search baseline Soft Recall 16.60% and Hard Recall 9.92%; Greedy w/ self-consistency Soft Recall 31.53% Hard Recall 17.73%. HHS-3 > HHS-1 (single LLM gradient) on both soft and hard recall.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric computed by LLM-judge after LLM-assisted decomposition; human experts co-designed the decomposition prompts and audited calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Scoring prompts and examples co-developed and audited by PhD chemists to ensure calibration and consistency; results corroborated against expert evaluations and aggregate performance differences across methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Recall measures coverage, not factual correctness (paper notes coverage may include experimentally important but incorrect details); decomposition and scoring depend on LLM-judge reliability and prompt design; thresholding decisions (score cutoffs) can affect sensitivity; possible model-evaluator bias if same model involved.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to the paper's post-2024 chemistry benchmark (extension of TOMATO-Chem) with expert-annotated fine-grained hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4417.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4417.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation criteria set</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Four-dimension evaluation criteria (Effectiveness, Novelty, Detailedness, Feasibility) + Overall</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured set of four explicit evaluation dimensions (plus an overall judgment) used consistently for both automated LLM comparisons and human expert rankings to assess quality of generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Effectiveness/Novelty/Detailedness/Feasibility (and Overall)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Hypotheses are evaluated on: Effectiveness — how well the hypothesis addresses the research question; Novelty — degree of originality; Detailedness — specificity and clarity (experimental details); Feasibility — practical plausibility and implementation ease. Each dimension is used for pairwise LLM comparisons and for expert ranking; results are reported per-dimension and overall (e.g., win/tie/lose rates in Table 1). Experts are instructed about trade-offs (effectiveness vs novelty; detailedness vs feasibility) to guide holistic ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Effectiveness, Novelty, Detailedness, Feasibility, plus Overall/holistic assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (applied), generalizable to other domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Fine-grained experimental hypotheses and mechanistic explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Dimension-level LLM-based comparisons (Table 1) show HHS outperforming baselines across many dimensions (e.g., Effectiveness (LLM) HHS wins 74.51% vs Greedy; Detailedness and Overall also strongly favor HHS). Per-dimension differences reported for both LLM and expert evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Used in both automated LLM pairwise evaluation and human expert ranking to provide consistent cross-modal criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Criteria and expert instructions provided in Appendix B and used consistently; expert audits and LLM evaluation cross-checks used to validate that metrics capture domain-relevant distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Trade-offs among criteria can complicate aggregate judgments; no single scalar objective—different methods may optimize different dimensions; human judgments are subjective and inter-rater reliability not numerically reported; criteria operationalization depends on prompt design for LLM evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4417.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4417.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Committee/ensemble evaluation + aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Committee-based evaluator with summarization aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework evaluating candidate hypotheses using committees (ensembles) of LLM instances (either diverse models or identical copies) whose judgments are aggregated by a separate summarizing LLM to yield a final evaluation decision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Committee-based LLM evaluation with aggregation (summarization by LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Three-model committees produce independent comparative judgments; a fourth LLM instance (GPT-4o-mini in experiments) aggregates the three judgments by producing a reasoned final decision rather than using simple majority voting. Three experimental setups compared: (1) Mixed Committee (GPT-4o-mini + Gemini-1.5-flash + Claude-3-haiku), (2) GPT-4o-mini Committee (three GPT-4o-mini instances), and (3) Gemini-1.5-flash Committee (three instances). Aggregation by summarization yields sensitivity to minority well-reasoned views and can promote novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same per-dimension criteria (Effectiveness, Novelty, Detailedness, Feasibility) and overall preference; committees supply votes that are aggregated into a final LLM judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini, Gemini-1.5-flash, Claude-3-haiku (as used in experimental committees)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (evaluation experiments); method is generalizable</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Fine-grained experimental hypotheses and mechanistic explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Empirical finding: committees composed of three instances of the strongest model (GPT-4o-mini committee) consistently outperformed mixed-model committees and Gemini committees across LLM-based evaluations (reported in Table 3 and described in text). HHS-3 (three-instance ensemble + summarizer) achieved higher Soft/Hard Recall and novelty than HHS-1 (single-instance), while single-instance showed slightly higher feasibility in some comparisons (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (LLM-committee) with human corroboration; aggregation performed by an LLM summarizer.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Cross-evaluation using different evaluators to detect evaluator-model bias; comparison across committee compositions; corroboration against expert rankings and recall metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Increased compute cost; potential for model-of-origin bias (evaluators favor outputs from same model family); summarizing aggregator can surface minority but well-reasoned views (increasing novelty) but may also amplify spurious rationales; choice of aggregator and committee composition materially affects outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4417.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4417.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Post-2024 TOMATO-Chem benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Post-2024 expert-annotated fine-grained chemistry hypothesis benchmark (extension of TOMATO-Chem)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A held-out benchmark constructed from 51 chemistry papers published after January 2024, each entry containing a research background, a coarse-grained hypothesis direction, and expert-annotated fine-grained ground-truth hypotheses used to evaluate alignment and recall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Post-2024 TOMATO-Chem extension benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Benchmark entries consist of (b) research background, (h_c) coarse-grained hypothesis, and (h_f) expert-annotated fine-grained hypotheses prepared by two PhD-level chemists. It was curated to prevent data contamination (models' pretraining cutoff Oct 2023). The benchmark is used to measure alignment (via recall metrics), LLM/evaluator comparisons, expert ranking tasks, and ablation studies of search algorithms and evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Used together with recall metrics and the four-dimension criteria (Effectiveness, Novelty, Detailedness, Feasibility) as ground-truth references for alignment and coverage evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (materials and reactions)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Fine-grained experimental hypotheses and mechanistic explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used as the primary evaluation benchmark; reported results show HHS produces hypotheses with higher recall (Soft/Hard) and higher wins in LLM/expert pairwise rankings compared to greedy baselines (e.g., HHS-3 Soft Recall 40.35%, Hard Recall 23.04%).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Serves as ground-truth for hybrid evaluation (LLM-judged recall + human expert rankings).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Dataset constructed from post-cutoff publications to avoid contamination; ground-truth annotations created by two PhD chemists; evaluation methods (prompts, scoring) co-designed and audited with domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relatively small dataset (51 papers); domain-limited to chemistry; ground-truth is a single published hypothesis (there may be multiple valid hypotheses), so evaluation measures alignment to one canonical published solution which may not capture all valid scientific paths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MOOSE-Chem <em>(Rating: 2)</em></li>
                <li>Moose-chem3: Toward experiment-guided hypothesis ranking via simulated experimental feedback <em>(Rating: 2)</em></li>
                <li>Large language models for automated open-domain scientific hypotheses discovery <em>(Rating: 2)</em></li>
                <li>SciMON <em>(Rating: 1)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4417",
    "paper_id": "paper-278904444",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "LLM-based pairwise comparison",
            "name_full": "LLM-based pairwise comparison (h_cur &gt; h_prev?)",
            "brief_description": "An automated evaluation mechanism that uses an LLM to perform repeated pairwise judgments between candidate hypotheses (e.g., h_cur vs h_prev) to serve both as an optimization 'gradient' during search and as an evaluation signal across methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "LLM-based pairwise comparison",
            "evaluation_method_description": "At each candidate-edit step, an LLM (or an ensemble/committee of LLMs) is prompted to compare two hypotheses and decide which is superior. This module is used both as the internal gradient during Hierarchical Heuristic Search (HHS) and as an automated evaluator for comparing local optima produced by different methods. Each pairwise comparison is repeated multiple times (six in the paper) with presentation order alternated to mitigate position bias; a candidate is declared a winner if it receives &gt;3 votes, a tie if exactly three votes each. Committees of three models produce three judgments which are then aggregated (via a summarizing LLM) into a final decision.",
            "evaluation_criteria": "Used to assess overall preference and dimension-specific criteria (effectiveness, novelty, detailedness, feasibility, overall) via prompted comparative judgments.",
            "model_name": "GPT-4o-mini (primary), Gemini-1.5-flash, Claude-3-haiku (used in committee experiments)",
            "model_size": null,
            "scientific_domain": "Chemistry (benchmark examples); method is domain-agnostic",
            "theory_type": "Fine-grained experimental hypotheses and mechanistic explanations",
            "human_comparison": true,
            "evaluation_results": "Used as automated evaluator across experiments; results shown in tables where LLM-based pairwise voting indicates HHS outperforms baselines (e.g., Table 1 LLM overall wins: HHS wins 73.53%, ties 18.63%, loses 7.84% against Greedy Search). Committee-vs-mixed comparisons also evaluated by LLM pairwise voting (GPT-4o-mini committee &gt; mixed committee &gt; Gemini committee across evaluators).",
            "automated_vs_human_evaluation": "Automated (LLM) evaluation mechanism; used in parallel with human expert rankings for corroboration.",
            "validation_method": "Mitigation of position bias via six repeated comparisons with alternating order; cross-evaluation using different LLM evaluators (GPT-4o-mini and Gemini-1.5-flash) to control evaluator-model bias; corroborated against independent expert rankings.",
            "limitations_challenges": "Evaluator-model bias (LLM may prefer outputs produced using the same model); known position bias in LLM comparators (mitigated but not eliminated); automated judgments measure relative preference not absolute empirical validity; repeated comparisons increase compute cost.",
            "benchmark_dataset": "Applied on the paper's post-2024 chemistry benchmark (extension of TOMATO-Chem) containing expert-annotated fine-grained hypotheses.",
            "uuid": "e4417.0",
            "source_info": {
                "paper_title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Expert ranking evaluation",
            "name_full": "Human expert ranking evaluation (PhD-level chemists)",
            "brief_description": "A human evaluation procedure in which domain experts rank candidate hypotheses against a ground-truth fine-grained hypothesis using a prespecified set of criteria and allowed ties.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Expert ranking evaluation (4-criterion ranking with ties allowed)",
            "evaluation_method_description": "For each benchmark item, experts are shown three candidate hypotheses and the ground-truth fine-grained hypothesis (order randomized). Experts rank the candidates using four criteria (Effectiveness, Novelty, Detailedness, Feasibility), assign holistic ranks (ties allowed), and consider trade-offs described in the instructions. Two PhD-level chemists performed qualitative error analyses and provided ranks used to corroborate LLM-based evaluations.",
            "evaluation_criteria": "Effectiveness, Novelty, Detailedness, Feasibility (and holistic/overall ranking); experts instructed to consider trade-offs between effectiveness vs novelty and detailedness vs feasibility.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Chemistry",
            "theory_type": "Fine-grained experimental hypotheses and mechanistic explanations",
            "human_comparison": true,
            "evaluation_results": "Expert rankings corroborate LLM-based findings: HHS local optima were preferred overall by experts (Table 1: Overall (Expert) HHS win 76.47%, tie 15.69%, lose 7.84% against Greedy Search). Qualitative expert analysis also provided convergence statistics (e.g., 24% vicinity with differing details, 16% vicinity but missing key details, 60% different region).",
            "automated_vs_human_evaluation": "Human-based; used as an independent check on automated LLM evaluations.",
            "validation_method": "Expert annotations/co-design of evaluation prompts and auditing; two independent PhD chemists performed analysis and error coding.",
            "limitations_challenges": "Small number of expert raters (two) limits statistical power; human judgments can be subjective and influenced by instructions; inter-rater reliability not reported numerically.",
            "benchmark_dataset": "Expert ranks applied to the paper's post-2024 chemistry benchmark (extension of TOMATO-Chem) with ground-truth fine-grained hypotheses.",
            "uuid": "e4417.1",
            "source_info": {
                "paper_title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Soft/Hard Recall",
            "name_full": "Soft Recall and Hard Recall metrics for coverage of ground-truth components",
            "brief_description": "Two complementary recall metrics that quantify how much methodological/experimental content from an expert-annotated ground-truth hypothesis is covered by a candidate hypothesis, computed after structured decomposition and scored by an LLM-judge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Soft Recall & Hard Recall",
            "evaluation_method_description": "Both ground-truth and candidate hypotheses are decomposed into methodological/experimental components using structured prompts. An LLM-judge searches for corresponding components in the candidate and assigns a coverage score 0–3 per ground-truth component (0=no match, 1=vague/partial match, 2=close but not exact, 3=exact/specific match). Soft Recall = (number of ground-truth components with score&gt;0) / (total ground-truth components). Hard Recall = (sum of (score_i == 3 ? 1 : 0) ) / (total ground-truth components) — alternately defined in paper as proportion weighted by exact matches (the paper gives an explicit formula: Soft Recall = #{components with score &gt; 0} / #{ground-truth components}; Hard Recall = (Σ_i [score_i == 3]) / #{ground-truth components}, and also reports a variant weighting by score/3). The scoring prompts and examples were co-designed and audited with PhD chemists to calibrate judgments.",
            "evaluation_criteria": "Coverage of methodological/experimental components (presence/partial/close/exact matches) between candidate and ground-truth hypotheses.",
            "model_name": "GPT-4o-mini (LLM-judge used for component extraction and scoring)",
            "model_size": null,
            "scientific_domain": "Chemistry",
            "theory_type": "Fine-grained experimental hypotheses (component-level coverage)",
            "human_comparison": true,
            "evaluation_results": "Reported in Table 2: HHS (HHS-3) achieved Soft Recall 40.35% and Hard Recall 23.04% (using ~282 reasoning steps); Greedy Search baseline Soft Recall 16.60% and Hard Recall 9.92%; Greedy w/ self-consistency Soft Recall 31.53% Hard Recall 17.73%. HHS-3 &gt; HHS-1 (single LLM gradient) on both soft and hard recall.",
            "automated_vs_human_evaluation": "Automated metric computed by LLM-judge after LLM-assisted decomposition; human experts co-designed the decomposition prompts and audited calibration.",
            "validation_method": "Scoring prompts and examples co-developed and audited by PhD chemists to ensure calibration and consistency; results corroborated against expert evaluations and aggregate performance differences across methods.",
            "limitations_challenges": "Recall measures coverage, not factual correctness (paper notes coverage may include experimentally important but incorrect details); decomposition and scoring depend on LLM-judge reliability and prompt design; thresholding decisions (score cutoffs) can affect sensitivity; possible model-evaluator bias if same model involved.",
            "benchmark_dataset": "Applied to the paper's post-2024 chemistry benchmark (extension of TOMATO-Chem) with expert-annotated fine-grained hypotheses.",
            "uuid": "e4417.2",
            "source_info": {
                "paper_title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Evaluation criteria set",
            "name_full": "Four-dimension evaluation criteria (Effectiveness, Novelty, Detailedness, Feasibility) + Overall",
            "brief_description": "A structured set of four explicit evaluation dimensions (plus an overall judgment) used consistently for both automated LLM comparisons and human expert rankings to assess quality of generated hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Effectiveness/Novelty/Detailedness/Feasibility (and Overall)",
            "evaluation_method_description": "Hypotheses are evaluated on: Effectiveness — how well the hypothesis addresses the research question; Novelty — degree of originality; Detailedness — specificity and clarity (experimental details); Feasibility — practical plausibility and implementation ease. Each dimension is used for pairwise LLM comparisons and for expert ranking; results are reported per-dimension and overall (e.g., win/tie/lose rates in Table 1). Experts are instructed about trade-offs (effectiveness vs novelty; detailedness vs feasibility) to guide holistic ranking.",
            "evaluation_criteria": "Effectiveness, Novelty, Detailedness, Feasibility, plus Overall/holistic assessment.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Chemistry (applied), generalizable to other domains",
            "theory_type": "Fine-grained experimental hypotheses and mechanistic explanations",
            "human_comparison": true,
            "evaluation_results": "Dimension-level LLM-based comparisons (Table 1) show HHS outperforming baselines across many dimensions (e.g., Effectiveness (LLM) HHS wins 74.51% vs Greedy; Detailedness and Overall also strongly favor HHS). Per-dimension differences reported for both LLM and expert evaluations.",
            "automated_vs_human_evaluation": "Used in both automated LLM pairwise evaluation and human expert ranking to provide consistent cross-modal criteria.",
            "validation_method": "Criteria and expert instructions provided in Appendix B and used consistently; expert audits and LLM evaluation cross-checks used to validate that metrics capture domain-relevant distinctions.",
            "limitations_challenges": "Trade-offs among criteria can complicate aggregate judgments; no single scalar objective—different methods may optimize different dimensions; human judgments are subjective and inter-rater reliability not numerically reported; criteria operationalization depends on prompt design for LLM evaluators.",
            "uuid": "e4417.3",
            "source_info": {
                "paper_title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Committee/ensemble evaluation + aggregation",
            "name_full": "Committee-based evaluator with summarization aggregation",
            "brief_description": "A framework evaluating candidate hypotheses using committees (ensembles) of LLM instances (either diverse models or identical copies) whose judgments are aggregated by a separate summarizing LLM to yield a final evaluation decision.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Committee-based LLM evaluation with aggregation (summarization by LLM)",
            "evaluation_method_description": "Three-model committees produce independent comparative judgments; a fourth LLM instance (GPT-4o-mini in experiments) aggregates the three judgments by producing a reasoned final decision rather than using simple majority voting. Three experimental setups compared: (1) Mixed Committee (GPT-4o-mini + Gemini-1.5-flash + Claude-3-haiku), (2) GPT-4o-mini Committee (three GPT-4o-mini instances), and (3) Gemini-1.5-flash Committee (three instances). Aggregation by summarization yields sensitivity to minority well-reasoned views and can promote novelty.",
            "evaluation_criteria": "Same per-dimension criteria (Effectiveness, Novelty, Detailedness, Feasibility) and overall preference; committees supply votes that are aggregated into a final LLM judgment.",
            "model_name": "GPT-4o-mini, Gemini-1.5-flash, Claude-3-haiku (as used in experimental committees)",
            "model_size": null,
            "scientific_domain": "Chemistry (evaluation experiments); method is generalizable",
            "theory_type": "Fine-grained experimental hypotheses and mechanistic explanations",
            "human_comparison": true,
            "evaluation_results": "Empirical finding: committees composed of three instances of the strongest model (GPT-4o-mini committee) consistently outperformed mixed-model committees and Gemini committees across LLM-based evaluations (reported in Table 3 and described in text). HHS-3 (three-instance ensemble + summarizer) achieved higher Soft/Hard Recall and novelty than HHS-1 (single-instance), while single-instance showed slightly higher feasibility in some comparisons (Table 4).",
            "automated_vs_human_evaluation": "Automated (LLM-committee) with human corroboration; aggregation performed by an LLM summarizer.",
            "validation_method": "Cross-evaluation using different evaluators to detect evaluator-model bias; comparison across committee compositions; corroboration against expert rankings and recall metrics.",
            "limitations_challenges": "Increased compute cost; potential for model-of-origin bias (evaluators favor outputs from same model family); summarizing aggregator can surface minority but well-reasoned views (increasing novelty) but may also amplify spurious rationales; choice of aggregator and committee composition materially affects outcomes.",
            "uuid": "e4417.4",
            "source_info": {
                "paper_title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Post-2024 TOMATO-Chem benchmark",
            "name_full": "Post-2024 expert-annotated fine-grained chemistry hypothesis benchmark (extension of TOMATO-Chem)",
            "brief_description": "A held-out benchmark constructed from 51 chemistry papers published after January 2024, each entry containing a research background, a coarse-grained hypothesis direction, and expert-annotated fine-grained ground-truth hypotheses used to evaluate alignment and recall.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Post-2024 TOMATO-Chem extension benchmark",
            "evaluation_method_description": "Benchmark entries consist of (b) research background, (h_c) coarse-grained hypothesis, and (h_f) expert-annotated fine-grained hypotheses prepared by two PhD-level chemists. It was curated to prevent data contamination (models' pretraining cutoff Oct 2023). The benchmark is used to measure alignment (via recall metrics), LLM/evaluator comparisons, expert ranking tasks, and ablation studies of search algorithms and evaluators.",
            "evaluation_criteria": "Used together with recall metrics and the four-dimension criteria (Effectiveness, Novelty, Detailedness, Feasibility) as ground-truth references for alignment and coverage evaluation.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Chemistry (materials and reactions)",
            "theory_type": "Fine-grained experimental hypotheses and mechanistic explanations",
            "human_comparison": true,
            "evaluation_results": "Used as the primary evaluation benchmark; reported results show HHS produces hypotheses with higher recall (Soft/Hard) and higher wins in LLM/expert pairwise rankings compared to greedy baselines (e.g., HHS-3 Soft Recall 40.35%, Hard Recall 23.04%).",
            "automated_vs_human_evaluation": "Serves as ground-truth for hybrid evaluation (LLM-judged recall + human expert rankings).",
            "validation_method": "Dataset constructed from post-cutoff publications to avoid contamination; ground-truth annotations created by two PhD chemists; evaluation methods (prompts, scoring) co-designed and audited with domain experts.",
            "limitations_challenges": "Relatively small dataset (51 papers); domain-limited to chemistry; ground-truth is a single published hypothesis (there may be multiple valid hypotheses), so evaluation measures alignment to one canonical published solution which may not capture all valid scientific paths.",
            "uuid": "e4417.5",
            "source_info": {
                "paper_title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MOOSE-Chem",
            "rating": 2,
            "sanitized_title": "moosechem"
        },
        {
            "paper_title": "Moose-chem3: Toward experiment-guided hypothesis ranking via simulated experimental feedback",
            "rating": 2,
            "sanitized_title": "moosechem3_toward_experimentguided_hypothesis_ranking_via_simulated_experimental_feedback"
        },
        {
            "paper_title": "Large language models for automated open-domain scientific hypotheses discovery",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        },
        {
            "paper_title": "SciMON",
            "rating": 1
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        }
    ],
    "cost": 0.01585775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search</p>
<p>Zonglin Yang zonglin001@ntu.edu.sg 
Nanyang Technological University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Wanhao Liu 
Shanghai Artificial Intelligence Laboratory</p>
<p>University of Science and Technology of China</p>
<p>Ben Gao 
Shanghai Artificial Intelligence Laboratory</p>
<p>Wuhan University</p>
<p>Yujie Liu 
Wei Li 
Shanghai Artificial Intelligence Laboratory</p>
<p>National University of Singapore</p>
<p>Tong Xie 
University of New South Wales
7 MiroMind</p>
<p>Lidong Bing 
Wanli Ouyang 
Shanghai Artificial Intelligence Laboratory</p>
<p>The Chinese University of Hong Kong</p>
<p>Erik Cambria cambria@ntu.edu.sg 
Nanyang Technological University</p>
<p>Dongzhan Zhou zhoudongzhan@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory</p>
<p>MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search
C55A47CAA8AB4BDF55510909B02ED427
Large language models (LLMs) have shown promise in automating scientific hypothesis generation, yet existing approaches primarily yield coarse-grained hypotheses lacking critical methodological and experimental details.We introduce and formally define the new task of fine-grained scientific hypothesis discovery, which entails generating detailed, experimentally actionable hypotheses from coarse initial research directions.We frame this as a combinatorial optimization problem and investigate the upper limits of LLMs' capacity to solve it when maximally leveraged.Specifically, we explore four foundational questions: (1) how to best harness an LLM's internal heuristics to formulate the fine-grained hypothesis it itself would judge as the most promising among all the possible hypotheses it might generate, based on its own internal scoring-thus defining a latent reward landscape over the hypothesis space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment with ground-truth hypotheses; (3) whether shaping the reward landscape using an ensemble of diverse LLMs of similar capacity yields better outcomes than defining it with repeated instances of the strongest LLM among them; and (4) whether an ensemble of identical LLMs provides a more reliable reward landscape than a single LLM.To address these questions, we propose a hierarchical search method that incrementally proposes and integrates details into the hypothesis, progressing from general concepts to specific experimental configurations.We show that this hierarchical process smooths the reward landscape and enables more effective optimization.Empirical evaluations on a new benchmark of expert-annotated fine-grained hypotheses from recent literature show that our method consistently outperforms strong baselines. 1</p>
<p>Introduction</p>
<p>Large language models (LLMs) have increasingly been applied to assist scientific research (Luo et al., 2025), with one of the most ambitious applications being the automated discovery of novel and valid scientific hypotheses.However, current methods produce hypotheses that are criticized for being overly coarse, lacking sufficient detail, offering simplistic suggestions, or omitting concrete implementation strategies (Wang et al., 2024;Hu et al., 2024;Si et al., 2025).</p>
<p>We present the first systematic investigation into how LLMs can be leveraged to formulate finegrained scientific hypotheses-those enriched not only with major concepts but also with precise methodological details and clearly specified experimental configurations.For example, a coarsegrained hypothesis in chemistry might state, "synthesize hierarchical 3D copper," while a fine-grained counterpart could elaborate, "Copper foils are chemically oxidized by immersion in a solution of 0.5 M ammonium persulfate and 2 M sodium hydroxide for 15 minutes at room temperature, forming a pentagonal hierarchical CuO nanostructure."Such fine-grained hypotheses significantly enhance clarity, feasibility, and experimental implementability.</p>
<p>Formally, we define the task as generating a fine-grained hypothesis given a research background-comprising a research question and a survey of established methodologies-and a coarsegrained hypothesis direction.We show that fine-grained scientific hypothesis discovery is a combinatorial search problem, as it requires selecting and composing a coherent set of concrete details from a vast space of plausible options-making it particularly challenging in practice.The difficulty is compounded by the fact that scientific hypothesis discovery is an inherently out-of-domain (OOD) problem: the correctness of a hypothesis is fundamentally unknown at the time of formulation.</p>
<p>In this work, we focus on the pre-experimental stage of discovery, mirroring how human scientists-prior to empirical testing-iteratively search through the hypothesis space using heuristics and domain knowledge to identify the hypothesis they themselves would judge as the most promising among all plausible candidates they could think of during the hypothesis search process.</p>
<p>Our goal is to emulate this cognitive search process using LLMs, which increasingly rival human scientists in heuristic reasoning and scientific knowledge understanding.This motivates our central research question (Q1): how to best harness an LLM's internal heuristics to formulate the finegrained hypothesis it itself would judge as the most promising among all possible hypotheses it might generate?We conceptualize a hypothesis space where each point along the input dimensions (the x-axis, potentially multidimensional) represents a candidate hypothesis, and each point is assigned a reward value (on the y-axis) by the LLM based on its internal heuristics.This defines a reward landscape over the hypothesis space, with the highest peak corresponding to the hypothesis the LLM internally judges as most promising.Framed this way, Q1 becomes an optimization problem: how can we navigate this landscape to find stronger local optima-or ideally the global optimum-thus eliciting the best fine-grained hypothesis the LLM can generate?</p>
<p>A straightforward baseline is greedy search over the reward landscape.However, its non-convex and complex structure makes naive greedy strategies prone to poor local optima.To address this, we propose a hierarchical search framework that explicitly models how a finite-capacity reasoning agent-human or LLM-navigates the hypothesis space.Specifically, it first explores higher-level conceptual spaces and then incrementally refines into more specific detail spaces.This hierarchical approach smooths the reward landscape at each hierarchy level-especially at higher, more abstract levels-enabling convergence to superior local optima compared to greedy search and greedy search with self-consistency (Wang et al., 2023).The proposed framework naturally scales with the capability of the underlying LLM's heuristics, yielding better optima as those heuristics become stronger.</p>
<p>Having investigated how to identify stronger local optima in Q1, we now turn to our second question (Q2): whether hypotheses judged better by LLMs exhibit stronger alignment with ground-truth hypotheses?To rigorously address Q2 while avoiding data contamination, we construct a benchmark of research backgrounds paired with expert-annotated fine-grained hypotheses from chemistry papers published after January 2024, ensuring these examples were unseen by our LLM (GPT-4o-mini, October 2023 cutoff).Using this benchmark, we indirectly evaluate Q2 by comparing the recall of hypotheses discovered by our hierarchical approach-which locates better LLM-internal local optima-with hypotheses identified by baseline methods.Our results consistently show that hypotheses generated by our method achieve higher recall than those from baselines, providing empirical support for the reliability of the LLM's internal reward signal in guiding fine-grained hypothesis discovery.</p>
<p>Until now, the reward landscape guiding hypothesis search has been defined by a single LLM serving as the evaluator.We now address our third question (Q3): whether defining this landscape with an ensemble of diverse LLMs of similar capacity yields better outcomes than using equally sized ensembles of the strongest LLM within that group.Our experiments show that ensembles of repeated instances of the strongest LLM consistently outperform equally sized ensembles of diverse models, suggesting that peak model quality is more critical than model diversity in this setting.</p>
<p>Finally, we consider a fourth question (Q4): whether an ensemble of identical LLMs provides better reward landscape than a single instance of the same LLM.While Q3 compares ensembles of different models, Q4 isolates the effect of aggregation alone by controlling for model identity.We find that even identical LLMs, when sampled independently and aggregated via summarization, yield a reward signal that better captures novelty without sacrificing overall quality-highlighting a subtle but important dimension in optimizing hypothesis discovery.Notably, while our experiments focus on chemistry, the task formulation, methodology, and analysis of Q1-Q4 are discipline-agnostic.The only domain-specific component is the manually designed hierarchy (used by the methodology), which can be instantiated for each new discipline encountered.</p>
<p>Overall, the contributions of this work are:</p>
<ol>
<li>We introduce and formalize the fine-grained scientific hypothesis discovery task as a combinatorial optimization problem, and release a post-2024 benchmark with expert-annotated fine-grained hypotheses, specifically designed to prevent data contamination.2. We explore the limits of LLMs for fine-grained scientific hypothesis discovery by framing it as an optimization problem over a reward landscape defined by LLM heuristics, with pairwise comparisons serving as the gradient signal.We also propose a hierarchical heuristic search framework that theoretically smooths the reward landscape, reduces search complexity, and identifies superior local optimum by interpolating among discovered optima.Empirically, this framework consistently outperforms strong baselines in locating better local optima.3. We analyze this optimization formalization through 4 foundational research questions.</li>
</ol>
<p>Methodology</p>
<p>2.1 Background and Task Motivation Yang et al. (2025) assume that many chemistry hypotheses can be constructed from a research background b-typically including the research question and/or background survey-and a set of inspirations i 1 , . . ., i k , representing concepts or findings from the literature.It can be formulated as:
h = f (b, i 1 , . . . , i k )(1)
In practice, however, most hypotheses h generated from Equation 1 tend to be coarse-grained: while they form cohesive associations between b and the i, they often lack clear hypothesis specification and the detailed experimental configurations required for direct implementation in a laboratory setting.</p>
<p>Additionally, many such hypotheses contain redundant elements-either due to the inclusion of unnecessary inspirations or from noise present in the literature that is unrelated to the core knowledge intended for hypothesis construction.</p>
<p>Problem Formulation: Fine-Grained Hypothesis Generation as Combinatorial Search</p>
<p>Let h c be a coarse-grained hypothesis direction and h f its fine-grained counterpart, defined as:
h f = {h c , d 1 , . . . , d m } (2)
Here, {h c , d 1 , . . ., d m } denotes the meaningful integration of edits d 1 , . . ., d m into h c , resulting in a coherent, fine-grained hypothesis.Each edit d corresponds to either (1) the addition of a fine-grained detail to an existing concept i or the introduction of a new concept i into h c , or (2) the deletion of a redundant detail or concept from h c .We define two edit candidate sets: D + , containing all details and concepts that may be added to h c ; and D − , containing all details and concepts within h c that may be removed.The overall edit space is then given by D = D + ∪ D − .</p>
<p>Inspired by coarse-to-fine strategies in computer vision-where a coarse image is first generated and then refined with fine-grained details (Tian et al., 2024)-we formulate the transition from h c to h f as an additional step building on Equation 1, which provides the initial h c .(2) the candidate set D is itself implicit and potentially very large; and (3) the edits d i are not independent-errors early in the reasoning chain can propagate and impair later decisions.
P (h f |b, h c ) = P ({d 1 , . . . , d m }|b, h c , D)(3)</p>
<p>Algorithmic Motivation for Hierarchical Heuristic Search (HHS)</p>
<p>Fine-grained hypothesis generation is generally intractable due to the exponential growth of the search space, where the candidate set |D| is often large or prohibitively so.</p>
<p>A notable exception occurs when the problem exhibits an optimal substructure-i.e., an optimal solution can be composed from the optimal solutions to its subproblems.This principle underlies dynamic programming, where solutions are built incrementally from smaller subproblems (first try to obtain the optimal solution for a smaller subproblem and then iteratively find the optimal solution for larger subproblems).</p>
<p>We observe that fine-grained hypothesis generation exhibits an optimal substructure.Specifically, the edits d 1 , . . ., d m can be organized hierarchically: some address high-level concepts (e.g., functional groups, catalyst classes), while others specify low-level details (e.g., reagents, catalysts, temperature, concentration).We assume these edits can be partitioned into p hierarchical levels (p &gt; 1), with higher levels corresponding to finer details.Then the overall problem can be seen as to determine d in {1, . . ., p} hierarchies.The subproblem of it can be seen as the determination of d in {1, . . ., p − 1} hierarchies, etc.Then it is obvious that the optimal solution of a problem can be derived from the optimal solution of its subproblem, etc.  Figure 1 illustrates an example hierarchical decomposition for chemistry, developed in collaboration with domain experts (PhD-level chemists).The hierarchy spans from high-level mechanistic intent to low-level experimental configurations, reflecting the granularity typically considered when translating a conceptual hypothesis into a testable laboratory procedure in chemistry.Now we have simplified the problem of determining d in all p hierarchies into the iteration of determining d in each hierarchy sequentially.Nonetheless, even within a single hierarchy, the number of candidates remains combinatorially large.</p>
<p>A practical approach to this combinatorial complexity is to use heuristics that approximate solutions rather than exhaustively searching for exact ones.This aligns with how chemists refine hypotheses: given that h c often represents an unexplored direction, while individual details may be retrievable from existing databases, the complete set of details is rarely available.Instead, chemists often rely on domain knowledge and intuition to heuristically identify and progressively integrate plausible details.</p>
<p>Analogously, we propose to leverage LLMs' internal heuristics to guide the search for d at each hierarchical level.As LLMs advance, their heuristics-emerging from pretraining over extensive scientific corpora-will increasingly approximate, and may surpass, those of human experts.The proposed framework naturally scales with the strength of these heuristics, yielding increasingly better optima for fine-grained hypothesis discovery as LLM capabilities continue to grow.</p>
<p>In this setting, the candidate space D is not explicitly enumerated but is implicitly embedded within the LLM's internal knowledge and reasoning capabilities.The LLM does not select d from a predefined list, but rather proposes candidates by navigating this latent, heuristic-driven space.</p>
<p>Hierarchical Factorization of the Search Problem</p>
<p>For formalization, we partition the implicit candidate space D into p hierarchical levels, where D (i) ⊆ D represents all potential edits at level i, and D * (i) ⊆ D (i) denotes the (unknown) groundtruth edits.The j-th ground-truth edit at level i is denoted as d * (i) j ∈ D * (i) .Since D is implicitly determined by h c , we have P (D | h c ) = 1, and explicitly condition on D for clarity in the subsequent factorization.Applying the chain rule hierarchically, Equation 3 can be reformulated as:
P (h f | b, hc) = P {D * (1) , . . . , D * (p) } | b, hc, D(4)= p i=1 P D * (i) | b, hc, D * (&lt;i) , D (i) (5) = p i=1 |D * (i) | j=1 P d * (i) j | b, hc, D * (&lt;i) , d * (i) &lt;j , D (i) ,(6)
where D * (&lt;i) = {D * (1) , . . ., D * (i−1) } and d * (i)
&lt;j = {d * (i) 1 , . . . , d * (i) j−1 }.
The key advantage of this hierarchical factorization is that at each level i, the search is restricted to the reduced candidate set D (i) rather than the full space D, significantly narrowing the search space.Moreover, as we will show in § 2.6, this hierarchical decomposition smooths the reward landscape at each hierarchy level, facilitating more stable optimization and enabling the discovery of stronger local optima in the hypothesis space.</p>
<dl>
<dt>LLM-Based Implementation of Hierarchical Heuristic Search</dt>
<dd>
<p>research background : hypothesis direction : final output from hierarchy : a candidate to consider for final output  We implement HHS as an LLM-driven agentic process that directly follows the hierarchical factorization formalized in Equation 6.As shown in Figure 2, at each hierarchy level i, H i−1 represents the accumulated edits from all previous levels, corresponding to D * (&lt;i) .Within the current level, h prev denotes the partial hypothesis incorporating the edits selected up to step j−1, i.e., d * (i) <j .The candidate set D (i) is not explicitly enumerated but emerges implicitly from the LLM's internal heuristics, conditioned on the background b, the hypothesis direction h c , and the edits selected so far.Specifically, the search for a local optimum h j i begins from the initial point h i−1 , using contextual information from b, h c , and H i−2 .For hierarchy level i = 1, we set h 0 = h c and H 0 = ∅, making the hypothesis direction h c the starting point.
𝐻𝑖 − 1 = ∅ if 𝑖 ≤ 1 {ℎ 1 , … , ℎ 𝑖 − 1 } if
At each iteration, the "Add one d at level i" module prompts an LLM to propose an edit d to h prev , producing a candidate h cur , which is then refined once for validity, novelty, and specificity.The "h cur > h prev " module evaluates whether the new hypothesis improves upon the previous one via LLM-based pairwise comparison, serving as an internal gradient signal for hypothesis optimization.This search process continues until no further improvement is observed over k consecutive steps (default k = 3), at which point the current hypothesis is accepted as a local optimum.Each edit d may involve either an addition or a deletion, allowing the search path to include retrospection and self-correction as needed.</p>
</dd>
</dl>
<p>Within each hierarchy level, we adapt the design of an evolutionary unit (Yang et al., 2025) to our task, where the search for the local optimum h i is independently repeated multiple times (set to three in our implementation), yielding several local optima h 1 i , h 2 i , and h 3 i .These candidates are then passed to a recombination module, which integrates their complementary strengths to interpolate a potentially superior local optimum h i within the subspace spanned by h 1 i , h 2 i , h 3 i .A key observation is that a hypothesis candidate's performance at lower (more abstract) hierarchy level can be viewed as an aggregated estimate-approximating an average or soft maximum-of its higher-level subspace (more concrete).For instance, when evaluating a coarse-grained (more abstract) concept like "hierarchical 3D copper," the LLM may implicitly account for its diverse finegrained (more concrete) structural variants, some highly relevant, others ineffective.We hypothesize that the LLM's assessment to the coarse-grained concept is an aggregation of its fine-grained outcomes, weighting promising variants within the broader distribution to produce an overall estimate of the coarse-grained concept's expected potential.</p>
<p>Theoretical</p>
<p>Building on this observation, the hierarchical abstraction smooths the reward landscape at lower levels by attenuating local irregularities in the fine-grained space, as the performance of a point at a lower level can be interpreted as an approximate aggregation or average of the performance across its corresponding higher-level subspace.This effect is illustrated in Figure 3 (a simplified schematic projection into a 1D space).Consequently, direct search over the flat, non-hierarchical space tends to be highly rugged and non-convex, often leading to premature convergence to suboptimal local optima.In contrast, introducing hierarchical structure progressively smooths the landscape, enabling more stable and efficient optimization, particularly at lowest levels.</p>
<p>This smoothing effect can also be interpreted in the frequency domain as a form of low-pass filtering, where high-frequency components of the landscape are attenuated, resulting in a (roughly) spectral cutoff in the spatial frequency domain, as illustrated in Figure 4. 3 Experiment: Investigating the Four Fundamental Questions</p>
<p>Benchmark Construction, LLM Selection, and Baselines</p>
<p>To our knowledge, no existing benchmark provides annotated fine-grained scientific hypotheses-detailed enough for direct experimental execution.We extend the TOMATO-Chem dataset (Yang et al., 2025), which includes 51 chemistry papers published after January 2024 in leading journals such as Nature and Science.Each entry contains a research background b and a coarse-grained hypothesis h c .We further annotated it with fine-grained hypotheses h f , serving as ground-truth references created by two PhD-level chemists.To prevent data contamination, all experiments are conducted using GPT-4o-mini, whose pretraining data cutoff is October 2023.</p>
<p>We compare HHS against two strong baselines widely used in search tasks: (1) greedy search and (2) greedy search with self-consistency.The latter serves as an ablation of HHS where the hierarchical decomposition is removed, performing the search in a single stage with each d sampled directly from the full candidate set D rather than hierarchy-specific subsets D (i) .The self-consistency mechanism is similar to the Recombination module in Figure 2, which interpolate multiple local optima trying to find a better one.Greedy search represents a further ablation, disabling the Recombination module entirely and following a single search trace where the first found local optimum (h 1 i ) is directly adopted as the output (h i = h 1 i in Figure 2).</p>
<p>3.2 Q1: How to Best Harness an LLM's Internal Heuristics to Formulate the Fine-Grained Hypothesis It Would Judge Most Promising Among All It Might Generate?</p>
<p>We frame this question as an optimization problem: Given only a coarse-grained hypothesis h c as the starting point, and relying entirely on a single LLM, how can we navigate the hypothesis space to approach the global optimum of the reward landscape, as defined by this same LLM's internal heuristics, where each optimization step consists of adding an edit d to h c ?In this setting, the LLM plays a dual role: it serves both as the proposal generator, proposing candidate edits d to formulate new hypotheses within the hypothesis space, and as the gradient provider, judging whether the new hypothesis improves upon the current one via its own internal heuristics (e.g., pairwise comparison).</p>
<p>While it is inherently infeasible to determine whether a found local optimum represents the global optimum, we can empirically compare local optima obtained by different methods with the gradient provider (pairwise comparison), and therefore check which one is a better local optimum.</p>
<p>As detailed in § 2, the hierarchical design of HHS offers two key advantages over flat search strategies:</p>
<p>(1) less search space to propose each d (from D (i) , instead of D), and (2) smoothing the reward landscape progressively in the hypothesis space.Among these, the smoothing effect is particularly critical, as it reduces the risk of early convergence to suboptimal local optima and facilitates progress toward higher peaks in the LLM's internal reward landscape.</p>
<p>We compare the local optima discovered by HHS against the two baselines.For each pair of local optima, we conduct both overall evaluations and dimension-specific assessments across four key criteria: effectiveness, novelty, detailedness, and feasibility.In this context, feasibility reflects the practical ease of implementing the proposed hypothesis, encompassing factors such as implementation complexity and the minimization of redundant steps.Hypotheses that are easy to implement and free of redundant components are preferred.</p>
<p>We further observe two common trade-offs among these dimensions: (1) between effectiveness and novelty, as highly novel hypotheses often entail greater scientific risk and uncertainty; and (2) between detailedness and feasibility, as increased specificity can introduce procedural complexity or redundancies that diminish experimental feasibility.</p>
<p>We also conducted an expert evaluation involving two chemistry PhD students.For each benchmark item, one hypothesis was randomly sampled from each method, and the experts were tasked to rank the three hypotheses.The results from both the LLM-based and expert evaluations on the quality of local optima discovered by each method are presented in Table 1.To mitigate known position bias in LLM-based pairwise comparisons-where models tend to favor the first option (Li et al., 2024)-each pair of local optima was compared six times, with the order of presentation alternated every three times.A hypothesis was considered to win if it received more than three votes; a tie was recorded if both received exactly three votes.Ground-Truth Hypotheses? § 3.2 shows that HHS consistently discovers superior local optima compared to baseline methods.We further investigate whether these optima exhibit stronger alignment with the ground-truth hypotheses.Table 3: "EF": Effectiveness, "NV": Novelty, "DT": Detailedness, "FS": Feasibility, "OV": Overall."(GT)" and "(GM)" indicate that the pairwise comparisons were conducted by GPT-4o-mini and Gemini-1.5-flash,respectively.</p>
<p>EF (GT) NV (GT) DT (GT) FS (GT) OV (GT) EF (GM) NV (GM) DT (GM) FS (GM) OV (GM) The optimization in HHS relies on the "h cur &gt; h prev ?" module (Figure 2), which acts as the gradient signal driving the search.This raises a key question: does a diverse ensemble of comparably capable LLMs improve search performance, or is it more effective to use the same number of parallel instances of the single strongest LLM among the ensemble?
Mixed committee v.s. GPT-4o-mini committee GPT-4o-mini committee v.s. Gemini-1.5-
To answer this, we design three experimental settings: (1) Mixed Committee: the "h cur &gt; h prev ?" module is implemented by an ensemble of three different LLMs-GPT-4o-mini (OpenAI, 2024), Gemini-1.5-flash(Georgiev et al., 2024), and Claude-3-haiku (Anthropic, 2024); (2) GPT-4o-mini Committee: the module is implemented by three instances of GPT-4o-mini;</p>
<p>(3) Gemini-1.5-flashCommittee: the module is implemented by three instances of Gemini-1.5-flash.Each committee's three judgments are then aggregated by a GPT-4o-mini, which produces the final decision for "h cur &gt; h prev ?" representing that committee.All three settings use GPT-4o-mini as the proposer module for generating edits d at each hierarchy level i.</p>
<p>We compare the local optima generated by these setups using LLM-based pairwise comparisons, following the protocol in § 3.2, where each pair is evaluated six times to mitigate position bias.However, since the evaluator is itself an LLM, an additional bias may occur-favoring optima discovered using gradients from the same model.To control for this, we conduct two sets of evaluations: one using GPT-4o-mini as the evaluator, and the other using Gemini-1.5-flash.</p>
<p>As shown in Table 3, across both evaluators, the GPT-4o-mini committee consistently outperforms the mixed committee, which in turn outperforms the Gemini-1.5-flashcommittee.These results suggest that leveraging repeated instances of the strongest single model provides a more effective gradient signal for hypothesis optimization than combining different models of similar capacity.</p>
<p>3.5 Q4: Do Multiple Identical LLMs Yield a Better Reward Landscape Than One?</p>
<p>In experiments of Tables 1 and 2 (except for the HHS-1 line in table 2), the reward landscape was defined using an ensemble of three identical LLMs, followed by a fourth instance of the same LLM that aggregated the three judgments into a final, reasoned decision.However, it is unclear on whether one LLM would be already enough.To evaluate this, we compare two variants: HHS-3, which uses an ensemble of three identical instances of GPT-4o-mini (and a fourth instance of the same LLM for aggregation) to provide the reward signal, and HHS-1, which relies on a single instance of the same model.Table 4 reports LLM-based pairwise evaluations between the two setups across four criteria.While overall quality, effectiveness, and detailedness are largely comparable, HHS-3 outperforms in novelty, whereas HHS-1 shows an advantage in feasibility.This result is somewhat counterintuitive.Notably, the summarization step is not a simple majority vote: the aggregating LLM assesses the relative strength of reasoning across the three perspectives and selects the most compelling argument.This allows it to surface minority-supported but wellreasoned views, promoting exploration of more novel or unconventional hypotheses.Consequently, the aggregated reward signal in HHS-3 becomes more sensitive to creative or atypical ideas that a single-shot evaluation might overlook, whereas HHS-1 relies on a single comparative judgment at each step-favoring conventional and well-established hypotheses, at the expense of novelty.</p>
<p>Table 2 presents the recall of ground-truth components for hypotheses generated by HHS-1 and HHS-3.Across both soft and hard recall metrics, HHS-3 outperforms HHS-1, indicating stronger alignment with expert-annotated reference hypotheses.</p>
<p>Related Work and Discussion</p>
<p>LLM-driven scientific discovery methods typically fall into two categories: (1) direct generation of hypotheses from a research background-comprising a research question and background survey (Qi et al., 2024); or (2) retrieval of seemingly unrelated yet potentially useful knowledge fragments, or inspirations, which are then combined with the background to construct a hypothesis (Yang et al., 2024(Yang et al., , 2025;;Wang et al., 2024;Liu et al., 2025b).While these methods show promise in generating novel ideas, they are often criticized for producing hypotheses that are overly coarse, lacking detail, or omitting actionable experimental steps (Wang et al., 2024;Hu et al., 2024;Si et al., 2025).In contrast, our goal is to investigate how LLMs can generate fine-grained scientific hypotheses-those sufficiently detailed to be directly implemented in laboratory settings.Although this work centers on the pre-experimental stage of discovery, the framework can in principle extend to the experimentguided stage (Liu et al., 2025a;Romera-Paredes et al., 2024;Shojaee et al., 2025;Novikov et al., 2025) by incorporating experimental feedback into the background survey.Likewise, details retrieved from papers relevant to the proposed hypothesis can also be integrated into the background survey.</p>
<p>Conclusion</p>
<p>We introduce and formalize the fine-grained scientific hypothesis discovery task as a combinatorial optimization problem.To explore the upper limit of LLMs' capacity for this task, we frame it as an optimization problem, and propose hierarchical heuristic search (HHS), which theoretically smooths the reward landscape, reduces combinatorial complexity through optimal substructure exploitation, and identifies superior local optimum by interpolating among discovered optima.Empirical results show that (1) HHS reliably discovers better local optima than flat greedy search baselines, (2) hypotheses preferred by LLMs often align more closely with expert annotations, (3) repeated use of the strongest model defines a more effective reward landscape than diverse ensembles, and (4) aggregating identical LLMs yields a reward signal more sensitive to novelty and higher in recall than single-instance evaluation.These findings illustrate how hierarchical search can better harness LLMs' internal heuristics for scientific discovery.Although evaluated on a chemistry dataset, the proposed task formulation and methodology are expected to generalize across disciplines, with only the manually designed hierarchy (Figure 1) requiring domain-specific adaptation.</p>
<p>B Expert Evaluation Instructions</p>
<p>For each research question, you will be presented with three candidate hypotheses alongside a ground-truth fine-grained hypothesis.The order of the hypotheses is randomized.Your task is to rank the three candidate hypotheses based on their quality, using the ground-truth hypothesis as a reference.</p>
<p>Please evaluate the hypotheses based on the following four criteria:</p>
<p>• Effectiveness: How well the hypothesis addresses the research question.</p>
<p>• Novelty: The degree of originality relative to existing knowledge.</p>
<p>• Detailedness: The specificity and clarity of the hypothesis.</p>
<p>• Feasibility: The practical plausibility of experimentally testing or implementing the hypothesis.</p>
<p>Note that two tradeoffs may arise:</p>
<p>• Between effectiveness and novelty</p>
<p>• Between detailedness and feasibility</p>
<p>Use your expert judgment to rank the hypotheses based on a holistic assessment of these criteria.In rare cases where two hypotheses appear to be of similar quality, assigning them the same rank is acceptable.</p>
<p>C Recall Metric Calculation</p>
<p>Each evaluation compares a pair of hypotheses-a ground-truth hypothesis (from literature) and a candidate hypothesis (generated by the model).The goal is to quantify how much methodological or experimental content from the ground-truth hypothesis is covered by the candidate.</p>
<p>Step 1. Decomposition.Both hypotheses are decomposed into methodological and experimental components (details) using structured prompts co-designed with chemistry PhD students.The prompts guide an LLM to extract key components such as procedures, specific materials, or reaction parameters.Each component is annotated with its role, function, and context.</p>
<p>Step 2. Scoring via LLM-Judge.For each ground-truth component, the LLM judge searches for functionally corresponding components in the candidate hypothesis and assigns a coverage score from 0-3:</p>
<p>• 0: no match;</p>
<p>• 1: vague or partial match;</p>
<p>• 2: close but not exact match;</p>
<p>• 3: exact or specific match.</p>
<p>The scoring prompts and examples were jointly developed and audited by PhD students to ensure calibration and consistency.</p>
<p>Step 3. Aggregation.Two recall metrics are computed:
Soft Recall = #{components with score &gt; 0} #{ground-truth components} ,(7)Hard Recall = i score i 3 × #{ground-truth components} . (8)
Soft Recall captures whether relevant components are recovered at all, while Hard Recall further reflects how precisely they are reproduced.</p>
<p>D Case Study</p>
<p>Example 1:</p>
<p>• Research Question: How can a cost-effective N-type quasi-solid-state thermocell be developed to boost electricity production from low-grade heat by improving both ion transport efficiency and electrode performance?• Hypothesis Candidate (from HHS): The development of a cost-effective N-type quasi-solidstate thermocell will be achieved through the strategic integration of three core components to enhance electricity production from low-grade heat (≤100°C):</p>
<ol>
<li>Hierarchical Metal Electrodes: Constructed from a copper-nickel alloy (70% Cu, 30% Ni) optimized for conductivity and corrosion resistance, these electrodes will feature a dual-layer design incorporating a titanium dioxide (TiO 2 ; CAS number: 13463-67-7) coating, precisely controlled at a thickness of 100 nm to enhance charge transport by providing a stable interface that reduces charge recombination losses.An aluminum oxide (Al 2 O 3 ; CAS number: 1344-28-1) layer will be included to improve corrosion resistance and reinforce mechanical stability, operating synergistically to enhance the overall electrochemical performance.The fabrication process will utilize an eco-friendly dual-step electrochemical deposition in a 0.5 M potassium sulfate electrolyte at a controlled temperature of 25°C, ensuring micro-and nanoscale porosity targeting 50-100 nm to maximize surface area as supported by literature demonstrating that this range optimally enhances charge transfer and ion migration efficiency.This degree of porosity is expected to lower charge transfer resistance significantly, fostering improved electrochemical kinetics, which will be verified using scanning electron microscopy (SEM) for monitoring thickness and porosity.2. Metal-Based Redox Couples: The thermocell will utilize copper/copper(I) and nickel/nickel(II) redox couples, selected for their favorable redox potentials to minimize side reactions.An integrated cobalt co-catalyst (0.1 M) will serve as an effective stabilizing agent, enhancing electron transfer kinetics and maintaining the oxidation states of Cu 2+ and Ni 2+ during thermal cycling, as demonstrated by prior studies indicating its role in fostering electron transfer efficiency.Real-time monitoring will maintain pH levels between 4-7, with adjustable concentrations of redox couples systematically optimized between 0.5 to 1.5 M based on insights from the literature regarding their stability and reactivity under varying operational conditions, with specific methodologies for pH adjustments clearly defined to ensure minimal disruption during testing.3. Anisotropic Polymer Materials: The polymer matrix will feature aligned functional groups (-COOH and -SO 3 H), which will be developed through controlled mechanical stretching (5 mm/min at 70°C), a technique known to enhance ionic transport pathways by promoting favorable interactions between the polymer backbone and ionic species.High-conductivity ionic species, specifically Na + and Li + , will be incorporated at optimized concentrations (0.5 to 1.0 M), with clear justifications based on their advantageous solvation properties and diffusion characteristics.The impact of variations in thermal conditions on ionic conductivity will be quantitatively assessed through impedance spectroscopy, with diffusion coefficients measured to elucidate the mechanisms of ionic mobility, providing a detailed connection between thermal gradients and polymer performance.</li>
</ol>
<p>This approach clarifies the synergistic interactions and functional enhancements among hierarchical metal electrodes, optimized redox couples, and engineered polymer materials, establishing a cohesive mechanism that uses low-grade heat to drive efficient ion migration and electron transfer within the quasi-solid-state thermocell.</p>
<p>• Expert's Analysis: The study addresses the question of how to develop a cost-effective N-type quasi-solid-state thermocell to enhance electricity production from low-grade heat by improving both ion transport efficiency and electrode performance.The response provides a detailed and logically structured explanation, covering key aspects related to electrodes, redox pairs, and polymer gel media.Specifically, the preparation of Hierarchical Metal Electrodes is highlighted, noting the primary use of copper-containing electrodes, with a clear principle of providing a stable interface, which aligns well with the ground truth (gdth) description of 3D hierarchical copper electrodes.Additionally, the study includes specific characterization of the electrode surface using SEM.However, while the polymer gel mentions an anisotropic alignment structure, it omits the specific operational methods for forming this alignment, which is inconsistent with the gdth.Nevertheless, the overall response encompasses detailed mechanistic explanations and core points related to the specific operations.</p>
<p>Example 2:</p>
<p>• Research Question: How can a Supramolecular amorphous glass material be engineered to simultaneously exhibit strong mechanical properties, optical transparency, self-healing capabilities, and adhesive strength at room temperature?</p>
<p>• Hypothesis Candidate (from HHS): The engineering of multifunctional peptide-based supramolecular amorphous glass materials will concentrate on creating a robust dynamic network facilitated by cooperative reversible hydrogen bonds derived from interactions among four key amino acids: lysine, glutamic acid, aspartic acid, and tyrosine, along with proline to enhance rigidity.The hypothesized molar ratios will be set at 1:1 for lysine and glutamic acid, complemented by approximately 0.5:0.5 ratios for aspartic acid and tyrosine.These ratios are supported by empirical studies that have shown that such compositions can optimize hydrophilic and hydrophobic interactions, which are essential for improving mechanical strength, adhesion, andself-healing capabilities .Lysine's positively charged ammonium group is hypothesized to establish strong ionic interactions with the negatively charged carboxylate groups of glutamic acid and aspartic acid, enhancing the stability of the hydrogen-bonding framework critical for effective energy dissipation during mechanical stress.Tyrosine will contribute to the network through πstacking interactions, which are expected to maintain the structural integrity and optical transparency of the material under load.Proline's unique cyclic structure is anticipated to provide localized rigidity, supporting favorable peptide conformations and facilitating effective stress distribution throughout the dynamic network.</p>
<p>A pivotal component of this dynamic system will be the incorporation of structured water , maintained at an optimal concentration of 10-15% by weight.Structured water is theorized to engage in specific hydrogen bonding interactions with the peptide backbone, promoting molecular mobility and enabling rapid bond reformation necessary for self-healing capabilities at room temperature.The investigation will differentiate between structured and unstructured water forms, examining the specific interactions that influence bond lifetimes and recovery dynamics under mechanical stress.Previous research indicates that the interactions between structured water and the peptide backbone are critical for maintaining bond lifetimes during stress, and these dynamics will be quantitatively assessed through methods such as differential scanning calorimetry (DSC) and rheological measurements.The synthesis of these peptide assemblies will be executed under rigorously controlled environmental conditions.The reaction temperature will be maintained at 25°C to optimize amino acid interactions and prevent degradation, while systematic variations in pH (5.5 to 8.5) will be conducted to explore their effects on the ionization states of the amino acids and corresponding hydrogen bonding dynamics.Additionally, ionic strength will be regulated at approximately 0.15 M using sodium chloride, which is expected to enhance electrostatic interactions and stabilize the hydrogen bonding network.</p>
<p>To thoroughly investigate these interactions and material properties, a combination of experimental methodologies will be utilized.Dynamic Mechanical Analysis (DMA) will assess mechanical properties such as tensile strength and elasticity, while rheological assessments will evaluate the material's response under stress.Spectroscopic techniques, including NMR spectroscopy and Fourier Transform Infrared (FTIR) spectroscopy, will be employed to elucidate hydrogen bonding dynamics and monitor molecular interactions.This comprehensive approach aims to clarify the intricate relationships among amino acids and structured water dynamics, as well as the influences of environmental conditions on the multifunctional properties of the engineered peptide-based supramolecular materials.</p>
<p>• Expert's Analysis: In addressing the research question-"How can a supramolecular amorphous glass material be engineered to simultaneously exhibit strong mechanical properties, optical transparency, self-healing capabilities, and adhesive strength at room temperature?"-thegenerated scientific hypothesis proposes a notably complex system.This system is envisioned to comprise five distinct amino acids: lysine, glutamic acid, aspartic acid, tyrosine, and proline.Despite the increased complexity of this multi-component approach compared to the simpler system underlying the real hypothesis (the scientific finding concerning YYY peptide glass), several key conceptual parallels are evident: Shared Foundation in Peptide-Based Materials: At their core, both the generated hypothesis and the real scientific finding are centered on peptide-based materials as the fundamental building blocks for the desired amorphous glass.Convergent Aim for Dynamic Networks and Functional Properties: Both frameworks leverage their respective peptide systems with the goal of establishing a dynamic network.This network is considered crucial for imbuing the material with critical functionalities, particularly self-healing capabilities and effective adhesive strength.Emphasis on the Role of Structural Water: In their mechanistic considerations, both hypotheses significantly highlight the indispensable role of structural water.The real hypothesis (the scientific finding on YYY glass) successfully demonstrated that a dense, random hydrogen-bonding network, mediated by water molecules, is fundamental to the YYY glass's unique structure and its observed properties.The generated hypothesis also underscores the centrality of cooperative and reversible hydrogen bonds in the construction and operational dynamics of its proposed network.</p>
<p>Example 3:</p>
<p>• Research Question: How can computational methods be used to accurately predict and improve the reactivity and selectivity of modular diazo transfer (MoDAT) reactions, especially reactions with primary amines?And to design new reagents based on computational models.</p>
<p>• Hypothesis Candidate 1 (from HHS): We propose to systematically investigate the reactivity and selectivity of modular diazo transfer (MoDAT) reactions utilizing azide-based reagents, with a specific focus on para-substituted benzyl azide derivatives modified with strong electron-withdrawing groups (EWGs) such as nitro (-NO 2 ) and cyano (-CN), as well as weaker electron-withdrawing groups (e.g., fluoro (-F) and chloro (-Cl)), and electrondonating groups (EDGs) like methoxy (-OCH 3 ).Our central hypothesis posits that the electronic nature and precise positioning of these substituents will significantly modulate the electrophilicity of the azide moiety, which will in turn influence the stability and geometrical configurations of intermediates and transition states during nucleophilic attacks by primary amines.</p>
<p>The experimental work will be executed under controlled laboratory conditions using a Schlenk line to maintain an inert nitrogen atmosphere for at least 30 minutes prior to reaction initiation, minimizing moisture exposure.Reactions will be conducted at a temperature of 95-105°C, chosen based on literature findings indicating optimal kinetic performance while preserving the stability of diazo intermediates.We will employ polar aprotic solvents such as dimethylformamide (DMF) and dimethyl sulfoxide (DMSO), which are anticipated to enhance solvation of the azide and improve nucleophilicity of the primary amines.A stoichiometric ratio of 1:1.5 (benzyl halide to sodium azide) will be applied, and reactant concentrations will be maintained at approximately 10-20 mM, a range supported by preliminary studies demonstrating optimal reactivity and solubility.</p>
<p>To deepen our mechanistic understanding, we will utilize advanced computational techniques, primarily Density Functional Theory (DFT) with specific emphasis on the B3LYP functional and a 6-31G(d) basis set.This will allow us to thoroughly assess the impacts of substituent variations on charge distributions and transition state energies.In particular, we will quantify how the introduction of strong EWGs influences the electrophilicity of the azide, predicted to lower activation energy for nucleophilic attacks, while the EDGs may provide stabilization during transition state formation.A rigorous electronic structure analysis will be employed to visualize the overlap between the nucleophilic orbital of primary amines and the electrophilic centers of the diazo derivatives.</p>
<p>To ensure a comprehensive evaluation of solvent effects, we will systematically conduct reactions in both polar protic solvents, such as ethanol, and polar aprotic solvents, like DMSO, maintaining a concentration of 50% v/v relative to the reaction volume.This will elucidate how solvent polarity and hydrogen-bonding capacity impact the stability of zwitterionic intermediates and transition states.Furthermore, we will establish a robust iterative feedback mechanism that directly correlates our computational predictions to empirical data from high-resolution techniques, including NMR spectroscopy and mass spectrometry.Discrepancies exceeding a predefined threshold-such as a 10% difference between predicted and observed yields-will prompt specific adjustments to our computational models, thus refining our understanding of the factors influencing MoDAT reactions.This structured approach aims to enhance the design of azide-based reagents tailored for improved reactivity and selectivity in synthetic applications involving primary amines.</p>
<p>• Hypothesis Candidate 2 (Greedy + self-consistency): This study aims to investigate the reactivity and selectivity of modular diazo transfer (MoDAT) reactions by designing tailored azide-based reagents that incorporate strategically positioned electron-withdrawing groups (EWGs), specifically nitro (-NO 2 ) and cyano (-CN) moieties.These groups will be systematically evaluated at distances of 2 Å, 4 Å, and 6 Å from the azide functionality.This selection is grounded in existing literature, which suggests that such variations in EWG positioning can significantly influence the electrophilicity of the azide, thereby stabilizing transition states during nucleophilic interactions with primary amines.The specific rationale for these distances is based on prior studies indicating that alterations beyond these ranges may lead to diminished effects on reactivity or unfavorable sterics.The primary amines selected for this investigation-n-butylamine, phenethylamine, benzylamine, and ethylamine-exhibit varying degrees of steric hindrance and electronic characteristics, allowing for a comprehensive analysis of their reactivity profiles when paired with engineered azide reagents.This choice effectively captures a spectrum of nucleophilic behavior and enhances our understanding of how both electronic and steric factors operate within MoDAT reactions.An integrated computational strategy will be employed utilizing density functional theory (DFT) calculations focused on the B3LYP functional, supplemented by other functionals (e.g., M06-2X and PBE0) to ensure accuracy and validation of outcomes.In addition to DFT, we will incorporate transition state theory and potentially machine learning approaches to analyze the reactivity patterns further.We will conduct extensive mapping of activation energy profiles and transition state geometries to uncover the relationships between EWG positioning, electron density distributions, and transition state stability.Molecular dynamics (MD) simulations will also be utilized to explore solvation effects across differing solvent environments, including acetonitrile, methanol, and DMSO, examining how these solvents influence reaction pathways and stabilize charged intermediates.Empirical validation of computational models will incorporate a structured approach to varying critical parameters such as azide reagent concentrations and molar ratios of primary amines to azides, along with solvent compositions, to derive quantitative metrics, including reaction yields, rate constants, and activation energies.Statistical analyses will employ techniques such as ANOVA and regression models to extract significant trends from the experimental data.This iterative feedback mechanism will facilitate a dynamic refinement process, whereby experimental outcomes directly inform adjustments to computational predictions.Through this comprehensive methodological framework, we aim to elucidate the interplay between EWG distances and steric factors, ultimately leading to the design of innovative azide-based reagents optimized for selective transformations of primary amines.• Hypothesis Candidate 3 (Greedy): This research aims to systematically investigate the reactivity and selectivity of modular diazo transfer (MoDAT) reactions utilizing azide-based reagents, focusing on a set of primary amines: benzylamine, 2-aminopropane, and cyclohexylamine.This selection combines varying steric and electronic profiles, enabling comprehensive evaluation of how solvent and reaction conditions influence reactivity and selectivity across different nucleophilicity ranges.Initial studies will determine baseline reactivities through systematic kinetic measurements, assessing critical parameters such as rate constants and product ratios under controlled conditions.Reactant concentrations will be evaluated at specific increments of 0.1 M (0.1 M, 0.5 M, and 1.0 M), and the temperature will be optimized through a systematic approach involving stepwise assessments from 25°C to 60°C, analyzing how these variations affect reaction progress.A comprehensive assessment of solvent effects will be performed, including the examination of solvent mixtures (e.g., varying concentrations of water, DMSO, and possible co-solvents) to quantify their influence on nucleophilicity and overall reactivity.Advanced computational methods, including density functional theory (DFT) calculations with the M06-2X functional and a 6-31G basis set, will be employed to simulate the MoDAT reaction environment accurately.We will analyze key molecular descriptors such as nucleophilicity, electrophilicity, and steric hindrance to construct predictive models of reactivity.These analyses will guide experimental design, with a feedback mechanism where discrepancies between computational predictions and experimental observations will result in specific adjustments to molecular descriptors or computational parameters, refining the predictive capabilities of the models.Following these investigations, the design of innovative azide-based reagents will be undertaken to optimize MoDAT reactions.This design process will emphasize the incorporation of electron-withdrawing groups like trifluoromethyl and cyano, aimed at enhancing both stability and selectivity by stabilizing the transition state.Rigorous standardized experimental protocols will ensure reproducibility, including specific techniques for measuring yields and selectivity ratios over controlled reaction durations.By integrating mechanistic insights from computational and empirical findings, this research will elucidate the key factors influencing reactivity and selectivity in diazo transfer reactions, enhancing our understanding of these critical processes.• Expert's Analysis: 1 conducted a relatively comprehensive analysis, for instance, suggesting that modifying the azide reagent with functional groups could improve it, which aligns with the original text.However, 2 and 3 did not.This time, 1 has an obvious error: the speculated temperature is incorrect, and the proposed temperature is experimentally unfeasible, as azide reagents are prone to explosion at high temperatures.Of course, temperature is a minor point, and overall, 1 is still acceptable.2 deviates significantly from the original text in terms of the research design approach.Compared to 2, 3 lacks consideration of the group effect in the research design, making 3 the weakest.Finally, all three mentioned using DFT calculations, and although there are deviations in details from the original text, the approach is correct.</p>
<p>Example 4:</p>
<p>• Research Question: How can photoredox catalysis be used to exploit the latent reactivity of phosphorus ylides, allowing them to participate in a formal three-component cycloaddition that converts inert C-H and C=P bonds into C-C and C=C bonds, creating versatile synthetic building blocks in an efficient, controlled manner?</p>
<p>• Hypothesis Candidate 1 (from HHS): The mechanism for activating phosphorus ylides in a formal three-component cycloaddition via photoredox catalysis can be articulated in four key steps, each supported by optimized experimental conditions: 1. Maintaining phosphorus ylide concentrations at 0.1-0.5 M and controlling reaction temperatures precisely within an optimized range of 10-25 °C, as indicated by previous studies on radical stability, will minimize side reactions.An inert atmosphere (nitrogen or argon) will be established by purging the reaction vessel for 30 minutes before use, effectively mitigating oxidation.Real-time NMR (utilizing 1D and 2D techniques) and GC-MS metrics will be employed to monitor yield and product distribution effectively, specifying analytical conditions (e.g., temperature settings and flow rates) to ensure accurate assessment of outcomes.4. Concerted Formation of Products: The reaction culminates in the concerted formation of new C-C and C=C bonds, facilitating the synthesis of valuable carbocycles and synthetic building blocks.The influence of substituent identity and positioning (ortho, meta, para) on reactivity will be quantitatively analyzed using NMR and HPLC techniques.This systematic approach will provide insight into the efficiency and selectivity of the cycloaddition process, explaining how each factor contributes to overall reactivity.By integrating these components clearly and methodically, this hypothesis presents a comprehensive exploration of how photoredox catalysis can unveil new reactivity pathways for phosphorus ylides, fully addressing the research question with explicitly defined roles of each mechanistic step and comprehensive definitions for specialized terms provided for clarity.</p>
<p>• Hypothesis Candidate 2 (Greedy + self-consistency): This study aims to investigate how photoredox catalysis can elucidate specific reactivity mechanisms in diphenylphosphinyl ylides, focusing on their participation as intermediates in formal three-component cycloaddition reactions that convert inert alkyl C-H bonds and coordinated C=P bonds into C-C and C=C bonds.We will concentrate on diphenylphosphinyl ylides due to their capacity to facilitate charge inversion processes, which are crucial for generating stabilized radical intermediates.Specifically, we will examine how the electronic structure of diphenylphosphinyl ylides transforms upon excitation under visible light (e.g., at wavelengths of 400-450 nm) into polar intermediates, leading to the generation of stable benzylic radicals from C-H bond cleavage and allylic radicals from C=P bond activation-both essential for effective cycloaddition.</p>
<ol>
<li>
<p>Charge Inversion Mechanisms: We will clarify the mechanisms of charge inversion by detailing the electronic transitions involved, emphasizing the role of excited state lifetimes and how solvent polarity affects the stabilization of generated radicals.Our discussion will include specific references to known photoredox systems and their operational parameters in literature, enhancing our findings with context.2. Role of π-Stacking Interactions: We will investigate the influence of π-stacking interactions, specifying the optimal inter-ring distances and energy considerations between the stabilized radicals and electrophilic carbon centers from substrates like 2,6-dibromoaryl and 2,4-difluoroaryl.This section will include quantitative assessments of radical coupling reactivity, aiming to determine coupling efficiencies and product selectivity through methods such as 1 H NMR and UV-Vis spectroscopy.</p>
</li>
<li>
<p>Solvent and Temperature Effects: We will analyze solvent effects by conducting reactions in a range of solvents with varied dielectric constants, including a thorough comparison of polar solvents like acetonitrile and non-polar solvents like dichloromethane.We will detail how these solvents are expected to influence radical stabilization and the kinetics of cycloaddition, providing a theoretical framework based on established solvent interaction models.Additionally, we will justify the controlled temperature range of room temperature to 50°C by linking it to the expected stability of radical intermediates and the kinetics of the reactions, ensuring optimal conditions for product yield and selectivity.4. Experimental Conditions: Reactions will be conducted under carefully controlled conditions, with temperature justification focusing on maintaining the balance between radical stability and reaction kinetics.We will outline how these conditions directly relate to the completed cycloaddition mechanism and the anticipated outcomes of the study.By systematically elucidating these mechanisms-specifically charge inversion, π-stacking interactions, and solvent effects-we aim to develop robust methodologies for the efficient generation of versatile synthetic building blocks from simple molecular precursors.</p>
</li>
</ol>
<p>• Hypothesis Candidate 3 (Greedy): Investigate how photoredox catalysis enhances the reactivity of phosphorus ylides through selective nucleophilic attack on α, β-unsaturated carbonyl compounds, such as crotonaldehyde, which are activated to form stable radical cation intermediates via photoredox-driven single-electron transfer (SET) processes.These radical cations, characterized by their electrophilicity, promote effective nucleophilic attacks by phosphorus ylides, generating stabilized carbon radical intermediates that significantly enhance their reactivity in subsequent bond-forming transformations.Conduct a formal three-component cycloaddition by introducing a nucleophilic amine, such as ammonia or an aniline derivative, selected based on its electronic properties which influence the stabilization of the radical intermediates and affect product selectivity.Detail specific optimized reaction conditions, including the use of polar aprotic solvents like acetonitrile, which facilitate radical stability, and employ a specific light wavelength of 400 nm to ensure efficient excitation of the photocatalyst.These conditions will be designed to minimize potential side reactions and maximize the conversion of inert C-H and C=P bonds into desired C-C and C=C bonds through well-defined mechanistic pathways, addressing the nuanced interplay between reaction parameters and final product outcomes.</p>
<p>• Expert's Analysis: 1 accurately predicted the light source wavelength range, metal catalyst system, and solvent system, such as the use of Ir catalyst and acetonitrile as the solvent, all of which align with the original text.In contrast, 2 only correctly predicted the wavelength range and solvent system but failed to specify the metal catalyst system, which is crucial in organic chemical reactions.Therefore, 2 is inferior to 1. Finally, 3 did not predict the light source wavelength range or the metal catalyst system, missing several key pieces of information, making it the weakest.</p>
<p>E Expert Analysis of Hypothesis Quality E.1 Convergence to Ground-Truth Local Optima</p>
<p>To complement our quantitative evaluations, we asked domain experts to qualitatively assess how well the hypotheses generated by HHS aligned with the expert-annotated fine-grained hypotheses in our benchmark.</p>
<p>The distribution of expert assessments across all evaluated examples is as follows:</p>
<p>• Reached a completely different region-likely a distinct local optimum with scientific plausibility: 60%</p>
<p>• Reached the vicinity of the ground-truth local optimum, but with differing details: 24% • Reached the vicinity of the ground-truth local optimum, but failed to fully elaborate or specify the key details: 16%</p>
<p>Here, the "ground-truth local optimum" refers to the expert-extracted fine-grained hypothesis from a publication, which serves as the reference target."Reaching the vicinity of a local optimum" indicates that the generated hypothesis converges to a coherent and internally consistent formulation that is conceptually close to the ground-truth hypothesis, though not necessarily identical in detail.</p>
<p>The relatively high divergence rate (60%) reflects an inherent tradeoff in our experimental setup.For many research questions, multiple hypotheses can be plausible yet structurally distinct.Guiding the model toward the exact ground-truth hypothesis requires:</p>
<ol>
<li>initializing the search process from a starting point sufficiently close to the ground-truth optimum; 2. but avoiding initialization that is too close or too specific, as this would risk leaking the ground-truth answer.</li>
</ol>
<p>To strike this balance, we derive the initial search point from the annotated coarse-grained hypothesis h c by applying an ambiguation procedure.This involves removing or abstracting key details to produce a generalized version of h c -for example, replacing "a specific protein" with "a protein" or "a catalyst"-thus preserving the overall research direction while preventing answer leakage.</p>
<p>Consequently, even when the search begins in the correct conceptual region, the model may naturally diverge toward a nearby but distinct local optimum, especially given the openness of the hypothesis space and the heuristic-driven nature of the optimization process.</p>
<p>E.2 Coverage of Experimentally Critical Details</p>
<p>In addition to alignment with the reference hypotheses, we evaluated the extent to which the generated hypotheses captured the critical experimental details required for practical implementation.</p>
<p>Among all the details mentioned in the generated hypotheses, approximately 40% are experimentally important-regardless of their accuracy (which is not the focus of this analysis).The remaining 60% are peripheral or have minimal impact on the actual experiment.</p>
<p>Among all the important details that should be included, about 50% are mentioned in the generated hypotheses.</p>
<p>Peripheral details refer to contextual or environmental factors with limited relevance to the core experiment-for instance, ambient humidity or weather conditions, which may only affect specific reactions.</p>
<p>This highlights a key challenge: while LLMs can generate rich and context-aware hypotheses, they often fail to prioritize the most essential components for experimental planning.Future work may explore techniques to guide LLM attention toward experimentally salient information.</p>
<p>F Error Analysis</p>
<p>Two PhD-level chemistry experts conducted the error analyses.</p>
<p>G Hypothesis Search Prompt</p>
<p>The following prompt is used to guide both the baseline methods and our proposed method, HHS, during the hypothesis refinement process.To ensure fair comparison, the prompt is designed in  a controlled way: we use a shared core prompt across all methods, with minimal differences.Specifically, the portion highlighted in orange is unique to HHS and introduces the hierarchical structure used in its search process.</p>
<p>This design isolates the effect of hierarchical search.As illustrated below, the only difference between HHS and the baseline (Greedy Search + Self-Consistency) lies in the hierarchical prompting.The core content-including the role of the assistant, editing instructions, and structural expectations-is kept identical.</p>
<p>This enables a controlled ablation-style comparison, attributing observed improvements specifically to the hierarchical design.</p>
<p>The complete prompt is as follows:</p>
<p>You are assisting with scientist's research.Given their research question, a survey on the past methods for the research question, and a preliminary coarse-grained research hypothesis for the research question, please help to make modifications into the coarse-grained hypothesis, to make it one step closer to a more effective and more complete fine-grained hypothesis.</p>
<p>The modification can be two-folds: (1): delete or change an existing improper detail or information in the existing hypothesis; (2) add and integrate one detail to the existing hypothesis.If you choose to add a detail, do not simply append new information to the existing hypothesis.Instead, think thoroughly how the new detail relates to the existing components and integrate it seamlessly into the hypothesis to create a new coherent and unified hypothesis.In addition if you choose to add a detail to a general information, if the corresponding general information is correct, you should try to keep the corresponding general information in the updated hypothesis and also mention the details, instead of replacing the general information with the details.In this way, it would be much easier for scientists to understand both the general infomration/structure and the details from your generated hypothesis.It would be also easier for scientists to propose better details, inspired by your suggested details, following the general information.</p>
<p>Please remind that this is about research: research is about discover a new solution to the problem that ideally is more effective and can bring new insights.Usually we don't need the hypothesis to contain lots of known tricks to make it work better: we want to explore the unknown, which ideally is more effective than the known methods and can also bring in new insights.Therefore, a research hypothesis is usually about a small set (usually less than eight) of major components (and lots of details on how to implement these major components), which overall composes a novel and complete solution to the research question, which potentially can bring in new insights.Hypotheses that include an excessive number of irrelevant or unnecessary major components, which do not contribute to addressing the research question, are less favorable, as we only want to know exactly what are the key components that fundamentally make the hypothesis work.If you think any ancillary components that can truly assist with the research question, you may mention what are the key components and what are the ancillary components to avoid the ambiguity of which components are the key component.</p>
<p>The reaction mechanism, however, is not classified as a major component or detail (and therefore not limited by the number of major components).Instead, a novel and valid reaction mechanism can be a good source of insights.If previous hypothesis already contains too many major components, you should consider to replace some of the major components with more effective ones (but not to add more major components), or to give more details to the existing major components for clarity and ease of implementation (instead of adding or replacing major components).</p>
<p>Here we are searching for the fine-grained hypothesis in a hierarchical way.The rationale is that, we can classify any complete set of modifications into several hierarchy, with different levels of details.If we do not search in a hierarchical way, we need to consider all the available details in all hierarchy levels for each search step, which (1) has a very high complexity, and (2) first search a low-level detail might largely influence the following search of a high-level detail: it might stuck in one high-level detail corresponding to the already searched low-level detail without considering the other low-level details corresponding to other high-level details, making the search process stuck in a local minumum at the beginning.</p>
<p>Here we roughly classify all possible modifications into five hierarchies: (1) Mechanism of the Reaction: Describes how the reaction proceeds at a conceptual level, focusing on electron flow, bond formation and breaking, and any intermediates or transition states involved.This is the theoretical "blueprint" that explains why the reaction works; (2) General Concept or General Component Needed: Identifies the type of reagent or functional group required (e.g., "a strong acid," "a Lewis base," "an activated aromatic ring") without committing to a specific chemical.It outlines the broader roles that are necessary for the mechanism to proceed; (3) Specific Components for the General Concept: Narrows down from the general category to a particular substance (e.g., "concentrated HCl" for a strong acid, "benzene" for an aromatic ring).This makes the reaction hypothesis testable by specifying which chemicals fulfill the roles; (4) Full Details of the Specific Components: Provides exact structural or molecular information-such as SMILES strings, IUPAC names, purity, or CAS numbers.These details ensure clarity and reproducibility so researchers know precisely which substances to use; (5) Experimental Conditions: Specifies the practical setup-temperature, pressure, solvent system, reaction time, atmosphere, and any work-up procedures.This final layer describes how to carry out the reaction in a laboratory setting.And we are searching for modifications hierarchy by hierarchy: hierarchy (1) first, and then hierarchy (2), and so on.Hypothesis from a higher hierarchy is an expansion of the hypothesis from its previous hierarchy, with additional information described above.</p>
<p>The research question is:</p>
<p>The survey is:</p>
<p>Now please help to make modifications into the coarse-grained hypothesis, to make it one step closer to a more effective and more complete fine-grained hypothesis.Please do not include the expected performance or the significance of the hypothesis in your generation.Please answer the question in the following response format.(response format: 'Reasoning Process: Revised Hypothesis: ')</p>
<p>H Experiment Compute Resources</p>
<p>We implement our proposed framework as an agentic LLM system using GPT-4o-mini using OpenAI's official API.Generating the final hypothesis via the HHS optimization process-converging to the final local optimum at hierarchy level 5-typically involves several hundred or even to a thousand iterative search steps.</p>
<p>I Limitation</p>
<p>While HHS consistently discovers higher-quality local optima compared to baseline methods, it does not guarantee convergence to the global optimum.Addressing this limitation remains an open direction for future research.</p>
<p>This formulation turns P (h f | b, h c ) into a combinatorial optimization problem, where the objective is to select a subset of edits d 1 , . . ., d m ⊆ D. Let |D| = n and |d 1 , . . ., d m | = m.The search space has at least combinatorial complexity C m n = n! m!(n−m)! .This makes the problem particularly challenging due to three factors: (1) both m and n are unknown;</p>
<p>Figure 1 :
1
Figure 1: Hierarchies designed for chemistry and material science by PhD-level domain expert.</p>
<p>Figure 2 :
2
Figure 2: Overview of the proposed Hierarchical Heuristic Search (HHS) framework.</p>
<p>Figure 3 :
3
Figure 3: The smoothing effect of hierarchy on the reward landscape of hypothesis space.</p>
<p>Figure 4 :
4
Figure 4: Hierarchical design as a low-pass filtering over the spectrum of the reward landscape.</p>
<p>Figure 5 :
5
Figure 5: Overview of the input and output of the Hierarchical Heuristic Search (HHS) framework, also known as MOOSE-Chem2.</p>
<p>Figure 5
5
Figure5presents an overview of the input and output of the Hierarchical Heuristic Search (HHS) framework, also referred to as MOOSE-Chem2.The input, a coarse-grained research direction, may range from a brief sentence outlining a general research direction to a hypothesis that already includes many methodological and experimental details-such as one produced by the MOOSE-Chem(Yang et al., 2025) framework.</p>
<p>Instance-Level Component Choice Parametric Specification of Components Full Experimental Configurations
,Recombination,,, , , ,The th hierarchyThe hierarchy thno:: research background : hypothesis direction,,yes: Add one at level?if no for a consecutive of times: final output from hierarchy : a candidate to consider for final output 𝐻𝑖 − 1 = ∅ if 𝑖 ≤ 1 {ℎ 1 , … , ℎ 𝑖 − 1 } if 𝑖 ≥ 2feedbackThe th hierarchy: in a previous search step : in the current search stepHierarchy 1ReactionMechanismHierarchy 2ComponentClass SelectionHierarchy 3Hierarchy 4Hierarchy 5ReactionMechanismGeneral Conceptor GeneralComponentNeededSpecific Components forthe General ConceptFull Details of theSpecific ComponentsExperimental Conditions</p>
<p>Table 1 :
1
Comparison between HHS and baseline methods across LLM-based and expert evaluations.
Win74.51%41.18%71.57%67.65%73.53%76.47%Tie18.63%18.63%28.43%10.78%18.63%15.69%Lose6.86%40.20%0.00%21.57%7.84%7.84%HHS v.s. Greedy Search + Self-consistencyWin59.31%42.16%56.37%48.53%53.43%74.51%Tie24.02%8.33%43.14%18.63%33.82%17.65%Lose16.67%49.51%0.49%32.84%12.75%7.84%Greedy Search + Self-consistency v.s. Greedy SearchWin57.84%48.04%29.41%51.96%54.90%62.75%Tie22.55%11.76%65.69%18.63%34.31%21.57%Lose19.61%40.20%4.90%29.41%10.78%15.69%
Effectiveness (LLM) Novelty (LLM) Detailedness (LLM) Feasibility (LLM) Overall (LLM) Overall (Expert) HHS v.s.Greedy Search 3.3 Q2: Whether Hypotheses Judged Better by LLMs Exhibit Stronger Alignment With</p>
<p>Table 2 :
2
Recall of ground-truth components by discovered hypotheses.#Stepsrepresents the number of reasoning steps used.HHS represents HHS-3 referred in § 3.5.As shown in Table2, the hypotheses discovered by HHS-corresponding to better local optima than those produced by greedy search baselines-achieve consistently higher recall scores than both greedy search baselines and other comparative methods.Here, in-context RL refers to a mechanism in which, if the current hypothesis h cur does not outperform the previous one h prev , h cur is inserted into the LLM's context to generate a new candidate.We also report the total number of reasoning steps used by each method.A general trend emerges: increasing the number of reasoning steps tends to improve recall up to a point, beyond which excessive steps lead to diminishing or negative returns.Whether Defining the Reward Landscape With an Ensemble of Diverse LLMs Yields Better Outcomes Than Using the Same Number of the Strongest LLMs Among Them?
Soft Recall Hard Recall #StepsChemCrow (M. Bran et al., 2024)12.28%7.20%-Qi et al. (2024)19.57%11.15%-SciMON (Wang et al., 2024)18.57%10.09%-MOOSE (Yang et al., 2024)20.04%11.76%-MOOSE-Chem (Yang et al., 2025)19.99%11.98%-Greedy Search16.60%9.92%9.69w/ In-context RL16.76%10.28%16.80w/ Self-consistency31.53%17.73%67.55HHS (HHS-3)40.35%23.04%282.04w/ In-context RL33.63%21.29%531.08w/ Single LLM Gradient (HHS-1)32.40%19.95%747.92
Given the lack of established metrics for this task, we introduce an LLM-based evaluation that measures how well the discovered hypotheses recover the methodological and experimental details of the ground-truth hypotheses.The detailed formulations of the two metrics-Soft Recall and Hard Recall-are provided in Appendix C.</p>
<p>Table 4 :
4
Pairwise comparison between HHS-1 and HHS-3 with LLM-based evaluation.
Effectiveness (LLM) Novelty (LLM) Detailedness (LLM) Feasibility (LLM) Overall (LLM)HHS-1 v.s. HHS-3Win21.08%25.49%4.41%41.67%8.82%Tie57.35%28.92%94.12%28.92%82.35%Lose21.57%45.59%1.47%29.41%8.82%</p>
<p>Initiation of Single-Electron Transfer (SET): Irradiation of phosphorus ylides with specific wavelengths of visible light (400-450 nm) from a high-intensity LED source (approximately 20 mW/cm 2 ), validated by studies demonstrating effective radical generation at this intensity(Smith et al., 2020), promotes SET using suitable photoredox catalysts (e.g., [Ru(bpy) 3 ] 2+ or [Ir(dF(CF 3 )ppy) 2 (bpy)]).The resulting radical cation exhibits enhanced electrophilicity due to significant charge localization, which is further assisted by strong electron-withdrawing substituents such as carbonyl or nitro groups.Empirical evidence indicates an increase in reactivity by up to 2.5-fold as supported by Hammett parameters.2.Stabilization via Zwitterionic Intermediate: The radical cation transitions to a zwitterionic intermediate, characterized by resonance stabilization through delocalized π-electrons and non-covalent interactions, such as hydrogen bonding in polar aprotic solvents like acetonitrile (dielectric constant ≈ 37) and DMSO (dielectric constant ≈ 47).To optimize stabilization, a 1:1 (v/v) mixture of these solvents will be used, taking advantage of their combined dielectric properties (≈ 38) to enhance charge separation and stabilize reactive intermediates.Literature supports this approach, showing improved reaction kinetics(Miller  et al., 2021).3. Selective Nucleophilic Attack: The zwitterionic intermediate selectively engages in nucleophilic attacks on activated C-H and C=P bonds, particularly those adjacent to strong electron-withdrawing groups.</p>
<p>Table 5 summarizes the main error types observed in hypotheses generated by HHS, while Table 6 analyzes the reasons why HHS outperforms the greedy search baseline.</p>
<p>Table 5 :
5
Error analysis of hypotheses generated by HHS.Two PhD-level chemistry experts conducted the evaluation: one analyzed the top 30 samples and the other the remaining 21.
Missing key chemical substances14/30Excessive details in characterization methods28/30Feasibility issues18/30Limitations of characterization methods08/30Insufficient basis for material selection22/30Lack of design comparison experiments12/30Ignoring data validation and reproducibility10/30Severe deviation from feasibility8/21Missing or incorrect key chemicals or reaction systems 9/21Incorrect explanation of chemical principles12/21Incorrect prediction of experimental system10/21</p>
<p>Table 6 :
6
Error analysis on why HHS's hypotheses are better than the greedy search baselines'.Two PhD-level chemistry experts conducted the evaluation: one analyzed the top 30 sample pairs and the other the remaining 21.
Insufficient performance metrics25/30Complexity of experimental conditions16/30Insufficient explanation details29/30Inadequate description of preparation plan 22/30Vague research objectives28/30Cost and scalability issues13/30Poor feasibility12/21Errors in research plan details21/21Insufficient explanation details19/21Clear experimental system21/21
All code and data can be found in https://github.com/ZonglinY/MOOSE-Chem2 ‡ Contribution during internship at Shanghai Artificial Intelligence Laboratory. † Corresponding author. 39th Conference on Neural Information Processing Systems (NeurIPS
).
AcknowledgmentsThis work was supported by a locally commissioned task from the Shanghai Municipal Government.This work is supported by Shanghai Artificial Intelligence Laboratory.This research/project is supported by the Ministry of Education, Singapore under its MOE Academic Research Fund Tier 2 (STEM RIE2025 Award MOE-T2EP20123-0005).
The claude 3 model family: Opus, sonnet. Anthropic, March 2024</p>
<p>Seven pillars for the future of artificial intelligence. Erik Cambria, Rui Mao, Melvin Chen, Zhaoxia Wang, Seng-Beng Ho, IEEE Intelligent Systems. 3862023</p>
<p>. Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Soroosh Wang, Yifan Mariooryad, Xinyang Ding, Fred Geng, Roy Alcober, Mark Frostig, Lexi Omernick, Cosmin Walker, Christina Paduraru, Andrea Sorokin, Colin Tacchetti, Samira Gaffney, Olcan Daruki, Zach Sercinoglu, Juliette Gleicher, Paul Love, Rohan Voigtlaender, Gabriela Jain, Kareem Surita, Rory Mohamed, Junwhan Blevins, Tao Ahn, Kornraphop Zhu, Orhan Kawintiranon, Yiming Firat, Yujing Gu, Matthew Zhang, Manaal Rahtz, Natalie Faruqui, Justin Clay, Gilmer, Ivo Co-Reyes, Rui Penchev, Nobuyuki Zhu, Kevin Morioka, Krishna Hui, Victor Haridasan, Mahdis Campos, Mandy Mahdieh, Samer Guo, Kevin Hassan, Arpi Kilgour, Vezer, Heng-Tze, Raoul Cheng, Siddharth De Liedekerke, Paul Goyal, Barham, Seb Strouse, Jonas Noury, Mukund Adler, Sharad Sundararajan, Dmitry Vikram, Michela Lepikhin, Xavier Paganini, Fan Garcia, Dasha Yang, Maja Valter, Kiran Trebacz, Chulayuth Vodrahalli, Roman Asawaroengchai, Norbert Ring, Kalb, Baldini Livio, Siddhartha Soares, David Brahma, Tianhe Steiner, Fabian Yu, Antoine Mentzer, Lucas He, Bibo Gonzalez, Raphael Xu, Laurent El Lopez Kaufman, Junhyuk Shafey, Tom Oh, George Hennigan, Seth Van Den Driessche, Mario Odoom, Becca Lucic, Sid Roelofs, Amit Lall, Betty Marathe, Santiago Chan, Luheng Ontanon, Denis He, Jonathan Teplyashin, Phil Lai, Bogdan Crone, Lewis Damoc, Sebastian Ho, Karel Riedel, Chih-Kuan Lenc, Aakanksha Yeh, Yang Chowdhery, Mehran Xu, Ehsan Kazemi, Anastasia Amid, Kevin Petrushkina, Ali Swersky, Gowoon Khodaei, Chris Chen, Mario Larkin, Geng Pinto, Adria Puigdomenech Yan, Piyush Badia, Steven Patil, Dave Hansen, Orr, M R Sebastien, Jordan Arnold, Andrew Grimstad, Sholto Dai, Rishika Douglas, Vikas Sinha, Xi Yadav, Elena Chen, Jacob Gribovskaya, Jeffrey Austin, Kaushal Zhao, Paul Patel, Sophia Komarek, Sebastian Austin, Linda Borgeaud, Abhimanyu Friso, Ben Goyal, Kris Caine, Cao, Da-Woon, Matthew Chung, Gabe Lamm, Thais Barth-Maron, Kate Kagohara, Mia Olszewska, Kaushik Chen, Rishabh Shivakumar, Harshal Agarwal, Ravi Godhia, Javier Rajwar, Xerxes Snaider, Yuan Dotiwalla, Aditya Liu, Victor Barua, Yuan Ungureanu, Bat-Orgil Zhang, Mateo Batsaikhan, James Wirth, Ivo Qin, Tulsee Danihelka, Martin Doshi, Jilin Chadwick, Sanil Chen, Quoc Jain, Arjun Le, Madhu Kar, Cheng Gurumurthy, Ruoxin Li, Fangyu Sang, Lampros Liu, Rich Lamprou, Nathan Munoz, Harsh Lintz, Heidi Mehta, Malcolm Howard, Lora Reynolds, Quan Aroyo, Lorenzo Wang, Albin Blanco, Jordan Cassirer, Dipanjan Griffith, Stephan Das, Jakub Lee, Zach Sygnowski, James Fisher, Richard Besley, Zafarali Powell, Dominik Ahmed, David Paulus, Zalan Reitter, Rishabh Borsos, Aedan Joshi, Steven Pope, Vittorio Hand, Vihan Selo, Nikhil Jain, Megha Sethi, Takaki Goel, Rhys Makino, Zhen May, Johan Yang, Christina Schalkwyk, Anja Butterfield, Alex Hauth, Will Goldin, Evan Hawkins, Sergey Senter, Oliver Brin, Marvin Woodman, Eric Ritter, Minh Noland, Vijay Giang, Lisa Bolina, Tim Lee, Ian Blyth, Machel Mackinnon, Obaid Reid, David Sarvana, Alexander Silver, Lily Chen, Loren Wang, Oscar Maggiore, Nithya Chang, Gregory Attaluri, Chung-Cheng Thornton, Oskar Chiu, Nir Bunyan, Timothy Levine, Evgenii Chung, Xiance Eltyshev, Timothy Si, Demetra Lillicrap, Vaibhav Brady, Boxi Aggarwal, Yuanzhong Wu, Ross Xu, Kartikeya Mcilroy, Paramjit Badola, Erica Sandhu, Wojciech Moreira, Ross Stokowiec, Dong Hemsley, Alex Li, Pranav Tudor, Elahe Shyam, Salem Rahimtoroghi, Pablo Haykal, Xiang Sprechmann, Diana Zhou, Yujia Mincu, Ravi Li, Kalpesh Addanki, Xiao Krishna, Alexandre Wu, Matan Frechette, Allan Eyal, Dave Dafoe, Jay Lacey, Thi Whang, Ye Avrahami, Emanuel Zhang, Hanzhao Taropa, Lin ; Xi, Ce Xiong, Fabio Zheng, Xiaowei Pardo, Dan Li, Joe Horgan, Moran Stanton, Fei Ambar, Alejandro Xia, Mingqiu Lince, Basil Wang, Albert Mustafa, Hyo Webson, Rohan Lee, Martin Anil, Timothy Wicke, Abhishek Dozat, Enrique Sinha, Elahe Piqueras, Shyam Dabir, Anudhyan Upadhyay, Lisa Anne Boral, Corey Hendricks, Josip Fry, Yi Djolonga, Jake Su, Jane Walker, Ronny Labanowski, Vedant Huang, Jeremy Misra, R J Chen, Avi Skerry-Ryan, Shruti Singh, Dian Rijhwani, Yu ; Yury, Shaobo Sulsky, Tom Le Hou, Antoine Paine, Jason Yang, Dominika Riesa, Dror Rogozinska, Dalia El Marcus, Qiao Badawy, Luyu Zhang, Helen Wang, Jeremy Miller, Greer, arXiv:2403.05530Lars Lowe Sjos. HyunJeong Choe, Alex Tomala, Chalence Safranek-Shrader, Nora Kassner, Mantas Pajarskas, Matt Harvey, Sean Sechrist, Meire Fortunato, Christina Lyu, Gamaleldin Elsayed, Chenkai Kuang, James Lottes, Eric Chu, Chao Jia, Chih-Wei Chen, Peter Humphreys, Kate Baumli, Connie Tao, Rajkumar Samuel, Cicero Nogueira dos Santos, Anders Andreassen, Nemanja Rakićević, Dominik Grewe, Aviral Kumar, Stephanie Winkler, Jonathan Caton, Andrew Brock, Hannah Sheahan, Iain Barr, Yingjie Miao, Paul Natsev, Jacob Devlin, Feryal Behbahani, Flavien Prost, Yanhua Sun, Artiom Myaskovsky, Thanumalayan Sankaranarayana Pillai, Dan Hurt, Angeliki Lazaridou,2024Azade Nova, Heiga ZenSumit Bagri, Arnar Mar Hrafnkelsson, Marcello Maggioni, Daniel Zheng,arXiv preprintRahma Chaabouni, Mihaela Rosca, Jiepu Jiang, Charlie Chen, Ruibo Liu, Tara Sainath, Maxim Krikun, Alex Polozov, Jean-Baptiste Lespiau, Josh Newlan, Zeynep Cankara, Soo Kwak, Yunhan Xu, Phil Chen, Andy Coenen, Clemens Meyer, Katerina Tsihlas, Ada Ma, Juraj Gottweis, Jinwei Xing, and Gu. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</p>
<p>Nova: An iterative planning and search approach to enhance novelty and diversity of LLM generated ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, 10.48550/arXiv.2410.142552024</p>
<p>Split and merge: Aligning position biases in llm-based evaluators. Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, Yang Liu, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Moose-chem3: Toward experiment-guided hypothesis ranking via simulated experimental feedback. Wanhao Liu, Zonglin Yang, Jue Wang, Lidong Bing, Di Zhang, Dongzhan Zhou, Yuqiang Li, Houqiang Li, Erik Cambria, Wanli Ouyang, arXiv:2505.178732025aarXiv preprint</p>
<p>Researchbench: Benchmarking llms in scientific discovery via inspiration-based task decomposition. Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou, arXiv:2503.212482025barXiv preprint</p>
<p>Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, Xinya Du, arXiv:2501.04306Llm4sr: A survey on large language models for scientific research. 2025arXiv preprint</p>
<p>Augmenting large language models with chemistry tools. Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller, Nature Machine Intelligence. 652024</p>
<p>Alphaevolve: A coding agent for scientific and algorithmic discovery. Alexander Novikov, Ngân Vũ, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Abbas Francisco Jr Ruiz, Mehrabian, arXiv:2506.13131OpenAI. Gpt-4o mini: Advancing cost-efficient intelligence. 2025. 2024arXiv preprint</p>
<p>Large language models as biomedical hypothesis generators: A comprehensive evaluation. Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Jinfang Hu, Bowen Zhou, First Conference on Language Modeling. 2024</p>
<p>Mathematical discoveries from program search with large language models. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, Nature. 62579952024</p>
<p>Llm-sr: Scientific equation discovery via programming with large language models. Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, Chandan K Reddy, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Can llms generate novel research ideas? A largescale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2025</p>
<p>Visual autoregressive modeling: Scalable image generation via next-scale prediction. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, Liwei Wang, Advances in neural information processing systems. 202437</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.18653/v1/2024.acl-long.18Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 11-16, 202412024ACL 2024</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, 10.18653/v1/2024.findings-acl.804Findings of the Association for Computational Linguistics, ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 11-16, 20242024and virtual meeting</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2025</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6643 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6643</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6643</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-7919cb1a1dcf70ed7803c43a71d43dba696ef149</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7919cb1a1dcf70ed7803c43a71d43dba696ef149" target="_blank">Making Language Models Better Tool Learners with Execution Feedback</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Experimental results show that TRICE can make the large language model selectively use tools by improving the accuracy of tool usage while enhancing insufficient tool learning and mitigating excessive reliance on tools.</p>
                <p><strong>Paper Abstract:</strong> Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed by further analysis, show that TRICE can make the large language model selectively use tools by improving the accuracy of tool usage while enhancing insufficient tool learning and mitigating excessive reliance on tools.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6643.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6643.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5_math</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (math reasoning evaluation in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 (text-davinci-003 via OpenAI API) is used as a high-performing prompt-based baseline on arithmetic word-problem benchmarks (ASDiv, SVAMP, GSM8K) in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Making Language Models Better Tool Learners with Execution Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003, via OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only (autoregressive transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper (pretrained foundation model; standard web/code/text pretraining assumed but not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ASDiv, SVAMP, GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Math reasoning — arithmetic word problems / multi-step arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language math word problems (instruction-following), evaluated by checking the last predicted number</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school to multi-step arithmetic (GSM8K = multi-step grade-school problems; ASDiv/SVAMP include simpler arithmetic word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Prompt-based evaluation via API (prompting; appears as the strong baseline in prompt-based rows — zero-shot/few-shot as used in paper's prompt baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (lenient: matches on last number predicted by model)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ASDiv: 64.6% ; SVAMP: 62.0% ; GSM8K: 19.8%</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic/activation-level probing reported for GPT-3.5; analysis is behavioral comparison only (accuracy numbers used as baseline). The paper does not inspect internal representations or token-level numeric processing for GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Relatively low performance on GSM8K (multi-step) indicating difficulty on multi-step arithmetic; no internal failure-mode probing provided in the paper for GPT-3.5 specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Paper does not report experiments varying GPT-3.5 size; indicates GPT-3.5 is a strong prompt baseline on simpler sets (ASDiv/SVAMP) but struggles on GSM8K compared to some Trice-trained smaller models in aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Making Language Models Better Tool Learners with Execution Feedback', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6643.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6643.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM-6B_math</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM-6B (math reasoning experiments in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGLM-6B (encoder-decoder architecture) is evaluated on arithmetic word-problem datasets and trained with the Trice pipeline (Behavior Cloning + RLEF) to learn selective calculator use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Making Language Models Better Tool Learners with Execution Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>encoder-decoder transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B (as reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper for pretraining; experiments fine-tune with LoRA on Trice-generated datasets (math instructive data + tool API pseudo-labels).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ASDiv, SVAMP, GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Math reasoning — arithmetic word problems (calculator-assisted where needed)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language math word problems; tool API format for calculator calls (calculator(formula))</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school to multi-step arithmetic (includes GSM8K multi-step problems)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Multiple modes compared: prompt-based zero/few-shot baselines; supervised fine-tuning (0% or 100% tool exposure); Trice Behavior Cloning (instruct-tuning) and RLEF (reinforcement learning with execution feedback). Tool usage is via explicit calculator API calls.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (lenient check: last number predicted)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Prompt Zero/Few-Shot baseline much lower; Trice results — Trice-SPLit: ASDiv 72.9%, SVAMP 64.0%, GSM8K 12.4%; Trice-MIX: ASDiv 75.6%, SVAMP 65.5%, GSM8K 15.8%. (Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper provides behavioral analyses: (1) Stage I (Behavior Cloning) teaches API-invocation schema and increases tool usage rate; (2) Stage II (RLEF) uses execution feedback (ranking loss + SFT) to reduce excessive tool dependence and improve tool-input correctness and use of tool return values. No neuron-level or attention-level mechanistic analysis of numeric processing is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Observed issues include (a) excessive reliance on tools after Stage I due to imbalanced data distributions, (b) incorrect generation of tool inputs (wrong formula passed to calculator), (c) improper use of tool return values leading to wrong final answers, and (d) residual tool-use errors possibly linked to small model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Experiments are on 6B models; Trice (especially Trice-MIX) improves performance over prompt and naive supervised fine-tuning. Larger scales not evaluated; authors note scale may limit tool-learning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Making Language Models Better Tool Learners with Execution Feedback', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6643.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6643.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca-7B_math</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca-7B (math reasoning experiments in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Alpaca-7B (LLaMA-based, decoder-only) is fine-tuned with LoRA and trained under the Trice pipeline to learn selective calculator use for arithmetic word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Making Language Models Better Tool Learners with Execution Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (as reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretraining details not specified in this paper; experimental fine-tuning uses LoRA on Trice datasets (pseudo-labeled tool API pairs generated with ChatGPT and model predictions).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ASDiv, SVAMP, GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Math reasoning — arithmetic word problems with optional calculator tool</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems; tool API calls in the form calculator(formula) when model elects to use tool</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school to multi-step arithmetic (GSM8K included)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Baseline prompt-based zero/few-shot; supervised fine-tuning (0% and 100% tool exposure); Trice two-stage training (Behavior Cloning then RLEF). Candidate responses used in RLEF were collected from multiple LLMs. Tool usage is explicit via API call.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (lenient, last-number matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Trice-SPLit: ASDiv 73.4%, SVAMP 45.0%, GSM8K 16.3%; Trice-MIX: ASDiv 75.2%, SVAMP 58.0%, GSM8K 21.5%. (Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral analysis only: Stage I provides basic ability to generate calculator calls; Stage II's execution feedback improves correct decision-making about when to call the calculator, improves the correctness of tool inputs, and reduces error propagation. No internal interpretability (e.g., attention or neuron probing) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Similar failure modes as for other small models: overuse of tools after supervised instruct-tuning, occasional incorrect formula generation for calculator inputs, wrong interpretation of tool outputs, and limitations purportedly tied to model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Trice substantially improves Alpaca-7B performance compared to prompt baselines and standard supervised fine-tuning. Mixed-task training (Trice-MIX) further improves generalization; no experiments on larger parameter counts were conducted in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Making Language Models Better Tool Learners with Execution Feedback', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6643.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6643.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-7B_math</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna-7B (math reasoning experiments in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vicuna-7B (LLaMA-derived, decoder-only) is evaluated on arithmetic word-problem datasets and trained with Trice; the paper reports that Trice-MIX yields the strongest gains among the 6–7B models on math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Making Language Models Better Tool Learners with Execution Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (as reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretraining not detailed in this paper; experiments fine-tune Vicuna-7B with LoRA on Trice datasets (behavior cloning + RLEF using pseudo-labeled tool APIs and candidate responses from multiple LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ASDiv, SVAMP, GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Math reasoning — arithmetic word problems (calculator as external tool)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems; when model decides to use the tool it outputs calculator(formula) API call; evaluation checks last predicted number.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school arithmetic up to multi-step (GSM8K multi-step problems included)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Prompt-based baselines (zero/few-shot) and Trice two-stage training (Behavior Cloning then RLEF). Candidate responses for ranking in RLEF came from ChatGPT, InstructGPT, Vicuna, Alpaca, and the gold regulated output.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (lenient: last-number matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Trice-SPLit: ASDiv 72.6%, SVAMP 49.0%, GSM8K 16.6%; Trice-MIX: ASDiv 81.2%, SVAMP 60.5%, GSM8K 21.8%. For comparison, Vicuna few-shot/zero-shot baselines were much lower (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper's interpretive analysis is behavioral: (a) Stage I (Behavior Cloning) establishes tool-invocation capability but tends to increase tool usage frequency; (b) Stage II (RLEF) leverages execution feedback (ranking loss tied to correctness and alignment with gold tool usage) to reduce unnecessary tool calls and improve correctness of calculator inputs and downstream answers. The paper includes tool-use rate statistics, case studies, and observations about insufficient tool learning in some tasks. No low-level mechanistic probing (e.g., logit lens, attention/neuron analysis) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Documented failure modes include: excessive dependence on tools after Stage I (leading to error propagation), incorrect tool input formulas, misuse or misinterpretation of tool outputs, and remaining tool-use errors especially on harder multi-step GSM8K problems. Authors attribute some errors to limited backbone scale (6–7B).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Trice produces consistent improvements for 7B Vicuna; Trice-MIX (multi-task mix) outperforms Trice-SPLit, indicating benefits from multi-tool/multi-task training. The study did not evaluate larger models, so broader scaling trends (e.g., emergent behavior at larger sizes) are not established in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Making Language Models Better Tool Learners with Execution Feedback', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Are NLP models really able to solve simple math word problems? <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>PAL: program-aided language models <em>(Rating: 1)</em></li>
                <li>TALM: tool augmented language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6643",
    "paper_id": "paper-7919cb1a1dcf70ed7803c43a71d43dba696ef149",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "GPT-3.5_math",
            "name_full": "GPT-3.5 (math reasoning evaluation in this paper)",
            "brief_description": "GPT-3.5 (text-davinci-003 via OpenAI API) is used as a high-performing prompt-based baseline on arithmetic word-problem benchmarks (ASDiv, SVAMP, GSM8K) in this study.",
            "citation_title": "Making Language Models Better Tool Learners with Execution Feedback",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003, via OpenAI API)",
            "model_family": "decoder-only (autoregressive transformer)",
            "model_size": "not specified in this paper",
            "training_data_description": "Not specified in this paper (pretrained foundation model; standard web/code/text pretraining assumed but not detailed here).",
            "benchmark_name": "ASDiv, SVAMP, GSM8K",
            "task_type": "Math reasoning — arithmetic word problems / multi-step arithmetic",
            "problem_format": "Natural-language math word problems (instruction-following), evaluated by checking the last predicted number",
            "difficulty_level": "Grade-school to multi-step arithmetic (GSM8K = multi-step grade-school problems; ASDiv/SVAMP include simpler arithmetic word problems)",
            "prompting_method": "Prompt-based evaluation via API (prompting; appears as the strong baseline in prompt-based rows — zero-shot/few-shot as used in paper's prompt baselines)",
            "performance_metric": "Accuracy (lenient: matches on last number predicted by model)",
            "performance_value": "ASDiv: 64.6% ; SVAMP: 62.0% ; GSM8K: 19.8%",
            "internal_analysis": "No mechanistic/activation-level probing reported for GPT-3.5; analysis is behavioral comparison only (accuracy numbers used as baseline). The paper does not inspect internal representations or token-level numeric processing for GPT-3.5.",
            "failure_modes": "Relatively low performance on GSM8K (multi-step) indicating difficulty on multi-step arithmetic; no internal failure-mode probing provided in the paper for GPT-3.5 specifically.",
            "scaling_trend": "Paper does not report experiments varying GPT-3.5 size; indicates GPT-3.5 is a strong prompt baseline on simpler sets (ASDiv/SVAMP) but struggles on GSM8K compared to some Trice-trained smaller models in aggregate.",
            "uuid": "e6643.0",
            "source_info": {
                "paper_title": "Making Language Models Better Tool Learners with Execution Feedback",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGLM-6B_math",
            "name_full": "ChatGLM-6B (math reasoning experiments in this paper)",
            "brief_description": "ChatGLM-6B (encoder-decoder architecture) is evaluated on arithmetic word-problem datasets and trained with the Trice pipeline (Behavior Cloning + RLEF) to learn selective calculator use.",
            "citation_title": "Making Language Models Better Tool Learners with Execution Feedback",
            "mention_or_use": "use",
            "model_name": "ChatGLM-6B",
            "model_family": "encoder-decoder transformer",
            "model_size": "6B (as reported in paper)",
            "training_data_description": "Not specified in this paper for pretraining; experiments fine-tune with LoRA on Trice-generated datasets (math instructive data + tool API pseudo-labels).",
            "benchmark_name": "ASDiv, SVAMP, GSM8K",
            "task_type": "Math reasoning — arithmetic word problems (calculator-assisted where needed)",
            "problem_format": "Natural-language math word problems; tool API format for calculator calls (calculator(formula))",
            "difficulty_level": "Grade-school to multi-step arithmetic (includes GSM8K multi-step problems)",
            "prompting_method": "Multiple modes compared: prompt-based zero/few-shot baselines; supervised fine-tuning (0% or 100% tool exposure); Trice Behavior Cloning (instruct-tuning) and RLEF (reinforcement learning with execution feedback). Tool usage is via explicit calculator API calls.",
            "performance_metric": "Accuracy (lenient check: last number predicted)",
            "performance_value": "Prompt Zero/Few-Shot baseline much lower; Trice results — Trice-SPLit: ASDiv 72.9%, SVAMP 64.0%, GSM8K 12.4%; Trice-MIX: ASDiv 75.6%, SVAMP 65.5%, GSM8K 15.8%. (Table 3)",
            "internal_analysis": "Paper provides behavioral analyses: (1) Stage I (Behavior Cloning) teaches API-invocation schema and increases tool usage rate; (2) Stage II (RLEF) uses execution feedback (ranking loss + SFT) to reduce excessive tool dependence and improve tool-input correctness and use of tool return values. No neuron-level or attention-level mechanistic analysis of numeric processing is provided.",
            "failure_modes": "Observed issues include (a) excessive reliance on tools after Stage I due to imbalanced data distributions, (b) incorrect generation of tool inputs (wrong formula passed to calculator), (c) improper use of tool return values leading to wrong final answers, and (d) residual tool-use errors possibly linked to small model scale.",
            "scaling_trend": "Experiments are on 6B models; Trice (especially Trice-MIX) improves performance over prompt and naive supervised fine-tuning. Larger scales not evaluated; authors note scale may limit tool-learning ability.",
            "uuid": "e6643.1",
            "source_info": {
                "paper_title": "Making Language Models Better Tool Learners with Execution Feedback",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Alpaca-7B_math",
            "name_full": "Alpaca-7B (math reasoning experiments in this paper)",
            "brief_description": "Alpaca-7B (LLaMA-based, decoder-only) is fine-tuned with LoRA and trained under the Trice pipeline to learn selective calculator use for arithmetic word problems.",
            "citation_title": "Making Language Models Better Tool Learners with Execution Feedback",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B",
            "model_family": "decoder-only transformer",
            "model_size": "7B (as reported in paper)",
            "training_data_description": "Pretraining details not specified in this paper; experimental fine-tuning uses LoRA on Trice datasets (pseudo-labeled tool API pairs generated with ChatGPT and model predictions).",
            "benchmark_name": "ASDiv, SVAMP, GSM8K",
            "task_type": "Math reasoning — arithmetic word problems with optional calculator tool",
            "problem_format": "Natural-language word problems; tool API calls in the form calculator(formula) when model elects to use tool",
            "difficulty_level": "Grade-school to multi-step arithmetic (GSM8K included)",
            "prompting_method": "Baseline prompt-based zero/few-shot; supervised fine-tuning (0% and 100% tool exposure); Trice two-stage training (Behavior Cloning then RLEF). Candidate responses used in RLEF were collected from multiple LLMs. Tool usage is explicit via API call.",
            "performance_metric": "Accuracy (lenient, last-number matching)",
            "performance_value": "Trice-SPLit: ASDiv 73.4%, SVAMP 45.0%, GSM8K 16.3%; Trice-MIX: ASDiv 75.2%, SVAMP 58.0%, GSM8K 21.5%. (Table 3)",
            "internal_analysis": "Behavioral analysis only: Stage I provides basic ability to generate calculator calls; Stage II's execution feedback improves correct decision-making about when to call the calculator, improves the correctness of tool inputs, and reduces error propagation. No internal interpretability (e.g., attention or neuron probing) is reported.",
            "failure_modes": "Similar failure modes as for other small models: overuse of tools after supervised instruct-tuning, occasional incorrect formula generation for calculator inputs, wrong interpretation of tool outputs, and limitations purportedly tied to model scale.",
            "scaling_trend": "Trice substantially improves Alpaca-7B performance compared to prompt baselines and standard supervised fine-tuning. Mixed-task training (Trice-MIX) further improves generalization; no experiments on larger parameter counts were conducted in this paper.",
            "uuid": "e6643.2",
            "source_info": {
                "paper_title": "Making Language Models Better Tool Learners with Execution Feedback",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Vicuna-7B_math",
            "name_full": "Vicuna-7B (math reasoning experiments in this paper)",
            "brief_description": "Vicuna-7B (LLaMA-derived, decoder-only) is evaluated on arithmetic word-problem datasets and trained with Trice; the paper reports that Trice-MIX yields the strongest gains among the 6–7B models on math benchmarks.",
            "citation_title": "Making Language Models Better Tool Learners with Execution Feedback",
            "mention_or_use": "use",
            "model_name": "Vicuna-7B",
            "model_family": "decoder-only transformer",
            "model_size": "7B (as reported in paper)",
            "training_data_description": "Pretraining not detailed in this paper; experiments fine-tune Vicuna-7B with LoRA on Trice datasets (behavior cloning + RLEF using pseudo-labeled tool APIs and candidate responses from multiple LLMs).",
            "benchmark_name": "ASDiv, SVAMP, GSM8K",
            "task_type": "Math reasoning — arithmetic word problems (calculator as external tool)",
            "problem_format": "Natural-language word problems; when model decides to use the tool it outputs calculator(formula) API call; evaluation checks last predicted number.",
            "difficulty_level": "Grade-school arithmetic up to multi-step (GSM8K multi-step problems included)",
            "prompting_method": "Prompt-based baselines (zero/few-shot) and Trice two-stage training (Behavior Cloning then RLEF). Candidate responses for ranking in RLEF came from ChatGPT, InstructGPT, Vicuna, Alpaca, and the gold regulated output.",
            "performance_metric": "Accuracy (lenient: last-number matching)",
            "performance_value": "Trice-SPLit: ASDiv 72.6%, SVAMP 49.0%, GSM8K 16.6%; Trice-MIX: ASDiv 81.2%, SVAMP 60.5%, GSM8K 21.8%. For comparison, Vicuna few-shot/zero-shot baselines were much lower (see Table 3).",
            "internal_analysis": "Paper's interpretive analysis is behavioral: (a) Stage I (Behavior Cloning) establishes tool-invocation capability but tends to increase tool usage frequency; (b) Stage II (RLEF) leverages execution feedback (ranking loss tied to correctness and alignment with gold tool usage) to reduce unnecessary tool calls and improve correctness of calculator inputs and downstream answers. The paper includes tool-use rate statistics, case studies, and observations about insufficient tool learning in some tasks. No low-level mechanistic probing (e.g., logit lens, attention/neuron analysis) is reported.",
            "failure_modes": "Documented failure modes include: excessive dependence on tools after Stage I (leading to error propagation), incorrect tool input formulas, misuse or misinterpretation of tool outputs, and remaining tool-use errors especially on harder multi-step GSM8K problems. Authors attribute some errors to limited backbone scale (6–7B).",
            "scaling_trend": "Trice produces consistent improvements for 7B Vicuna; Trice-MIX (multi-task mix) outperforms Trice-SPLit, indicating benefits from multi-tool/multi-task training. The study did not evaluate larger models, so broader scaling trends (e.g., emergent behavior at larger sizes) are not established in this paper.",
            "uuid": "e6643.3",
            "source_info": {
                "paper_title": "Making Language Models Better Tool Learners with Execution Feedback",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Are NLP models really able to solve simple math word problems?",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2
        },
        {
            "paper_title": "PAL: program-aided language models",
            "rating": 1
        },
        {
            "paper_title": "TALM: tool augmented language models",
            "rating": 1
        }
    ],
    "cost": 0.014789,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Making Language Models Better Tool Learners with Execution Feedback</h1>
<p>Shuofei Qiao ${ }^{1,2}$, Honghao Gui ${ }^{1,2}$, Chengfei $\mathbf{L v}^{4}$, Qianghuai Jia ${ }^{4}$, Huajun Chen ${ }^{1,2,3}$, Ningyu Zhang ${ }^{1,2 *}$<br>${ }^{1}$ College of Computer Science and Technology, Zhejiang University<br>${ }^{2}$ ZJU-Ant Group Joint Research Center for Knowledge Graphs, Zhejiang University<br>${ }^{3}$ ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University<br>${ }^{4}$ Alibaba Group<br>{shuofei, guihonghao, huajunsir, zhangningyu}@zju.edu.cn</p>
<h4>Abstract</h4>
<p>Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed by further analysis, show that Trice can make the large language model selectively use tools by improving the accuracy of tool usage while enhancing insufficient tool learning and mitigating excessive reliance on tools ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>The recent rapid advancement of foundation models (Brown et al., 2020; Ouyang et al., 2022; Chowdhery et al., 2022; Qiao et al., 2023; Zhao et al., 2023b) makes it practical for AI machines to create (Cai et al., 2023; Qian et al., 2023) and utilize tools effectively (Shen et al., 2023; Lu et al., 2023), which greatly transcends their inherent limitations in various underlying areas, including arithmetic (Cobbe et al., 2021; Parisi et al., 2022), knowledge updating (Sun et al., 2023; Zhao et al., 2023a), multi-modal semantic analysis (Wu et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Large language model learns to use tools from execution feedback.</p>
<p>2023; Driess et al., 2023), etc. Existing research has shed light on the potential of Large Language Models (LLMs) to exhibit a promising level of dexterity and finesse in tool use (Qin et al., 2023a; Wang et al., 2023). Prior works view tools as external resources to augment LLMs for better performance (Schick et al., 2023; Hao et al., 2023; Patil et al., 2023; Tang et al., 2023) or employ LLMs as a hub for human-tool interaction, responsible for orchestrating the deployment and usage of tools (Shen et al., 2023; Ge et al., 2023; Lu et al., 2023; Driess et al., 2023).</p>
<p>Despite the empirical success of previous work, a critical issue remains: LLMs often do not understand when and how to properly use which tools. On one hand, the use of tools is necessary to augment LLMs when facing complex problems that surpass their inherent capabilities. On the other hand, for simpler problems that can readily be solved by the models themselves, introducing tools can paradoxically propagate errors rather than enhance performance. These errors can include but are not limited to, improper selection of tool types, generation of incorrect tool inputs, and ineffective utilization of tool return results. Intuitively, it's crucial for LLMs to develop an awareness of when tools are necessary and when they are not, and to be able to make decisions about selecting the most appropriate tools for the task at hand.</p>
<p>To address the above issues, we propose Tool</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">LM</th>
<th style="text-align: center;">Model Scale</th>
<th style="text-align: center;">Mechanism</th>
<th style="text-align: center;">Feedback</th>
<th style="text-align: center;">Peft</th>
<th style="text-align: center;">Teacher</th>
<th style="text-align: center;">Unseen</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Toolformer (Schick et al., 2023)</td>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">6B</td>
<td style="text-align: center;">instruct-tuning</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">ToolkenGPT (Hao et al., 2023)</td>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">13B, 30B</td>
<td style="text-align: center;">fine-tuning</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">HuggingGPT (Shen et al., 2023)</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$&gt;=100 \mathrm{~B}$</td>
<td style="text-align: center;">prompt</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Chameleon (Lu et al., 2023)</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">$&gt;=100 \mathrm{~B}$</td>
<td style="text-align: center;">prompt</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">ChatCoT (Chen et al., 2023)</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$&gt;=100 \mathrm{~B}$</td>
<td style="text-align: center;">prompt</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Gorilla (Patil et al., 2023)</td>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">instruct-tuning</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">ToolLLaMA (Qin et al., 2023b)</td>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">instruct-tuning</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4Tools (Yang et al., 2023)</td>
<td style="text-align: center;">Vicuna</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">instruct-tuning</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Trice (ours)</td>
<td style="text-align: center;">ChatGLM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">instruct-tuning</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Alpaca</td>
<td style="text-align: center;">6B, 7B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Vicuna</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">reinforcement learning</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of related works. Mechanism denotes how the LM learns to invoke tools. Feedback stands for whether the LM learns from execution feedback. Peft means the parameter efficient tuning. Teacher expresses whether learning from a powerful teacher like ChatGPT. Unseen indicates the zero-shot capability on unseen tools.
leaRning wIth exeCution fEedback (Trice) as shown in Figure 1, a two-stage end-to-end framework that enables the model to continually learn through feedback derived from execution, thereby learning when and how to use tools effectively. Specifically, we first prepare a dataset that helps discern when tool usage is necessary for LLMs and when it is not. Given the lack of gold labels, we utilize ChatGPT (OpenAI, 2022) to automatically generate tool usage APIs. Then, we introduce a two-stage training strategy to teach the model when to use tools: 1) Behavior Cloning. We conduct instruct-tuning on the dataset to let the model imitate the tool-using behavior. 2) Reinforcement Learning with Execution Feedback (RLEF). We further reinforce the model with execution feedback by aligning it with desirable candidate responses, guiding the model to selectively use tools to avoid error propagation. We detail the main difference of Trice with related works in Table 1.</p>
<p>We train and evaluate Trice on various tasks and backbone models. Experimental results and further analyses demonstrate that Trice successfully instructs the model to judiciously use tools, simultaneously enhancing insufficient tool learning, reducing excessive reliance on tools, and improving the accuracy of tool usage. In summary, the key contributions of our study are as follows:</p>
<ul>
<li>We introduce Trice, a two-stage end-to-end training framework that leverages execution feedback to help LLMs become more proficient tool learners.</li>
<li>We perform superior on eight benchmark datasets of four tasks with various models.</li>
<li>Extensive empirical analysis demonstrates that Trice can guide the model in judicious
tool use, thereby enhancing insufficient tool use, reducing excessive dependency on tools, and improving the effectiveness of tool use.</li>
</ul>
<h2>2 Related Work</h2>
<p>Tool Learning. Though possessing remarkable capabilities (Qiao et al., 2023; Yao et al., 2023), LLMs still struggle in many basic aspects where much smaller and simpler tools may precisely excel. Under this circumstance, a new paradigm, called Tool Learning (Qin et al., 2023a), is born to combine the strengths of both LLMs and specialized tools. Some works (Driess et al., 2023; Shen et al., 2023; Lu et al., 2023) regard LLMs as a decision-making hub for compositional tool using which can be called Tool-Oriented Learning (Qin et al., 2023a), while others (Gao et al., 2022; Liu et al., 2023; Schick et al., 2023) treat tools as complementary resources to extend the power of LLMs which can be called Tool-Augmented Learning (Mialon et al., 2023; Qin et al., 2023a). Despite their success, tool-augmented approaches tend to force LLMs to use tools mindlessly regardless of whether they actually need tools for help. This may, in some scenarios, steer LMs to erroneously choose the type of tools or the way to use tools, making the loss outweighs the gain. Compared to previous works, we try to make LMs better tool learners by teaching them to use tools selectively instead of blindly.</p>
<p>Learning from Feedback. An intuitive approach of tool learning is to fit LLMs on examples with human-labeled tools directly (Torabi et al., 2018; Li et al., 2022). However, this is often impractical to annotate every possible scenario (Codevilla et al., 2019) and difficult to generalize to new ones. It is worth noting that humans generally have the ability to correct and reflect on their own behavior from</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The overview of our proposed framework Trice. In stage-I (Behavior Cloning), We conduct instructtuning on the dataset to let the model imitate the tool-using behavior. In stage-II (RLEF), we further reinforce the model with tool execution feedback by aligning it with desirable candidate responses.</p>
<p>trial and error (Allen et al., 2019). Intuitively, feedbacks from the environments or humans enable LLMs to understand the impact of their actions and adapt their behavior accordingly. Reinforcement learning (RL) excels at enabling models to learn decision-making abilities in complex environments through feedback (Schrittwieser et al., 2020; Yao et al., 2022; Ge et al., 2023). Ouyang et al. (2022) apply a state-of-the-art RL algorithm, PPO (Schulman et al., 2017), to align LLMs with human feedback. Liu et al. (2022) reinforce knowledge for commonsense question answering with a fixed QA model providing feedback. Yuan et al. (2023); Lee et al. (2023) offer a promising alternative that leverages powerful off-the-shelf LLMs to generate preferences. Compared to the previous feedback framework, we introduce RLEF for tool learning which reinforces the LLMs with the execution result of tools.</p>
<h2>3 Methodology</h2>
<p>Problem Overview. We mainly focus on four kinds of tasks, with each instance in the format of $x=(s, q, t, a)$, where $s$ denotes the specialized instruction of each task, $q$ refers to the question, $t$ stands for the tool API and $a$ is the gold answer. Following an instruction-following paradigm, the complete input of the LLM is as follows:</p>
<p>$$
\text { input }=[s, q]
$$</p>
<p>where [,] stands for text concatenating. In terms of the output, when LLM deems that no tool is necessary, it generates the answer $a$. Conversely, if the model identifies the need for a tool, it produces the tool API $t$, which encompasses the specific type of tool and its corresponding input:</p>
<p>$$
\text { output }= \begin{cases}a &amp; \text { use_tool }=\text { false } \ t &amp; \text { use_tool }= \text { true }\end{cases}
$$</p>
<p>The detailed format of each task is shown in A.1.
Given the problem, the main challenges lie in 1) determining the LLM when to or not to harness tools for help and 2) how to impart the ability to the LLM to make selective use of tools. For the former, we allow the untrained model to directly infer answers, considering the correct ones as not requiring tools and the incorrect ones as indicating the need for tool assistance. For the latter, we adopt Trice, a two-stage training strategy. In the first stage, we use Behavior Cloning to make the model imitate tools invoking. Building upon this, we continue to train the model for selective tool usage with RLEF in the second stage. The overview of our method is illustrated in Figure 2. Please note that all symbols are globally defined in sections 3&amp;4.</p>
<p>Data Preparation. The data preparation follows the principles outlined in Eq.1&amp;2. Given a raw initial dataset $\mathcal{D}<em i="1">{\text {init }}={(q, a)}</em>$ from the bench-}^{|\mathcal{D}_{\text {init }}|</p>
<p>mark, we utilize LLM without fine-tuning to generate predictions. Since we do not have gold labels for tool APIs, we employ ChatGPT (OpenAI, 2022) to generate pseudo-labels under few-shot prompting. Specifically, we generate tool API labels $t=\operatorname{tool}<em _input="{input" _text="\text">{\text {name }}\left(\right.$ tool $\left.</em>}}\right)$ for questions where the generated predictions are incorrect. For questions with correct predictions, we directly set $t=$ None to indicate that tool APIs are not required. We design particular instructions $s$ tailored to each task. In the end, we obtain $\mathcal{D<em i="1">{\text {tool }}={(s, q, t, a)}</em>$.}^{\left|\mathcal{D}_{\text {tool }}\right|}$ according with Eq. $1 \&amp; 2$ containing the tool demand information of the specific LLM as we desire ${ }^{2</p>
<p>Training. As shown in Figure 2, based on $\mathcal{D}<em _text="\text" _tool="{tool">{\text {tool }}$, we conduct a two-stage training approach: I) Behavior Cloning (§3.1). In this stage, we teach the model to imitate the tool usage behavior by fine-tuning it on $\mathcal{D}</em>$ in an instruct-tuning manner. This empowers the model with preliminary functionality of tool API invocation. II) Reinforcement Learning with Execution Feedback (§3.2). Drawing inspiration from fine-tuning with human feedback (Ouyang et al., 2022), we continue to reinforce our model obtained in stage I with execution feedback by steering it to align with desirable candidate responses.}</p>
<h3>3.1 Training Stage I: Behavior Cloning</h3>
<p>During the behavior cloning stage, we aim to enable the LLM to master the schema of tool API invocation and develop preliminary skills in selectively utilizing tools. We perform supervised fine-tuning on $\mathcal{D}_{\text {tool }}$ in this stage.</p>
<p>Specifically, for the model $p_{\mathrm{LM}}$ with tunable parameters $\theta$, the training loss of stage I can be formulated as:</p>
<p>$$
\mathcal{L}<em _in="\in" _mathcal_D="\mathcal{D" _s_="(s," a_="a)" q_="q," t_="t,">{\text {clone }}(\theta)=\sum</em><em _mathrm_LM="\mathrm{LM">{\text {tool }}}-\log p</em>(o \mid s, q ; \theta)
$$}</p>
<p>where $o$ is the specified output of the model as defined in Eq.2. The final parameterized model of this stage is denoted as $\theta_{\text {clone }}$.</p>
<h3>3.2 Training Stage II: RLEF</h3>
<p>In stage II, we continue to optimize $\theta_{\text {clone }}$ with execution feedback, so as to enhance its capability</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to selectively utilize tools and improve the accuracy of decision-making regarding tool types and corresponding inputs.</p>
<p>Overall Loss. Following Yuan et al. (2023), for each question $q$, we have $k$ different candidate responses $y_{i}(1 \leq i \leq k)$ marshaled from other LLMs (e.g. ChatGPT, LLaMA) or human experts. We apply a reward strategy to score each $y_{i}$ with $r_{i}=R\left(a, y_{i}\right)$ where $a$ is the gold answer of question $q$. Our goal is to instruct the LLM to determine the more desirable response by aligning with scores $\left{r_{i}\right}<em i="i">{k}$. So we then score each $y</em>$ with the LLM:</p>
<p>$$
p_{i}=\frac{\sum_{t} \log p_{\mathrm{LM}}\left(y_{i, m} \mid q, y_{i,&lt;m} ; \theta_{\text {clone }}\right)}{\left|y_{i}\right|}
$$</p>
<p>where $m$ denotes the $m$ th token of $y_{i}, p_{i}$ is the conditional log probability of $y_{i}$ and $\left|y_{i}\right|$ refers to the length-normalized factor.</p>
<p>To facilitate the LLM in learning the correct score ordering of different $y_{i}$, we introduce a ranking loss during training:</p>
<p>$$
\mathcal{L}<em r__i="r_{i">{\text {rank }}=\sum</em>\right)
$$}&lt;r_{j}} \max \left(0, p_{i}-p_{j</p>
<p>Meanwhile, in order to prevent the model from deviating too far from the original parameters and generating unreasonable tool API invocation structure, we reintroduce the supervised fine-tuning loss:</p>
<p>$$
\mathcal{L}<em m="m">{\mathrm{sft}}=-\sum</em>\right)
$$} \log p_{\mathrm{LM}}\left(o_{m} \mid s, q, o_{&lt;m} ; \theta_{\text {clone }</p>
<p>Finally, the overall RLEF loss is defined as follows:</p>
<p>$$
\mathcal{L}<em _rank="{rank" _text="\text">{\text {RLEF }}=\alpha \cdot \mathcal{L}</em>
$$}}+\mathcal{L}_{\text {sft }</p>
<p>where $\alpha$ is a hyperparameter that determines the proportion of the rank loss.</p>
<p>Reward Strategy. The reward strategy aims to give each $y_{i}$ an $r_{i}$ and rank them accordingly for a given question $q$. We view the output $o$ regulated in $\mathcal{D}_{\text {tool }}$ as the pseudo-human-expert (gold) response. Then the reward strategy is derived from two indicators: 1) accuracy of the answer and 2) consistency of tool usage with the gold response. Specifically, we employ a five-level scoring strategy. We assign the gold response with the maximum score. For the remaining, assuming that the correctness of the response is denoted as True for correct answers and False for incorrect answers, and whether the use of tools aligns with the gold</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Tool</th>
<th style="text-align: center;">Datasets</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Math <br> Reasoning</td>
<td style="text-align: center;">Calculator</td>
<td style="text-align: center;">ASDiv (Miao et al., 2020) <br> SVAMP (Patel et al., 2021) <br> GSM8K (Cobbe et al., 2021)</td>
</tr>
<tr>
<td style="text-align: center;">Question <br> Answering</td>
<td style="text-align: center;">WikiSearch</td>
<td style="text-align: center;">WebQuestions (Berant et al., 2013) <br> NaturalQuestions (Kwiatkowski et al., 2019) <br> TriviaQA (Joshi et al., 2017)</td>
</tr>
<tr>
<td style="text-align: center;">LAMA</td>
<td style="text-align: center;">QA Model</td>
<td style="text-align: center;">T-REs (Petroni et al., 2019)</td>
</tr>
<tr>
<td style="text-align: center;">Multilingual <br> QA</td>
<td style="text-align: center;">Translator</td>
<td style="text-align: center;">MLQA (Lewis et al., 2020)</td>
</tr>
</tbody>
</table>
<p>Table 2: Tasks, datasets and the corresponding tools.
response is denoted as Yes for alignment and No for misalignment, to ensure accurate and selective tool usage, our scoring is prioritized as follows:</p>
<p>TrueYes $&gt;$ TrueNo $&gt;$ FalseYes $&gt;$ FalseNo.
If two responses share the same state, they would receive the same score.</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental Settings</h3>
<p>Tasks and Tools. As shown in Table 2, we mainly evaluate our method on four tasks with each task specified to an external tool. Due to limited computational resources, we randomly sample train and test sets from each dataset to reduce the data scale. We display the detailed data distribution for each task in Figure 11. Following Schick et al. (2023), we use a more lenient evaluation criterion than exact match. We simply check for the last number predicted by the model for the math reasoning task and check whether the correct answer is within the first twenty words for other tasks. The QA model we use for LAMA is a retrievalaugmented LM fine-tuned on Natural Questions (Kwiatkowski et al., 2019) named Atlas (Izacard et al., 2023). We use the 600M parameter NLLB (Costa-jussà et al., 2022) as our machine Translation model for Multilingual QA.</p>
<p>Candidate Response Generation. We collect five responses for each question from four different models, e.g. ChatGPT, InstuctGPT, Vicuna-7B, Alpaca-7B, and the output regulated in $\mathcal{D}<em _text="\text" _tool="{tool">{\text {tool }}$ as the gold response. To differentiate whether or not to use tools among candidate responses, we compel ChatGPT and InstructGPT to utilize tools while allowing Alpaca and Vicuna to make the choice of using tools. For ChatGPT and InstructGPT, we prompt them with instructions and few-shot examples, and for Alpaca-7B and Vicuna-7B, we finetune them on $\mathcal{D}</em>$ with LoRA (Hu et al., 2022) for
a few steps in order to equip them with initial abilities for question answering and tool generation ${ }^{3}$.}</p>
<p>Baselines. We mainly experiment with the following LLMs: 1) GPT-3.5 (OpenAI, 2022). We utilize the text-davinci-003 version of GPT series. 2) ChatGLM-6B (Du et al., 2022), a general language model pre-trained with an autoregressive blank-filling objective. 3) Alpaca-7B (Taori et al., 2023), a model further trained on LLaMA-7B (Touvron et al., 2023) with self-instruction. 4) Vicuna7B (Chiang et al., 2023), an open-source chatbot trained by fine-tuning LLaMA-7B. For GPT-3.5, we directly utilize the OpenAI API, while for other models, we train them all with LoRA (Hu et al., 2022) for efficiency in both stage-I\&amp;II.</p>
<p>Based on the differences in training status, we classify the baselines of our primary experiment into three categories (see Table 3\&amp;4): 1) PromptBased. Models are directly evaluated without training under Zero-Shot or Few-Shot manners. 2) Supervised Fine-Tuning. Models are trained purely on question-answer paired data ( $\mathbf{0 \%}$ Tool usage) or trained purely on question-tool paired data ( $\mathbf{1 0 0 \%}$ Tool usage). 3) Trice-Based. Models are trained separately for each task (Trice-SPLIT) or by combining training data from all tasks (Trice-Mix) with Trice. Furthermore, we observe the role of each training stage (see Figure 3): 1) Trice-I. Models are trained only by the Behavior Cloning stage. 2) Trice-II. Models are trained only by the RLEF stage. 3) Trice-all. Models are trained by both Trice-I and Trice-II. In our analysis, we use arrows to indicate $\uparrow$ positive and $\downarrow$ negative performance compared to the specific baseline.</p>
<p>Setups. We fine-tune all our models with LoRA (Hu et al., 2022) in the format proposed in Alpaca (). All the models are trained for 5 epochs in stageI and 2 epochs in stage-II. We use the learning rates of ${2 \mathrm{e}-5,1 \mathrm{e}-4,3 \mathrm{e}-4}$ for ChatGLM-6B and ${2 \mathrm{e}-5,1 \mathrm{e}-4}$ for Alpaca-7B and Vicuna-7B. The $\alpha$ is set to ${0.01,0.1,1}$ for all the models. The detailed hyper-parameters we use are shown in Appendix A.4. Since sampling responses and training are separated, our whole training procedure only needs to load one model. All our training can be completed on one 80G A800 GPU within 10 hours.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Math Reasoning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Question Answering</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LAMA</th>
<th style="text-align: center;">Multilingual QA</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ASDiv</td>
<td style="text-align: center;">SVAMP</td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">WebQ</td>
<td style="text-align: center;">NaturalQ</td>
<td style="text-align: center;">TriviaQA</td>
<td style="text-align: center;">T-REx</td>
<td style="text-align: center;">MLQA</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">42.8</td>
</tr>
<tr>
<td style="text-align: center;">Prompt</td>
<td style="text-align: center;">ChatGLM (Zero-Shot)</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">17.9</td>
</tr>
<tr>
<td style="text-align: center;">Based</td>
<td style="text-align: center;">ChatGLM (Few-Shot)</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">18.7</td>
</tr>
<tr>
<td style="text-align: center;">Supervised</td>
<td style="text-align: center;">ChatGLM (0\% Tool)</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">23.9</td>
</tr>
<tr>
<td style="text-align: center;">Fine-Tuning</td>
<td style="text-align: center;">ChatGLM (100\% Tool)</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">29.8</td>
</tr>
<tr>
<td style="text-align: center;">Trice</td>
<td style="text-align: center;">ChatGLM (Trice-SPLit)</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">32.7</td>
</tr>
<tr>
<td style="text-align: center;">Based</td>
<td style="text-align: center;">ChatGLM (Trice-MIX)</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">36.8</td>
</tr>
<tr>
<td style="text-align: center;">Prompt</td>
<td style="text-align: center;">Alpaca (Zero-Shot)</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">23.4</td>
</tr>
<tr>
<td style="text-align: center;">Based</td>
<td style="text-align: center;">Alpaca (Few-Shot)</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">26.2</td>
</tr>
<tr>
<td style="text-align: center;">Supervised</td>
<td style="text-align: center;">Alpaca (0\% Tool)</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">30.4</td>
</tr>
<tr>
<td style="text-align: center;">Fine-Tuning</td>
<td style="text-align: center;">Alpaca (100\% Tool)</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">37.7</td>
</tr>
<tr>
<td style="text-align: center;">Trice</td>
<td style="text-align: center;">Alpaca (Trice-SPLit)</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">41.5</td>
</tr>
<tr>
<td style="text-align: center;">Based</td>
<td style="text-align: center;">Alpaca (Trice-MIX)</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">45.7</td>
</tr>
<tr>
<td style="text-align: center;">Prompt</td>
<td style="text-align: center;">Vicuna (Zero-Shot)</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">28.4</td>
</tr>
<tr>
<td style="text-align: center;">Based</td>
<td style="text-align: center;">Vicuna (Few-Shot)</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">30.5</td>
</tr>
<tr>
<td style="text-align: center;">Supervised</td>
<td style="text-align: center;">Vicuna (0\% Tool)</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">33.4</td>
</tr>
<tr>
<td style="text-align: center;">Fine-Tuning</td>
<td style="text-align: center;">Vicuna (100\% Tool)</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">38.7</td>
</tr>
<tr>
<td style="text-align: center;">Trice</td>
<td style="text-align: center;">Vicuna (Trice-SPLit)</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">42.4</td>
</tr>
<tr>
<td style="text-align: center;">Based</td>
<td style="text-align: center;">Vicuna (Trice-MIX)</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">46.9</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance of Trice across various tasks with different backbone models. Zero-Shot: models are directly evaluated. Few-Shot: models are prompted by 3 examples during evaluation. $\mathbf{0 \%}$ Tool: models are trained purely on question-answer paired data. During the above settings, the model does not rely on tools. $\mathbf{1 0 0 \%}$ Tool: models are trained purely on question-tool paired data. Trice-SPLit: models are trained with Trice separately for each task. Trice-MIX: models are trained with Trice by combining training data from all tasks.</p>
<h3>4.2 Main Results</h3>
<p>Selective Tool Learning of Single Tool. Within each task, we train the model to learn the corresponding tool as shown in Table 2, thereby evaluating the model's proficiency in handling a single tool. From the rows labeled Trice-SPLit in Table 3, it is evident that training by Trice, Alpaca and Vicuna perform on par with GPT-3.5, exhibiting only a slight decrease of $\overline{4} 1.3 \%$ and $\overline{4} 0.4 \%$ on average. Meanwhile, across all backbone models, Trice-SPLit demonstrates significant improvements compared to the prompt-based baselines, surpassing the few-shot setting with $\uparrow 14.0 \%$ for ChatGLM, $\uparrow 15.3 \%$ for Alpaca, and $\uparrow 11.9 \%$ for Vicuna. This indicates that Trice consistently empowers LLMs to use tools effectively, irrespective of the model architecture and scale (ChatGLM-6B is encoder-decoder, while Alpaca-7B and Vicuna7B are decoder-only). Moreover, whether it is completely independent ( $0 \%$ Tool) or dependent ( $100 \%$ Tool) on tools, supervised fine-tuning fails to beat Trice-based training, which highlights the necessity and efficacy of judicious tool learning.</p>
<p>Selective Tool Learning of Multi-Tools. Across all tasks, we train the model to simultaneously learn all the tools, assessing its capabilities in multi-tool learning. As indicated in the rows labeled TriceMIX in Table 3, training across tasks achieves state-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Unseen Dataset</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Unseen Tool</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Calculator</td>
<td style="text-align: center;">QA Model</td>
<td style="text-align: center;">Retriever</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">MultiArith</td>
<td style="text-align: center;">AddSub</td>
<td style="text-align: center;">SQuAD</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">45.2</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna (Zero-Shot)</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">28.6</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna (Few-Shot)</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">31.2</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna (Trice-SPLit)</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">30.9</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna (Trice-MIX)</td>
<td style="text-align: center;">$\mathbf{6 6 . 6}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 5}$</td>
<td style="text-align: center;">35.7</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance to unseen datasets and tools. The presence of an empty value in the Unseen Tool TRICEAPLIT section is due to our reliance on the generalization achieved through mixed-tool training (MIX). Testing the new tool individually through separate training (SPLIT) using any single tool would not be suitable.
of-the-art performance by further exceeding the Trice-SPLit with over $\uparrow 4.0 \%$ average score gains across different models. Meanwhile, both Alpaca and Vicuna outperform GPT-3.5, exhibiting improvements of $\uparrow 2.9 \%$ and $\uparrow 4.1 \%$, respectively. These results declare the potential of Trice in selective multi-tool learning, which paves the way for expanding the capabilities of LLMs to wisely handle more complex and diverse types of tools.</p>
<p>Generalization of Tool Learning. To analyze the generalization ability of Trice, we extend the trained model to unseen datasets and tools. As illustrated in Table 4, we evaluate Vicuna on another two math reasoning datasets (MultiArith (Roy and Roth, 2015) and AddSub (Hosseini et al., 2014))</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance of Trice across all tasks at different training stages. Trice-I: only train by Behavior Cloning (instruct-tuning) stage. Trice-II: only train by RLEF (reinforcement learning with execution feedback) stage. Trice-ALL: train by both Trice-I and Trice-II.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Comparison of tool use rate statistics among different training stages. In the Zero-Shot stage, we consider a need for tools when the model reaches a wrong answer.</p>
<p>and one LAMA dataset (SQuAD (Petroni et al., 2019)). Our approach enables continuous optimization of the model's performance on unseen datasets, with Trice-MIX yielding superior results compared to Trice-Split. This suggests that Trice equips the model with general tool usage capabilities. Furthermore, we steer the model towards unseen tools by simply modifying the instructions. The performance of Vicuna (Trice-MIX) augmented by a retriever on HotpotQA (Yang et al., 2018) advances ↑6.7% than the few-shot manner. Despite the disparities between GPT-3.5 on certain datasets, these findings highlight the promise of multi-tool training based on Trice for facilitating the generalization of tool learning.</p>
<h3>4.3 Analyses of Selective Tool Learning</h3>
<p><strong>Stage-I is the Foundation of Stable Selective Tool Learning.</strong> Figure 3 showcases the performance of Trice at different training stages, with Vicuna as the representative. It is evident that only trained in stage I (Trice-I), the model acquires efficacious tool usage capabilities, resulting in a substantial performance improvement. Upon further training in stage II (Trice-ALL), the model experiences additional performance enhancements in both the Split and Mix training settings. However, the results obtained solely from stage II (Trice-II) are unsatisfactory, indicating that the initial tool generation ability bestowed upon the model during stage I is crucial for more stable training.</p>
<p><strong>Stage-II Plays a Pivotal Role in Selective Tool Learning.</strong> To investigate how the models learn to use tools selectively, we analyze the tool usage rate statistics of Vicuna during each training stage in Figure 4. After stage I, we notice that the model's reliance on tools has significantly deepened on most tasks. This indicates that the model effectively learns the pattern of tool usage in stage I. Still, due to the imbalanced data distribution regard-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Case study. We mainly show the responses and predictions of stages I and All.
ing the presence or absence of tools in the training set, instruct-tuning tends to make the model overly dependent on tools. However, after stage II, the model not only shows performance improvement (see Figure 3) but visibly reduces its dependency on tools, which illustrates that the execution feedback can help mitigate the model's excessive reliance on tools and alleviate error propagation in tool usage. Moreover, it cannot be ignored that the fluctuation of LAMA differs from others. The decisionmaking process for invoking the QA model poses challenges, leading to insufficient tool learning during stage I. The improvement in tool usage rate during stage II implies that the execution feedback can help address the issue of inadequate tool learning. The above two phenomena highlight the validity of TRICE for selective tool usage.</p>
<p>Case Study. In Figure 5, we present several cases featuring responses and predictions from different stages. Case 1 suggests that stage II can alleviate the insufficient tool learning in stage I, urging the model to seek assistance from tools for questions it struggles to answer. Though stage I equips the model with a certain level of tool generation capability, it may not excel in making optimal decisions about the tool's input, as shown in Case 2. Stage II mitigates this limitation and enhances the accuracy of tool use. Case 3 confirms that our proposed method enables the model to use tools judiciously. In Case 4, despite having the same tool invocation
in both stages I and II, the model may generate completely opposite answers. This indicates that stage II can further optimize the model's ability to leverage the return results of tools. However, as shown in Case 5, our model still exhibits certain flaws leading to errors in tool usage. We speculate that this could be attributed to the scale of our backbone models, which generally range from 6-7B, potentially limiting their tool learning ability.</p>
<h2>5 Discussion</h2>
<p>Knowledge Conflicts. In tool learning, LLMs manipulate tools and respond to users conditioned on a variety of knowledge sources. One particularly challenging issue in tool learning is the problem of knowledge conflicts (Qin et al., 2023a) which may derive from the conflicts between model knowledge and augmented knowledge from tools, and among augmented knowledge from different tools. This may lead to a lack of explainability in model prediction and planning. LLMs need to have the ability to differentiate knowledge from various sources and discern which ones are valuable, which ones are irrelevant, and even which ones may be harmful. This ability becomes even more critical in highly specialized fields such as biomedical research and legal assistance, where the accuracy and reliability of knowledge are of utmost importance. Our approach leverages the feedback loop of trial and error (see Figure 2) to learn when to use tools and</p>
<p>when not to. The model learns to recognize situations where relying solely on its intrinsic knowledge may not be sufficient and utilizing tools is more reliable. Similarly, it learns to identify scenarios where its own learned knowledge is capable of solving the problem without the need for extensive tool usage. This learning process allows the model to adapt and make informed decisions about when to rely on its own capabilities and when to utilize tools effectively (see Figure 4).</p>
<p>Interactive Learning. Recent NLP has witnessed rapid advancement in interactive learning which considers language models as agents capable of observing, acting, and receiving feedback in a loop with external objects such as humans, knowledge bases, tools, models, and environments (Wang et al., 2023). Collaboration among multiagents (Lin et al., 2023; Liang et al., 2023) and learning from feedback (Chen et al., 2022; Ichter et al., 2022) are the keys to achieving general embodied intelligence as of now. Our approach is a preliminary endeavor to explore the incorporation of embodied methods into tool learning. By leveraging feedback obtained through interactions between the environment (tools) and multi-agents with varying capabilities, we enable language models to learn more desirable execution strategies (see Figure 5). However, our current method is unable to learn the usage of multi-tool compositions. In the future, more sophisticated trial-and-error processes and feedback mechanisms will be necessary for LLMs to better utilize tools (e.g. learning multitool compositions) and even create new tools.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we focus on addressing the challenge of selective utilization of tools by LLMs and propose a two-stage end-to-end training framework dubbed Trice to make LLMs better tool learners with execution feedback. Through comprehensive experiments, we show that our method can achieve better performance compared to GPT-3.5. Further analyses illustrate that Trice can selectively use tools by improving the accuracy of tool usage while enhancing insufficient tool learning and mitigating excessive reliance on tools.</p>
<h2>Limitations</h2>
<p>In this paper, we focus on addressing the challenge of selective utilization of external tools by LLMs
and propose a two-stage end-to-end training framework dubbed Trice to make LLMs better tool learners with execution feedback. Despite our best efforts, there may be still some limitations that remain in this paper.</p>
<p>Method. Our approach can be applied to any toollearning scenario, including embodied robotics. However, due to the iterative nature of execution feedback, which relies on continuous trial-anderror, it is typically more suitable for computationally feasible virtual environments, while real-world scenarios often require a significant time investment. In the future, we will explore more scientific and intricate feedback mechanisms to address the limitations above.</p>
<p>Language Models. Given our limited computational resources, we only conduct experiments on three backbone models with scales of 6-7B. In the future, we may advent on LLMs with different architectures and larger scales.</p>
<p>Tasks and Datasets. Due to the limited resources, we only experiment on four tasks containing eight datasets. There are also numerous tasks and scenarios that require the utilization of more diverse and complex tools. In the future, we will embark on further research endeavors.</p>
<h2>Acknowledgements</h2>
<p>We would like to express gratitude to the anonymous reviewers for their kind comments. This work was supported by the National Natural Science Foundation of China (No. 62206246), the Fundamental Research Funds for the Central Universities (226-2023-00138), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), CAAI-Huawei MindSpore Open Fund, Ningbo Natural Science Foundation (2021J190), Yongjiang Talent Introduction Programme (2021A-156-G), CCF-Tencent Rhino-Bird Open Research Fund, and Information Technology Center and State Key Lab of CAD\&amp;CG, Zhejiang University.</p>
<h2>References</h2>
<p>Kelsey R. Allen, Kevin A. Smith, and Joshua B. Tenenbaum. 2019. The tools challenge: Rapid trial-anderror learning in physical problem solving. CoRR, abs/1907.09620.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1533-1544. ACL.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2023. Large language models as tool makers. CoRR, abs/2305.17126.</p>
<p>Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Keerthana Gopalakrishnan, Michael S. Ryoo, Austin Stone, and Daniel Kappler. 2022. Open-vocabulary queryable scene representations for real world planning. CoRR, abs/2209.09874.</p>
<p>Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Chatcot: Tool-augmented chain-of-thought reasoning on chat-based large language models. CoRR, abs/2305.14323.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,</p>
<p>Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168.</p>
<p>Felipe Codevilla, Eder Santana, Antonio M. López, and Adrien Gaidon. 2019. Exploring the limitations of behavior cloning for autonomous driving. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 9328-9337. IEEE.</p>
<p>Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Y. Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanaj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling human-centered machine translation. CoRR, abs/2207.04672.</p>
<p>Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. 2023. Palm-e: An embodied multimodal language model. CoRR, abs/2303.03378.</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: general language model pretraining with autoregressive blank infilling. pages 320-335.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. PAL: program-aided language models. CoRR, abs/2211.10435.</p>
<p>Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, and Yongfeng Zhang. 2023. Openagi: When LLM meets domain experts. CoRR, abs/2304.04370.</p>
<p>Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2023. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. CoRR, abs/2305.11554.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 523-533. ACL.</p>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, KuangHuei Lee, Yuheng Kuang, Sally Jesmonth, Nikhil J. Joshi, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng, and Chuyuan Kelly Fu. 2022. Do as I can, not as I say: Grounding language in robotic affordances. In Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 287-318. PMLR.</p>
<p>Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot learning with retrieval augmented language models. J. Mach. Learn. Res., 24:251:1-251:43.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 1601-1611. Association for Computational Linguistics.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452466.</p>
<p>Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023. RLAIF: scaling
reinforcement learning from human feedback with AI feedback. CoRR, abs/2309.00267.</p>
<p>Patrick S. H. Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2020. MLQA: evaluating cross-lingual extractive question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7315-7330. Association for Computational Linguistics.</p>
<p>Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, and Yuke Zhu. 2022. Pre-trained language models for interactive decisionmaking. In NeurIPS.</p>
<p>Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023. Encouraging divergent thinking in large language models through multi-agent debate. CoRR, abs/2305.19118.</p>
<p>Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2023. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. CoRR, abs/2305.17390.</p>
<p>Jiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, and Yejin Choi. 2022. Rainier: Reinforced knowledge introspector for commonsense question answering. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 8938-8958. Association for Computational Linguistics.</p>
<p>Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, and Andrew M. Dai. 2023. Mind's eye: Grounded language model reasoning through simulation. In The Eleventh International Conference on Learning Representations.</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, KaiWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-play compositional reasoning with large language models. CoRR, abs/2304.09842.</p>
<p>Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. 2023. Augmented language models: a survey. CoRR, abs/2302.07842.</p>
<p>Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July</p>
<p>5-10, 2020, pages 975-984. Association for Computational Linguistics.</p>
<p>OpenAI. 2022. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/ chatgpt/.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In NeurIPS.</p>
<p>Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. TALM: tool augmented language models. CoRR, abs/2205.12255.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 2080-2094. Association for Computational Linguistics.</p>
<p>Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large language model connected with massive apis. CoRR, abs/2305.15334.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 2463-2473. Association for Computational Linguistics.</p>
<p>Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. 2023. CREATOR: disentangling abstract and concrete reasonings of large language models through tool creation. CoRR, abs/2305.14318.</p>
<p>Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2023. Reasoning with language model prompting: A survey. In ACL. The Association for Computational Linguistics.</p>
<p>Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng</p>
<p>Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. 2023a. Tool learning with foundation models. CoRR, abs/2304.08354.</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023b. Toolllm: Facilitating large language models to master 16000+ real-world apis. CoRR, abs/2307.16789.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 1743-1752. The Association for Computational Linguistics.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. CoRR, abs/2302.04761.</p>
<p>Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, and David Silver. 2020. Mastering atari, go, chess and shogi by planning with a learned model. Nature., 588(7839):604609 .</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. CoRR, abs/1707.06347.</p>
<p>Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580.</p>
<p>Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2023. Recitation-augmented language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. 2023. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. CoRR, abs/2306.05301.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.</p>
<p>Faraz Torabi, Garrett Warnell, and Peter Stone. 2018. Behavioral cloning from observation. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, pages 4950-4957. ijcai.org.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.</p>
<p>Zekun Wang, Ge Zhang, Kexin Yang, Ning Shi, Wangchunshu Zhou, Shaochun Hao, Guangzheng Xiong, Yizhi Li, Mong Yuan Sim, Xiuying Chen, Qingqing Zhu, Zhenzhu Yang, Adam Nik, Qi Liu, Chenghua Lin, Shi Wang, Ruibo Liu, Wenhu Chen, Ke Xu, Dayiheng Liu, Yike Guo, and Jie Fu. 2023. Interactive natural language processing. CoRR, abs/2305.13246.</p>
<p>Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023. Visual chatgpt: Talking, drawing and editing with visual foundation models. CoRR, abs/2303.04671.</p>
<p>Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. 2023. Gpt4tools: Teaching large language model to use tools via self-instruction. CoRR, abs/2305.18752.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 2369-2380. Association for Computational Linguistics.</p>
<p>Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable realworld web interaction with grounded language agents. In NeurIPS.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. RRHF: rank responses to align language models with human feedback without tears. CoRR, abs/2304.05302.</p>
<p>Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. 2023a. Verify-and-edit: A knowledge-enhanced chain-of-thought framework. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 5823-5840. Association for Computational Linguistics.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Stage-I</th>
<th>Stage-II</th>
</tr>
</thead>
<tbody>
<tr>
<td>lora_r</td>
<td>8</td>
<td>8</td>
</tr>
<tr>
<td>lora_alpha</td>
<td>16</td>
<td>16</td>
</tr>
<tr>
<td>lora_target_modules</td>
<td>q_proj v_proj</td>
<td>q_proj v_proj</td>
</tr>
<tr>
<td>lora_dropout</td>
<td>0.05</td>
<td>0.05</td>
</tr>
<tr>
<td>max_length</td>
<td>2048</td>
<td>2048</td>
</tr>
<tr>
<td>batch_size_per_device</td>
<td>48</td>
<td>8</td>
</tr>
<tr>
<td>gradient_accumulation_steps</td>
<td>8</td>
<td>32</td>
</tr>
<tr>
<td>warmup_steps</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>epochs</td>
<td>5</td>
<td>2</td>
</tr>
<tr>
<td>lr</td>
<td>$1 \mathrm{e}-4,3 \mathrm{e}-4$</td>
<td>$1 \mathrm{e}-4,2 \mathrm{e}-5$</td>
</tr>
<tr>
<td>$\alpha$</td>
<td>-</td>
<td>$0.01,0.1,1$</td>
</tr>
</tbody>
</table>
<p>Table 5: Hyperparameters to train Chatglm-6B.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Stage-I</th>
<th>Stage-II</th>
</tr>
</thead>
<tbody>
<tr>
<td>lora_r</td>
<td>8</td>
<td>8</td>
</tr>
<tr>
<td>lora_alpha</td>
<td>16</td>
<td>16</td>
</tr>
<tr>
<td>lora_target_modules</td>
<td>q_proj v_proj</td>
<td>q_proj v_proj</td>
</tr>
<tr>
<td>lora_dropout</td>
<td>0.05</td>
<td>0.05</td>
</tr>
<tr>
<td>max_length</td>
<td>512</td>
<td>512</td>
</tr>
<tr>
<td>batch_size_per_device</td>
<td>128</td>
<td>8</td>
</tr>
<tr>
<td>gradient_accumulation_steps</td>
<td>8</td>
<td>32</td>
</tr>
<tr>
<td>warmup_steps</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>epochs</td>
<td>5</td>
<td>2</td>
</tr>
<tr>
<td>lr</td>
<td>$1 \mathrm{e}-4,2 \mathrm{e}-5$</td>
<td>$1 \mathrm{e}-4,2 \mathrm{e}-5$</td>
</tr>
<tr>
<td>$\alpha$</td>
<td>-</td>
<td>$0.01,0.1,1$</td>
</tr>
</tbody>
</table>
<p>Table 6: Hyperparameters to train Alpaca-7B and Vicuna-7B.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Training loss variations of Vicuna-7B in stage I of TRICE-MIX.</p>
<p>Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023b. A survey of large language models. CoRR, abs/2303.18223.</p>
<h2>A Appendix</h2>
<h2>A. 1 Task Format</h2>
<p>We mainly evaluate our method on four kinds of tasks as shown in Table 2. Eq.1\&amp;2 formally define the input and output of each task in general. Here is the detailed format of each task.</p>
<h2>Math Reasoning :</h2>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Training loss variations of Vicuna-7B in stage II of TRICE-MIX.</p>
<p>Instruction $s$ : Given a math problem, please solve it and you can use a calculator for help.</p>
<p>Question $q$ : Mrs. Hilt has 50 cents. A pencil costs 5 cents. How many pencils can she buy with the money she has?</p>
<p>Tool API $t$ (if needed): calculator(50/5)
Gold Answer $a: 10$</p>
<h2>Question Answering :</h2>
<p>Instruction $s$ : Given a question, please answer it and you can use a WikiSearch for help.</p>
<p>Question $q$ : Where are sunbeam microwaves made?</p>
<p>Tool API $t$ (if needed): WikiSearch(Sunbeam microwaves manufacturing location)</p>
<p>Gold Answer $a$ : Florida</p>
<h2>LAMA :</h2>
<p>Instruction $s$ : Given a question, please answer it and you can use a QA model for help.</p>
<p>Question $q$ : Winners of the festivals «Chervona Ruta» (Ukraine), «Pearls of the Season» (Ukraine), «Boards» (Moscow), «Woodstock» ( ?</p>
<p>Tool API $t$ (if needed): QA(Where is the Woodstock festival held?)</p>
<p>Gold Answer $a$ : Poland</p>
<h2>Multilingual QA :</h2>
<p>Instruction $s$ : Given a context, please answer the question in English and you can use a translator for help.</p>
<p>Question $q$ : Context: Over the next decade, she went on more than 40 field missions, meeting with refugees and internally
displaced persons in over 30 countries. In 2002, when asked what she hoped to accomplish, she stated, "Awareness of the plight of these people. I think they should be commended for what they have survived, not looked down upon." To that end, her 2001-02 field visits were chronicled in her book Notes from My Travels, which was published in October 2003 in conjunction with the release of her humanitarian drama Beyond Borders. Question: 她在10年内完成了多少任务?</p>
<p>Tool API $t$ (if needed): translator(她在10年内完成了多少任务?)</p>
<p>Gold Answer $a$ : more than 40</p>
<h2>A. 2 Data Preparation</h2>
<p>We present the prompt used to generate tool APIs for Math Reasoning, Question Answering, and LAMA in Figure 8-10. Since the sentence to be translated happens to be the provided question, Multilingual QA does not require ChatGPT to generate tool APIs. Due to limited computational resources, we randomly sample train and test sets from each dataset to reduce data scale and training/testing costs. The final data distribution for each task is illustrated in Figure 11.</p>
<h2>A. 3 Response Generation</h2>
<p>We show the prompt used to generate candidate responses for ChatGPT and GPT-3.5 in Figure 12-15. We use the same instructions in Figure 5 to generate candidate responses for Vicuna and Alpaca.</p>
<h2>A. 4 Training Details</h2>
<p>The hyperparameters we use to train ChatGLM-6B are shown in Table 5 and Alpaca-7B, Vicuna-7B are shown in Table 6. We present the training loss variations of Vicuna-7B in stages I and II of TrICEMIX in Figure 6\&amp;7. During training, we observe that despite the decrease in training loss, prolonged reinforcement learning training will result in a significant performance loss. Typically, the model achieves optimal performance within the first 1040 steps.</p>
<p>I will provide you with a math Question and a Golden answer. I need you to write "calculator(formula)" to invoke the API for assistance in solving the question, where "formula" is the formula to reach the Golden answer. Here are some examples:</p>
<p>Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?
Golden answer: 72
Output: calculator(48+48/2)
Question: Weng earns $\$ 12$ an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?
Golden answer: 10
Output: calculator( $(12 / 60)^{<em>} 50$ )
Question: Weng earns $\$ 12$ an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?
Golden answer: 10
Output: calculator( $(12 / 60)^{</em>} 50$ )</p>
<p>Question: Betty is saving money for a new wallet which costs $\$ 100$. Betty has only half of the money she needs. Her parents decided to give her $\$ 15$ for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?
Golden answer: 5
Output: calculator(100-100/2-15-15*2)
Question: {question}
Golden answer: {answer}
Output:</p>
<p>Figure 8: Prompt used for Math Reasoning to generate tool APIs.</p>
<p>I will provide you with a Question, Golden answers. I need you to write "WikiSearch(term)" to invoke the API for assistance in answering the Question, where "term" is the search term you want to look up to obtain the Golden answers. Here are some examples:</p>
<p>Question: Where are sunbeam microwaves made?
Golden answers: ['Florida']
Output: WikiSearch(Sunbeam microwaves manufacturing location)
Question: What type of car does Michael Weston drive?
Golden answers: ['Wishcraft']
Output: WikiSearch(Michael Weston car)
Question: What is Nina Dobrev nationality?
Golden answers: ['Bulgaria']
Output: WikiSearch(Nina Dobrev nationality)
Question: What religion are people in Russia?
Golden answers: ['Islam', 'Russian Orthodox Church']
Output: WikiSearch(Religion in Russia)
Question: {question}
Golden answers: {answers}
Output:</p>
<p>Figure 9: Prompt used for Question Answering to generate tool APIs.</p>
<p>I will provide you with a Question, Golden answers. I need you to write "QA(question)" to invoke the API for assistance in answering the Question, where "question" is the question you want to ask to obtain the Golden answers. Here are some examples:</p>
<p>Question: The army held Rome for a brief time, but was then forced to retreat to the city of Perusia (modern Perugia, ?
Golden answers: ['Italy']
Output: QA(Which country is Perusia, or modern Perugia, located in ?)</p>
<p>Question: Winners of the festivals «Chervona Ruta» (Ukraine), «Pearls of the Season» (Ukraine), «Boards» (Moscow), «Woodstock» ( ?
Golden answers: ['Poland']
Output: QA(Where is the Woodstock festival held?)
Question: It is native to the Alps and the Pyrenees Mountains of Europe (Spain, France, Italy, Switzerland, Austria and ?
Golden answers: ['Germany']
Output: QA(Which country is mentioned as being native to the Alps and the Pyrenees Mountains alongside Spain, France, Italy, Switzerland, and Austria ?)</p>
<p>Question: Heorhiy Kyrylovych Tkachenko (May 5, 1898 in Hlushkovo, Kursk region of the Russian Empire 1993 in Kiev, ?
Golden answers: ['Ukraine']
Output: QA(Where did Heorhiy Kyrylovych Tkachenko die ?)</p>
<p>Question: {question}
Golden answers: {answers}
Output:</p>
<p>Figure 10: Prompt used for LAMA to generate tool APIs.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 11: Data distribution for each task.</p>
<p>Given a math problem, please solve it and you can use a calculator for help.
Here are some examples:
Input: Mrs. Hilt has 50 cents. A pencil costs 5 cents. How many pencils can she buy with the money she has? Output: calculator(50/5)</p>
<p>Input: James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year?
Output: calculator( $3^{<em>} 2^{</em>} 2 * 52$ )
Input: Weng earns $\$ 12$ an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?
Output: calculator((12/60)<em>50)
Input: Betty is saving money for a new wallet which costs $\$ 100$. Betty has only half of the money she needs. Her parents decided to give her $\$ 15$ for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?
Output: calculator(100-100/2-15</em>2-15)
Input:{question}
Output:
Figure 12: Prompt used for Math Reasoning to generate candidate responses.</p>
<p>Given a question, please answer it and you can use a WikiSearch for help.
Here are some examples:
Input: Who has scored most runs in test cricket
Output: WikiSearch(most runs scorer in test cricket)
Input: How did Jock die in Dallas?
Output: WikiSearch(Jock Ewing death)
Input: Where are the Netherlands on a world map?
Output: WikiSearch(Location of the Netherlands on world map)
Input: What is Nina Dobrev nationality?
Output: WikiSearch(Nina Dobrev nationality)
Input: {question}
Output:</p>
<p>Figure 13: Prompt used for Question Answering to generate candidate responses.</p>
<p>Given a question, please answer it and you can use a QA model for help.
Here are some examples:</p>
<p>Input: The City Council divide itself into ?
Output: QA(What did the City Council divide itself into?)
Input: Arcos de Canasí is a small town in the east of the La Habana Province of ?
Output: QA(Which country is Arcos de Canasí located in?)
Input: The steel is named after Damascus, the capital city of ?
Output: QA(What is the capital city of Syria?)
Input: Winners of the festivals «Chervona Ruta» (Ukraine), «Pearls of the Season» (Ukraine), «Boards» (Moscow), «Woodstock» (?
Output: QA(Where is the Woodstock festival held?)
Input: (question)
Output:</p>
<p>Figure 14: Prompt used for LAMA to generate candidate responses.</p>
<p>Given a context, please answer the question in English and you can use a translator for help. Here are some examples:</p>
<p>Input:
Context: Over the next decade, she went on more than 40 field missions, meeting with refugees and internally displaced persons in over 30 countries. In 2002, when asked what she hoped to accomplish, she stated, "Awareness of the plight of these people. I think they should be commended for what they have survived, not looked down upon." To that end, her 2001-02 field visits were chronicled in her book Notes from My Travels, which was published in October 2003 in conjunction with the release of her humanitarian drama Beyond Borders.
Question: cô ấy đã thực hiện bao nhiêu nhiệm vụ trong hơn 10 năm?
Output: translator(cô ấy đã thực hiện bao nhiêu nhiệm vụ trong hơn 10 năm?)</p>
<p>Input:
Context: John Canfield Spencer (January 8, 1788 - May 17, 1855) was an American lawyer, politician, judge and United States Cabinet secretary in the administration of President John Tyler.
Question: John Canfield Spencer làm việc với Tổng thống nào?
Output: translator(John Canfield Spencer làm việc với Tổng thống nào? )
Input:
Context: The story follows the adventures of Garde pilot Nagate Tanikaze, who lived in the underground layer of Sidonia since birth and was raised by his grandfather. Never having met anyone else, he trains himself in an old Guardian pilot simulator every day, eventually mastering it. After his grandfather's death, he emerges to the surface and is selected as a Guardian pilot, just as Sidonia is once again threatened by the Gauna.
Question: Ông được xét chọn là gì sau khi qua đời?
Output: translator(Ông được xét chọn là gì sau khi qua đời?)
Input:
Context: Emma Goldman: A Documentary History of the American Years, Volume 1 - Made for America, 1890-1901. Berkeley: University of California Press, 2003. ISBN 0-520-08670-8.
Question: Phim tài liệu dựa trên khoảng thời gian nào?
Output: translator(Phim tài liệu dựa trên khoảng thời gian nào?)
Input:
Context: {context}
Question: {question}
Output:</p>
<p>Figure 15: Prompt used for Multilingual QA to generate candidate responses.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ For more details of candidate response generation, please refer to Appendix A.3.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
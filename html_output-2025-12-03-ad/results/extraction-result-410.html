<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-410 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-410</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-410</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-259308904</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.17249v1.pdf" target="_blank">A Hybrid System for Systematic Generalization in Simple Arithmetic Problems</a></p>
                <p><strong>Paper Abstract:</strong> Solving symbolic reasoning problems that require compositionality and systematicity is considered one of the key ingredients of human intelligence. However, symbolic reasoning is still a great challenge for deep learning models, which often cannot generalize the reasoning pattern to out-of-distribution test cases. In this work, we propose a hybrid system capable of solving arithmetic problems that require compositional and systematic reasoning over sequences of symbols. The model acquires such a skill by learning appropriate substitution rules, which are applied iteratively to the input string until the expression is completely resolved. We show that the proposed system can accurately solve nested arithmetical expressions even when trained only on a subset including the simplest cases, significantly outperforming both a sequence-to-sequence model trained end-to-end and a state-of-the-art large language model.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e410.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e410.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid Solver+Combiner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Neuro-Symbolic Solver with Deterministic Combiner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular neuro-symbolic system that combines a Transformer seq2seq 'solver' which proposes candidate symbolic substitutions, and a deterministic 'combiner' that filters, selects and applies those substitutions iteratively to simplify arithmetic expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Iterative Neuro-Symbolic Hybrid (Solver + Combiner)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A two-module hybrid architecture: (1) a trainable Transformer encoder-decoder ('solver') that receives a nested arithmetic expression and outputs candidate simplifications formatted as 'replacementCandidate_replacementTarget'; (2) a deterministic 'combiner' that takes multiple sampled outputs from the solver, filters well-formed outputs, selects the most frequent candidate, and deterministically substitutes the target substring in the original expression. The pipeline is applied iteratively (feeding the combiner's output back to the solver) until the expression is fully reduced to an integer. The solver uses label positional encodings to improve out-of-distribution generalization and a multi-output sampling strategy to reduce malformed outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Deterministic, rule-based string substitution module (combiner) that applies explicit symbolic replacement rules on the input string; the combiner enforces well-formed output syntax and performs exact, deterministic substitutions (a symbolic/string-manipulation rule system).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural seq2seq Transformer (encoder-decoder, one-layer encoder and decoder in experiments; selected configuration: 8 attention heads, 1024 embedding and feed-forward dims) trained autoregressively to produce candidate replacement strings; uses label positional encodings and sampling from the softmax logits to produce multiple outputs per input.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular pipeline / staged integration: the neural module (solver) is trained alone to emit candidate symbolic substitutions in a fixed syntax; at inference time the combiner deterministically integrates solver outputs with the original input by filtering well-formed outputs, selecting the most frequently generated candidate among sampled outputs, and applying the substitution. The overall system is iterative: the combiner output is fed back into the same pipeline until convergence. The integration is not end-to-end differentiable (the combiner is deterministic and not trained).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>1) Systematic compositional generalization: the hybrid system can apply learned local substitution steps iteratively to expressions more deeply nested than those seen in training. 2) Robustness via modularization: deterministic symbolic post-processing (combiner) reduces catastrophic end-to-end failure modes of pure neural generation by enforcing syntactic well-formedness and deterministic application of substitutions. 3) Traceability: intermediate substitution steps are explicit (replacementCandidate_replacementTarget), enabling inspection of each applied operation. 4) Error trade-off: multi-sample generation reduces halted (malformed) sequences but the iterative pipeline accumulates solver errors across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Simplification/resolution of nested arithmetic expressions (sums, subtractions, multiplications) represented as symbol strings; trained on expressions with up to 2 nested operations and tested on expressions with up to 10 nested operations to measure out-of-distribution compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Sequence accuracy (hybrid system, multi-output sampling = 100 outputs per input) by nesting depth 1..10 (mean ± s.d., %): [100.0±0.0, 93.8±1.9, 92.2±2.7, 91.4±2.5, 89.8±2.9, 86.0±2.9, 85.1±4.7, 81.6±4.0, 77.0±4.8, 74.1±3.6]. Character accuracy measures are also reported in the paper but sequence-accuracy is primary for full-expression tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Strong out-of-distribution compositional generalization: trained only on up to 2 nested operations, the hybrid system solves many expressions with up to 10 nested operations with only a gradual performance decline. Label positional encodings in the solver and multi-output sampling are key contributors; the decline is attributed to error accumulation across iterative steps (solver mistakes propagate). Compared to an end-to-end Transformer and a fine-tuned large language model (GPT-3.5 text-davinci-003), the hybrid achieves substantially better extrapolation to deeper nesting.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability of intermediate reasoning: solver outputs enumerate explicit substitution triplets (replacement and target) and combiner applies explicit deterministic replacements, so the full sequence of symbolic simplification steps is inspectable and explains the final answer. The deterministic combiner provides exact symbolic traceability for each applied transformation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>1) Accumulation of errors across iterations: solver mistakes compound over multiple substitution steps leading to a gradual decline in accuracy on deeper nestings. 2) Halted sequences: solver outputs that do not conform to the required syntax force the system to stop (counted as failures); mitigation requires multi-output sampling at the cost of computation. 3) Training assumptions limit scope: datasets were constructed so that exactly one innermost sub-expression exists per expression and intermediate/final results are limited to two digits, so the method's behavior on ambiguous / concurrently reducible sub-expressions, larger numeric ranges, or non-chain (tree) transformations is not fully evaluated. 4) Combiner is non-differentiable: cannot be trained jointly end-to-end with the solver.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>A division-of-labor neuro-symbolic framework: the paper argues for complementary strengths—neural networks learn local substitution steps and generalize those patterns, while a deterministic symbolic module enforces correctness and enables iterative composition. The framework is presented as an applied design principle rather than a formal mathematical theory.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e410.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e410.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepProbLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepProbLog (Neural Probabilistic Logic Programming)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic framework that augments probabilistic logic programming (ProbLog) with neural predicates, enabling the combination of learned perception/neural components with symbolic probabilistic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepproblog: Neural probabilistic logic programming</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepProbLog (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A hybrid system that embeds neural networks as probabilistic predicates inside a probabilistic logic programming language (ProbLog). Neural networks provide perceptual or sub-symbolic probabilities which are used by the logic engine to perform symbolic and probabilistic inference; the combined model can be trained end-to-end in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Probabilistic logic programming (ProbLog) — a logic-programming language with probabilistic facts and symbolic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks (e.g., convolutional or other differentiable models) used to produce probabilities for predicates or perceptual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Tight neuro-symbolic integration: neural predicates are treated as probabilistic facts used by the logic program's inference engine; learning can be performed end-to-end by backpropagating through the neural components and the probabilistic reasoning (via suitable differentiable/gradient-estimation mechanisms provided in the DeepProbLog framework).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines perception and symbolic probabilistic reasoning: supports structured, explainable probabilistic inference using learned perceptual components, enabling tasks that require both perception and logical reasoning; provides principled probabilistic uncertainty estimation on top of learned neural outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Mentioned generically as an example of embedding neural networks into automatic reasoning frameworks; canonical DeepProbLog use-cases include perceptual reasoning tasks (e.g., visual question answering with logical constraints) though the current paper does not evaluate it directly.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Paper cites such frameworks as enabling systematic generalization by leveraging symbolic constructs of the host language, but specific generalization metrics are not reported in the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Provides interpretable symbolic derivations and probabilistic explanations since logic-program derivations are explicit; the symbolic layer can produce human-readable proofs/inference traces grounded on learned neural facts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not discussed in detail in this paper; general limitations of such frameworks include scalability of symbolic inference with large neural outputs and potential complexities in differentiable probabilistic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Representative of approaches that leverage pre-existing symbolic programming languages (probabilistic logic) combined with trainable neural components; motivated by complementary strengths of symbolic and neural methods.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e410.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e410.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neuro-symbolic frameworks (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural networks embedded into automatic reasoning frameworks (logic / probabilistic programming)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of hybrid approaches that combine neural learning components with formal symbolic reasoning engines (logic programming, probabilistic programming), cited as prior art and motivating complementary neural and algorithmic mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural + Automatic Reasoning Frameworks (generic mention)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generic category of hybrid systems where neural networks (perception/approximation modules) are integrated with symbolic reasoning systems (logic/probabilistic programming) so that learning handles sub-symbolic inputs while symbolic components provide structured reasoning and domain knowledge. The paper cites these as alternative hybrid strategies that achieve systematic generalization by leveraging constructs of the programming languages.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Logic-based or probabilistic programming languages (e.g., Prolog, ProbLog) and their rule-based symbolic inference mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks (various architectures) used to supply probabilities, features, or symbolic predictions to the reasoning engine.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Embedding/augmentation: neural outputs are consumed as inputs/predicates by the symbolic engine; integration may be engineered (non-differentiable glue) or end-to-end differentiable depending on the framework; cited works include differentiable/neural-probabilistic integrations.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Ability to perform structured reasoning on top of learned perceptual representations, improved sample efficiency on symbolic tasks by using symbolic priors, and clearer interpretability via symbolic derivations. The present paper contrasts these more complex frameworks with its simpler deterministic combiner approach.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Mentioned generically; examples in cited literature include logical reasoning, program induction, and symbolic generalization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Cited as enabling systematic generalization through symbolic constructs; the present paper positions its approach as aiming for similar systematicity but using simpler deterministic mechanisms combined with neural learning biases.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Typically provides interpretable derivations/proofs from the symbolic side; depends on specific framework for how neural outputs are presented to the symbolic engine.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not enumerated in detail in this paper, but the authors note such frameworks often require complex engineering and larger supporting systems compared to the simpler hybrid architecture they propose.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Motivated by theoretical standpoints that human cognition is a hybrid of neural and algorithmic mechanisms; used as background motivation in the paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e410.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e410.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neuro-symbolic AI Survey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuro-symbolic artificial intelligence: The state of the art</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent survey of neuro-symbolic AI approaches that combines symbolic reasoning with neural methods; cited to contextualize hybrid approaches in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neuro-symbolic artificial intelligence: The state of the art</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Survey: Neuro-symbolic artificial intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Survey paper summarizing state-of-the-art neuro-symbolic approaches that mix symbolic reasoning systems (logic, rule-based, probabilistic programming) with neural networks, covering integration methods, applications, and research trends.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Survey-level coverage of logic, rule systems, knowledge representations, and probabilistic programming across many hybrid approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Survey-level coverage of neural network architectures used in neuro-symbolic systems (CNNs, RNNs, Transformers, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Survey-level discussion of multiple integration strategies: tight embedding (neural predicates), modular pipelines, differentiable logic layers, and hybrid architectures combining learned components with engineered symbolic modules.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Survey documents claimed benefits across works: improved interpretability, incorporation of background knowledge, sample efficiency, and structured generalization, though tradeoffs and open problems remain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Survey; covers many tasks across referenced works (perception+reasoning, program synthesis, logical inference, mathematical reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Survey reports that many neuro-symbolic approaches aim to improve compositional and out-of-distribution generalization by leveraging symbolic structure; the specific hybrid in the present paper is positioned among these approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Survey emphasizes that symbolic components typically increase interpretability and that hybrid architectures trade off expressivity, scalability and interpretability in different ways.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Survey notes open challenges: scalability, engineering complexity, integrating learning and reasoning smoothly, and evaluation benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Synthesizes various theoretical motivations for neuro-symbolic integration, including complementary strengths and the need to combine statistical learning with symbolic priors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepproblog: Neural probabilistic logic programming <em>(Rating: 2)</em></li>
                <li>Neuro-symbolic artificial intelligence: The state of the art <em>(Rating: 2)</em></li>
                <li>Abductive knowledge induction from raw data <em>(Rating: 1)</em></li>
                <li>Rect: A recursive transformer architecture for generalizable mathematical reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-410",
    "paper_id": "paper-259308904",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "Hybrid Solver+Combiner",
            "name_full": "Iterative Neuro-Symbolic Solver with Deterministic Combiner",
            "brief_description": "A modular neuro-symbolic system that combines a Transformer seq2seq 'solver' which proposes candidate symbolic substitutions, and a deterministic 'combiner' that filters, selects and applies those substitutions iteratively to simplify arithmetic expressions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Iterative Neuro-Symbolic Hybrid (Solver + Combiner)",
            "system_description": "A two-module hybrid architecture: (1) a trainable Transformer encoder-decoder ('solver') that receives a nested arithmetic expression and outputs candidate simplifications formatted as 'replacementCandidate_replacementTarget'; (2) a deterministic 'combiner' that takes multiple sampled outputs from the solver, filters well-formed outputs, selects the most frequent candidate, and deterministically substitutes the target substring in the original expression. The pipeline is applied iteratively (feeding the combiner's output back to the solver) until the expression is fully reduced to an integer. The solver uses label positional encodings to improve out-of-distribution generalization and a multi-output sampling strategy to reduce malformed outputs.",
            "declarative_component": "Deterministic, rule-based string substitution module (combiner) that applies explicit symbolic replacement rules on the input string; the combiner enforces well-formed output syntax and performs exact, deterministic substitutions (a symbolic/string-manipulation rule system).",
            "imperative_component": "Neural seq2seq Transformer (encoder-decoder, one-layer encoder and decoder in experiments; selected configuration: 8 attention heads, 1024 embedding and feed-forward dims) trained autoregressively to produce candidate replacement strings; uses label positional encodings and sampling from the softmax logits to produce multiple outputs per input.",
            "integration_method": "Modular pipeline / staged integration: the neural module (solver) is trained alone to emit candidate symbolic substitutions in a fixed syntax; at inference time the combiner deterministically integrates solver outputs with the original input by filtering well-formed outputs, selecting the most frequently generated candidate among sampled outputs, and applying the substitution. The overall system is iterative: the combiner output is fed back into the same pipeline until convergence. The integration is not end-to-end differentiable (the combiner is deterministic and not trained).",
            "emergent_properties": "1) Systematic compositional generalization: the hybrid system can apply learned local substitution steps iteratively to expressions more deeply nested than those seen in training. 2) Robustness via modularization: deterministic symbolic post-processing (combiner) reduces catastrophic end-to-end failure modes of pure neural generation by enforcing syntactic well-formedness and deterministic application of substitutions. 3) Traceability: intermediate substitution steps are explicit (replacementCandidate_replacementTarget), enabling inspection of each applied operation. 4) Error trade-off: multi-sample generation reduces halted (malformed) sequences but the iterative pipeline accumulates solver errors across steps.",
            "task_or_benchmark": "Simplification/resolution of nested arithmetic expressions (sums, subtractions, multiplications) represented as symbol strings; trained on expressions with up to 2 nested operations and tested on expressions with up to 10 nested operations to measure out-of-distribution compositional generalization.",
            "hybrid_performance": "Sequence accuracy (hybrid system, multi-output sampling = 100 outputs per input) by nesting depth 1..10 (mean ± s.d., %): [100.0±0.0, 93.8±1.9, 92.2±2.7, 91.4±2.5, 89.8±2.9, 86.0±2.9, 85.1±4.7, 81.6±4.0, 77.0±4.8, 74.1±3.6]. Character accuracy measures are also reported in the paper but sequence-accuracy is primary for full-expression tasks.",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Strong out-of-distribution compositional generalization: trained only on up to 2 nested operations, the hybrid system solves many expressions with up to 10 nested operations with only a gradual performance decline. Label positional encodings in the solver and multi-output sampling are key contributors; the decline is attributed to error accumulation across iterative steps (solver mistakes propagate). Compared to an end-to-end Transformer and a fine-tuned large language model (GPT-3.5 text-davinci-003), the hybrid achieves substantially better extrapolation to deeper nesting.",
            "interpretability_properties": "High interpretability of intermediate reasoning: solver outputs enumerate explicit substitution triplets (replacement and target) and combiner applies explicit deterministic replacements, so the full sequence of symbolic simplification steps is inspectable and explains the final answer. The deterministic combiner provides exact symbolic traceability for each applied transformation.",
            "limitations_or_failures": "1) Accumulation of errors across iterations: solver mistakes compound over multiple substitution steps leading to a gradual decline in accuracy on deeper nestings. 2) Halted sequences: solver outputs that do not conform to the required syntax force the system to stop (counted as failures); mitigation requires multi-output sampling at the cost of computation. 3) Training assumptions limit scope: datasets were constructed so that exactly one innermost sub-expression exists per expression and intermediate/final results are limited to two digits, so the method's behavior on ambiguous / concurrently reducible sub-expressions, larger numeric ranges, or non-chain (tree) transformations is not fully evaluated. 4) Combiner is non-differentiable: cannot be trained jointly end-to-end with the solver.",
            "theoretical_framework": "A division-of-labor neuro-symbolic framework: the paper argues for complementary strengths—neural networks learn local substitution steps and generalize those patterns, while a deterministic symbolic module enforces correctness and enables iterative composition. The framework is presented as an applied design principle rather than a formal mathematical theory.",
            "uuid": "e410.0"
        },
        {
            "name_short": "DeepProbLog",
            "name_full": "DeepProbLog (Neural Probabilistic Logic Programming)",
            "brief_description": "A neuro-symbolic framework that augments probabilistic logic programming (ProbLog) with neural predicates, enabling the combination of learned perception/neural components with symbolic probabilistic inference.",
            "citation_title": "Deepproblog: Neural probabilistic logic programming",
            "mention_or_use": "mention",
            "system_name": "DeepProbLog (as cited)",
            "system_description": "A hybrid system that embeds neural networks as probabilistic predicates inside a probabilistic logic programming language (ProbLog). Neural networks provide perceptual or sub-symbolic probabilities which are used by the logic engine to perform symbolic and probabilistic inference; the combined model can be trained end-to-end in many settings.",
            "declarative_component": "Probabilistic logic programming (ProbLog) — a logic-programming language with probabilistic facts and symbolic inference.",
            "imperative_component": "Neural networks (e.g., convolutional or other differentiable models) used to produce probabilities for predicates or perceptual inputs.",
            "integration_method": "Tight neuro-symbolic integration: neural predicates are treated as probabilistic facts used by the logic program's inference engine; learning can be performed end-to-end by backpropagating through the neural components and the probabilistic reasoning (via suitable differentiable/gradient-estimation mechanisms provided in the DeepProbLog framework).",
            "emergent_properties": "Combines perception and symbolic probabilistic reasoning: supports structured, explainable probabilistic inference using learned perceptual components, enabling tasks that require both perception and logical reasoning; provides principled probabilistic uncertainty estimation on top of learned neural outputs.",
            "task_or_benchmark": "Mentioned generically as an example of embedding neural networks into automatic reasoning frameworks; canonical DeepProbLog use-cases include perceptual reasoning tasks (e.g., visual question answering with logical constraints) though the current paper does not evaluate it directly.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Paper cites such frameworks as enabling systematic generalization by leveraging symbolic constructs of the host language, but specific generalization metrics are not reported in the present paper.",
            "interpretability_properties": "Provides interpretable symbolic derivations and probabilistic explanations since logic-program derivations are explicit; the symbolic layer can produce human-readable proofs/inference traces grounded on learned neural facts.",
            "limitations_or_failures": "Not discussed in detail in this paper; general limitations of such frameworks include scalability of symbolic inference with large neural outputs and potential complexities in differentiable probabilistic inference.",
            "theoretical_framework": "Representative of approaches that leverage pre-existing symbolic programming languages (probabilistic logic) combined with trainable neural components; motivated by complementary strengths of symbolic and neural methods.",
            "uuid": "e410.1"
        },
        {
            "name_short": "Neuro-symbolic frameworks (generic)",
            "name_full": "Neural networks embedded into automatic reasoning frameworks (logic / probabilistic programming)",
            "brief_description": "A class of hybrid approaches that combine neural learning components with formal symbolic reasoning engines (logic programming, probabilistic programming), cited as prior art and motivating complementary neural and algorithmic mechanisms.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Neural + Automatic Reasoning Frameworks (generic mention)",
            "system_description": "Generic category of hybrid systems where neural networks (perception/approximation modules) are integrated with symbolic reasoning systems (logic/probabilistic programming) so that learning handles sub-symbolic inputs while symbolic components provide structured reasoning and domain knowledge. The paper cites these as alternative hybrid strategies that achieve systematic generalization by leveraging constructs of the programming languages.",
            "declarative_component": "Logic-based or probabilistic programming languages (e.g., Prolog, ProbLog) and their rule-based symbolic inference mechanisms.",
            "imperative_component": "Neural networks (various architectures) used to supply probabilities, features, or symbolic predictions to the reasoning engine.",
            "integration_method": "Embedding/augmentation: neural outputs are consumed as inputs/predicates by the symbolic engine; integration may be engineered (non-differentiable glue) or end-to-end differentiable depending on the framework; cited works include differentiable/neural-probabilistic integrations.",
            "emergent_properties": "Ability to perform structured reasoning on top of learned perceptual representations, improved sample efficiency on symbolic tasks by using symbolic priors, and clearer interpretability via symbolic derivations. The present paper contrasts these more complex frameworks with its simpler deterministic combiner approach.",
            "task_or_benchmark": "Mentioned generically; examples in cited literature include logical reasoning, program induction, and symbolic generalization tasks.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Cited as enabling systematic generalization through symbolic constructs; the present paper positions its approach as aiming for similar systematicity but using simpler deterministic mechanisms combined with neural learning biases.",
            "interpretability_properties": "Typically provides interpretable derivations/proofs from the symbolic side; depends on specific framework for how neural outputs are presented to the symbolic engine.",
            "limitations_or_failures": "Not enumerated in detail in this paper, but the authors note such frameworks often require complex engineering and larger supporting systems compared to the simpler hybrid architecture they propose.",
            "theoretical_framework": "Motivated by theoretical standpoints that human cognition is a hybrid of neural and algorithmic mechanisms; used as background motivation in the paper.",
            "uuid": "e410.2"
        },
        {
            "name_short": "Neuro-symbolic AI Survey",
            "name_full": "Neuro-symbolic artificial intelligence: The state of the art",
            "brief_description": "A recent survey of neuro-symbolic AI approaches that combines symbolic reasoning with neural methods; cited to contextualize hybrid approaches in the literature.",
            "citation_title": "Neuro-symbolic artificial intelligence: The state of the art",
            "mention_or_use": "mention",
            "system_name": "Survey: Neuro-symbolic artificial intelligence",
            "system_description": "Survey paper summarizing state-of-the-art neuro-symbolic approaches that mix symbolic reasoning systems (logic, rule-based, probabilistic programming) with neural networks, covering integration methods, applications, and research trends.",
            "declarative_component": "Survey-level coverage of logic, rule systems, knowledge representations, and probabilistic programming across many hybrid approaches.",
            "imperative_component": "Survey-level coverage of neural network architectures used in neuro-symbolic systems (CNNs, RNNs, Transformers, etc.).",
            "integration_method": "Survey-level discussion of multiple integration strategies: tight embedding (neural predicates), modular pipelines, differentiable logic layers, and hybrid architectures combining learned components with engineered symbolic modules.",
            "emergent_properties": "Survey documents claimed benefits across works: improved interpretability, incorporation of background knowledge, sample efficiency, and structured generalization, though tradeoffs and open problems remain.",
            "task_or_benchmark": "Survey; covers many tasks across referenced works (perception+reasoning, program synthesis, logical inference, mathematical reasoning).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Survey reports that many neuro-symbolic approaches aim to improve compositional and out-of-distribution generalization by leveraging symbolic structure; the specific hybrid in the present paper is positioned among these approaches.",
            "interpretability_properties": "Survey emphasizes that symbolic components typically increase interpretability and that hybrid architectures trade off expressivity, scalability and interpretability in different ways.",
            "limitations_or_failures": "Survey notes open challenges: scalability, engineering complexity, integrating learning and reasoning smoothly, and evaluation benchmarks.",
            "theoretical_framework": "Synthesizes various theoretical motivations for neuro-symbolic integration, including complementary strengths and the need to combine statistical learning with symbolic priors.",
            "uuid": "e410.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepproblog: Neural probabilistic logic programming",
            "rating": 2,
            "sanitized_title": "deepproblog_neural_probabilistic_logic_programming"
        },
        {
            "paper_title": "Neuro-symbolic artificial intelligence: The state of the art",
            "rating": 2,
            "sanitized_title": "neurosymbolic_artificial_intelligence_the_state_of_the_art"
        },
        {
            "paper_title": "Abductive knowledge induction from raw data",
            "rating": 1,
            "sanitized_title": "abductive_knowledge_induction_from_raw_data"
        },
        {
            "paper_title": "Rect: A recursive transformer architecture for generalizable mathematical reasoning",
            "rating": 1,
            "sanitized_title": "rect_a_recursive_transformer_architecture_for_generalizable_mathematical_reasoning"
        }
    ],
    "cost": 0.01366775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Hybrid System for Systematic Generalization in Simple Arithmetic Problems</p>
<p>Flavio Petruzzellis 
Department of Mathematics
University of Padova
PadovaItaly</p>
<p>Alberto Testolin 
Department of Mathematics
University of Padova
PadovaItaly</p>
<p>Department of General Psychology
University of Padova
PadovaItaly</p>
<p>Alessandro Sperduti 
Department of Mathematics
University of Padova
PadovaItaly</p>
<p>A Hybrid System for Systematic Generalization in Simple Arithmetic Problems
deep learningneural networksmathematical reasoningneuro-symbolic systemsformula simplification
Solving symbolic reasoning problems that require compositionality and systematicity is considered one of the key ingredients of human intelligence. However, symbolic reasoning is still a great challenge for deep learning models, which often cannot generalize the reasoning pattern to out-of-distribution test cases. In this work, we propose a hybrid system capable of solving arithmetic problems that require compositional and systematic reasoning over sequences of symbols. The model acquires such a skill by learning appropriate substitution rules, which are applied iteratively to the input string until the expression is completely resolved. We show that the proposed system can accurately solve nested arithmetical expressions even when trained only on a subset including the simplest cases, significantly outperforming both a sequence-to-sequence model trained end-to-end and a state-of-the-art large language model.</p>
<p>Introduction</p>
<p>Designing systems that are able to learn abstract reasoning patterns from data is one of the fundamental challenges in artificial intelligence research. In particular, the capacity to generalize compositionally [1], to reuse learned rules on unseen algorithmic problems that fall outside of the training distribution [2], and more generally to exhibit high-level reasoning abilities are considered long-standing issues by the machine learning community. One possibility to empower artificial intelligence systems with reasoning abilities is to combine statistical-based approaches, which can flexibly adapt to the properties of the observed data, with logic-based modules, which can be engineered according to a set of predefined rules [3].</p>
<p>In this work, we introduce a novel neuro-symbolic architecture that solves reasoning problems on sequences of symbols in an iterative fashion. Our goal is to build a system that can learn to solve compositional tasks by only observing a subset of problem instances and then generalize the acquired knowledge to out-of-distribution (possibly more difficult) instances. The proposed system is made up of two parts: a deep learning module that is trained to generate candidate solutions that simplify a given input sequence, and a deterministic component that combines 15_(21-6) 15_(21-6) 6_  10_(21-6) solver ((21-6)<em>2) (15</em>2) combiner ((21-6)*2) Figure 1: A visual representation of the hybrid system. The system takes an arithmetical expression as input. The solver learns to generate a set of candidate simplifications, which are produced as output according to a predefined syntax that describes the result of the operation concatenated with the target substitution. The combiner then filters well-formed outputs, selects the most frequently generated candidate and applies the corresponding replacement of the target sub-string. The output of the combiner is then fed back to the system, enabling the iterative solution of complex problems.</p>
<p>the original input and the output of the neural module to produce the final output. Thanks to its hybrid nature, our system can learn the fundamental solution steps for a set of symbolic reasoning problems, and then apply these operations iteratively to novel problem instances. We evaluate our model on the task of simplifying arithmetic expressions, which is well suited to study systematic reasoning abilities and is still considered a great challenge even for state-of-the-art deep learning models [4]. The goal of the system is to generate the final solution of arithmetical expressions that can involve a variable number of nested elementary operations (sums, subtractions, and multiplications) delimited by parentheses. The model is trained on a limited subset of possible problems that contain only up to 2 nested operations. In order to evaluate its extrapolation capabilities, in the testing phase the model is probed with problems involving up to 10 nested operations.</p>
<p>Our results show that the proposed hybrid architecture achieves better out-of-distribution generalization compared to a Transformer encoder-decoder trained to solve the same problem end-to-end, and to a large language model tested on the same task, adapting the original inputs to create appropriate prompts 1 .</p>
<p>The Proposed Hybrid System</p>
<p>The proposed architecture is composed of two parts ( Figure 1): a trainable seq2seq model which we call the 'solver', and a deterministic module named the 'combiner'. The system operates by pipelining the computation of the solver and the combiner, and then applying the pipeline iteratively.</p>
<p>We can define as the set of nested arithmetical expressions represented as strings of symbols on which we test our model, and ⊂ as the subset of expressions that consist only of integer values, and thus represent the result (or values) of complex expressions. Furthermore, since each ∈ ∖ is composed of nested expressions, for each we can identify one innermost sub-expression (only one in our specific case, see Data in Section 3) which can be solved independently from the context (we can define ∈ as the result of ). Finally, we can define a function which associates to each ∈ a chain of solution steps: ℎ ( ) = ( , −1 , ..., 0 ), where , ∈ , ∈ {1, . . . , }, 0 ∈ is the final solution of , and −1 can be described as where the innermost sub-expression has been solved.</p>
<p>Given the compositional nature of the problem, we can distinguish between the goal of solving the entire expression , i.e. finding 0 , and the sub-goals of reducing sub-expressions, i.e. computing the result of each sub-expression , ∈ {1, . . . , }.</p>
<p>In our approach, the solver is trained to match arithmetical sub-expressions in their context to the corresponding numerical solution, by generating as output a string composed of two parts: a replacement candidate (i.e. the result of the innermost sub-expression ) and a replacement target (i.e. the innermost sub-expression itself ), concatenated using the 'underscore' symbol. For example, if the model receives as input the expression ((21-6)*2) it should produce as output the string 15_(21-6). More formally, the solver can be defined as a function</p>
<p>:
→ × , ( ) = ( , )
where ∈ is the nested input expression, ∈ is the innermost sub-expression in , and ∈ is the result of . The combiner receives the same string provided in input to the solver together with the solver's output, i.e. ( , ), and produces −1 , i.e. the simplified version of where the sub-expression has been substituted by . −1 can be fed back into the system until the problem is completely solved (note that the combiner module is not required during training of the seq2seq model). Formally, the combiner can be defined as a function : × ( × ) → , ( , ( , )) = −1 . Finally, the whole hybrid system can be defined as a function ℎ :
→ , ℎ ( ) = ( ,( ))
= −1 . Since the output domain of ℎ is the same as its input domain, it can be applied iteratively on its own output to solve arithmetical expressions step by step. Indeed, the system receives the expression represented as a sequence of characters as input and outputs a reduced version of the expression, which is fed back in the system unless the output only contains a positive or negative integer.</p>
<p>The solver</p>
<p>The solver is a standard Transformer encoder-decoder [5] with a one-layer encoder and a onelayer decoder. Our version of the model differs from the standard Transformer only due to the use of label positional encodings [6] rather than standard sinusoidal positional encodings. Label positional encodings have recently been proposed as a method to improve out-of-distribution generalization of a Transformer decoder in simple algorithmic tasks [6]. In this work, we have applied this technique to an encoder-decoder Transformer exploring the extent to which these encodings can be applied to the solution of arithmetic problems, a task that is relatively similar but slightly more complex than simple algorithmic tasks. Given a sequence of symbols, the corresponding sinusoidal positional encodings [5] are the first vectors in a sequence of , ≥ vectors that carry a position signal which is generated according to the formulas
( ,2 ) = ( /10000 2 / ), ( ,2 +1) = ( /10000 2 / )
, where is the position of the token in the sequence and is the dimension of the vector. When using label positional encodings, to associate a position signal to a sequence of symbols, rather than considering the first sinusoidal encodings, we generate the position signal sampling integers in the interval [0, − 1], sorting them in ascending order and then taking the corresponding sinusoidal positional encodings.</p>
<p>The combiner</p>
<p>The purpose of the combiner is to deterministically replace part of the input string with the candidate solution proposed by the solver model. To enable the deterministic substitution in the combiner, the output of the solver must contain the expression and its numerical solution formatted according to a specific syntax. However, this may not be the case: we call 'halted sequences', the ones for which the solver output is not compliant with the required format. Indeed, in such cases the processing of the sequence should stop because there is no meaningful way to proceed with the simplification.</p>
<p>In order to reduce the number of halted sequences, we have chosen to employ a multi-output generation strategy. For each input expression, we generate outputs sampling from the distributions obtained from the softmaxed logits returned by the solver decoder. Since the Transformer generates the output in an auto-regressive fashion, for each token in the output sequence we sample from the distribution and feed back the updated sequence of output tokens to the decoder. The set of outputs is then given as input to the combiner, which first filters well-formed outputs, then selects the most frequently generated sequence, and finally applies the substitution of the target with the candidate in the input expression.</p>
<p>Methodology</p>
<p>Data One of the goals in the design of our hybrid system is to combine the generalization capabilities of the solver with the deterministic nature of the combiner, which should allow learning by using a relatively small subset of the data. Intuitively, this subset should include the minimal amount of examples necessary for the model to learn the base solution step of a complex expression. We thus include in the training set only expressions with up to two nested operations between integers of up to two digits.</p>
<p>For simplicity, in all dataset splits and nesting levels, we build expressions that have only one operation for each nesting level. Therefore, the model never faces the ambiguous situation in which there are multiple operations that could be solved at the same time.</p>
<p>Since at test time the full hybrid system will be evaluated on expressions with up to 10 nested operations, it may happen that test expressions have intermediate or final results with more than two digits. We therefore take some measures to guarantee that the system will behave with more complex expressions. In particular, to ensure that the distribution of the expressions that will be used to test the model is coherent with the one that was used to train it, we limit both training and testing expressions to the ones that have intermediate and final results with at most two digits. Furthermore, we ensure that at training time the Transformer observes examples of a complete solution path of an arithmetical expression, i.e. all the steps leading to the final result. Hence, we ensure that the first-level simplification of some training samples involving two nested operations is also present in the training set.</p>
<p>To test out-of-distribution generalization more easily, we sample batches of data on-the-fly from the distribution described above, parameterized by the number of operations and size of operands in the expression. Given the data generation procedure and the number of expressions that could virtually be generated, the actual number of training examples observed by the model depends on the number of learning iterations. To ensure the separation of the training, validation and test splits, we use both a pre-computed split that ensures some of the training samples have a full solution path, as well as a hash-based split that sorts all remaining samples in three different splits on-the-fly. Evaluation Metrics We use two metrics in the performance evaluation of all systems: character and sequence accuracy. Given a batch of target sequences, we define character accuracy as the percentage of correct characters in the output of a system, and sequence accuracy as the percentage of output sequences that match the target exactly. When measuring the performance of the solver and that of the hybrid system, these two metrics are applied to different kinds of output, as the solver and the hybrid system solve two different tasks. Both metrics, in the case of the hybrid system, are computed considering halted sequences as mistakes, i.e. the output sequence is assumed to be completely different from the target. Model training The Transformer is trained in an auto-regressive regime, i.e. without teacher forcing. We have trained all models for 150k updates using Adam optimizer with a learning rate of 1 − 4 and batch size of 128. To choose the best configuration for the solver hyperparameters we performed a random search on the number of heads and the dimensionality of embeddings. For both the Transformer encoder and decoder we have compared the performance of models with 4 and 8 heads, 256, 512 and 1024-dimensional embeddings, and 256, 512 and 1024-dimensional hidden feed-forward layer on the in-distribution validation set. We finally selected the solver model with 8 attention heads, a 1024-dimensional state and a 1024-dimensional feed-forward layer.</p>
<p>Using the multi-output generation strategy, the number of outputs generated for each input is an hyperparameter whose choice clearly requires trading model accuracy for speed. We compared the performance of the hybrid system on the solution of arithmetical expressions using = 1, 20, 40, 60, 80, 100 generated outputs per input. We have observed that sampling as few as 20 outputs for each input already guarantees much better results than a 1-shot output generation, especially in terms of surviving sequences. Nevertheless, we chose to generate 100 outputs as we observed marginal performance gains when continuing to increase the number of outputs.</p>
<p>Results</p>
<p>Given the hybrid nature of our system, in order to measure its performance it is meaningful to consider both the trained solver module in isolation, and the hybrid system as a whole.</p>
<p>We also compare the performance of the hybrid system with that of a Transformer encoderdecoder trained to directly output the result of arithmetical expressions. Finally, we compare the  Table 1: Character (top rows) and sequence (bottom rows) accuracy of three different solver models on the sub-expression solution task. The models have 256-, 512-and 1024dimensional embeddings, respectively. We report average and standard deviation of both metrics over 10 batches of 100 sequences. The figures show that the label positional embeddings endow the solver with strong out-of-distribution generalization capability on the sub-expression solution task. performance of the system with that of a Large Language Model fine-tuned for text completion.</p>
<p>Solver</p>
<p>We hypothesize that the solver module can leverage label positional encodings to achieve out-of-distribution generalization capacity on the sub-expression solution task. Therefore, the solver receives at test time complex arithmetical expressions with up to 10 nested operations, i.e. up to 8 more nested operations than seen during training. The goal of the model remains to output the result of the innermost expression contained in the input (replacement candidate) concatenated to the expression itself (replacement target). The sequence accuracy of the solver on the sub-expression solution task is represented in Figure 2a, which shows that the use of Label Positional Encodings enables out-of-distribution generalization. We also report in Table 1 the performance of solver models with different embedding dimensionality to provide further insight on the impact of label positional encodings. Indeed, our results show that, independently of model size, a Transformer encoder-decoder using label positional encodings can generalize to out-of-distribution samples on this task. Figure 2a shows that using Label Positional Encodings also leads to a lower performance on test expressions, especially on the nesting level 1. However, this has no impact on the functioning of the hybrid system when the multi-output generation strategy is used.</p>
<p>Hybrid System</p>
<p>We then test the entire hybrid system on the arithmetic expressions resolution task. Figure 2b shows the sequence accuracy of the hybrid system exploiting the multi-output generation strategy with 100 outputs per input on arithmetical expressions involving up to 10 nested operations. The performance metric shows that the combination of the architectural elements in our model allows to produce exact answers to harder problems than seen during training in a large fraction of cases. The slow performance decline is proportional to expression complexity, as it is due to the accumulation of errors committed by the solver module in several Label PE (multi) refers to a solver that generates 100 outputs per input, applying the same filtering rules as the combiner.  iterations on the same expression. We have also experimented on a possible alternative combiner mechanism, where the selection of the most frequently generated output for a given input is done before filtering ill-formed outputs. Figure 2b shows that the method (Hybrid alt.) globally still achieves superior performance compared to baselines, but has worse performance with respect to the default combiner mechanism in harder problems due to the higher percentage of halted sequences, as shown in Table 2. The similar sequence accuracy of the two methods on expressions with fewer nested operations is due to the fact that the default combiner mechanism, despite halting fewer sequences, commits more semantic errors on the final results.</p>
<p>In the Appendix we provide further experimental evidence to shed light on the functioning of the proposed system. For completeness, we report character accuracy of the models we consider on the expressions resolution task in Table 3. In Table 4 we provide further insight on the impact of the number of outputs per input on both the accuracy of the hybrid system and the percentage of halted sequences. Finally, in Table 5 we report out-of-distribution performance metrics for hybrid systems built with differently-sized solvers to provide experimental evidence on the impact of model size on the generalization capacity of the system.</p>
<p>Comparison with end-to-end model</p>
<p>To motivate empirically the presence of a deterministic component in our hybrid system, we compare the results of our system on the solution of arithmetical expressions with those obtained on the same problem by a Transformer trained to directly output the result of a given  Table 2: Percentage of halted sequences using two different combiner mechanisms. The top row is relative to the default combiner mechanism, while the bottom row refers to a combiner where the most frequently generated outputs are selected before filtering out ill-formed ones.</p>
<p>arithmetical expression. Also in this case, we use label positional encodings on the Transformer encoder and decoder inputs, in order to control the effect of this architectural element on the performance of the system. The training hyperparameters and dataset splits are the same used to train the seq2seq part of the hybrid system. Furthermore, the Transformer has the same architecture and model size as the trained component in our system. Figure 2b represents the sequence accuracy of the end-to-end trained system on this task. It is evident that, while on in-distribution samples the end-to-end trained Transformer has reasonably good performance, it nevertheless fails to generalize the learned solution process on longer and more complex expressions.</p>
<p>Comparison with a Large Language Model</p>
<p>Finally, we compare our hybrid system with a Large Language Model (LLM), since these models often achieve state-of-the-art performance in sophisticated reasoning tasks [4]. We consider the outputs provided by a version of GPT-3 [7], more precisely a variant of the model iteration generally referred to as GPT-3.5. The version of the model we tested is the one referred to as text-davinci-003 in the OpenAI API at the time of writing. According to OpenAI's official website 2 , the model is an InstructGPT model [8] fine-tuned for text completion. We prompt the model with an adapted version of our inputs, first showing the model an example of a correctly solved expression and then asking to complete the solution of a new expression. For example, given the expression (((3<em>2)-2)+5), a possible corresponding prompt could be ((2+4)</em>6)=36<END>\n(((3*2)-2)+5)=. We then require via API to stop the generation of the output string when the model outputs the <END> token. In this way, we ensure that the vast majority of the model outputs is the solution of the input expression proposed by the model. Figure 2b shows the performance of the model in solving arithmetic expressions. Despite the gigantic size of the model and the amount of training data, the model cannot directly output the solution of complex arithmetical expressions.</p>
<p>Related work</p>
<p>In this work, we explored the idea that neural networks can learn to solve problems that require composition of elementary solution steps, like arithmetic expressions, by iteratively applying a learned substitution rule on the input string. Other works have recently explored similar ideas, often in different kind of problems. In [9] the authors propose a Transformer model that is trained to iteratively solve arithmetical expressions involving sums and subtractions. Differently from our system, it requires twice the number of iterations as it is trained to first highlight and then solve relevant parts of the input expressions. The authors of [10] propose a Transformerbased architecture to iteratively solve compositional generalization tasks. In this work, authors exploit relative positional encodings [11] to enable out-of-distribution generalization. Differently from our case, the proposed architecture includes an ad-hoc trainable component named copydecoder, designed to facilitate learning to copy parts of the input in the output. The authors of [12] propose two neural architectures that incorporate ad-hoc learning biases to learn addition and multiplication generalizing beyond the training range. However, their approach is limited to arithmetic problems and cannot be extended to more general symbolic reasoning problems. In [13] the idea of iteratively processing the input to learn a systematic solution strategy is applied to the problem of visual navigation with a multimodal architecture.</p>
<p>Recently, researchers have also focused on studying whether standard seq2seq models can generalize to out-of-distribution samples in compositional tasks. In [14] the authors show that relative positional encodings, carefully tuned early stopping and scaling of encodings can improve the performance of a Universal Transformer [15] on compositional generalization tasks. In a follow-up work [16], the same authors propose an approach to the solution of algorithmic tasks similar to the one we propose in this work. Their proposal consists of learning step-by-step solution strategies modifying the architecture of a Transformer encoder. Differently from our case, the authors propose to recursively modify internal representations, rather than iteratively manipulating symbolic, external representations.</p>
<p>Another recent approach to exploit deep learning models to solve reasoning problems is to embed neural networks into existing automatic reasoning frameworks, such as logic or probabilistic programming languages [17,18]. In such frameworks, systematic generalization is achieved by leveraging constructs in the pre-existing programming languages, supporting theoretical standpoints arguing that human cognition should be understood as a hybrid of neural and algorithmic mechanisms [19]. In our work, instead, we are interested in exploring the possibility that the solution process of symbolic reasoning problems can be implemented with neural systems, leveraging simpler additional mechanisms than the ones used in the works mentioned above, and by injecting learning biases in the neural system to allow the iterative solution of symbolic reasoning problems exploiting their recursive structure.</p>
<p>Discussion</p>
<p>In this paper we have proposed a hybrid system that leverages the combination of a trained module and a deterministic one to achieve strong generalization abilities on the solution of simple arithmetical expressions. While we have only considered this specific task, it should be emphasized that the framework described in section 2 can be used to describe a more general class of symbolic problems where the solution can be derived iteratively. Indeed, one can easily imagine cases (such as symbolic mathematics) in which starting from the initial symbolic expression one can derive a tree -rather than a chain -of syntactically correct expressions.</p>
<p>Furthermore, each expression could be the result of a more generic transformation than a solution or simplification step, which may even make the expression longer. As a consequence, the hybrid system we propose could in principle be adapted to a wider range of symbolic problems than the one considered here. In future work, we plan to explore this aspect, experimenting on tasks like symbolic mathematics, algorithmic tasks and program synthesis.   Table 5: Comparison of hybrid model performance varying the embedding dimensionality of the solver, using a multi-output generation strategy with 100 outputs per sequences. Smaller models make more mistakes and lead both to higher percentage of halted sequences and to lower accuracy of the final results.</p>
<p>Performance of solver modules trained with Label and Sinusoidal Positional Encodings.</p>
<p>Performance of hybrid system, end-to-end and LLM baselines. Hybrid alt. refers to a hybrid system where the combiner selects the most frequent outputs before selecting wellformed outputs.</p>
<p>Figure 2 :
2Sequence accuracy of all models we consider. The solver modules (left) are tested on the sub-expression solution task, while the hybrid systems and baseline models (right) are tested on the task of solution of the full expression. In all cases we report mean and standard deviation computed across 10 batches of 100 sequences per nesting value, except for the Davinci model for which batches contain 10 sequences.</p>
<p>Table 4 :
4Comparison of hybrid model performance varying the number of samples generated using the multi-output generation strategy. Figures show that increasing the number of generated samples strongly impacts the percentage of halted sequences, especially for more complex expressions. 8±1.1 92.9±2.0 87.3±2.2 83.6±2.2 79.4±4.2 70.6±2.6 61.1±3.6 50.7±4.6 46.2±4.2 39.4±3.4 medium 100.0±0.1 93.9±1.4 91.9±1.6 90.9±2.5 90.0±1.6 86.4±2.1 78.7±3.7 72.1±3.9 66.4±5.2 62.2±3.6 large 100.0±0.0 93.8±1.9 92.2±2.7 91.4±2.5 89.8±2.9 86.0±2.9 85.1±4.7 81.6±4.0 77.0±4.8 74.1±3.6 3±2.4 84.2±4.0 74.8±4.0 66.5±4.8 61.8±6.0 50.2±5.7 45.8±4.2 33.4±5.2 30.1±2.9 23.0±4.1 medium 99.9±0.3 85.6±2.4 80.6±2.5 77.7±4.7 74.6±3.7 69.9±5.9 60.2±5.2 55.6±6.2 50.1±7.1 44.8±6.1 large 100.0±0.0 89.8±3.2 85.2±4.0 83.8±2.7 80.5±3.6 74.8±3.2 74.6±6.9 69.0±6.0 63.8±6.5 63.4±5.3Nesting </p>
<p>1 
2 
3 
4 
5 
6 
7 
8 
9 
10 </p>
<p>Char Acc </p>
<p>small 
98.Seq Acc </p>
<p>small 
97.
Our code is available at https://github.com/flavio2018/nesy23.
https://platform.openai.com/docs/model-index-for-researchers
AppendixTable 3: Character accuracy of the three models. We report mean and standard deviation computed across 10 batches of 100 sequences per nesting value, except for the Davinci model for which batches contain 10 sequences. Char Acc
Compositionality decomposed: How do neural networks generalise?. D Hupkes, V Dankers, M Mul, E Bruni, 10.1613/jair.1.11674J. Artif. Intell. Res. 67D. Hupkes, V. Dankers, M. Mul, E. Bruni, Compositionality decomposed: How do neural networks generalise?, J. Artif. Intell. Res. 67 (2020) 757-795. URL: https://doi.org/10.1613/ jair.1.11674. doi:10.1613/jair.1.11674.</p>
<p>Towards better out-of-distribution generalization of neural algorithmic reasoning tasks. S Mahdavi, K Swersky, T Kipf, M Hashemi, C Thrampoulidis, R Liao, Transactions on Machine Learning Research. S. Mahdavi, K. Swersky, T. Kipf, M. Hashemi, C. Thrampoulidis, R. Liao, Towards better out-of-distribution generalization of neural algorithmic reasoning tasks, Transactions on Machine Learning Research (2023). URL: https://openreview.net/forum?id=xkrtvHlp3P.</p>
<p>Neuro-symbolic artificial intelligence: The state of the art. P Hitzler, P. Hitzler, Neuro-symbolic artificial intelligence: The state of the art (2022).</p>
<p>Can neural networks do arithmetic? a survey on the elementary numerical skills of state-of-the-art deep learning models. A Testolin, arXiv:2303.07735arXiv preprintA. Testolin, Can neural networks do arithmetic? a survey on the elementary numerical skills of state-of-the-art deep learning models, arXiv preprint arXiv:2303.07735 (2023).</p>
<p>Polosukhin, Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I , Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, R. GarnettLong Beach, CA, USAA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polo- sukhin, Attention is all you need, in: I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, R. Garnett (Eds.), Advances in Neural Information Pro- cessing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 2017, pp. 5998-6008. URL: https://proceedings. neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.</p>
<p>Systematic generalization and emergent structures in transformers trained on structured tasks. Y Li, J Mcclelland, NeurIPS '22 Workshop on All Things Attention: Bridging Different Perspectives on Attention. Y. Li, J. McClelland, Systematic generalization and emergent structures in transformers trained on structured tasks, in: NeurIPS '22 Workshop on All Things Attention: Bridg- ing Different Perspectives on Attention, 2022. URL: https://openreview.net/forum?id= BTNaKmYdQmE.</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, H. LinCurran Associates, Inc33T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, D. Amodei, Language models are few-shot learners, in: H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, H. Lin (Eds.), Advances in Neural Information Processing Systems, volume 33, Curran Associates, Inc., 2020, pp. 1877-1901. URL: https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Gray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, Advances in Neural Information Processing Systems. A. H. Oh, A. Agarwal, D. Belgrave, K. ChoL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Gray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welin- der, P. Christiano, J. Leike, R. Lowe, Training language models to follow instructions with human feedback, in: A. H. Oh, A. Agarwal, D. Belgrave, K. Cho (Eds.), Advances in Neural Information Processing Systems, 2022. URL: https://openreview.net/forum?id= TG8KACxEON.</p>
<p>Rect: A recursive transformer architecture for generalizable mathematical reasoning. R Deshpande, J Chen, I G Lee, International Workshop on Neural-Symbolic Learning and Reasoning. R. Deshpande, J. Chen, I. G. Lee, Rect: A recursive transformer architecture for generalizable mathematical reasoning, in: International Workshop on Neural-Symbolic Learning and Reasoning, 2021.</p>
<p>Recursive decoding: A situated cognition approach to compositional generation in grounded language understanding. M Setzler, S Howland, L A Phillips, arXiv:2201.11766M. Setzler, S. Howland, L. A. Phillips, Recursive decoding: A situated cognition approach to compositional generation in grounded language understanding, CoRR abs/2201.11766 (2022). URL: https://arxiv.org/abs/2201.11766. arXiv:2201.11766.</p>
<p>Self-attention with relative position representations. P Shaw, J Uszkoreit, A Vaswani, 10.18653/v1/N18-2074Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics2Short PapersP. Shaw, J. Uszkoreit, A. Vaswani, Self-attention with relative position representations, in: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), Association for Computational Linguistics, New Orleans, Louisiana, 2018, pp. 464-468. URL: https://aclanthology.org/N18-2074. doi:10.18653/v1/N18-2074.</p>
<p>A Trask, F Hill, S E Reed, J Rae, C Dyer, P Blunsom, Advances in Neural Information Processing Systems. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, R. GarnettCurran Associates, Inc31Neural arithmetic logic unitsA. Trask, F. Hill, S. E. Reed, J. Rae, C. Dyer, P. Blunsom, Neural arithmetic logic units, in: S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, R. Gar- nett (Eds.), Advances in Neural Information Processing Systems, volume 31, Curran Associates, Inc., 2018. URL: https://proceedings.neurips.cc/paper_files/paper/2018/file/ 0e64a7b00c83e3d22ce6b3acf2c582b6-Paper.pdf.</p>
<p>Iterative decoding for compositional generalization in transformers. L Ruiz, J Ainslie, S Ontañón, arXiv:2110.04169L. Ruiz, J. Ainslie, S. Ontañón, Iterative decoding for compositional generalization in transformers, CoRR abs/2110.04169 (2021). URL: https://arxiv.org/abs/2110.04169. arXiv:2110.04169.</p>
<p>The devil is in the detail: Simple tricks improve systematic generalization of transformers. R Csordás, K Irie, J Schmidhuber, 10.18653/v1/2021.emnlp-main.49Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics, Online and Punta CanaR. Csordás, K. Irie, J. Schmidhuber, The devil is in the detail: Simple tricks improve systematic generalization of transformers, in: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Lin- guistics, Online and Punta Cana, Dominican Republic, 2021, pp. 619-634. URL: https: //aclanthology.org/2021.emnlp-main.49. doi:10.18653/v1/2021.emnlp-main.49.</p>
<p>Universal transformers. M Dehghani, S Gouws, O Vinyals, J Uszkoreit, L Kaiser, International Conference on Learning Representations. M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, L. Kaiser, Universal transformers, in: International Conference on Learning Representations, 2019. URL: https://openreview.net/ forum?id=HyzdRiR9Y7.</p>
<p>The neural data router: Adaptive control flow in transformers improves systematic generalization. R Csordás, K Irie, J Schmidhuber, International Conference on Learning Representations. R. Csordás, K. Irie, J. Schmidhuber, The neural data router: Adaptive control flow in transformers improves systematic generalization, in: International Conference on Learning Representations, 2022. URL: https://openreview.net/forum?id=KBQP4A_J1K.</p>
<p>Abductive knowledge induction from raw data. W.-Z Dai, S Muggleton, 10.24963/ijcai.2021/254doi:10.24963/ijcai.2021/ 254Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, International Joint Conferences on Artificial Intelligence Organization, 2021. Z.-H. Zhouthe Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, International Joint Conferences on Artificial Intelligence Organization, 2021main TrackW.-Z. Dai, S. Muggleton, Abductive knowledge induction from raw data, in: Z.-H. Zhou (Ed.), Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, International Joint Conferences on Artificial Intelligence Organization, 2021, pp. 1845-1851. URL: https://doi.org/10.24963/ijcai.2021/254. doi:10.24963/ijcai.2021/ 254, main Track.</p>
<p>Deepproblog: Neural probabilistic logic programming. R Manhaeve, S Dumancic, A Kimmig, T Demeester, L De Raedt, Advances in Neural Information Processing Systems. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, R. GarnettCurran Associates, Inc31R. Manhaeve, S. Dumancic, A. Kimmig, T. Demeester, L. De Raedt, Deepproblog: Neural probabilistic logic programming, in: S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, R. Garnett (Eds.), Advances in Neural Information Processing Systems, volume 31, Curran Associates, Inc., 2018. URL: https://proceedings.neurips.cc/paper_files/ paper/2018/file/dc5d637ed5e62c36ecb73b654b05ba2a-Paper.pdf.</p>
<p>The Algebraic Mind. G F Marcus, 10.7551/mitpress/1187.001.0001doi:10.7551/mitpress/1187.001.0001. 1-shot 59.7±21.5 44.6±20.4 52.1±20.1 42.4±19.9 33The MIT Press3±15.6 28.8±15.3 26.2±10.2 24.9±9.8 22.5±4.9 22.0±10.8 20 samples 99.5±0.8 92.4±2.3 89.4±4.1 88.4±3.6 88.0±4.7 85.6±3.9 80.8±4.9 76.6±3.5 72.0±6.0 67.3±3.4 40 samples 99.8±0.4 92.4±2.8 91.6±2.2 90.9±2.9 88.0±2.8 87.0±3.1 83.7±3.4 79.6±3.6 74.9±4.9 70.0±5.0 60 samples 100.0±0.1 92.6±2.2 90.4±3.2 88.5±4.8 89.9±3.4 88.3±2.6 85.1±2.6 83.8±3.7 76.9±2.9 72.6±5.6 80 samples 99.9±0.3 94.9±1.3 91.4±2.4 90.8±2.2 90.2±2.6 89.0±3.7 85.5±1.4 80.6±3.5 79.5±3.9 70.8±3.5 100 samples 100.0±0.0 93.8±1.9 92.2±2.7 91.4±2.5 89.8±2.9 86.0±2.9 85.1±4.7 81.6±4.0 77.0±4.8 74.1±3.6G. F. Marcus, The Algebraic Mind, The MIT Press, 2001. URL: https://doi.org/10.7551/ mitpress/1187.001.0001. doi:10.7551/mitpress/1187.001.0001. 1-shot 59.7±21.5 44.6±20.4 52.1±20.1 42.4±19.9 33.3±15.6 28.8±15.3 26.2±10.2 24.9±9.8 22.5±4.9 22.0±10.8 20 samples 99.5±0.8 92.4±2.3 89.4±4.1 88.4±3.6 88.0±4.7 85.6±3.9 80.8±4.9 76.6±3.5 72.0±6.0 67.3±3.4 40 samples 99.8±0.4 92.4±2.8 91.6±2.2 90.9±2.9 88.0±2.8 87.0±3.1 83.7±3.4 79.6±3.6 74.9±4.9 70.0±5.0 60 samples 100.0±0.1 92.6±2.2 90.4±3.2 88.5±4.8 89.9±3.4 88.3±2.6 85.1±2.6 83.8±3.7 76.9±2.9 72.6±5.6 80 samples 99.9±0.3 94.9±1.3 91.4±2.4 90.8±2.2 90.2±2.6 89.0±3.7 85.5±1.4 80.6±3.5 79.5±3.9 70.8±3.5 100 samples 100.0±0.0 93.8±1.9 92.2±2.7 91.4±2.5 89.8±2.9 86.0±2.9 85.1±4.7 81.6±4.0 77.0±4.8 74.1±3.6</p>            </div>
        </div>

    </div>
</body>
</html>
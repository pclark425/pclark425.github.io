<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7455 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7455</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7455</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-261242705</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.14306v1.pdf" target="_blank">Evaluating the Robustness to Instructions of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Recently, Instruction fine-tuning has risen to prominence as a potential method for enhancing the zero-shot capabilities of Large Language Models (LLMs) on novel tasks. This technique has shown an exceptional ability to boost the performance of moderately sized LLMs, sometimes even reaching performance levels comparable to those of much larger model variants. The focus is on the robustness of instruction-tuned LLMs to seen and un-seen tasks. We conducted an exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction datasets as case studies. We carried out a comprehensive evaluation of these instruction-following LLMs which have been tuned based on open-domain instructions and task-oriented instructions. The main discussion is their performance and robustness towards instructions. We have observed that in most cases, the model’s performance in dealing with unfamiliar instructions tends to worsen significantly, and the robustness of the model for RE instructions deteriorates compared to QA. Further, we discovered that up until a certain parameter size threshold (3B), the performance of the FLAN-T5 model improves as the parameter count increases. The robustness of different scales of FLAN-T5 models to RE instruction is worse than the robustness to QA instruction.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7455.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7455.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-3B @ TACRED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (3B) on TACRED comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned Flan-T5 (3B) evaluated on TACRED with same input examples presented as QA (seen) or RE (unseen) instructions; performance differs by instruction framing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-finetuned T5 variant (FLAN family) trained on a multitask instruction mixture to follow natural-language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TACRED (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Relation extraction dataset (TACRED) evaluated both as original RE and as reformulated QA using QA4RE; same input examples used for both formats.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction (multiple-choice/QA style); Unseen: RE instruction (relation extraction formulation)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (instruction framing)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same input examples for both formats via QA4RE; three paraphrased phrasings provided for QA and RE instructions; zero-shot evaluation on 1000 examples; measured average F1 and standard deviation across instruction phrasings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 51.6 F1 (±1.8); UNSEEN (RE): 55.7 F1 (±2.1)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 51.6 F1 (±1.8)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+4.1 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, 1000 examples, three paraphrased instruction phrasings per format, same inputs via QA4RE, F1 averaged over phrasings.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7455.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-3B @ RETACRED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (3B) on RETACRED comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Flan-T5-3B shows substantially lower performance when the same inputs are framed with RE instructions versus QA instructions on RETACRED.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-finetuned T5 variant (FLAN family).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RETACRED (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>RE dataset RETACRED presented as QA vs RE with identical inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same-input comparison, three paraphrased phrasings per instruction type, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 65.1 F1 (±1.0); UNSEEN (RE): 50.6 F1 (±2.4)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 65.1 F1 (±1.0)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-14.5 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, 1000 examples, three paraphrased instruction phrasings, same inputs via QA4RE.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7455.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-3B @ TACREV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (3B) on TACREV comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Flan-T5-3B slightly improves on TACREV when inputs are reframed as RE versus QA, demonstrating format sensitivity can go either way per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-finetuned T5 variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TACREV (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Relation extraction (TACREV) evaluated as QA vs RE on identical examples.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs, three paraphrased instruction phrasings per format, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 53.7 F1 (±2.3); UNSEEN (RE): 56.8 F1 (±2.6)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 53.7 F1 (±2.3)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+3.2 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, 1000 examples, three paraphrased phrasings, same inputs via QA4RE.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7455.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-3B @ SemEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (3B) on SemEval comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Flan-T5-3B performs better when SemEval inputs are presented as QA (seen) than as RE (unseen), reflecting format sensitivity particularly on multi-choice-like RE conversions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-finetuned T5 family model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SemEval-2010 Task 8 (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>SemEval relation classification (19 choices) evaluated as QA vs RE with identical inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs, three paraphrased instruction phrasings, zero-shot, 1000 examples; SemEval converted to a 19-choice QA in QA4RE.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 35.6 F1 (±0.8); UNSEEN (RE): 30.9 F1 (±1.4)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 35.6 F1 (±0.8)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-4.7 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, three paraphrased instructions per format, same inputs via QA4RE, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7455.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca-7B @ TACRED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca (7B) on TACRED comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Alpaca-7B (instruction-tuned via Alpaca dataset) shows degraded performance when the same inputs are asked in RE format versus QA on TACRED.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-based model fine-tuned on the Alpaca instruction dataset (user-shared instructions), less exposure to conventional NLP foundational tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TACRED (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Relation extraction framed as QA vs RE using identical inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same-input comparison, three paraphrased instruction phrasings per format, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 25.5 F1 (±1.7); UNSEEN (RE): 19.0 F1 (±2.4)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 25.5 F1 (±1.7)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-6.5 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, identical inputs via QA4RE, average over three paraphrases, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7455.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca-7B @ RETACRED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca (7B) on RETACRED comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Alpaca-7B suffers a large drop when RETACRED examples are presented as RE vs QA, indicating strong sensitivity to instruction framing for this model/dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-based instruction-tuned model (Alpaca).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RETACRED (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>RE vs QA same-input evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs, three paraphrased phrasings per instruction, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 42.7 F1 (±4.0); UNSEEN (RE): 18.5 F1 (±2.1)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 42.7 F1 (±4.0)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-24.2 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, 1000 examples, averaged over instruction phrasings, same inputs via QA4RE.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7455.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca-7B @ TACREV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca (7B) on TACREV comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Alpaca-7B shows lower F1 on TACREV when evaluated under RE instructions compared to QA instructions with identical inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLaMA variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TACREV (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Relation extraction framed as QA vs RE with identical inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs, three paraphrased phrasings, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 26.5 F1 (±1.8); UNSEEN (RE): 20.0 F1 (±0.9)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 26.5 F1 (±1.8)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-6.6 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, same inputs via QA4RE, averaged over three paraphrases, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7455.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca-7B @ SemEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca (7B) on SemEval comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Alpaca-7B uniquely improves when SemEval examples are presented in RE format vs QA (very low absolute scores), reflecting instability at low absolute performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-derived instruction-tuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SemEval (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>SemEval relation classification (19-way) reformulated as QA vs RE.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs, three paraphrased instruction phrasings, zero-shot, difficult 19-way choice setup.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 0.6 F1 (±2.2); UNSEEN (RE): 5.9 F1 (±0.4)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 0.6 F1 (±2.2)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+5.3 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, same inputs via QA4RE, three paraphrases, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7455.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-7B @ TACRED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna (7B) on TACRED comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vicuna-7B shows a large drop when TACRED examples are posed as RE vs QA, indicating format sensitivity possibly due to its conversational fine-tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-derived conversational model fine-tuned on user-shared conversations; less emphasis on foundational NLP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TACRED (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>RE vs QA same-input evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs, three paraphrased instructions per format, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 31.3 F1 (±1.7); UNSEEN (RE): 18.2 F1 (±4.1)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 31.3 F1 (±1.7)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-13.1 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, three paraphrases per instruction type, same inputs via QA4RE, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7455.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-7B @ RETACRED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna (7B) on RETACRED comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vicuna-7B shows a small drop when RETACRED is framed as RE vs QA, indicating moderate sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversationally fine-tuned LLaMA variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RETACRED (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Relation extraction framed both ways with same inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs across formats, three paraphrases, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 38.2 F1 (±3.8); UNSEEN (RE): 36.2 F1 (±2.0)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 38.2 F1 (±3.8)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-2.0 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, same inputs via QA4RE, averaged over paraphrases, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7455.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-7B @ TACREV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna (7B) on TACREV comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vicuna-7B performance on TACREV drops substantially when the instruction is RE rather than QA, showing format-based degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-derived conversational model fine-tuned on conversation data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TACREV (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>RE vs QA same-input evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Three paraphrased instruction phrasings per format, zero-shot, same inputs via QA4RE, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 31.9 F1 (±1.7); UNSEEN (RE): 19.5 F1 (±2.4)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 31.9 F1 (±1.7)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-12.4 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, three paraphrases, 1000 examples, same inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7455.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-7B @ SemEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna (7B) on SemEval comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vicuna-7B shows a minor absolute improvement when SemEval is presented in RE format vs QA, but absolute scores are low.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversational LLaMA variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SemEval (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>19-way relation classification evaluated as QA and RE with same inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs, three paraphrased instruction variants, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 4.7 F1 (±0.6); UNSEEN (RE): 5.4 F1 (±0.4)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 4.7 F1 (±0.6)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.7 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, averaged over paraphrases, same inputs via QA4RE, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7455.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T0++ 11B @ TACRED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T0++ (11B) on TACRED comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>T0++ 11B (multitask instruction-tuned model) loses F1 when TACRED inputs are presented as RE vs QA, showing sensitivity to unseen instruction phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0++ (11B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T0++ is an instruction-tuned seq2seq style transformer trained on a large mixture of prompted tasks (PromptSource/Super-NaturalInstructions family).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TACRED (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Relation extraction presented as QA vs RE using identical examples.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs across formats, three paraphrases per instruction type, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 33.5 F1 (±0.4); UNSEEN (RE): 27.1 F1 (±1.4)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 33.5 F1 (±0.4)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-6.4 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, averaged over paraphrases, 1000 same-input examples, QA4RE conversions.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7455.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T0++ 11B @ RETACRED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T0++ (11B) on RETACRED comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>T0++ shows a large drop on RETACRED when the instruction format is switched from QA to RE for identical inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0++ (11B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-finetuned transformer trained on many prompted tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RETACRED (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>RE converted to QA vs original RE, same examples evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same-input evaluation, three paraphrases per format, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 47.0 F1 (±0.7); UNSEEN (RE): 31.5 F1 (±2.0)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 47.0 F1 (±0.7)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-15.5 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, three paraphrased instructions per format, same inputs via QA4RE, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e7455.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T0++ 11B @ TACREV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T0++ (11B) on TACREV comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>T0++ loses F1 on TACREV when instruction framing changes from QA to RE using the same inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0++ (11B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multitask instruction-tuned transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TACREV (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Relation extraction vs QA formulation with identical inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs via QA4RE, three paraphrases, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 32.1 F1 (±0.2); UNSEEN (RE): 25.7 F1 (±2.1)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 32.1 F1 (±0.2)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-6.4 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, averaged across paraphrases, 1000 same-input examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e7455.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T0++ 11B @ SemEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T0++ (11B) on SemEval comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>T0++ shows an unusual large absolute improvement presenting SemEval as RE vs QA (but both are challenging), illustrating dataset-specific format effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0++ (11B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned seq2seq transformer trained on many prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SemEval (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>19-way relation classification evaluated as QA and RE with identical inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs, three paraphrases, zero-shot; SemEval converted to multi-choice-like QA with 19 choices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 2.6 F1 (±0.4); UNSEEN (RE): 17.6 F1 (±0.6)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 2.6 F1 (±0.4)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+15.0 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, 1000 examples, three paraphrases per instruction, same inputs via QA4RE.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e7455.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-11B @ TACRED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (11B) on TACRED comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Flan-T5-11B is robust but still loses F1 when TACRED is presented as RE vs QA, showing that instruction sensitivity persists at larger mid-model scales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large FLAN-T5 model finetuned on instruction collections across many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TACRED (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>RE vs QA same-input evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs via QA4RE, three paraphrases per instruction type, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 53.2 F1 (±1.3); UNSEEN (RE): 49.1 F1 (±2.4)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 53.2 F1 (±1.3)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-4.1 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, averaged across three paraphrases, 1000 same-input examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e7455.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-11B @ RETACRED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (11B) on RETACRED comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Flan-T5-11B shows a substantial performance drop when RETACRED is presented as RE vs QA, demonstrating instruction-format vulnerability even at 11B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-finetuned FLAN-T5 model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RETACRED (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Relation extraction dataset evaluated under QA and RE instruction framings with identical inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same input examples, three paraphrased instruction phrasings, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 66.8 F1 (±1.3); UNSEEN (RE): 49.8 F1 (±3.8)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 66.8 F1 (±1.3)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-17.0 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, averaged over paraphrases, same inputs via QA4RE, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.18">
                <h3 class="extraction-instance">Extracted Data Instance 18 (e7455.18)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-11B @ TACREV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (11B) on TACREV comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Flan-T5-11B shows nearly identical performance for TACREV when comparing QA vs RE instruction framings on the same inputs, suggesting dataset-dependent format sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-finetuned FLAN-T5 large model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TACREV (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Relation extraction evaluated as QA vs RE with same examples.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs, three paraphrased instruction variants, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 51.5 F1 (±0.6); UNSEEN (RE): 51.3 F1 (±3.0)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 51.5 F1 (±0.6)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-0.2 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, identical inputs via QA4RE, averaged across instruction phrasings, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.19">
                <h3 class="extraction-instance">Extracted Data Instance 19 (e7455.19)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-11B @ SemEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (11B) on SemEval comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Flan-T5-11B displays a large drop on SemEval when the instruction is switched from QA to RE, demonstrating strong sensitivity to instruction presentation on this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large FLAN-T5 instruction-finetuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SemEval (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>19-way relation classification converted to QA and RE formats with identical inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs via QA4RE; three paraphrased instruction phrasings; zero-shot; SemEval is especially challenging (19 choices).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 35.0 F1 (±0.1); UNSEEN (RE): 16.1 F1 (±3.0)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 35.0 F1 (±0.1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-18.9 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, 1000 examples, averaged over paraphrases, same inputs via QA4RE.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.20">
                <h3 class="extraction-instance">Extracted Data Instance 20 (e7455.20)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WizardLM-13B @ TACRED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WizardLM (13B) on TACRED comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>WizardLM-13B (fine-tuned with Evol-Instruct-style complex instructions) shows performance degradation when TACRED inputs are presented as RE vs QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WizardLM-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-based model fine-tuned using complex instruction rewrites (Evol-Instruct), intended to follow more complex instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TACRED (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>RE vs QA same-input evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs via QA4RE, three paraphrased phrasings, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 27.3 F1 (±4.1); UNSEEN (RE): 22.1 F1 (±1.2)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 27.3 F1 (±4.1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-5.1 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, averaged over instruction phrasings, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.21">
                <h3 class="extraction-instance">Extracted Data Instance 21 (e7455.21)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WizardLM-13B @ RETACRED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WizardLM (13B) on RETACRED comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>WizardLM-13B suffers a large drop on RETACRED when using RE-format instructions versus QA, reflecting format-dependent performance variability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WizardLM-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-fine-tuned LLaMA variant using complex instruction rewriting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RETACRED (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>RE dataset evaluated as QA and RE with identical inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs, three paraphrased instruction phrasings per format, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 48.2 F1 (±1.8); UNSEEN (RE): 27.6 F1 (±2.2)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 48.2 F1 (±1.8)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-20.6 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, same inputs via QA4RE, averaged across instruction phrasings.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.22">
                <h3 class="extraction-instance">Extracted Data Instance 22 (e7455.22)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WizardLM-13B @ TACREV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WizardLM (13B) on TACREV comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>WizardLM-13B shows a modest drop on TACREV when the instruction changes from QA to RE for identical inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WizardLM-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-based instruction-following model fine-tuned with complex instruction rewrites.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TACREV (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Relation extraction vs QA with same examples.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same input examples, three paraphrases per instruction type, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 27.0 F1 (±3.7); UNSEEN (RE): 22.7 F1 (±1.6)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 27.0 F1 (±3.7)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-4.3 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, averaged over paraphrases, same inputs via QA4RE.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.23">
                <h3 class="extraction-instance">Extracted Data Instance 23 (e7455.23)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WizardLM-13B @ SemEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WizardLM (13B) on SemEval comparing QA (seen) vs RE (unseen) instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>WizardLM-13B shows a small absolute improvement on SemEval when framed as RE vs QA, though overall scores are low and dataset is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WizardLM-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following LLaMA variant fined-tuned using Evol-Instruct rewrites.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SemEval (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>19-way relation classification evaluated as QA vs RE with same inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same inputs via QA4RE, three paraphrases per instruction type, zero-shot, 1000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SEEN (QA): 5.1 F1 (±1.4); UNSEEN (RE): 6.7 F1 (±0.7)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SEEN (QA): 5.1 F1 (±1.4)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+1.6 absolute (RE vs QA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, averaged across instruction phrasings, same inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7455.24">
                <h3 class="extraction-instance">Extracted Data Instance 24 (e7455.24)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5 scaling (80M→11B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 series scaling analysis comparing robustness/performance across instruction formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Across Flan-T5 sizes (80M up to 11B) performance generally improves with size up to XLarge, and robustness to QA instructions is consistently better than to RE instructions; models under 1B sometimes show the reverse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5 (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Family of instruction-finetuned T5 models evaluated across multiple parameter counts (Small 80M → XXL 11B).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>80M, <1B, 1B, 3B, 11B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TACRED / RETACRED / TACREV / SemEval (via QA4RE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Relation extraction datasets evaluated as QA (seen) and RE (unseen) instructions using identical input examples to measure format sensitivity across model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Seen: QA instruction; Unseen: RE instruction (comparison across model sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / model scale</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot evaluation using same inputs via QA4RE, three paraphrased instruction phrasings per format; robustness measured as standard deviation across paraphrases; analysis of performance trends from Small (80M) to XXL (11B).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1 (and std across instruction phrasings as robustness)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: performance generally increases from Small→Large; improvements plateau/turn after XLarge; 3B and 11B do not give strictly proportional gains beyond threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative: robustness to QA > RE across most sizes; in 20 comparisons only 3 (all <1B) showed better robustness to RE than QA.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot, multiple model sizes tested (80M→11B), same-input QA4RE conversions, robustness quantified by std across paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Robustness to Instructions of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Aligning instruction tasks unlocks large language models as zero-shot relation extractors <em>(Rating: 2)</em></li>
                <li>The flan collection: Designing data and methods for effective instruction tuning <em>(Rating: 2)</em></li>
                <li>Multitask prompted training enables zero-shot task generalization <em>(Rating: 2)</em></li>
                <li>T0: Multitask prompted training enables zero-shot task generalization <em>(Rating: 1)</em></li>
                <li>PromptSource: An integrated development environment and repository for natural language prompts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7455",
    "paper_id": "paper-261242705",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Flan-T5-3B @ TACRED",
            "name_full": "Flan-T5 (3B) on TACRED comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Instruction-tuned Flan-T5 (3B) evaluated on TACRED with same input examples presented as QA (seen) or RE (unseen) instructions; performance differs by instruction framing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-3B",
            "model_description": "Instruction-finetuned T5 variant (FLAN family) trained on a multitask instruction mixture to follow natural-language instructions.",
            "model_size": "3B",
            "task_name": "TACRED (via QA4RE)",
            "task_description": "Relation extraction dataset (TACRED) evaluated both as original RE and as reformulated QA using QA4RE; same input examples used for both formats.",
            "problem_format": "Seen: QA instruction (multiple-choice/QA style); Unseen: RE instruction (relation extraction formulation)",
            "format_category": "prompt style (instruction framing)",
            "format_details": "Same input examples for both formats via QA4RE; three paraphrased phrasings provided for QA and RE instructions; zero-shot evaluation on 1000 examples; measured average F1 and standard deviation across instruction phrasings.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 51.6 F1 (±1.8); UNSEEN (RE): 55.7 F1 (±2.1)",
            "baseline_performance": "SEEN (QA): 51.6 F1 (±1.8)",
            "performance_change": "+4.1 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, 1000 examples, three paraphrased instruction phrasings per format, same inputs via QA4RE, F1 averaged over phrasings.",
            "statistical_significance": null,
            "uuid": "e7455.0",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Flan-T5-3B @ RETACRED",
            "name_full": "Flan-T5 (3B) on RETACRED comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Flan-T5-3B shows substantially lower performance when the same inputs are framed with RE instructions versus QA instructions on RETACRED.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-3B",
            "model_description": "Instruction-finetuned T5 variant (FLAN family).",
            "model_size": "3B",
            "task_name": "RETACRED (via QA4RE)",
            "task_description": "RE dataset RETACRED presented as QA vs RE with identical inputs.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same-input comparison, three paraphrased phrasings per instruction type, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 65.1 F1 (±1.0); UNSEEN (RE): 50.6 F1 (±2.4)",
            "baseline_performance": "SEEN (QA): 65.1 F1 (±1.0)",
            "performance_change": "-14.5 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, 1000 examples, three paraphrased instruction phrasings, same inputs via QA4RE.",
            "statistical_significance": null,
            "uuid": "e7455.1",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Flan-T5-3B @ TACREV",
            "name_full": "Flan-T5 (3B) on TACREV comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Flan-T5-3B slightly improves on TACREV when inputs are reframed as RE versus QA, demonstrating format sensitivity can go either way per dataset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-3B",
            "model_description": "Instruction-finetuned T5 variant.",
            "model_size": "3B",
            "task_name": "TACREV (via QA4RE)",
            "task_description": "Relation extraction (TACREV) evaluated as QA vs RE on identical examples.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs, three paraphrased instruction phrasings per format, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 53.7 F1 (±2.3); UNSEEN (RE): 56.8 F1 (±2.6)",
            "baseline_performance": "SEEN (QA): 53.7 F1 (±2.3)",
            "performance_change": "+3.2 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, 1000 examples, three paraphrased phrasings, same inputs via QA4RE.",
            "statistical_significance": null,
            "uuid": "e7455.2",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Flan-T5-3B @ SemEval",
            "name_full": "Flan-T5 (3B) on SemEval comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Flan-T5-3B performs better when SemEval inputs are presented as QA (seen) than as RE (unseen), reflecting format sensitivity particularly on multi-choice-like RE conversions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-3B",
            "model_description": "Instruction-finetuned T5 family model.",
            "model_size": "3B",
            "task_name": "SemEval-2010 Task 8 (via QA4RE)",
            "task_description": "SemEval relation classification (19 choices) evaluated as QA vs RE with identical inputs.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs, three paraphrased instruction phrasings, zero-shot, 1000 examples; SemEval converted to a 19-choice QA in QA4RE.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 35.6 F1 (±0.8); UNSEEN (RE): 30.9 F1 (±1.4)",
            "baseline_performance": "SEEN (QA): 35.6 F1 (±0.8)",
            "performance_change": "-4.7 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, three paraphrased instructions per format, same inputs via QA4RE, 1000 examples.",
            "statistical_significance": null,
            "uuid": "e7455.3",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Alpaca-7B @ TACRED",
            "name_full": "Alpaca (7B) on TACRED comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Alpaca-7B (instruction-tuned via Alpaca dataset) shows degraded performance when the same inputs are asked in RE format versus QA on TACRED.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B",
            "model_description": "LLaMA-based model fine-tuned on the Alpaca instruction dataset (user-shared instructions), less exposure to conventional NLP foundational tasks.",
            "model_size": "7B",
            "task_name": "TACRED (via QA4RE)",
            "task_description": "Relation extraction framed as QA vs RE using identical inputs.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same-input comparison, three paraphrased instruction phrasings per format, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 25.5 F1 (±1.7); UNSEEN (RE): 19.0 F1 (±2.4)",
            "baseline_performance": "SEEN (QA): 25.5 F1 (±1.7)",
            "performance_change": "-6.5 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, identical inputs via QA4RE, average over three paraphrases, 1000 examples.",
            "statistical_significance": null,
            "uuid": "e7455.4",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Alpaca-7B @ RETACRED",
            "name_full": "Alpaca (7B) on RETACRED comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Alpaca-7B suffers a large drop when RETACRED examples are presented as RE vs QA, indicating strong sensitivity to instruction framing for this model/dataset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B",
            "model_description": "LLaMA-based instruction-tuned model (Alpaca).",
            "model_size": "7B",
            "task_name": "RETACRED (via QA4RE)",
            "task_description": "RE vs QA same-input evaluation.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs, three paraphrased phrasings per instruction, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 42.7 F1 (±4.0); UNSEEN (RE): 18.5 F1 (±2.1)",
            "baseline_performance": "SEEN (QA): 42.7 F1 (±4.0)",
            "performance_change": "-24.2 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, 1000 examples, averaged over instruction phrasings, same inputs via QA4RE.",
            "statistical_significance": null,
            "uuid": "e7455.5",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Alpaca-7B @ TACREV",
            "name_full": "Alpaca (7B) on TACREV comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Alpaca-7B shows lower F1 on TACREV when evaluated under RE instructions compared to QA instructions with identical inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B",
            "model_description": "Instruction-tuned LLaMA variant.",
            "model_size": "7B",
            "task_name": "TACREV (via QA4RE)",
            "task_description": "Relation extraction framed as QA vs RE with identical inputs.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs, three paraphrased phrasings, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 26.5 F1 (±1.8); UNSEEN (RE): 20.0 F1 (±0.9)",
            "baseline_performance": "SEEN (QA): 26.5 F1 (±1.8)",
            "performance_change": "-6.6 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, same inputs via QA4RE, averaged over three paraphrases, 1000 examples.",
            "statistical_significance": null,
            "uuid": "e7455.6",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Alpaca-7B @ SemEval",
            "name_full": "Alpaca (7B) on SemEval comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Alpaca-7B uniquely improves when SemEval examples are presented in RE format vs QA (very low absolute scores), reflecting instability at low absolute performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B",
            "model_description": "LLaMA-derived instruction-tuned model.",
            "model_size": "7B",
            "task_name": "SemEval (via QA4RE)",
            "task_description": "SemEval relation classification (19-way) reformulated as QA vs RE.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs, three paraphrased instruction phrasings, zero-shot, difficult 19-way choice setup.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 0.6 F1 (±2.2); UNSEEN (RE): 5.9 F1 (±0.4)",
            "baseline_performance": "SEEN (QA): 0.6 F1 (±2.2)",
            "performance_change": "+5.3 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, same inputs via QA4RE, three paraphrases, 1000 examples.",
            "statistical_significance": null,
            "uuid": "e7455.7",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Vicuna-7B @ TACRED",
            "name_full": "Vicuna (7B) on TACRED comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Vicuna-7B shows a large drop when TACRED examples are posed as RE vs QA, indicating format sensitivity possibly due to its conversational fine-tuning data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-7B",
            "model_description": "LLaMA-derived conversational model fine-tuned on user-shared conversations; less emphasis on foundational NLP tasks.",
            "model_size": "7B",
            "task_name": "TACRED (via QA4RE)",
            "task_description": "RE vs QA same-input evaluation.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs, three paraphrased instructions per format, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 31.3 F1 (±1.7); UNSEEN (RE): 18.2 F1 (±4.1)",
            "baseline_performance": "SEEN (QA): 31.3 F1 (±1.7)",
            "performance_change": "-13.1 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, three paraphrases per instruction type, same inputs via QA4RE, 1000 examples.",
            "statistical_significance": null,
            "uuid": "e7455.8",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Vicuna-7B @ RETACRED",
            "name_full": "Vicuna (7B) on RETACRED comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Vicuna-7B shows a small drop when RETACRED is framed as RE vs QA, indicating moderate sensitivity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-7B",
            "model_description": "Conversationally fine-tuned LLaMA variant.",
            "model_size": "7B",
            "task_name": "RETACRED (via QA4RE)",
            "task_description": "Relation extraction framed both ways with same inputs.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs across formats, three paraphrases, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 38.2 F1 (±3.8); UNSEEN (RE): 36.2 F1 (±2.0)",
            "baseline_performance": "SEEN (QA): 38.2 F1 (±3.8)",
            "performance_change": "-2.0 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, same inputs via QA4RE, averaged over paraphrases, 1000 examples.",
            "statistical_significance": null,
            "uuid": "e7455.9",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Vicuna-7B @ TACREV",
            "name_full": "Vicuna (7B) on TACREV comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Vicuna-7B performance on TACREV drops substantially when the instruction is RE rather than QA, showing format-based degradation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-7B",
            "model_description": "LLaMA-derived conversational model fine-tuned on conversation data.",
            "model_size": "7B",
            "task_name": "TACREV (via QA4RE)",
            "task_description": "RE vs QA same-input evaluation.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Three paraphrased instruction phrasings per format, zero-shot, same inputs via QA4RE, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 31.9 F1 (±1.7); UNSEEN (RE): 19.5 F1 (±2.4)",
            "baseline_performance": "SEEN (QA): 31.9 F1 (±1.7)",
            "performance_change": "-12.4 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, three paraphrases, 1000 examples, same inputs.",
            "statistical_significance": null,
            "uuid": "e7455.10",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Vicuna-7B @ SemEval",
            "name_full": "Vicuna (7B) on SemEval comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Vicuna-7B shows a minor absolute improvement when SemEval is presented in RE format vs QA, but absolute scores are low.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-7B",
            "model_description": "Conversational LLaMA variant.",
            "model_size": "7B",
            "task_name": "SemEval (via QA4RE)",
            "task_description": "19-way relation classification evaluated as QA and RE with same inputs.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs, three paraphrased instruction variants, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 4.7 F1 (±0.6); UNSEEN (RE): 5.4 F1 (±0.4)",
            "baseline_performance": "SEEN (QA): 4.7 F1 (±0.6)",
            "performance_change": "+0.7 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, averaged over paraphrases, same inputs via QA4RE, 1000 examples.",
            "statistical_significance": null,
            "uuid": "e7455.11",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "T0++ 11B @ TACRED",
            "name_full": "T0++ (11B) on TACRED comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "T0++ 11B (multitask instruction-tuned model) loses F1 when TACRED inputs are presented as RE vs QA, showing sensitivity to unseen instruction phrasing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0++ (11B)",
            "model_description": "T0++ is an instruction-tuned seq2seq style transformer trained on a large mixture of prompted tasks (PromptSource/Super-NaturalInstructions family).",
            "model_size": "11B",
            "task_name": "TACRED (via QA4RE)",
            "task_description": "Relation extraction presented as QA vs RE using identical examples.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs across formats, three paraphrases per instruction type, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 33.5 F1 (±0.4); UNSEEN (RE): 27.1 F1 (±1.4)",
            "baseline_performance": "SEEN (QA): 33.5 F1 (±0.4)",
            "performance_change": "-6.4 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, averaged over paraphrases, 1000 same-input examples, QA4RE conversions.",
            "statistical_significance": null,
            "uuid": "e7455.12",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "T0++ 11B @ RETACRED",
            "name_full": "T0++ (11B) on RETACRED comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "T0++ shows a large drop on RETACRED when the instruction format is switched from QA to RE for identical inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0++ (11B)",
            "model_description": "Instruction-finetuned transformer trained on many prompted tasks.",
            "model_size": "11B",
            "task_name": "RETACRED (via QA4RE)",
            "task_description": "RE converted to QA vs original RE, same examples evaluated.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same-input evaluation, three paraphrases per format, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 47.0 F1 (±0.7); UNSEEN (RE): 31.5 F1 (±2.0)",
            "baseline_performance": "SEEN (QA): 47.0 F1 (±0.7)",
            "performance_change": "-15.5 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, three paraphrased instructions per format, same inputs via QA4RE, 1000 examples.",
            "statistical_significance": null,
            "uuid": "e7455.13",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "T0++ 11B @ TACREV",
            "name_full": "T0++ (11B) on TACREV comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "T0++ loses F1 on TACREV when instruction framing changes from QA to RE using the same inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0++ (11B)",
            "model_description": "Multitask instruction-tuned transformer.",
            "model_size": "11B",
            "task_name": "TACREV (via QA4RE)",
            "task_description": "Relation extraction vs QA formulation with identical inputs.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs via QA4RE, three paraphrases, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 32.1 F1 (±0.2); UNSEEN (RE): 25.7 F1 (±2.1)",
            "baseline_performance": "SEEN (QA): 32.1 F1 (±0.2)",
            "performance_change": "-6.4 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, averaged across paraphrases, 1000 same-input examples.",
            "statistical_significance": null,
            "uuid": "e7455.14",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "T0++ 11B @ SemEval",
            "name_full": "T0++ (11B) on SemEval comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "T0++ shows an unusual large absolute improvement presenting SemEval as RE vs QA (but both are challenging), illustrating dataset-specific format effects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0++ (11B)",
            "model_description": "Instruction-tuned seq2seq transformer trained on many prompts.",
            "model_size": "11B",
            "task_name": "SemEval (via QA4RE)",
            "task_description": "19-way relation classification evaluated as QA and RE with identical inputs.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs, three paraphrases, zero-shot; SemEval converted to multi-choice-like QA with 19 choices.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 2.6 F1 (±0.4); UNSEEN (RE): 17.6 F1 (±0.6)",
            "baseline_performance": "SEEN (QA): 2.6 F1 (±0.4)",
            "performance_change": "+15.0 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, 1000 examples, three paraphrases per instruction, same inputs via QA4RE.",
            "statistical_significance": null,
            "uuid": "e7455.15",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Flan-T5-11B @ TACRED",
            "name_full": "Flan-T5 (11B) on TACRED comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Flan-T5-11B is robust but still loses F1 when TACRED is presented as RE vs QA, showing that instruction sensitivity persists at larger mid-model scales.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-11B",
            "model_description": "Large FLAN-T5 model finetuned on instruction collections across many tasks.",
            "model_size": "11B",
            "task_name": "TACRED (via QA4RE)",
            "task_description": "RE vs QA same-input evaluation.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs via QA4RE, three paraphrases per instruction type, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 53.2 F1 (±1.3); UNSEEN (RE): 49.1 F1 (±2.4)",
            "baseline_performance": "SEEN (QA): 53.2 F1 (±1.3)",
            "performance_change": "-4.1 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, averaged across three paraphrases, 1000 same-input examples.",
            "statistical_significance": null,
            "uuid": "e7455.16",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Flan-T5-11B @ RETACRED",
            "name_full": "Flan-T5 (11B) on RETACRED comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Flan-T5-11B shows a substantial performance drop when RETACRED is presented as RE vs QA, demonstrating instruction-format vulnerability even at 11B.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-11B",
            "model_description": "Large instruction-finetuned FLAN-T5 model.",
            "model_size": "11B",
            "task_name": "RETACRED (via QA4RE)",
            "task_description": "Relation extraction dataset evaluated under QA and RE instruction framings with identical inputs.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same input examples, three paraphrased instruction phrasings, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 66.8 F1 (±1.3); UNSEEN (RE): 49.8 F1 (±3.8)",
            "baseline_performance": "SEEN (QA): 66.8 F1 (±1.3)",
            "performance_change": "-17.0 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, averaged over paraphrases, same inputs via QA4RE, 1000 examples.",
            "statistical_significance": null,
            "uuid": "e7455.17",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Flan-T5-11B @ TACREV",
            "name_full": "Flan-T5 (11B) on TACREV comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Flan-T5-11B shows nearly identical performance for TACREV when comparing QA vs RE instruction framings on the same inputs, suggesting dataset-dependent format sensitivity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-11B",
            "model_description": "Instruction-finetuned FLAN-T5 large model.",
            "model_size": "11B",
            "task_name": "TACREV (via QA4RE)",
            "task_description": "Relation extraction evaluated as QA vs RE with same examples.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs, three paraphrased instruction variants, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 51.5 F1 (±0.6); UNSEEN (RE): 51.3 F1 (±3.0)",
            "baseline_performance": "SEEN (QA): 51.5 F1 (±0.6)",
            "performance_change": "-0.2 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, identical inputs via QA4RE, averaged across instruction phrasings, 1000 examples.",
            "statistical_significance": null,
            "uuid": "e7455.18",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Flan-T5-11B @ SemEval",
            "name_full": "Flan-T5 (11B) on SemEval comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "Flan-T5-11B displays a large drop on SemEval when the instruction is switched from QA to RE, demonstrating strong sensitivity to instruction presentation on this dataset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-11B",
            "model_description": "Large FLAN-T5 instruction-finetuned model.",
            "model_size": "11B",
            "task_name": "SemEval (via QA4RE)",
            "task_description": "19-way relation classification converted to QA and RE formats with identical inputs.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs via QA4RE; three paraphrased instruction phrasings; zero-shot; SemEval is especially challenging (19 choices).",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 35.0 F1 (±0.1); UNSEEN (RE): 16.1 F1 (±3.0)",
            "baseline_performance": "SEEN (QA): 35.0 F1 (±0.1)",
            "performance_change": "-18.9 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, 1000 examples, averaged over paraphrases, same inputs via QA4RE.",
            "statistical_significance": null,
            "uuid": "e7455.19",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "WizardLM-13B @ TACRED",
            "name_full": "WizardLM (13B) on TACRED comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "WizardLM-13B (fine-tuned with Evol-Instruct-style complex instructions) shows performance degradation when TACRED inputs are presented as RE vs QA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "WizardLM-13B",
            "model_description": "LLaMA-based model fine-tuned using complex instruction rewrites (Evol-Instruct), intended to follow more complex instructions.",
            "model_size": "13B",
            "task_name": "TACRED (via QA4RE)",
            "task_description": "RE vs QA same-input evaluation.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs via QA4RE, three paraphrased phrasings, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 27.3 F1 (±4.1); UNSEEN (RE): 22.1 F1 (±1.2)",
            "baseline_performance": "SEEN (QA): 27.3 F1 (±4.1)",
            "performance_change": "-5.1 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, averaged over instruction phrasings, 1000 examples.",
            "statistical_significance": null,
            "uuid": "e7455.20",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "WizardLM-13B @ RETACRED",
            "name_full": "WizardLM (13B) on RETACRED comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "WizardLM-13B suffers a large drop on RETACRED when using RE-format instructions versus QA, reflecting format-dependent performance variability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "WizardLM-13B",
            "model_description": "Instruction-fine-tuned LLaMA variant using complex instruction rewriting.",
            "model_size": "13B",
            "task_name": "RETACRED (via QA4RE)",
            "task_description": "RE dataset evaluated as QA and RE with identical inputs.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs, three paraphrased instruction phrasings per format, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 48.2 F1 (±1.8); UNSEEN (RE): 27.6 F1 (±2.2)",
            "baseline_performance": "SEEN (QA): 48.2 F1 (±1.8)",
            "performance_change": "-20.6 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, same inputs via QA4RE, averaged across instruction phrasings.",
            "statistical_significance": null,
            "uuid": "e7455.21",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "WizardLM-13B @ TACREV",
            "name_full": "WizardLM (13B) on TACREV comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "WizardLM-13B shows a modest drop on TACREV when the instruction changes from QA to RE for identical inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "WizardLM-13B",
            "model_description": "LLaMA-based instruction-following model fine-tuned with complex instruction rewrites.",
            "model_size": "13B",
            "task_name": "TACREV (via QA4RE)",
            "task_description": "Relation extraction vs QA with same examples.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same input examples, three paraphrases per instruction type, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 27.0 F1 (±3.7); UNSEEN (RE): 22.7 F1 (±1.6)",
            "baseline_performance": "SEEN (QA): 27.0 F1 (±3.7)",
            "performance_change": "-4.3 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, averaged over paraphrases, same inputs via QA4RE.",
            "statistical_significance": null,
            "uuid": "e7455.22",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "WizardLM-13B @ SemEval",
            "name_full": "WizardLM (13B) on SemEval comparing QA (seen) vs RE (unseen) instruction formats",
            "brief_description": "WizardLM-13B shows a small absolute improvement on SemEval when framed as RE vs QA, though overall scores are low and dataset is challenging.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "WizardLM-13B",
            "model_description": "Instruction-following LLaMA variant fined-tuned using Evol-Instruct rewrites.",
            "model_size": "13B",
            "task_name": "SemEval (via QA4RE)",
            "task_description": "19-way relation classification evaluated as QA vs RE with same inputs.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction",
            "format_category": "prompt style",
            "format_details": "Same inputs via QA4RE, three paraphrases per instruction type, zero-shot, 1000 examples.",
            "performance_metric": "F1",
            "performance_value": "SEEN (QA): 5.1 F1 (±1.4); UNSEEN (RE): 6.7 F1 (±0.7)",
            "baseline_performance": "SEEN (QA): 5.1 F1 (±1.4)",
            "performance_change": "+1.6 absolute (RE vs QA)",
            "experimental_setting": "Zero-shot, averaged across instruction phrasings, same inputs.",
            "statistical_significance": null,
            "uuid": "e7455.23",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Flan-T5 scaling (80M→11B)",
            "name_full": "Flan-T5 series scaling analysis comparing robustness/performance across instruction formats",
            "brief_description": "Across Flan-T5 sizes (80M up to 11B) performance generally improves with size up to XLarge, and robustness to QA instructions is consistently better than to RE instructions; models under 1B sometimes show the reverse.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5 (various sizes)",
            "model_description": "Family of instruction-finetuned T5 models evaluated across multiple parameter counts (Small 80M → XXL 11B).",
            "model_size": "80M, &lt;1B, 1B, 3B, 11B (various)",
            "task_name": "TACRED / RETACRED / TACREV / SemEval (via QA4RE)",
            "task_description": "Relation extraction datasets evaluated as QA (seen) and RE (unseen) instructions using identical input examples to measure format sensitivity across model scale.",
            "problem_format": "Seen: QA instruction; Unseen: RE instruction (comparison across model sizes)",
            "format_category": "prompt style / model scale",
            "format_details": "Zero-shot evaluation using same inputs via QA4RE, three paraphrased instruction phrasings per format; robustness measured as standard deviation across paraphrases; analysis of performance trends from Small (80M) to XXL (11B).",
            "performance_metric": "F1 (and std across instruction phrasings as robustness)",
            "performance_value": "Qualitative: performance generally increases from Small→Large; improvements plateau/turn after XLarge; 3B and 11B do not give strictly proportional gains beyond threshold.",
            "baseline_performance": null,
            "performance_change": "Qualitative: robustness to QA &gt; RE across most sizes; in 20 comparisons only 3 (all &lt;1B) showed better robustness to RE than QA.",
            "experimental_setting": "Zero-shot, multiple model sizes tested (80M→11B), same-input QA4RE conversions, robustness quantified by std across paraphrases.",
            "statistical_significance": null,
            "uuid": "e7455.24",
            "source_info": {
                "paper_title": "Evaluating the Robustness to Instructions of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Aligning instruction tasks unlocks large language models as zero-shot relation extractors",
            "rating": 2,
            "sanitized_title": "aligning_instruction_tasks_unlocks_large_language_models_as_zeroshot_relation_extractors"
        },
        {
            "paper_title": "The flan collection: Designing data and methods for effective instruction tuning",
            "rating": 2,
            "sanitized_title": "the_flan_collection_designing_data_and_methods_for_effective_instruction_tuning"
        },
        {
            "paper_title": "Multitask prompted training enables zero-shot task generalization",
            "rating": 2,
            "sanitized_title": "multitask_prompted_training_enables_zeroshot_task_generalization"
        },
        {
            "paper_title": "T0: Multitask prompted training enables zero-shot task generalization",
            "rating": 1,
            "sanitized_title": "t0_multitask_prompted_training_enables_zeroshot_task_generalization"
        },
        {
            "paper_title": "PromptSource: An integrated development environment and repository for natural language prompts",
            "rating": 1,
            "sanitized_title": "promptsource_an_integrated_development_environment_and_repository_for_natural_language_prompts"
        }
    ],
    "cost": 0.025430499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating the Robustness to Instructions of Large Language Models
28 Aug 2023</p>
<p>Yuansheng Ni yuansheng.ni@zju.edu.cn 
Zhejiang University
The Ohio State University ♡ University of Zurich</p>
<p>Sichao Jiang 
Zhejiang University
The Ohio State University ♡ University of Zurich</p>
<p>Xinyu Wu 
Zhejiang University
The Ohio State University ♡ University of Zurich</p>
<p>Hui Shen 
Zhejiang University
The Ohio State University ♡ University of Zurich</p>
<p>Yuli Zhou yuli.zhou@uzh.ch 
Zhejiang University
The Ohio State University ♡ University of Zurich</p>
<p>Evaluating the Robustness to Instructions of Large Language Models
28 Aug 2023968FAD22124F248F36325AA0FC3A9A45arXiv:2308.14306v1[cs.CL]
Recently, Instruction fine-tuning has risen to prominence as a potential method for enhancing the zero-shot capabilities of Large Language Models (LLMs) on novel tasks.This technique has shown an exceptional ability to boost the performance of moderately sized LLMs, sometimes even reaching performance levels comparable to those of much larger model variants.The focus is on the robustness of instruction-tuned LLMs to seen and unseen tasks.We conducted an exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction datasets as case studies.We carried out a comprehensive evaluation of these instruction-following LLMs which have been tuned based on open-domain instructions and task-oriented instructions.The main discussion is their performance and robustness towards instructions.We have observed that in most cases, the model's performance in dealing with unfamiliar instructions tends to worsen significantly, and the robustness of the model for RE instructions deteriorates compared to QA.Further, we discovered that up until a certain parameter size threshold (3B), the performance of the FLAN-T5 model improves as the parameter count increases.The robustness of different scales of FLAN-T5 models to RE instruction is worse than the robustness to QA instruction.</p>
<p>Introduction</p>
<p>Large Language Models have established a strong presence in Natural Language Processing, largely due to their ability to adapt to new tasks through zero-or few-shot 'prompting' (Brown et al., 2020;Chowdhery et al., 2022).</p>
<p>Recent research suggests promising outcomes from fine-tuning these models via natural language instructions.This approach, known as 'instruction tuning,' enhances the performance of LLMs in zero-and few-shot settings quite substantially in some instances, particularly for 'mid-sized' models (Chung et al., 2022b;Ouyang et al., 2022b).For example, in some benchmarks, the Flan-T5-XL model tuned with instruction, with 3 billion parameters, outperforms the GPT-3 model, which has 175 billion parameters, despite its significantly smaller size (Chung et al., 2022b).</p>
<p>Moreover, following the fine-tuning of the LLaMa-7B model on the Alpaca instruction set with a large corpus, it exceeds the performance of the GPT-3 model on a number of NLP benchmarks (Touvron et al., 2023).These empirical successes have spurred efforts to compile instructionenhanced task collections for meta-learning (Wang et al., 2022a;Wei et al., 2021) and research into improving instruction-tuning (Longpre et al., 2023;Xu et al., 2022;Sanh et al., 2021;Lou et al., 2023).</p>
<p>In this work, we investigate how robust instruction-tuned models are.More specifically, we ask: How sensitive are instruction-tuned LMs to shifts in instruction phrasings at test time?This is particularly important given that the primary motivation of instruction tuning is to facilitate zero-shot adaptation via natural language instruction.If models are overly sensitive to the particular phrasing of a task instruction it may greatly limit their utility in practice.</p>
<p>Previous research has already established that large models fine-tuned according to instructions exhibit reduced performance when handling phrasings that are not encountered during training.However, these efforts did not fix the input examples between the seen and unseen tasks.</p>
<p>To address this issue, we propose evaluating the instruction robustness of LLMs with aligned unseen and seen tasks in the same input examples.</p>
<p>More specifically, we find several task instruction datasets aligned by LLM-QA4RE (Zhang et al., 2023).With the same input example, they reformulate an unseen task, relation extraction (RE) to a seen task, question answering (QA), achieving a significant performance boost across the Flan-T5 (Longpre et al., 2023) and GPT-3.5 series (Ouyang et al., 2022a).</p>
<p>Evaluating instruction-tuned LLMs with the same input examples while different task formats and corresponding instructions could alleviate the bias of different input examples, leading to a more fair and accurate robustness evaluation across instruction-tuned LLM models.As seen in Figure 1, when dealing with the same input example, the LLM output for the RE instruction could be incorrect.</p>
<p>We then assess the performance of LLMs finetuned on the instructions collection when given these seen or unseen task instructions.We observed a significant decrease in accuracy when employing unseen task instructions in zero-shot applications.The detailed contributions of this paper are as follows:</p>
<p>• We thoroughly analyze the instruction robustness of instruction-tuned LLMs across six models with paraphrased instructions using four datasets.We observe that in the majority of settings, the model's performance in handling unseen instructions tends to significantly deteriorate.Furthermore, the robustness of the model is significantly reduced after exposure to the paraphrased unseen instructions.</p>
<p>• We delved into the impact that variations in the parameter size of FLAN-T5 have on its overall performance.We discovered that up until a certain parameter size threshold (3B), the performance of the FLAN-T5 model improves as the parameter count increases.The robustness of different scales of FLAN-T5 models to RE instruction is worse than the robustness to RE instruction.</p>
<p>Related Work</p>
<p>Large Language Model.In recent years, the field of NLP has experienced a paradigm shift with the advent of LLMs (Ouyang et al., 2022a;Longpre et al., 2023;Touvron et al., 2023;Li et al., 2023;Sun et al., 2023).These language models characterized by their large sizes, represent a breakthrough in the ability to process and generate human-like text.The evolution of large-language models can be traced back to the introduction of transformer architectures.These architectures, notably the paper "Attention is All You Need" (Vaswani et al., 2017), laid the foundation for large language models like GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022) and LLaMA (Touvron et al., 2023).</p>
<p>The global modeling capability and excellent convergence ability brought about by the attention mechanism have enabled the emergence of LLMs with parameter counts as high as tens to hundreds of billions.</p>
<p>Instruction Tuning.The process of fine-tuning large language models has remained a promising area of research.Prior to the emergence of modern prompting and instructing strategies, multi-tasking fine-tuning (Liu et al., 2019;Khashabi et al., 2020;Sun et al., 2021;Ma et al., 2021) show improvements in performance by leveraging data from multiple related tasks.Most recently, fine-tuning an extensive collection of diverse instructions that employ a straightforward task description has been shown to improve LLM zero-shot performance on unseen tasks (Lou et al., 2023;Mishra et al., 2021;Sanh et al., 2021;Li et al., 2023).An example of this is T0 (Sanh et al., 2021), which is fine-tuned on instruction datasets, with a specific emphasis on assessing performance in zero-shot setting and the ability to withstand variations in phrasing, and comes with over 2000 prompts for diverse datasets via PromptSource (Bach et al., 2022).A similar collection of instructions is Super-NaturalInstructions (Wang et al., 2022b), which compiled over 1600 diverse NLP tasks and their instructions.Datasets encompassing a wide array of tasks provide the opportunity to train models that follow instructions and facilitate a thorough evaluation of their capacity for broad task adaptation.</p>
<p>3 Evaluating the Robustness</p>
<p>Datasets and Models</p>
<p>Studies have shown that LLMs can degrade performance when dealing with phrasings that are not encountered during training, but these efforts do not have a fixed input example between seen and unseen tasks during evaluation.To solve this problem, we propose an evaluation scheme that uses the same input example to evaluate the robustness of LLMs to instructions.We used 4 RE datasets aligned with LLM-QA4RE, and the unprocessed datasets were derived from: (1) TACRED (Zhang et al., 2017), ( 2) RETACRED (Stoica et al., 2021), (3) TACREV (Alt et al., 2020), and (4) SemEval 2010 Task 8 (SemEval for brevity) (Hendrickx et al., 2010).</p>
<p>QA4RE is a framework proposed by Zhang et al. (2023) to improve the performance of LLMs on the unseen task of RE by reconstructing RE into the QA form of seen.To evaluate the LLM instruction more comprehensively, we provided three new phrasings for the QA and RE instruction, respectively, on the data set processed by the QA4RE framework, as shown in Table 1.</p>
<p>We selected six fine-tuned LLMs Flan-T5-3B, Alpaca-7B, Vicuna-7B, T0++ 11B, Flan-T5-11B, and WizardLM-13B for evaluation.Instruction in the form of QA is a Seen task for these LLMs, but RE is not, then we can evaluate the robustness of the model to the instruction on the processed data set.In the data set, we evaluate 1000 input examples of the same task using the same Instructions, which are generic (including instructions that go through phases in Table 1)</p>
<p>Result</p>
<p>Our experimental results on four datasets can be found in Table 2, and the comprehensive performance of six LLMs on QA and RE instructions is shown in Figure 2. We use the F1 Score to mea- sure the model's performance and use the standard deviation of the model's performance on different instructions to measure its robustness; the smaller the standard deviation, the stronger the model's resistance to interference from different instructions, indicating better model robustness.We have the following observations from our results:</p>
<p>(1) All models perform worse on RE than QA, indicative of the impact unseen instructions have on fine-tuned LLMs, lowering performance on the same input example (notably a reduction of 9.9 for Flan-T5-11B).This is because all six models have been instruction-tuned for the QA task, allowing them to better comprehend our instructions and answer questions, resulting in better performance than the non-instruction-tuned RE task.The results in Figure 2 also indirectly prove the effectiveness and applicability of the QA4RE framework (Zhang et al., 2023), which effectively improves model performance on RE.</p>
<p>(2) The models Flan-T5-3B, Flan-T5-11B, and T0++ all show worse robustness on RE instructions than QA instructions across all datasets (their standard deviation is higher on RE than QA).</p>
<p>(3) A different pattern emerges when assessing the performance of Alpaca-7B, Vicuna-7B, and WizardLM on RE instructions.Alpaca-7B only showed decreased robustness on the TACRED dataset, but increased robustness on the RETA-CRED, TACREV, and SemEval datasets.Vicuna-7B had diminished robustness on the TACRED and TACREV datasets but improved on the others.WizardLM only improved on RETACRED.It may seem as though these results contradict our observations in point ( 2), but they do not.Vicuna (Zheng et al., 2023) was fine-tuned using user-shared conversations.Alpaca (Cui et al., Seen Task QA Unseen Task RE</p>
<p>Instruction</p>
<p>Which option can be inferred from the given Sentence?Given a Sentence, and two Entities within the Sentence, classify the relationship between the two Entities based on the provided Sentence.</p>
<p>All possible Relationships are listed below:</p>
<p>You are given a multi-choice question answering task, given the question and multiple answer options, select the option that can be reasonably derived from the sentence's context and content.</p>
<p>Involving a given sentence and two entities within it, your goal is to identify the relationship between the two entities using the provided sentence.</p>
<p>Choose from the set of relationships provided below.</p>
<p>Phrasing</p>
<p>Select the alternative that can be logically extrapolated from the information given in the sentence.</p>
<p>You'll be working with a sentence that contains two entities.Your objective is to categorize the relationship between these entities using the provided sentence.Refer to the list of potential relationships provided.</p>
<p>Pinpoint the correct choice that can be inferred through logical analysis of the given statement.</p>
<p>The challenge is to establish the relationship between two entities within a given sentence.Your task is to classify this relationship based on the sentence provided.Explore the list of possible relationships below.2023) models mainly expanded and optimized Chinese vocabulary beyond the original Llama-2.Wiz-ardLM (Xu et al., 2023) employed Evol-Instruct to rewrite them into more complex instructions to fine-tune LLaMA.</p>
<p>We noticed that Alpaca and Vicuna received little or no training on NLP foundational tasks.While WizardLM employed more complex instructions, it never encountered RE instruction-related tasks.Moreover, our QA datasets came from the QA4RE framework, essentially exerting an influence connected to RE, only changing the rendition of the instruction.Hence, we find these three models on four datasets all scored lowly, with, for instance, Alpaca's highest average score on RE instructions not exceeding 20-basically equivalent to random selection.In conclusion, the increased robustness of these models when handling RE instructions should not be inferred as the end point of our discussion.</p>
<p>(4) Though the results of Alpaca-7B, Vicuna-7B, and WizardLM's robustness tests on instructions may not be taken into account, some interesting discoveries are still apparent.Firstly, all three models' performance dropped when facing RE instructions on all datasets except the SemEval one (we will handle this case later and will not consider it here), exposing the influence of RE instructions on models.Secondly, the performance of WizardLM, whether on QA or RE instructions, is obviously superior to Alpaca and Vicuna, which is related to its fine-tuned content.</p>
<p>(5) Similarly, Flan-T5 (Chung et al., 2022a), and T0++ (Sanh et al., 2022) models were all finetuned on a pre-trained language model on a multitask mixture covering various NLP tasks.Therefore, these three models exhibited superior performance and robustness on our dataset and instructions, compared to Alpaca-7b, Vicuna-7B, and Wiz-ardLM.Moreover, the performance of T0pp was significantly worse than that of the Flan-T5 models.</p>
<p>(6) Finally, the SemEval dataset poses a significant challenge to all models, even causing an apparent decline in the performance of the bestperforming model, Flan-T5.This is because there are no type constraints on this dataset, and the RE questions within the QA4RE framework have been transformed into a question-answering task with 19 choices.Both the 19-choice QA and original RE tasks on this dataset are extremely challenging for LLMs.The results on Alpaca-7B, Vicuna-7B, WizardLM, and T0++ could not substantiate our conclusion.However, fortunately, although the performance of both the 3B and 11B models of Flan-T5 on this dataset noticeably decreased, they still scored 35.6 and 35.0 respectively on QA instructions, surpassing the performance of other models on QA outside of the SemEval dataset.As a result, we still analyzed the robustness of Flan-T5-3B and 11B on SemEval against various instructions.It was found that the Flan-T5 models display strong robustness, with standard deviations of 0.8 and 0.1, against QA instructions on SemEval.However, they performed rather poorly against RE instructions.</p>
<p>Scaling</p>
<p>From 3.2, we can see that the Flan-T5-3B and 11B models have relatively good performance on both QA and RE compared to other models, which piques our interest in the Flan-T5 series.We are curious if the size of the model could have an impact on its robustness and performance with respect to instructions.Therefore, we selected the Flan-T5 model scale ranging from small (80M parameters) to XXL (11B) and repeated the experiment as described in 3.1.The performance is as shown in Figure 3 and the model's robustness to instructions is presented in Table 3. From the analysis of the graphs and tables, we can draw the following conclusions:</p>
<p>(1) Indeed, the performance of the Flan-T5 se-ries models on these four datasets increases from the Small to Large phase with the increase of the model's size.However, the performance under the 3B and 11B scale varies.Let's take the most representative and challenging dataset-SemEval, we can clearly see that the performance of the models improves with the increase in size, but there is a turning point after the XLarge size.Therefore, we believe that unless the parameter volume reaches a certain size, the performance on QA and RE instructions will increase along with the scale of the model.(2) The robustness of QA instructions of Flan-T5 models of different sizes is better than that of RE instructions.Out of 20 sets of comparative data, only 3 sets show better robustness to RE instructions than QA, and they occur in models with parameters less than 1B (small, base, and large models, each occurring once).Combined with the performance shown in Figure 3, we can see that the performance of the three models with parameter volume less than 1B is not very good.Therefore, the three cases where the robustness of RE instructions is better than QA will not affect the final conclusion.</p>
<p>Conclusions</p>
<p>In this study, we conducted a comprehensive evaluation of the robustness and performance of six LLMs to QA and RE instructions, and we utilized four RE tasks: TACRED, RETACRED, TACREV, and SemEval for the evaluation process.</p>
<p>A major observation from our evaluation is that all of the LLMs evaluated had lower performance whilst handling RE instructions compared to handling QA instructions.This suggests that unseen instructions can significantly impact a fine-tuned LLM's performance on the same input example.More specifically, we found out that Flan-T5-3B, Flan-T5-11B, and T0++ models display worse robustness for RE instructions than for QA instructions across all datasets considered.However, this pattern was not uniform across all models.Alpaca-7B, Vicuna-7B, and WizardLM had mixed robustness scores across the datasets, reflecting that a model's training and fine-tuning process can influence its robustness.For these models, the robustness fluctuated depending on the dataset, indicating that the fine-tuning content related to specific tasks played a crucial role in this variance.</p>
<p>In comparing the model scales, the Flan-T5 series was observed to have relatively strong performance and thus was further analyzed.Increasing the size of the model improved performance on both QA and RE instructions, but there existed a limit beyond which the performance did not significantly improve -namely at the XLarge scale.A further increase of model parameters from 1B to 3B and 11B did not yield a proportional increase in performance.In terms of robustness, models with a parameter volume less than 1B demonstrated better robustness for RE instructions than QA instructions in some cases, but this did not deviate from the overall trend.</p>
<p>To summarize, our evaluation provides insight into the performance and robustness of LLMs under different instructions and across various datasets.We unveil the impact that unseen instructions and model scale can have on performance and robustness.This work hence provides a comprehensive guide for future model development and evaluation.</p>
<p>Limitations</p>
<p>There are certain limitations to our study.For instance, our evaluation only focuses on "medium" models, with less than 200 billion parameters.We cannot confirm whether our findings apply to larger, instruction-tuned models.Additionally, for the performance and robustness of the model scale in QA and RE instructions, we only evaluated the Flan-T5 series and did not evaluate models of different scales within the same series.</p>
<p>Figure 1 :
1
Figure 1: In the same input example, LLMs give the incorrect answer to RE instruction.</p>
<p>Figure 2 :
2
Figure 2: Visualizes the average performance of the model using instructions for both seen and unseen tasks.</p>
<p>Figure 3 :
3
Figure 3: Performance on instruction by different scales of Flan-T5 models</p>
<p>Table 1 :
1
QA and RE instruction after the phrase.
ModelTACRED Avg. Std.RETACRED Avg. Std.TACREV Avg. Std.SEMEVAL Avg. Std.Flan-T5-3BSEEN (QA)51.6 (±1.8)65.1 (±1.0)53.7 (±2.3) 35.6 (±0.8)UNSEEN (RE)55.7 (±2.1)50.6 (±2.4)56.8 (±2.6) 30.9 (±1.4)Performance ∆↑ 4.1↓ 14.5↑ 3.2↓ 4.7Alpaca-7BSEEN (QA)25.5 (±1.7)42.7 (±4.0)26.5 (±1.8)0.6 (±2.2)UNSEEN (RE)19.0 (±2.4) 18.5 (±2.1) 20.0 (±0.9)5.9 (±0.4)Performance ∆↓ 6.5↓ 24.2↓ 6.6↑ 5.3Vicuna-7BSEEN (QA)31.3 (±1.7)38.2 (±3.8)31.9 (±1.7)4.7 (±0.6)UNSEEN (RE)18.2 (±4.1) 36.2 (±2.0) 19.5 (±2.4)5.4 (±0.4)Performance ∆↓ 13.1↓ 2.0↓ 12.4↑ 0.7T0++ 11BSEEN (QA)33.5 (±0.4)47.0 (±0.7)32.1 (±0.2)2.6 (±0.4)UNSEEN (RE)27.1 (±1.4) 31.5 (±2.0) 25.7 (±2.1) 17.6 (±0.6)Performance ∆↓ 6.4↓ 15.5↓ 6.4↑ 15.0Flan-T5-11BSEEN (QA)53.2 (±1.3)66.8 (±1.3)51.5 (±0.6) 35.0 (±0.1)UNSEEN (RE)49.1 (±2.4) 49.8 (±3.8) 51.3 (±3.0) 16.1 (±3.0)Performance ∆↓ 4.1↓ 17.0↓ 0.2↓ 18.9WizardLM-13BSEEN (QA)27.3 (±4.1)48.2 (±1.8)27.0 (±3.7)5.1 (±1.4)UNSEEN (RE)22.1 (±1.2) 27.6 (±2.2) 22.7 (±1.6)6.7 (±0.7)Performance ∆↓ 5.1↓ 20.6↓ 4.3↑ 1.6</p>
<p>Table 2 :
2
Evaluation experiment results of 6 LLMs on QA instructions and RE instructions, where the Avg value represents the average performance of the model on the instructions for this dataset, and Std is the standard deviation of the corresponding F1 Score, used to reflect the model's robustness to instructions.</p>
<p>Table 3 :
3
Robustness to instruction by different scales of Flan-T5 models</p>
<p>TACRED revisited: A thorough evaluation of the TACRED relation extraction task. Christoph Alt, Aleksandra Gabryszak, Leonhard Hennig, 10.18653/v1/2020.acl-main.142Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Victor Stephen H Bach, Zheng-Xin Sanh, Albert Yong, Colin Webson, Raffel, Abheesht Nayak, Taewoon Sharma, Saiful Kim, Thibault Bari, Fevry, arXiv:2202.01279Promptsource: An integrated development environment and repository for natural language prompts. 2022arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Sharan Chowdhery, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, 10.48550/ARXIV.2210.114162022aScaling instruction-finetuned language models</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, arXiv:2210.11416Siddhartha Brahma, et al. 2022b. Scaling instruction-finetuned language models. arXiv preprint</p>
<p>Yiming Cui, Ziqing Yang, Xin Yao, arXiv:2304.08177Efficient and effective text encoding for chinese llama and alpaca. 2023arXiv preprint</p>
<p>SemEval-2010 task 8: Multiway classification of semantic relations between pairs of nominals. Iris Hendrickx, Nam Su, Zornitsa Kim, Preslav Kozareva, Nakov, Ó Diarmuid, Sebastian Séaghdha, Marco Padó, Lorenza Pennacchiotti, Stan Romano, Szpakowicz, Proceedings of the 5th International Workshop on Semantic Evaluation. the 5th International Workshop on Semantic EvaluationUppsala, Sweden2010Association for Computational Linguistics</p>
<p>UNIFIEDQA: Crossing format boundaries with a single QA system. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi, 10.18653/v1/2020.findings-emnlp.171Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge. Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, You Zhang, Cureus. 6152023</p>
<p>Multi-task deep neural networks for natural language understanding. Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao, 10.18653/v1/P19-1441Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Barret Quoc V Le, Jason Zoph, Wei, arXiv:2301.13688The flan collection: Designing data and methods for effective instruction tuning. 2023arXiv preprint</p>
<p>Is prompt all you need? no. a comprehensive and broader view of instruction learning. Renze Lou, Kai Zhang, Wenpeng Yin, arXiv:2303.104752023arXiv preprint</p>
<p>GradTS: A gradient-based automatic auxiliary task selection method based on transformer networks. Weicheng Ma, Renze Lou, Kai Zhang, Lili Wang, Soroush Vosoughi, 10.18653/v1/2021.emnlp-main.455Proceedings of EMNLP 2021. EMNLP 2021Dominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Swaroop Mishra, Daniel Khashabi, arXiv:2104.08773Chitta Baral, and Hannaneh Hajishirzi. 2021. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, 10.48550/arXiv.2203.02155CoRR, abs/2203.02155Jan Leike, and Ryan Lowe. 2022a</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, arXiv:2203.021552022barXiv preprint</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Wang, International Conference on Learning Representations. Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason , Alan Fries, Ryan Teehan, Le Teven, Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush2022</p>
<p>Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, arXiv:2110.08207arXiv preprint</p>
<p>Re-tacred: Addressing shortcomings of the TACRED dataset. George Stoica, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9. AAAI Press2021. 2021Emmanouil Antonios Platanios, and Barnabás Póczos</p>
<p>Rpbert: A text-image relation propagation-based BERT model for multimodal NER. Lin Sun, Jiquan Wang, Kai Zhang, Yindu Su, Fangsheng Weng, Proceedings of AAAI 2021. AAAI 2021AAAI Press2021</p>
<p>Yuxuan Sun, Chenglu Zhu, Sunyi Zheng, Kai Zhang, Zhongyi Shui, Xiaoxuan Yu, Yizhi Zhao, Honglin Li, Yunlong Zhang, Ruojia Zhao, Xinheng Lyu, Lin Yang, Pathasst: Redefining pathology through generative foundation ai assistant for pathology. 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Benchmarking generalization via in-context instructions on 1,600+ language tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, arXiv:2204.077052022aarXiv preprint</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, arXiv:2204.07705Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprintet al. 2022b. Super-naturalinstructions</p>
<p>Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, arXiv:2109.01652arXiv preprint</p>
<p>Wizardlm: Empowering large language models to follow complex instructions. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, 2023</p>
<p>Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. Zhiyang Xu, Ying Shen, Lifu Huang, arXiv:2212.107732022arXiv preprint</p>
<p>Aligning instruction tasks unlocks large language models as zero-shot relation extractors. Kai Zhang, Bernal Jimenez Gutierrez, Yu Su, Findings of ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Position-aware attention and supervised data improve slot filling. Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, Christopher D Manning, 10.18653/v1/d17-1004Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, Denmark2017. 2017. September 9-11, 2017Association for Computational Linguistics</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. </p>            </div>
        </div>

    </div>
</body>
</html>
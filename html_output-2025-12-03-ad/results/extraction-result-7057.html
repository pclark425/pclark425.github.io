<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7057 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7057</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7057</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-75c19f3249f644f5cb2182282fc117c089fd3f65</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/75c19f3249f644f5cb2182282fc117c089fd3f65" target="_blank">The Expressive Power of Transformers with Chain of Thought</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This paper aims to demonstrate how transformers’ reasoning can be improved by allowing them to use a “chain of thought” or “scratchpad”, i.e., generate and condition on a sequence of intermediate tokens before answering.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7057.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7057.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/architectural technique that lets autoregressive transformers generate and condition on intermediate tokens (a 'chain of thought' or 'scratchpad') before producing a final answer; modeled in this paper as allowing t(n) additional decoder steps and analyzed as a computational resource.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought (approach)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting/usage pattern for autoregressive (decoder-only) transformers in which the model emits intermediate tokens (reasoning steps) before the final output; formalized here as t(n) additional decoding steps on a decoder-only transformer with attention over all previous states.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>decoder-only transformer with intermediate decoding (CoT(t(n)))</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>chain-of-thought / scratchpad (intermediate token generation modeled as t(n) decoding steps)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>sequential logical reasoning tasks (automata simulation, graph connectivity, modular arithmetic, linear equalities, formal language recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Theoretical comparison: permitting t(n) decoding steps strictly increases expressivity vs. zero-step transformers — e.g., O(log n) steps expand from TC^0 to L, O(n) steps enable simulating automata/recognizing all regular languages (with projected pre-norm), polynomial steps characterize P.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The amount of intermediate generation is a computational resource: O(log n) steps only marginally increase power (to L), O(n) steps (with projected pre-norm and strict causal saturated attention) let decoders simulate automata and recognize all regular languages, and polynomial steps make them equivalent to P; chain-of-thought can thus in principle overcome many transformer limitations but requires sufficiently many steps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>All results are theoretical; constructions assume log-precision, projected pre-norm (or multi-pre-norm), and (for lower bounds) saturated attention and strict causal masking. The paper does not provide empirical performance numbers nor guarantees about learnability in practice.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7057.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7057.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scratchpad (intermediate computation memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical variant of chain-of-thought where the model writes intermediate computation tokens (a scratchpad) that it can attend to later; cited as an empirically effective way to improve sequential reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show your work: Scratchpads for intermediate computation with language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Scratchpad (approach)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An approach that augments autoregressive generation with an explicit sequence of intermediate tokens (scratchpad) that the model conditions on for downstream computation; treated in this paper as equivalent to allowing intermediate decoding steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>decoder-only transformer with scratchpad (intermediate outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>scratchpad / intermediate token generation</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>sequential logical/computational tasks (examples discussed: modular arithmetic, automata simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Conceptual/theoretical: scratchpad (intermediate decoding) can increase expressive power when allowed for sufficiently many steps; no empirical deltas reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Scratchpad-style intermediate generation can be formalized as t(n) decoding steps and is a key mechanism by which transformers can simulate recurrent computation; the number of allowed steps determines the class of languages recognizable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No empirical evaluation; theoretical constructions assume specific architectural features (projected pre-norm, saturated attention).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7057.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7057.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT(t(n))</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoder-only transformers with t(n) intermediate decoding steps</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The formal class CoT(t(n)) studied in the paper: languages recognized by a decoder-only transformer that reads the n input tokens and then performs t(n) generated decoding steps before producing an accept/reject token.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT(t(n)) theoretical model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A formal decoder-only transformer model (h heads, d layers, model dim m) with log precision that consumes n input tokens then generates t(n) intermediate tokens; attention can view all prior hidden states; lower-bound constructions assume projected pre-norm, saturated attention, and strict causal masking.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>theoretical decoder-only transformer (with projected pre-norm, strict causal attention option)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>explicit intermediate decoding steps as computational resource (t(n))</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formal language recognition and sequential computation (regular languages, automata, Turing machine simulation, directed graph connectivity, linear equalities, context-free recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Theoretical: CoT(0) (no intermediate steps) is contained in uniform TC^0; CoT(O(log n)) ⊆ L; CoT(O(n)) contains TIME(n) (can simulate automata) and is contained in SPACE(n) and ~TIME(n^2 + n^2); CoT(polynomial) exactly equals P under given model assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Formal characterization linking number of decoding steps to standard complexity classes: TIME(t(n)) ⊆ CoT(t(n)) ⊆ SPACE(t(n)+log n) ⊆ ~TIME(t(n)^2 + n^2); linear steps add the ability to simulate automata, polynomial steps make the class P.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lower-bound simulations require architectural generalizations (projected pre-norm/multi-pre-norm), log precision, and idealized/saturated attention; practical LLMs may not meet these assumptions and learnability of the constructions is unaddressed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7057.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7057.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>projected pre-norm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Projected pre-norm (generalized pre-norm)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An architectural generalization where each transformer sublayer applies a learned linear projection before layer normalization, enabling selective normalization over subspaces of the hidden state.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Projected pre-norm (architecture variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Each sublayer applies a linear projection M to the input vector then applies layer_norm(M v), allowing the layer to focus normalization on a projected subset of dimensions; multi-pre-norm (multiple projections) can be simulated by repeated projected pre-norm layers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer normalization variant (pre-norm generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>used in theoretical constructions to enable simulation of automata and Turing machines</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Used in lower-bound constructions; without projected pre-norm the provided simulations (Theorems 1 and 2) may not hold as stated.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Projected pre-norm is an enabling architectural assumption for the paper's lower-bound constructions (simulating automata and bounded-time Turing machines).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>This is a theoretical modification; the paper notes it is a 'slight generalization' and suggests investigating incorporation into practical models, but does not provide empirical evidence.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7057.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7057.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>layer-norm hash</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Layer-norm hash</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanism introduced in this paper that uses layer normalization on a crafted 4-vector (x,y,-x,-y) to produce a scale-invariant unit vector representation enabling exact-equality retrieval across positions under uniform attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Layer-norm hash (mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Compute phi(x,y) = layer_norm(x,y,-x,-y); using phi(x/i,1/i) produces a representation invariant to position denominator i, enabling attention-based equality tests and retrieval of the most-recent write to a simulated tape cell.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>attention-compatible retrieval primitive implemented with layer-norm and feedforward layers</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>enables exact-match retrieval and rightmost retrieval in transformer attention with uniform scores; used inside constructions that simulate automata and Turing machines</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>memory retrieval for formal computation simulation (tape cell reads/writes)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Provides a retrieval primitive not previously formalized in this exact form; used to overcome scaling/denominator mismatches in uniform-attention algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Layer-norm hash yields scale-invariant keys so equality checks succeed across positions and enables efficient simulated-memory retrieval via attention; essential to the paper's decoder-only TM simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on precise behavior of layer-norm/RMS-norm and on the model's ability to compute the required projections and concatenations; implementation assumptions (log precision, projected pre-norm) are necessary in the theoretical proofs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7057.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7057.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>saturated attention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Saturated attention (averaging hard attention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An idealized attention model used in lower-bound constructions where per-head attention scores are either 0 or an equal weight 1/v over a selected subset of positions (includes uniform attention as a special case).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Saturated attention (idealized)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A theoretical attention abstraction in which attention per head selects a subset of positions and assigns equal nonzero mass 1/v to them (or zero elsewhere), enabling exact averaging and hard selection behaviors in proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>idealized attention model used in proofs</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>used to implement retrieval and simulation algorithms in theoretical constructions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Assumed for lower bounds; standard continuous softmax attention may only approximate this behavior in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Assuming saturated attention simplifies constructions and is standard in prior theoretical transformer work for proving lower bounds/lifting capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>This is an idealization that may not match learned attention precisely; practical models may not achieve exact saturated attention.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7057.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7057.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>strict causal masking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Strict causal masking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A slight variant of causal attention assumed in some constructions where, at position i, attention can view positions up to i-1 but not the current token i (i.e., the current token is excluded from its own attention keys).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Strict causal masking (attention constraint)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Attention formulation where c(i) = i-1 for decoding positions (cannot attend to current column), required for some decoder-only simulations (notably Theorem 2, TM simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>attention masking variant</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>facilitates correct retrieval and tie-breaking in Turing machine simulation constructions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>A slight nonstandard assumption compared to typical causal attention (which can attend up to and including current token); required to avoid self-observation in certain proof steps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used as an architectural assumption enabling the decoder-only transformer to simulate Turing machine steps correctly with intermediate decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Nonstandard relative to most practical transformer implementations; implications for learnability or empirical performance are not addressed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7057.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7057.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4/ChatGPT (mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT and GPT-4 (empirical transformer LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large autoregressive transformer-based language models referenced in the paper as empirically struggling on certain sequential reasoning problems inspired by theoretical limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How language model hallucinations can snowball.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large-scale transformer-based autoregressive language models (referenced generically; no architecture/training specifics provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (large autoregressive LM)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>empirical sequential reasoning tasks cited (e.g., graph connectivity, depth-dependent compositional tasks, matrix equalities)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited empirical findings: (1) Some reasoning problems inspired by theoretical limits cannot be solved by ChatGPT/GPT-4 (Zhang et al., 2023); (2) GPT-4's reasoning performance negatively correlates with depth of the problem's computation graph (Dziri et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No quantitative performance numbers or benchmarks are reported in this paper; claims are citations to empirical studies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7057.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7057.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feng2023 (CoT modular arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feng et al., 2023: chain-of-thought enabling modular arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent theoretical result (cited) showing that chain-of-thought allows transformers to solve a specific modular-arithmetic problem that they likely cannot solve without intermediate generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards revealing the mystery behind chain of thought: A theoretical perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Feng et al. (2023) result</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior theoretical work demonstrating a concrete example where chain-of-thought enables a transformer to perform a computation (modular arithmetic) infeasible without intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>theoretical transformer constructions with CoT</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>chain-of-thought / intermediate decoding</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>modular arithmetic (theoretical task showing CoT advantage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Shows existence of tasks solvable with CoT but likely unsolvable without it (qualitative/theoretical demonstration).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Provides motivating prior evidence that CoT can enable computations beyond zero-step transformers; cited to motivate the broader theoretical study in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Specific to a constructed modular arithmetic problem; does not provide broad empirical evaluation across standard logical reasoning benchmarks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models. <em>(Rating: 2)</em></li>
                <li>Towards revealing the mystery behind chain of thought: A theoretical perspective. <em>(Rating: 2)</em></li>
                <li>Attention is Turing complete. <em>(Rating: 2)</em></li>
                <li>How language model hallucinations can snowball. <em>(Rating: 1)</em></li>
                <li>Faith and fate: Limits of transformers on compositionality. <em>(Rating: 1)</em></li>
                <li>The parallelism tradeoff: Limitations of log-precision transformers. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7057",
    "paper_id": "paper-75c19f3249f644f5cb2182282fc117c089fd3f65",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting/architectural technique that lets autoregressive transformers generate and condition on intermediate tokens (a 'chain of thought' or 'scratchpad') before producing a final answer; modeled in this paper as allowing t(n) additional decoder steps and analyzed as a computational resource.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought (approach)",
            "model_description": "Prompting/usage pattern for autoregressive (decoder-only) transformers in which the model emits intermediate tokens (reasoning steps) before the final output; formalized here as t(n) additional decoding steps on a decoder-only transformer with attention over all previous states.",
            "model_size": null,
            "architecture_type": "decoder-only transformer with intermediate decoding (CoT(t(n)))",
            "training_data": null,
            "reasoning_method": "chain-of-thought / scratchpad (intermediate token generation modeled as t(n) decoding steps)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "sequential logical reasoning tasks (automata simulation, graph connectivity, modular arithmetic, linear equalities, formal language recognition)",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "Theoretical comparison: permitting t(n) decoding steps strictly increases expressivity vs. zero-step transformers — e.g., O(log n) steps expand from TC^0 to L, O(n) steps enable simulating automata/recognizing all regular languages (with projected pre-norm), polynomial steps characterize P.",
            "key_findings": "The amount of intermediate generation is a computational resource: O(log n) steps only marginally increase power (to L), O(n) steps (with projected pre-norm and strict causal saturated attention) let decoders simulate automata and recognize all regular languages, and polynomial steps make them equivalent to P; chain-of-thought can thus in principle overcome many transformer limitations but requires sufficiently many steps.",
            "limitations": "All results are theoretical; constructions assume log-precision, projected pre-norm (or multi-pre-norm), and (for lower bounds) saturated attention and strict causal masking. The paper does not provide empirical performance numbers nor guarantees about learnability in practice.",
            "uuid": "e7057.0"
        },
        {
            "name_short": "Scratchpad",
            "name_full": "Scratchpad (intermediate computation memory)",
            "brief_description": "A practical variant of chain-of-thought where the model writes intermediate computation tokens (a scratchpad) that it can attend to later; cited as an empirically effective way to improve sequential reasoning.",
            "citation_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "mention_or_use": "use",
            "model_name": "Scratchpad (approach)",
            "model_description": "An approach that augments autoregressive generation with an explicit sequence of intermediate tokens (scratchpad) that the model conditions on for downstream computation; treated in this paper as equivalent to allowing intermediate decoding steps.",
            "model_size": null,
            "architecture_type": "decoder-only transformer with scratchpad (intermediate outputs)",
            "training_data": null,
            "reasoning_method": "scratchpad / intermediate token generation",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "sequential logical/computational tasks (examples discussed: modular arithmetic, automata simulation)",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "Conceptual/theoretical: scratchpad (intermediate decoding) can increase expressive power when allowed for sufficiently many steps; no empirical deltas reported in this paper.",
            "key_findings": "Scratchpad-style intermediate generation can be formalized as t(n) decoding steps and is a key mechanism by which transformers can simulate recurrent computation; the number of allowed steps determines the class of languages recognizable.",
            "limitations": "No empirical evaluation; theoretical constructions assume specific architectural features (projected pre-norm, saturated attention).",
            "uuid": "e7057.1"
        },
        {
            "name_short": "CoT(t(n))",
            "name_full": "Decoder-only transformers with t(n) intermediate decoding steps",
            "brief_description": "The formal class CoT(t(n)) studied in the paper: languages recognized by a decoder-only transformer that reads the n input tokens and then performs t(n) generated decoding steps before producing an accept/reject token.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CoT(t(n)) theoretical model",
            "model_description": "A formal decoder-only transformer model (h heads, d layers, model dim m) with log precision that consumes n input tokens then generates t(n) intermediate tokens; attention can view all prior hidden states; lower-bound constructions assume projected pre-norm, saturated attention, and strict causal masking.",
            "model_size": null,
            "architecture_type": "theoretical decoder-only transformer (with projected pre-norm, strict causal attention option)",
            "training_data": null,
            "reasoning_method": "explicit intermediate decoding steps as computational resource (t(n))",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "formal language recognition and sequential computation (regular languages, automata, Turing machine simulation, directed graph connectivity, linear equalities, context-free recognition)",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "Theoretical: CoT(0) (no intermediate steps) is contained in uniform TC^0; CoT(O(log n)) ⊆ L; CoT(O(n)) contains TIME(n) (can simulate automata) and is contained in SPACE(n) and ~TIME(n^2 + n^2); CoT(polynomial) exactly equals P under given model assumptions.",
            "key_findings": "Formal characterization linking number of decoding steps to standard complexity classes: TIME(t(n)) ⊆ CoT(t(n)) ⊆ SPACE(t(n)+log n) ⊆ ~TIME(t(n)^2 + n^2); linear steps add the ability to simulate automata, polynomial steps make the class P.",
            "limitations": "Lower-bound simulations require architectural generalizations (projected pre-norm/multi-pre-norm), log precision, and idealized/saturated attention; practical LLMs may not meet these assumptions and learnability of the constructions is unaddressed.",
            "uuid": "e7057.2"
        },
        {
            "name_short": "projected pre-norm",
            "name_full": "Projected pre-norm (generalized pre-norm)",
            "brief_description": "An architectural generalization where each transformer sublayer applies a learned linear projection before layer normalization, enabling selective normalization over subspaces of the hidden state.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Projected pre-norm (architecture variant)",
            "model_description": "Each sublayer applies a linear projection M to the input vector then applies layer_norm(M v), allowing the layer to focus normalization on a projected subset of dimensions; multi-pre-norm (multiple projections) can be simulated by repeated projected pre-norm layers.",
            "model_size": null,
            "architecture_type": "transformer normalization variant (pre-norm generalization)",
            "training_data": null,
            "reasoning_method": null,
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "used in theoretical constructions to enable simulation of automata and Turing machines",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "Used in lower-bound constructions; without projected pre-norm the provided simulations (Theorems 1 and 2) may not hold as stated.",
            "key_findings": "Projected pre-norm is an enabling architectural assumption for the paper's lower-bound constructions (simulating automata and bounded-time Turing machines).",
            "limitations": "This is a theoretical modification; the paper notes it is a 'slight generalization' and suggests investigating incorporation into practical models, but does not provide empirical evidence.",
            "uuid": "e7057.3"
        },
        {
            "name_short": "layer-norm hash",
            "name_full": "Layer-norm hash",
            "brief_description": "A mechanism introduced in this paper that uses layer normalization on a crafted 4-vector (x,y,-x,-y) to produce a scale-invariant unit vector representation enabling exact-equality retrieval across positions under uniform attention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Layer-norm hash (mechanism)",
            "model_description": "Compute phi(x,y) = layer_norm(x,y,-x,-y); using phi(x/i,1/i) produces a representation invariant to position denominator i, enabling attention-based equality tests and retrieval of the most-recent write to a simulated tape cell.",
            "model_size": null,
            "architecture_type": "attention-compatible retrieval primitive implemented with layer-norm and feedforward layers",
            "training_data": null,
            "reasoning_method": "enables exact-match retrieval and rightmost retrieval in transformer attention with uniform scores; used inside constructions that simulate automata and Turing machines",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "memory retrieval for formal computation simulation (tape cell reads/writes)",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "Provides a retrieval primitive not previously formalized in this exact form; used to overcome scaling/denominator mismatches in uniform-attention algorithms.",
            "key_findings": "Layer-norm hash yields scale-invariant keys so equality checks succeed across positions and enables efficient simulated-memory retrieval via attention; essential to the paper's decoder-only TM simulation.",
            "limitations": "Relies on precise behavior of layer-norm/RMS-norm and on the model's ability to compute the required projections and concatenations; implementation assumptions (log precision, projected pre-norm) are necessary in the theoretical proofs.",
            "uuid": "e7057.4"
        },
        {
            "name_short": "saturated attention",
            "name_full": "Saturated attention (averaging hard attention)",
            "brief_description": "An idealized attention model used in lower-bound constructions where per-head attention scores are either 0 or an equal weight 1/v over a selected subset of positions (includes uniform attention as a special case).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Saturated attention (idealized)",
            "model_description": "A theoretical attention abstraction in which attention per head selects a subset of positions and assigns equal nonzero mass 1/v to them (or zero elsewhere), enabling exact averaging and hard selection behaviors in proofs.",
            "model_size": null,
            "architecture_type": "idealized attention model used in proofs",
            "training_data": null,
            "reasoning_method": null,
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "used to implement retrieval and simulation algorithms in theoretical constructions",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "Assumed for lower bounds; standard continuous softmax attention may only approximate this behavior in practice.",
            "key_findings": "Assuming saturated attention simplifies constructions and is standard in prior theoretical transformer work for proving lower bounds/lifting capabilities.",
            "limitations": "This is an idealization that may not match learned attention precisely; practical models may not achieve exact saturated attention.",
            "uuid": "e7057.5"
        },
        {
            "name_short": "strict causal masking",
            "name_full": "Strict causal masking",
            "brief_description": "A slight variant of causal attention assumed in some constructions where, at position i, attention can view positions up to i-1 but not the current token i (i.e., the current token is excluded from its own attention keys).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Strict causal masking (attention constraint)",
            "model_description": "Attention formulation where c(i) = i-1 for decoding positions (cannot attend to current column), required for some decoder-only simulations (notably Theorem 2, TM simulation).",
            "model_size": null,
            "architecture_type": "attention masking variant",
            "training_data": null,
            "reasoning_method": null,
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "facilitates correct retrieval and tie-breaking in Turing machine simulation constructions",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "A slight nonstandard assumption compared to typical causal attention (which can attend up to and including current token); required to avoid self-observation in certain proof steps.",
            "key_findings": "Used as an architectural assumption enabling the decoder-only transformer to simulate Turing machine steps correctly with intermediate decoding.",
            "limitations": "Nonstandard relative to most practical transformer implementations; implications for learnability or empirical performance are not addressed.",
            "uuid": "e7057.6"
        },
        {
            "name_short": "GPT-4/ChatGPT (mentions)",
            "name_full": "ChatGPT and GPT-4 (empirical transformer LMs)",
            "brief_description": "Large autoregressive transformer-based language models referenced in the paper as empirically struggling on certain sequential reasoning problems inspired by theoretical limitations.",
            "citation_title": "How language model hallucinations can snowball.",
            "mention_or_use": "mention",
            "model_name": "GPT-4 / ChatGPT",
            "model_description": "Large-scale transformer-based autoregressive language models (referenced generically; no architecture/training specifics provided in this paper).",
            "model_size": null,
            "architecture_type": "transformer (large autoregressive LM)",
            "training_data": null,
            "reasoning_method": null,
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "empirical sequential reasoning tasks cited (e.g., graph connectivity, depth-dependent compositional tasks, matrix equalities)",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Cited empirical findings: (1) Some reasoning problems inspired by theoretical limits cannot be solved by ChatGPT/GPT-4 (Zhang et al., 2023); (2) GPT-4's reasoning performance negatively correlates with depth of the problem's computation graph (Dziri et al., 2023).",
            "limitations": "No quantitative performance numbers or benchmarks are reported in this paper; claims are citations to empirical studies.",
            "uuid": "e7057.7"
        },
        {
            "name_short": "Feng2023 (CoT modular arithmetic)",
            "name_full": "Feng et al., 2023: chain-of-thought enabling modular arithmetic",
            "brief_description": "A recent theoretical result (cited) showing that chain-of-thought allows transformers to solve a specific modular-arithmetic problem that they likely cannot solve without intermediate generation.",
            "citation_title": "Towards revealing the mystery behind chain of thought: A theoretical perspective.",
            "mention_or_use": "mention",
            "model_name": "Feng et al. (2023) result",
            "model_description": "Prior theoretical work demonstrating a concrete example where chain-of-thought enables a transformer to perform a computation (modular arithmetic) infeasible without intermediate steps.",
            "model_size": null,
            "architecture_type": "theoretical transformer constructions with CoT",
            "training_data": null,
            "reasoning_method": "chain-of-thought / intermediate decoding",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "modular arithmetic (theoretical task showing CoT advantage)",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "Shows existence of tasks solvable with CoT but likely unsolvable without it (qualitative/theoretical demonstration).",
            "key_findings": "Provides motivating prior evidence that CoT can enable computations beyond zero-step transformers; cited to motivate the broader theoretical study in this paper.",
            "limitations": "Specific to a constructed modular arithmetic problem; does not provide broad empirical evaluation across standard logical reasoning benchmarks.",
            "uuid": "e7057.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models.",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "rating": 2
        },
        {
            "paper_title": "Towards revealing the mystery behind chain of thought: A theoretical perspective.",
            "rating": 2
        },
        {
            "paper_title": "Attention is Turing complete.",
            "rating": 2
        },
        {
            "paper_title": "How language model hallucinations can snowball.",
            "rating": 1
        },
        {
            "paper_title": "Faith and fate: Limits of transformers on compositionality.",
            "rating": 1
        },
        {
            "paper_title": "The parallelism tradeoff: Limitations of log-precision transformers.",
            "rating": 1
        }
    ],
    "cost": 0.019091499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Expressive Power of Transformers with Chain of Thought</h1>
<p>William Merrill<br>New York University<br>willm@nyu.edu</p>
<p>Ashish Sabharwal<br>Allen Institute for AI<br>ashishs@allenai.org</p>
<h4>Abstract</h4>
<p>Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps, assuming projected pre-norm (a slight generalization of standard pre-norm), adds a clear new ability (under standard complexity conjectures): recognizing all regular languages. Our results also imply that linear steps keep transformer decoders within context-sensitive languages, and polynomial steps with generalized pre-norm make them recognize exactly the class of polynomial-time solvable problems-the first exact characterization of a type of transformers in terms of standard complexity classes. Together, this provides a nuanced framework for understanding how the length of a transformer's chain of thought or scratchpad impacts its reasoning power.</p>
<h2>1 INTRODUCTION</h2>
<p>A series of recent theoretical results (Merrill \&amp; Sabharwal, 2023b;a; Merrill et al., 2022; Liu et al., 2023; Chiang et al., 2023; Hao et al., 2022) has unveiled surprising limits on realistic formal models of transformers. They have shown that standard transformers, even with ideal parameters, cannot perfectly solve many sequential reasoning problems at scale, such as simulating finite-state machines, deciding whether nodes in a graph are connected, or solving matrix equalities. The intuition here is that the transformer lacks recurrent connections, and recurrence is required to solve these sequential reasoning problems. Empirically, reasoning problems inspired by these results cannot be solved by cutting-edge transformer language models such as ChatGPT and GPT-4 (Zhang et al., 2023), and the reasoning performance of GPT-4 negatively correlates with the depth of the problem's computation graph (Dziri et al., 2023). These results show certain kinds of sequential reasoning pose a challenge for the transformer and motivate extensions to address this issue.</p>
<p>One method that has been empirically successful for improving sequential reasoning with transformers is adding a so-called chain of thought (Wei et al., 2022) or scratchpad (Nye et al., 2021). These methods allow the transformer to output a sequence of intermediate tokens before answering, rather than answering right away after reading the input. Intuitively, such methods could unlock greater expressive power on sequential reasoning problems because the model can use each intermediate token as a kind of recurrent state. Feng et al. (2023) recently showed how chain of thought lets transformers solve a specific modular arithmetic problem that they likely cannot solve without one. Yet there is no general characterization of the class of problems transformers can solve with chain of thought. Thus, the extent to which chain of thought alleviates transformers' weaknesses is unclear, as well as the number of chain of thought steps required to gain reasoning power.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Summary of results: transformers with intermediate generation against various classes of formal languages. A logarithmic number of chain-of-thought steps remains in log-space (L). A linear number of steps adds more power, enabling recognizing all regular languages (Reg), but is contained within context-sensitive languages (CSL). We assume context-free languages (CFL) require $\tilde{\omega}\left(n^{2}\right)$ time to recognize. Some regions with area in the plot are not known to be non-empty.</p>
<p>In this work, we address these open questions by characterizing the reasoning power of transformer decoders that can take intermediate steps before generating an answer and comparing them against transformers without intermediate steps. A transformer with a chain of thought constitutes a special case of a transformer decoder with intermediate steps. Our fine-grained results give upper and lower bounds on transformers' power depending on $t(n)$ : the number of allowed intermediate steps as a function of the input size $n$. We focus mainly on understanding three regimes: logarithmic steps (when $t(n)=\Theta(\log n)$ ), linear steps (when $t(n)=\Theta(n)$ ), and polynomial steps:</p>
<ol>
<li>Prior Work: No Intermediate Steps. Recent work has shown transformer decoders without any intermediate steps can only solve problems that lie inside the fairly small circuit complexity class $\mathrm{TC}^{0}$ (Merrill \&amp; Sabharwal, 2023b) and related logical classes (Merrill \&amp; Sabharwal, 2023a; Chiang et al., 2023). This implies basic transformers are far from Turing-complete: they cannot even solve problems complete for classes larger than $\mathrm{TC}^{0}$ such as simulating automata ( $\mathrm{NC}^{1}$-complete), deciding directed graph connectivity (NL-complete), or solving linear equalities ( P -complete). ${ }^{1}$</li>
<li>Logarithmic Steps. With a logarithmic number of intermediate steps, we show that the upper bound for transformers expands slightly from $\mathrm{TC}^{0}$ to L . This means transformers with a logarithmic number of intermediate steps might gain power, but they still cannot solve NL-complete problems like directed graph connectivity or P-complete problems like solving linear equalities. ${ }^{2}$</li>
<li>Linear Steps. Linear intermediate steps allow transformers with projected pre-norm ${ }^{3}$ to simulate automata ( $\mathrm{NC}^{1}$-complete), which cannot be done without intermediate steps unless $\mathrm{TC}^{0}=\mathrm{NC}^{1}$. Polynomial Steps. With a polynomial number of decoding steps, we show that transformers with strict causal attention and projected pre-norm are equivalent to the class P . This, to our best knowledge, is the first equivalence between a class of transformers and a standard complexity class.</li>
</ol>
<p>Together, our results provide a framework for understanding how the length of a transformer's chain of thought affects its reasoning power. We find a logarithmic chain does not add much, while a linear chain affords more power on inherently sequential reasoning problems.</p>
<h1>1.1 Main Results: Power of Transformers with Intermediate Decoding</h1>
<p>Let $\operatorname{TIME}(t(n))$ be the class of languages $L$ for which there exists a Turing machine that runs in time $\mathrm{O}(t(n))$ and accepts $L .{ }^{4}$ Let $\widetilde{\operatorname{TIME}}(t(n))$ be the class of problems in $\operatorname{TIME}\left(t(n) \log ^{k} n\right)$ for some $k$, which is meaningful for $t(n) \geq n$. Let $\operatorname{SPACE}(s(n))$ be the class of languages $L$ for which there exists a Turing machine with tape size bounded by $\mathrm{O}(s(n))$ that accepts $L$. We show the following</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>relationship between transformers with $t(n)$ steps and standard time/space complexity classes:</p>
<p>$$
\operatorname{TIME}(t(n)) \subseteq \operatorname{CoT}(t(n)) \begin{aligned}
&amp; \subseteq \operatorname{SPACE}(t(n)+\log n) \
&amp; \subseteq \widetilde{\operatorname{TIME}}\left(t(n)^{2}+n^{2}\right)
\end{aligned}
$$</p>
<p>Here $\operatorname{CoT}(t(n))$ denotes the set of languages recognized by some transformer using $t(n)$ decoding steps. Our lower bound (left side of Equation (1)) assumes strict causal saturated attention and projected pre-norm, while upper bounds hold both with and without these architectural assumptions. Both our time lower bound and space upper bound are fairly tight: improving either by a factor larger than $\log t(n)$ would result in a fundamental complexity theory advance (Hopcroft et al., 1977).</p>
<p>Capabilities of Transformers with CoT. The left side of Equation (1) implies that transformer decoders with $\Theta(n)$ steps can simulate real-time models of computation like automata or counter machines (Merrill, 2020). Under standard assumptions in complexity theory, transformers with no decoding steps cannot simulate all automata (Merrill \&amp; Sabharwal, 2023b; Merrill, 2023; Liu et al., 2023). Thus, a linear number of decoding steps makes transformers strictly more powerful. Similarly, the left side of Equation (1) implies transformers with a quadratic number of steps can express a linear-time algorithm (for a random access Turing machine) to solve directed graph connectivity (Wigderson, 1992), again a problem known to be beyond the limits of standard transformers. In the same vein, with a polynomial number of decoding steps, transformers can solve linear equalities, Horn-clause satisfiability, and universal context-free recognition, all of which are P-complete and thus known to be inexpressible by standard transformers (Merrill \&amp; Sabharwal, 2023b).</p>
<p>The left side of Equation (1) is proven by showing transformer decoders can simulate $t$ Turing machine steps with $t$ intermediate steps. Similar prior results have assumed a transformer with external memory (Schuurmans, 2023) or an encoder-decoder model with nonstandard-positional decodings (Pérez et al., 2021). Our construction adapts these ideas to work for a decoder-only model without external memory or extra positional encodings, but with strict causal masking and projected pre-norm (cf. Section 2.1). ${ }^{5}$ The key idea behind our more general construction is the layer-norm hash (Section 3.1): a simple module for effectively storing memory in decoder-only transformers. We believe the layer-norm hash could be broadly useful for building algorithms in transformers. For example, Yao et al. (2021) used a related idea to construct transformers that recognize boundeddepth Dyck languages, although in a more ad hoc way.</p>
<p>Limitations of Transformers with CoT. The right side of Equation (1) establishes two upper bounds on transformer decoders with $t(n)$ intermediate steps that depend on both $t(n)$ and $n$. We turn to the implications of this general result in different regimes for $t(n)$ :</p>
<ol>
<li>Log Steps: Transformer decoders with $\mathrm{O}(\log n)$ intermediate steps can only recognize languages in $\mathrm{L}=\operatorname{SPACE}(\log n)$. This implies that transformers with $\mathrm{O}(\log n)$ intermediate steps cannot solve NL- or P-complete problems ${ }^{2}$ like directed graph connectivity, just like transformers with no intermediate decoding (Merrill \&amp; Sabharwal, 2023b).</li>
<li>Linear Steps: Transformer decoders with $\mathrm{O}(n)$ intermediate steps can only recognize languages that are in both $\widetilde{\operatorname{TIME}}\left(n^{2}\right)$ and $\operatorname{SPACE}(n)$. Since $\operatorname{SPACE}(n)$ falls within the context-sensitive languages (Kuroda, 1964), transformers with linear steps can recognize at most context-sensitive languages. Alongside our lower bound, this shows transformer decoders with $\Theta(n)$ steps fall somewhere between regular and context-sensitive languages in the Chomsky hierarchy. Further, transformers with $\mathrm{O}(n)$ steps cannot recognize all context-free languages unless context-free languages can be parsed in soft quadratic time. ${ }^{6}$</li>
<li>Polynomial Steps: If $t(n)=\mathrm{O}\left(n^{c}\right)$ for some $c$, we get an upper bound of $\mathrm{P}=$ $\bigcup_{c=1}^{\infty} \operatorname{TIME}\left(n^{c}\right)$. Combined with our lower bound, this shows that transformer decoders with a polynomial number of steps recognize exactly the class P. Thus, a polynomial number of steps turns transformers into strong reasoners, though running a polynomial number of forward passes with a large transformer is likely intractable in practice.</li>
</ol>
<p>Together, these results show that intermediate generation like chain of thought or scratchpad can add reasoning power to transformers and that the number of steps matters as a computational resource</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>akin to time or space. Some of the limitations identified in prior work (Merrill \&amp; Sabharwal, 2023b; Chiang et al., 2023, etc.) can be overcome with a linear or quadratic number of steps, and a polynomial number of steps covers all problems in $\mathbb{P}$. On the other hand, we have not identified any concrete reasoning problem where a logarithmic number of steps would help. These results provide a unified understanding of the power of transformer decoders across decoding lengths and problems.</p>
<h1>2 Preliminaries</h1>
<p>We study the power of decoder-only transformers that can generate intermediate tokens between reading the input and generating an answer. On input $x \in \Sigma^{n}$, the transformer consumes tokens $x_{1}, \ldots, x_{n}$ for the first $n$ steps, and then, for $t(n)$ intermediate steps, consumes the token generated by the previous step. At each step, the transformer can attend over all previous hidden states. This standard method of generating text from a decoder-only model can be described formally as follows. Let $\Sigma$ be a finite alphabet and $f: \Sigma^{*} \rightarrow \Sigma$ be a function mapping a prefix to a next token (parameterized by a transformer). Let $\cdot$ be concatenation. We define the $k$-step extension of $f$ as</p>
<p>$$
f^{0}(x)=x, \quad f^{k+1}(x)=f^{k}(x) \cdot f\left(f^{k}(x)\right)
$$</p>
<p>We say we have run $f$ on $x$ with $t(n)$ (additional) decoding steps if we compute the function $f^{t(|x|)}(x)$. We consider $f$ with $t(n)$ steps to recognize the language of strings such that $f^{t(|x|)}(x)=1$, where $1 \in \Sigma$ is a special "accept" symbol. We denote by $\operatorname{CoT}(t(n))$ the set of languages that are recognized by $t(n)$ decoding steps for some transformer $f$.</p>
<h3>2.1 TRANSFORMERS</h3>
<p>A transformer is a neural network parameterizing a function $\Sigma^{*} \rightarrow \Sigma$. Let $\mathbb{D}<em p="p">{p}$ be the datatype of $p$-precision floats and define $p$-truncated addition $(+, \sum)$, multiplication $(\cdot)$, and division $(/)$ over $\mathbb{D}</em>$ as in Merrill \&amp; Sabharwal (2023b). We now define the high-level structure of the transformer in terms of its core components, with the details of those components in Appendix A.
Definition 1 (Merrill \&amp; Sabharwal 2023a). A $p$-precision decoder-only transformer with $h$ heads, $d$ layers, model dimension $m$ (divisible by $h$ ), and feedforward width $w$ is specified by:</p>
<ol>
<li>An embedding function $e: \Sigma \times \mathbb{N} \rightarrow \mathbb{D}_{p}^{m}$ whose form is defined in Appendix A.2;</li>
<li>For each $1 \leq \ell \leq d$ and $1 \leq k \leq h$, a head similarity function $s_{k}^{\ell}: \mathbb{D}<em p="p">{p}^{m} \times \mathbb{D}</em>$ whose form is defined in Appendix A. 3 (and includes projected layer-norm);}^{m} \rightarrow \mathbb{D}_{p</li>
<li>For each $1 \leq \ell \leq d$ and $1 \leq k \leq h$, a head value function $v_{k}^{\ell}: \mathbb{D}<em p="p">{p}^{m} \rightarrow \mathbb{D}</em>$ whose form is defined in Appendix A. 3 (and includes projected layer-norm);}^{m / h</li>
<li>For each $1 \leq \ell \leq d$, an activation function $f^{\ell}:\left(\mathbb{D}<em p="p">{p}^{m / h}\right)^{h} \times \mathbb{D}</em>$ whose form is defined in Appendix A. 4 and implicitly uses the feedforward dimension $w$ (and includes projected layer-norm);}^{m} \rightarrow \mathbb{D}_{p}^{m</li>
<li>An output function $\gamma: \mathbb{D}_{p}^{m} \rightarrow \Sigma$ parameterized as a linear transformation.</li>
</ol>
<p>Definition 2. We define one decoding step $\Sigma^{n} \rightarrow \Sigma$ with a decoder-only transformer as follows:</p>
<ol>
<li>Embeddings: For $1 \leq i \leq n, \mathbf{h}<em i="i">{i}^{0}=e\left(x</em>, i\right)$.</li>
<li>Multihead Self Attention: For each layer $1 \leq \ell \leq d$, we compute $h$ attention heads:</li>
</ol>
<p>$$
\mathbf{a}<em j="1">{i, k}^{\ell}=\sum</em>}^{c(i)} \frac{s_{k}^{\ell}\left(\mathbf{h<em j="j">{i}^{\ell-1}, \mathbf{h}</em>}^{\ell-1}\right)}{Z_{i, k}^{\ell}} \cdot v_{k}^{\ell}\left(\mathbf{h<em i_="i," k="k">{j}^{\ell-1}\right), \quad \text { where } Z</em>}^{\ell}=\sum_{j=1}^{c(i)} s_{k}^{\ell}\left(\mathbf{h<em j="j">{i}^{\ell-1}, \mathbf{h}</em>\right)
$$}^{\ell-1</p>
<p>and $c(i)$ is $i$ for standard causal attention and $i-1$ for strict causal attention.
3. Activation Block: For $1 \leq \ell \leq d$, activation block $\ell$ maps the head outputs to $\mathbf{h}^{\ell}$ :</p>
<p>$$
\mathbf{h}<em 1="1" i_="i,">{i}^{\ell}=f^{\ell}\left(\mathbf{a}</em>}^{\ell}, \ldots, \mathbf{a<em i="i">{i, h}^{\ell}, \mathbf{h}</em>\right)
$$}^{\ell-1</p>
<ol>
<li>Classifier Head: The transformer output is $\gamma\left(\mathbf{h}_{n}^{d}\right)$.</li>
</ol>
<p>These definitions use 1-indexing, but when the input contains a beginning-of-sequence token $\$$ (Theorems 1 and 2), we will use 0 -indexing starting at $\$$ in the natural way.</p>
<p>Transformer Precision. We consider log-precision transformers (Merrill \&amp; Sabharwal, 2023b), i.e., we allow the transformer at most $c \log m$ precision for $m$ decoding steps. As a transformer with intermediate generation runs for $n$ input steps and $t(n)$ intermediate decoding steps, this means we have precision at most $c \log (n+t(n))$. Log precision has been analyzed in prior work (Pérez et al., 2021; Merrill \&amp; Sabharwal, 2023b;a) because it gives the transformer just enough precision to represent indexes and sums across different positions. This means it naturally formalizes a boundedprecision transformer that is capable of representing position and computing uniform attention, two important capabilities for constructing algorithms with transformers.</p>
<p>Our lower bound constructions (Theorems 1 and 2) assume the following:</p>
<ol>
<li>Saturated Attention. A saturated transformer (Merrill et al., 2021) is an idealized transformer with "averaging hard attention" (Strobl et al., 2024): per head, all attention scores are either 0 or $1 / v$ for some $v$. This includes uniform attention ( $1 / n$ over $n$ tokens) or hard attention as special cases. Following common practice (Pérez et al., 2021; Merrill \&amp; Sabharwal, 2023b), we use saturated attention for our lower bound constructions.</li>
<li>Strict Causal Masking. The formulation of attention in Definition 2 makes the slightly nonstandard assumption that causally masked attention at position $i$ can view tokens at all positions up to $i-1$ but not the current token $i$. This is required in Theorem 2.</li>
<li>Projected Pre-Norm. Our lower bound constructions require $s_{\ell}$ and $f_{\ell}$ in Definition 2 to allow a generalization of standard pre-norm. Normally, a layer-norm is applied to the entire input to each sublayer. We generalize this, allowing each sublayer to apply a linear projection before layer-norm. Crucially, in particular, this enables each layer to pick out a subset of the previous hidden state to apply layer-norm to (cf. Definition 4 in Appendix A.1).</li>
</ol>
<p>For convenience, our proofs with projected pre-norm use an even more general notion of pre-norm, namely multi-pre-norm, which allows each sublayer to take $k$ different projections of its input, apply layer-norm to each, and concatenate (cf. Definition 5 in Appendix A.1). Multi-pre-norm can, however, be simulated by multiple layers of projected pre-norm (see Appendix A. 1 for a proof):
Proposition 1 (Chiang, 2024). Multi-pre-norm with $k$ norms can be simulated by $k+1$ projected pre-norm layers.</p>
<h1>2.2 MODELS OF COMPUTATION</h1>
<p>Automata. A deterministic finite-state automaton is a tuple $A=\left\langle\Sigma, Q, q_{0}, \delta, F\right\rangle$ where $\Sigma$ is a finite input vocabulary, $Q$ is a finite set of states containing initial state $q_{0}, \delta$ is a transition function $Q \times \Sigma \rightarrow Q$, and $F \subseteq Q$ is a set of final states. $A$ processes an input string $\sigma \in \Sigma^{n}$ as follows. $A$ starts with state $q_{0}$ and reads $\sigma$ one token at a time, updating $q_{i}=\delta\left(q_{i-1}, \sigma_{i}\right)$ until $i=n$. $A$ accepts $\sigma$ if $q_{n} \in F$ and rejects it otherwise. The language recognized by $A$ is the set of strings it accepts.</p>
<p>Turing Machines. Adapting the notation of Hopcroft et al. (2001), a multitape Turing machine is a tuple $\left\langle\Sigma, \Gamma, k, b, Q, q_{0}, \delta, F\right\rangle$ where:</p>
<ol>
<li>$\Sigma$ is a finite input vocabulary</li>
<li>$\Gamma$ is a finite tape vocabulary with $\Sigma \subseteq \Gamma$</li>
<li>$k$ is the number of work tapes</li>
<li>$b$ is a blank symbol such that $b \in \Gamma$ and $b \notin \Sigma$</li>
<li>$Q$ is a finite set of states containing initial state $q_{0}$</li>
<li>$\delta$ is a transition function $(Q \backslash F) \times \Gamma^{k+2} \rightarrow Q \times \Gamma^{k+1} \times{ \pm 1}^{k+2}$</li>
<li>$F \subseteq Q$ is a set of halting states</li>
</ol>
<p>We define Turing machine computation in the standard way (cf. Appendix B).</p>
<h2>3 Lower Bounds for Transformer Decoders</h2>
<p>Prior work (Merrill \&amp; Sabharwal, 2023a) has established strong upper bounds on the reasoning problems transformers can solve. Specifically, under standard conjectures in complexity, transformers without intermediate decoding cannot recognize all regular languages. In this section, we show some of these shortcomings can be overcome with a suitable number of intermediate decoding steps</p>
<p>(and projected pre-norm). Specifically, a linear number of steps enables simulating an automaton. We also show this can be extended to simulate $t(n)$ Turing machine steps with $t(n)$ decoding steps.</p>
<h1>3.1 Introducing Layer-Norm Hash</h1>
<p>We first introduce a useful building block for our results that we call the layer-norm hash. The layer-norm hash is a mechanism that enables retrieval across different columns in the transformer based on query-key matching of numerical values. Exact-match retrieval is trivial when the query $q_{i}$ and keys $k_{1}, \ldots k_{i}$ are items in a finite set: just one-hot encode $q_{i}$ and $k_{j}$ and the inner product will be maximized when $q_{i}$ and $k_{j}$ match. But this does not work when the keys and values are counts produced by uniform attention, which many transformer algorithms use (Weiss et al., 2021), as the key $q_{i} / i$ and query $k_{j} / j$ have different denominators. The layer-norm hash helps by transforming $q_{i} / i$ and $k_{j} / j$ so hard attention retrieves $j$ s.t. $q_{i}=k_{j}$. Let layer_norm $(\mathbf{x})=\frac{\mathbf{x}^{\prime}}{|\mathbf{x}^{\prime}|}$, where $\mathbf{x}^{\prime}=\mathbf{x}-\bar{x}$.
Definition 3 (Layer-norm hash). For $x, y \in \mathbb{R}$, let $\phi(x, y) \triangleq \operatorname{layer} _ \operatorname{norm}(x, y,-x,-y)$.
$\phi(x, y)$ is a unit vector in $\mathbb{R}^{4}$. A key feature is scale invariance, and, in particular, that $\phi(x / i, 1 / i)$ is invariant w.r.t. $i$ in the sense that it is only a function of $x$, independent of $i$. Let $\phi_{x} \triangleq \phi(x, 1)$. Then we have the following properties, whose proof may be found in Appendix C.
Lemma 1 (Scale invariance). For any $x \in \mathbb{R}$ and $i \in \mathbb{R}<em x="x">{&gt;0}, \phi(x / i, 1 / i)=\phi</em>$.
Lemma 2 (Equality check via layer-norm hash). For any $q, k \in \mathbb{R}, \phi_{q} \cdot \phi_{k}=1$ if and only if $q=k$.
In other words, the inner product of these representations of two scalars $q$ and $k$, even if computed at different positions $i$ and $j$, respectively, allows us to check for the equality of $q$ and $k$. We can look up key $q_{i} / i$ in a sequence of keys $k_{1} / 1, \ldots, k_{i-1} /(i-1)$ by attending with query $\phi\left(q_{i} / i, 1 / i\right)$ at position $i$ and key $\phi\left(k_{j} / j, 1 / j\right)$ at each $j&lt;i$. By Lemmas 1 and 2 this averages the values at all $j$ such that $q_{i}=k_{j}$. The layer-norm hash can also be used to directly compare two values $q_{i}, k_{j}$ without removing the denominator by computing $\phi\left(q_{i}, 1\right)$ and $\phi\left(k_{j}, 1\right)$.</p>
<h3>3.2 Simulating Automata</h3>
<p>We can use the layer-norm hash to simulate models of computation like automata or Turing machines with intermediate-generation transformers. To warm up, we first show how to use the layer-norm hash to simulate an automaton (i.e., recognize a regular language) and then extend it in Section 3.3 to show how a transformer can simulate a Turing machine for a bounded number of steps.
Theorem 1 (Regular language recognition). For any regular language $L$. there is a decoder-only projected pre-norm transformer with strict causal saturated attention (with or without positional encodings) that, on input $\$ x,{ }^{7,8}$ checks whether $x \in L$ with $|x|+1$ decoding steps.</p>
<p>Proof. Let $A$ be a finite-state automaton recognizing $L$. We will simulate one step of $A$ with one transformer decoding step (after first reading $n$ input tokens). We refer to tokens with 0 indexing: $\$$ is token $0, x_{1}$ is token 1 , etc. At step $i, n \leq i \leq 2 n$, we will output a token $q_{i-n}$ encoding the next state of $A$. After printing the final state $q_{n}$, we use one additional step to output 1 iff $q_{n} \in F$, the set of final states of $A$. At each token $i&gt;0$, we compute $1 / i$ by attending uniformly over the strict left context with value $\mathbb{1}\left[x_{j}=\$\right]$. We show by induction that at step $i \geq n$, we can output $q_{i-n}$.
Base Case: $i=n$. For $i \leq n$, we output $q_{0}$. Crucially, at $i=n$, this becomes the next input.
Inductive Case: $i&gt;n$. We already have a sequence of intermediate tokens $q_{0}, \ldots, q_{i-n-1}$. Our goal is to compute $q_{i-n}=\delta\left(q_{i-n-1}, \sigma_{i-n}\right)$, which first involves retrieving $q_{i-n-1}$ and $\sigma_{i-n} . q_{i-n-1}$ is the input to the current column of the transformer. We will use hard attention to retrieve the current input symbol $\sigma_{i-n}$. To do this, we attend uniformly over the prior decoding tokens and $\$$ with a value of 1 at $\$$ and 0 elsewhere. At tokens $i&gt;n$ (i.e., decoding tokens), this yields $\frac{1}{i-n}$. Recall that projected pre-norm can simulate multi-pre-norm (Proposition 1). We now leverage the multi-pre-norm architecture to pass two layer-norms to a feedforward network:</p>
<p>$$
\phi_{i}^{\mathrm{I}} \triangleq \phi(1 / i, 1), \quad \phi_{i}^{\mathrm{D}} \triangleq \phi(1 /(i-n), 1)
$$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Let $d_{i} \triangleq \mathbb{1}\left[x_{i} \in Q\right]$, where $Q$ is the set of states of $A$. Based on $d_{i}$, we select between $\phi_{i}^{\mathrm{I}}$ and $\phi_{i}^{\mathrm{D}}$ :</p>
<p>$$
\phi_{i} \triangleq \operatorname{ReLU}\left(-d_{i} \overrightarrow{1}+\phi_{i}^{\mathrm{I}}\right)+\operatorname{ReLU}\left(d_{i} \overrightarrow{1}-\overrightarrow{1}+\phi_{i}^{\mathrm{D}}\right)
$$</p>
<p>We attend with query layer_norm $\left(\phi_{i}\right)=\phi_{i}$, key layer_norm $\left(\phi_{j}\right)=\phi_{j}$ if $d_{j}=0$ and $\overrightarrow{0}$ otherwise, and value $\sigma_{j}$ if $d_{j}=0$ and $\overrightarrow{0}$ otherwise. By Lemma 2, at the current step $i$, the attention score is maximized when $j=i-n$, thus retrieving $\sigma_{i-n}$. We now have the previous state $q_{i-n-1}$ and current token $\sigma_{i-n}$. We conclude by computing $q_{i-n}=\delta\left(q_{i-n-1}, \sigma_{i-n}\right)$ with a feedforward network.</p>
<p>Theorem 1 shows that a linear number of decoding steps gives additional reasoning power to logprecision transformers with projected pre-norm (assuming $\mathrm{TC}^{0} \neq \mathrm{NC}^{1}$ ). This follows because logprecision transformers with no decoding steps are contained in uniform $\mathrm{TC}^{0}$ (Merrill \&amp; Sabharwal, 2023b), which means they cannot recognize all regular languages. In contrast, Theorem 1 says a linear number of steps is sufficient for recognizing all regular languages, establishing a conditional separation. This is an example of simple and familiar additional computational power granted by additional decoding steps. The core challenge in simulating an automaton is recurrence, which cannot be done without decoding steps (Merrill \&amp; Sabharwal, 2023b). A linear number of decoding steps allows simulating recurrence, which is where the additional power comes from. However, this added power does not stop with finite-state machines: the layer-norm hash can be used to simulate more complex models of computation such as Turing machines, which we will turn to next.</p>
<h1>3.3 Simulating Turing Machines</h1>
<p>We now show how a transformer decoder can simulate a Turing machine in real time using the layer-norm hash. Our decoder-only construction resembles the encoder-decoder construction of Pérez et al. (2021). However, it avoids simplifying assumptions from Pérez et al. (2021). In addition to assuming non-standard attention and no layer-norm, they required $1 / i, 1 / i^{2}$, and $i$ in the positional embeddings, which is problematic because transformers cannot represent unbounded scalars like $i$ due to layer-norm. In contrast, our construction works with or without positional encodings. However, it assumes strict causal masking and projected pre-norm (Section 2.1).
Theorem 2 (Turing machine simulation). Let $M$ be a Turing machine that, on input length $1+$ $n$, runs for at most $t(n)$ steps (at most polynomial). There is a decoder-only projected pre-norm transformer with strict causal saturated attention (with or without positional encodings) that, on input $\$ x,{ }^{0}$ takes $t(n)$ decoding steps and then, with $|M(x)|$ additional steps, outputs $M(x)$.</p>
<p>Proof. We construct a transformer decoder that uses a single decoding step to simulate each Turing machine step. The main difficulty is representing a Turing machine tape in a sequence of transformer state vectors so that the transformer can always correctly reconstruct the value on the tape at the current head position. The key idea will be to store "diffs" to the tape at each step and use the layernorm hash to dynamically reconstruct the contents at the head position at future steps. Concretely, let $\Delta$ be a finite vocabulary representing elements of $Q \times \Gamma^{k+1} \times{0, \pm 1}^{k+2}$. The deterministic Turing machine run induces a diff sequence $\delta_{0}, \ldots, \delta_{t(n)} \in \Delta$ capturing the state entered, tokens written, and directions moved after each token. Following the proof of Theorem 1, we use 0-indexing starting at the $\$$ token and compute $1 / i$ at each token $i&gt;0$ as a representation of position. We show by induction that at step $i \geq n$, we can output $\delta_{i-n}$.
Base Case: $i=n$. At every input token (crucially, the last one), we output $\delta_{0}=\left\langle q_{0}, b^{k+1}, 0^{k+2}\right\rangle$.
Inductive Case: $i&gt;n$. We first reconstruct $h_{i}^{\tau}$, the current position on each tape $\tau$. For each $\tau$, a head attends with query 1 , key $\mathbb{1}\left[x_{j} \notin \Sigma\right]$, and value being the move direction of $\tau$ at $j$. Since we assume strict causal attention (for reasons that will become clear later), head $\tau$ thus computes $h_{i-1}^{\tau} / i$. Since we need $h_{i}^{\tau}$, we write both $\left(h_{i-1}^{\tau} \pm 1\right) / i$ to the residual stream. When we need $h_{i}^{\tau} / i$ going forward, we use a linear layer to select either $\left(h_{i-1}^{\tau}+1\right) / i$ or $\left(h_{i-1}^{\tau}-1\right) / i$ depending on if the current input $\delta_{i-n-1}$ contains a +1 move or a -1 move for $\tau$, respectively.
We now use two layers to compute the contents at $h_{i}^{0}$ on the input tape. Similar to Theorem 1, we use a feedforward network to implement the following piecewise comparison:</p>
<p>$$
\phi_{i}^{0} \triangleq \begin{cases}\phi(1,1 / i)=\phi(i, 1) &amp; \text { if } x_{i} \in \Sigma \ \phi\left(h_{i}^{0} / i, 1 / i\right)=\phi\left(h_{i}^{0}, 1\right) &amp; \text { otherwise }\end{cases}
$$</p>
<p>With some abuse of notation, let $\langle\cdot\rangle$ denote vector concatenation. We attend with query $\left\langle\phi_{i}^{0},-1\right\rangle$, key $\left\langle\phi_{j}^{0}, \mathbb{1}\left[x_{j} \notin \Sigma\right]\right\rangle$, and value $\left\langle\phi_{j}^{0}, \sigma_{j}\right\rangle .{ }^{9}$ Let $\left\langle\bar{\phi}^{0}, \bar{\sigma}\right\rangle$ be the head output. We show in Appendix D that two properties hold. First, by Lemma 3, $\bar{\phi}^{0}=\phi_{i}^{0}$ iff $1 \leq h_{i}^{0} \leq n$. Second, by Lemma 4, if $1 \leq h_{i}^{0} \leq n$, then $\bar{\sigma}=\sigma_{h_{i}}$. Based on this, we compute the value read from the input tape as $\gamma_{i}^{0}=\bar{\sigma}$ if $\bar{\phi}^{0}=\phi_{i}^{0}$ and as $\gamma_{i}^{0}=b$ otherwise.</p>
<p>We now use a single attention layer to compute $\gamma_{i}^{\tau}$, the contents at $h_{i}^{\tau}$ on each non-input tape $\tau$. The layer uses two layer-norm hashes, again taking advantage of the multi-pre-norm architecture that projected pre-norm can simulate (Proposition 1):</p>
<p>$$
\begin{aligned}
&amp; \phi_{i}^{\tau} \triangleq \phi\left(h_{i}^{\tau} / i, 1 / i\right)=\phi\left(h_{i}^{\tau}, 1\right) \
&amp; \psi_{i}^{\tau} \triangleq \phi(f(i), 1)
\end{aligned}
$$</p>
<p>where $f(i)$ is defined in Definition 7 in Appendix E. Crucially, $f(i)$ is computable with a single transformer layer and monotonically decreasing with $i$. With strict causal masking, we attend with query $\left\langle\phi_{i}^{\tau}, e_{1}\right\rangle$, key $\left\langle\phi_{j}^{\tau},-\psi_{j}^{\tau}\right\rangle$, and value $\left\langle\phi_{j}^{\tau}, \delta_{j-n-1}\right\rangle$. Let $\left\langle\bar{\phi}^{\tau}, \bar{\delta}\right\rangle$ be the head output. We show in Appendix E that two properties hold. First, by Lemma 6, $\bar{\phi}^{\tau}=\phi_{j}^{\tau}$ iff there is some $j&lt;i$ s.t. $h_{i}^{\tau}=h_{j}^{\tau}$. Second, by Lemma 7, if there is some $j&lt;i$ s.t. $h_{i}^{\tau}=h_{j}^{\tau}$, then the head retrieves $\left\langle\phi_{j}^{\tau}, \delta_{j}\right\rangle$ for the greatest such $j$. Based on this, we compute the last-written value on tape $\tau$ at $h_{i}^{\tau}$ as $\gamma_{i}^{\tau}=[\bar{\delta}]<em i="i">{2+\tau}$ if $\bar{\phi}^{\tau}=\phi</em>\right)$ with a feedforward net.}^{\tau}$ and $\gamma_{i}^{\tau}=b$ otherwise. Having obtained all arguments for the transition function, we now compute $\delta_{i-n}=\delta\left(q_{i-n-1}, \sigma_{h_{i}^{0}}, \gamma_{i}^{1}, \ldots \gamma_{i}^{k+1</p>
<p>Finally, we use $|M(x)|$ steps to write the Turing machine output. We detect we are at an output step if either some $\delta_{j}$ token to the left or the current input encodes a halting state. At each such token $i$, we compute $h_{i}^{k+1} / i$ as before (recall that tape $k+1$ is the output tape) via attention, except the value now is $d_{i}^{k+1}$ if $x_{i} \in \Delta$ and +1 otherwise. We attend as before using $h_{i}^{k+1} / i$ to retrieve (and output) $\gamma_{i}^{k+1}$. Thus, the $|M(x)|$ tokens generated after a final state are precisely $M(x)$.</p>
<p>The critical role projected pre-norm or multi-pre-norm play in Theorems 1 and 2 suggest it could be interesting to investigate incorporating these generalized pre-norms into transformers in practice.</p>
<p>Theorem 2 gives us a general result connecting the power of transformer decoders with $t(n)$ steps to Turing machines with the same number of intermediate steps:
Corollary 2.1. $\operatorname{TIME}(t(n)) \subseteq \operatorname{CoT}(t(n))$.
Thus, simulating an automaton (cf. Theorem 1) is not the only new capability unlocked with $\mathrm{O}(n)$ decoding steps: rather, such transformers can solve any problem a Turing machine can solve in $\mathrm{O}(n)$ time, such as simulating real-time counter machines (Weiss et al., 2018). With $\mathrm{O}\left(n^{2}\right)$ steps, we can solve directed graph connectivity using standard graph traversal algorithms like depth-first search. Depth-first search runs in $\mathrm{O}(n)$ time on a random access Turing machine (Wigderson, 1992), which can be simulated in $\mathrm{O}\left(n^{2}\right)$ time without random access. Possibly, transformers can solve directed graph connectivity with fewer than $\mathrm{O}\left(n^{2}\right)$ steps, as results from Zhang et al. (2023) hint at.</p>
<h1>4 UPPER BOUNDS FOR TRANSFORMER DECODERS</h1>
<p>Having shown lower bounds on transformers with $t(n)$ steps, we present two different upper bounds: one that relates transformer decoders to time complexity classes, and one that relates them to space complexity classes. The relative strength of the two different bounds will vary depending on $t(n)$. A simple upper bound on transformers with chain of thought can be obtained based on the fact that transformers can be simulated using a quadratic number of arithmetic operations.
Theorem 3. $\operatorname{CoT}(t(n)) \subseteq \widehat{\operatorname{TIME}}\left(n^{2}+t(n)^{2}\right)$.
Proof. We sketch a multitape Turing machine that will simulate the transformer. Each forward pass $i$ appends key $i$ onto a work tape and value $i$ onto another work tape. To simulate the forward pass at time $i$, it suffices to show how to simulate computing self-attention at time $i$. To compute self</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>attention, the Turing machine first computes the query at time $i$. It then iterates over pairs on the key and value work tapes. For each pair $j$, we compute the attention score between query $i$ and key $j$ and then multiply it by value $j$ using additional work tapes. We then add this value to a running sum tape. We treat the final sum at the output of the attention mechanism.
This runs $n+t(n)$ forward passes, and each forward pass loops over $n+t(n)$ key-value pairs. This means we run at most $\mathrm{O}\left(n^{2}+t(n)^{2}\right)$ inner loop calls. It remains to be shown that one inner loop runs in polylogarithmic time. An inner loop just involves adding and multiplying $\mathrm{O}(\log n)$-bit numbers. $p$-bit numbers can be added in time $\mathrm{O}(p)=\mathrm{O}(\log n)$. Similarly, $p$-bit numbers can be multiplied in time $\mathrm{O}(p \log p) \leq \mathrm{O}\left(p^{2}\right)$, which comes out to $\mathrm{O}\left(\log ^{2}(n+t(n))\right)$ with $\log$ precision. Thus, one inner loop can be run in polylogarithmic time. We conclude that a transformer decoder with $t(n)$ intermediate steps can be simulated by a multitape Turing machine in time $\widehat{\mathrm{O}}\left(n^{2}+t(n)^{2}\right)$.</p>
<p>Our second upper bound relies on the $\mathrm{TC}^{0}$ upper bound for transformers without intermediate steps.
Theorem 4. $\operatorname{CoT}(t(n)) \subseteq \operatorname{SPACE}(t(n)+\log n)$.
Proof. Since log-precision transformers can be simulated in uniform $\mathrm{TC}^{0}$ (Merrill \&amp; Sabharwal, 2023b), they can be simulated in L, i.e., with at most $c \log n$ space overhead on inputs of size $n$. To compute $t(n)$ intermediate decoding steps of a transformer, we store a buffer of at most $t(n)$ generated tokens, which has size $\mathrm{O}(t(n))$. To compute the next token, we call the transformer with an input of size $\mathrm{O}(n+t(n))$ using at most $c \log (n+t(n))$ space overhead. We then clear the memory used and append the finite token generated to the input buffer. It follows from this algorithm that</p>
<p>$$
\begin{aligned}
\operatorname{CoT}(t(n)) &amp; \subseteq \operatorname{SPACE}(t(n)+c \log (n+t(n))) \
&amp; \subseteq \operatorname{SPACE}(t(n)+\log n)
\end{aligned}
$$</p>
<p>With at least $\Omega(\log n)$ steps, this upper bound can be simplified to $\operatorname{SPACE}(t(n))$. The $t(n)=\Theta(n)$ case establishes the context-sensitive languages as an upper bound for transformers with linear steps. Given our $\operatorname{TIME}(t(n))$ lower bound (Theorem 2), the tightest possible space upper bound without making fundamental complexity advances would be $\operatorname{SPACE}(t(n) / \log t(n))$ (Hopcroft et al., 1977). Conversely, our lower bound can only be tightened to $\operatorname{TIME}(t(n) \log t(n))$.
On the other hand, with only $\mathrm{O}(\log n)$ decoding steps, intermediate decoding does not increase expressive power much beyond $\mathrm{TC}^{0}$, because the upper bound simplifies to $\operatorname{SPACE}(t(n))=\mathrm{L}$. Thus, under standard assumptions, transformers with a logarithmic number of decoding steps cannot solve directed graph connectivity, Horn formula satisfiability, or other NL- or P-complete problems. Yet, they may be able to solve L-complete problems, unlike transformers without decoding steps.</p>
<h1>5 CONCLUSION</h1>
<p>We have shown that intermediate decoding steps extend the formal power of transformers well beyond previously known upper bounds, such as $\mathrm{TC}^{0}$ circuits and $\mathrm{FO}(\mathrm{M})$ logic, on transformers without intermediate decoding. Further, the amount of additional power is closely related to the number of decoding steps. In particular, transformers with a linear number of decoding steps have the capacity to recognize regular languages, but cannot recognize languages beyond context-sensitive. With a $\log$ number of decoding steps, such transformers can only recognize languages in $L$, which is a complexity class relatively close to $\mathrm{TC}^{0}$. Thus, it appears that a linear number of intermediate decoding steps may be required to overcome the limitations of transformers on many sequential reasoning problems of interest. In future work, it may be possible to derive a strict separation between transformers with a log and a linear number of decoding steps and show that certain problems that currently have a quadratic bound can in fact be solved with a roughly linear number of steps.
We have focused on expressive power, rather than analyzing learnability. Whereas our upper bounds directly reveal limitations on what transformers with intermediate generation can learn, one caveat is that our lower bounds do not directly imply transformers can learn to use intermediate steps effectively. It would be interesting to formally investigate transformers with CoT from a learningtheoretic lens, possibly along the lines of Malach (2023), and how different kinds of fine-tuning, such as reinforcement learning, might better allow models to use the power of chain of thought.</p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>We thank David Chiang for the valuable feedback and for identifying a mismatch between the transformer definition in an earlier version of this paper and standard pre-norm transformers. We also appreciate helpful comments from Gabriel Faria, Ofir Press, Abulhair Saparov, Jason Wei, and Avi Wigderson, as well as researchers in ML2 at NYU and at AI2. WM was supported by NSF Award 1922658, an NSF Graduate Research Fellowship, and AI2.</p>
<h2>REFERENCES</h2>
<p>David Chiang. Personal communication, March 2024.
David Chiang, Peter Cholak, and Anand Pillay. Tighter bounds on the expressivity of transformer encoders. In ICML, 2023.</p>
<p>Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaïd Harchaoui, and Yejin Choi. Faith and fate: Limits of transformers on compositionality. In NeurIPS, 2023.</p>
<p>Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: A theoretical perspective. In NeurIPS, 2023.</p>
<p>Sophie Hao, Dana Angluin, and Roberta Frank. Formal language recognition by hard attention transformers: Perspectives from circuit complexity. TACL, 10:800-810, 2022.</p>
<p>John E. Hopcroft, Wolfgang J. Paul, and Leslie G. Valiant. On time versus space. J. ACM, 24: 332-337, 1977.</p>
<p>John E Hopcroft, Rajeev Motwani, and Jeffrey D Ullman. Introduction to automata theory, languages, and computation. ACM Sigact News, 32(1):60-65, 2001.</p>
<p>S-Y Kuroda. Classes of languages and linear-bounded automata. Information and control, 7(2): 207-223, 1964.</p>
<p>Lillian Lee. Fast context-free grammar parsing requires fast boolean matrix multiplication. J. ACM, 49(1):1-15, Jan 2002.</p>
<p>Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. In $I C L R, 2023$.</p>
<p>Eran Malach. Auto-regressive next-token predictors are universal learners. arXiv, abs/2309.06979, 2023.</p>
<p>William Merrill. On the linguistic capacity of real-time counter automata. arXiv, abs/2004.06866, 2020.</p>
<p>William Merrill. Formal languages and neural models for learning on sequences. In François Coste, Faissal Ouardi, and Guillaume Rabusseau (eds.), ICGI, volume 217 of PMLR, Jul 2023.</p>
<p>William Merrill and Ashish Sabharwal. A logic for expressing log-precision transformers. In NeurIPS, 2023a.</p>
<p>William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. TACL, 11:531-545, 2023b.</p>
<p>William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah A. Smith. Effects of parameter norm growth during transformer training: Inductive bias from gradient descent. In EMNLP, 2021.</p>
<p>William Merrill, Ashish Sabharwal, and Noah A. Smith. Saturated transformers are constant-depth threshold circuits. TACL, 10:843-856, 2022.</p>
<p>Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. arXiv, abs/2112.00114, 2021.</p>
<p>Jorge Pérez, Pablo Barceló, and Javier Marinkovic. Attention is Turing complete. JMLR, 22(1), January 2021.</p>
<p>Dale Schuurmans. Memory augmented large language models are computationally universal. ArXiv, 2023.</p>
<p>Lena Strobl, William Merrill, Gail Weiss, David Chiang, and Dana Angluin. Transformers as recognizers of formal languages: A survey on expressivity. TACL, 2024.</p>
<p>Leslie G. Valiant. General context-free recognition in less than cubic time. Journal of Computer and System Sciences, 10(2):308-315, 1975.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), NeurIPS, 2022.</p>
<p>Gail Weiss, Yoav Goldberg, and Eran Yahav. On the practical computational power of finite precision RNNs for language recognition. In ACL, July 2018.</p>
<p>Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In ICML, 2021.
Avi Wigderson. The complexity of graph connectivity. In International Symposium on Mathematical Foundations of Computer Science, pp. 112-132. Springer, 1992.</p>
<p>Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture. In ICML, 2020.</p>
<p>Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Self-attention networks can process bounded hierarchical languages. In ACL, 2021.</p>
<p>Biao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, 2019.
Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith. How language model hallucinations can snowball. arXiv, 2023.</p>
<h1>A TRANSFORMER COMPONENTS</h1>
<p>This section formally defines our generalizations of pre-norm and then recalls the definition from Merrill \&amp; Sabharwal (2023a) for the components of the transformer layer.</p>
<h2>A. 1 GENERALIZED PRE-NORM</h2>
<p>We assume a pre-norm (Xiong et al., 2020) parameterization of the transformer for concreteness, as this is more standard in newer transformers. As stated in Section 2.1, we allow transformer sublayers to apply a linear projection before layer-norm. Concretely, we define proj_layer_norm(v) as follows:
Definition 4 (Projected pre-norm). Let $\mathbf{M}: \mathbb{R}^{m} \rightarrow \mathbb{R}^{m}$ be a parameter matrix that projects $m$ dimensional vectors to $m$-dimensional vectors.</p>
<p>$$
\text { proj_layer_norm }(\mathbf{v} ; \mathbf{M})=\text { layer_norm }(\mathbf{M} \mathbf{v})
$$</p>
<p>We will omit the parameter M for convenience, instead writing proj_layer_norm(v).
Our proofs also allow multiple pre-norms of different projections of the hidden state for our lowerbound constructions. Concretely, we define multi_layer_norm(v) as follows.</p>
<p>Definition 5 (Multi-pre-norm). Let $m$ be divisible by $k$. For $1 \leq i \leq k$, let $\mathbf{M}<em i="1">{i}: \mathbb{R}^{m} \rightarrow \mathbb{R}^{m / k}$ be a parameter matrix that projects $m$-dimensional vectors to $m / k$-dimensional vectors. Let $\langle\cdot\rangle</em>$ is defined as}^{k}$ denote iterated concatenation. The multi-pre-norm of $\mathbf{v} \in \mathbb{R}^{m</p>
<p>$$
\text { multi_layer_norm }\left(\mathbf{v} ; \mathbf{M}<em k="k">{1}, \ldots \mathbf{M}</em>}\right)=\left\langle\text { proj_layer_norm }\left(\mathbf{v} ; \mathbf{M<em i="1">{i}\right)\right\rangle</em>
$$}^{k</p>
<p>As for projected pre-norm, we will omit the parameters $\mathbf{M}<em k="k">{1}, \ldots \mathbf{M}</em>)$.}$ for multi-pre-norm, instead writing multi_layer_norm $(\mathbf{v</p>
<p>As noted earlier, projected pre-norm can simulate multi-pre-norm:
Proposition 1 (Chiang, 2024). Multi-pre-norm with $k$ norms can be simulated by $k+1$ projected pre-norm layers.</p>
<p>Proof. We will simulate a multi-pre-norm layer that takes as input</p>
<p>$$
\text { layer_norm }\left(\mathbf{M}<em k="k">{1} \mathbf{v}\right), \ldots, \text { layer_norm }\left(\mathbf{M}</em>\right)
$$} \mathbf{v</p>
<p>using only projected pre-norm layers. The idea is to use $k$ different projected pre-norm layers (one for each input layer norm). At layer $i$, the layer takes as input layer_norm $\left(\mathbf{M}_{i} \mathbf{v}\right)$ and writes this to the residual stream. Then, after these $k$ layers, a final layer reads as input</p>
<p>$$
\text { layer_norm }\left(\left\langle\text { layer_norm }\left(\mathbf{M}<em i="1">{i} \mathbf{v}\right)\right\rangle</em>\right)
$$}^{k</p>
<p>Since each vector in the concatenation has unit norm, this is equivalent to</p>
<p>$$
\frac{1}{\sqrt{k}}\left\langle\text { layer_norm }\left(\mathbf{M}<em i="1">{i} \mathbf{v}\right)\right\rangle</em>
$$}^{k</p>
<p>It follows that this layer receives essentially the same input as the original multi-pre-norm layer, up to a constant factor. The original weights for the layer can be multiplied by $\sqrt{k}$ to implement the same computation as the original layer. To make sure that $\sqrt{k}$ is exactly representable, we can pad the number of entries so that $k$ is a perfect square.</p>
<h1>A. 2 TRANSFORMER EMBEDDINGS</h1>
<p>For each position $1 \leq i \leq n$, the transformer embedding function represents token $\sigma_{i} \in \Sigma$ and its position $i$ with a vector. Let $\mathbf{V}$ be an embedding matrix of size $|\Sigma| \times m$ where each row represents the embedding for some $\sigma$. Let $f: \mathbb{N} \rightarrow \mathbb{D}_{p}^{m}$ be computable in time $\mathrm{O}(\log n)$. Then,</p>
<p>$$
e\left(\sigma_{i}, i\right)=\mathbf{v}<em i="i">{\sigma</em>+f(i)
$$}</p>
<h2>A. 3 SELF ATTENTION</h2>
<p>The two components of the self attention block are $s$, the similarity function, and $v$, the value function. Let $\mathbf{h}<em i="i">{i}$ be the hidden state at the previous layer and $\hat{\mathbf{h}}</em>\right)$. We define similarity of keys and queries as follows:}=$ multi_layer_norm $\left(\mathbf{h}_{i</p>
<p>$$
s\left(\mathbf{h}<em j="j">{i}, \mathbf{h}</em>}\right)=\exp \left(\frac{\mathbf{q<em i="i">{i}^{\top} \mathbf{k}</em>
\mathbf{q}}}{\sqrt{m / h}}\right), \quad \text { where } \begin{array}{l<em q="q">{i}=\mathbf{W}</em>} \hat{\mathbf{h}<em q="q">{i}+\mathbf{b}</em> \
\mathbf{k}<em k="k">{i}=\mathbf{W}</em>} \mathbf{h<em k="k">{i}+\mathbf{b}</em>
\end{array}
$$</p>
<p>Then the value function is defined $v\left(\mathbf{h}<em h="h">{i}\right)=\mathbf{W}</em>} \hat{\mathbf{h}<em h="h">{i}+\mathbf{b}</em>$.</p>
<h2>A. 4 ACTIVATION BLOCK</h2>
<p>The activation function $f$ encapsulates the aggregation of the attention head outputs and the feedforward subnetwork of the transformer. $f$ takes as input attention head outputs $\mathbf{a}<em h="h" i_="i,">{i, 1}, \ldots, \mathbf{a}</em>} \in \mathbb{D<em i="i">{p}^{m / h}$ and the previous layer value $\mathbf{h}</em>$.</p>
<p>The first part of the activation block simulates the pooling part of the self-attention sublayer. The head outputs are first concatenated to form a vector $\mathbf{a}_{i}$, which is then passed through an affine</p>
<p>transformation $\left(\mathbf{W}<em o="o">{o}, \mathbf{b}</em>}\right): \mathbb{D<em p="p">{p}^{m} \rightarrow \mathbb{D}</em>}^{m}$ followed by residual connections to form the sublayer output $\mathbf{o<em p="p">{i} \in \mathbb{D}</em>$ :}^{m</p>
<p>$$
\mathbf{o}<em o="o">{i}=\mathbf{W}</em>} \mathbf{a<em o="o">{i}+\mathbf{b}</em>
$$}+\mathbf{h}_{i</p>
<p>The second part of the activation block first applies multi-layer-norm and then simulates the feedforward subnetwork to compute the next layer vector $\mathbf{h}<em i="i">{i}^{\prime}$. Let $\bar{\mathbf{o}}</em>}=$ multi_layer_norm $\left(\mathbf{o<em 1="1">{i}\right)$. Let $\sigma$ be a nonlinearity computable in linear time on its input (in the most standard transformer, ReLU). Then, for affine transformations $\left(\mathbf{W}</em>}, \mathbf{b<em p="p">{1}\right): \mathbb{D}</em>}^{m} \rightarrow \mathbb{D<em 2="2">{p}^{w}$ and $\left(\mathbf{W}</em>}, \mathbf{b<em p="p">{2}\right): \mathbb{D}</em>$, the feedforward subnetwork can be defined as:}^{w} \rightarrow \mathbb{D}_{p}^{m</p>
<p>$$
\mathbf{h}<em 2="2">{i}^{\prime}=\mathbf{W}</em>} \sigma\left(\mathbf{W<em i="i">{1} \bar{\mathbf{o}}</em>}+\mathbf{b<em 2="2">{1}\right)+\mathbf{b}</em>
$$}+\mathbf{o}_{i</p>
<h1>B Turing MACHines</h1>
<p>A Turing machine takes as input a string $\sigma \in \Sigma^{*}$. A configuration of a Turing machine is a finite state $q$ along with the contents of an input tape $c^{0}, k$ work tapes $c^{1}, \ldots, c^{k}$, and an output tape $c^{k+1}$. Finally, for each tape $\tau$, a configuration specifies a head position $h^{\tau}$. We start with the initial state $q_{0}$ and the input tape $c_{0}^{0}$ containing $\sigma$ starting at position 0 with infinite $b$ 's on each side, and $h_{0}^{0}=0$. All other tapes start containing all $b$ 's and with their head at 0 . At each time step $i$, if $q_{i} \notin F$, we recurrently update the configuration by first computing:</p>
<p>$$
\left\langle q_{i+1}, \gamma_{i}^{1}, \ldots, \gamma_{i}^{k+1}, d_{i}^{0}, \ldots, d_{i}^{k+1}\right\rangle=\delta\left(q_{i}, c_{i}^{0}\left[h_{i}^{0}\right], \ldots, c_{i}^{k+1}\left[h_{i}^{k+1}\right]\right)
$$</p>
<p>We then update tape $\tau$ by setting $c_{i+1}^{\tau}\left[h_{i}^{j}\right]=\gamma_{i}^{j}$ and keeping all other tape cells the same. We update the head position on tape $\tau$ according to $h_{i+1}^{\tau}=h_{i}^{\tau}+d_{i}^{\tau}$. On the other hand, if $q_{i} \in F$, the Turing machine halts and outputs the string of tokens on the output tape from the current head position on the left up to (but not including) the first $b$ on the right. A Turing machine can also be viewed as a language recognizer if we set $\Sigma={0,1}$ and check if the first output token is 0 or 1 .</p>
<h2>C LAYER-NORM HASH</h2>
<p>Lemma 1 (Scale invariance). For any $x \in \mathbb{R}$ and $i \in \mathbb{R}<em x="x">{&gt;0}, \phi(x / i, 1 / i)=\phi</em>$.
Proof. Let $\mathbf{v}<em x="x">{x}=\langle x / i, 1 / i,-x / i,-1 / i\rangle . \mathbf{v}</em>$ is constructed with mean 0 , so layer-norm reduces to RMS-norm (Zhang \&amp; Sennrich, 2019). Thus,</p>
<p>$$
\begin{aligned}
\phi(x / i, 1 / i) &amp; =\mathbf{v}<em x="x">{x} /\left|\mathbf{v}</em>\right| \
&amp; =\mathbf{v}_{x} \cdot \frac{i}{\sqrt{2 x^{2}+2}} \
&amp; =\frac{1}{\sqrt{2 x^{2}+2}}\langle x, 1,-x,-1\rangle \
&amp; =\phi(x, 1)
\end{aligned}
$$</p>
<p>which, by definition, is $\phi_{x}$.
Lemma 2 (Equality check via layer-norm hash). For any $q, k \in \mathbb{R}, \phi_{q} \cdot \phi_{k}=1$ if and only if $q=k$.
Proof. By the definition of layer-norm hash, we have</p>
<p>$$
\begin{aligned}
\phi(q, 1) \cdot \phi(k, 1) &amp; =\frac{1}{\sqrt{2 q^{2}+2}}\langle q, 1,-q,-1\rangle \cdot \frac{1}{\sqrt{2 k^{2}+2}}\langle k, 1,-k,-1\rangle \
&amp; =\frac{2 q k+2}{\sqrt{\left(2 q^{2}+2\right)\left(2 k^{2}+2\right)}} \
&amp; =\frac{q k+1}{\sqrt{\left(q^{2}+1\right)\left(k^{2}+1\right)}}
\end{aligned}
$$</p>
<p>The inner product of unit-norm vectors is maximized at 1 . In this case, we show that it achieves 1 only when $q=k$, meaning that is the unique maximum:</p>
<p>$$
\begin{aligned}
1 &amp; =\frac{q k+1}{\sqrt{\left(q^{2}+1\right)\left(k^{2}+1\right)}} \
(q k+1)^{2} &amp; =\left(q^{2}+1\right)\left(k^{2}+1\right) \
(q k)^{2}+2 q k+1 &amp; =(q k)^{2}+q^{2}+k^{2}+1 \
2 q k &amp; =q^{2}+k^{2} \
0 &amp; =(q-k)^{2}
\end{aligned}
$$</p>
<p>We conclude that $\phi_{q} \cdot \phi_{k}$ is maximized (to 1 ) if and only if $q=k$.</p>
<p>As the layer-norm hash is constructed to have mean 0 , it does not require a fully general layer-norm implementation and can, in fact, be implemented with simplified RMS norm (Zhang \&amp; Sennrich, 2019).</p>
<h1>D InPut Tape Retrieval via the Layer-Norm Hash</h1>
<p>We now describe an attention head that uses the layer-norm hash to read from the input tape in Theorem 2. Define a sequence $h_{1}, \ldots, h_{i-1}$, which represents Turing machine tape position in Theorem 2.</p>
<p>We define the following layer-norm hash based quantity, which is instantiated in Theorem 2 in a particular way:</p>
<p>$$
\phi_{i}= \begin{cases}\phi(i, 1) &amp; \text { if } 1 \leq i \leq n \ \phi\left(h_{i}, 1\right) &amp; \text { otherwise }\end{cases}
$$</p>
<p>The attention head we construct can then be described as follows:</p>
<ul>
<li>Query: $\left\langle\phi_{i},-1\right\rangle$</li>
<li>Key: $\left\langle\phi_{j}, 1 \mid n&lt;j\right\rangle\rangle$</li>
<li>Value: $\mathbf{v}<em j="j">{j} \triangleq\left\langle\phi</em>\right\rangle$}, \sigma_{j</li>
</ul>
<p>Let $\overline{\mathbf{v}} \triangleq\langle\bar{\phi}, \bar{\sigma}\rangle$ be the head output. This head obeys the following properties:
Lemma 3. Let $i&gt;n$. Then, $1 \leq h_{i} \leq n$ if and only if $\bar{\phi}=\phi_{i}$.
Proof. We proceed by considering the two directions.
Forward Direction. The query-key inner product has two terms $\kappa_{i j}^{1}+\kappa_{i j}^{2}$. By Lemma 2, $\kappa_{i j}^{1}$ is maximized either when $h_{i}^{0}=j$ (and $1 \leq j \leq n$ ) or when $h_{i}=h_{j}$ (and $n&lt;j$ ). However, if $n&lt;j$, the second term $\kappa_{i j}^{2}=1$. Thus, the attention score is maximized uniquely when $j=h_{i}$, so the head retrieves $\overline{\mathbf{v}}=\left\langle\phi\left(h_{i}, 1\right), \sigma_{h_{i}}\right\rangle$. Thus, $\bar{\phi}=\phi\left(h_{1}, 1\right)=\phi_{i}$.
Backward Direction. We establish bidirectionality by proving the contrapositive. Assume that either $h_{i}&lt;1$ or $h_{i}&gt;n$. The head retrieves $\bar{\phi}=\frac{1}{|M|} \sum_{j \in M} \phi(j, 1)$ for some $M \subseteq{1, \ldots, n}$. It holds that, for all $1 \leq j \leq n, h_{i}<j$, or the other way around (i.e., for all $1 \leq j \leq n, h_{i}>j$ ). Thus, by Lemma 5, $\bar{\phi} \neq \phi\left(h_{i}, 1\right)=\phi_{i}$.</p>
<p>The following property also emerges from the proof of the forward direction in Lemma 3:
Lemma 4. Let $i&gt;n$. Then, if $1 \leq h_{i} \leq n, \bar{\sigma}=\sigma_{h_{i}}$.
The backward direction in Lemma 3 relies on the following lemma:
Lemma 5. Let $q \in \mathbb{Z}$ and $k_{j} \in \mathbb{Z}$ for $1 \leq j \leq m$. Let $\succ \in{&lt;,&gt;}$. If, for all $j, q \succ k_{j}$, then</p>
<p>$$
\phi_{q} \neq \frac{1}{m} \sum_{j=1}^{m} \phi_{k_{j}}
$$</p>
<p>Proof. Recall that $\phi_{x}=\phi(x, 1) \in \mathbb{R}^{4}$ has first element $x / \sqrt{2 x^{2}+2}$, which we will denote as $z_{x}$. Observe that $z_{x}$ is a monotonically increasing function of $x \in \mathbb{R}$. Thus, $z_{q} \succ z_{k_{j}}$ for $1 \leq j \leq m$, which implies $z_{q} \succ \frac{1}{m} \sum_{j=1}^{m} z_{k_{j}}$, from which the lemma conclusion follows.</p>
<h1>E Rightmost Retrieval via the Layer-Norm HASn</h1>
<p>We now describe an attention head that can attend to the rightmost token satisfying a certain property, capturing the construction in Theorem 2 to retrieve the most recent write to a Turing machine work tape. Define a sequence $h_{1}, \ldots, h_{i-1}$, which represents Turing machine tape position in Theorem 2. As is natural for Turing machine tapes, we assume that if $h \neq h_{i}$ for all $i$, then it must be that $h \prec h_{i}$ for all $i$, where $\prec$ is fixed as either $&gt;$ or $&lt;$.
Let $f(i)$ be a tie-breaking term that we will define later in Appendix E.1. We define two layer-norm hash quantities:</p>
<p>$$
\begin{aligned}
&amp; \phi_{i} \triangleq \phi\left(h_{1} / i, 1 / i\right) \
&amp; \psi_{i} \triangleq \phi(f(i), 1)
\end{aligned}
$$</p>
<p>Recall that $e_{1}=\langle 1,0,0,0\rangle$. Construct an attention head as follows:</p>
<ul>
<li>Query: $\left\langle\phi_{i}, e_{1}\right\rangle$</li>
<li>Key: $\left\langle\phi_{j},-\psi_{j}\right\rangle$</li>
<li>Value: $\mathbf{v}<em j="j">{j} \triangleq\left\langle\phi</em>\right\rangle$}, \delta_{j</li>
</ul>
<p>Let $\overline{\mathbf{v}} \triangleq\langle\bar{\phi}, \bar{\delta}\rangle$ be the head output. The following properties hold for such a head:
Lemma 6. There exists $j&lt;i$ such that $h_{i}=h_{j}$ if and only if $\bar{\phi}=\phi_{i}$.</p>
<p>Proof. We proceed by considering the two directions.
Forward Direction. The query-key inner product has two terms $\kappa_{i j}^{1}+\kappa_{i j}^{2}$. By Lemma 2, the first term $\kappa_{i j}^{1}$ is maximized at 1 for each $j$ such that $h_{i}=h_{j}$. For $h_{i} \neq h_{j}, \kappa_{i j}^{1}&lt;1-1 /\left(2 i^{4}\right)$ by Lemma 8 . The second component $\kappa_{i j}^{2}$ monotonically increases with $j$ and satisfies $\kappa_{i j}^{2}&lt;f(i)&lt;1 /\left(2 i^{4}\right)$ by Lemma 10. Thus, the attention score is maximized for the largest $j&lt;i$ such that $h_{i}^{<em>}=h_{j}^{</em>}$. Thus, $\overline{\mathbf{v}}=\mathbf{v}<em j="j">{j}$ and $\bar{\phi}=\phi</em>$.
Backward Direction. We establish bidirectionality by proving the contrapositive. Assume there is no $j&lt;i$ such that $h_{i}=h_{j}$. Then $\bar{\phi}=\frac{1}{|M|} \sum_{j \in M} \phi\left(h_{j}, 1\right)$ for some $M \subseteq{1, \ldots, n}$. By assumption (top of Appendix E), we have $h_{j} \prec h_{i}$ for all $j&lt;i$. It follows from Lemma 5 that $\bar{\phi} \neq \phi\left(h_{i}, 1\right)=\phi_{i}$, completing the proof.}$ for this $j$, which means $\bar{\phi}=\phi_{i</p>
<p>The following property also emerges from the forward direction of the proof above:
Lemma 7. If there exists $j&lt;i$ such that $h_{i}=h_{j}$, then $\bar{\delta}=\delta_{j}$ for the greatest such $j$.</p>
<h2>E. 1 Tie-Breaking Term</h2>
<p>The construction above uses a tie-breaker that favors retrieving tokens further to the right. We will justify the construction of such a tie-breaking term here. To begin, we will establish a bound on the layer-norm hash inner product similarity for inexact matches.</p>
<p>Lemma 8. For any $i \geq 2, \phi(i, 1) \cdot \phi(i-1,1) \leq 1-1 /\left(2 i^{4}\right)$.</p>
<p>Proof. Consider the squared dot product:</p>
<p>$$
\begin{aligned}
(\phi(i, 1) \cdot \phi(i-1,1))^{2} &amp; =\frac{(\langle i, 1,-i,-1\rangle \cdot\langle i-1,1,-(i-1),-1\rangle)^{2}}{\left(2 i^{2}+2\right)\left(2(i-1)^{2}+2\right)} \
&amp; =\frac{(i(i-1)+1)^{2}}{\left(i^{2}+1\right)\left((i-1)^{2}+1\right)} \
&amp; =\frac{i^{2}(i-1)^{2}+2 i(i-1)+1}{i^{2}(i-1)^{2}+i^{2}+(i-1)^{2}+1} \
&amp; =\frac{i^{2}(i-1)^{2}+2 i(i-1)+1}{i^{2}(i-1)^{2}+2 i(i-1)+2} \
&amp; =1-\frac{1}{i^{2}(i-1)^{2}+2 i(i-1)+2} \
&amp; =1-\frac{1}{i^{4}-2 i^{3}+3 i^{2}-2 i+2}
\end{aligned}
$$</p>
<p>Since $(1-y / 2)^{2} \geq 1-y$ for any $y$, we have $\sqrt{1-y} \leq 1-y / 2$ for any $y \leq 1$. Applying this to the right hand side of the above equation, we obtain:</p>
<p>$$
\begin{aligned}
\phi(i, 1) \cdot \phi(i-1,1) &amp; =\sqrt{1-\frac{1}{i^{4}-2 i^{3}+3 i^{2}-2 i+2}} \
&amp; \leq 1-\frac{1}{2 i^{4}-4 i^{3}+6 i^{2}-4 i+4} \
&amp; \leq 1-\frac{1}{2 i^{4}} \text { for } i \geq 2
\end{aligned}
$$</p>
<p>which completes the proof.</p>
<p>To break ties in attention, we aim to construct a function of $i$ that is computable in the transformer, monotonically decreasing with $i$, and smaller than $1 /\left(2 i^{4}\right)$. The following definition will accomplish this:</p>
<p>Definition 6. We define the following inductively:</p>
<p>$$
\begin{aligned}
f(i, 0) &amp; =1 / i \
f(i, k+1) &amp; =f(i-1, k)-f(i, k)
\end{aligned}
$$</p>
<p>By construction, $f(i, k)$ is monotonically increasing and a linear combination of $1 / i, 1 /(i-$ $1), \ldots, 1 /(i-k)$. The latter property means it is computable by a single multihead self-attention layer. To do this, we construct $k$ heads in parallel, where head $h$ attends to all tokens besides the first $h$ and puts value 1 at token $h+1$ and 0 elsewhere. ${ }^{10}$ Head $h$ thus computes $1 /(i-h)$. We use the linear transformation at the end of the layer to compute $f(i, k)$ via a linear combination of the head outputs.
Lemma 9. For any $k$ and $i&gt;k$, we have</p>
<p>$$
f(i, k)=\frac{k!}{\prod_{j=0}^{k}(i-j)}
$$</p>
<p>Proof. By induction over $k$.
Base Case: $i=0$. We have $f(i, 0)=0!/ i$.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Inductive Case. We analyze the form of $f(i, k+1)$ :</p>
<p>$$
\begin{aligned}
f(i, k+1) &amp; =f(i-1, k)-f(i, k) \
&amp; =\frac{k!}{\prod_{j=0}^{k}(i-1-j)}-\frac{k!}{\prod_{j=0}^{k}(i-j)} \
&amp; =k!\cdot \frac{\prod_{j=0}^{k}(i-j)-\prod_{j=0}^{k}(i-1-j)}{i \prod_{j=1}^{k}(i-j)^{2}(i-k-1)} \quad \text { (Inductive assumption) } \
&amp; =k!\cdot \frac{i \prod_{j=1}^{k}(i-j)-(i-k-1) \prod_{j=1}^{k}(i-j)}{i \prod_{j=1}^{k}(i-j)^{2}(i-k-1)} \
&amp; =k!\cdot \frac{(k+1) \prod_{j=1}^{k}(i-j)}{i \prod_{j=1}^{k}(i-j)^{2}(i-k-1)} \
&amp; =\frac{(k+1)!}{i \prod_{j=1}^{k}(i-j)(i-k-1)} \
&amp; =\frac{(k+1)!}{\prod_{j=0}^{k+1}(i-j)}
\end{aligned}
$$</p>
<p>(Inductive assumption)
(Form common denominator)
(Pull out factors)
(Distributive property)
(Simplify)
It remains to be shown that $f(i, k)$ can be made smaller than $1 /\left(2 i^{4}\right)$. To handle edge cases around small values of $i$, we define:
Definition 7. Let $\epsilon=10^{-10}$. For $i \geq 1$, let</p>
<p>$$
f(i)= \begin{cases}1 / 1000-\epsilon i &amp; \text { if } i \leq 4 \ f(i, 3) / 100 &amp; \text { if } i \geq 5\end{cases}
$$</p>
<p>Lemma 10. For $i \geq 1, f(i)&lt;1 /\left(2 i^{4}\right)$.
Proof. When $i \leq 4$, we have $f(i)&lt;1 / 1000&lt;1 /\left(2 i^{4}\right)$.
When $i \geq 5$, by Lemma 9 , we have:</p>
<p>$$
\frac{f(i, 3)}{100}=\frac{3!}{100 i(i-1)(i-2)(i-3)}
$$</p>
<p>It can be verified that the values of $i$ for which this expression equals $1 /\left(2 i^{4}\right)$ are all in the interval $[0,5)$, and that for $i \geq 5,(100 / 6) i(i-1)(i-2)(i-3)&gt;2 i^{4}$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ This head can be implemented by setting a flag in the previous layer at each $i$ for whether $i \leq h$ by hardcoding a comparison between $\phi(1,1 / i)$ and $\phi(h, 1)$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1935 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1935</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1935</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-278714573</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.11214v1.pdf" target="_blank">Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions</a></p>
                <p><strong>Paper Abstract:</strong> Vision-Language-Action (VLA) models have recently become highly prominent in the field of robotics. Leveraging vision-language foundation models trained on large-scale internet data, the VLA model can generate robotic actions directly from visual observations and human instructions through a single end-to-end neural network. Despite their effectiveness, current VLA models usually accept only one form of human prompting, language instructions, which may constrain their applicability in open-ended human-robot interactions. For example, a user might expect the robot to retrieve an object shown in an image, follow an instruction written on the whiteboard, or imitate a behavior demonstrated in a video, rather than relying solely on language-based descriptions. To address this gap, we introduce OE-VLA, which explores the potential of VLA models for open-ended multimodal instructions. Extensive results demonstrate that our OE-VLA not only achieves comparable performance to traditional VLA models with linguistic input but also delivers impressive results across four additional categories of open-ended tasks. The proposed methodology could significantly expand the applications of VLA models across various everyday scenarios and facilitate human-robot interaction.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1935.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1935.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OE-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-Ended Vision-Language-Action (OE-VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action model finetuned to accept open-ended multimodal instructions (interleaved text + images + short video frames) and output discretized robot action tokens for long-horizon manipulation; trained with a two-stage curriculum (multi-image grounding then open-ended instruction tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OE-VLA (Interleave-1B and Interleave-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses a SigLIP-400M ViT vision encoder to produce visual tokens mapped by a 2-layer MLP into the hidden space of a Qwen-1.5 LLM backbone; concatenates observation tokens, interleaved text and instruction images into a single sequence consumed by the LLM which autoregressively emits discretized action tokens (continuous actions quantized into 256 bins). No separate diffusion policy head was used; actions predicted as 5-step action chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Component-wise: vision-language pretraining for SigLIP (CLIP-style), text-only pretraining for Qwen-1.5, and foundation multi-image finetuning (LLaVA-Next-Interleave). OE-VLA itself is finetuned on robot demonstration data.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Vision encoder (SigLIP) trained on image–text pairs (CLIP-style). LLaVA-Next-Interleave foundation was finetuned on multi-image tasks (visual storytelling, video captioning, difference identification). Stage-1 uses the MGrounding dataset composed of grounding tasks (object tracking, referring grounding, group grounding, free-form multi-image grounding). Stage-2 uses curated robot manipulation datasets converted to multimodal instruction format.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Long-horizon robotic manipulation (CALVIN / OE-CALVIN benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Manipulation tasks from the CALVIN suite (real-world robotics benchmark): long-horizon sequences of 5 consecutive subtasks across 34 tasks. Inputs: static workspace view + wrist camera; instruction modalities include pure text, object images (VOS), optical-instruction images (OIF), single-image goals (VGR), and short video demonstrations (VDL). Action space: originally continuous robot actions discretized into 256 bins and emitted as 5-step chunks. Evaluations use ABC→D and ABCD→D environment splits.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper explicitly discusses semantic alignment issues: foundation VLM tasks (storytelling, captioning) do not fully match robotic manipulation needs (spatial relationships, affordances), motivating Stage-1 multi-image grounding to increase overlap; training data explicitly includes spatial relationships in MGrounding and robot demonstrations, so alignment is moderate after the two-stage finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported empirical results (behavioral metrics after finetuning): On CALVIN ABC→D split (language-conditioned baseline evaluation) OE-VLA Interleave-1B average successful sequence length = 2.70; OE-VLA Interleave-7B = 2.99 (Table 1). On OE-CALVIN base (open-ended multimodal) ABC→D average lengths: OE-VLA 1B = 2.75, OE-VLA 7B = 3.48 (per-task high performance on VOS/OIF/VDL). On OE-CALVIN hard (more out-of-distribution multimodal instructions) average lengths: OE-VLA 1B = 1.79, OE-VLA 7B ≈ 2.68 (derived from per-task lengths VOS/OIF/VGR/VDL reported in Table 3). Per-task LH-1 success rates reported (examples): OE-VLA 1B VOS LH-1 = 94.0%, VDL LH-1 = 93.7%; OE-VLA 7B VOS LH-1 = 95.0%, VDL LH-1 = 93.3% (see Tables 2 and 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>The paper does not present a direct quantitative sample-efficiency comparison (e.g., episodes-to-performance) between language-pretrained versus non-language-pretrained models. Authors report dataset construction choices (selected ~40% of raw robot data per task, resulting in a training set ≈2× original size) and training for one epoch for fair comparison, and that Stage-1 grounding reduces domain-gap issues, but no explicit episode/sample counts to reach performance are given.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No internal attention-visualization or attention-pattern analyses are reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>The paper does not present analyses of embedding spaces, clustering, or representational geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Behavioral evidence: improved performance on multimodal tasks (especially Visual Object Specification and Video Demo Learning) after Stage-1 grounding and Stage-2 finetuning indicates that the model grounds instruction images to actions at a functional level. However, the paper provides no mechanistic/internal analyses (e.g., probes linking verbs to affordance features or motor primitives).</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No explicit analysis of hierarchical feature levels (low-level vs high-level) is provided; the Stage-1 objective is motivated to improve spatial/high-level grounding but this is argued rather than shown with layer-wise evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer success depends on (a) model scale (7B > 1B), (b) domain similarity between training and test environment (stage-1 pretraining helps more in ABC→D split where target D is out-of-distribution), and (c) instruction modality distribution (models trained with a high proportion of image-based instructions improve on multimodal tasks but that can reduce performance on pure-text tasks unless balanced). The paper reports Stage-1 grounding helps more when evaluation environment is OOD and less when training data already contains environment D.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Indirect comparison via OE-CALVIN base vs hard: performance degrades on OE-CALVIN hard (instructions using web images, handwritten fonts, diverse viewpoints), indicating lower performance on more novel/unseen instruction sources; e.g., OE-VLA 7B average length drops from 3.48 (base) to ≈2.68 (hard). No explicit per-object 'seen vs unseen' numerical split is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>The model demonstrates generalization to out-of-distribution multimodal instructions (OE-CALVIN hard) after finetuning, but the paper does not present pure zero-shot experiments (i.e., no finetuning) or controlled few-shot ablations quantifying how many demonstrations are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer-wise ablation, freezing, or probing analyses are reported to identify which layers/components contribute most to transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Yes — authors report that the higher proportion of image-based multimodal training samples in their recipe correlates with a relative drop in performance on purely textual instructions; training models exclusively on textual instructions (OE-VLA-text) improves performance on linguistic inputs (Table 4). This is presented as evidence of a trade-off in the data recipe.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct controlled comparison to a vision-only-pretrained model (e.g., pure ImageNet or self-supervised vision) is reported. The paper does compare multi-view interleaved-input models vs single-view (OpenVLA single-view performed worse), suggesting multi-view visual inputs and VLM components help, but not an explicit vision-only pretraining ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Authors report the two-stage curriculum and show that Stage-1 improves early transfer for the ABC→D split; however, no detailed training dynamics (e.g., learning curves over epochs/steps) or representation evolution analyses are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No explicit measurements of representation dimensionality or intrinsic dimension (PCA / effective rank) are reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1935.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1935.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-NeXT-Interleave</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-NeXT-Interleave</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-image capable vision-language foundation model (selected as the OE-VLA foundation) that supports free-form multi-image input and has been finetuned on interleaved image/text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-Next-Interleave</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal VLM that handles interleaved multi-image and text inputs; used as the base/foundation model for OE-VLA because of its multi-image handling capability.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining followed by finetuning on multi-image tasks (visual storytelling, video captioning, difference identification); used as a foundation for downstream finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Finetuned on datasets for visual storytelling, video captioning, multi-image difference tasks — tasks that focus on image-text alignment but not specifically robotic affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Foundation for robot manipulation finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Serves as the multimodal encoder/LLM backbone basis that is later finetuned on robot manipulation data to produce OE-VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper notes a mismatch between LLaVA-Next-Interleave tasks (storytelling/captioning) and robotic manipulation requirements (spatial relationships/affordances), motivating Stage-1 grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Used as foundation; Stage-1 grounding is intended to bridge task-type mismatch to improve transfer to manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1935.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1935.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SigLIP-400M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SigLIP-400M Vision Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CLIP-style vision transformer trained with a sigmoid loss (SigLIP) used as the vision encoder in OE-VLA; encodes images into visual tokens at 384×384 resolution with 14×14 patching.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sigmoid Loss for Language Image Pre-Training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SigLIP-400M (ViT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision Transformer (ViT) trained in a CLIP-style vision-language pretraining paradigm with a sigmoid loss; produces visual token sequences used by the LLM backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining on image-text pairs (CLIP-style).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Standard image–text pairs used for CLIP-style pretraining; paper does not enumerate exact dataset composition (no claim of action verbs/affordances coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Visual encoding for robot manipulation finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Encodes observation images (static and wrist) and instruction images/frames used as input to the LLM which predicts action tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Component provides general image–text-alignment capability; authors add Stage-1 grounding to better align with spatial/robotic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1935.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1935.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Autoregressive large language model (LLM) used as the backbone LLM in OE-VLA with a 32k token context window for interleaved multimodal tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Qwen Technical Report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Text-pretrained LLM (large autoregressive transformer) adopted as the backbone that consumes concatenated visual-projected tokens and text tokens and generates discrete action tokens autoregressively.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Text-only large-scale language model pretraining (per citation).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large text corpora used to train Qwen (paper cites Qwen technical report); no direct action/affordance data in standard text pretraining is documented here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>LLM decoder for action-token autoregression in robot manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Generates sequences of discretized action tokens conditioned on projected visual+text tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Text-pretraining supplies strong sequence modeling but lacks direct embodied affordance priors; grounding to robot actions is achieved via Stage-2 finetuning on robot demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1935.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1935.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KosMos (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KosMos (Inter.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strong vision-language-action baseline reported in the paper; used for comparison against OE-VLA (reported to be one of the strongest baselines without a diffusion head).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KosMos-2B (reported baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as an effective VLA baseline (KosMos Inter.) in table comparisons; architecture details are not provided in this paper beyond that it is a VLM-based VLA without a diffusion policy head.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Mentioned as a VLM-based model (vision-language pretraining) but specific pretraining type/details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper; referenced as a prior VLA-like model that transfers web knowledge to robotics in other work.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Evaluated on CALVIN (ABC→D) as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Long-horizon manipulation sequences from CALVIN; reported performance used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Used as a baseline demonstrating benefits of VLM transfer; no direct semantic-overlap analysis provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>From Table 1 (ABC→D): KosMos Inter. reported average successful sequence length = 2.70 and per-horizon success rates shown (e.g., LH-1 82.4%, LH-2 68.4%, LH-3 52.4%, LH-4 37.6%, LH-5 29.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1935.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1935.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-VLA (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-VLA (authors' baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA variant built by the authors that accepts both static and wrist views and uses a five-step action chunk; included as a comparative baseline and underperforms OE-VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-VLA (LLaVA-7B based baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VLA model constructed similarly to OpenVLA but accepting both static and wrist views; uses discretized 5-step action chunks and an architecture akin to that of OpenVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Built on LLaVA foundation (vision-language finetuned model) prior to robot finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>LLaVA pretraining/finetuning (multi-image tasks) as foundation; no new pretraining introduced by authors beyond finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Evaluated on CALVIN ABC→D as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same CALVIN long-horizon robotic manipulation evaluation; reported to perform worse than OE-VLA and KosMos in the ABC→D split.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not deeply analyzed; used to evaluate effect of interleaved-image foundation vs OE-VLA's training recipe.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table 1: LLaVA-VLA (LLaVA-7B) reported average successful sequence length = 2.14 and per-horizon success rates (e.g., LH-1 83.1%, LH-2 58.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1935.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1935.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source VLA baseline included in experiments; reported to underperform other baselines, possibly due to single-view input limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenVLA: An Open-Source Vision-Language-Action Model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA (Prismatic-7B variant reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source VLA model referenced and finetuned as a baseline; in the paper it is noted to use a single-view image input which the authors hypothesize limits performance.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language / VLA pipeline in cited OpenVLA work; specifics not re-described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed here; OpenVLA is reported as having single-view input which likely limits spatial/contextual alignment with robot observations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Evaluated on CALVIN ABC→D as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Long-horizon CALVIN sequences; reported to have lower average successful sequence length compared to other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper hypothesizes limited alignment due to single-view input, reducing performance on multi-view robot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table 1: OpenVLA (Prismatic-7B) average successful sequence length = 0.90 (much lower than OE-VLA and KosMos in reported split).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1935.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1935.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboFlamingo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboFlamingo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mentioned VLA-style model that incorporates historical visual information and predicts continuous robot actions (referenced in related work and included in baseline table).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboFlamingo (Flamingo-3B variant in table)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A model that extends Flamingo-like multi-modal capabilities to robotics by incorporating history and predicting continuous actions (as referenced); table shows it as a baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Built on multimodal pretraining (Flamingo family) and adapted to robotics in prior work; not re-trained here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed in this paper; prior work indicates multimodal web-scale image–text and multimodal sequences used.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Evaluated on CALVIN as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Reported in Table 1 with per-horizon success rates (e.g., LH-1 82.4%, Len = 2.48).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table 1: reported average successful sequence length = 2.48 with reported per-horizon success rates (e.g., LH-1 82.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1935.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1935.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior VLA work (Vision-Language-Action) that demonstrated transfer of web-scale VLM knowledge to robotic control; mentioned in related work as a motivating prior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior VLA approach that repurposes a pretrained vision-language model and fine-tunes on robot data to output actions; referenced as an influential prior.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining on web image-text data (per original RT-2 work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Web-scale image-text corpora containing object descriptions and general world knowledge; original RT-2 paper describes the transfer to robotics but details are in that referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>RT-2 applied VLM knowledge to robotic manipulation via finetuning on demonstration data (details in RT-2 paper).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Mentioned as an example where web VLM knowledge provides useful priors for robotics; this paper builds on that idea and extends it to open-ended multimodal prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1935.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1935.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior multimodal-prompt robot manipulation model that is object-centric and requires an object detector; noted as related work with different design trade-offs from OE-VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VIMA: Robot Manipulation with Multimodal Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Object-centric robot policy built on a language model (e.g., T5) that requires object segmentation/detection to combine object images and text for instruction conditioning; different from end-to-end VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Language-model-based (text pretraining); VIMA uses multimodal prompting but is not a VLM-based VLA in the same sense.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed here; referenced to highlight differences (object-centric, requires detectors).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robot manipulation with multimodal prompts</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>VIMA handles multimodal prompts within the same working space and is object-centric; differs architecturally from OE-VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1935.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1935.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that leverages depth and 3D information to enhance VLA capabilities; mentioned as related research improving spatial grounding for robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>3D-VLA: A 3D Vision-Language-Action Generative World Model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>3D-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approach that incorporates 3D/depth information and a generative world model into the VLA pipeline to better ground actions in 3D spatial reasoning (details in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not detailed in this paper; the cited work focuses on 3D and world models.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation / 3D-aware VLA</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Enhances spatial reasoning for embodied control using depth/3D representations; referenced as complementary direction.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control <em>(Rating: 2)</em></li>
                <li>VIMA: Robot Manipulation with Multimodal Prompts <em>(Rating: 2)</em></li>
                <li>OpenVLA: An Open-Source Vision-Language-Action Model <em>(Rating: 2)</em></li>
                <li>3D-VLA: A 3D Vision-Language-Action Generative World Model <em>(Rating: 2)</em></li>
                <li>LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models <em>(Rating: 2)</em></li>
                <li>MGrounding (paper by Li et al., 2025) -- multi-image grounding dataset <em>(Rating: 2)</em></li>
                <li>RoboFlamingo <em>(Rating: 1)</em></li>
                <li>KosMos (Interleaved VLM baseline references) <em>(Rating: 1)</em></li>
                <li>CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1935",
    "paper_id": "paper-278714573",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "OE-VLA",
            "name_full": "Open-Ended Vision-Language-Action (OE-VLA)",
            "brief_description": "A vision-language-action model finetuned to accept open-ended multimodal instructions (interleaved text + images + short video frames) and output discretized robot action tokens for long-horizon manipulation; trained with a two-stage curriculum (multi-image grounding then open-ended instruction tuning).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OE-VLA (Interleave-1B and Interleave-7B)",
            "model_description": "Uses a SigLIP-400M ViT vision encoder to produce visual tokens mapped by a 2-layer MLP into the hidden space of a Qwen-1.5 LLM backbone; concatenates observation tokens, interleaved text and instruction images into a single sequence consumed by the LLM which autoregressively emits discretized action tokens (continuous actions quantized into 256 bins). No separate diffusion policy head was used; actions predicted as 5-step action chunks.",
            "pretraining_type": "Component-wise: vision-language pretraining for SigLIP (CLIP-style), text-only pretraining for Qwen-1.5, and foundation multi-image finetuning (LLaVA-Next-Interleave). OE-VLA itself is finetuned on robot demonstration data.",
            "pretraining_data_description": "Vision encoder (SigLIP) trained on image–text pairs (CLIP-style). LLaVA-Next-Interleave foundation was finetuned on multi-image tasks (visual storytelling, video captioning, difference identification). Stage-1 uses the MGrounding dataset composed of grounding tasks (object tracking, referring grounding, group grounding, free-form multi-image grounding). Stage-2 uses curated robot manipulation datasets converted to multimodal instruction format.",
            "target_task_name": "Long-horizon robotic manipulation (CALVIN / OE-CALVIN benchmarks)",
            "target_task_description": "Manipulation tasks from the CALVIN suite (real-world robotics benchmark): long-horizon sequences of 5 consecutive subtasks across 34 tasks. Inputs: static workspace view + wrist camera; instruction modalities include pure text, object images (VOS), optical-instruction images (OIF), single-image goals (VGR), and short video demonstrations (VDL). Action space: originally continuous robot actions discretized into 256 bins and emitted as 5-step chunks. Evaluations use ABC→D and ABCD→D environment splits.",
            "semantic_alignment": "Paper explicitly discusses semantic alignment issues: foundation VLM tasks (storytelling, captioning) do not fully match robotic manipulation needs (spatial relationships, affordances), motivating Stage-1 multi-image grounding to increase overlap; training data explicitly includes spatial relationships in MGrounding and robot demonstrations, so alignment is moderate after the two-stage finetuning.",
            "performance_with_language_pretraining": "Reported empirical results (behavioral metrics after finetuning): On CALVIN ABC→D split (language-conditioned baseline evaluation) OE-VLA Interleave-1B average successful sequence length = 2.70; OE-VLA Interleave-7B = 2.99 (Table 1). On OE-CALVIN base (open-ended multimodal) ABC→D average lengths: OE-VLA 1B = 2.75, OE-VLA 7B = 3.48 (per-task high performance on VOS/OIF/VDL). On OE-CALVIN hard (more out-of-distribution multimodal instructions) average lengths: OE-VLA 1B = 1.79, OE-VLA 7B ≈ 2.68 (derived from per-task lengths VOS/OIF/VGR/VDL reported in Table 3). Per-task LH-1 success rates reported (examples): OE-VLA 1B VOS LH-1 = 94.0%, VDL LH-1 = 93.7%; OE-VLA 7B VOS LH-1 = 95.0%, VDL LH-1 = 93.3% (see Tables 2 and 3).",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "The paper does not present a direct quantitative sample-efficiency comparison (e.g., episodes-to-performance) between language-pretrained versus non-language-pretrained models. Authors report dataset construction choices (selected ~40% of raw robot data per task, resulting in a training set ≈2× original size) and training for one epoch for fair comparison, and that Stage-1 grounding reduces domain-gap issues, but no explicit episode/sample counts to reach performance are given.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No internal attention-visualization or attention-pattern analyses are reported in the paper.",
            "embedding_space_analysis": "The paper does not present analyses of embedding spaces, clustering, or representational geometry.",
            "action_grounding_evidence": "Behavioral evidence: improved performance on multimodal tasks (especially Visual Object Specification and Video Demo Learning) after Stage-1 grounding and Stage-2 finetuning indicates that the model grounds instruction images to actions at a functional level. However, the paper provides no mechanistic/internal analyses (e.g., probes linking verbs to affordance features or motor primitives).",
            "hierarchical_features_evidence": "No explicit analysis of hierarchical feature levels (low-level vs high-level) is provided; the Stage-1 objective is motivated to improve spatial/high-level grounding but this is argued rather than shown with layer-wise evidence.",
            "transfer_conditions": "Transfer success depends on (a) model scale (7B &gt; 1B), (b) domain similarity between training and test environment (stage-1 pretraining helps more in ABC→D split where target D is out-of-distribution), and (c) instruction modality distribution (models trained with a high proportion of image-based instructions improve on multimodal tasks but that can reduce performance on pure-text tasks unless balanced). The paper reports Stage-1 grounding helps more when evaluation environment is OOD and less when training data already contains environment D.",
            "novel_vs_familiar_objects": "Indirect comparison via OE-CALVIN base vs hard: performance degrades on OE-CALVIN hard (instructions using web images, handwritten fonts, diverse viewpoints), indicating lower performance on more novel/unseen instruction sources; e.g., OE-VLA 7B average length drops from 3.48 (base) to ≈2.68 (hard). No explicit per-object 'seen vs unseen' numerical split is provided.",
            "zero_shot_or_few_shot": "The model demonstrates generalization to out-of-distribution multimodal instructions (OE-CALVIN hard) after finetuning, but the paper does not present pure zero-shot experiments (i.e., no finetuning) or controlled few-shot ablations quantifying how many demonstrations are needed.",
            "layer_analysis": "No layer-wise ablation, freezing, or probing analyses are reported to identify which layers/components contribute most to transfer.",
            "negative_transfer_evidence": "Yes — authors report that the higher proportion of image-based multimodal training samples in their recipe correlates with a relative drop in performance on purely textual instructions; training models exclusively on textual instructions (OE-VLA-text) improves performance on linguistic inputs (Table 4). This is presented as evidence of a trade-off in the data recipe.",
            "comparison_to_vision_only": "No direct controlled comparison to a vision-only-pretrained model (e.g., pure ImageNet or self-supervised vision) is reported. The paper does compare multi-view interleaved-input models vs single-view (OpenVLA single-view performed worse), suggesting multi-view visual inputs and VLM components help, but not an explicit vision-only pretraining ablation.",
            "temporal_dynamics": "Authors report the two-stage curriculum and show that Stage-1 improves early transfer for the ABC→D split; however, no detailed training dynamics (e.g., learning curves over epochs/steps) or representation evolution analyses are provided.",
            "dimensionality_analysis": "No explicit measurements of representation dimensionality or intrinsic dimension (PCA / effective rank) are reported.",
            "uuid": "e1935.0"
        },
        {
            "name_short": "LLaVA-NeXT-Interleave",
            "name_full": "LLaVA-NeXT-Interleave",
            "brief_description": "A multi-image capable vision-language foundation model (selected as the OE-VLA foundation) that supports free-form multi-image input and has been finetuned on interleaved image/text tasks.",
            "citation_title": "LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models",
            "mention_or_use": "use",
            "model_name": "LLaVA-Next-Interleave",
            "model_description": "Multimodal VLM that handles interleaved multi-image and text inputs; used as the base/foundation model for OE-VLA because of its multi-image handling capability.",
            "pretraining_type": "Vision-language pretraining followed by finetuning on multi-image tasks (visual storytelling, video captioning, difference identification); used as a foundation for downstream finetuning.",
            "pretraining_data_description": "Finetuned on datasets for visual storytelling, video captioning, multi-image difference tasks — tasks that focus on image-text alignment but not specifically robotic affordances.",
            "target_task_name": "Foundation for robot manipulation finetuning",
            "target_task_description": "Serves as the multimodal encoder/LLM backbone basis that is later finetuned on robot manipulation data to produce OE-VLA.",
            "semantic_alignment": "Paper notes a mismatch between LLaVA-Next-Interleave tasks (storytelling/captioning) and robotic manipulation requirements (spatial relationships/affordances), motivating Stage-1 grounding.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": null,
            "embedding_space_analysis": null,
            "action_grounding_evidence": null,
            "hierarchical_features_evidence": null,
            "transfer_conditions": "Used as foundation; Stage-1 grounding is intended to bridge task-type mismatch to improve transfer to manipulation.",
            "novel_vs_familiar_objects": null,
            "zero_shot_or_few_shot": null,
            "layer_analysis": null,
            "negative_transfer_evidence": null,
            "comparison_to_vision_only": null,
            "temporal_dynamics": null,
            "dimensionality_analysis": null,
            "uuid": "e1935.1"
        },
        {
            "name_short": "SigLIP-400M",
            "name_full": "SigLIP-400M Vision Transformer",
            "brief_description": "A CLIP-style vision transformer trained with a sigmoid loss (SigLIP) used as the vision encoder in OE-VLA; encodes images into visual tokens at 384×384 resolution with 14×14 patching.",
            "citation_title": "Sigmoid Loss for Language Image Pre-Training",
            "mention_or_use": "use",
            "model_name": "SigLIP-400M (ViT)",
            "model_description": "Vision Transformer (ViT) trained in a CLIP-style vision-language pretraining paradigm with a sigmoid loss; produces visual token sequences used by the LLM backbone.",
            "pretraining_type": "Vision-language pretraining on image-text pairs (CLIP-style).",
            "pretraining_data_description": "Standard image–text pairs used for CLIP-style pretraining; paper does not enumerate exact dataset composition (no claim of action verbs/affordances coverage).",
            "target_task_name": "Visual encoding for robot manipulation finetuning",
            "target_task_description": "Encodes observation images (static and wrist) and instruction images/frames used as input to the LLM which predicts action tokens.",
            "semantic_alignment": "Component provides general image–text-alignment capability; authors add Stage-1 grounding to better align with spatial/robotic tasks.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": null,
            "embedding_space_analysis": null,
            "action_grounding_evidence": null,
            "hierarchical_features_evidence": null,
            "transfer_conditions": null,
            "novel_vs_familiar_objects": null,
            "zero_shot_or_few_shot": null,
            "layer_analysis": null,
            "negative_transfer_evidence": null,
            "comparison_to_vision_only": null,
            "temporal_dynamics": null,
            "dimensionality_analysis": null,
            "uuid": "e1935.2"
        },
        {
            "name_short": "Qwen-1.5",
            "name_full": "Qwen-1.5",
            "brief_description": "Autoregressive large language model (LLM) used as the backbone LLM in OE-VLA with a 32k token context window for interleaved multimodal tokens.",
            "citation_title": "Qwen Technical Report",
            "mention_or_use": "use",
            "model_name": "Qwen-1.5",
            "model_description": "Text-pretrained LLM (large autoregressive transformer) adopted as the backbone that consumes concatenated visual-projected tokens and text tokens and generates discrete action tokens autoregressively.",
            "pretraining_type": "Text-only large-scale language model pretraining (per citation).",
            "pretraining_data_description": "Large text corpora used to train Qwen (paper cites Qwen technical report); no direct action/affordance data in standard text pretraining is documented here.",
            "target_task_name": "LLM decoder for action-token autoregression in robot manipulation",
            "target_task_description": "Generates sequences of discretized action tokens conditioned on projected visual+text tokens.",
            "semantic_alignment": "Text-pretraining supplies strong sequence modeling but lacks direct embodied affordance priors; grounding to robot actions is achieved via Stage-2 finetuning on robot demonstrations.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": null,
            "embedding_space_analysis": null,
            "action_grounding_evidence": null,
            "hierarchical_features_evidence": null,
            "transfer_conditions": null,
            "novel_vs_familiar_objects": null,
            "zero_shot_or_few_shot": null,
            "layer_analysis": null,
            "negative_transfer_evidence": null,
            "comparison_to_vision_only": null,
            "temporal_dynamics": null,
            "dimensionality_analysis": null,
            "uuid": "e1935.3"
        },
        {
            "name_short": "KosMos (baseline)",
            "name_full": "KosMos (Inter.)",
            "brief_description": "A strong vision-language-action baseline reported in the paper; used for comparison against OE-VLA (reported to be one of the strongest baselines without a diffusion head).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "KosMos-2B (reported baseline)",
            "model_description": "Referenced as an effective VLA baseline (KosMos Inter.) in table comparisons; architecture details are not provided in this paper beyond that it is a VLM-based VLA without a diffusion policy head.",
            "pretraining_type": "Mentioned as a VLM-based model (vision-language pretraining) but specific pretraining type/details not provided in this paper.",
            "pretraining_data_description": "Not specified in this paper; referenced as a prior VLA-like model that transfers web knowledge to robotics in other work.",
            "target_task_name": "Evaluated on CALVIN (ABC→D) as baseline",
            "target_task_description": "Long-horizon manipulation sequences from CALVIN; reported performance used for comparison.",
            "semantic_alignment": "Used as a baseline demonstrating benefits of VLM transfer; no direct semantic-overlap analysis provided here.",
            "performance_with_language_pretraining": "From Table 1 (ABC→D): KosMos Inter. reported average successful sequence length = 2.70 and per-horizon success rates shown (e.g., LH-1 82.4%, LH-2 68.4%, LH-3 52.4%, LH-4 37.6%, LH-5 29.6%).",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": null,
            "embedding_space_analysis": null,
            "action_grounding_evidence": null,
            "hierarchical_features_evidence": null,
            "transfer_conditions": null,
            "novel_vs_familiar_objects": null,
            "zero_shot_or_few_shot": null,
            "layer_analysis": null,
            "negative_transfer_evidence": null,
            "comparison_to_vision_only": null,
            "temporal_dynamics": null,
            "dimensionality_analysis": null,
            "uuid": "e1935.4"
        },
        {
            "name_short": "LLaVA-VLA (baseline)",
            "name_full": "LLaVA-VLA (authors' baseline)",
            "brief_description": "A VLA variant built by the authors that accepts both static and wrist views and uses a five-step action chunk; included as a comparative baseline and underperforms OE-VLA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-VLA (LLaVA-7B based baseline)",
            "model_description": "VLA model constructed similarly to OpenVLA but accepting both static and wrist views; uses discretized 5-step action chunks and an architecture akin to that of OpenVLA.",
            "pretraining_type": "Built on LLaVA foundation (vision-language finetuned model) prior to robot finetuning.",
            "pretraining_data_description": "LLaVA pretraining/finetuning (multi-image tasks) as foundation; no new pretraining introduced by authors beyond finetuning.",
            "target_task_name": "Evaluated on CALVIN ABC→D as baseline",
            "target_task_description": "Same CALVIN long-horizon robotic manipulation evaluation; reported to perform worse than OE-VLA and KosMos in the ABC→D split.",
            "semantic_alignment": "Not deeply analyzed; used to evaluate effect of interleaved-image foundation vs OE-VLA's training recipe.",
            "performance_with_language_pretraining": "Table 1: LLaVA-VLA (LLaVA-7B) reported average successful sequence length = 2.14 and per-horizon success rates (e.g., LH-1 83.1%, LH-2 58.4%).",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": null,
            "embedding_space_analysis": null,
            "action_grounding_evidence": null,
            "hierarchical_features_evidence": null,
            "transfer_conditions": null,
            "novel_vs_familiar_objects": null,
            "zero_shot_or_few_shot": null,
            "layer_analysis": null,
            "negative_transfer_evidence": null,
            "comparison_to_vision_only": null,
            "temporal_dynamics": null,
            "dimensionality_analysis": null,
            "uuid": "e1935.5"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA",
            "brief_description": "An open-source VLA baseline included in experiments; reported to underperform other baselines, possibly due to single-view input limitation.",
            "citation_title": "OpenVLA: An Open-Source Vision-Language-Action Model",
            "mention_or_use": "use",
            "model_name": "OpenVLA (Prismatic-7B variant reported)",
            "model_description": "Open-source VLA model referenced and finetuned as a baseline; in the paper it is noted to use a single-view image input which the authors hypothesize limits performance.",
            "pretraining_type": "Vision-language / VLA pipeline in cited OpenVLA work; specifics not re-described in this paper.",
            "pretraining_data_description": "Not detailed here; OpenVLA is reported as having single-view input which likely limits spatial/contextual alignment with robot observations.",
            "target_task_name": "Evaluated on CALVIN ABC→D as baseline",
            "target_task_description": "Long-horizon CALVIN sequences; reported to have lower average successful sequence length compared to other baselines.",
            "semantic_alignment": "Paper hypothesizes limited alignment due to single-view input, reducing performance on multi-view robot tasks.",
            "performance_with_language_pretraining": "Table 1: OpenVLA (Prismatic-7B) average successful sequence length = 0.90 (much lower than OE-VLA and KosMos in reported split).",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": null,
            "embedding_space_analysis": null,
            "action_grounding_evidence": null,
            "hierarchical_features_evidence": null,
            "transfer_conditions": null,
            "novel_vs_familiar_objects": null,
            "zero_shot_or_few_shot": null,
            "layer_analysis": null,
            "negative_transfer_evidence": null,
            "comparison_to_vision_only": null,
            "temporal_dynamics": null,
            "dimensionality_analysis": null,
            "uuid": "e1935.6"
        },
        {
            "name_short": "RoboFlamingo",
            "name_full": "RoboFlamingo",
            "brief_description": "A mentioned VLA-style model that incorporates historical visual information and predicts continuous robot actions (referenced in related work and included in baseline table).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoboFlamingo (Flamingo-3B variant in table)",
            "model_description": "A model that extends Flamingo-like multi-modal capabilities to robotics by incorporating history and predicting continuous actions (as referenced); table shows it as a baseline in comparisons.",
            "pretraining_type": "Built on multimodal pretraining (Flamingo family) and adapted to robotics in prior work; not re-trained here.",
            "pretraining_data_description": "Not detailed in this paper; prior work indicates multimodal web-scale image–text and multimodal sequences used.",
            "target_task_name": "Evaluated on CALVIN as baseline",
            "target_task_description": "Reported in Table 1 with per-horizon success rates (e.g., LH-1 82.4%, Len = 2.48).",
            "semantic_alignment": null,
            "performance_with_language_pretraining": "Table 1: reported average successful sequence length = 2.48 with reported per-horizon success rates (e.g., LH-1 82.4%).",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": null,
            "embedding_space_analysis": null,
            "action_grounding_evidence": null,
            "hierarchical_features_evidence": null,
            "transfer_conditions": null,
            "novel_vs_familiar_objects": null,
            "zero_shot_or_few_shot": null,
            "layer_analysis": null,
            "negative_transfer_evidence": null,
            "comparison_to_vision_only": null,
            "temporal_dynamics": null,
            "dimensionality_analysis": null,
            "uuid": "e1935.7"
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2",
            "brief_description": "Referenced prior VLA work (Vision-Language-Action) that demonstrated transfer of web-scale VLM knowledge to robotic control; mentioned in related work as a motivating prior.",
            "citation_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
            "mention_or_use": "mention",
            "model_name": "RT-2",
            "model_description": "Prior VLA approach that repurposes a pretrained vision-language model and fine-tunes on robot data to output actions; referenced as an influential prior.",
            "pretraining_type": "Vision-language pretraining on web image-text data (per original RT-2 work).",
            "pretraining_data_description": "Web-scale image-text corpora containing object descriptions and general world knowledge; original RT-2 paper describes the transfer to robotics but details are in that referenced work.",
            "target_task_name": "Robotic manipulation (prior work)",
            "target_task_description": "RT-2 applied VLM knowledge to robotic manipulation via finetuning on demonstration data (details in RT-2 paper).",
            "semantic_alignment": "Mentioned as an example where web VLM knowledge provides useful priors for robotics; this paper builds on that idea and extends it to open-ended multimodal prompting.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": null,
            "embedding_space_analysis": null,
            "action_grounding_evidence": null,
            "hierarchical_features_evidence": null,
            "transfer_conditions": null,
            "novel_vs_familiar_objects": null,
            "zero_shot_or_few_shot": null,
            "layer_analysis": null,
            "negative_transfer_evidence": null,
            "comparison_to_vision_only": null,
            "temporal_dynamics": null,
            "dimensionality_analysis": null,
            "uuid": "e1935.8"
        },
        {
            "name_short": "VIMA",
            "name_full": "VIMA",
            "brief_description": "Referenced prior multimodal-prompt robot manipulation model that is object-centric and requires an object detector; noted as related work with different design trade-offs from OE-VLA.",
            "citation_title": "VIMA: Robot Manipulation with Multimodal Prompts",
            "mention_or_use": "mention",
            "model_name": "VIMA",
            "model_description": "Object-centric robot policy built on a language model (e.g., T5) that requires object segmentation/detection to combine object images and text for instruction conditioning; different from end-to-end VLA.",
            "pretraining_type": "Language-model-based (text pretraining); VIMA uses multimodal prompting but is not a VLM-based VLA in the same sense.",
            "pretraining_data_description": "Not detailed here; referenced to highlight differences (object-centric, requires detectors).",
            "target_task_name": "Robot manipulation with multimodal prompts",
            "target_task_description": "VIMA handles multimodal prompts within the same working space and is object-centric; differs architecturally from OE-VLA.",
            "semantic_alignment": null,
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": null,
            "embedding_space_analysis": null,
            "action_grounding_evidence": null,
            "hierarchical_features_evidence": null,
            "transfer_conditions": null,
            "novel_vs_familiar_objects": null,
            "zero_shot_or_few_shot": null,
            "layer_analysis": null,
            "negative_transfer_evidence": null,
            "comparison_to_vision_only": null,
            "temporal_dynamics": null,
            "dimensionality_analysis": null,
            "uuid": "e1935.9"
        },
        {
            "name_short": "3D-VLA",
            "name_full": "3D-VLA",
            "brief_description": "Referenced prior work that leverages depth and 3D information to enhance VLA capabilities; mentioned as related research improving spatial grounding for robotics.",
            "citation_title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
            "mention_or_use": "mention",
            "model_name": "3D-VLA",
            "model_description": "Approach that incorporates 3D/depth information and a generative world model into the VLA pipeline to better ground actions in 3D spatial reasoning (details in the cited work).",
            "pretraining_type": "Not detailed in this paper; the cited work focuses on 3D and world models.",
            "pretraining_data_description": "Not specified here.",
            "target_task_name": "Robotic manipulation / 3D-aware VLA",
            "target_task_description": "Enhances spatial reasoning for embodied control using depth/3D representations; referenced as complementary direction.",
            "semantic_alignment": null,
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": null,
            "embedding_space_analysis": null,
            "action_grounding_evidence": null,
            "hierarchical_features_evidence": null,
            "transfer_conditions": null,
            "novel_vs_familiar_objects": null,
            "zero_shot_or_few_shot": null,
            "layer_analysis": null,
            "negative_transfer_evidence": null,
            "comparison_to_vision_only": null,
            "temporal_dynamics": null,
            "dimensionality_analysis": null,
            "uuid": "e1935.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
            "rating": 2
        },
        {
            "paper_title": "VIMA: Robot Manipulation with Multimodal Prompts",
            "rating": 2
        },
        {
            "paper_title": "OpenVLA: An Open-Source Vision-Language-Action Model",
            "rating": 2
        },
        {
            "paper_title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
            "rating": 2
        },
        {
            "paper_title": "LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models",
            "rating": 2
        },
        {
            "paper_title": "MGrounding (paper by Li et al., 2025) -- multi-image grounding dataset",
            "rating": 2
        },
        {
            "paper_title": "RoboFlamingo",
            "rating": 1
        },
        {
            "paper_title": "KosMos (Interleaved VLM baseline references)",
            "rating": 1
        },
        {
            "paper_title": "CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation",
            "rating": 1
        }
    ],
    "cost": 0.02547225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions
16 May 2025</p>
<p>Wei Zhao 
Westlake University</p>
<p>Gongsheng Li 
Zhejiang University</p>
<p>Zhefei Gong 
Westlake University</p>
<p>Pengxiang Ding 
Westlake University</p>
<p>Zhejiang University</p>
<p>Han Zhao 
Westlake University</p>
<p>Zhejiang University</p>
<p>Donglin Wang 
Westlake University</p>
<p>Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions
16 May 20252C741ECAFD2C5BD5DEDBA8B376366E34arXiv:2505.11214v1[cs.RO]
Vision-Language-Action (VLA) models have recently become highly prominent in the field of robotics.Leveraging vision-language foundation models trained on large-scale internet data, the VLA model can generate robotic actions directly from visual observations and human instructions through a single end-to-end neural network.Despite their effectiveness, current VLA models usually accept only one form of human prompting, language instructions, which may constrain their applicability in open-ended human-robot interactions.For example, a user might expect the robot to retrieve an object shown in an image, follow an instruction written on the whiteboard, or imitate a behavior demonstrated in a video, rather than relying solely on language-based descriptions.To address this gap, we introduce OE-VLA, which explores the potential of VLA models for open-ended multimodal instructions.Extensive results demonstrate that our OE-VLA not only achieves comparable performance to traditional VLA models with linguistic input but also delivers impressive results across four additional categories of open-ended tasks.The proposed methodology could significantly expand the applications of VLA models across various everyday scenarios and facilitate human-robot interaction.</p>
<p>Introduction</p>
<p>Large language models (LLMs), such as ChatGPT [Ouyang et al., 2022], LLaMA[Touvron et al., 2023], and Gemini [Team et al., 2024], have achieved significant success in various domains and greatly eased people's daily lives.In particular, researchers have also conducted studies on multimodal large language models (MLLMs), such as vision language models (VLMs) [Liu et al., 2023, Li et al., 2023a, Awadalla et al., 2023, Karamcheti et al., 2024, Zhang et al., 2024], to enable LLMs to understand human intentions using both linguistic and visual modalities.The aforementioned versatile models provide a robust foundation for the advancement of artificial general intelligence (AGI) within cyberspace.</p>
<p>As these techniques undergo continuous improvement, a critical question emerges: How can we develop similar models capable of interacting with the physical world?One possible approach is the vision-language-action (VLA) model proposed in RT-2 [Zitkovich et al., 2023] for robot manipulation.This approach leverages a pre-trained VLM as the base model and fine-tunes it with carefully curated robotic data, aiming to fully harness the knowledge acquired by the VLM.Specifically, numerous studies have been conducted to investigate the potential of the VLA model.RT-2 [Zitkovich et al., 2023], RT-X[Collaboration et al., 2023], and OpenVLA [Kim et al., 2024] demonstrate that, compared to traditional policies [Brohan et al., 2023, Lynch<em> andSermanet</em>, 2021], these end-to-end models can achieve superior generalization across diverse language commands and environments.Building on this starting point, numerous studies have been conducted to refine the VLA model into a robust and versatile general-purpose solution.One group of studies focuses on searching for better approaches to simulate the action space.For example, instead of discretizing robot actions into language tokens, these studies treat raw continuous actions as the predictive target.Therefore, these works [Li et al., 2023b, 2024a, Liu et al., 2025, Li et al., 2024b, Wen et al., 2025a,b] employ a separate policy head, using an RNN, Transformer [Vaswani et al., 2017], diffusion network [Chi et al., 2023], or the flow matching [Black et al., 2024] technique, to enhance the model performance.Another group of studies [Zhen et al., 2024, Barcellona et al., 2025] focuses on promoting the VLA by incorporating augmented robot observations, such as improving representations, utilizing historical information, or grounding robot actions in a world model.</p>
<p>However, most previous studies place an emphasis on directly improving the performance (e.g., success rate) of the VLA, while overlooking the enhancement of the interactive process between humans and robots.Specifically, previous VLA models typically use pure linguistic instructions to guide the robot in performing a task, whereas in daily life, human instructions are not confined to a single form.For example, human guidance could take the form of optical instructions on a whiteboard, a combination of text and images, a one-shot video demonstration, or simply a goal image.</p>
<p>To address this issue, we present OE-VLA, a novel approach capable of handling the aforementioned open-ended multimodal human instructions.Accompanied by our method, we also introduce two new benchmarks, OE-CALVIN base and OE-CALVIN hard , to ensure reproducibility and comparability.These two benchmarks are built upon the popular CALVIN test suite and feature varying levels of difficulty for open-ended instructions.Experimental results show that the proposed OE-VLA not only matches the performance of traditional VLA models in following language instructions but also demonstrates acceptable performance in tackling diverse free-form human prompts.In conclusion, our contributions are three-fold: with carefully curated robot demonstration datasets.Leveraging the vast knowledge contained within the VLM, these models outperform traditional robot policies, yielding remarkable results.</p>
<p>RoboFlamingo [Li et al., 2023b] has made further efforts to incorporate historical visual information as input and predict continuous robot actions, rather than relying on discretized action tokens.Following the remarkable success of diffusion policy [Chi et al., 2023], numerous VLA studies, such as CogACT [Li et al., 2024a], π 0 [Black et al., 2024], andGR00T[NVIDIA et al., 2025], have integrated it as a separate output head and achieved SOTA performance.Numerous works have also attempted to improve VLA performance from other perspectives.3D-VLA [Zhen et al., 2024] leverages depth information, RT-H[Belkhale et al., 2024] introduces action hierarchy techniques, and Embodied-COT [Zawalski et al., 2024] integrates classic chain-of-thought reasoning into the VLA model.Moreover, there is also a line of work [Wu et al., 2023, Barcellona et al., 2025] that leverages world models to further enhance performance.</p>
<p>However, previous studies have predominantly concentrated on improving the success rate of robot manipulation using language modality instructions.Few studies have explored equipping VLA models with the capability to handle more open-ended task specifications.Perhaps the study that has the most similar motivation to ours is VIMA [Jiang et al., 2023], which is capable of receiving multimodal prompts for robot control.However, our study differs significantly in many aspects.First, VIMA is an object-centric robot policy model, rather than a traditional VLA model.It requires an object detector to segment each object from the observations and process the combination of object images and textual prompts.Thus, it does not support the pure language-conditioned working pattern as VLA does.Second, VIMA is built on a language model, such as T5, which lacks knowledge about the visual world.Third, the multimodal instructions in VIMA are constrained to originate from the same working space.In contrast, our model is capable of handling instructions that may come from different environments or even from the Internet.We are pleased to note that a concurrent work [Fan et al., 2025] shares a similar motivation to ours; however, our focus is on addressing more open-ended tasks.</p>
<p>3 Open-Ended Tasks with Multimodal Instructions for VLA</p>
<p>In addition to the original language-conditioned tasks, this study further investigates the following diverse open-ended tasks for VLA as depicted in Figure 1.</p>
<p>• Visual object specification (VOS).In this task, the objects to be manipulated are represented by a snapshot image rather than a language description.The task prompt follows a format like "Given observation <obs>: grasp the <img 1 > and put it into <img 2 >," where <img 1 > and <img 2 > represent the relevant objects, <obs> is the original observation image in VLA.• Optical instruction following (OIF).For tasks in this category, the robot must follow instructions presented in an image, rather than in a textual form.The task prompt is structured as "Given observation <obs>: follow the command in <img>."• Visual goal reaching (VGR).This task requests the robot to identify the differences between the observed state and the goal state, then take appropriate actions to achieve the goal state.</p>
<p>The task prompt is expressed as "Given observation <obs>: reach the goal state in <img>."• Video demo learning (VDL).In this task, the robot is provided with a single video demonstration consisting of about four frames that show what the robot is required to do.The robot is expected to infer what happens in the video and take the appropriate actions.The whole prompt is "Given observation <obs>: perform the demonstrated actions in <img 1 > <img 2 > <img 3 > <img 4 >," where <img i > represents the video frame at step i.</p>
<p>Method</p>
<p>Overall Architecture</p>
<p>Figure 2 illustrates the overall architecture of our model, which consists mainly of three components: a vision encoder, an MLP projector, and an LLM backbone.Since our work focuses on endowing the VLA with the ability to handle diverse open-ended tasks, we do not incorporate a separate policy head, such as the diffusion policy, to pursue further performance enhancement.Therefore, we adopt classical discretized action tokens as the predictive target, which can be directly generated by the LLM backbone.Specifically, we select LLaVA-Next-Interleave [Li et al., 2024c] as the foundation model due to its capability to handle free-form multi-image input and its outstanding performance on relevant benchmarks.</p>
<p>Vision encoder The foundation model uses the SigLIP-400M [Zhai et al., 2023] Vision Transformer (ViT) [Dosovitskiy et al., 2021], a CLIP model trained with a more effective sigmoid loss function.This encoder receives an image with a resolution of 384×384 and splits it into patches using a patch size of 14.In particular, our visual information comes from two sources.One source is for the observation side, which includes a static view of the workspace and a dynamic view from the wrist camera.The other source consists of descriptive images from the instruction.We apply this vision encoder to all images, obtaining two types of tokens as follows:</p>
<p>T obs = SigLip(I obs )</p>
<p>(1)
T img 1 , T img 2 , ..., T img i = SigLip(I img 1 , I img 2 , ..., I img i )(2)
LLM backbone The LLM backbone used in this foundation model is Qwen-1.5 [Bai et al., 2023], which supports a maximum context length of 32k tokens.Any textual fragments L j in the task prompt are tokenized using the Qwen tokenizer as below:
T lang 1 , T lang 2 , ..., T lang j = T okenizer(L 1 , L 2 , ..., L j )(3)
A two-layer MLP is used to map the visual tokens from the vision encoder to the same hidden space as the language tokens.We concatenate all of these tokens while maintaining their original positions to form the final sequence of tokens that is then fed into the LLM.
T f inal = Concat(T obs , T lang 1 , T img 1 , T lang 2 , T img 2 , ...)(4)
Action Tokenizer To enable the foundation model to produce action tokens, the continuous robot actions are discretized into 256 bins.These bins are denoted by reusing the least frequently used language tokens in the Qwen vocabulary.The entire model then constructs a conditional probability distribution for the robot actions as follows.
P ϕ (A 1 , A 2 , ..., A n |T f inal ) = n i=1 P ϕ (A i |T f inal , A 1:i−1 )(5)
Using the maximum log-likelihood loss, the model parameters ϕ can be determined after training.We use a simple five-step action chunk as the predictive target.The last frame of each segment is designated as the goal image.</p>
<p>Training Pipeline</p>
<p>We propose a two-stage curriculum learning strategy to endow the OE-VLA model with the capability to handle open-ended instructions.</p>
<p>Stage 1: Multi-Image Grounding This training stage is designed to further enhance the foundation model's ability to accurately perceive spatial relationships between objects.Although LLaVA-Interleave-next has been fine-tuned on an interleaved multi-image dataset, its associated tasks are primarily centered on visual storytelling, video captioning, and difference identification.We hypothesize that there is still a gap between these tasks and robot manipulation.Intuitively, spatial relationships among objects play a pivotal role in robotic manipulation.Therefore, we introduce this multi-image grounding stage to help bridge the gap.During this training stage, we finetune our foundation model using the MGrounding dataset introduced by [Li et al., 2025].MGrounding is a compilation of various grounding datasets that include object tracking, referencing grounding, group grounding, and more.It also includes a new free-form multi-image grounding dataset that contains samples such as "Please find the same person shown in <img 1 > and locate the person in <img 2 >."</p>
<p>We employ a customized recipe to reduce training time and avoid corrupted or unusable files.Further details are provided in the Appendix.</p>
<p>Stage 2: Open-ended Instruction Tuning After fine-tuning the foundation model in Stage 1, we expect our model to demonstrate more accurate environmental perception and be better prepared to perform embodied tasks.Subsequently, we train the model using our constructed robot manipulation datasets with open-ended task specifications.As described in Section 3, all training samples for these various tasks can be formatted uniformly as ((<obs>, (text 1 , <img 1 >, text 2 , <img 2 >)), <act>), where <obs> refers to the robot observation captured by the camera, (text 1 , <img 1 >, text 2 , <img 2 >))</p>
<p>represents the multimodal human instruction, and <act> corresponds to the actions.During training, these samples are randomly shuffled.In particular, camera images from both the static and wrist views are concatenated into a single image to reduce the number of tokens to be processed.For the sake of simplicity and clarity, we utilize only the static view for figures presented in the paper.</p>
<p>Experiments</p>
<p>Settings</p>
<p>To ensure reproducibility and facilitate consistent comparisons, our experiments were conducted primarily using the CALVIN [Mees et al., 2022] evaluation suite.CALVIN serves as a robust benchmark for language-conditioned robot policy learning, comprising 1,000 evaluation sequences across 34 tasks to assess model performance.Each evaluation sample consists of a long-horizon task sequence comprising five consecutive subtasks, each accompanied by its corresponding language annotation.In addition, to evaluate our OE-VLA under open-ended instructions, we introduce two new benchmarks of increasing difficulty: OE-CALVIN base and OE-CALVIN hard .Each benchmark also contains approximately 1,000 evaluation sequences in total, similar to those in CALVIN.However, all language annotations are replaced with open-ended, multimodal instructions.For the OE-CALVIN base benchmark, the relevant instructions are constructed using object images cropped from raw environmental observations, optical instructions featuring plain backgrounds and regular fonts, and goal images and video demonstrations captured from the same environment without changes in perspective.For the OE-CALVIN hard benchmark, the instructions are constructed using object images sourced from the Web, optical instructions featuring special backgrounds and hand-written fonts, and goal images and videos captured from diverse environments and perspectives.Figure 4 presents representative task samples from our OE-CALVIN base and OE-CALVIN hard benchmarks.Additional details regarding these benchmarks and other experiments are provided in the Appendix.</p>
<p>Results</p>
<p>We conducted experiments on the CALVIN benchmark using two data set-split configurations: ABC→D and ABCD→D.The results for the ABC→D split are mainly reported and discussed, as this configuration poses a greater challenge.Meanwhile, we trained both a 0.5B-parameter model and a 7B-parameter model to validate our method.Since no previous VLA models have been capable</p>
<p>of handling such open-ended tasks, we introduce strong traditional language-conditioned models as baselines to address the following concerns.1.How does our method perform compared to traditional methods, both using language instructions?2. How does our model perform on multimodal open-ended instructions compared to its performance on exclusively linguistic inputs?</p>
<p>To ensure a fair comparison, we select baseline models that do not incorporate a separate diffusion policy head, since such a head can improve the performance across all models [Li et al., 2024b].As demonstrated in Table 1, although our OE-VLA is designed to accommodate a wide range of open-ended multimodal instructions, it still achieves highly competitive performance when evaluated with pure linguistic input.Our OE-VLA 1b attains results comparable to those of the KosMos model that does not use a diffusion policy head.The KosMos recipe has previously been shown to serve as the strongest baseline among various VLA models, even compared to models of larger sizes.In addition, we also include the widely used OpenVLA model for comparison.However, the finetuned OpenVLA does not outperform other models.We conjecture that this is primarily due to its reliance on a single-view image as input.Therefore, we also introduce LLaVA-VLA, a VLA model that accepts both static view and wrist view inputs and features an architecture similar to that of OpenVLA.For LLaVA-VLA, we apply the same five-step action chunk as OE-VLA.Nevertheless, the performance of this model is still inferior to that of our OE-VLA and the KosMos model.Specifically, our OE-VLA 7b achieves the best performance under the CALVIN ABC→D setting, with an average successful sequence length of 2.99.-----2.68highly promising results.Specifically, the average successful sequence length of OE-VLA 1b is 2.75, which is even slightly higher than that of the language-conditioned baseline.We find that OE-VLA demonstrates exceptional performance in visual object specification (VOS) tasks, which are quite common requirements in everyday life.Moreover, the results on the tasks of optical instruction following (OIF) and video demonstration learning (VDL) are also satisfactory.The visual goal reaching (VGR) task may be the most challenging, as the model exhibits a noticeable performance degradation compared to the others.As we scale up our model to OE-VLA 7b , we observe a substantial improvement in performance, which even exceeds that of prior language-based baselines.Given that the number of image-based instructions is nearly four times greater than that of text-only instructions, this training data configuration allows larger models to achieve significantly improved performance on multimodal tasks.</p>
<p>Results with Linguistic Task Specifications</p>
<p>Results with Open-Ended Multimodal Instructions</p>
<p>Furthermore, our model is evaluated on the more challenging OE-CALVIN hard benchmark.This benchmark requires the model to possess strong generalization capabilities to capture intent from multimodal instructions, which may include web images, images or videos from diverse environments and viewpoints, as well as hand-written commands.The results in Table 3 demonstrate that our method is also capable of handling such challenging situations, despite some performance degradation compared to OE-CALVIN base .It is notable that our model continues to perform competitively on visual object specification (VOS) tasks.In addition, OE-VLA 7b further mitigates performance degradation on this benchmark and demonstrates performance comparable to that of traditional language-conditioned VLAs.Nevertheless, visual goal reaching (VGR) remains difficult due to the limited information contained in a single image.These results demonstrate the potential of using free-form multimodal commands to guide a robot in performing tasks.</p>
<p>Ablation Studies for Training Pipeline</p>
<p>As depicted in Figure 5, the two-stage training pipeline substantially improves the performance of our model on the OE-CALVIN base benchmark, particularly for the most challenging ABC→D split.</p>
<p>For the ABCD→D split, the improvement is relatively modest.We hypothesize that the model has acquired sufficient knowledge from the training data within the same D environment as the evaluation suite, thus reducing the impact of stage-1 training.However, the proposed two-stage training pipeline still has a positive impact on the overall performance of our model when handling open-ended instructions.A similar phenomenon can also be observed in the results on the OE-CALVIN hard benchmark, as shown in Figure 6.Although the performance gain is less substantial than that on OE-CALVIN base , the method is nonetheless effective.Furthermore, our empirical findings suggest that the improvement from stage-1 training for OE-VLA 7b is relatively limited, as the foundation model may already possess a strong understanding of spatial relationships.</p>
<p>Analysis for Training Data Recipe</p>
<p>As outlined in Section 4, our multimodal instruction training data are not derived from the original complete dataset, since this would result in a dataset that is five times larger, which significantly needs more time to run experiments.Thus, for each type of task, we randomly select about 40% of the raw data to construct the new training set, which finally results in a dataset twice the original size.Thus, we trained all our models for one epoch for a fair comparison.In particular, the amount of data with multimodal instructions significantly exceeds that of text-only instructions, which may limit the model's performance on purely linguistic inputs.As shown in Table 4, training models exclusively on textual instructions further improves their performance on linguistic inputs.The results suggest that a better trade-off of the data recipe may be explored in the future.</p>
<p>Step Figure 5: Performance of our method on the OE-CALVIN base benchmark.The upper row presents the results for the ABC→D split, while the lower row presents the results for the ABCD→D split.</p>
<p>Step</p>
<p>Conclusion</p>
<p>In this work, we presented OE-VLA, a Vision-Language-Action framework designed to handle open-ended multimodal instructions, thereby overcoming the limitations of existing VLA models that rely solely on language input.By enabling the model to interpret various forms of human prompting, including interleaved text and images, handwritten text within images, demonstration videos, OE-VLA substantially expands the scope of human-robot interaction.Experimental results demonstrate that OE-VLA not only matches the performance of conventional language-driven VLA models but also excels in handling various non-linguistic inputs.</p>
<p>Figure 1 :
1
Figure 1: The introduced OE-VLA with four open-ended robot manipulation tasks.</p>
<p>Figure 2 :
2
Figure 2: The comprehensive architecture of our OE-VLA model.</p>
<p>Figure 3 :
3
Figure 3: The data construction method for transforming traditional dataset into the target dataset with open-ended instructions.</p>
<p>Figure 4 :
4
Figure 4: The introduced two new benchmarks with diverse open-ended task instructions.</p>
<p>Figure 6 :
6
Figure6: Performance of our method on the OE-CALVIN hard benchmark.The upper row presents the results for the ABC→D split, while the lower row presents the results for the ABCD→D split.</p>
<p>1 &gt; to open <img 2 > Follow the command in <img 1 >
Reach the state in thePerform the demo<img 1 ><img 1 ><img 2 ><img 1 ><img 1 ><img 1 ><img 2 ><img 1 ><img 1 >
"." and execute them.Press the &lt;img</p>
<p>actions in <img 1 >| <img 2 >|<img 3 >|<img 4 >
<img 1 ><img 2 ><img 3 ><img 4 ></p>
<p>Visual Object Specification Optical Instruction Following Visual Goal Reaching Video Demo Learning
BASE<img 1 ><img 2 ><img 3 ><img 4 >HARD</p>
<p>Table 1 :
1
Performance of different robot policy models with ABC→D splitting.
ModelsVLABase VLMLH-1LH-2LH-3LH-4LH-5LenMCIL✗-30.4%1.3%0.2%0.0%0.0%0.31RT-1✗-53.3% 22.2%9.4%3.8%1.3%0.90RoboFlam.✓Flamingo-3B 82.4% 61.9% 46.6% 33.1% 23.5% 2.48OpenVLA✓Prismatic-7B 62.8% 18.3%5.8%1.8%1.0%0.90LLaVA-VLA✓LLaVA-7B83.1% 58.4% 34.7% 23.1% 15.1% 2.14KosMos Inter.✓KosMos-2B82.4% 68.4% 52.4% 37.6% 29.6% 2.70OE-VLA 1b✓Interleave-1B 92.3% 69.5% 49.5% 34.1% 24.5% 2.70OE-VLA 7b✓Interleave-7B 91.8% 73.8% 56.2% 44.2% 33.4% 2.99</p>
<p>Table 2 :
2
Performance on our introduced OE-CALVIN base (ABC→D) benchmark.In this section, we evaluate our method on both the OE-CALVIN base and OE-CALVIN hard benchmarks using open-ended instructions.It can be observed from Table 2 that our method achieves
ModelsTask Type LH-1LH-2LH-3LH-4LH-5LenVOS94.0% 78.7% 56.3% 42.0% 30.0% 3.01OIF92.3% 70.0% 51.7% 35.7% 24.3% 2.74OE-VLA 1bVGR88.3% 63.7% 39.7% 24.0% 14.7% 2.30VDL93.7% 72.3% 55.7% 42.0% 31.3% 2.95Avg.-----2.75VOS95.0% 83.3% 68.7% 54.3% 44.7% 3.46OIF92.3% 81.3% 70.7% 61.0% 52.3% 3.58OE-VLA 7bVGR89.7% 76.0% 64.0% 51.7% 44.3% 3.26VDL93.3% 80.7% 72.0% 61.7% 51.7% 3.60Avg.-----3.48</p>
<p>Table 3 :
3
Performance on our introduced OE-CALVIN hard (ABC→D) benchmark.
ModelsTask Type LH-1LH-2LH-3LH-4LH-5LenVOS85.5% 61.8% 43.2% 29.7% 22.6% 2.43OIF73.3% 48.0% 27.4% 16.2%8.8%1.74OE-VLA 1bVGR64.5% 35.5% 16.2%7.8%4.7%1.29VDL73.6% 44.9% 27.4% 13.9%9.5%1.70Avg.-----1.79VOS92.9% 77.7% 63.9% 54.4% 43.9% 3.33OIF90.9% 75.7% 61.1% 49.3% 36.8% 3.14OE-VLA 7bVGR54.7% 27.7% 17.2% 14.5% 11.1% 1.25VDL88.2% 71.3% 55.1% 46.6% 38.9% 3.00Avg.</p>
<p>Table 4 :
4
Performance of our model trained solely on textual instructions with the ABC→D splitting.
ModelsLH-1LH-2LH-3LH-4LH-5 LenOE-VLA-text 1b 93.2% 76.4% 61.0% 48.7% 39.6% 3.18OE-VLA-text 7b 95.1% 83.7% 68.7% 56.1% 45.0% 3.49</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, ; , Ryan Lowe, Jan Leike,. 2022</p>
<p>Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, 2023</p>
<p>Gemini: A Family of Highly Capable Multimodal Models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Alayrac , arXiv:2312.11805June 2024</p>
<p>Visual Instruction Tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in Neural Information Processing Systems. December 202336</p>
<p>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningPMLRJuly 2023a</p>
<p>Openflamingo: An open-source framework for training large autoregressive vision-language models. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Yonatan Kalyani Marathe, Samir Bitton, Shiori Gadre, Jenia Sagawa, Simon Jitsev, Pang Wei Kornblith, Gabriel Koh, Mitchell Ilharco, Ludwig Wortsman, Schmidt, arXiv:2308.013902023arXiv preprint</p>
<p>Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models. Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy , Dorsa Sadigh, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine LearningPMLRJuly 2024</p>
<p>LLaMA-adapter: Efficient fine-tuning of large language models with zero-initialized attention. Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Yu Qiao, Hongsheng Li, Peng Gao, The Twelfth International Conference on Learning Representations. 2024</p>
<p>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, R Pannag, Grecia Sanketi, Michael S Salazar, Krista Ryoo, Kanishka Reymann, Karl Rao, Igor Pertsch, Henryk Mordatch, Yao Michalewski, Sergey Lu, Lisa Levine, Tsang-Wei Edward Lee, Isabel Lee, Yuheng Leal, Dmitry Kuang, Ryan Kalashnikov, Nikhil J Julian, Alex Joshi, Brian Irpan, Jasmine Ichter, Alexander Hsu, Karol Herzog, Keerthana Hausman, Chuyuan Gopalakrishnan, Pete Fu, Chelsea Florence, Finn, Avinava Kumar, Danny Dubey, Tianli Driess, Ding, Marcin Krzysztof, Xi Choromanski, Chen, Proceedings of The 7th Conference on Robot Learning. Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, Kehang Han, The 7th Conference on Robot LearningPMLRDecember 2023</p>
<p>. Abhishek Padalkar, Embodiment CollaborationAcorn Pooley, Embodiment CollaborationAjay Mandlekar, Embodiment CollaborationAjinkya Jain, Embodiment CollaborationAlex Tung, Embodiment CollaborationAlex Bewley, Embodiment CollaborationAlex Herzog, Embodiment CollaborationAlexander Irpan, Embodiment CollaborationAnant Khazatsky, Embodiment CollaborationAnikait Rai, Embodiment CollaborationAnimesh Singh, Embodiment CollaborationAnthony Garg, Embodiment CollaborationAntonin Brohan, Embodiment CollaborationAyzaan Raffin, Embodiment CollaborationBen Wahid, Embodiment CollaborationBeomjoon Burgess-Limerick, Embodiment CollaborationBernhard Kim, Embodiment CollaborationBrian Schölkopf, Embodiment CollaborationCewu Ichter, Embodiment CollaborationCharles Lu, Embodiment CollaborationChelsea Xu, Embodiment CollaborationChenfeng Finn, Embodiment CollaborationCheng Xu, Embodiment CollaborationChenguang Chi, Embodiment CollaborationChristine Huang, Embodiment CollaborationChuer Chan, Embodiment CollaborationChuyuan Pan, Embodiment CollaborationColine Fu, Embodiment CollaborationDanny Devin, Embodiment CollaborationDeepak Driess, Embodiment CollaborationDhruv Pathak, Embodiment CollaborationDieter Shah, Embodiment CollaborationDmitry Büchler, Embodiment CollaborationDorsa Kalashnikov, Embodiment CollaborationEdward Sadigh, Embodiment CollaborationFederico Johns, Embodiment CollaborationFei Ceola, Embodiment CollaborationFreek Xia, Embodiment CollaborationGaoyue Stulp, Embodiment CollaborationGaurav S Zhou, Embodiment CollaborationGautam Sukhatme, Embodiment CollaborationGe Salhotra, Embodiment CollaborationGiulio Yan, Embodiment CollaborationGregory Schiavi, Embodiment CollaborationHao Kahn, Embodiment CollaborationHao-Shu Su, Embodiment CollaborationHaochen Fang, Embodiment CollaborationHeni Shi, Embodiment CollaborationHenrik I Ben Amor, Embodiment CollaborationHiroki Christensen, Embodiment CollaborationHomer Furuta, Embodiment CollaborationHongjie Walke, Embodiment CollaborationIgor Fang, Embodiment CollaborationIlija Mordatch, Embodiment CollaborationIsabel Radosavovic, Embodiment CollaborationJacky Leal, Embodiment CollaborationJad Liang, Embodiment CollaborationJaehyung Abou-Chakra, Embodiment CollaborationJan Kim, Embodiment CollaborationJan Peters, Embodiment CollaborationJasmine Schneider, Embodiment CollaborationJeannette Hsu, Embodiment CollaborationJeffrey Bohg, Embodiment CollaborationJiajun Bingham, Embodiment CollaborationJialin Wu, Embodiment CollaborationJianlan Wu, Embodiment CollaborationJiayuan Luo, Embodiment CollaborationJie Gu, Embodiment CollaborationJihoon Tan, Embodiment CollaborationJitendra Oh, Embodiment CollaborationJonathan Malik, Embodiment CollaborationJonathan Booher, Embodiment CollaborationJonathan Tompson, Embodiment CollaborationJoseph J Yang, Embodiment CollaborationJoão Lim, Embodiment CollaborationJunhyek Silvério, Embodiment CollaborationKanishka Han, Embodiment CollaborationKarl Rao, Embodiment CollaborationKarol Pertsch, Embodiment CollaborationKeegan Hausman, Embodiment CollaborationKeerthana Go, Embodiment CollaborationKen Gopalakrishnan, Embodiment CollaborationKendra Goldberg, Embodiment CollaborationKenneth Byrne, Embodiment CollaborationKento Oslund, Embodiment CollaborationKevin Kawaharazuka, Embodiment CollaborationKrishan Zhang, Embodiment CollaborationKrishnan Rana, Embodiment CollaborationLawrence Srinivasan, Embodiment CollaborationLerrel Yunliang Chen, Embodiment CollaborationLi Pinto, Embodiment CollaborationLiam Fei-Fei, Embodiment CollaborationLionel Tan, Embodiment CollaborationLisa Ott, Embodiment CollaborationMasayoshi Lee, Embodiment CollaborationMax Tomizuka, Embodiment CollaborationMaximilian Spero, Embodiment CollaborationMichael Du, Embodiment CollaborationMingtong Ahn, Embodiment CollaborationMingyu Zhang, Embodiment CollaborationMohan Ding, Embodiment CollaborationMohit Kumar Srirama, Embodiment CollaborationSharma, Embodiment CollaborationJin Moo, Embodiment CollaborationNaoaki Kim, Embodiment CollaborationNicklas Kanazawa, Embodiment CollaborationNicolas Hansen, Embodiment CollaborationHeess, Embodiment CollaborationJ Nikhil, Embodiment CollaborationNiko Joshi, Embodiment CollaborationNorman Suenderhauf, Embodiment CollaborationNur Di Palo, Embodiment CollaborationMuhammad Mahi, Embodiment CollaborationOier Shafiullah, Embodiment CollaborationOliver Mees, Embodiment CollaborationKroemer, Embodiment CollaborationR Pannag, Embodiment CollaborationPaul Sanketi, Embodiment CollaborationPeng Wohlhart, Embodiment CollaborationPierre Xu, Embodiment CollaborationPriya Sermanet, Embodiment CollaborationQuan Sundaresan, Embodiment CollaborationRafael Vuong, Embodiment CollaborationRan Rafailov, Embodiment CollaborationRia Tian, Embodiment CollaborationRoberto Doshi, Embodiment CollaborationRussell Martín-Martín, Embodiment CollaborationRutav Mendonca, Embodiment CollaborationRyan Shah, Embodiment CollaborationRyan Hoque, Embodiment CollaborationSamuel Julian, Embodiment CollaborationSean Bustamante, Embodiment CollaborationSergey Kirmani, Embodiment CollaborationSherry Levine, Embodiment CollaborationShikhar Moore, Embodiment CollaborationShivin Bahl, Embodiment CollaborationShubham Dass, Embodiment CollaborationShuran Sonawani, Embodiment CollaborationSichun Song, Embodiment CollaborationSiddhant Xu, Embodiment CollaborationSimeon Haldar, Embodiment CollaborationSimon Adebola, Embodiment CollaborationSoroush Guist, Embodiment CollaborationStefan Nasiriany, Embodiment CollaborationStefan Schaal, Embodiment CollaborationStephen Welker, Embodiment CollaborationSudeep Tian, Embodiment CollaborationSuneel Dasari, Embodiment CollaborationTakayuki Belkhale, Embodiment CollaborationTatsuya Osa, Embodiment CollaborationTatsuya Harada, Embodiment CollaborationTed Matsushima, Embodiment CollaborationTianhe Xiao, Embodiment CollaborationTianli Yu, Embodiment CollaborationTodor Ding, Embodiment CollaborationTony Z Davchev, Embodiment CollaborationTravis Zhao, Embodiment CollaborationTrevor Armstrong, Embodiment CollaborationVidhi Darrell, Embodiment CollaborationVincent Jain, Embodiment CollaborationWei Vanhoucke, Embodiment CollaborationWenxuan Zhan, Embodiment CollaborationWolfram Zhou, Embodiment CollaborationXi Burgard, Embodiment CollaborationXiaolong Chen, Embodiment CollaborationXinghao Wang, Embodiment CollaborationXuanlin Zhu, Embodiment CollaborationYao Li, Embodiment CollaborationYevgen Lu, Embodiment CollaborationYifan Chebotar, Embodiment CollaborationYifeng Zhou, Embodiment CollaborationYing Zhu, Embodiment CollaborationYixuan Xu, Embodiment CollaborationWang, Embodiment CollaborationOctober 2023Yueh-Hua WuLee, Yuchen Cui; Yutaka MatsuoYujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa. Zhuo Xu, and Zichen Jeff Cui. Open X-Embodiment: Robotic Learning Datasets and RT-X Models</p>
<p>OpenVLA: An Open-Source Vision-Language-Action Model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Quan Sanketi, Thomas Vuong, Benjamin Kollar, Russ Burchfiel, Dorsa Tedrake, Sergey Sadigh, Percy Levine, Chelsea Liang, Finn, arXiv:2406.09246June 2024</p>
<p>RT-1: Robotics Transformer for Real-World Control at Scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich, 10.15607/RSS.2023.XIX.025Robotics: Science and Systems XIX. Robotics: Science and Systems Foundation. July 2023</p>
<p>Language Conditioned Imitation Learning Over Unstructured Data. Corey Lynch, * , Pierre Sermanet, * , 10.15607/RSS.2021.XVII.047Robotics: Science and Systems XVII. Robotics: Science and Systems Foundation. July 2021</p>
<p>Vision-Language Foundation Models as Effective Robot Imitators. Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, Tao Kong, The Twelfth International Conference on Learning Representations. October 2023b</p>
<p>CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation. Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Yu Wei, Sicheng Deng, Yizhong Xu, Xiaofan Zhang, Bei Wang, Jianlong Liu, Jianmin Fu, Dong Bao, Yuanchun Chen, Jiaolong Shi, Baining Yang, Guo, arXiv:2411.19650November 2024a</p>
<p>RDT-1b: a diffusion foundation model for bimanual manipulation. Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, Jun Zhu, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models. Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, Huaping Liu, arXiv:2412.14058December 2024b</p>
<p>TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation. Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, Feifei Feng, Jian Tang, arXiv:2409.12514May 2025a</p>
<p>Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression. Junjie Wen, Minjie Zhu, Yichen Zhu, Zhibin Tang, Jinming Li, Zhongyi Zhou, Chengmeng Li, Xiaoyu Liu, Yaxin Peng, Chaomin Shen, Feifei Feng, arXiv:2412.03293May 2025b</p>
<p>Attention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Long Beach, CA201730</p>
<p>Diffusion Policy: Visuomotor Policy Learning via Action Diffusion. Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, Shuran Song, 10.15607/RSS.2023.XIX.026Robotics: Science and Systems XIX. Robotics: Science and Systems Foundation. July 2023</p>
<p>$pi_0$: A Vision-Language-Action Flow Model for General Robot Control. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, Ury Zhilinsky, arXiv:2410.24164November 2024</p>
<p>3D-VLA: A 3D Vision-Language-Action Generative World Model. Xiaowen Haoyu Zhen, Peihao Qiu, Jincheng Chen, Xin Yang, Yilun Yan, Yining Du, Chuang Hong, Gan, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine LearningPMLRJuly 2024</p>
<p>Dream to manipulate: Compositional world models empowering robot imitation learning with imagination. Leonardo Barcellona, Andrii Zadaianchuk, Davide Allegro, Samuele Papa, Stefano Ghidoni, Efstratios Gavves, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Johan Nvidia, Fernando Bjorck, Nikita Castañeda, Xingye Cherniadev, Runyu Da, Ding, " Linxi, Yu Jim" Fan, Dieter Fang, Fengyuan Fox, Spencer Hu, Joel Huang, Zhenyu Jang, Jan Jiang, Kaushil Kautz, Lawrence Kundalia, Zhiqi Lao, Zongyu Li, Kevin Lin, Guilin Lin, Edith Liu, Loic Llontop, Ajay Magne, Avnish Mandlekar, Soroush Narayan, Scott Nasiriany, You Reed, Guanzhi Liang Tan, Zu Wang, Jing Wang, Qi Wang, Jiannan Wang, Yuqi Xiang, Yinzhen Xie, Zhenjia Xu, Seonghyeon Xu, Zhiding Ye, Ao Yu, Hao Zhang, Yizhou Zhang, Zhao, arXiv:2503.14734Ruijie Zheng, and Yuke Zhu. GR00T N1: An Open Foundation Model for Generalist Humanoid Robots. March 2025</p>
<p>RT-H: Action Hierarchies Using Language. Tianli Suneel Belkhale, Ted Ding, Pierre Xiao, Sermanet, Jonathan Vuong, Yevgen Tompson, Debidatta Chebotar, Dorsa Dwibedi, Sadigh, arXiv:2403.01823May 2024</p>
<p>Robotic control via embodied chain-of-thought reasoning. Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, Sergey Levine, 8th Annual Conference on Robot Learning. 2024</p>
<p>DayDreamer: World Models for Physical Robot Learning. Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, Ken Goldberg, Proceedings of The 6th Conference on Robot Learning. The 6th Conference on Robot LearningPMLRMarch 2023</p>
<p>VIMA: Robot Manipulation with Multimodal Prompts. Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningPMLRJuly 2023</p>
<p>Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions. Cunxin Fan, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei, Ziyang Gong, Xiangyu Zhao, Masayoshi Tomizuka, Xue Yang, Junchi Yan, Mingyu Ding, arXiv:2505.02152May 2025</p>
<p>LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models. Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, Chunyuan Li, arXiv:2407.07895July 2024c</p>
<p>Sigmoid Loss for Language Image Pre-Training. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer, 10.1109/ICCV51070.2023.011002023 IEEE/CVF International Conference on Computer Vision (ICCV). Paris, FranceOctober 2023</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, International Conference on Learning Representations. 2021</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu, arXiv:2309.16609Qwen Technical Report. September 2023</p>
<p>Migician: Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models. You Li, Heyu Huang, Chi Chen, Kaiyu Huang, Chao Huang, Zonghao Guo, Zhiyuan Liu, Jinan Xu, Yuhua Li, Ruixuan Li, Maosong Sun, arXiv:2501.05767January 2025</p>
<p>CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks. Oier Mees, Lukas Hermann, Erick Rosete-Beas, Wolfram Burgard, arXiv:2112.03227July 2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5608 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5608</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5608</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-a66ff335f5934fe7503a99d3eb3abed493994df1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a66ff335f5934fe7503a99d3eb3abed493994df1" target="_blank">NLPositionality: Characterizing Design Biases of Datasets and Models</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models, is introduced and it is found that dataset and models align predominantly with Western, White, college-educated, and younger populations.</p>
                <p><strong>Paper Abstract:</strong> Design biases in NLP systems, such as performance differences for different populations, often stem from their creator’s positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks—social acceptability and hate speech detection. To date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries.We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as non-binary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5608.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5608.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose large language model used zero-shot in this paper to produce categorical judgments for social acceptability and hate-speech classification; its outputs were compared to aggregated, demographically-grouped human annotations to quantify positionality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General-purpose large language model (GPT-4) evaluated in zero-shot settings via prompt templates; in this study GPT-4 produced categorical labels for social acceptability (good/okay/bad) and hate-speech (hate/not sure/not hate).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Social NLP / computational social science (moral reasoning / social acceptability and hate-speech/toxicity detection)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Zero-shot text classification: (1) judge social acceptability of scenarios (Likert categories aggregated to good/okay/bad), and (2) detect hate speech (hate / not sure / not hate).</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Pearson's r correlation between model outputs (categorical labels for GPT-4) and aggregated human annotations by demographic; statistical significance via Bonferroni-corrected p-values; Krippendorff's alpha reported for inter-annotator agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Reported correlations vary by demographic: Social acceptability — e.g., r ≈ 0.74 (people who grew up in English-speaking countries), r ≈ 0.73 (people who live in English-speaking countries), r ≈ 0.69 (college-educated), r ≈ 0.70 (White), r ≈ 0.70 (age 20-30). Hate speech detection — lower correlations: e.g., r ≈ 0.41 (people who grew up in English-speaking countries), r ≈ 0.42 (people who live in English-speaking countries), r ≈ 0.39 (college-educated), r ≈ 0.38 (White), r ≈ 0.42 (age 20-30). (Values reported per demographic in Table 2 of the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Demographic and cultural alignment (country of upbringing/residence, native language), education level, ethnicity, age, gender (non-binary groups showed lower alignment), annotator population of original datasets (dataset positionality), task subjectivity, prompt/template (zero-shot prompt design).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Systematic differences in Pearson's r across demographic groups (Table 2) showing higher alignment with Western/English-speaking, White, college-educated, younger populations; comparison to dataset annotator demographics showing dataset-model alignment with original annotators; statistically significant p-values after Bonferroni correction reported (Appendix C.1).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>For each demographic group, aggregated human annotation scores (mean per instance) were computed and Pearson's r was calculated between that vector and the model's predictions (GPT-4 categorical labels). Bonferroni correction applied for multiple tests; Krippendorff's alpha used to report inter-annotator agreement per demographic group.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>GPT-4 exhibits positionality/biased alignment: stronger agreement with WEIRD (Western, Educated, Industrialized, Rich, Democratic) populations and native English speakers; lower alignment with marginalized groups (non-binary, Black, Latinx, Native American, non-native English speakers). Reported correlations vary substantially by demographic and are task-dependent (higher for social acceptability subset studied, lower for hate-speech subset).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared directly to Delphi, Perspective API, Rewire API, ToxiGen RoBERTa, and original datasets (Social Chemistry and Dynahate). GPT-4 showed similar WEIRD-skewed alignment to datasets and Delphi but with differing strengths per demographic and per task (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>The paper recommends documenting model and dataset positionality, recruiting diverse annotators, reporting disaggregated annotations, using perspectivist approaches (representing distributions of perspectives rather than forcing single labels), and culturally adaptive modeling — all intended to mitigate misalignment observed in GPT-4 and other models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLPositionality: Characterizing Design Biases of Datasets and Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5608.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5608.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Delphi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Delphi (Jiang et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A morality-prediction model (originally presented by Jiang et al.) used here as a comparator; evaluated zero-shot for social acceptability and its outputs correlated with demographic-grouped annotations to measure positionality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can machines learn morality? the delphi experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Delphi</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Delphi is a morality model introduced by Jiang et al. (2021); in this paper it is treated as a model producing social acceptability judgments (using the original GPT-3 zero-shot evaluation setup from the Delphi paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Social NLP / computational ethics / moral reasoning (social acceptability)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Zero-shot classification of social scenarios into acceptability categories (good/okay/bad) to simulate moral judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Pearson's r correlation between Delphi's predictions (probability scores) and aggregated human annotations by demographic; Bonferroni-corrected p-values; Krippendorff's alpha for annotator reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Delphi correlations with demographics reported in Table 2 — e.g., Social acceptability: r ≈ 0.61–0.66 for English-speaking and college-educated groups, and up to r ≈ 0.72 for some non-English groups (e.g., grew up in Baltic countries); overall Delphi shows moderate positive correlations with WEIRD groups but with nuanced variation across cultural spheres.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Cultural and demographic alignment (country, language, education, ethnicity), the annotator population used for Delphi's development, and task subjectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Observed differences in Pearson's r across demographics in Table 2 and comparison notes in §4.1 showing Delphi aligns with English-speaking and college-educated demographics but also shows alignment with some non-English groups (e.g., Baltic).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Aggregated demographic annotation vectors correlated with Delphi's output probability scores using Pearson's r; statistical testing with Bonferroni correction; Krippendorff's alpha reported for annotator agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Delphi exhibits similar WEIRD-skewed alignment as datasets and other models; it does not uniformly represent non-Western perspectives and shows variation by cultural sphere (some unexpected stronger alignments for non-English groups).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared to GPT-4, dataset labels (Social Chemistry), and other toxicity models; Delphi often aligns with WEIRD groups but less extremely than some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Same community-level recommendations as for other models: document positionality, recruit diverse annotators, surface disaggregated judgments rather than forcing single consensus labels, and consider culturally adaptive approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLPositionality: Characterizing Design Biases of Datasets and Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5608.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5608.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PerspectiveAPI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perspective API (toxicity detection API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used toxicity-detection API employed as a comparand in the hate-speech experiments; its predictions were correlated with demographic-grouped human annotations to assess positionality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Perspective API</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A deployed toxicity/hate-speech scoring API (accessed via perspectiveapi.com) used to score Dynahate instances; model internals not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Social NLP / toxicity and hate-speech detection</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Automatic toxicity/hate-speech scoring of short text instances to simulate human judgments of hatefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Pearson's r correlation between Perspective API scores (probabilities) and aggregated human annotations by demographic; Bonferroni-corrected p-values; Krippendorff's alpha for human annotator agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Perspective API shows moderate but lower correlations with WEIRD groups compared to datasets: e.g., Dynahate alignment with English-speaking groups r ≈ 0.33–0.34, college-educated r ≈ 0.34, White r ≈ 0.29, age 20–30 r ≈ 0.34 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Alignment with English-speaking and Western perspectives, dataset bias (Dynahate), cultural/language variation in expressions of hate, and annotator demographics.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Differences in Pearson's r across demographic groups in Table 2 and the discussion in §4.2 noting Perspective API's tendency to align with WEIRD populations though to a lesser degree than Dynahate.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Correlate Perspective API output scores for each instance with aggregated human annotation vectors per demographic using Pearson's r; significance via Bonferroni correction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Shows Western/English bias and lower alignment for some demographics; White group displayed unexpectedly low Pearson's r in some comparisons; overall alignment is lower than some dataset-model pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to Dynahate labels, GPT-4, Rewire API, ToxiGen RoBERTa and others; generally shows weaker but present WEIRD-skewed alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Authors recommend documenting positionality and diversifying annotators and datasets to reduce mismatches between toxicity APIs and non-Western perspectives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLPositionality: Characterizing Design Biases of Datasets and Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5608.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5608.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RewireAPI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rewire API (toxicity detection API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online toxicity/hate-speech detection API used as another comparator on the Dynahate hate-speech task; its outputs were correlated with demographic-grouped annotations to measure alignment and bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Rewire API</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A deployed toxicity/hate-speech classifier accessed via rewire.online; the paper treats it as a black-box model and uses its predictions for correlation analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Social NLP / toxicity and hate-speech detection</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Automatic classification of text as hate speech or not to simulate human judgments of hatefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Pearson's r correlation between Rewire API outputs and aggregated human annotations by demographic; Bonferroni-corrected p-values; Krippendorff's alpha reported for annotator groups.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Rewire API shows moderate alignment with WEIRD populations: e.g., correlations for Dynahate with English-speaking groups r ≈ 0.58–0.60, college-educated r ≈ 0.56, White r ≈ 0.56, age 20–30 r ≈ 0.56 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Cultural and linguistic coverage (English vs. non-English), dataset composition, and annotator demographics.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Pearson's r differences across demographic groups in Table 2 and narrative in §4.2 describing Rewire API's moderate correlations with English-speaking and WEIRD groups.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Correlation of API output probabilities/labels with aggregated demographic annotation vectors via Pearson's r; Bonferroni correction for statistical testing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Displays a Western bias in alignment and lower alignment for marginalized demographic groups; performance varies by cultural sphere and demographic group.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared with Perspective API, ToxiGen RoBERTa, GPT-4, and Dynahate labels; shows intermediate alignment in this suite of models.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Same dataset/model positionality recommendations: document design choices, recruit diverse annotators, represent disaggregated perspectives rather than collapse to a single label.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLPositionality: Characterizing Design Biases of Datasets and Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5608.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5608.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToxiGen RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToxiGen RoBERTa (model evaluated using ToxiGen material)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RoBERTa-based toxicity model evaluated in the hate-speech case study (cited via Hartvigsen et al., 2022); used as a supervised comparator whose outputs were correlated with demographic-grouped annotations to quantify positionality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toxigen: Controlling language models to generate implied and adversarial toxicity.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ToxiGen RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A supervised model variant (RoBERTa) associated with the ToxiGen project/paper; in this study it is used as a toxicity classifier (treated as a black-box predictor) and compared against human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Social NLP / toxicity and hate-speech detection</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Supervised classification of text as hateful or not (toxicity detection) to simulate human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Pearson's r correlation between model predictions and aggregated demographic annotations; Bonferroni-corrected p-values; Krippendorff's alpha used to report annotator agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>ToxiGen RoBERTa correlations in Table 2 indicate some alignment with English-speaking and WEIRD groups but generally moderate: e.g., Dynahate alignment r ≈ 0.37–0.38 for English-speaking groups, college-educated r ≈ 0.38, White r ≈ 0.32, age 20–30 r ≈ 0.38.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Training data representativeness (ToxiGen-generated examples), cultural and language mismatch, dataset positionality, and demographic subjectivity of hate judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Observed demographic-correlated Pearson's r values and discussion in §4.2 showing Western skew; paper cites Hartvigsen et al. (2022) for ToxiGen but uses the model outputs in correlation analyses to show positionality effects.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Correlation between ToxiGen RoBERTa outputs and aggregated LabintheWild annotations per demographic using Pearson's r; significance testing via Bonferroni correction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Shows Western/English bias in alignment; lower concordance with marginalized demographics; performance varies by cultural sphere and dataset subset.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared with Perspective API, Rewire API, GPT-4, Dynahate dataset labels; shows similar WEIRD-skewed behavior though magnitude varies by model and demographic.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Authors recommend dataset and model documentation, diverse annotator recruitment, and modeling approaches that represent multiple perspectives rather than enforcing a single aggregated label.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLPositionality: Characterizing Design Biases of Datasets and Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>Can machines learn morality? the delphi experiment. <em>(Rating: 2)</em></li>
                <li>Toxigen: Controlling language models to generate implied and adversarial toxicity. <em>(Rating: 2)</em></li>
                <li>Annotators with attitudes: How annotator beliefs and identities bias toxic language detection <em>(Rating: 2)</em></li>
                <li>Detecting crossgeographic biases in toxicity modeling on social media <em>(Rating: 1)</em></li>
                <li>Learning from the worst: Dynamically generated datasets to improve online hate detection <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5608",
    "paper_id": "paper-a66ff335f5934fe7503a99d3eb3abed493994df1",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A general-purpose large language model used zero-shot in this paper to produce categorical judgments for social acceptability and hate-speech classification; its outputs were compared to aggregated, demographically-grouped human annotations to quantify positionality.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "General-purpose large language model (GPT-4) evaluated in zero-shot settings via prompt templates; in this study GPT-4 produced categorical labels for social acceptability (good/okay/bad) and hate-speech (hate/not sure/not hate).",
            "model_size": null,
            "scientific_subdomain": "Social NLP / computational social science (moral reasoning / social acceptability and hate-speech/toxicity detection)",
            "simulation_task": "Zero-shot text classification: (1) judge social acceptability of scenarios (Likert categories aggregated to good/okay/bad), and (2) detect hate speech (hate / not sure / not hate).",
            "accuracy_metric": "Pearson's r correlation between model outputs (categorical labels for GPT-4) and aggregated human annotations by demographic; statistical significance via Bonferroni-corrected p-values; Krippendorff's alpha reported for inter-annotator agreement.",
            "reported_accuracy": "Reported correlations vary by demographic: Social acceptability — e.g., r ≈ 0.74 (people who grew up in English-speaking countries), r ≈ 0.73 (people who live in English-speaking countries), r ≈ 0.69 (college-educated), r ≈ 0.70 (White), r ≈ 0.70 (age 20-30). Hate speech detection — lower correlations: e.g., r ≈ 0.41 (people who grew up in English-speaking countries), r ≈ 0.42 (people who live in English-speaking countries), r ≈ 0.39 (college-educated), r ≈ 0.38 (White), r ≈ 0.42 (age 20-30). (Values reported per demographic in Table 2 of the paper.)",
            "factors_affecting_accuracy": "Demographic and cultural alignment (country of upbringing/residence, native language), education level, ethnicity, age, gender (non-binary groups showed lower alignment), annotator population of original datasets (dataset positionality), task subjectivity, prompt/template (zero-shot prompt design).",
            "evidence_for_factors": "Systematic differences in Pearson's r across demographic groups (Table 2) showing higher alignment with Western/English-speaking, White, college-educated, younger populations; comparison to dataset annotator demographics showing dataset-model alignment with original annotators; statistically significant p-values after Bonferroni correction reported (Appendix C.1).",
            "evaluation_method": "For each demographic group, aggregated human annotation scores (mean per instance) were computed and Pearson's r was calculated between that vector and the model's predictions (GPT-4 categorical labels). Bonferroni correction applied for multiple tests; Krippendorff's alpha used to report inter-annotator agreement per demographic group.",
            "limitations_or_failure_cases": "GPT-4 exhibits positionality/biased alignment: stronger agreement with WEIRD (Western, Educated, Industrialized, Rich, Democratic) populations and native English speakers; lower alignment with marginalized groups (non-binary, Black, Latinx, Native American, non-native English speakers). Reported correlations vary substantially by demographic and are task-dependent (higher for social acceptability subset studied, lower for hate-speech subset).",
            "comparisons": "Compared directly to Delphi, Perspective API, Rewire API, ToxiGen RoBERTa, and original datasets (Social Chemistry and Dynahate). GPT-4 showed similar WEIRD-skewed alignment to datasets and Delphi but with differing strengths per demographic and per task (see Table 2).",
            "recommendations_or_best_practices": "The paper recommends documenting model and dataset positionality, recruiting diverse annotators, reporting disaggregated annotations, using perspectivist approaches (representing distributions of perspectives rather than forcing single labels), and culturally adaptive modeling — all intended to mitigate misalignment observed in GPT-4 and other models.",
            "uuid": "e5608.0",
            "source_info": {
                "paper_title": "NLPositionality: Characterizing Design Biases of Datasets and Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Delphi",
            "name_full": "Delphi (Jiang et al., 2021)",
            "brief_description": "A morality-prediction model (originally presented by Jiang et al.) used here as a comparator; evaluated zero-shot for social acceptability and its outputs correlated with demographic-grouped annotations to measure positionality.",
            "citation_title": "Can machines learn morality? the delphi experiment.",
            "mention_or_use": "use",
            "model_name": "Delphi",
            "model_description": "Delphi is a morality model introduced by Jiang et al. (2021); in this paper it is treated as a model producing social acceptability judgments (using the original GPT-3 zero-shot evaluation setup from the Delphi paper).",
            "model_size": null,
            "scientific_subdomain": "Social NLP / computational ethics / moral reasoning (social acceptability)",
            "simulation_task": "Zero-shot classification of social scenarios into acceptability categories (good/okay/bad) to simulate moral judgments.",
            "accuracy_metric": "Pearson's r correlation between Delphi's predictions (probability scores) and aggregated human annotations by demographic; Bonferroni-corrected p-values; Krippendorff's alpha for annotator reliability.",
            "reported_accuracy": "Delphi correlations with demographics reported in Table 2 — e.g., Social acceptability: r ≈ 0.61–0.66 for English-speaking and college-educated groups, and up to r ≈ 0.72 for some non-English groups (e.g., grew up in Baltic countries); overall Delphi shows moderate positive correlations with WEIRD groups but with nuanced variation across cultural spheres.",
            "factors_affecting_accuracy": "Cultural and demographic alignment (country, language, education, ethnicity), the annotator population used for Delphi's development, and task subjectivity.",
            "evidence_for_factors": "Observed differences in Pearson's r across demographics in Table 2 and comparison notes in §4.1 showing Delphi aligns with English-speaking and college-educated demographics but also shows alignment with some non-English groups (e.g., Baltic).",
            "evaluation_method": "Aggregated demographic annotation vectors correlated with Delphi's output probability scores using Pearson's r; statistical testing with Bonferroni correction; Krippendorff's alpha reported for annotator agreement.",
            "limitations_or_failure_cases": "Delphi exhibits similar WEIRD-skewed alignment as datasets and other models; it does not uniformly represent non-Western perspectives and shows variation by cultural sphere (some unexpected stronger alignments for non-English groups).",
            "comparisons": "Directly compared to GPT-4, dataset labels (Social Chemistry), and other toxicity models; Delphi often aligns with WEIRD groups but less extremely than some datasets.",
            "recommendations_or_best_practices": "Same community-level recommendations as for other models: document positionality, recruit diverse annotators, surface disaggregated judgments rather than forcing single consensus labels, and consider culturally adaptive approaches.",
            "uuid": "e5608.1",
            "source_info": {
                "paper_title": "NLPositionality: Characterizing Design Biases of Datasets and Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "PerspectiveAPI",
            "name_full": "Perspective API (toxicity detection API)",
            "brief_description": "A widely used toxicity-detection API employed as a comparand in the hate-speech experiments; its predictions were correlated with demographic-grouped human annotations to assess positionality.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Perspective API",
            "model_description": "A deployed toxicity/hate-speech scoring API (accessed via perspectiveapi.com) used to score Dynahate instances; model internals not specified in this paper.",
            "model_size": null,
            "scientific_subdomain": "Social NLP / toxicity and hate-speech detection",
            "simulation_task": "Automatic toxicity/hate-speech scoring of short text instances to simulate human judgments of hatefulness.",
            "accuracy_metric": "Pearson's r correlation between Perspective API scores (probabilities) and aggregated human annotations by demographic; Bonferroni-corrected p-values; Krippendorff's alpha for human annotator agreement.",
            "reported_accuracy": "Perspective API shows moderate but lower correlations with WEIRD groups compared to datasets: e.g., Dynahate alignment with English-speaking groups r ≈ 0.33–0.34, college-educated r ≈ 0.34, White r ≈ 0.29, age 20–30 r ≈ 0.34 (Table 2).",
            "factors_affecting_accuracy": "Alignment with English-speaking and Western perspectives, dataset bias (Dynahate), cultural/language variation in expressions of hate, and annotator demographics.",
            "evidence_for_factors": "Differences in Pearson's r across demographic groups in Table 2 and the discussion in §4.2 noting Perspective API's tendency to align with WEIRD populations though to a lesser degree than Dynahate.",
            "evaluation_method": "Correlate Perspective API output scores for each instance with aggregated human annotation vectors per demographic using Pearson's r; significance via Bonferroni correction.",
            "limitations_or_failure_cases": "Shows Western/English bias and lower alignment for some demographics; White group displayed unexpectedly low Pearson's r in some comparisons; overall alignment is lower than some dataset-model pairs.",
            "comparisons": "Compared to Dynahate labels, GPT-4, Rewire API, ToxiGen RoBERTa and others; generally shows weaker but present WEIRD-skewed alignment.",
            "recommendations_or_best_practices": "Authors recommend documenting positionality and diversifying annotators and datasets to reduce mismatches between toxicity APIs and non-Western perspectives.",
            "uuid": "e5608.2",
            "source_info": {
                "paper_title": "NLPositionality: Characterizing Design Biases of Datasets and Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "RewireAPI",
            "name_full": "Rewire API (toxicity detection API)",
            "brief_description": "An online toxicity/hate-speech detection API used as another comparator on the Dynahate hate-speech task; its outputs were correlated with demographic-grouped annotations to measure alignment and bias.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Rewire API",
            "model_description": "A deployed toxicity/hate-speech classifier accessed via rewire.online; the paper treats it as a black-box model and uses its predictions for correlation analyses.",
            "model_size": null,
            "scientific_subdomain": "Social NLP / toxicity and hate-speech detection",
            "simulation_task": "Automatic classification of text as hate speech or not to simulate human judgments of hatefulness.",
            "accuracy_metric": "Pearson's r correlation between Rewire API outputs and aggregated human annotations by demographic; Bonferroni-corrected p-values; Krippendorff's alpha reported for annotator groups.",
            "reported_accuracy": "Rewire API shows moderate alignment with WEIRD populations: e.g., correlations for Dynahate with English-speaking groups r ≈ 0.58–0.60, college-educated r ≈ 0.56, White r ≈ 0.56, age 20–30 r ≈ 0.56 (Table 2).",
            "factors_affecting_accuracy": "Cultural and linguistic coverage (English vs. non-English), dataset composition, and annotator demographics.",
            "evidence_for_factors": "Pearson's r differences across demographic groups in Table 2 and narrative in §4.2 describing Rewire API's moderate correlations with English-speaking and WEIRD groups.",
            "evaluation_method": "Correlation of API output probabilities/labels with aggregated demographic annotation vectors via Pearson's r; Bonferroni correction for statistical testing.",
            "limitations_or_failure_cases": "Displays a Western bias in alignment and lower alignment for marginalized demographic groups; performance varies by cultural sphere and demographic group.",
            "comparisons": "Compared with Perspective API, ToxiGen RoBERTa, GPT-4, and Dynahate labels; shows intermediate alignment in this suite of models.",
            "recommendations_or_best_practices": "Same dataset/model positionality recommendations: document design choices, recruit diverse annotators, represent disaggregated perspectives rather than collapse to a single label.",
            "uuid": "e5608.3",
            "source_info": {
                "paper_title": "NLPositionality: Characterizing Design Biases of Datasets and Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "ToxiGen RoBERTa",
            "name_full": "ToxiGen RoBERTa (model evaluated using ToxiGen material)",
            "brief_description": "A RoBERTa-based toxicity model evaluated in the hate-speech case study (cited via Hartvigsen et al., 2022); used as a supervised comparator whose outputs were correlated with demographic-grouped annotations to quantify positionality.",
            "citation_title": "Toxigen: Controlling language models to generate implied and adversarial toxicity.",
            "mention_or_use": "use",
            "model_name": "ToxiGen RoBERTa",
            "model_description": "A supervised model variant (RoBERTa) associated with the ToxiGen project/paper; in this study it is used as a toxicity classifier (treated as a black-box predictor) and compared against human annotations.",
            "model_size": null,
            "scientific_subdomain": "Social NLP / toxicity and hate-speech detection",
            "simulation_task": "Supervised classification of text as hateful or not (toxicity detection) to simulate human judgments.",
            "accuracy_metric": "Pearson's r correlation between model predictions and aggregated demographic annotations; Bonferroni-corrected p-values; Krippendorff's alpha used to report annotator agreement.",
            "reported_accuracy": "ToxiGen RoBERTa correlations in Table 2 indicate some alignment with English-speaking and WEIRD groups but generally moderate: e.g., Dynahate alignment r ≈ 0.37–0.38 for English-speaking groups, college-educated r ≈ 0.38, White r ≈ 0.32, age 20–30 r ≈ 0.38.",
            "factors_affecting_accuracy": "Training data representativeness (ToxiGen-generated examples), cultural and language mismatch, dataset positionality, and demographic subjectivity of hate judgments.",
            "evidence_for_factors": "Observed demographic-correlated Pearson's r values and discussion in §4.2 showing Western skew; paper cites Hartvigsen et al. (2022) for ToxiGen but uses the model outputs in correlation analyses to show positionality effects.",
            "evaluation_method": "Correlation between ToxiGen RoBERTa outputs and aggregated LabintheWild annotations per demographic using Pearson's r; significance testing via Bonferroni correction.",
            "limitations_or_failure_cases": "Shows Western/English bias in alignment; lower concordance with marginalized demographics; performance varies by cultural sphere and dataset subset.",
            "comparisons": "Compared with Perspective API, Rewire API, GPT-4, Dynahate dataset labels; shows similar WEIRD-skewed behavior though magnitude varies by model and demographic.",
            "recommendations_or_best_practices": "Authors recommend dataset and model documentation, diverse annotator recruitment, and modeling approaches that represent multiple perspectives rather than enforcing a single aggregated label.",
            "uuid": "e5608.4",
            "source_info": {
                "paper_title": "NLPositionality: Characterizing Design Biases of Datasets and Models",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2
        },
        {
            "paper_title": "Can machines learn morality? the delphi experiment.",
            "rating": 2
        },
        {
            "paper_title": "Toxigen: Controlling language models to generate implied and adversarial toxicity.",
            "rating": 2
        },
        {
            "paper_title": "Annotators with attitudes: How annotator beliefs and identities bias toxic language detection",
            "rating": 2
        },
        {
            "paper_title": "Detecting crossgeographic biases in toxicity modeling on social media",
            "rating": 1
        },
        {
            "paper_title": "Learning from the worst: Dynamically generated datasets to improve online hate detection",
            "rating": 1
        }
    ],
    "cost": 0.01781075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>NLPositionality: Characterizing Design Biases of Datasets and Models</h1>
<p>Sebastin Santy ${ }^{\dagger <em>}$ Jenny T. Liang ${ }^{\ddagger </em>}$<br>Ronan Le Bras ${ }^{\circ}$ Katharina Reinecke ${ }^{\dagger}$ Maarten Sap ${ }^{\ddagger <em>}$<br>${ }^{\dagger}$ University of Washington ${ }^{\ddagger}$ Carnegie Mellon University<br></em>Allen Institute for AI<br>{ssanty,reinecke}@cs.washington.edu,<br>{jtliang,maartensap}@cs.cmu.edu, ronanlb@allenai.org</p>
<h4>Abstract</h4>
<p>Design biases in NLP systems, such as performance differences for different populations, often stem from their creator's positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks-social acceptability and hate speech detection. To date, we have collected 16, 299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries. We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as nonbinary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.</p>
<h2>1 Introduction</h2>
<p>"Treating different things the same can generate as much inequality as treating the same things differently."</p>
<ul>
<li>Kimberlé Crenshaw</li>
</ul>
<p>When creating NLP datasets and models, researchers' design choices are partly influenced</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example Scenario. Carl from the U.S. and Aditya from India both want to use Perspective API, but it works better for Carl than it does for Aditya. This is because toxicity researchers' positionalities lead them to make design choices that make toxicity datasets, and thus Perspective API, to have positionalities that are Western-centric.
by their positionality, i.e., their views shaped by their lived experience, identity, culture, and background (Savin-Baden and Howell-Major, 2013). While researcher positionality is commonly discussed outside of NLP, it is highly applicable to NLP research but remains largely overlooked. For example, a U.S.-born English-speaking researcher building a toxicity detection system will likely start with U.S.-centric English statements to annotate for toxicity. This can cause the tool to work poorly for other populations (e.g., not detect offensive terms</p>
<p>like “presstitute” in Indian contexts; see Figure 1).</p>
<p>Such design biases in the creation of datasets and models, i.e., disparities in how well datasets and models work for different populations, stem from factors including latent design choices and the researcher’s positionality. However, they can perpetuate systemic inequalities by imposing one group’s standards onto the rest of the world (Ghosh et al., 2021; Gururangan et al., 2022; Blasi et al., 2022). The challenge is that design biases arise from the myriad of design choices made; in the context of creating datasets and models, only some of these choices may be documented (e.g., through model cards and data sheets; Bender and Friedman, 2018; Mitchell et al., 2019; Gebru et al., 2021). Further, many popular deployed models are hidden behind APIs, and thus design biases can only be characterized indirectly (e.g., by observing model behavior).</p>
<p>We introduce NLPositionality, a framework for characterizing design biases and positionality of NLP datasets and models. For a given dataset and task, we obtain a wide set of new annotations for a data sample, from a diverse pool of volunteers from various countries and of different backgrounds (recruited through LabintheWild; Reinecke and Gajos, 2015). We then quantify design biases by comparing which identities and backgrounds have higher agreement with the original dataset labels or model predictions. NLPositionality offers three advantages over other approaches (e.g., paid crowdsourcing or laboratory studies). First, the demographic diversity of participants on LabintheWild is better than on other crowdsourcing platforms (Reinecke and Gajos, 2015) and in traditional laboratory studies. Second, the compensation and incentives in our approach rely on a participant’s motivation to learn about themselves instead of monetary compensation. This has been shown to result in higher data quality compared to using paid crowdsourcing platforms (August and Reinecke, 2019), as well as in opportunities for participant learning (Oliveira et al., 2017). This allows our framework to <em>continuously collect</em> new annotations and reflect more up-to-date measurements of design biases for free over long periods of time, compared to one-time paid studies such as in previous works (Sap et al., 2022; Davani et al., 2022). Finally, our approach is dataset- and model-agnostic and can be applied</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>post-hoc to any dataset or model using only instances and their labels or predictions.</p>
<p>We apply NLPositionality to two case studies of NLP tasks—social acceptability and hate speech detection—which are known to exhibit design biases (Talat et al., 2022; Sap et al., 2022; Ghosh et al., 2021). We examine datasets and supervised models related to these tasks as well as general-purpose large language models (i.e., GPT-4). As of May 25 2023, a total of 16,299 annotations were collected from 1,096 annotators from 87 countries, with an average of 38 annotations per day. We discover that the datasets and models we investigate are most aligned with White and educated young people from English-speaking countries, which are a subset of “WEIRD” (Western, Educated, Industrialized, Rich, Democratic; Henrich et al., 2010) populations. We also see that datasets exhibit close alignment with their original annotators, emphasizing the importance of gathering data and annotations from diverse groups.</p>
<p>Our paper highlights the importance of considering design biases in NLP. Our findings showcase the usefulness of our framework in quantifying dataset and model positionality. In a discussion of the implications of our results, we consider how positionality may manifest in other NLP tasks.</p>
<h2>2 Dataset &amp; Model Positionality: Definitions and Background</h2>
<p>A person’s positionality is the perspectives they hold as a result of their demographics, identity, and life experiences (Holmes, 2020; Savin-Baden and Howell-Major, 2013). For researchers, positionality “reflects the position that [they have] chosen to adopt within a given research study” (Savin-Baden and Howell-Major, 2013). It influences the research process and its outcomes and results (Rowe, 2014). Some aspects of positionality, such as gender, race, skin color, and nationality, are culturally ascribed and part of one’s identity. Others, such as political views and life history, are more subjective (Holmes, 2020; Foote and Gau Bartell, 2011).</p>
<h3>Dataset and Model Positionality</h3>
<p>While positionality is often attributed to a person, in this work, we focus on <em>dataset and model positionality</em>. Cambo and Gergle (2022) introduced model positionality, defining it as “<em>the social and cultural position of a model with regard to the stakeholders with which it interfaces.</em>” We extend this definition to add that datasets also encode positionality, in a</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example Annotation. An example instance from the Social Chemistry dataset that was sent to LabintheWild along with the mean of the received annotation scores across various demographics.</p>
<p>Similar way as models. This results in perspectives embedded within language technologies, making them less inclusive towards certain populations.</p>
<h4>Design Biases</h4>
<p>In NLP, design biases occur when a researcher or practitioner makes design choices—often based on their positionality—that cause models and datasets to systematically work better for some populations over others. Curating datasets involves design choices such as what source to use, what language to use, what perspectives to include or exclude, or who to get annotations from. For example, a researcher's native language may influence them to create datasets in that language due to their familiarity with the domain (as in the example in Figure 1). When training models, these choices include the type of training data, data pre-processing techniques, or the objective function (Hall et al., 2022). For example, a researcher's institutional affiliation may influence the training datasets they select (e.g., choosing a dataset made by a coworker). Since the latent choices that result in design biases are fundamental to research itself, some researchers have argued that it is impossible to completely de-bias datasets and models (Waseem et al., 2021).</p>
<p>Current discussions around bias in NLP often focus on ones that originate from social biases embedded within the data. In comparison, design biases originate from the developer who makes assumptions. Based on Friedman and Nissenbaum (1996)'s framework on bias, social biases are preexisting biases in society, whereas design biases are emergent biases that originate from the computing system itself. 'Gender bias' in computing systems means that the system does not perform well for some genders; "man is to doctor as woman is to nurse" (Bolukbasi et al., 2016) is a social bias, while captioning systems that fail to understand women's voices (Tatman, 2017) is a design bias.</p>
<p>One prominent example of design bias in NLP is the overt emphasis on English (Joshi et al., 2020; Blasi et al., 2022). Others include the use of block lists in dataset creation or toxicity classifiers as a filter, which can marginalize minority voices (Dodge et al., 2021; Xu et al., 2021). In this work, we extend the discussion of design biases from prior work into NLP, discuss it in relation to researcher positionality, and show its effects on datasets and models.</p>
<h1>3 NLPositionality: Quantifying Dataset and Model Positionality</h1>
<p>Our NLPositionality framework follows a two-step process for characterizing the design biases and positionality of datasets and models. First, a subset of data for a task is re-annotated by annotators from around the world to obtain globally representative data in order to quantify positionality (§3.1). We specifically rely on re-annotation to capture self-reported demographic data of annotators with each label. Then, the positionality of the dataset or model is computed by comparing the responses of the dataset or model with different demographic groups for identical instances (§3.2). While relying on demographics as a proxy for positionality is limited (see discussion in §7), we use demographic information for an initial exploration in uncovering design biases in datasets and models.</p>
<h3>3.1 Collecting Diverse Annotations</h3>
<p>Cost-effectively collecting annotations from a diverse crowd at scale is challenging. Popular crowdsourcing platforms like Amazon Mechanical Turk (MTurk) are not culturally diverse, as a majority of workers are from the United States and India (Difallah et al., 2018; Ipeirotis, 2010). Further, MTurk does not easily support continuous and longitudinal data collection. To address these challenges, we use LabintheWild (Reinecke and Gajos, 2015), which hosts web-based online experiments. Compared to traditional laboratory settings, it has more diverse participants and collects equally high-quality data for free (August and Reinecke, 2019; Oliveira et al.,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Overview of the NLPositionality Framework. Collection (steps 1-4): A subset of datasets' instances are re-annotated via diverse volunteers recruited on LabintheWild. Processing (step 5): We compare the labels collected from LabintheWild with the dataset's original labels and models' predictions. Analysis (step 6): We compute the Pearson's <em>r</em> correlation between the LabintheWild annotations by demographic for the dataset's original labels and the models' predictions. We apply the Bonferroni correction to account for multiple hypothesis testing.</p>
<p>2017); instead of monetary compensation, participants typically partake in LabintheWild experiments because they learn something about themselves. Thus, we motivate people to participate in our IRB-approved study (§8) by enabling them to learn how their responses on a given task (e.g., judging hate speech) compare to a judgment by AI systems as well as by others who are demographically similar to them (see Appendix B.1).</p>
<p>For a given task, we choose a dataset to be annotated. To select instances for re-annotation, we filter the dataset based on relevant information that could indicate subjectivity (such as <em>controversiality</em> label for the social acceptability dataset), and then sample 300 diverse instances by stratified sampling across different dataset metadata, (such as the <em>targeted groups of toxic speech</em> label for the hate speech dataset) (see Appendix A.1). These instances are then hosted as an experiment on LabintheWild to be annotated by a diverse crowd, where participants report their demographics. To ensure consistency in the re-annotated data, the instructions and annotation setups are similar to the original tasks'. Figure 2 is an example from the Social Chemistry dataset and its annotations.</p>
<h3>3.2 Quantifying Positionality</h3>
<p>We use correlation as a quantitative construct for positionality. First, we group the annotations by specific demographics. When datasets contain multiple annotations from the same demographic for the same instance, we take the mean of the labels from annotators of that demographic to obtain an aggregated score (see Table 1). Next, for each demographic, we compute Pearson's <em>r</em> using the demographic's aggregated score for each instance and correlated it to the dataset label or model prediction<sup>2</sup>. We then apply the Bonferroni correction to account for multiple hypotheses testing (Wickens and Keppel, 2004). We rank the correlations to reveal which demographic groups best align with the positionality of datasets and models. Finally, we report the total number of annotators and inter-annotator agreements for each demographic using Krippendorff's <em>α</em> (Krippendorff, 2006).</p>
<h3>4 Case Studies</h3>
<p>We present case studies of applying NLPositionality to two different socially-aligned tasks: social acceptability and hate speech detection.</p>
<p><sup>2</sup>We use models' output probability scores for supervised models and categorical labels for GPT-4.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Instance</th>
<th style="text-align: center;">Demographic</th>
<th style="text-align: center;">Label</th>
<th style="text-align: center;">$\mu$ Score</th>
<th style="text-align: center;">$\sigma^{2}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Telling a woman what to do.</td>
<td style="text-align: center;">Man</td>
<td style="text-align: center;">It’s okay</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.55</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Non-binary</td>
<td style="text-align: center;">It’s okay</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Woman</td>
<td style="text-align: center;">It’s bad</td>
<td style="text-align: center;">1.31</td>
<td style="text-align: center;">0.27</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Questioning your sexual orientation.</td>
<td style="text-align: center;">10-20 years old</td>
<td style="text-align: center;">It’s good</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">20-30 years old</td>
<td style="text-align: center;">It’s okay</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">1.48</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">30-40, 40-50 years old</td>
<td style="text-align: center;">It’s okay</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Going to church to practice your religion.</td>
<td style="text-align: center;">Christian</td>
<td style="text-align: center;">It’s good</td>
<td style="text-align: center;">1.33</td>
<td style="text-align: center;">1.33</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Buddhist</td>
<td style="text-align: center;">It’s okay</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">It’s okay</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Justice for yellow man lol</td>
<td style="text-align: center;">Asian, Black, Latino/Latina</td>
<td style="text-align: center;">Hate speech</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">White</td>
<td style="text-align: center;">Not sure</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">1.33</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iran’s Supreme Court claims gender equality is 'Zionist plot' aiming to corrupt role of women in society. I fucking second that!</td>
<td style="text-align: center;">Christian</td>
<td style="text-align: center;">Not sure</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Jewish</td>
<td style="text-align: center;">Hate speech</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Muslim</td>
<td style="text-align: center;">Not hate speech</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Literally anyone can be a shitty person, LGBT people especially!</td>
<td style="text-align: center;">College, Graduate school</td>
<td style="text-align: center;">Hate speech</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pre-high school</td>
<td style="text-align: center;">Not hate speech</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
</tr>
</tbody>
</table>
<p>Table 1: Disagreement Examples. Examples where there was disagreement within and between demographics on what the label should be from LabintheWild participants. Disagreement is shown for each label through the variance $\left(\sigma^{2}\right)$ and the mean $(\mu)$ of obtained annotations for a particular demographic.</p>
<h3>4.1 Social Acceptability</h3>
<p>Social acceptability is the task of evaluating how acceptable certain actions are in society. Related to making moral judgments, it requires models to have commonsense rules about social norms and acceptable behavior (Forbes et al., 2020). Social acceptability is likely to vary by background, as morality (Atari et al., 2022), values (Haerpfer and Kizilova, 2012), and moral decision-making (Awad et al., 2018, 2020) vary by culture. In NLP, social acceptability models’ behavior have been shown to systematically display certain values (Arora et al., 2023) and biases (Hämmerl et al., 2022), some of which are Western-centric (Jiang et al., 2021; Talat et al., 2022).</p>
<p>Setup 612 participants annotated the social acceptability of 300 examples from the Social Chemistry dataset (Forbes et al., 2020). We collected 11, 294 annotations for this task and compared participants’ responses to the original Social Chemistry dataset, the Delphi model (Jiang et al., 2021), and GPT-4 zero-shot (OpenAI, 2023). In order to evaluate GPT-4, we combine Delphi’s GPT-3 zeroshot evaluation setup from Jiang et al. (2021) with the template used by Rae et al. (2021):</p>
<div class="codehilite"><pre><span></span><code>Text: &lt;Social Chemistry action&gt;
Question: How socially acceptable is the above
    situation: good, okay, or bad?
Answer:
</code></pre></div>

<p>Results We find that the instances we selected for the social acceptability task can vary by the annotator’s demographics (see Table 1). For example, men and non-binary people are more likely than women to say it’s okay to tell a woman what to do.</p>
<p>We also report the Pearson’s $r$ results in Table 2, with corresponding $p$-values after applying the Bonferroni correction in the Appendix C.1. We use * to represent statistically significant Pearson’s $r$ coefficients ( $p&lt;2.04 e-05$ ). Social Chemistry is most aligned with people who grow up $\left(r=0.76^{<em>}\right)$ and live in $\left(r=0.76^{</em>}\right)$ English-speaking countries, who have a college education $\left(r=0.74^{<em>}\right)$, are White $\left(r=0.73^{</em>}\right)$, and are 20-30 years old $\left(r=0.74^{*}\right)$, indicating a preference to younger WEIRD populations.</p>
<p>Delphi also exhibits a similar pattern, but to a lesser degree. While it strongly aligns with people who grow up $\left(r=0.61^{<em>}\right)$ and live in $\left(r=0.65^{</em>}\right)$ English-speaking countries, who have a college education $\left(r=0.66^{<em>}\right)$, are White $\left(r=0.61^{</em>}\right)$, and are 20-30 years old $\left(r=0.66^{8}\right)$; it also correlates more with other populations, such as people who grow up $\left(r=0.72^{*}\right)$ in Baltic countries compared to English-speaking countries.</p>
<p>We also observe a similar pattern with GPT-4. It has the highest Pearson’s $r$ value for people who grow up $\left(r=0.74^{<em>}\right)$ and live in $\left(r=0.73^{</em>}\right)$ English-speaking countries, are college-educated $\left(r=0.69^{<em>}\right)$, are White $\left(r=0.70^{</em>}\right)$ and are between 20-30 years old $\left(r=0.70^{*}\right)$. However, it</p>
<table>
<thead>
<tr>
<th style="text-align: center;">DATASETS: $\sim$ SocialChemistry</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DynaMate</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MODELS: OPT-4</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Delphi</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PerspectiveAPI</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RewireAPI</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ToxiGen RoBERTa</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Demographic</td>
<td style="text-align: center;">Pearson's $r$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Social Acceptability</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Toxicity \&amp; Hate Speech</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$#$</td>
<td style="text-align: center;">$\alpha$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\alpha$</td>
<td style="text-align: center;">$\alpha$</td>
<td style="text-align: center;">$#$</td>
<td style="text-align: center;">$\alpha$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Country (Lived Longest)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">African Islamic</td>
<td style="text-align: center;">316</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.54*</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">234</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Baltic</td>
<td style="text-align: center;">140</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.73*</td>
<td style="text-align: center;">0.72*</td>
<td style="text-align: center;">0.71*</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">$-0.08$</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Catholic Europe</td>
<td style="text-align: center;">452</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.64*</td>
<td style="text-align: center;">0.59*</td>
<td style="text-align: center;">0.68*</td>
<td style="text-align: center;">183</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Confucian</td>
<td style="text-align: center;">528</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.75*</td>
<td style="text-align: center;">0.58*</td>
<td style="text-align: center;">0.74*</td>
<td style="text-align: center;">154</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.51*</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.52*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">English-Speaking</td>
<td style="text-align: center;">8289</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.76*</td>
<td style="text-align: center;">0.61*</td>
<td style="text-align: center;">0.74*</td>
<td style="text-align: center;">4025</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.70*</td>
<td style="text-align: center;">0.33*</td>
<td style="text-align: center;">0.58*</td>
<td style="text-align: center;">0.37*</td>
<td style="text-align: center;">0.41*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Latin American</td>
<td style="text-align: center;">281</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Orthodox Europe</td>
<td style="text-align: center;">426</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.56*</td>
<td style="text-align: center;">0.58*</td>
<td style="text-align: center;">0.67*</td>
<td style="text-align: center;">139</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Protestant Europe</td>
<td style="text-align: center;">706</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.65*</td>
<td style="text-align: center;">0.57*</td>
<td style="text-align: center;">0.67*</td>
<td style="text-align: center;">387</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.40*</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">West South Asia</td>
<td style="text-align: center;">413</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.63*</td>
<td style="text-align: center;">0.60*</td>
<td style="text-align: center;">0.59*</td>
<td style="text-align: center;">116</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Education Level</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">College</td>
<td style="text-align: center;">4489</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.74*</td>
<td style="text-align: center;">0.66*</td>
<td style="text-align: center;">0.69*</td>
<td style="text-align: center;">2383</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.66*</td>
<td style="text-align: center;">0.34*</td>
<td style="text-align: center;">0.56*</td>
<td style="text-align: center;">0.38*</td>
<td style="text-align: center;">0.39*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Graduate School</td>
<td style="text-align: center;">1116</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.72*</td>
<td style="text-align: center;">0.54*</td>
<td style="text-align: center;">0.69*</td>
<td style="text-align: center;">604</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.59*</td>
<td style="text-align: center;">0.28*</td>
<td style="text-align: center;">0.51*</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.38*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">High School</td>
<td style="text-align: center;">2163</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.67*</td>
<td style="text-align: center;">0.54*</td>
<td style="text-align: center;">0.64*</td>
<td style="text-align: center;">908</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.60*</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.49*</td>
<td style="text-align: center;">0.30*</td>
<td style="text-align: center;">0.37*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PhD</td>
<td style="text-align: center;">709</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.65*</td>
<td style="text-align: center;">0.55*</td>
<td style="text-align: center;">0.61*</td>
<td style="text-align: center;">359</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.48*</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.43*</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Pre-High School</td>
<td style="text-align: center;">406</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.56*</td>
<td style="text-align: center;">0.46*</td>
<td style="text-align: center;">0.59*</td>
<td style="text-align: center;">116</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.45*</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Professional School</td>
<td style="text-align: center;">460</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.53*</td>
<td style="text-align: center;">0.46*</td>
<td style="text-align: center;">0.49*</td>
<td style="text-align: center;">195</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.61*</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Ethnicity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Asian, Asian American</td>
<td style="text-align: center;">1160</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.66*</td>
<td style="text-align: center;">0.55*</td>
<td style="text-align: center;">0.63*</td>
<td style="text-align: center;">644</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.57*</td>
<td style="text-align: center;">0.35*</td>
<td style="text-align: center;">0.47*</td>
<td style="text-align: center;">0.33*</td>
<td style="text-align: center;">0.39*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Black, African American</td>
<td style="text-align: center;">465</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.61*</td>
<td style="text-align: center;">0.59*</td>
<td style="text-align: center;">0.57*</td>
<td style="text-align: center;">287</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.56*</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.36*</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.37*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Latino / Latina, Hispanic</td>
<td style="text-align: center;">314</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.62*</td>
<td style="text-align: center;">0.52*</td>
<td style="text-align: center;">0.54*</td>
<td style="text-align: center;">239</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.43*</td>
<td style="text-align: center;">0.39*</td>
<td style="text-align: center;">0.46*</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Native American, Alaskan Native</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.59*</td>
<td style="text-align: center;">0.52*</td>
<td style="text-align: center;">0.64*</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Pacific Islander, Native Australian</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.65*</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">White</td>
<td style="text-align: center;">3102</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.73*</td>
<td style="text-align: center;">0.61*</td>
<td style="text-align: center;">0.70*</td>
<td style="text-align: center;">1831</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.69*</td>
<td style="text-align: center;">0.29*</td>
<td style="text-align: center;">0.56*</td>
<td style="text-align: center;">0.32*</td>
<td style="text-align: center;">0.38*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Gender</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Man</td>
<td style="text-align: center;">4082</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.73*</td>
<td style="text-align: center;">0.63*</td>
<td style="text-align: center;">0.69*</td>
<td style="text-align: center;">1798</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.65*</td>
<td style="text-align: center;">0.34*</td>
<td style="text-align: center;">0.56*</td>
<td style="text-align: center;">0.34*</td>
<td style="text-align: center;">0.36*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Non-Binary</td>
<td style="text-align: center;">858</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.60*</td>
<td style="text-align: center;">0.51*</td>
<td style="text-align: center;">0.55*</td>
<td style="text-align: center;">329</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.57*</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.37*</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.31*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Woman</td>
<td style="text-align: center;">4368</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.74*</td>
<td style="text-align: center;">0.60*</td>
<td style="text-align: center;">0.73*</td>
<td style="text-align: center;">2357</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.63*</td>
<td style="text-align: center;">0.34*</td>
<td style="text-align: center;">0.53*</td>
<td style="text-align: center;">0.38*</td>
<td style="text-align: center;">0.37*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Native Language</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">English</td>
<td style="text-align: center;">7338</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.76*</td>
<td style="text-align: center;">0.64*</td>
<td style="text-align: center;">0.71*</td>
<td style="text-align: center;">3622</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.70*</td>
<td style="text-align: center;">0.33*</td>
<td style="text-align: center;">0.60*</td>
<td style="text-align: center;">0.39*</td>
<td style="text-align: center;">0.42*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Not English</td>
<td style="text-align: center;">2157</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.62*</td>
<td style="text-align: center;">0.54*</td>
<td style="text-align: center;">0.64*</td>
<td style="text-align: center;">1020</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.46*</td>
<td style="text-align: center;">0.32*</td>
<td style="text-align: center;">0.39*</td>
<td style="text-align: center;">0.32*</td>
<td style="text-align: center;">0.36*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Age</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">10-20 yrs old</td>
<td style="text-align: center;">3360</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.70*</td>
<td style="text-align: center;">0.61*</td>
<td style="text-align: center;">0.69*</td>
<td style="text-align: center;">1615</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.61*</td>
<td style="text-align: center;">0.32*</td>
<td style="text-align: center;">0.55*</td>
<td style="text-align: center;">0.36*</td>
<td style="text-align: center;">0.36*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">20-30 yrs old</td>
<td style="text-align: center;">4066</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.74*</td>
<td style="text-align: center;">0.66*</td>
<td style="text-align: center;">0.70*</td>
<td style="text-align: center;">2114</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.65*</td>
<td style="text-align: center;">0.34*</td>
<td style="text-align: center;">0.56*</td>
<td style="text-align: center;">0.38*</td>
<td style="text-align: center;">0.42*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">30-40 yrs old</td>
<td style="text-align: center;">870</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.66*</td>
<td style="text-align: center;">0.52*</td>
<td style="text-align: center;">0.61*</td>
<td style="text-align: center;">419</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.48*</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.41*</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">40-50 yrs old</td>
<td style="text-align: center;">655</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.62*</td>
<td style="text-align: center;">0.55*</td>
<td style="text-align: center;">0.63*</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.63*</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.57*</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.37*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">50-60 yrs old</td>
<td style="text-align: center;">308</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.69*</td>
<td style="text-align: center;">0.53*</td>
<td style="text-align: center;">0.60*</td>
<td style="text-align: center;">199</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.57*</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.41*</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">60-70 yrs old</td>
<td style="text-align: center;">204</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.64*</td>
<td style="text-align: center;">0.49*</td>
<td style="text-align: center;">0.60*</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">$-0.18$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">70-80 yrs old</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.56*</td>
<td style="text-align: center;">0.52*</td>
<td style="text-align: center;">0.56*</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.85*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">80+ yrs old</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">$-0.09$</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Country (Residence)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">African Islamic</td>
<td style="text-align: center;">164</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">116</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Baltic</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Catholic Europe</td>
<td style="text-align: center;">406</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.53*</td>
<td style="text-align: center;">0.43*</td>
<td style="text-align: center;">0.64*</td>
<td style="text-align: center;">172</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Confucian</td>
<td style="text-align: center;">268</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.68*</td>
<td style="text-align: center;">0.55*</td>
<td style="text-align: center;">0.77*</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">English-Speaking</td>
<td style="text-align: center;">7315</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.76*</td>
<td style="text-align: center;">0.65*</td>
<td style="text-align: center;">0.73*</td>
<td style="text-align: center;">3819</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.72*</td>
<td style="text-align: center;">0.34*</td>
<td style="text-align: center;">0.60*</td>
<td style="text-align: center;">0.38*</td>
<td style="text-align: center;">0.42*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Latin American</td>
<td style="text-align: center;">166</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.54*</td>
<td style="text-align: center;">0.56*</td>
<td style="text-align: center;">0.59*</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">$-0.04$</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Orthodox Europe</td>
<td style="text-align: center;">264</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.57*</td>
<td style="text-align: center;">0.60*</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Protestant Europe</td>
<td style="text-align: center;">736</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.63*</td>
<td style="text-align: center;">0.57*</td>
<td style="text-align: center;">0.61*</td>
<td style="text-align: center;">387</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.45*</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">West South Asia</td>
<td style="text-align: center;">166</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.61*</td>
<td style="text-align: center;">0.57*</td>
<td style="text-align: center;">0.53*</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Religion</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Buddhist</td>
<td style="text-align: center;">189</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.64*</td>
<td style="text-align: center;">0.58*</td>
<td style="text-align: center;">0.55*</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Christian</td>
<td style="text-align: center;">1969</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.73*</td>
<td style="text-align: center;">0.55*</td>
<td style="text-align: center;">0.73*</td>
<td style="text-align: center;">1080</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.56*</td>
<td style="text-align: center;">0.34*</td>
<td style="text-align: center;">0.49*</td>
<td style="text-align: center;">0.36*</td>
<td style="text-align: center;">0.34*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Hindu</td>
<td style="text-align: center;">201</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.65*</td>
<td style="text-align: center;">0.60*</td>
<td style="text-align: center;">0.58*</td>
<td style="text-align: center;">109</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.63*</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Jewish</td>
<td style="text-align: center;">204</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.66*</td>
<td style="text-align: center;">0.60*</td>
<td style="text-align: center;">0.60*</td>
<td style="text-align: center;">144</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.64*</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.43*</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Muslim</td>
<td style="text-align: center;">319</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.63*</td>
<td style="text-align: center;">0.59*</td>
<td style="text-align: center;">0.72*</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Spiritual</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.61*</td>
<td style="text-align: center;">0.60*</td>
<td style="text-align: center;">0.72*</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">$-0.16$</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Positionality of NLP datasets and models quantified using Pearson's $r$ correlation coefficients. $#$ denotes the number of annotations associated with a demographic group. $\alpha$ denotes Krippendorff's alpha of a demographic group for a task. * denotes statistical significance ( $p&lt;2.04 e-05$ after Bonferroni correction). For each dataset or model, we denote the minimum and maximum Pearson's $r$ value for in demographic category in red $(\mathrm{X})$ and blue $(\mathrm{X})$ respectively.</p>
<p>does have strong correlations with other populations, such as people who grow up in Confucian countries $\left(r=0.74^{<em>}\right)$ and have a graduate degree $\left(r=0.69^{</em>}\right)$.</p>
<p>Native English speakers strongly correlate with both datasets and models. Non-binary people align less to both Social Chemistry, Delphi, and GPT-4 compared to men and women. People with professional school education and Black, Latinx, and Native American populations consistently rank least in correlation for education level and ethnicity.</p>
<h3>4.2 Hate Speech Detection</h3>
<p>Hate speech detection is the task of determining whether an instance is "abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation" (Warner and Hirschberg, 2012). Rating whether text is hate speech is likely to vary by background. Also, annotators rate the toxicity of tweets with African American English (AAE) differently by background, such as political affiliation (Sap et al., 2022). Hate speech classifiers also produce errors for instances outside Western contexts (Ghosh et al., 2021).</p>
<p>Setup For the hate speech detection task, 484 LabintheWild participants annotated 300 examples from the Dynahate dataset (Vidgen et al., 2021) and rated whether an instance was hate speech or not. We collected 5, 005 annotations and compared participants' responses to the original Dynahate dataset as well as Perspective API $^{3}$, Rewire API ${ }^{4}$, ToxiGen RoBERTa (Hartvigsen et al., 2022), and GPT-4 zero-shot. To evaluate GPT-4, we use the prompt used by Rae et al. (2021):</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Text:</th>
<th style="text-align: left;"><Dynahate instance></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Question:</td>
<td style="text-align: left;">Does the above text contain rude,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">hateful, aggressive, disrespectful,</td>
</tr>
<tr>
<td style="text-align: left;">or unreasonable language?</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Answer:</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Results We find that the instances we selected for this task can vary by the annotator's demographics (see Table 1). For instance, annotators who were racial minorities rated the instance "Justice for the yellow man lol" as hate speech, while White annotators voted being not sure.</p>
<p>We also report the results in Table 2. We report the $p$-values after applying the Bonferroni correction in the Appendix C.1. Dynahate is highly correlated with people who grow up in English-speaking</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>countries $\left(r=0.70^{<em>}\right)$, who have a college education $\left(r=0.66^{</em>}\right)$, are White $\left(r=0.69^{<em>}\right)$, and are 20-30 years old $\left(r=0.65^{</em>}\right)$. However, it also has high alignment with other populations, such as people who live in West South Asia $(r=0.77)$.</p>
<p>Perspective API also tends to align with WEIRD populations, though to a lesser degree than Dynahate. Perspective API exhibits some alignment with people who grow up and live in English-speaking $\left(r=0.33^{<em>}, r=0.34^{</em>}\right.$ respectively), have a college education $\left(r=0.34^{<em>}\right)$, are White $\left(r=0.29^{</em>}\right)$, and are 20-30 years old $\left(r=0.34^{*}\right)$. It also exhibits higher alignment with other populations, such as people who live in Confucian countries $(r=0.36)$ compared to English-speaking countries. Unexpectedly, White people rank lowest in Pearson's $r$ score within the ethnicity category.</p>
<p>Rewire API similarly shows this bias. It has a moderate correlation with people who grow up and live in English-speaking countries ( $r=0.58^{<em>}$, $r=0.60^{</em>}$ respectively), have a college education $\left(r=0.56^{<em>}\right)$, are White $\left(r=0.56^{</em>}\right)$, and are 20-30 years old $\left(r=0.56^{*}\right)$.</p>
<p>A Western bias is also shown in ToxiGen RoBERTa. ToxiGen RoBERTa shows some alignment with people who grow up $\left(r=0.37^{<em>}\right)$ and live in $\left(r=0.38^{</em>}\right)$ English-speaking countries, have a college education $\left(r=0.38^{<em>}\right)$, are White $\left(r=0.32^{</em>}\right)$, and are between 20-30 years of age $\left(r=0.38^{*}\right)$.</p>
<p>We also observe similar behavior with GPT4. The demographics with some of the higher Pearson's $r$ values in its category are people who grow up $\left(r=0.41^{<em>}\right)$ and live in $\left(r=0.42^{</em>}\right)$ English-speaking countries, are college-educated $\left(r=0.39^{<em>}\right)$, are White $\left(r=0.38^{</em>}\right)$, and are 20-30 years old $\left(r=0.42^{<em>}\right)$. It shows stronger alignment to Asian-Americans $\left(r=0.39^{</em>}\right)$ compared to White people, as well as people who live in Baltic countries $(r=0.75)$ and people who grow up in Confucian countries $\left(r=0.52^{*}\right)$ compared to people from English-speaking countries.</p>
<p>As in the previous task, labels from native English speakers are strongly correlated with datasets and models. Non-binary people align less with Dynahate, Perspective API, Rewire, ToxiGen RoBERTa, and GPT-4 compared to other genders. Also, people who are professional school-educated or are Black, Latinx, and Native American rank least in alignment for education and ethnicity respectively.</p>
<p>5 Discussion</p>
<p>In this paper, we characterized design biases and the positionality of datasets and models in NLP. We introduced the NLPositionality framework for identifying design biases in NLP datasets and models. NLPositionality consists of a two-step process of collecting annotations from diverse annotators for a specific task and then computing the alignment of the annotations to dataset labels and model predictions using Pearson's $r$. We applied NLPositionality to two tasks: social acceptability and hate speech detection, with two datasets and five models in total. In this section, we discuss key takeaways from our experiments and offer recommendations to account for design biases in datasets and models.</p>
<p>There Is Positionality in NLP Models and datasets have positionality, as they align better with some populations than others. This corroborates work from Cambo and Gergle (2022) on model positionality, which quantifies positionality by inspecting the content of annotated documents, as well as work from Rogers (2021), who argues that collecting a corpus of speech inherently encodes a particular world view (e.g., via linguistic structures, topic of conversations, and the speaker's social context). We extend these works by showing design biases and quantifying dataset and model positionality by computing correlations between LabintheWild annotations, dataset labels, and model predictions.</p>
<p>Our case studies show examples of positionality in NLP. However, most socially-aligned tasks may encode design biases due to differences in language use between demographic groups, for example, commonsense reasoning (Shwartz, 2022), question answering (Gor et al., 2021), and sentiment analysis (Mohamed et al., 2022). Even tasks that are considered purely linguistic have seen design biases: in parsing and tagging, performance differences exist between texts written by people of different genders (Garimella et al., 2019), ages (Hovy and Søgaard, 2015), and races (Johannsen et al., 2015; Jørgensen et al., 2015). This shows how common design biases are in NLP, as language is a social construct (Burr, 2015) and technologies are imbued with their creator's values (Friedman, 1996). This raises the question of whether there are any valueneutral language technologies (Birhane et al., 2022; Winner, 2017).</p>
<p>Datasets and Models Skew Western Across all tasks, models, and datasets, we find statistically significant moderate correlations with Western, educated, White, and young populations, indicating that language technologies are WEIRD to an extent, though each to varying degrees. Prior work identifies Western-centric biases in NLP research (Hershcovich et al., 2022), as a majority of research is conducted in the West (ACL, 2017; Caines, 2021). Joshi et al. (2020); Blasi et al. (2022) find disproportionate amounts of resources dedicated to English in NLP research, while Ghosh et al. (2021) identify cross-geographic errors made by toxicity models in non-Western contexts. This could lead to serious downstream implications such as language extinction (Kornai, 2013). Not addressing these biases risks imposing Western standards on non-Western populations, potentially resulting in a new kind of colonialism in the digital age (Irani et al., 2010).</p>
<p>Some Populations Are Left Behind Certain demographics consistently rank lowest in their alignment with datasets and models across both tasks compared to other demographics of the same type. Prior work has also reported various biases against these populations in datasets and models: people who are non-binary (e.g., Dev et al., 2021), Black (e.g., Sap et al., 2019; Davidson et al., 2019), Latinx (e.g., Dodge et al., 2021), Native American (e.g., Mager et al., 2018); and people who are not native English speakers (e.g., Joshi et al., 2020). These communities are historically marginalized by technological systems (Bender et al., 2021).</p>
<h2>Datasets Tend to Align with Their Annotators</h2>
<p>We observe that the positionality we compute is similar to the reported annotator demographics of the datasets, indicating that annotator background contributes to dataset positionality. Social Chemistry reports their annotators largely being women, White, between 30-39 years old, having a college education, and from the U.S. (Forbes et al., 2020), all of which have high correlation to the dataset. Similarly, Dynahate exhibits high correlation with their annotator populations, which are mostly women, White, 18-29 years old, native English speakers, and British (Vidgen et al., 2021). This could be because annotators' positionalities cause them to make implicit assumptions about the context of subjective annotation tasks, which affects its labels (Wan et al., 2023; Birhane et al., 2022). In toxicity modeling, men and women value</p>
<p>speaking freely versus feeling safe online differently (Duggan et al., 2014).</p>
<p>Recommendations Based on these findings, we discuss some recommendations. Following prior work on documenting the choices made in building datasets (Gebru et al., 2021) and models (Bender and Friedman, 2018; Bender et al., 2021), researchers should keep a record of all design choices made while building them. This can improve reproducibility (NAACL, 2021; AAAI, 2023) and aid others in understanding the rationale behind the decisions, revealing some of the researcher's positionality. Similar to the "Bender Rule" (Bender, 2019), which suggests stating the language used, researchers should report their positionality and the assumptions they make (potentially after paper acceptance to preserve anonymity).</p>
<p>We echo prior work in recommending methods to center the perspectives of communities who are harmed by design biases (Blodgett et al., 2020; Hanna et al., 2020; Bender et al., 2021). This can be done using approaches such as participatory design (Spinuzzi, 2005), including interactive storyboarding (Madsen and Aiken, 1993), as well as value-sensitive design (Friedman, 1996), including panels of experiential experts (Madsen and Aiken, 1993). Building datasets and models with large global teams such as BigBench (Srivastava et al., 2022) and NL-Augmenter (Dhole et al., 2021) could also reduce design biases by having diverse teams (Li, 2020).</p>
<p>To account for annotator subjectivity (Aroyo and Welty, 2015), researchers should make concerted efforts to recruit annotators from diverse backgrounds. Websites like LabintheWild can be platforms where these annotators are recruited. Since new design biases could be introduced in this process, we recommend following the practice of documenting the demographics of annotators as in prior works (e.g., Forbes et al., 2020; Vidgen et al., 2021) to record a dataset's positionality.</p>
<p>We urge considering research through the lens of perspectivism (Basile et al., 2021), i.e. being mindful of different perspectives by sharing datasets with disaggregated annotations and finding modeling techniques that can handle inherent disagreements or distributions (Plank, 2022), instead of forcing a single answer in the data (e.g., by majority vote; Davani et al., 2022) or model (e.g., by classification to one label; Costanza-Chock, 2018). Researchers also should carefully consider how they aggregate labels from diverse annotators during modeling so their perspectives are represented, such as not averaging annotations to avoid the "tyranny of the mean" (Talat et al., 2022).</p>
<p>Finally, we argue that the notion of "inclusive NLP" does not mean that all language technologies have to work for everyone. Specialized datasets and models are immensely valuable when the data collection process and other design choices are intentional and made to uplift minority voices or historically underrepresented cultures and languages, such as Masakhane-NER (Adelani et al., 2021) and AfroLM (Dossou et al., 2022). There have also been efforts to localize the design of technologies, including applications that adapt their design and functionality to the needs of different cultures (e.g., Oyibo, 2016; Reinecke and Bernstein, 2011, 2013). Similarly, language models could be made in more culturally adaptive ways, because one size does not fit all (Groenwold et al., 2020; Rettberg, 2022). Therefore, we urge the NLP community to value the adaptation of language technologies from one language or culture to another (Joshi et al., 2020).</p>
<h2>6 Conclusion</h2>
<p>We introduce NLPositionality, a framework to quantify design biases and positionality of datasets and models. In this work, we present how researcher positionality leads to design biases and subsequently gives positionality to datasets and models, potentially resulting in these artifacts not working equally for all populations. Our framework involves recruiting a demographically diverse pool of crowdworkers from around the world on LabintheWild, who then re-annotate a sample of a dataset for an NLP task. We apply NLPositionality to two tasks, social acceptability and hate speech detection, to show that models and datasets have a positionality and design biases by aligning better with Western, White, college-educated, and younger populations. Our results indicate the need for more inclusive models and datasets, paving the way for NLP research that benefits all people.</p>
<h2>7 Limitations</h2>
<p>Our study has several limitations. First, demographics may not be the best construct for positionality, as there may be variability of beliefs within demographic groups. Assuming that there is homogeneity within demographic groups is reductionist and limited. Rather, capturing an individual's attitudes</p>
<p>or beliefs may be a more reliable way to capture one's positionality that future work can investigate.</p>
<p>Study annotators could also purposefully answer untruthfully, producing low-quality annotations. We address this risk by using LabintheWild. LabintheWild has been shown to produce highquality data because participants are intrinsically motivated to participate by learning something about themselves (Reinecke and Gajos, 2015). However, as is the case for all online recruiting methods, our sample of participants is not representative of the world's population due to the necessity of having access to the Internet. In addition, there is likely a selection bias in who decides to participate in a LabintheWild study.</p>
<p>Pearson's $r$ may not fully capture alignment as it does not consider interaction effects between different demographics (i.e., intersectionality). Thus, there may be additional mediating or moderating variables that may explain the results that our analysis does not consider. We also took the average of the annotations per group, which could mask individual variations (Talat et al., 2022). Also, having a low number of participants from specific demographic groups may limit how well the results generalize to the entire group; further, it may risk tokenizing already marginalized communities.</p>
<p>As part of our study, we apply NLPositionality to only two tasks which have relatively straightforward annotation schemes. It may be difficult to generalize to other NLP tasks which have harder annotation schemes, especially ones that require a lot of explanation to the annotators, for example, natural language inference (NLI) tasks.</p>
<p>Our approach is evaluated and works the best for classification tasks and classifiers. Generation tasks would need more careful annotator training which is difficult to achieve on a voluntary platform without adequate incentives. Having annotators use one Likert scale to rate the social acceptability and toxicity of a situation or text may not be a sufficient measure to represent these complex social phenomena. To reduce this threat, we provide detailed instructions that describe how to provide annotations and followed the original annotation setup as closely as possible.</p>
<h2>8 Ethics Statement</h2>
<p>Towards Inclusive NLP Systems Building inclusive NLP systems is important so that everyone can benefit from their usage. Currently, these sys-
tems exhibit many design biases that negatively impact minoritized or underserved communities in NLP (Joshi et al., 2020; Blodgett et al., 2020; Bender et al., 2021). Our work is a step towards reducing these disparities by understanding that models and datasets have positionalities and by identifying design biases. The authors take inspiration from fields outside of NLP by studying positionality (Rowe, 2014) and acknowledge crossdisciplinary research as crucial to building inclusive AI systems.</p>
<p>Ethical Considerations We recognize that the demographics we collected only represent a small portion of a person's positionality. There are many aspects of positionality that we did not collect, such as sexual orientation, socioeconomic status, ability, and size. Further, we acknowledge the limitation of assigning labels to people as being inherently reductionist. As mentioned in $\S 7$, using a single Likert scale for social acceptability and toxicity is not sufficient in capturing the complexities in these phenomena, such as situational context.</p>
<p>We note that quantifying positionality of existing systems is not an endorsement of the system. In addition to making sure that language technologies work for all populations, researchers should also continue to examine whether these systems should exist in the first place (Denton and Gebru, 2020; Keyes et al., 2019). Further, we note that understanding a dataset or model's positionality does not preclude researchers from the responsibilities of adjusting it further.</p>
<p>This study was undertaken following approval from the IRB at the University of Washington (STUDY00014813). LabintheWild annotators were not compensated financially. They were lay people from a wide range of ages (including minors) and diverse backgrounds. Participants were asked for informed consent to the study procedures as well as the associated risks, such as being exposed to toxic or mature content, prior to beginning the study.</p>
<p>Research Team Positionality We discuss aspects of our positionality below that we believe are most relevant to this research. The research team is comprised of computer scientists who study human-computer interaction and NLP and have a bent for using quantitative methods. Thus, we approach the topic from a perspective that assumes that positionality can be characterized, fixed, and</p>
<p>quantified.
The entire research team currently resides in the United States. In alphabetical order, the team members originate from Belgium and Switzerland, France, Germany, India, and the United States; and identify as East Asian, South Asian, and White. These nationalities and ethnicities are overrepresented in the development of NLP technologies. Thus, we acknowledge that our knowledge of how design biases in NLP datasets and models impact people is largely through research, rather than personal experience.</p>
<h2>Acknowledgements</h2>
<p>We thank Yejin Choi and Liwei Jiang for their invaluable inputs in the early stages of the project, especially their ideas in shaping the direction of this work, as well as the ReViz team at the Allen Institute for AI for their technical support for building the LabintheWild experiments. We also thank the members of the University of Washington NLP, HCI, and ML/AI groups for their feedback throughout the project. We give a special thanks to Mei an outstanding canine researcher, for providing support and motivation throughout the study.</p>
<p>Jenny T. Liang was supported by the National Science Foundation under grants DGE1745016 and DGE2140739. This research was partially supported by the National Science Foundation under grant 2230466 .</p>
<h2>References</h2>
<p>AAAI. 2023. Reproducibility checklist.
ACL. 2017. ACL Diversity Statistics.
David Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D’souza, Julia Kreutzer, Constantine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti Rijhwani, Sebastian Ruder, et al. 2021. Masakhaner: Named entity recognition for african languages. Transactions of the Association for Computational Linguistics, 9:1116-1131.</p>
<p>Arnav Arora, Lucie-Aimée Kaffee, and Isabelle Augenstein. 2023. Probing pre-trained language models for cross-cultural differences in values. In Workshop on Cross-Cultural Considerations in NLP, page $114-130$.</p>
<p>Lora Aroyo and Chris Welty. 2015. Truth is a lie: Crowd truth and the seven myths of human annotation. AI Magazine, 36(1):15-24.</p>
<p>Mohammad Atari, Jonathan Haidt, Jesse Graham, Sena Koleva, Sean T Stevens, and Morteza Dehghani.
2022. Morality beyond the WEIRD: How the nomological network of morality varies across cultures.</p>
<p>Tal August and Katharina Reinecke. 2019. Pay attention, please: Formal language improves attention in volunteer and paid online experiments. In ACM SIGCHI Conference on Human Factors in Computing Systems, page 1-11.</p>
<p>Edmond Awad, Sohan Dsouza, Richard Kim, Jonathan Schulz, Joseph Henrich, Azim Shariff, Jean-François Bonnefon, and Iyad Rahwan. 2018. The moral machine experiment. Nature, 563(7729):59-64.</p>
<p>Edmond Awad, Sohan Dsouza, Azim Shariff, Iyad Rahwan, and Jean-François Bonnefon. 2020. Universals and variations in moral decisions made in 42 countries by 70,000 participants. National Academy of Sciences, 117(5):2332-2337.</p>
<p>Valerio Basile, Federico Cabitza, Andrea Campagner, and Michael Fell. 2021. Toward a perspectivist turn in ground truthing for predictive computing. arXiv preprint arXiv:2109.04270.</p>
<p>Emily Bender. 2019. The# benderrule: On naming the languages we study and why it matters. The Gradient, 14 .</p>
<p>Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587-604.</p>
<p>Emily M. Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In ACM Conference on Fairness, Accountability, and Transparency, page 610-623.</p>
<p>Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. 2022. The values encoded in machine learning research. In ACM Conference on Fairness, Accountability, and Transparency, pages 173-184.</p>
<p>Damian Blasi, Antonios Anastasopoulos, and Graham Neubig. 2022. Systematic inequalities in language technology performance across the world's languages. In Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5486-5505.</p>
<p>Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of "bias" in NLP. In Annual Meeting of the Association for Computational Linguistics, pages 5454-5476.</p>
<p>Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings. volume 29.</p>
<p>Vivien Burr. 2015. Social Constructionism. Routledge.</p>
<p>Andrew Caines. 2021. The geographic diversity of NLP conferences.</p>
<p>Scott Allen Cambo and Darren Gergle. 2022. Model positionality and computational reflexivity: Promoting reflexivity in data science. In ACM SIGCHI Conference on Human Factors in Computing Systems, pages $1-19$.</p>
<p>Sasha Costanza-Chock. 2018. Design justice, AI, and escape from the matrix of domination. Journal of Design and Science, 3(5).</p>
<p>Aida Mostafazadeh Davani, Mark Díaz, and Vinodkumar Prabhakaran. 2022. Dealing with disagreements: Looking beyond the majority vote in subjective annotations. Transactions of the Association for Computational Linguistics, 10:92-110.</p>
<p>Thomas Davidson, Debasmita Bhattacharya, and Ingmar Weber. 2019. Racial bias in hate speech and abusive language detection datasets.</p>
<p>Emily Denton and Timnit Gebru. 2020. Tutorial on fairness, accountability, transparency, and ethics in computer vision at CVPR 2020.</p>
<p>Sunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Arjun Subramonian, Jeff Phillips, and Kai-Wei Chang. 2021. Harms of gender exclusivity and challenges in non-binary representation in language technologies. In Conference on Empirical Methods in Natural Language Processing, pages 1968-1994.</p>
<p>Kaustubh D Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh Gupta, Zhenhao Li, Saad Mahamood, Abinaya Mahendiran, Simon Mille, Ashish Srivastava, Samson Tan, et al. 2021. NL-augmenter: A framework for task-sensitive natural language augmentation. arXiv preprint arXiv:2112.02721.</p>
<p>Djellel Difallah, Elena Filatova, and Panos Ipeirotis. 2018. Demographics and dynamics of mechanical turk workers. In ACM International Conference on Web Search and Data Mining, page 135-143.</p>
<p>Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Conference on Empirical Methods in Natural Language Processing, pages $1286-1305$.</p>
<p>Bonaventure F.P. Dossou, Atnafu Lambebo Tonja, Oreen Yousuf, Salomey Osei, Abigail Oppong, Iyanuoluwa Shode, Oluwabusayo Olufunke Awoyomi, and Chris Emezue. 2022. Afrolm: A selfactive learning-based multilingual pretrained language model for 23 African languages. In Workshop on Simple and Efficient Natural Language Processing, pages 52-64.</p>
<p>Maeve Duggan, L Rainie, A Smith, C Funk, A Lenhart, and M Madden. 2014. Online harassment. Pew Research Center, Washington, DC, USA, Technical Rep.</p>
<p>Mary Q Foote and Tonya Gau Bartell. 2011. Pathways to equity in mathematics education: How life experiences impact researcher positionality. Educational Studies in Mathematics, 78(1):45-68.</p>
<p>Maxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. Social chemistry 101: Learning to reason about social and moral norms. In Conference on Empirical Methods in Natural Language Processing, pages 653-670.</p>
<p>Batya Friedman. 1996. Value-sensitive design. Interactions, 3(6):16-23.</p>
<p>Batya Friedman and Helen Nissenbaum. 1996. Bias in computer systems. ACM Transactions on Information Systems, 14(3):330-347.</p>
<p>Aparna Garimella, Carmen Banea, Dirk Hovy, and Rada Mihalcea. 2019. Women's syntactic resilience and men's grammatical luck: Gender-bias in part-ofspeech tagging and dependency parsing. In Annual Meeting of the Association for Computational Linguistics, pages 3493-3498.</p>
<p>Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021. Datasheets for datasets. Communications of the $A C M, 64(12): 86-92$.</p>
<p>Sayan Ghosh, Dylan Baker, David Jurgens, and Vinodkumar Prabhakaran. 2021. Detecting crossgeographic biases in toxicity modeling on social media. In Workshop on Noisy User-generated Text, page $313-328$.</p>
<p>Maharshi Gor, Kellie Webster, and Jordan Boyd-Graber. 2021. Toward deconfounding the effect of entity demographics for question answering accuracy. In Conference on Empirical Methods in Natural Language Processing, pages 5457-5473.</p>
<p>Sophie Groenwold, Lily Ou, Aesha Parekh, Samhita Honnavalli, Sharon Levy, Diba Mirza, and William Yang Wang. 2020. Investigating AfricanAmerican Vernacular English in transformer-based text generation. In Conference on Empirical Methods in Natural Language Processing, pages 5877-5883.</p>
<p>Suchin Gururangan, Dallas Card, Sarah K Drier, Emily K Gade, Leroy Z Wang, Zeyu Wang, Luke Zettlemoyer, and Noah A Smith. 2022. Whose language counts as high quality? measuring language ideologies in text data selection. "Conference on Empirical Methods in Natural Language Processing", pages 2562-2580.</p>
<p>Christian W Haerpfer and Kseniya Kizilova. 2012. The world values survey. The Wiley-Blackwell Encyclopedia of Globalization, pages 1-5.</p>
<p>Melissa Hall, Laurens van der Maaten, Laura Gustafson, and Aaron Adcock. 2022. A systematic study of bias amplification. arXiv preprint arXiv:2201.11706.</p>
<p>Katharina Hämmerl, Björn Deiseroth, Patrick Schramowski, Jindřich Libovický, Constantin A Rothkopf, Alexander Fraser, and Kristian Kersting. 2022. Speaking multiple languages affects the moral bias of language models. arXiv preprint arXiv:2211.07733.</p>
<p>Alex Hanna, Emily Denton, Andrew Smart, and Jamila Smith-Loud. 2020. Towards a critical race methodology in algorithmic fairness. In ACM Conference on Fairness, Accountability, and Transparency, pages $501-512$.</p>
<p>Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022. Toxigen: Controlling language models to generate implied and adversarial toxicity. In Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Joseph Henrich, Steven J Heine, and Ara Norenzayan. 2010. The weirdest people in the world? Behavioral and Brain Sciences, 33(2-3):61-83.</p>
<p>Daniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, and Anders Søgaard. 2022. Challenges and strategies in cross-cultural NLP. In Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6997-7013.</p>
<p>Andrew Gary Darwin Holmes. 2020. Researcher positionality-A consideration of its influence and place in qualitative research-A new researcher guide. Shanlax International Journal of Education, 8(4):110 .</p>
<p>Dirk Hovy and Anders Søgaard. 2015. Tagging performance correlates with author age. In Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 483-488.</p>
<p>Panagiotis G Ipeirotis. 2010. Demographics of Mechanical Turk.</p>
<p>Lilly Irani, Janet Vertesi, Paul Dourish, Kavita Philip, and Rebecca E. Grinter. 2010. Postcolonial computing: A lens on design and development. In ACM SIGCHI Conference on Human Factors in Computing Systems, page 1311-1320.</p>
<p>Liwei Jiang, Jena D Hwang, Chandra Bhagavatula, Ronan Le Bras, Jenny Liang, Jesse Dodge, Keisuke Sakaguchi, Maxwell Forbes, Jon Borchardt, Saadia Gabriel, et al. 2021. Can machines learn morality? the delphi experiment. arXiv e-prints, pages arXiv2110.</p>
<p>Anders Johannsen, Dirk Hovy, and Anders Søgaard. 2015. Cross-lingual syntactic variation over age and gender. In Conference on Computational Natural Language Learning, pages 103-112.</p>
<p>Anna Jørgensen, Dirk Hovy, and Anders Søgaard. 2015. Challenges of studying and processing dialects in social media. In ACL Workshop on Noisy Usergenerated Text, pages 9-18.</p>
<p>Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Annual Meeting of the Association for Computational Linguistics, pages 6282-6293.</p>
<p>Os Keyes, Jevan Hutson, and Meredith Durbin. 2019. A mulching proposal: Analysing and improving an algorithmic system for turning the elderly into highnutrient slurry. In Extended abstracts of the SIGCHI Conference on Human Factors in Computing Systems, pages $1-11$.</p>
<p>András Kornai. 2013. Digital language death. PLOS ONE, 8(10):1-11.</p>
<p>Klaus Krippendorff. 2006. Reliability in content analysis: Some common misconceptions and recommendations. Human Communication Research, 30(3):411433.</p>
<p>Michael Li. 2020. To build less-biased AI, hire a more diverse team. Harvard Business Review.</p>
<p>Kim Halskov Madsen and Peter H. Aiken. 1993. Experiences using cooperative interactive storyboard prototyping. Communications of the ACM, 36(6):57-64.</p>
<p>Manuel Mager, Ximena Gutierrez-Vasques, Gerardo Sierra, and Ivan Meza-Ruiz. 2018. Challenges of language technologies for the indigenous languages of the Americas. In International Conference on Computational Linguistics, pages 55-69.</p>
<p>Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In ACM Conference on Fairness, Accountability, and Transparency, page 220-229.</p>
<p>Youssef Mohamed, Mohamed Abdelfattah, Shyma Alhuwaider, Feifan Li, Xiangliang Zhang, Kenneth Ward Church, and Mohamed Elhoseiny. 2022. Artelingo: A million emotion annotations of WikiArt with emphasis on diversity over language and culture. In Conference on Empirical Methods in Natural Language Processing, pages 8770-8785.</p>
<p>NAACL. 2021. Reproducibility checklist.
Nigini Oliveira, Eunice Jun, and Katharina Reinecke. 2017. Citizen science opportunities in volunteerbased online experiments. In ACM SIGCHI Conference on Human Factors in Computing Systems, page $6800-6812$.</p>
<p>OpenAI. 2023. Gpt-4 technical report. arXiv.</p>
<p>Kiemute Oyibo. 2016. Designing culture-based persuasive technology to promote physical activity among university students. In Proceedings of the 2016 conference on user modeling adaptation and personalization, pages 321-324.</p>
<p>Barbara Plank. 2022. The 'problem' of human label variation: On ground truth in data, modeling and evaluation. In Conference on Empirical Methods in Natural Language Processing, pages 10671-10682.</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis \&amp; insights from training Gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Katharina Reinecke and Abraham Bernstein. 2011. Improving performance, perceived usability, and aesthetics with culturally adaptive user interfaces. ACM Transactions on Computer-Human Interaction, 18(2):1-29.</p>
<p>Katharina Reinecke and Abraham Bernstein. 2013. Knowing what a user likes: A design science approach to interfaces that automatically adapt to culture. Mis Quarterly, pages 427-453.</p>
<p>Katharina Reinecke and Krzysztof Z. Gajos. 2015. LabInTheWild: Conducting large-scale online experiments with uncompensated samples. In ACM Conference on Computer Supported Cooperative Work \&amp; Social Computing, pages 1364-1378.</p>
<p>Jill Walker Rettberg. 2022. ChatGPT is multilingual but monocultural, and it's learning your values. https://jilltxt.net/right-now-chatgpt-is-multilingual-but-monocultural-but-its-learning-your-values/. Accessed: 2023-5-25.</p>
<p>Anna Rogers. 2021. Changing the world by changing the data. In Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2182-2194.</p>
<p>Wendy E Rowe. 2014. Positionality. The SAGE encyclopedia of action research, 628:627-628.</p>
<p>Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A. Smith. 2019. The risk of racial bias in hate speech detection. In Annual Meeting of the Association for Computational Linguistics, pages 16681678 .</p>
<p>Maarten Sap, Swabha Swayamdipta, Laura Vianna, Xuhui Zhou, Yejin Choi, and Noah A. Smith. 2022. Annotators with attitudes: How annotator beliefs and identities bias toxic language detection. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5884-5906.</p>
<p>Maggi Savin-Baden and Claire Howell-Major. 2013. Qualititative research: The essential guide to theory and practice. Qualitative Research: The Essential Guide to Theory and Practice. Routledge.</p>
<p>Vered Shwartz. 2022. Good night at 4 pm?! time expressions in different cultures. In Findings of the Association for Computational Linguistics: ACL, pages 2842-2853.</p>
<p>Clay Spinuzzi. 2005. The methodology of participatory design. Technical Communication, 52(2):163-174.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, and Adina Williams. 2022. On the machine learning of ethical judgments from natural language. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 769-779.</p>
<p>Rachael Tatman. 2017. Gender and dialect bias in YouTube's automatic captions. In ACL Workshop on Ethics in Natural Language Processing, pages 53-59.</p>
<p>Bertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela. 2021. Learning from the worst: Dynamically generated datasets to improve online hate detection. In Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1667-1682.</p>
<p>Ruyuan Wan, Jaehyung Kim, and Dongyeop Kang. 2023. Everyone's voice matters: Quantifying annotation disagreement using demographic information. arXiv preprint arXiv:2301.05036.</p>
<p>William Warner and Julia Hirschberg. 2012. Detecting hate speech on the world wide web. In Workshop on Language in Social Media, pages 19-26.</p>
<p>Zeerak Waseem, Smarika Lulz, Joachim Bingel, and Isabelle Augenstein. 2021. Disembodied machine learning: On the illusion of objectivity in NLP. arXiv preprint arXiv:2101.11974.</p>
<p>Thomas D Wickens and Geoffrey Keppel. 2004. Design and Analysis: A Researcher's Handbook. PrenticeHall.</p>
<p>Langdon Winner. 2017. Do artifacts have politics? In Computer Ethics, pages 177-192. Routledge.</p>
<p>Albert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten Sap, and Dan Klein. 2021. Detoxifying language models risks marginalizing minority</p>
<p>voices. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2390-2397.</p>
<p>In this section, we describe all the decisions that went into sampling data points from the different datasets and its post-processing.</p>
<h3>A.1 Sampling</h3>
<p>For Social Chemistry, we sample instances whose label for anticipated agreement by the general public was "Controversial ( 50\%)". We ensure the samples are equally represented by the moral foundation label, which we compute based on majority vote across annotators. In the study, annotators respond whether they found a presented action socially acceptable.</p>
<p>For Dynahate, we randomly sample instances from rounds 3 and 4. In these rounds, annotators generated examples of implicit hate, which is subtler and harder to detect and could yield differences in annotations. We ensure that there are equal amounts of hateful and not hateful instances and that the types of targets of the hateful instances are equally represented. During the studsy, annotators respond whether they found a presented instance toxic.</p>
<p>For both social acceptability and hate speech detection, annotators responded whether they found the situation moral and whether they found the instance to be hate speech respectively.</p>
<h3>A.2 Post-Processing</h3>
<p>Because Social Chemistry has multiple annotations for each instance, we compute an aggregate score by taking the average score across annotators. This score is then used to correlate to the annotators’ aggregated scores.</p>
<h2>B Study Design</h2>
<p>In this section, we discuss the design of the LabintheWild experiments. The social acceptability task was released to the public in April 2022. The hate speech detection task was released August 2022. To reduce confounding factors on the data collection process, we conduct multiple user studies of the LabintheWild experiments prior to the public release. Additionally, all the annotations collected through the experiments are anonymous and are stored securely.</p>
<p>The social acceptability task is marketed as "Could you live with an AI and its morals?" Participants for this study provide annotations for 25 situations. The hate speech detection task is marketed as "Do you and AI agree on what is hate speech? Let’s find out!" Participants provid annotations for 15 instances.</p>
<h3>B.1 LabintheWild Study Flow</h3>
<p>We describe the format of the LabintheWild experiment. The phases of the experiment are: obtaining consent, collecting demographics, explaining instructions, collecting annotations, collecting study feedback, and displaying results.</p>
<p>Obtaining Consent Prior to beginning the study, participants review a consent form. The consent form includes information on the purpose of the research, what the participant will do, risks and benefits of the research, privacy and data collection methods, and contact information of the researchers. At the end of the form, participants give explicit consent to participate in the study.</p>
<p>Collecting Demographics We then collect the demographics of study participants. LabintheWild participants enter in whether they had taken this test before, the country they lived in the longest, the country of residence, age, native language, religion, education, and ethnicity. No demographics are required except for the country the participant lived in the longest and whether they have taken the test before. Additionally, we only display ethnicity for people within the United States.</p>
<p>Explaining Instructions For each task, we provide instructions to participants on how to perform the annotation task. For social acceptability, we explain social acceptability as rating "what you think about the situation in general from an ethical perspective" (see Figure 5). For hate speech detection, use the definition of hate speech from Dynahate and we provide three examples of hate speech (see Figure 6). We also present examples of negative sentiment, profanity, or discussing groups that could be confused as hate speech, but are not hate speech.</p>
<p>Collecting Annotations After being presented with instructions, participants begin data collection from the 300 instances selected from Section A.1. For each task, we keep the annotation setup identical to the original one. For social acceptability, we collect Likert-scale ratings of situations ranging from "It’s very bad", "It’s bad", "It’s okay", "It’s good", and "It’s very good". Participants can provide rationale for their decision by using an open</p>
<p>text box. The data collection interface is presented in Figure 4. For hate speech detection, we collect ratings of instances ranging from "Hate speech", "Not sure", "Not hate speech". We also provide an optional open-text box for participants to explain their rationale. The data collection interface is presented in Figure 7. After submitting the annotation, the participant is able to see a visualization on how the AI responded as well as how other participants from the same country responded to the instance.</p>
<p>We also specifically sample which instances to present to participants for annotation. We sample a third of the instances that did not have any annotations from the demographic and a third that are already sampled by participants of the demographic. The rest are equally split across the different of types of instances (i.e., moral foundation for Social Chemistry, hate type for Dynahate).</p>
<p>Providing Study Feedback Following typical LabintheWild experiment procedures, we collect feedback from participants about the study. Participants can enter open-text feedback on anything. They also submit whether they encountered technical difficulties during the study or whether they cheated. Participants can elaborate on their answers from the prior questions in an open-text box.</p>
<p>Displaying Overall Results Finally, participants see their overall results for the experiment task. First, participants are presented with the percentage of time they agreed with the AI as well as with participants as the same demographic as them (see Figure 8). Each of these agreement scores are further broken down by the type of the instance (i.e., moral foundation for Social Chemistry and hate type for Dynahate).</p>
<h2>C Additional Results</h2>
<p>In this section, we report additional results from our analyses of the LabintheWild data.</p>
<h2>C. $1 \quad p$-values</h2>
<p>We report the $p$-values from our analyses from Table 3 .</p>
<h2>D Cultural Spheres</h2>
<p>Division of countries can be done through continents. However, continents are often not representative of the countries within it and clustering based on them can lead to inaccurate findings. For example, Asia includes both Japan and Saudi</p>
<p>Arabia, which are different culturally. We instead adopt cultural spheres as used in World Values Survey (Haerpfer and Kizilova, 2012), which clusters the countries in terms of the values they uphold and norms they follow. Table 4 shows the countries and the spheres.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Data collection interface for the social acceptability task. Participants were given a sentence (an action from the Social Chemsitry dataset) and asked to rate how ethical the action was. Participants are shown how other people from their country responded after each attempt.</p>
<h1>Instructions</h1>
<p>You will be shown 25 different scenarios. For each, select what you think about the situation in general from an ethical perspective. You can elaborate on your perspective if you wish. Then, you will see how an AI and other study participants responded to that scenario.</p>
<p>At the end of the study, we will show how your responses compared with an AI's and other participants' overall.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Iat's start!</p>
<p>Figure 5: Instructions for the social acceptability task. Participants were asked to describe their thoughts about a situation from an ethical perspective.</p>
<h1>Instructions</h1>
<p>You will be shown 12 different examples. For each, rate whether you think the speech is hateful towards a group. You can elaborate on your rating if you wish. Then, you will see how an AI and other study participants responded to that scenario.</p>
<p>At the end of the study, we will show how your responses compared with an AI's and other participants' overall.</p>
<p>Important: Please note that some examples may contain content that is offensive, problematic, or harmful. You are able to skip examples if you'd like.</p>
<h2>What is hateful speech?</h2>
<p>Hateful speech is abusive speech targeting specific social groups and their characteristics (e.g., ethnicity, religion, gender, or sexual orientation).</p>
<h2>Probably Hateful</h2>
<ul>
<li>"On 9/11, Arabs brought terror to Americans. So I think it's only fair that Arabs are not welcome in the U.S."
This statement implies that Arab people do not belong.</li>
<li>"Gay teachers should not be allowed. At the very least they should keep their personal life to themselves, away from students."
This statement implies gay people do not have the same right to teach as straight people.</li>
<li>"To have a good life, you should have light skin, if he lightened his skin he could pass for white."
This statement implies that people should lighten their skin to be accepted.</li>
</ul>
<h2>Probably Not Hateful</h2>
<ul>
<li>"The photos came out so blurry, I can't even tell who's in them. This shitty camera is awful at taking pictures."
This statement includes profanity but does not target a group of people.</li>
<li>"Chinese people know how to make good food, my favorite food of all time is dim sum."
This statement references as group of people but is not abusive language.</li>
<li>"I'm always suprised when people say they love Apple. Their products suck, I hate my phone."
This statement includes negative sentiment but does not target a group of people.</li>
</ul>
<p>Figure 6: Instructions for the toxicity task. Participants were provided with examples of hate speech examples and not hate examples.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Data collection interface for the hate speech task. Participants were given a sentence (an instance from the Dynahate dataset) and asked to rate whether the instance was toxic or not. Participants are shown how other people from their country responded after each attempt.</p>
<p>Here's how often your responses matched with the AI's.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>People from the same demographic as you scored...
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Your Age + AI
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Country You Lived in Longest + AI
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Your Country of Residence + AI</p>
<h1>When did you agree with the AI?</h1>
<p>Here's a breakdown of your match score with the AI by the type of moral situation. Higher values indicate higher agreement with the AI.</p>
<h2>Types of moral situations</h2>
<p>Care/harm is morals of having empathy towards the pain of others (e.g., valuing kindness).
Fairness/cheating relates to morals from reciprocated altruism (e.g., valuing justice).
Loyalty/betrayal is morals from building alliances (e.g., valuing patriotism).
Authority/subversion is morals based on social hierarchies (e.g., valuing leadership).
Sanctity/degredation relates to morals of living in an elevated and noble manner.
Everyday refers to everyday situations which have no moral implications.</p>
<p>AGREEMENT OF
You + AI ( $86 \%$ match)
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 8: Results interface for the social acceptability task. Participants can view how well they aligned with the AI, as well as how other demographics they reported aligned with the AI. The AI alignment is further broken down by the type of moral foundation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">DATASETS: 8 SocialChemistry 7 DynaHate 6 MODELS: 6 GPT-4 50 Delphi 4 PerspectiveAPI 4 RewireAPI 7 ToxiGen RoBERTa</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Demographic</td>
<td style="text-align: center;">$p$-value ( $\alpha=2.04 e-05$ )</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Social Acceptability</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Toxicity \&amp; Hate Speech</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$a^{2} b$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">Country (Lived Longest)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">African Islamic</td>
<td style="text-align: center;">1.74e-04</td>
<td style="text-align: center;">2.01e-03</td>
<td style="text-align: center;">4.40e-03</td>
<td style="text-align: center;">4.02e-03</td>
<td style="text-align: center;">2.37e-01</td>
<td style="text-align: center;">3.50e-03</td>
<td style="text-align: center;">3.28e-01</td>
<td style="text-align: center;">6.82e-01</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Baltic</td>
<td style="text-align: center;">2.98e-06</td>
<td style="text-align: center;">7.11e-06</td>
<td style="text-align: center;">1.27e-05</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">4.34e-01</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Catholic Europe</td>
<td style="text-align: center;">1.40e-09</td>
<td style="text-align: center;">1.98e-07</td>
<td style="text-align: center;">3.77e-11</td>
<td style="text-align: center;">2.21e-01</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">2.01e-01</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Confucian</td>
<td style="text-align: center;">5.23e-15</td>
<td style="text-align: center;">3.89e-07</td>
<td style="text-align: center;">1.58e-14</td>
<td style="text-align: center;">3.15e-03</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">4.27e-04</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">3.07e-04</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">English-Speaking</td>
<td style="text-align: center;">6.67e-55</td>
<td style="text-align: center;">4.12e-29</td>
<td style="text-align: center;">2.21e-49</td>
<td style="text-align: center;">3.31e-44</td>
<td style="text-align: center;">3.59e-07</td>
<td style="text-align: center;">8.74e-27</td>
<td style="text-align: center;">3.17e-09</td>
<td style="text-align: center;">5.38e-12</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Latin American</td>
<td style="text-align: center;">2.50e-02</td>
<td style="text-align: center;">9.08e-02</td>
<td style="text-align: center;">1.52e-02</td>
<td style="text-align: center;">7.87e-01</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Orthodox Europe</td>
<td style="text-align: center;">1.02e-06</td>
<td style="text-align: center;">2.42e-07</td>
<td style="text-align: center;">1.38e-10</td>
<td style="text-align: center;">1.37e-01</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">3.34e-03</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Protestant Europe</td>
<td style="text-align: center;">1.17e-14</td>
<td style="text-align: center;">2.18e-10</td>
<td style="text-align: center;">6.14e-16</td>
<td style="text-align: center;">1.15e-04</td>
<td style="text-align: center;">1.46e-02</td>
<td style="text-align: center;">5.09e-01</td>
<td style="text-align: center;">5.43e-02</td>
<td style="text-align: center;">5.07e-03</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">West South Asia</td>
<td style="text-align: center;">1.63e-09</td>
<td style="text-align: center;">2.10e-08</td>
<td style="text-align: center;">4.53e-08</td>
<td style="text-align: center;">3.30e-01</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">4.34e-01</td>
<td style="text-align: center;">9.13e-01</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Education Level</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">College</td>
<td style="text-align: center;">1.02e-50</td>
<td style="text-align: center;">1.19e-35</td>
<td style="text-align: center;">8.21e-41</td>
<td style="text-align: center;">8.96e-37</td>
<td style="text-align: center;">8.42e-08</td>
<td style="text-align: center;">7.75e-25</td>
<td style="text-align: center;">9.17e-10</td>
<td style="text-align: center;">8.75e-11</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Graduate School</td>
<td style="text-align: center;">5.80e-44</td>
<td style="text-align: center;">1.97e-21</td>
<td style="text-align: center;">1.74e-39</td>
<td style="text-align: center;">9.60e-23</td>
<td style="text-align: center;">3.79e-04</td>
<td style="text-align: center;">4.51e-16</td>
<td style="text-align: center;">3.15e-03</td>
<td style="text-align: center;">4.12e-08</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">High School</td>
<td style="text-align: center;">9.32e-38</td>
<td style="text-align: center;">1.31e-21</td>
<td style="text-align: center;">4.85e-33</td>
<td style="text-align: center;">6.01e-24</td>
<td style="text-align: center;">2.74e-03</td>
<td style="text-align: center;">1.19e-14</td>
<td style="text-align: center;">4.48e-05</td>
<td style="text-align: center;">5.12e-08</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PhD</td>
<td style="text-align: center;">4.16e-28</td>
<td style="text-align: center;">2.29e-18</td>
<td style="text-align: center;">4.32e-24</td>
<td style="text-align: center;">1.63e-09</td>
<td style="text-align: center;">5.54e-01</td>
<td style="text-align: center;">9.82e-08</td>
<td style="text-align: center;">2.54e-02</td>
<td style="text-align: center;">1.93e-03</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Pre-High School</td>
<td style="text-align: center;">4.48e-17</td>
<td style="text-align: center;">8.53e-11</td>
<td style="text-align: center;">7.00e-20</td>
<td style="text-align: center;">2.25e-02</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">8.06e-04</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.43e-02</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Professional School</td>
<td style="text-align: center;">2.19e-13</td>
<td style="text-align: center;">1.50e-09</td>
<td style="text-align: center;">3.50e-11</td>
<td style="text-align: center;">1.65e-12</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">3.08e-03</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Ethnicity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Asian, Asian American</td>
<td style="text-align: center;">6.37e-35</td>
<td style="text-align: center;">2.04e-22</td>
<td style="text-align: center;">4.77e-31</td>
<td style="text-align: center;">1.85e-21</td>
<td style="text-align: center;">4.80e-07</td>
<td style="text-align: center;">1.46e-13</td>
<td style="text-align: center;">4.19e-06</td>
<td style="text-align: center;">9.54e-09</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Black, African American</td>
<td style="text-align: center;">3.50e-24</td>
<td style="text-align: center;">8.08e-15</td>
<td style="text-align: center;">2.03e-20</td>
<td style="text-align: center;">8.82e-14</td>
<td style="text-align: center;">1.01e-03</td>
<td style="text-align: center;">6.16e-05</td>
<td style="text-align: center;">1.79e-03</td>
<td style="text-align: center;">2.34e-05</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Latino / Latina, Hispanic</td>
<td style="text-align: center;">1.47e-19</td>
<td style="text-align: center;">8.00e-13</td>
<td style="text-align: center;">6.30e-14</td>
<td style="text-align: center;">6.39e-07</td>
<td style="text-align: center;">2.39e-05</td>
<td style="text-align: center;">5.23e-08</td>
<td style="text-align: center;">3.19e-03</td>
<td style="text-align: center;">3.26e-03</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Native American, Alaskan Native</td>
<td style="text-align: center;">2.33e-07</td>
<td style="text-align: center;">3.11e-05</td>
<td style="text-align: center;">3.44e-09</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">6.37e-01</td>
<td style="text-align: center;">6.72e-01</td>
<td style="text-align: center;">6.07e-01</td>
<td style="text-align: center;">4.81e-01</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Pacific Islander, Native Australian</td>
<td style="text-align: center;">6.63e-04</td>
<td style="text-align: center;">1.38e-03</td>
<td style="text-align: center;">2.22e-03</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.32e-02</td>
<td style="text-align: center;">1.77e-01</td>
<td style="text-align: center;">1.59e-02</td>
<td style="text-align: center;">1.01e-01</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">White</td>
<td style="text-align: center;">1.27e-48</td>
<td style="text-align: center;">4.94e-29</td>
<td style="text-align: center;">1.44e-42</td>
<td style="text-align: center;">4.51e-42</td>
<td style="text-align: center;">1.47e-05</td>
<td style="text-align: center;">2.00e-24</td>
<td style="text-align: center;">1.18e-06</td>
<td style="text-align: center;">8.31e-10</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Gender</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Man</td>
<td style="text-align: center;">2.55e-47</td>
<td style="text-align: center;">2.19e-31</td>
<td style="text-align: center;">8.72e-41</td>
<td style="text-align: center;">1.99e-34</td>
<td style="text-align: center;">1.09e-07</td>
<td style="text-align: center;">3.55e-24</td>
<td style="text-align: center;">7.84e-08</td>
<td style="text-align: center;">1.46e-08</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Non-Binary</td>
<td style="text-align: center;">3.61e-26</td>
<td style="text-align: center;">4.94e-18</td>
<td style="text-align: center;">1.14e-21</td>
<td style="text-align: center;">3.00e-16</td>
<td style="text-align: center;">1.64e-01</td>
<td style="text-align: center;">6.67e-06</td>
<td style="text-align: center;">8.00e-03</td>
<td style="text-align: center;">8.49e-04</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Woman</td>
<td style="text-align: center;">7.04e-51</td>
<td style="text-align: center;">1.25e-27</td>
<td style="text-align: center;">1.76e-48</td>
<td style="text-align: center;">4.02e-33</td>
<td style="text-align: center;">6.36e-08</td>
<td style="text-align: center;">8.19e-22</td>
<td style="text-align: center;">4.27e-10</td>
<td style="text-align: center;">2.17e-09</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Native Language</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">English</td>
<td style="text-align: center;">8.54e-55</td>
<td style="text-align: center;">2.04e-33</td>
<td style="text-align: center;">1.91e-44</td>
<td style="text-align: center;">1.22e-44</td>
<td style="text-align: center;">3.38e-07</td>
<td style="text-align: center;">1.28e-29</td>
<td style="text-align: center;">2.10e-10</td>
<td style="text-align: center;">2.39e-12</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Not English</td>
<td style="text-align: center;">1.04e-25</td>
<td style="text-align: center;">5.10e-18</td>
<td style="text-align: center;">1.05e-27</td>
<td style="text-align: center;">9.78e-11</td>
<td style="text-align: center;">1.58e-04</td>
<td style="text-align: center;">2.40e-07</td>
<td style="text-align: center;">1.93e-04</td>
<td style="text-align: center;">6.29e-06</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Age</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">10-20 yrs old</td>
<td style="text-align: center;">5.54e-43</td>
<td style="text-align: center;">9.00e-29</td>
<td style="text-align: center;">1.46e-40</td>
<td style="text-align: center;">2.89e-29</td>
<td style="text-align: center;">1.85e-06</td>
<td style="text-align: center;">2.23e-22</td>
<td style="text-align: center;">7.63e-09</td>
<td style="text-align: center;">8.33e-09</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">20-30 yrs old</td>
<td style="text-align: center;">5.35e-50</td>
<td style="text-align: center;">1.49e-36</td>
<td style="text-align: center;">1.23e-42</td>
<td style="text-align: center;">1.79e-34</td>
<td style="text-align: center;">1.22e-07</td>
<td style="text-align: center;">6.51e-24</td>
<td style="text-align: center;">5.61e-10</td>
<td style="text-align: center;">2.90e-12</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">30-40 yrs old</td>
<td style="text-align: center;">2.71e-33</td>
<td style="text-align: center;">2.24e-18</td>
<td style="text-align: center;">7.56e-27</td>
<td style="text-align: center;">2.25e-10</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">2.37e-07</td>
<td style="text-align: center;">4.49e-02</td>
<td style="text-align: center;">3.21e-03</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">40-50 yrs old</td>
<td style="text-align: center;">2.48e-24</td>
<td style="text-align: center;">4.36e-18</td>
<td style="text-align: center;">2.98e-26</td>
<td style="text-align: center;">3.43e-16</td>
<td style="text-align: center;">1.49e-02</td>
<td style="text-align: center;">2.12e-12</td>
<td style="text-align: center;">5.43e-03</td>
<td style="text-align: center;">1.68e-04</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">50-60 yrs old</td>
<td style="text-align: center;">9.40e-23</td>
<td style="text-align: center;">9.98e-12</td>
<td style="text-align: center;">4.58e-16</td>
<td style="text-align: center;">1.96e-10</td>
<td style="text-align: center;">1.49e-01</td>
<td style="text-align: center;">9.98e-05</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">2.47e-01</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">60-70 yrs old</td>
<td style="text-align: center;">4.85e-17</td>
<td style="text-align: center;">9.35e-09</td>
<td style="text-align: center;">1.92e-14</td>
<td style="text-align: center;">4.99e-01</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">70-80 yrs old</td>
<td style="text-align: center;">5.14e-05</td>
<td style="text-align: center;">4.20e-04</td>
<td style="text-align: center;">3.91e-05</td>
<td style="text-align: center;">8.78e-01</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">2.96e-05</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">80+ yrs old</td>
<td style="text-align: center;">4.75e-01</td>
<td style="text-align: center;">9.08e-01</td>
<td style="text-align: center;">8.63e-02</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Country (Residence)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">African Islamic</td>
<td style="text-align: center;">2.01e-02</td>
<td style="text-align: center;">2.64e-02</td>
<td style="text-align: center;">4.28e-02</td>
<td style="text-align: center;">2.75e-01</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Baltic</td>
<td style="text-align: center;">8.25e-03</td>
<td style="text-align: center;">8.25e-03</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.66e-01</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Catholic Europe</td>
<td style="text-align: center;">6.35e-08</td>
<td style="text-align: center;">3.01e-04</td>
<td style="text-align: center;">7.84e-13</td>
<td style="text-align: center;">1.68e-01</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.82e-02</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Confucian</td>
<td style="text-align: center;">3.36e-08</td>
<td style="text-align: center;">1.83e-04</td>
<td style="text-align: center;">1.35e-11</td>
<td style="text-align: center;">1.62e-01</td>
<td style="text-align: center;">4.59e-01</td>
<td style="text-align: center;">5.03e-02</td>
<td style="text-align: center;">8.55e-01</td>
<td style="text-align: center;">2.13e-02</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">English-Speaking</td>
<td style="text-align: center;">1.96e-53</td>
<td style="text-align: center;">8.43e-35</td>
<td style="text-align: center;">6.34e-48</td>
<td style="text-align: center;">7.43e-47</td>
<td style="text-align: center;">1.17e-07</td>
<td style="text-align: center;">2.65e-29</td>
<td style="text-align: center;">3.29e-10</td>
<td style="text-align: center;">6.96e-13</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Latin American</td>
<td style="text-align: center;">1.14e-04</td>
<td style="text-align: center;">5.20e-05</td>
<td style="text-align: center;">7.76e-06</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Orthodox Europe</td>
<td style="text-align: center;">2.23e-03</td>
<td style="text-align: center;">1.60e-05</td>
<td style="text-align: center;">3.18e-06</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">4.34e-01</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Protestant Europe</td>
<td style="text-align: center;">6.59e-18</td>
<td style="text-align: center;">5.21e-14</td>
<td style="text-align: center;">3.82e-16</td>
<td style="text-align: center;">3.23e-06</td>
<td style="text-align: center;">1.43e-02</td>
<td style="text-align: center;">3.54e-01</td>
<td style="text-align: center;">1.66e-02</td>
<td style="text-align: center;">1.21e-02</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">West South Asia</td>
<td style="text-align: center;">3.46e-08</td>
<td style="text-align: center;">8.91e-07</td>
<td style="text-align: center;">1.29e-05</td>
<td style="text-align: center;">1.89e-03</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">3.46e-01</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Religion</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Buddhist</td>
<td style="text-align: center;">7.42e-13</td>
<td style="text-align: center;">3.16e-10</td>
<td style="text-align: center;">7.78e-09</td>
<td style="text-align: center;">2.44e-02</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.27e-02</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Christian</td>
<td style="text-align: center;">3.47e-48</td>
<td style="text-align: center;">2.43e-22</td>
<td style="text-align: center;">9.04e-47</td>
<td style="text-align: center;">1.21e-22</td>
<td style="text-align: center;">1.66e-07</td>
<td style="text-align: center;">3.99e-17</td>
<td style="text-align: center;">3.03e-08</td>
<td style="text-align: center;">3.61e-07</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Hindu</td>
<td style="text-align: center;">4.62e-14</td>
<td style="text-align: center;">3.57e-11</td>
<td style="text-align: center;">2.97e-10</td>
<td style="text-align: center;">1.12e-08</td>
<td style="text-align: center;">7.96e-02</td>
<td style="text-align: center;">6.02e-03</td>
<td style="text-align: center;">3.03e-01</td>
<td style="text-align: center;">1.89e-02</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Jewish</td>
<td style="text-align: center;">8.32e-17</td>
<td style="text-align: center;">1.85e-13</td>
<td style="text-align: center;">4.97e-13</td>
<td style="text-align: center;">8.13e-11</td>
<td style="text-align: center;">1.95e-01</td>
<td style="text-align: center;">4.75e-04</td>
<td style="text-align: center;">1.89e-01</td>
<td style="text-align: center;">4.87e-02</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Muslim</td>
<td style="text-align: center;">2.72e-14</td>
<td style="text-align: center;">1.81e-12</td>
<td style="text-align: center;">1.37e-20</td>
<td style="text-align: center;">7.50e-02</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Spiritual</td>
<td style="text-align: center;">9.75e-08</td>
<td style="text-align: center;">3.49e-07</td>
<td style="text-align: center;">3.56e-12</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">1.00e+00</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Associated $p$-values of each associated Pearson's $r$ correlation value after applying Bonferroni corrections. $\alpha=0.001$ and $\alpha=2.04 \mathrm{e}-05$ before and after applying Bonferroni corrections respectively.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ perspectiveapi.com
${ }^{4}$ rewire.online&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
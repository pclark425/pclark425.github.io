<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1897 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1897</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1897</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-38.html">extraction-schema-38</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different evaluation metrics (citations, peer review, journal prestige, automated systems) perform when assessing scientific discoveries of varying novelty or transformational nature, including comparisons between early/proxy metrics and later ground-truth impact measures.</div>
                <p><strong>Paper ID:</strong> paper-277622178</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.04677v1.pdf" target="_blank">The Disruption Index Measures Displacement Between a Paper and Its Most Cited Reference</a></p>
                <p><strong>Paper Abstract:</strong> Initially developed to capture technical innovation and later adapted to identify scientific breakthroughs, the Disruption Index (D-index) offers the first quantitative framework for analyzing transformative research. Despite its promise, prior studies have struggled to clarify its theoretical foundations, raising concerns about potential bias. Here, we show that-contrary to the common belief that the D-index measures absolute innovation-it captures relative innovation: a paper's ability to displace its most-cited reference. In this way, the D-index reflects scientific progress as the replacement of older answers with newer ones to the same fundamental question-much like light bulbs replacing candles. We support this insight through mathematical analysis, expert surveys, and large-scale bibliometric evidence. To facilitate replication, validation, and broader use, we release a dataset of D-index values for 49 million journal articles (1800-2024) based on OpenAlex.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1897.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1897.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different evaluation metrics (citations, peer review, journal prestige, automated systems) perform when assessing scientific discoveries of varying novelty or transformational nature, including comparisons between early/proxy metrics and later ground-truth impact measures.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D-index</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Disruption Index (D-index)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bibliometric measure that quantifies a paper's ability to displace its references by comparing subsequent papers that cite the focal paper only versus those that cite both the focal paper and its references; decomposed here into a local displacement factor (d_p) and a knowledge-burden factor (b_p).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>empirical analysis of citation patterns, mathematical analysis, and expert survey validation</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metrics_studied</strong></td>
                            <td>D-index (and variants D0-D4), citation counts, journal impact indicators (mentioned), h-index (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>expert peer nominations (survey), long-term citation patterns (stabilized D after long windows), Nobel Prize papers (examples)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_classification</strong></td>
                            <td>disruptive vs consolidating (disruptive: D>0; highly disruptive often D>0.2); local classification via d_p (d_p>0 disruptive, d_p<0 consolidating, d_p=0 neutral)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_characteristics</strong></td>
                            <td>OpenAlex-derived dataset of 49 million journal articles (1800-2024); specific analyses on 42M+ papers with ≥1 citation and ≥1 reference, 22M with ≥10 citations for some decompositions; expert survey: 20 scholars, 190 nominations; set of 49,077 high-impact (>100 citations) & highly disruptive (D>0.2) papers (1900-2020); Nobel-winning papers N=877 (1902-2009)</td>
                        </tr>
                        <tr>
                            <td><strong>key_quantitative_findings</strong></td>
                            <td>D decomposes as D_p ≈ d_p / (1 + b_p). Median d_p ≈ -0.2; median b_p ≈ 119, giving 1/(1+b_p) ≈ 0.01 and a characteristic D of ≈ -0.002 (actual median ≈ -0.0001). 38% of papers have positive D; 62% have zero/negative D. Only ≈1% of papers have b_p < 1 (able to shed knowledge burden). Example contrasts: Watson & Crick (1953) D = 0.96 (top 1%); Human Genome Project (1999/2001) D ≈ -0.017 (bottom 6%) despite both having high citation counts. Expert survey: mean D for nominated disruptive papers = 0.21 (top 1%); for consolidating = -0.011 (bottom 13%); AUC = 0.83 vs expert judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap_magnitude</strong></td>
                            <td>Quantified indirectly: many highly-cited works (e.g., Human Genome Project) have near-zero or negative D while other highly-cited works (Watson & Crick) have very high D; median b_p of 119 implies local displacement is scaled down ~100x by knowledge burden, explaining large gaps between raw citation counts and displacement-based impact; expert-agreement (AUC=0.83) indicates D aligns well with peer judgment while raw citations can conflate popularity with displacement.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>D-index requires long citation windows to stabilize; short windows (e.g., 5 years) can substantially under-estimate D for disruptive work (especially from small teams). Empirical turning point for team-size effects at ~10 years; disruptive papers often accumulate recognition slowly ('sleeping beauties'), so stabilization often takes ≥10 years.</td>
                        </tr>
                        <tr>
                            <td><strong>field_specific_findings</strong></td>
                            <td>High topic alignment between displacing papers and their most-cited references: among 49,077 high-impact & highly disruptive papers, 52% share a field with their top reference vs a combinatorial expectation of 0.014 (≈37× higher), indicating breakthroughs mostly displace predecessors within the same fundamental question/field.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_shape</strong></td>
                            <td>Analytical/hyperbolic relation: D_p ≈ d_p / (1 + b_p) (i.e., inverse dependence on the burden factor b_p); thus D falls rapidly as the top-reference impact grows relative to the focal paper (nonlinear, threshold-like: only when b_p << 1 is d_p preserved).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_identified</strong></td>
                            <td>Two mechanisms: (1) local displacement (d_p) capturing whether subsequent literature cites the focal paper instead of references; (2) knowledge burden (b_p = C_max / C_p) representing the citation dominance of the most-cited reference that the focal paper must displace; also citation inflation, reference completeness, and citation-window length affect measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_approaches</strong></td>
                            <td>Use longer citation windows (≥10 years) to let D stabilize; focus on sign (disruptive vs consolidating) rather than mean D to reduce reference-length bias; restrict analyses to items with ≥1 citation and ≥1 reference and to consistent document types (e.g., journal articles); consider D variants (exclude self-citations, restrict to popular references, or weight by citation counts) to test robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_exceptions</strong></td>
                            <td>Consolidating works can be foundational yet have negative D (e.g., Ketterle's Bose-Einstein condensation experimental paper D = -0.58 but Nobel-winning); many highly-cited collaborative projects (e.g., Human Genome Project) register near-zero or negative D while being influential in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>Supports the Proxy-to-Ground-Truth Gap Theory: demonstrates systematic circumstances where raw citation-based proxies mischaracterize transformational/disruptive impact and shows delayed recognition for disruptive work; provides mechanistic explanation (b_p and d_p) and quantifies how early proxies may fail for high-novelty discoveries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1897.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1897.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different evaluation metrics (citations, peer review, journal prestige, automated systems) perform when assessing scientific discoveries of varying novelty or transformational nature, including comparisons between early/proxy metrics and later ground-truth impact measures.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Citation-based indicators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citation counts and citation-based indicators (journal impact factor, h-index)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Traditional bibliometric proxies (total citations, journal impact factors, h-index) widely used for evaluation; authors argue these reflect popularity and cumulative impact rather than displacement/novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>critical background discussion and comparative examples</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metrics_studied</strong></td>
                            <td>Total citations, journal impact factor, h-index</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>long-term transformative impact implied (e.g., displacement measured by D-index or expert judgment); Nobel Prize examples used to illustrate limits</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_classification</strong></td>
                            <td>implicit split: popularity/incremental vs disruptive/transformational</td>
                        </tr>
                        <tr>
                            <td><strong>sample_characteristics</strong></td>
                            <td>Examples and literature citations rather than a new empirical sample; contrasted via case examples (Watson & Crick vs Human Genome Project)</td>
                        </tr>
                        <tr>
                            <td><strong>key_quantitative_findings</strong></td>
                            <td>No new numeric calibration beyond case examples: both Watson & Crick and Human Genome Project have high citation counts, yet D differentiates them (0.96 vs −0.017). Authors state citation-based indicators often reflect popularity rather than innovation and can incentivize incremental work.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap_magnitude</strong></td>
                            <td>Not universally quantified in this paper; illustrated by case contrasts where raw citations are similar but displacement (D) differs dramatically (example above).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Implicit: early citation-based signals reward popular/cumulative work and may miss disruptive work that has delayed recognition; citation inflation increases reference lengths over time and complicates temporal comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>field_specific_findings</strong></td>
                            <td>Paper cites literature arguing large fields may experience slowed canonical progress and that evaluation incentives favor incremental work; no precise cross-field quantification here.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_shape</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_identified</strong></td>
                            <td>Citational popularity biases, incentive structures favoring productivity over high-risk breakthroughs, and citation inflation (growing reference lists) that affects comparability over time.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_approaches</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_exceptions</strong></td>
                            <td>Some highly-cited works are also disruptive (e.g., Watson & Crick) but citation counts alone cannot reliably separate these from highly-cited consolidating/large-team works (e.g., Human Genome).</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>Supports the theory that citation-based proxies systematically under-represent novelty/transformational work and can conflate popularity with displacement; motivates alternative measures like D-index and longer windows.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1897.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1897.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different evaluation metrics (citations, peer review, journal prestige, automated systems) perform when assessing scientific discoveries of varying novelty or transformational nature, including comparisons between early/proxy metrics and later ground-truth impact measures.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expert peer judgment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expert survey / peer nominations (global breakthrough survey)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-ended expert nominations of disruptive vs consolidating papers used as a human-ground-truth benchmark to evaluate the D-index's validity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>peer judgment experiment / expert survey comparison</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metrics_studied</strong></td>
                            <td>peer nominations (disruptive vs consolidating) compared against D-index</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>treated as human expert ground truth for disruptive vs consolidating classification</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_classification</strong></td>
                            <td>respondents labelled nominated papers as 'consolidating (developmental)' or 'disruptive' per survey definitions</td>
                        </tr>
                        <tr>
                            <td><strong>sample_characteristics</strong></td>
                            <td>20 scholars from 10 research-intensive institutions across multiple disciplines providing 190 nominations of disruptive/developing papers</td>
                        </tr>
                        <tr>
                            <td><strong>key_quantitative_findings</strong></td>
                            <td>All nominations for the most disruptive paper aligned with the D-index; for most consolidating nominations all but six aligned. Mean D of nominated disruptive papers = 0.21 (top 1%); mean D of nominated consolidating papers = -0.011 (bottom 13%); area under curve (AUC) = 0.83 indicating strong agreement between D-index and expert judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap_magnitude</strong></td>
                            <td>AUC=0.83 implies strong concordance; the paper does not present a separate quantified discrepancy measure beyond AUC and mean D differences between classes.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_specific_findings</strong></td>
                            <td>Survey spanned many fields; paper reports broad alignment across disciplines but does not break down AUC by field.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_shape</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_identified</strong></td>
                            <td>D-index aligns with experts because it operationalizes displacement of canonical references—what experts often identify as breakthrough.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_approaches</strong></td>
                            <td>Authors suggest D-index can be used as an automated proxy that aligns with peer judgment for identifying disruptive papers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_exceptions</strong></td>
                            <td>Six nominations for consolidating papers did not align with D-index; consolidating but seminal works (e.g., empirical validations) can have strongly negative D despite being transformative in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>Supports the theory that displacement-based measures (D) better match expert judgments about novelty and transformational impact than raw citation counts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1897.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1897.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different evaluation metrics (citations, peer review, journal prestige, automated systems) perform when assessing scientific discoveries of varying novelty or transformational nature, including comparisons between early/proxy metrics and later ground-truth impact measures.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Citation-window effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citation window length (early vs stabilized measures)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The length of the citation observation window substantially affects measured disruption; short windows (e.g., 5 years) undercount disruptive signals that often take ≥10 years to stabilize.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>empirical cohort analysis of time-window effects on D-index and covariate relationships</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metrics_studied</strong></td>
                            <td>short-window D-index (e.g., 1–5 years) vs long-window/stabilized D-index (≥10 years); early citations as proxy</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>stabilized long-term D-index (citation windows up to 25 years used as reference for long-term behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_classification</strong></td>
                            <td>focus on small-team disruptive papers vs large-team consolidating papers (team-size as proxy for discovery type differences)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_characteristics</strong></td>
                            <td>Regression sample: 1.7M papers with 1≤k_f≤10 coauthors, 5≤r_f≤50 references, 10≤c_f≤1000 citations; cohorts from 1995–2019 analyzed with windows of 1, 3, 5, 10, 20, 25 years (N cohorts specified in Fig.5 description).</td>
                        </tr>
                        <tr>
                            <td><strong>key_quantitative_findings</strong></td>
                            <td>Short 5-year windows produced positive marginal effect of team size on D in prior replications; using extended windows the previously reported negative effect of team size on D re-emerges. Empirical turning point for the team-size effect appears at ~10 years; regression slopes change across increasing citation windows (1→25 years cohorts).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap_magnitude</strong></td>
                            <td>Qualitative: short windows can reverse inferred sign of team-size effect (positive versus negative); paper does not give a single scalar gap but documents systematic bias introduced by short windows and recovers original effect with long windows.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Delayed recognition for disruptive work; D-index life cycle that often requires ≥10 years to stabilize; disruptive papers accumulate recognition slower than consolidating ones.</td>
                        </tr>
                        <tr>
                            <td><strong>field_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>relationship_shape</strong></td>
                            <td>Time-dependent: sign/magnitude of covariate relationships (e.g., team size) with D change as window lengthens—nonstationary temporal pattern rather than simple linear bias.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_identified</strong></td>
                            <td>Small teams and more novel/disruptive work often have slower citation accrual ('sleeping beauties'), so short windows undercount their displacement; citation accrual dynamics create temporal measurement bias.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_approaches</strong></td>
                            <td>Use longer citation windows (≥10 years) to measure stabilized D; control for cohort effects when comparing across time.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_exceptions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>Supports the Proxy-to-Ground-Truth Gap Theory by empirically showing early proxy windows can systematically misrepresent long-term disruptive impact and reverse inferred relationships.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1897.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1897.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different evaluation metrics (citations, peer review, journal prestige, automated systems) perform when assessing scientific discoveries of varying novelty or transformational nature, including comparisons between early/proxy metrics and later ground-truth impact measures.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D-index variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alternative D-index definitions (D0–D4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Several alternative operationalizations of disruption (exclude self-citations, only popular references, simplified fractions, citation-weighted) that behave similarly to the original D-index but offer robustness checks or different emphases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>methodological comparison / robustness analysis</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metrics_studied</strong></td>
                            <td>D0 (main), D1 (exclude self-citations), D2 (popular references only), D3 (fraction of papers citing focal paper only among those citing focal paper), D4 (citation-weighted)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_classification</strong></td>
                            <td>same disruptive vs consolidating classification applied to each variant</td>
                        </tr>
                        <tr>
                            <td><strong>sample_characteristics</strong></td>
                            <td>100,000 randomly selected Web of Science papers used in prior analysis (97,188 after exclusions) to compare variants; also analyses on OpenAlex dataset elsewhere in paper</td>
                        </tr>
                        <tr>
                            <td><strong>key_quantitative_findings</strong></td>
                            <td>All alternative measures decrease consistently with team size; factor analysis in prior literature indicates variants measure similar dimensions; weighted and exclusion variants produce behavior consistent with D0 (robustness).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>relationship_shape</strong></td>
                            <td>Variants are strongly correlated (convergent behavior) and thus do not fundamentally alter the D-vs-novelty relationship, though weighting can emphasize magnitude/reach.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_identified</strong></td>
                            <td>Excluding self-citations or focusing on popular references can adjust sensitivity to social/cumulative citation practices; weighting by citations accounts for magnitude as well as reach.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_approaches</strong></td>
                            <td>Use variant definitions (e.g., exclude self-citations, apply citation-weighting) as robustness checks or to emphasize different aspects of displacement; authors note weighted versions (similar to D4) have been proposed.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_exceptions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>Supports robustness of displacement-based measurement: variants behave similarly and therefore reinforce findings that displacement (not raw counts) identifies disruptive work.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1897.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1897.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different evaluation metrics (citations, peer review, journal prestige, automated systems) perform when assessing scientific discoveries of varying novelty or transformational nature, including comparisons between early/proxy metrics and later ground-truth impact measures.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Network centrality metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Betweenness centrality / PageRank and related network measures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Topological network measures (betweenness centrality, PageRank) can identify structurally important papers, and the D-index is interpretable as a time-aware form of such centrality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>methodological interpretation / theoretical comparison</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metrics_studied</strong></td>
                            <td>betweenness centrality, PageRank-like centralities</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_classification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_characteristics</strong></td>
                            <td>Referenced theoretical comparisons and related literature (Gebhart & Funk 2023, Freeman 1977, Brin & Page 1998); specific empirical mapping not presented here</td>
                        </tr>
                        <tr>
                            <td><strong>key_quantitative_findings</strong></td>
                            <td>No new numerical comparison; conceptual point: D-index resembles betweenness but adds a hidden time dimension (citations referencing earlier works vs later), giving 'gatekeepers in time' interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>D-index leverages time ordering of citations explicitly, whereas many centrality measures do not, creating different temporal sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>field_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>relationship_shape</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_identified</strong></td>
                            <td>Topological centrality identifies bridging/mediating nodes, but without temporal ordering it can miss displacement dynamics captured by the D-index.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_approaches</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_exceptions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>Positions D-index as complementary to network centrality measures and supports the idea that temporal information is crucial to detecting displacement/novelty.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1897.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1897.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different evaluation metrics (citations, peer review, journal prestige, automated systems) perform when assessing scientific discoveries of varying novelty or transformational nature, including comparisons between early/proxy metrics and later ground-truth impact measures.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Citation inflation / reference-length artifact</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citation inflation and reference-length confounding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The increase over time in the number of references per paper (citation inflation) can bias temporal comparisons of disruption; authors argue true burden is dominated by the impact of the most-cited reference (b_p) rather than sheer reference count.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>empirical critique and analytical argument about temporal artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metrics_studied</strong></td>
                            <td>D-index time trends, reference length, aggregated temporal D decline claims</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_classification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_characteristics</strong></td>
                            <td>Discussion cites Park et al. (2023), Holst et al. (2024), and authors' own OpenAlex analyses across 49M articles; Fig.4 uses ~929,900 papers for demonstration.</td>
                        </tr>
                        <tr>
                            <td><strong>key_quantitative_findings</strong></td>
                            <td>Authors show D-index is independent of reference length after accounting for d_p and b_p (Fig.4); median b_p = 119 implies number of references is not the primary driver of low D values. Prior replication claims of decline may be artifacts due to zero-reference records or mixing document types.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap_magnitude</strong></td>
                            <td>Paper argues that citation-inflation-induced bias can materially affect observed temporal trends (e.g., apparent decline in disruptiveness), but provides correction strategy rather than a single scalar gap.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>As references per paper grow, naive temporal comparisons of average D may show decline; after accounting for burden and excluding zero-reference items and heterogeneous document types, decline estimates change.</td>
                        </tr>
                        <tr>
                            <td><strong>field_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>relationship_shape</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_identified</strong></td>
                            <td>Apparent decline in D over time can be driven by dataset artefacts (zero-backward-citation items), increasing reference lists, and the rising citation dominance of canonical references (increasing b_p).</td>
                        </tr>
                        <tr>
                            <td><strong>correction_approaches</strong></td>
                            <td>Control for d_p and b_p, exclude zero-reference items, restrict analysis to comparable document types (journal articles), and focus on sign of D rather than mean to reduce inflation bias.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_exceptions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>Challenges naive interpretations that average D decline implies less disruption over time by showing how dataset artefacts and citation inflation can produce misleading trends; refines Proxy-to-Ground-Truth view by isolating key confounders.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1897.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1897.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different evaluation metrics (citations, peer review, journal prestige, automated systems) perform when assessing scientific discoveries of varying novelty or transformational nature, including comparisons between early/proxy metrics and later ground-truth impact measures.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Data-completeness artifact</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Missing reference/citation data effects (dataset artefacts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Incomplete retrieval of references or mixing document types (books, chapters, theses) leads to inflated zero-reference counts and can bias D-index calculations and temporal trends.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>data-quality cautionary analysis with empirical examples</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metrics_studied</strong></td>
                            <td>D-index sensitivity to missing reference/citation fields in databases (OpenAlex, Web of Science, combined datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_classification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_characteristics</strong></td>
                            <td>Discussion references Holst et al. (2024) combining journal articles with other scholarly outputs and the authors' own OpenAlex dataset of 49M journal articles.</td>
                        </tr>
                        <tr>
                            <td><strong>key_quantitative_findings</strong></td>
                            <td>Combining heterogeneous document types increased zero-reference records (~three times more in one dataset versus Web of Science per cited critique); papers with references missing or citations missing can misleadingly produce D=0 and distort analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Missing data can spuriously create or amplify apparent temporal declines in disruptive measures if missingness varies over time or by document type.</td>
                        </tr>
                        <tr>
                            <td><strong>field_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>relationship_shape</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_identified</strong></td>
                            <td>Data-missingness creates biased zeros in D calculations; uneven missingness across time or document types confounds temporal trends.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_approaches</strong></td>
                            <td>Restrict analyses to items with ≥1 citation and ≥1 reference and to a single document type (journal articles); release and use open, cleaned datasets (authors release OpenAlex-based D dataset) to improve reproducibility and reduce artefacts.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_exceptions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>Supports the need to distinguish measurement artefacts from genuine changes in disruptive discovery rates; highlights how proxy-based conclusions can be driven by data quality issues rather than underlying scientific change.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Papers and patents are becoming less disruptive over time <em>(Rating: 2)</em></li>
                <li>Large teams develop and small teams disrupt science and technology <em>(Rating: 2)</em></li>
                <li>Are disruption index indicators convergently valid? The comparison of several indicator variants with assessments by peers <em>(Rating: 2)</em></li>
                <li>Dataset Artefacts are the Hidden Drivers of the Declining Disruptiveness in Science <em>(Rating: 2)</em></li>
                <li>A dynamic network measure of technological change <em>(Rating: 2)</em></li>
                <li>The disruption index suffers from citation inflation: Re-analysis of temporal CD trend and relationship with team size reveal discrepancies <em>(Rating: 2)</em></li>
                <li>A Mathematical Framework for Citation Disruption <em>(Rating: 1)</em></li>
                <li>The Burden of Knowledge and the "Death of the Renaissance Man": Is Innovation Getting Harder? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1897",
    "paper_id": "paper-277622178",
    "extraction_schema_id": "extraction-schema-38",
    "extracted_data": [
        {
            "name_short": "D-index",
            "name_full": "Disruption Index (D-index)",
            "brief_description": "A bibliometric measure that quantifies a paper's ability to displace its references by comparing subsequent papers that cite the focal paper only versus those that cite both the focal paper and its references; decomposed here into a local displacement factor (d_p) and a knowledge-burden factor (b_p).",
            "citation_title": "The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference",
            "mention_or_use": "use",
            "study_type": "empirical analysis of citation patterns, mathematical analysis, and expert survey validation",
            "proxy_metrics_studied": "D-index (and variants D0-D4), citation counts, journal impact indicators (mentioned), h-index (mentioned)",
            "ground_truth_measure": "expert peer nominations (survey), long-term citation patterns (stabilized D after long windows), Nobel Prize papers (examples)",
            "discovery_type_classification": "disruptive vs consolidating (disruptive: D&gt;0; highly disruptive often D&gt;0.2); local classification via d_p (d_p&gt;0 disruptive, d_p&lt;0 consolidating, d_p=0 neutral)",
            "sample_characteristics": "OpenAlex-derived dataset of 49 million journal articles (1800-2024); specific analyses on 42M+ papers with ≥1 citation and ≥1 reference, 22M with ≥10 citations for some decompositions; expert survey: 20 scholars, 190 nominations; set of 49,077 high-impact (&gt;100 citations) & highly disruptive (D&gt;0.2) papers (1900-2020); Nobel-winning papers N=877 (1902-2009)",
            "key_quantitative_findings": "D decomposes as D_p ≈ d_p / (1 + b_p). Median d_p ≈ -0.2; median b_p ≈ 119, giving 1/(1+b_p) ≈ 0.01 and a characteristic D of ≈ -0.002 (actual median ≈ -0.0001). 38% of papers have positive D; 62% have zero/negative D. Only ≈1% of papers have b_p &lt; 1 (able to shed knowledge burden). Example contrasts: Watson & Crick (1953) D = 0.96 (top 1%); Human Genome Project (1999/2001) D ≈ -0.017 (bottom 6%) despite both having high citation counts. Expert survey: mean D for nominated disruptive papers = 0.21 (top 1%); for consolidating = -0.011 (bottom 13%); AUC = 0.83 vs expert judgment.",
            "proxy_truth_gap_magnitude": "Quantified indirectly: many highly-cited works (e.g., Human Genome Project) have near-zero or negative D while other highly-cited works (Watson & Crick) have very high D; median b_p of 119 implies local displacement is scaled down ~100x by knowledge burden, explaining large gaps between raw citation counts and displacement-based impact; expert-agreement (AUC=0.83) indicates D aligns well with peer judgment while raw citations can conflate popularity with displacement.",
            "temporal_pattern": "D-index requires long citation windows to stabilize; short windows (e.g., 5 years) can substantially under-estimate D for disruptive work (especially from small teams). Empirical turning point for team-size effects at ~10 years; disruptive papers often accumulate recognition slowly ('sleeping beauties'), so stabilization often takes ≥10 years.",
            "field_specific_findings": "High topic alignment between displacing papers and their most-cited references: among 49,077 high-impact & highly disruptive papers, 52% share a field with their top reference vs a combinatorial expectation of 0.014 (≈37× higher), indicating breakthroughs mostly displace predecessors within the same fundamental question/field.",
            "relationship_shape": "Analytical/hyperbolic relation: D_p ≈ d_p / (1 + b_p) (i.e., inverse dependence on the burden factor b_p); thus D falls rapidly as the top-reference impact grows relative to the focal paper (nonlinear, threshold-like: only when b_p &lt;&lt; 1 is d_p preserved).",
            "automated_system_performance": null,
            "mechanism_identified": "Two mechanisms: (1) local displacement (d_p) capturing whether subsequent literature cites the focal paper instead of references; (2) knowledge burden (b_p = C_max / C_p) representing the citation dominance of the most-cited reference that the focal paper must displace; also citation inflation, reference completeness, and citation-window length affect measurement.",
            "correction_approaches": "Use longer citation windows (≥10 years) to let D stabilize; focus on sign (disruptive vs consolidating) rather than mean D to reduce reference-length bias; restrict analyses to items with ≥1 citation and ≥1 reference and to consistent document types (e.g., journal articles); consider D variants (exclude self-citations, restrict to popular references, or weight by citation counts) to test robustness.",
            "counterexamples_or_exceptions": "Consolidating works can be foundational yet have negative D (e.g., Ketterle's Bose-Einstein condensation experimental paper D = -0.58 but Nobel-winning); many highly-cited collaborative projects (e.g., Human Genome Project) register near-zero or negative D while being influential in practice.",
            "supports_or_challenges_theory": "Supports the Proxy-to-Ground-Truth Gap Theory: demonstrates systematic circumstances where raw citation-based proxies mischaracterize transformational/disruptive impact and shows delayed recognition for disruptive work; provides mechanistic explanation (b_p and d_p) and quantifies how early proxies may fail for high-novelty discoveries.",
            "uuid": "e1897.0"
        },
        {
            "name_short": "Citation-based indicators",
            "name_full": "Citation counts and citation-based indicators (journal impact factor, h-index)",
            "brief_description": "Traditional bibliometric proxies (total citations, journal impact factors, h-index) widely used for evaluation; authors argue these reflect popularity and cumulative impact rather than displacement/novelty.",
            "citation_title": "The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference",
            "mention_or_use": "mention",
            "study_type": "critical background discussion and comparative examples",
            "proxy_metrics_studied": "Total citations, journal impact factor, h-index",
            "ground_truth_measure": "long-term transformative impact implied (e.g., displacement measured by D-index or expert judgment); Nobel Prize examples used to illustrate limits",
            "discovery_type_classification": "implicit split: popularity/incremental vs disruptive/transformational",
            "sample_characteristics": "Examples and literature citations rather than a new empirical sample; contrasted via case examples (Watson & Crick vs Human Genome Project)",
            "key_quantitative_findings": "No new numeric calibration beyond case examples: both Watson & Crick and Human Genome Project have high citation counts, yet D differentiates them (0.96 vs −0.017). Authors state citation-based indicators often reflect popularity rather than innovation and can incentivize incremental work.",
            "proxy_truth_gap_magnitude": "Not universally quantified in this paper; illustrated by case contrasts where raw citations are similar but displacement (D) differs dramatically (example above).",
            "temporal_pattern": "Implicit: early citation-based signals reward popular/cumulative work and may miss disruptive work that has delayed recognition; citation inflation increases reference lengths over time and complicates temporal comparisons.",
            "field_specific_findings": "Paper cites literature arguing large fields may experience slowed canonical progress and that evaluation incentives favor incremental work; no precise cross-field quantification here.",
            "relationship_shape": null,
            "automated_system_performance": null,
            "mechanism_identified": "Citational popularity biases, incentive structures favoring productivity over high-risk breakthroughs, and citation inflation (growing reference lists) that affects comparability over time.",
            "correction_approaches": null,
            "counterexamples_or_exceptions": "Some highly-cited works are also disruptive (e.g., Watson & Crick) but citation counts alone cannot reliably separate these from highly-cited consolidating/large-team works (e.g., Human Genome).",
            "supports_or_challenges_theory": "Supports the theory that citation-based proxies systematically under-represent novelty/transformational work and can conflate popularity with displacement; motivates alternative measures like D-index and longer windows.",
            "uuid": "e1897.1"
        },
        {
            "name_short": "Expert peer judgment",
            "name_full": "Expert survey / peer nominations (global breakthrough survey)",
            "brief_description": "Open-ended expert nominations of disruptive vs consolidating papers used as a human-ground-truth benchmark to evaluate the D-index's validity.",
            "citation_title": "The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference",
            "mention_or_use": "use",
            "study_type": "peer judgment experiment / expert survey comparison",
            "proxy_metrics_studied": "peer nominations (disruptive vs consolidating) compared against D-index",
            "ground_truth_measure": "treated as human expert ground truth for disruptive vs consolidating classification",
            "discovery_type_classification": "respondents labelled nominated papers as 'consolidating (developmental)' or 'disruptive' per survey definitions",
            "sample_characteristics": "20 scholars from 10 research-intensive institutions across multiple disciplines providing 190 nominations of disruptive/developing papers",
            "key_quantitative_findings": "All nominations for the most disruptive paper aligned with the D-index; for most consolidating nominations all but six aligned. Mean D of nominated disruptive papers = 0.21 (top 1%); mean D of nominated consolidating papers = -0.011 (bottom 13%); area under curve (AUC) = 0.83 indicating strong agreement between D-index and expert judgment.",
            "proxy_truth_gap_magnitude": "AUC=0.83 implies strong concordance; the paper does not present a separate quantified discrepancy measure beyond AUC and mean D differences between classes.",
            "temporal_pattern": null,
            "field_specific_findings": "Survey spanned many fields; paper reports broad alignment across disciplines but does not break down AUC by field.",
            "relationship_shape": null,
            "automated_system_performance": null,
            "mechanism_identified": "D-index aligns with experts because it operationalizes displacement of canonical references—what experts often identify as breakthrough.",
            "correction_approaches": "Authors suggest D-index can be used as an automated proxy that aligns with peer judgment for identifying disruptive papers.",
            "counterexamples_or_exceptions": "Six nominations for consolidating papers did not align with D-index; consolidating but seminal works (e.g., empirical validations) can have strongly negative D despite being transformative in practice.",
            "supports_or_challenges_theory": "Supports the theory that displacement-based measures (D) better match expert judgments about novelty and transformational impact than raw citation counts.",
            "uuid": "e1897.2"
        },
        {
            "name_short": "Citation-window effect",
            "name_full": "Citation window length (early vs stabilized measures)",
            "brief_description": "The length of the citation observation window substantially affects measured disruption; short windows (e.g., 5 years) undercount disruptive signals that often take ≥10 years to stabilize.",
            "citation_title": "The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference",
            "mention_or_use": "use",
            "study_type": "empirical cohort analysis of time-window effects on D-index and covariate relationships",
            "proxy_metrics_studied": "short-window D-index (e.g., 1–5 years) vs long-window/stabilized D-index (≥10 years); early citations as proxy",
            "ground_truth_measure": "stabilized long-term D-index (citation windows up to 25 years used as reference for long-term behavior)",
            "discovery_type_classification": "focus on small-team disruptive papers vs large-team consolidating papers (team-size as proxy for discovery type differences)",
            "sample_characteristics": "Regression sample: 1.7M papers with 1≤k_f≤10 coauthors, 5≤r_f≤50 references, 10≤c_f≤1000 citations; cohorts from 1995–2019 analyzed with windows of 1, 3, 5, 10, 20, 25 years (N cohorts specified in Fig.5 description).",
            "key_quantitative_findings": "Short 5-year windows produced positive marginal effect of team size on D in prior replications; using extended windows the previously reported negative effect of team size on D re-emerges. Empirical turning point for the team-size effect appears at ~10 years; regression slopes change across increasing citation windows (1→25 years cohorts).",
            "proxy_truth_gap_magnitude": "Qualitative: short windows can reverse inferred sign of team-size effect (positive versus negative); paper does not give a single scalar gap but documents systematic bias introduced by short windows and recovers original effect with long windows.",
            "temporal_pattern": "Delayed recognition for disruptive work; D-index life cycle that often requires ≥10 years to stabilize; disruptive papers accumulate recognition slower than consolidating ones.",
            "field_specific_findings": null,
            "relationship_shape": "Time-dependent: sign/magnitude of covariate relationships (e.g., team size) with D change as window lengthens—nonstationary temporal pattern rather than simple linear bias.",
            "automated_system_performance": null,
            "mechanism_identified": "Small teams and more novel/disruptive work often have slower citation accrual ('sleeping beauties'), so short windows undercount their displacement; citation accrual dynamics create temporal measurement bias.",
            "correction_approaches": "Use longer citation windows (≥10 years) to measure stabilized D; control for cohort effects when comparing across time.",
            "counterexamples_or_exceptions": null,
            "supports_or_challenges_theory": "Supports the Proxy-to-Ground-Truth Gap Theory by empirically showing early proxy windows can systematically misrepresent long-term disruptive impact and reverse inferred relationships.",
            "uuid": "e1897.3"
        },
        {
            "name_short": "D-index variants",
            "name_full": "Alternative D-index definitions (D0–D4)",
            "brief_description": "Several alternative operationalizations of disruption (exclude self-citations, only popular references, simplified fractions, citation-weighted) that behave similarly to the original D-index but offer robustness checks or different emphases.",
            "citation_title": "The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference",
            "mention_or_use": "use",
            "study_type": "methodological comparison / robustness analysis",
            "proxy_metrics_studied": "D0 (main), D1 (exclude self-citations), D2 (popular references only), D3 (fraction of papers citing focal paper only among those citing focal paper), D4 (citation-weighted)",
            "ground_truth_measure": null,
            "discovery_type_classification": "same disruptive vs consolidating classification applied to each variant",
            "sample_characteristics": "100,000 randomly selected Web of Science papers used in prior analysis (97,188 after exclusions) to compare variants; also analyses on OpenAlex dataset elsewhere in paper",
            "key_quantitative_findings": "All alternative measures decrease consistently with team size; factor analysis in prior literature indicates variants measure similar dimensions; weighted and exclusion variants produce behavior consistent with D0 (robustness).",
            "proxy_truth_gap_magnitude": null,
            "temporal_pattern": null,
            "field_specific_findings": null,
            "relationship_shape": "Variants are strongly correlated (convergent behavior) and thus do not fundamentally alter the D-vs-novelty relationship, though weighting can emphasize magnitude/reach.",
            "automated_system_performance": null,
            "mechanism_identified": "Excluding self-citations or focusing on popular references can adjust sensitivity to social/cumulative citation practices; weighting by citations accounts for magnitude as well as reach.",
            "correction_approaches": "Use variant definitions (e.g., exclude self-citations, apply citation-weighting) as robustness checks or to emphasize different aspects of displacement; authors note weighted versions (similar to D4) have been proposed.",
            "counterexamples_or_exceptions": null,
            "supports_or_challenges_theory": "Supports robustness of displacement-based measurement: variants behave similarly and therefore reinforce findings that displacement (not raw counts) identifies disruptive work.",
            "uuid": "e1897.4"
        },
        {
            "name_short": "Network centrality metrics",
            "name_full": "Betweenness centrality / PageRank and related network measures",
            "brief_description": "Topological network measures (betweenness centrality, PageRank) can identify structurally important papers, and the D-index is interpretable as a time-aware form of such centrality.",
            "citation_title": "The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference",
            "mention_or_use": "mention",
            "study_type": "methodological interpretation / theoretical comparison",
            "proxy_metrics_studied": "betweenness centrality, PageRank-like centralities",
            "ground_truth_measure": null,
            "discovery_type_classification": null,
            "sample_characteristics": "Referenced theoretical comparisons and related literature (Gebhart & Funk 2023, Freeman 1977, Brin & Page 1998); specific empirical mapping not presented here",
            "key_quantitative_findings": "No new numerical comparison; conceptual point: D-index resembles betweenness but adds a hidden time dimension (citations referencing earlier works vs later), giving 'gatekeepers in time' interpretation.",
            "proxy_truth_gap_magnitude": null,
            "temporal_pattern": "D-index leverages time ordering of citations explicitly, whereas many centrality measures do not, creating different temporal sensitivity.",
            "field_specific_findings": null,
            "relationship_shape": null,
            "automated_system_performance": null,
            "mechanism_identified": "Topological centrality identifies bridging/mediating nodes, but without temporal ordering it can miss displacement dynamics captured by the D-index.",
            "correction_approaches": null,
            "counterexamples_or_exceptions": null,
            "supports_or_challenges_theory": "Positions D-index as complementary to network centrality measures and supports the idea that temporal information is crucial to detecting displacement/novelty.",
            "uuid": "e1897.5"
        },
        {
            "name_short": "Citation inflation / reference-length artifact",
            "name_full": "Citation inflation and reference-length confounding",
            "brief_description": "The increase over time in the number of references per paper (citation inflation) can bias temporal comparisons of disruption; authors argue true burden is dominated by the impact of the most-cited reference (b_p) rather than sheer reference count.",
            "citation_title": "The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference",
            "mention_or_use": "use",
            "study_type": "empirical critique and analytical argument about temporal artifacts",
            "proxy_metrics_studied": "D-index time trends, reference length, aggregated temporal D decline claims",
            "ground_truth_measure": null,
            "discovery_type_classification": null,
            "sample_characteristics": "Discussion cites Park et al. (2023), Holst et al. (2024), and authors' own OpenAlex analyses across 49M articles; Fig.4 uses ~929,900 papers for demonstration.",
            "key_quantitative_findings": "Authors show D-index is independent of reference length after accounting for d_p and b_p (Fig.4); median b_p = 119 implies number of references is not the primary driver of low D values. Prior replication claims of decline may be artifacts due to zero-reference records or mixing document types.",
            "proxy_truth_gap_magnitude": "Paper argues that citation-inflation-induced bias can materially affect observed temporal trends (e.g., apparent decline in disruptiveness), but provides correction strategy rather than a single scalar gap.",
            "temporal_pattern": "As references per paper grow, naive temporal comparisons of average D may show decline; after accounting for burden and excluding zero-reference items and heterogeneous document types, decline estimates change.",
            "field_specific_findings": null,
            "relationship_shape": null,
            "automated_system_performance": null,
            "mechanism_identified": "Apparent decline in D over time can be driven by dataset artefacts (zero-backward-citation items), increasing reference lists, and the rising citation dominance of canonical references (increasing b_p).",
            "correction_approaches": "Control for d_p and b_p, exclude zero-reference items, restrict analysis to comparable document types (journal articles), and focus on sign of D rather than mean to reduce inflation bias.",
            "counterexamples_or_exceptions": null,
            "supports_or_challenges_theory": "Challenges naive interpretations that average D decline implies less disruption over time by showing how dataset artefacts and citation inflation can produce misleading trends; refines Proxy-to-Ground-Truth view by isolating key confounders.",
            "uuid": "e1897.6"
        },
        {
            "name_short": "Data-completeness artifact",
            "name_full": "Missing reference/citation data effects (dataset artefacts)",
            "brief_description": "Incomplete retrieval of references or mixing document types (books, chapters, theses) leads to inflated zero-reference counts and can bias D-index calculations and temporal trends.",
            "citation_title": "The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference",
            "mention_or_use": "use",
            "study_type": "data-quality cautionary analysis with empirical examples",
            "proxy_metrics_studied": "D-index sensitivity to missing reference/citation fields in databases (OpenAlex, Web of Science, combined datasets)",
            "ground_truth_measure": null,
            "discovery_type_classification": null,
            "sample_characteristics": "Discussion references Holst et al. (2024) combining journal articles with other scholarly outputs and the authors' own OpenAlex dataset of 49M journal articles.",
            "key_quantitative_findings": "Combining heterogeneous document types increased zero-reference records (~three times more in one dataset versus Web of Science per cited critique); papers with references missing or citations missing can misleadingly produce D=0 and distort analyses.",
            "proxy_truth_gap_magnitude": null,
            "temporal_pattern": "Missing data can spuriously create or amplify apparent temporal declines in disruptive measures if missingness varies over time or by document type.",
            "field_specific_findings": null,
            "relationship_shape": null,
            "automated_system_performance": null,
            "mechanism_identified": "Data-missingness creates biased zeros in D calculations; uneven missingness across time or document types confounds temporal trends.",
            "correction_approaches": "Restrict analyses to items with ≥1 citation and ≥1 reference and to a single document type (journal articles); release and use open, cleaned datasets (authors release OpenAlex-based D dataset) to improve reproducibility and reduce artefacts.",
            "counterexamples_or_exceptions": null,
            "supports_or_challenges_theory": "Supports the need to distinguish measurement artefacts from genuine changes in disruptive discovery rates; highlights how proxy-based conclusions can be driven by data quality issues rather than underlying scientific change.",
            "uuid": "e1897.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Papers and patents are becoming less disruptive over time",
            "rating": 2
        },
        {
            "paper_title": "Large teams develop and small teams disrupt science and technology",
            "rating": 2
        },
        {
            "paper_title": "Are disruption index indicators convergently valid? The comparison of several indicator variants with assessments by peers",
            "rating": 2
        },
        {
            "paper_title": "Dataset Artefacts are the Hidden Drivers of the Declining Disruptiveness in Science",
            "rating": 2
        },
        {
            "paper_title": "A dynamic network measure of technological change",
            "rating": 2
        },
        {
            "paper_title": "The disruption index suffers from citation inflation: Re-analysis of temporal CD trend and relationship with team size reveal discrepancies",
            "rating": 2
        },
        {
            "paper_title": "A Mathematical Framework for Citation Disruption",
            "rating": 1
        },
        {
            "paper_title": "The Burden of Knowledge and the \"Death of the Renaissance Man\": Is Innovation Getting Harder?",
            "rating": 1
        }
    ],
    "cost": 0.019247,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</p>
<p>Yiling Lin 
Linzhuo Li linzhuoli@zju.edu.cn 
School of Computing and Information
University of Pittsburgh
15260PittsburghPA</p>
<p>Lingfei Wu 
School of Computing and Information
University of Pittsburgh
15260PittsburghPA</p>
<p>Department of Sociology
Zhejiang University
310058HangzhouChina</p>
<p>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference
DA7C602C115FDCAC7BDC6539F7327146
Initially developed to capture technical innovation and later adapted to identify scientific breakthroughs, the Disruption Index (D-index) offers the first quantitative framework for analyzing transformative research.Despite its promise, prior studies have struggled to clarify its theoretical foundations, raising concerns about potential bias.Here, we show that-contrary to the common belief that the D-index measures absolute innovation-it captures relative innovation: a paper's ability to displace its most-cited reference.In this way, the D-index reflects scientific progress as the replacement of older answers with newer ones to the same fundamental question-much like light bulbs replacing candles.We support this insight through mathematical analysis, expert surveys, and large-scale bibliometric evidence.To facilitate replication, validation, and broader use, we release a dataset of D-index values for 49 million journal articles (1800-2024) based on OpenAlex.</p>
<p>Introduction</p>
<p>Scientific breakthroughs are the engine of progress, driving technological advancements, economic growth, and societal transformation.From the development of mRNA-based COVID-19 vaccines to the rise of artificial intelligence (AI) and progress in quantum computing, transformative discoveries have reshaped industries and redefined global competitiveness.Recognizing the profound impact of such breakthroughs, national agencies and global alliances are increasingly focused on securing leadership in science and technology, making the ability to identify and support emerging research frontiers a strategic priority.</p>
<p>However, for decades, decision-makers have relied on citation-based indicators-such as journal impact factors and the h-index-to identify high-impact research.These metrics often reflect popularity rather than innovation, shaping hiring, promotion, and funding decisions in ways that favor incremental research from established scholars (Franzoni et al., 2022).This evaluation system creates incentives for researchers to maximize publication counts on familiar topics rather than pursue high-risk, high-reward discoveries.As a result, there is growing concern about scientific stagnation (Chu &amp; Evans, 2021), where transformative ideas struggle to gain recognition within an ecosystem optimized for productivity over breakthrough potential (Bhattacharya &amp; Packalen, 2020).</p>
<p>To address this gap, the Disruption Index (D-index)-originally developed to measure technological innovation (Funk &amp; Owen-Smith, 2017)-was introduced as the first metric to capture scientific breakthroughs (Wu et al., 2019), offering an alternative to traditional citation counts.The D-index has gained wide interest among scholars, funding agencies, media, and the general public due to its success in highlighting landmark discoveries.For example, it assigns a high disruption score to Watson and Crick's 1953 DNA paper (D = 0.96, top 1%) and a low score to the 1999 Human Genome Project paper (D = -0.017,bottom 6%), even though both have similarly high citation counts.This distinction, invisible to citation metrics alone, underscores the D-index's ability to differentiate truly novel work from cumulative efforts.</p>
<p>Despite widespread interest, efforts to expand and replicate the D-index have been hindered by limited theoretical clarity, lack of large-scale open data, and inconsistent implementation.For example, a study of historical D-index trends using Web of Science data reported a consistent decline over the past six decades (Park et al., 2023).Replication attempts using alternative data sources-without access to Web of Science-found that the decline is much smaller after removing zero-reference works, raising concerns about whether the original finding was an artifact (Holst et al., 2024).However, the dataset Holst et al. used (Z. Lin et al., 2023) contained roughly three times more zero-reference records than the original study, as it combined journal articles-which typically include references-with other scholarly works, such as book chapters, which often do not.Recent analyses confirm that Park et al.'s findings remain robust even after excluding zero-reference items (Park et al., 2025).</p>
<p>A second example concerns the relationship between team size and the D-index.Using Web of Science data, Wu et al. (2019) reported a consistent negative association between team size and disruption.In contrast, Petersen, Arroyave, andPammolli (2024, 2025), using Microsoft Academic data, found a positive marginal effect after controlling for covariates, raising the possibility that the earlier findings were influenced by untested confounders.However, while Wu et al. used the longest available time window in calculating the D-index, the replication relied on a fixed 5-year window, which may bias results in favor of large teams.Since small teams often take longer to accumulate citations-acting as "sleeping beauties"-short windows can understate their disruptive impact.Reproducing Petersen et al.'s results using the same model, data, and software, Wu et al. confirmed the positive effect under a 5-year window, but showed that the negative association reemerges when extending the window, with a clear turning point at 10 years (Lin et al., 2025).</p>
<p>Without a clear understanding of its theoretical foundations or standardized practices for datasets and code, the D-index risks remaining a fun intellectual toy model rather than a practical tool for research evaluation and policy-making.This raises a central question: what does the D-index actually measure?</p>
<p>In this paper, we position the D-index as a foundational metric for identifying breakthrough research.Contrary to the common belief that it measures absolute innovation (Park et al., 2023;Wu et al., 2019), we show that the D-index captures relative innovation-a paper's ability to displace its most-cited reference.In doing so, it reflects scientific progress as the replacement of older answers with better ones to the same fundamental question-much like light bulbs replacing candles.We support this insight through mathematical analysis, expert surveys, and large-scale bibliometric evidence.To facilitate replication, verification, and broader use, we release a dataset of D-index values for 49 million journal articles (1800-2024) based on OpenAlex.</p>
<p>The Disruption Index Captures Displacement Between a Paper and its Most Cited Reference</p>
<p>Calculating the D-index begins by classifying all subsequent papers citing a focal paper p into three types: those that cite p but not its references (type i), those that cite both p and its references (type j), and those that cite only its references but not p (type k).The D-index is then defined as the difference between the number of type i and type j papers, normalized by the total number of type i, j, and k papers (Funk &amp; Owen-Smith, 2017;Wu et al., 2019).This formulation captures the extent to which a focal paper displaces its predecessors in subsequent literature.The formula is shown in Eq. 1, with N denoting the number of each type of paper.A simplified illustration is provided in Fig. 1.  (Wu et al., 2019).</p>
<p>Through years of investigation, we find that the D-index captures displacement between a paper and its most cited reference.This is because to achieve a high D-index, a paper must compete against its references for future citations, primarily against the most cited reference due to the long-tail distribution of citation impacts among these references.We can see this insight by rearranging Eq. 1:</p>
<p>Eq. ( 2)
𝐷 𝑝 = 𝑁 𝑖 − 𝑁 𝑗 ( ) / 𝑁 𝑖 + 𝑁 𝑗 ( ) 1+𝑁 𝑘 / 𝑁 𝑖 + 𝑁 𝑗 ( ) ≈ 1 1+𝑅 𝑘 𝑑 𝑝
Here, we decompose the D-index into two terms, d p and R k .d p = (N i -N j )/(N i +N j ) is a "local" measure reflecting the focal paper p's intrinsic innovative level based on the two types of citing papers of it.The other term, R k = N k /(N i +N j ), approximates C total /(N i +N j ), where N k (the exclusive citations to the references) serves as a proxy for the total citations to the references C total .This approximation holds because shared citations between the focal paper and its references are generally an order of magnitude smaller than total reference citations.As a result, R k quantifies the ratio of citation impact between the focal paper's references and the focal paper itself.</p>
<p>Notably, this competition primarily occurs between the focal paper and its most cited reference, as reference citations follow Zipf's law:</p>
<p>Eq. ( 3)
𝐶 𝑟 ∝ 𝑐 1 (𝑏+𝑟) 𝑎
where c is a constant, rank r represents the decreasing rank of the reference by citations, and C r denotes the citation impact of the corresponding reference.Parameter a indicates how unequal the citation distribution is, and parameter b is a fitted constant.Empirical results are shown in Fig. 2. To quantify the dominance of the most-cited reference (r = 1) over the total citations to all references, we calculate the following ratio:</p>
<p>Eq. ( 4)
𝐶 𝑚𝑎𝑥 / 𝐶 𝑡𝑜𝑡𝑎𝑙 = 𝑐 1 (𝑏+1) 𝑎 / 𝑟 = 1 𝑁 ∑ 𝑐 1 (𝑏+𝑟) 𝑎
where C max represents the citation count of the most cited reference, and C total is the total number of citations across all references.Note that C total can also be expressed as:</p>
<p>Eq. ( 5)
𝐶 𝑡𝑜𝑡𝑎𝑙 = 𝑟 = 1 𝑁 ∑ 𝑐 1 (𝑏+𝑟) 𝑎 ≈ 𝑐 1 𝑁 ∫ 𝑑𝑥 (𝑏+𝑥) 𝑎 ≈ 𝑐( (𝑏+𝑁) 1−𝑎 1−𝑎 − (𝑏+1) 1−𝑎 1−𝑎 )
Thus, the ratio C max /C total simplifies to:</p>
<p>Eq. ( 6)
𝐶 𝑚𝑎𝑥 / 𝐶 𝑡𝑜𝑡𝑎𝑙 ≈ 1 (𝑏+1) 𝑎 /( (𝑏+𝑁) 1−𝑎 1−𝑎 − (𝑏+1) 1−𝑎 1−𝑎 ) ≈ 1 (𝑏+1) 𝑎 / (𝑏+1) 1−𝑎 𝑎−1 ≈ 𝑎−1 1+𝑏
This is because a &gt;1 (Fig. 2c), so the exponent (1 - a) is negative, making (b + N) 1-a a rapidly shrinking term as N increases.As a result, its influence is negligible.Substituting the empirical values a = 2.0 and b = 1.4 into Eq.6, we estimate C max /C total 0.42, closely matching the ≈ empirical value of 0.40 and validating our analytical reasoning (Fig. 2e).</p>
<p>Given that C total 2.5C max we can rewrite Eq. 2 as: ≈ Eq. ( 7)
𝐷 𝑝 = 1 1+𝑅 𝑘 𝑑 𝑝 ≈ 1 1+𝐶 𝑡𝑜𝑡𝑎𝑙 / 𝑁 𝑖 + 𝑁 𝑗 ( ) 𝑑 𝑝 ≈ 1 1+2.5<em>𝐶 𝑚𝑎𝑥 / 𝑁 𝑖 + 𝑁 𝑗 ( ) 𝑑 𝑝 = 1 1+2.5</em>𝐶 𝑚𝑎𝑥 /𝐶 𝑝 𝑑 𝑝 where C p = N i +N j , or, Eq. (8) 𝐷 𝑝 ≈ 1 1+𝑏 𝑝 𝑑 𝑝
Where b p = C max /C p .Eq. 8 shows that the D-index is primarily determined by two variables, d p and b p .We define these terms, discuss their interpretations, and present their empirical values below.</p>
<p>The local displacement factor, d p = N i /C p -N j /C p = p i -p j , measures the disparity between the probability of two types of citing papers, those that cite only the focal paper p and those that cite it along with its references.This metric reflects the intrinsic innovativeness of the focal paper.If d p &gt; 0, the focal paper tends to undermine the influence of prior work; if d f &lt; 0, it consolidates and enhances previous contributions; and if d f = 0, it is neutral.In our dataset, only 38% of papers are disruptive, with a median d p of -0.2 (Fig. 3a).</p>
<p>The knowledge burden factor, b p = C max /C p , represents the ratio of the most-cited reference's citation impact to that of the focal paper.The name draws from the "burden of knowledge" theory (Jones, 2009) and captures the challenge a paper faces in displacing or consolidating its most influential predecessor.A truly disruptive paper must surpass the impact of its most-cited reference-thus carrying little burden (b p &lt; 1).</p>
<p>In our dataset, however, most papers fall short of this standard: b p has a median value of 119, yielding 1/(1+b p ) ≈ 0.01 (Fig. 3b).This helps explain why most papers have D-index values close to zero-not because they lack the potential to displace or build upon prior work, but because their influence remains largely localized within a field rather than recognized across fields.As Newton famously said, great papers may "stand on the shoulders of giants"-but not all who stand on giants become giants themselves.Only a small fraction of papers-about 1%-manage to shed the burden of past knowledge and displace their predecessors (b p &lt; 1).</p>
<p>Combining these two factors yields a characteristic D-index value of -0.2 × 0.01 = -0.002(Fig. 3c), helping explain why most papers have a small negative D-index.Of course, this is a rough approximation-the actual median D-index is even smaller, at approximately -0.0001.This reflects how science typically progresses: through incremental contributions by the many, with only a few papers redefining the field.Notably, the average D-index for Nobel Prize-winning papers (N = 877, 1902-2009) is just 0.1 (Wu et al., 2019).</p>
<p>The Disruption Index Captures How New Answers Replace Old Ones to the Same Scientific Question</p>
<p>Given the difficulty of displacing science, how do high D-index papers emerge?Do they represent true breakthroughs, displacing their most-cited reference with genuine alternatives, or can the D-index be manipulated to obscure sources and claim intellectual credit across fields?</p>
<p>To investigate, we re-analyzed data from our 2019 global breakthrough survey (Wu et al., 2019).In 2019, we conducted an open-ended survey on identifying breakthrough research in science, performed in person, over the telephone, or using Skype, approved by the University of Chicago Institutional Review Board (IRB18-1248).The survey asked scholars across various fields to propose papers that either disrupt or consolidate science in their fields, using the following definitions: (a) Consolidating (labeled "developmental" in the survey) papers: Extensions or improvements of previous theory, methods, or findings; (b) Disruptive papers: Punctuated advances beyond previous theory, methods, or findings.</p>
<p>We provided respondents with examples like the BTW model (Bak et al., 1987) and Bose-Einstein condensation (Davis et al., 1995) papers to illustrate disrupting and consolidating papers.Respondents then proposed three to ten disruptive and developing papers.Our panel included scientists from ten prominent research-intensive institutions across the United States, China, Japan, France, and Germany, with training in mathematics, physics, chemistry, biology, medicine, engineering, computer science, psychology, and economics.</p>
<p>Among the 20 scholars who submitted 190 responses, all nominations for the most disruptive paper aligned with our measure, and all but six for the most consolidating paper did as well.The average D-index of papers nominated as disruptive is 0.21, placing them in the top 1% of most disruptive papers.The average D-index of papers nominated as consolidating is −0.011, placing them in the bottom 13%.This analysis yielded an overall area under the curve (AUC) of 0.83, indicating strong agreement between the D-index and expert judgment.</p>
<p>For the current research, we re-analyzed the data to select the top nominated papers from the survey and identified their most cited references, as presented in Table 1.In addition to the nominated papers from the global expert survey, we analyze a second set of extraordinary papers selected by Nature editors to celebrate the journal's 150th anniversary (10 Extraordinary Nature Papers, 2019), in order to enhance the representativeness of our dataset in capturing groundbreaking science.Notably, Watson and Crick's 1953 paper on the structure of DNA appears in both sets, reflecting broad consensus on its breakthrough significance.This set of Nature breakthrough papers is also highly disruptive, confirming the strong alignment between the D-index and expert judgment.</p>
<p>Our review of these two sets of breakthrough papers and their most-cited references reveals a consistent pattern: displacement within the same fundamental question, with newer work offering clearer or more powerful answers.In biology, for example, Watson and Crick's 1953 paper (Watson &amp; Crick, 1953) displaced Pauling and Corey's competing model (Pauling &amp; Corey, 1953) by introducing the double-helix structure of DNA-correcting the earlier triple-helix hypothesis and transforming molecular biology.In computer science, Turing's 1937 work (Turing, 1937) reframed Gödel's incompleteness theorems (Gödel, 1931) by introducing the Turing machine, laying the foundation for modern computation.</p>
<p>To examine whether the insight from case studies holds at scale, we quantified how often displacing papers (D &gt; 0) and their most-cited references share overlapping topics.OpenAlex journal articles are classified into an average of two fields (e.g., Discrete Mathematics, Molecular Biology, and Organic Chemistry) using a 292-category taxonomy (Sinha et al., 2015).We analyzed 49,077 high-impact (&gt;100 citations) and highly disruptive (D&gt;0.2) papers (1900-2020) alongside their most-cited references.If displacement were random, combinatorial calculations predict a 0.014 probability that these highly disruptive papers share a field with their most-cited reference (Eq.9).Yet, empirical analysis shows a 0.52 probability-37 times higher than expected.The high topic alignment between displacing papers and their top references suggests that breakthroughs often occur as purposeful innovations, rather than merely from the random recombination of prior knowledge (Weitzman, 1998).</p>
<p>Eq. ( 9)
𝑝 = 1 − 290 2 ( ) 292 2 ( ) ≈ 0. 014</p>
<p>Evaluating the Robustness of the Disruption Index</p>
<p>Our understanding of what the D-index actually measures-the displacing relationship between a focal paper and its most-cited reference-sheds light on ongoing debates about its technical complexity (Bentley et al., 2023;Holst et al., 2024;Leydesdorff et al., 2021;Macher et al., 2023), as well as its patterns and interpretation (Park et al., 2023).Below, we address recent concerns aimed at informing better decisions on these technical issues.</p>
<p>1 The D-index and Reference Length</p>
<p>As science advances, more papers are published every year, and each paper cites more prior papers.This phenomenon is called "citation inflation" due to its similarity with monetary inflation caused by an increase in the money supply in economic systems (Pan et al., 2018).A recent study raised concerns about whether this could confound the observed decline in the average value of the D-index of all papers (Park et al., 2023), and make the temporal analysis of the D-index challenging in general (Petersen et al., 2024(Petersen et al., , 2025)).The rationale is that the more references a focal paper includes, the less likely it is to have a high D-index, as it becomes increasingly difficult to eclipse all its references.If this reasoning holds, the D-index may converge toward zero as citation counts continue to inflate.</p>
<p>While this rationale helps highlight the growing knowledge burden faced by papers over time, it is not directly relevant to understanding the D-index.As we have shown, the true burden-captured by the knowledge burden factor b p -is not about the number of references but about the impact of the most-cited reference.In other words, the challenge lies not in how many prior works exist, but in which "giant" in the canonical literature the focal paper attempts to displace-not the era in which the paper is published.To better illustrate this point, we show that the D-index is independent of reference length after accounting for the local displacement index (d p ) and the burden factor (b p ). See Fig. 4. We acknowledge that the analysis here is based on approximations.In research evaluation, when assessing the overall innovation performance of a collection of papers, focusing on the sign of the D-index rather than its average value can help minimize the influence of reference length (Petersen et al., 2024).</p>
<p>2 The D-index and Citation Window Length.</p>
<p>The D-index has a life cycle: it changes as the focal paper and its references receive more citations and stabilizes when citation growth ceases (Bornmann &amp; Tekles, 2019;Lin et al., 2022).Therefore, to calculate a stabilized D-index, the citation window, i.e., the time window for analyzing subsequent citations to the focal paper, must not be too short.Recent studies have used a five-year citation window (Park et al., 2023;Petersen et al., 2024Petersen et al., , 2025), but we do not recommend this approach, as the D-index may take ten years or more to fully stabilize-especially for disruptive papers, which tend to accumulate recognition more slowly than consolidating ones (Bornmann &amp; Tekles, 2019;Lin et al., 2022).</p>
<p>The short, five-year citation window has caused issues; for example, recent research observed the positive marginal effect of team size on the D-index while accounting for various confounders (Petersen et al., 2024(Petersen et al., , 2025)), contrary to previous reports (Wu et al., 2019).This is because it takes a longer time for small teams to accumulate citations compared to large teams.As a result, a short time window underestimates the D-index of small teams (see Extended Data Fig. 7 in previous research (Wu et al., 2019)).Here we show in Fig. 5 that with an extended citation window, the negative effect of team size on the D-index reported in (Wu et al., 2019) is recovered using the same model suggested by (Petersen et al., 2024).</p>
<p>Eq. ( 10 Eq. 10 controls for the temporal change of the D-index using yearly fixed effects, denoted by D t .</p>
<p>The results of the ordinary least squares (OLS) estimation, conducted using the STATA 13.0 package reg, are shown in Fig. 5.These results are based on a dataset of 1.7 million papers with 1 ≤ k f ≤ 10 coauthors, 5 ≤ r f ≤ 50 references, and 10 ≤ c f ≤ 1000 citations, following the same parameters as in (Petersen et al., 2024(Petersen et al., , 2025)).The independent variables are modeled using a logarithmic transform due to their right-skewed distribution.The marginal effects of team size on the D-index are calculated with all other covariates held at their mean values.This figure is reproduced from our early research, where we also controlled for cohort effects and found that the time window effect remains consistent (Lin et al., 2025).</p>
<p>3 The D-index's Discriminative Power and Emerging Alternatives</p>
<p>Figure 6.Alternative versions of D-index.The figure is reproduced from our earlier research (Wu et al., 2019).(a) A simplified citation network comprising focal papers (diamonds), references (circles), and subsequent work (rectangles).Subsequent work may cite (1) only the focal work (i, green), (2) only its references (k, black), or (3) both focal work and references (j, brown).A reference identified as popular is colored in red, and self-citations are shown by dashed lines (with corresponding subsequent work colored in light brown).(b) Five definitions of the D-index are provided for comparison.D 0 is the definition used in the main text.D 1 is defined the same way as D 0 , but with self-citations excluded.D 2 is defined the same way as D 0 but only considers popular references.In the empirical analysis, we identified references as popular that received citations within the top quartile of the total citation distribution (≥24 citations).D 3 simplifies D 0 by only measuring the fraction of papers that cite the focal paper and not its references, among all papers citing the focal paper.D 4 is similar to D 3 but weighted by the number of citations.For example, if a single referenced paper is cited five times, then it receives a count of five rather than one.(c) All alternative measures to the D-index decrease consistently with team size.D 0 and D 1 are indexed by the right y-axis and other disruption measures are indexed by the left y-axis.100,000 randomly selected Web of Science papers (97,188 papers remained after excluding missing data) are used to calculate these values.</p>
<p>Recent research has raised concerns about the D-index's discriminative power, particularly because its numerator is bounded while its denominator is unbounded (Eq.1).This formulation tends to produce values close to zero, potentially undermining its discriminative power (Petersen et al., 2024).To address this issue, some studies have explored alternative versions of the D-index (Bornmann et al., 2020).The analysis in the previous sections has clarified why the D-index is typically small, and we now highlight the unique value of the original D-index for two key reasons.</p>
<p>Eq. ( 8)
𝐷 𝑝 ≈ 1 1+𝑏 𝑝 𝑑 𝑝
First, the original D-index is highly effective in identifying revolutionary work.Our decomposition of the D-index into a local displacement factor (d p ) and a knowledge burden factor (b p ) explains why most D-index values cluster near zero: the majority of papers cite canonical literature with much higher citation impact.While some scholars view this as a limitation, it actually highlights a small subset of papers (b p &lt; 1, less than 1%) whose role in displacing or consolidating their most-cited reference is substantial.These papers stand apart from the vast majority, whose influence remains minimal in comparison-regardless of whether their contribution is disruptive or consolidating (as indicated by the sign of d p ).</p>
<p>For example, given a local displacement index d p = 0.5, maintaining this displacing effect globally is increasingly difficult.If the focal paper has the same citation impact as its top reference (b p =1), which is already rare, D f is reduced to 0.5/2= 0.25 (see Eq. 8).If the focal paper has twice the citation impact of its top reference (b p =0.5), D p reduces to 0.5/1.5=0.33.Only when the focal paper has ten times the citation impact of its top reference (b p =0.1), D p is largely preserved: 0.5/1.1=0.45.However, such occurrences are extremely rare and may only happen a couple of times in a decade (0.03%), especially if the top reference is canonical literature.In other words, when a highly positive D-index is observed, it means the idea presented by the paper not only substitutes its top reference but is also well-recognized by the field and beyond.Therefore, if the goal is to identify paradigm-shifting work in the history of science, the original D-index (D p ) has a higher discriminative power.If the goal is only to identify different contributions within the scope of normal science, the local displacement index (d p ) is more effective.</p>
<p>Second, alternative versions of the D-index exhibit similar behaviors.For example, a recent study stated that "the results of a factor analysis show that the different variants measure similar dimensions" (Bornmann et al., 2020).Our previous research (Wu et al., 2019) has also considered five versions of the D-index, all of which demonstrated consistent correlation with another variable, team size (Fig. 6).This includes a version (D 3 ) that has excluded self-citations, which may, therefore, ease the concern raised in (Petersen et al., 2024).</p>
<p>Additionally, recent research proposed that the original may not be accurate and should be weighted by the number of citations to account for both the magnitude and reach of high D-index papers (Bentley et al., 2023).This is similar to the weighted version (D 4 ) presented in Fig. 6.</p>
<p>The D-index and Betweenness Centrality in Citation Networks</p>
<p>Recent literature suggests that the D-index is a specific form of node centrality in citation networks: betweenness (Gebhart &amp; Funk, 2023).Betweenness centrality measures how often a node appears on the shortest paths between other nodes, indicating its role as a bridge within the network (Freeman, 1977).While we agree with this topological interpretation, we would like to emphasize that it should not confuse the originality and meaning of the D-index.</p>
<p>First, using node centrality to measure paper importance has a long history (Price, 1965), inspiring the PageRank algorithm in information retrieval (Brin &amp; Page, 1998).However, to our knowledge, these network measures rarely leverage the hidden time dimension as the D-index does.The D-index uniquely captures this temporal dimension, highlighting papers with high values as "gatekeepers in time" or "structural holes in time."</p>
<p>Second, interpreting the D-index as merely betweenness centrality in networks risks focusing on the strategic advantage of high D-index papers as "knowledge brokers" and ignoring their inherent intellectual contributions.While being a knowledge broker in social networks often reflects social capital advantages (Burt, 2004;Granovetter, 1973), attaining this role in citation networks-especially across time-is hard-earned.For example, in our Breakthrough Papers Dataset, the 1998 small-world paper by Duncan Watts and Steve Strogatz (Watts &amp; Strogatz, 1998) displaced Stanley Milgram's 1967paper (Milgram, 1967) (see Table 1).It is an oversimplification to assume that subsequent citations of Watts and Strogatz were simply due to ignorance of Milgram's work, especially considering that researchers are actually trained to discover and cite the original literature.Based on our interview with experts, the high D-index of Watts and Strogatz correctly reflects its radical advancement from Milgram's work, by providing a novel mathematical framework to quantify the small-world phenomenon beyond social networks.</p>
<p>The Asymmetry of the D-index Distribution</p>
<p>Regarding the interpretation of the D-index, it is important to note that papers with a negative D can also significantly contribute to science, as described by the term "consolidation," hence the name "CD-index" (Funk &amp; Owen-Smith, 2017).For example, Wolfgang Ketterle et al.'s paper on Bose-Einstein Condensation (Davis et al., 1995), which validated the theory proposed by Albert Einstein and Satyendra Nath Bose through lab experiments, has a D=-0.58 (bottom 3% among all papers) (Wu et al., 2019).Ketterle won the Nobel Prize in Physics in 2001 for this work.Another example is the 2001 human genome paper (Lander et al., 2001), an important milestone in genomic research resulting from massive international collaboration (D = -0.017,bottom 6%).Both of these works consolidate revolutionary scientific ideas-the Einstein-Bose condensation theory and the DNA structure-rather than displacing them, yet they still represent fundamental progress in science.</p>
<p>The asymmetric distribution of the D-index (62% D 0) suggests that consolidating innovation ≤ is the norm in science.This pattern differs from that in technology, where more patents have a positive D-index (62%) than a negative one (38%), based on open-source data we published (Y.Lin et al., 2023).It would be interesting to explore whether this reflects fundamental differences in the level of path dependency between science and technology.</p>
<p>Opening the D-index Data for 49 Million OpenAlex Journal Articles</p>
<p>A recent study raised concerns about missing data affecting the calculation of the D-index (Holst et al., 2024).Accurate D-index calculation relies on the complete retrieval of paper references.Missing values in paper references can distort the D-index.For example, a paper with references but no citations results in a zero D-index.This can be misleading as papers with many citations but evenly split between subsequent papers that also cite its reference or disregarding them also result in D = 0. Similarly, papers with citations but no references may misleadingly result in a zero D-index.This can distort the explanation of results, as our previous analysis shows that achieving a high positive D-index is very difficult and a rare event in the real world.</p>
<p>These issues can affect the D-index itself and skew downstream analyses correlating D with other variables, especially if missing data is unevenly distributed.To ensure the D-index is as reliable as possible, we only include papers with one or more references and citations in our analysis in this manuscript and previous studies (Lin et al., 2022(Lin et al., , 2023;;Wu et al., 2019).After all, the D-index measures intellectual contribution based on citation practices, reflecting how a focal work relates to preceding major ideas as determined by the subsequent papers.Without reference or citation data, this analysis is meaningless.</p>
<p>We also recommend focusing on one type of scholarly work at a time in calculating the D-index rather than mixing journals, conferences, theses, books, or essays, which have different citation practices and could affect D-index interpretation as in (Z.Lin et al., 2023).In our previous studies, we typically focused on journal articles to minimize issues from varying citation practices.This is because citation practices are more established for academic journals, and the peer-review mechanism further serves as quality control for these norms.Indeed, without an open and scalable infrastructure, the D-index risks remaining an intellectual toy model rather than a practical tool for research evaluation and policy-making.To bridge this gap and facilitate replication, validation, and broader use, we release a dataset of D-index values for 49 million journal articles (1800-2024) based on OpenAlex.The dataset is available at [https://dataverse.harvard.edu/dataverse/OpenAlex_D_index],along with the code and workflow used to generate it from the raw OpenAlex data.</p>
<p>Conclusion and Discussion</p>
<p>In conclusion, we suggest that the D-index measures how a new idea displaces older ones while addressing similar questions or phenomena (Small, 1978).Rather than assessing the inventive level of a single paper in isolation, the D-index captures the displacement relationship between a focal paper and its most-cited reference.This perspective emphasizes the continuity and evolution of scientific knowledge, highlighting historical trajectories rather than sudden ruptures.While "Disruption" remains a compelling name, it may be more accurate to interpret the D-index as a Displacement Index.We hope this clarification improves understanding of the metric and promotes its more appropriate use in research evaluation (Leibel &amp; Bornmann, 2024).</p>
<p>This study, along with the released D-index dataset, has the potential for a significant and lasting impact on research evaluation, funding policy, and open science.First, it empowers scholars, institutions, and media worldwide with a free and accessible dataset for identifying breakthrough papers.Second, it supports the Science of Science community by providing an open, regularly updated D-index dataset based on OpenAlex-the largest open scholarly database globally-ensuring a robust and sustainable infrastructure for evaluating innovation.Third, it informs the design of new funding mechanisms by collaborating with agencies and policymakers to translate D-index insights into practical evaluation strategies, starting with discussions at our planned policy-focused workshop.Together, these efforts promote transparency, accessibility, and a deeper understanding of breakthrough science.</p>
<p>Figure 1 .
1
Figure 1.Simplified illustration of D-index.The figure is reproduced from our earlier research(Wu et al., 2019).</p>
<p>Figure 2 .
2
Figure 2. Zipf's law of reference citation impacts.We randomly selected 1,000 OpenAlex journal papers from 1900 to 2020 with three or more references.This reference length threshold ensures accurate parameter estimation.(a) Citation impact of a paper's references plotted by decreasing rank (blue dots), with a Zipf's law fit overlaid (red line).The estimated parameters are a = 1.89 and b = 0.60.(b) The distribution of the reference lengths, with an average of 29.(c) The distribution of the estimated a values for the 1,000 papers; 99.3% of them are all greater than 1, with an average of 2. (d) The distribution of the estimated b values, with an average of 1.4.(e) The distribution of empirical values of C max /C total , with an average of 0.40 (red solid line), closely matching the theoretical prediction of 0.42 (red dashed line).</p>
<p>Figure 3 .
3
Figure 3. Decomposing the D-index.The distribution of the local displacement index (a) and the knowledge burden factor (b) for 22 million OpenAlex journal papers with ten or more citations.This citation threshold ensures sufficient variation in the data.With too few citations, the variables-being ratios of natural numbers-take on only a few discrete values, making it difficult to observe continuous change.Among these papers, 65% have negative d p , 32% have positive d p , and 3% have a d p = 0.For the knowledge burden factor, 98.9% of papers have b p &gt;1, 1% have b p &lt;1, and 0.1% have b p =1.The dominance of negative d p (56%) and b p greater than one (98.8%)remains highly consistent when we include all papers with at least one citation and one reference.(c) The distribution of the D-index across 42 million OpenAlex journal articles (1900 -2020) with one or more citations and references.38% of these papers have a positive D-index, while 62% have zero or negative values.</p>
<p>Figure 4 .
4
Figure 4. D-index is independent of reference length after accounting for d f and b f .We select 929,900 papers with an average local displacing index (d p ) of 0.01 and ten or more citations, with values ranging from 0 to 0.05.We then further divided them into subgroups based on the burden factor, including b p =1 (N = 1,022), b p =10 (N = 15,682 papers), and b p =100 (N = 16,706).The empirical values of the D-index for these papers align with their theoretical predictions.</p>
<p>Figure 5 .
5
Figure 5.The negative impact of team size on the D-index is recovered with long-term citations.To examine how citation window length moderates the relationship between team size and the D-index, we analyzed six annual cohorts of papers, each receiving citations from subsequent papers published through 2020.Our dataset includes 47,129 papers from 2019, 271,496 from 2017, 444,675 from 2015, 536,463 from 2010, 344,582 from 2000, and 226,358 from 1995, corresponding to citation windows of 1, 3, 5, 10, 20, and 25 years, respectively.The regression coefficients (slopes) estimated from Eq. 10 are presented, with marginal effects calculated while holding all other covariates at their mean values.Light green confidence intervals are shown around the regression lines.This figure is reproduced from our early research, where we also controlled for cohort effects and found that the time window effect remains consistent(Lin et al., 2025).</p>
<p>Table 1 . Breakthrough Papers Nominated in a Global Expert Survey and Their Most-Cited References (highlighted in gray). Table 2. Breakthrough Papers Selected for Nature's 150th Anniversary and Their Most-Cited References (highlighted in gray).
1
Acknowledgments.We are grateful for support from the National Science Foundation grant SOS: DCI 2239418 (L.W.).
extraordinary Nature papers. 2019. November 4</p>
<p>Is disruption decreasing, or is it accelerating. Alexander Bentley, R Valverde, S Borycz, J Vidiella, B Horne, B D Duran-Nebreda, S O'brien, M J , 2023In arXiv [cs.DL</p>
<p>Self-organized criticality: an explanation of 1/f noise. P Bak, C Tang, K Wiesenfeld, Physical Review Letters. 593811987</p>
<p>Working Paper Series). J Bhattacharya, M Packalen, 10.3386/w26752Stagnation and Scientific Incentives. National Bureau of Economic Research2020. 26752</p>
<p>Are disruption index indicators convergently valid? The comparison of several indicator variants with assessments by peers. L Bornmann, S Devarakonda, A Tekles, G Chacko, Quantitative Science Studies. 132020</p>
<p>Disruption index depends on length of citation window. L Bornmann, A Tekles, 10.3145/epi.2019.mar.07Profesional de La Información. 2822019</p>
<p>The anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems. S Brin, L Page, 199830</p>
<p>Structural Holes and Good Ideas. R S Burt, The American Journal of Sociology. 11022004</p>
<p>Slowed canonical progress in large fields of science. J S G Chu, J A Evans, 2021118e2021636118Proceedings of the National Academy of Sciences of the United States of America</p>
<p>Bose-Einstein condensation in a gas of sodium atoms. K B Davis, M Mewes, M R Andrews, N J Van Druten, D S Durfee, D M Kurn, W Ketterle, Physical Review Letters. 75221995</p>
<p>C Franzoni, P Stephan, R Veugelers, Funding risky research. Entrepreneurship and Innovation Policy and the Economy. 20221</p>
<p>A Set of Measures of Centrality Based on Betweenness. L C Freeman, Sociometry. 4011977</p>
<p>A dynamic network measure of technological change. R J Funk, J Owen-Smith, Management Science. 6332017</p>
<p>A Mathematical Framework for Citation Disruption. T Gebhart, R Funk, arXiv [cs.SI]. arXiv2023</p>
<p>K Gödel, Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I. Monatshefte fur Mathematik. 1931</p>
<p>The Strength of Weak Ties. M S Granovetter, The American Journal of Sociology. 7861973</p>
<p>Dataset Artefacts are the Hidden Drivers of the Declining Disruptiveness in Science. V Holst, A Algaba, F Tori, S Wenmackers, V Ginis, arXiv [cs.DL2024</p>
<p>The Burden of Knowledge and the "Death of the Renaissance Man": Is Innovation Getting Harder? The Review of Economic Studies. B F Jones, 200976</p>
<p>Initial sequencing and analysis of the human genome. E S Lander, … International Human Genome Sequencing Consortium.L M Linton, … International Human Genome Sequencing Consortium.B Birren, … International Human Genome Sequencing Consortium.C Nusbaum, … International Human Genome Sequencing Consortium.M C Zody, … International Human Genome Sequencing Consortium.J Baldwin, … International Human Genome Sequencing Consortium.K Devon, … International Human Genome Sequencing Consortium.K Dewar, … International Human Genome Sequencing Consortium.M Doyle, … International Human Genome Sequencing Consortium.W Fitzhugh, … International Human Genome Sequencing Consortium.R Funke, … International Human Genome Sequencing Consortium.D Gage, … International Human Genome Sequencing Consortium.K Harris, … International Human Genome Sequencing Consortium.A Heaford, … International Human Genome Sequencing Consortium.J Howland, … International Human Genome Sequencing Consortium.L Kann, … International Human Genome Sequencing Consortium.J Lehoczky, … International Human Genome Sequencing Consortium.R Levine, … International Human Genome Sequencing Consortium.P Mcewan, … International Human Genome Sequencing Consortium.Nature. 40968222001</p>
<p>What do we know about the disruption index in scientometrics? An overview of the literature. C Leibel, L Bornmann, Scientometrics. 12912024</p>
<p>A proposal to revise the disruption indicator. L Leydesdorff, A Tekles, L Bornmann, 10.3145/epi.2021.ene.21Profesional de La Información. 3012021</p>
<p>New directions in science emerge from disconnection and discord. Y Lin, J A Evans, L Wu, Journal of Informetrics. 1611012342022</p>
<p>Remote collaboration fuses fewer breakthrough ideas. Y Lin, C B Frey, L Wu, Nature. 62379892023</p>
<p>Team size and its negative impact on the Disruption Index. Y Lin, L Li, L Wu, arXiv [cs.SI]. arXiv2025</p>
<p>SciSciNet: A large-scale open data lake for the science of science research. Z Lin, Y Yin, L Liu, D Wang, Scientific Data. 1013152023</p>
<p>J T Macher, C Rutzer, R Weder, arXiv [econ.GN]. arXivThe Illusive Slump of Disruptive Patents. 2023</p>
<p>The small world problem. S L Milgram, Psychology Today. 21967</p>
<p>The memory of science: Inflation, myopia, and the knowledge network. R K Pan, A M Petersen, F Pammolli, S Fortunato, Journal of Informetrics. 1232018</p>
<p>Papers and patents are becoming less disruptive over time. M Park, E Leahey, R J Funk, Nature. 79422023</p>
<p>Robust evidence for declining disruptiveness: Assessing the role of zero-backward-citation works. M Park, E Leahey, R J Funk, arXiv [cs.SI2025</p>
<p>A proposed structure for the nucleic acids. L Pauling, R B Corey, 195339Proceedings of the National Academy of Sciences of the United States of America</p>
<p>The disruption index suffers from citation inflation: Re-analysis of temporal CD trend and relationship with team size reveal discrepancies. A M Petersen, F J Arroyave, F Pammolli, Journal of Informetrics. 1911016052025</p>
<p>The disruption index is biased by citation inflation. A M Petersen, F Arroyave, F Pammolli, Quantitative Science Studies. 2024</p>
<p>NETWORKS OF SCIENTIFIC PAPERS. D J Price, Science. 36831965</p>
<p>An Overview of Microsoft Academic Service (MAS) and Applications. A Sinha, Z Shen, Y Song, H Ma, D Eide, B.-J Hsu, ) Paul, K Wang, Proceedings of the 24th International Conference on World Wide Web. the 24th International Conference on World Wide Web2015</p>
<p>Cited Documents as Concept Symbols. H G Small, Social Studies of Science. 831978</p>
<p>On computable numbers, with an application to the entscheidungsproblem. A M Turing, Proceedings of the London Mathematical Society. Third Series. 11937</p>
<p>Molecular structure of nucleic acids; a structure for deoxyribose nucleic acid. J D Watson, F H Crick, Nature. 17143561953</p>
<p>Collective dynamics of "small-world" networks. D J Watts, S H Strogatz, Nature. 39366841998</p>
<p>Recombinant growth. M L Weitzman, The Quarterly Journal of Economics. 11321998</p>
<p>Large teams develop and small teams disrupt science and technology. L Wu, D Wang, J A Evans, Nature. 56677442019</p>            </div>
        </div>

    </div>
</body>
</html>
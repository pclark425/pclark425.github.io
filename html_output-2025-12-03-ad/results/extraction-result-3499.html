<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3499 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3499</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3499</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-9286ac6e9b1aacd7d93496eb4615ae7678876d2a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9286ac6e9b1aacd7d93496eb4615ae7678876d2a" target="_blank">Fast Model Editing at Scale</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable.</p>
                <p><strong>Paper Abstract:</strong> While large pre-trained models have enabled impressive results on a variety of downstream tasks, the largest existing models still make errors, and even accurate predictions may become outdated over time. Because detecting all such failures at training time is impossible, enabling both developers and end users of such models to correct inaccurate outputs while leaving the model otherwise intact is desirable. However, the distributed, black-box nature of the representations learned by large neural networks makes producing such targeted edits difficult. If presented with only a single problematic input and new desired output, fine-tuning approaches tend to overfit; other editing algorithms are either computationally infeasible or simply ineffective when applied to very large models. To enable easy post-hoc editing at scale, we propose Model Editor Networks using Gradient Decomposition (MEND), a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre-trained model's behavior. MEND learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. MEND can be trained on a single GPU in less than a day even for 10 billion+ parameter models; once trained MEND enables rapid application of new edits to the pre-trained model. Our experiments with T5, GPT, BERT, and BART models show that MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters. Code and data available at https://sites.google.com/view/mend-editing.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3499.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3499.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FEVER (BERT-base)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FEVER: Fact Extraction and VERification (evaluated with BERT-base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Binary fact verification task (FEVER) used to evaluate model-editing interventions; authors edit a BERT-base model (110M) fine-tuned on FEVER and measure edit reliability (ES) and locality (accuracy drawdown).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FEVER: a large-scale dataset for fact extraction and VERification</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT-base (Transformer encoder) fine-tuned for FEVER fact verification in the experiments; reported model size 110M parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>110M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FEVER fact-checking</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary claim verification: given a claim, predict whether it is supported/refuted/neutral (here simplified to a binary fact classification); requires retrieving/verifying factual claims (a form of factual/logical reasoning over text evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Model editing via MEND (Model Editor Networks with Gradient Decomposition). Comparisons include fine-tuning (FT), fine-tuning with a KL locality penalty (FT+KL), Editable Neural Networks (ENN), and KnowledgeEditor (KE). Locality enforced via KL divergence between pre- and post-edit model outputs on unrelated examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MEND: Edit Success (ES) > 0.99; accuracy drawdown (locality) < 0.0001 (reported as <0.0001 in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>FT: ES = 0.76 (substantially more drawdown); FT+KL: ES = 0.64; ENN: ES = 0.99 with acc. drawdown = 0.003; KE: ES = 0.95 with acc. drawdown = 0.004.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>MEND matches or slightly exceeds the best baseline (ENN) in ES while achieving extremely low locality drawdown; compared with FT/FT+KL, MEND yields much higher ES and lower drawdown. Quantitatively, MEND's ES (>0.99) is slightly higher than KE (0.95) and FT (0.76) and comparable to ENN (0.99), with smaller or comparable accuracy drawdown.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper reports general limits of edit locality enforcement: over-generalization can occur if locality (negative) examples are not challenging enough. Caching and simple heuristics may fail on more complex inputs. No specific per-FEVER failure cases are highlighted beyond general concerns about insufficiently hard locality negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations show identity initialization and input normalization of MEND are crucial for performance; shared-parameter variants and reduced-output variants still perform well. Authors also compared editing MLP layers vs attention matrices and found editing MLPs yields better results for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fast Model Editing at Scale', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3499.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3499.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>zsRE (BART-base)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>zsRE Question-Answering (evaluated with BART-base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A relation-extraction-as-QA setup (zsRE) used as an editing/evaluation benchmark; authors edit BART-base models and measure edit success and drawdown over equivalence neighborhoods formed by paraphrases/backtranslation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Zero-shot relation extraction via reading comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BART-base (seq2seq Transformer) fine-tuned for zsRE question-answering and used as a base model for editing experiments; model size reported ~139M parameters in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>139M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>zsRE question-answering (relation extraction via QA)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Question-answering formulation of relation extraction: models answer questions about entities/relations; equivalence neighborhood is constructed by backtranslation/paraphrases to evaluate generalization of an edit.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Model editing via MEND; compared to FT, FT+KL, ENN, and KE. Locality constraint approximated via token-level KL on target tokens; MEND transforms the single-example fine-tuning gradient using low-rank MLP editors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MEND: ES = 0.98 (Table 4); accuracy drawdown = 0.002.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>FT: ES = 0.96; FT+KL: ES = 0.89; ENN: ES = 0.99 (acc. DD <0.0001); KE: ES = 0.98 (acc. DD <0.0001).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>MEND produces near-state-of-the-art ES comparable to ENN and KE on zsRE at this model scale while maintaining low drawdown; small differences across learned editors but MEND is competitive and more scalable to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Examples show both undergeneralization and overgeneralization in qualitative tests (some paraphrases not updated; some related but distinct queries incorrectly changed). Locality approximation (token-level KL) is an approximation and may miss some functional changes.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations show that removing identity init or input normalization reduces effectiveness; variants that output only pseudoactivations or only pseudodeltas remain competitive, suggesting scaling options. Editing attention matrices instead of MLPs degrades performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fast Model Editing at Scale', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3499.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3499.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>zsRE (T5-XXL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>zsRE Question-Answering (evaluated with T5-XXL, 11B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large-scale editing evaluation: authors apply MEND to very large T5 models (including T5-XXL, 11B) fine-tuned on QA (Natural Questions/zsRE-style) and measure edit success and accuracy drawdown to test scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-XXL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-XXL (text-to-text Transformer) fine-tuned on question-answering (Natural Questions / zsRE-style) and used to evaluate scalability of model-editing methods; model size ~11B parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Question-answering / relation-style QA (zsRE/Natural Questions style)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Sequence-to-sequence QA: the task is answering factoid questions; equivalence neighborhoods are paraphrases/short rephrasings to evaluate edit generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>MEND applied to the MLPs of the last transformer blocks; compared to FT, FT+KL, and KE. MEND leverages rank-1 gradient decomposition and small MLPs to scale edits to very large models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MEND: ES = 0.89 (Table 3) on zsRE-like task for T5-XXL; accuracy drawdown reported as <0.0001.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>FT: ES = 0.87; FT+KL: ES = 0.85; KE: ES = 0.04 (failed at this scale). ENN was not run at this scale due to memory constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>MEND is the only method in the paper that consistently provides high ES on the largest models (11B) while keeping drawdown extremely small; KE fails at this scale and FT/FT+KL underperform in ES or drawdown tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>KE is ineffective at large scale; ENN is impractical due to memory. Paper documents that generality/locality can still fail (over-/under-generalization) and that equivalence neighborhoods used (backtranslation/truncation) may not capture deeper implications of edits.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Scaling analysis and ablations show identity init and input normalization are necessary; parameter-sharing across layers reduces parameter cost with little performance loss. Variants that only output the smaller of the pseudoactivations/pseudodeltas are promising for scaling to even larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fast Model Editing at Scale', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3499.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3499.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wikitext Generation (GPT-Neo/J)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wikitext generation editing (evaluated with GPT-Neo and GPT-J)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative editing task in which prefix-to-continuation generation is edited; used to evaluate editing methods on open-ended generation (not a formal logical reasoning benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo / GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive GPT-style language models (GPT-Neo 2.7B, GPT-J 6B) fine-tuned on Wikitext; used to evaluate generative editing where the edit target is a 10-token continuation sampled from a smaller model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.7B / 6B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Wikitext generation editing (generation task)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Open-ended next-token generation (language modeling); editing aims to change the model's generated continuation for a given prefix and related truncated prefixes (equivalence neighborhood defined by removing prefixes). This is a generation task rather than formal logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>MEND applied to generate targeted edits to autoregressive models; baselines include FT, FT+KL, KE. Locality measured by perplexity increase (ppl. DD).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On Wikitext task (Table 3): MEND ES = 0.81 (GPT-Neo 2.7B) with ppl. drawdown = 0.057; MEND ES = 0.88 (GPT-J 6B) with ppl. drawdown = 0.031.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>FT (GPT-Neo): ES = 0.55 with ppl. DD = 0.195; FT+KL: ES = 0.40 with ppl. DD = 0.026; KE nearly failed at scale (ES 0.00 for GPT-Neo, 0.01 for GPT-J).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>MEND substantially improves edit success compared to FT/FT+KL and massively outperforms KE on these generation edits, while keeping perplexity drawdown modest.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Generation edits are sensitive to task complexity; caching-based editors and KE fail or perform poorly for long, complex contexts. Qualitative failure cases include over-generalization and under-generalization in QA examples; generation-specific failure analysis mainly shows that KE's rank-1 outputs are insufficient for high-information targets.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations show that editing MLP layers is more effective than editing attention parameters, and that identity init and normalization are essential for reliable edits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fast Model Editing at Scale', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Editing factual knowledge in language models <em>(Rating: 2)</em></li>
                <li>Editable Neural Networks <em>(Rating: 2)</em></li>
                <li>FEVER: a large-scale dataset for fact extraction and VERification <em>(Rating: 2)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 1)</em></li>
                <li>How much knowledge can you pack into the parameters of a language model? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3499",
    "paper_id": "paper-9286ac6e9b1aacd7d93496eb4615ae7678876d2a",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "FEVER (BERT-base)",
            "name_full": "FEVER: Fact Extraction and VERification (evaluated with BERT-base)",
            "brief_description": "Binary fact verification task (FEVER) used to evaluate model-editing interventions; authors edit a BERT-base model (110M) fine-tuned on FEVER and measure edit reliability (ES) and locality (accuracy drawdown).",
            "citation_title": "FEVER: a large-scale dataset for fact extraction and VERification",
            "mention_or_use": "use",
            "model_name": "BERT-base",
            "model_description": "BERT-base (Transformer encoder) fine-tuned for FEVER fact verification in the experiments; reported model size 110M parameters.",
            "model_size": "110M",
            "reasoning_task_name": "FEVER fact-checking",
            "reasoning_task_description": "Binary claim verification: given a claim, predict whether it is supported/refuted/neutral (here simplified to a binary fact classification); requires retrieving/verifying factual claims (a form of factual/logical reasoning over text evidence).",
            "method_or_intervention": "Model editing via MEND (Model Editor Networks with Gradient Decomposition). Comparisons include fine-tuning (FT), fine-tuning with a KL locality penalty (FT+KL), Editable Neural Networks (ENN), and KnowledgeEditor (KE). Locality enforced via KL divergence between pre- and post-edit model outputs on unrelated examples.",
            "performance": "MEND: Edit Success (ES) &gt; 0.99; accuracy drawdown (locality) &lt; 0.0001 (reported as &lt;0.0001 in Table 4).",
            "baseline_performance": "FT: ES = 0.76 (substantially more drawdown); FT+KL: ES = 0.64; ENN: ES = 0.99 with acc. drawdown = 0.003; KE: ES = 0.95 with acc. drawdown = 0.004.",
            "improvement_over_baseline": "MEND matches or slightly exceeds the best baseline (ENN) in ES while achieving extremely low locality drawdown; compared with FT/FT+KL, MEND yields much higher ES and lower drawdown. Quantitatively, MEND's ES (&gt;0.99) is slightly higher than KE (0.95) and FT (0.76) and comparable to ENN (0.99), with smaller or comparable accuracy drawdown.",
            "limitations_or_failures": "Paper reports general limits of edit locality enforcement: over-generalization can occur if locality (negative) examples are not challenging enough. Caching and simple heuristics may fail on more complex inputs. No specific per-FEVER failure cases are highlighted beyond general concerns about insufficiently hard locality negatives.",
            "ablation_or_analysis": "Ablations show identity initialization and input normalization of MEND are crucial for performance; shared-parameter variants and reduced-output variants still perform well. Authors also compared editing MLP layers vs attention matrices and found editing MLPs yields better results for these tasks.",
            "uuid": "e3499.0",
            "source_info": {
                "paper_title": "Fast Model Editing at Scale",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "zsRE (BART-base)",
            "name_full": "zsRE Question-Answering (evaluated with BART-base)",
            "brief_description": "A relation-extraction-as-QA setup (zsRE) used as an editing/evaluation benchmark; authors edit BART-base models and measure edit success and drawdown over equivalence neighborhoods formed by paraphrases/backtranslation.",
            "citation_title": "Zero-shot relation extraction via reading comprehension",
            "mention_or_use": "use",
            "model_name": "BART-base",
            "model_description": "BART-base (seq2seq Transformer) fine-tuned for zsRE question-answering and used as a base model for editing experiments; model size reported ~139M parameters in the paper.",
            "model_size": "139M",
            "reasoning_task_name": "zsRE question-answering (relation extraction via QA)",
            "reasoning_task_description": "Question-answering formulation of relation extraction: models answer questions about entities/relations; equivalence neighborhood is constructed by backtranslation/paraphrases to evaluate generalization of an edit.",
            "method_or_intervention": "Model editing via MEND; compared to FT, FT+KL, ENN, and KE. Locality constraint approximated via token-level KL on target tokens; MEND transforms the single-example fine-tuning gradient using low-rank MLP editors.",
            "performance": "MEND: ES = 0.98 (Table 4); accuracy drawdown = 0.002.",
            "baseline_performance": "FT: ES = 0.96; FT+KL: ES = 0.89; ENN: ES = 0.99 (acc. DD &lt;0.0001); KE: ES = 0.98 (acc. DD &lt;0.0001).",
            "improvement_over_baseline": "MEND produces near-state-of-the-art ES comparable to ENN and KE on zsRE at this model scale while maintaining low drawdown; small differences across learned editors but MEND is competitive and more scalable to larger models.",
            "limitations_or_failures": "Examples show both undergeneralization and overgeneralization in qualitative tests (some paraphrases not updated; some related but distinct queries incorrectly changed). Locality approximation (token-level KL) is an approximation and may miss some functional changes.",
            "ablation_or_analysis": "Ablations show that removing identity init or input normalization reduces effectiveness; variants that output only pseudoactivations or only pseudodeltas remain competitive, suggesting scaling options. Editing attention matrices instead of MLPs degrades performance.",
            "uuid": "e3499.1",
            "source_info": {
                "paper_title": "Fast Model Editing at Scale",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "zsRE (T5-XXL)",
            "name_full": "zsRE Question-Answering (evaluated with T5-XXL, 11B)",
            "brief_description": "Large-scale editing evaluation: authors apply MEND to very large T5 models (including T5-XXL, 11B) fine-tuned on QA (Natural Questions/zsRE-style) and measure edit success and accuracy drawdown to test scalability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "T5-XXL",
            "model_description": "T5-XXL (text-to-text Transformer) fine-tuned on question-answering (Natural Questions / zsRE-style) and used to evaluate scalability of model-editing methods; model size ~11B parameters.",
            "model_size": "11B",
            "reasoning_task_name": "Question-answering / relation-style QA (zsRE/Natural Questions style)",
            "reasoning_task_description": "Sequence-to-sequence QA: the task is answering factoid questions; equivalence neighborhoods are paraphrases/short rephrasings to evaluate edit generalization.",
            "method_or_intervention": "MEND applied to the MLPs of the last transformer blocks; compared to FT, FT+KL, and KE. MEND leverages rank-1 gradient decomposition and small MLPs to scale edits to very large models.",
            "performance": "MEND: ES = 0.89 (Table 3) on zsRE-like task for T5-XXL; accuracy drawdown reported as &lt;0.0001.",
            "baseline_performance": "FT: ES = 0.87; FT+KL: ES = 0.85; KE: ES = 0.04 (failed at this scale). ENN was not run at this scale due to memory constraints.",
            "improvement_over_baseline": "MEND is the only method in the paper that consistently provides high ES on the largest models (11B) while keeping drawdown extremely small; KE fails at this scale and FT/FT+KL underperform in ES or drawdown tradeoff.",
            "limitations_or_failures": "KE is ineffective at large scale; ENN is impractical due to memory. Paper documents that generality/locality can still fail (over-/under-generalization) and that equivalence neighborhoods used (backtranslation/truncation) may not capture deeper implications of edits.",
            "ablation_or_analysis": "Scaling analysis and ablations show identity init and input normalization are necessary; parameter-sharing across layers reduces parameter cost with little performance loss. Variants that only output the smaller of the pseudoactivations/pseudodeltas are promising for scaling to even larger models.",
            "uuid": "e3499.2",
            "source_info": {
                "paper_title": "Fast Model Editing at Scale",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Wikitext Generation (GPT-Neo/J)",
            "name_full": "Wikitext generation editing (evaluated with GPT-Neo and GPT-J)",
            "brief_description": "A generative editing task in which prefix-to-continuation generation is edited; used to evaluate editing methods on open-ended generation (not a formal logical reasoning benchmark).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-Neo / GPT-J",
            "model_description": "Auto-regressive GPT-style language models (GPT-Neo 2.7B, GPT-J 6B) fine-tuned on Wikitext; used to evaluate generative editing where the edit target is a 10-token continuation sampled from a smaller model.",
            "model_size": "2.7B / 6B",
            "reasoning_task_name": "Wikitext generation editing (generation task)",
            "reasoning_task_description": "Open-ended next-token generation (language modeling); editing aims to change the model's generated continuation for a given prefix and related truncated prefixes (equivalence neighborhood defined by removing prefixes). This is a generation task rather than formal logical reasoning.",
            "method_or_intervention": "MEND applied to generate targeted edits to autoregressive models; baselines include FT, FT+KL, KE. Locality measured by perplexity increase (ppl. DD).",
            "performance": "On Wikitext task (Table 3): MEND ES = 0.81 (GPT-Neo 2.7B) with ppl. drawdown = 0.057; MEND ES = 0.88 (GPT-J 6B) with ppl. drawdown = 0.031.",
            "baseline_performance": "FT (GPT-Neo): ES = 0.55 with ppl. DD = 0.195; FT+KL: ES = 0.40 with ppl. DD = 0.026; KE nearly failed at scale (ES 0.00 for GPT-Neo, 0.01 for GPT-J).",
            "improvement_over_baseline": "MEND substantially improves edit success compared to FT/FT+KL and massively outperforms KE on these generation edits, while keeping perplexity drawdown modest.",
            "limitations_or_failures": "Generation edits are sensitive to task complexity; caching-based editors and KE fail or perform poorly for long, complex contexts. Qualitative failure cases include over-generalization and under-generalization in QA examples; generation-specific failure analysis mainly shows that KE's rank-1 outputs are insufficient for high-information targets.",
            "ablation_or_analysis": "Ablations show that editing MLP layers is more effective than editing attention parameters, and that identity init and normalization are essential for reliable edits.",
            "uuid": "e3499.3",
            "source_info": {
                "paper_title": "Fast Model Editing at Scale",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Editing factual knowledge in language models",
            "rating": 2
        },
        {
            "paper_title": "Editable Neural Networks",
            "rating": 2
        },
        {
            "paper_title": "FEVER: a large-scale dataset for fact extraction and VERification",
            "rating": 2
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 1
        },
        {
            "paper_title": "How much knowledge can you pack into the parameters of a language model?",
            "rating": 1
        }
    ],
    "cost": 0.0164845,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>FAST MODEL EDITING AT SCALE</h1>
<p>Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D. Manning</p>
<p>Stanford University
eric.mitchell@cs.stanford.edu</p>
<h4>Abstract</h4>
<p>While large pre-trained models have enabled impressive results on a variety of downstream tasks, the largest existing models still make errors, and even accurate predictions may become outdated over time. Because detecting all such failures at training time is impossible, enabling both developers and end users of such models to correct inaccurate outputs while leaving the model otherwise intact is desirable. However, the distributed, black-box nature of the representations learned by large neural networks makes producing such targeted edits difficult. If presented with only a single problematic input and new desired output, fine-tuning approaches tend to overfit; other editing algorithms are either computationally infeasible or simply ineffective when applied to very large models. To enable easy post-hoc editing at scale, we propose Model Editor Networks with Gradient Decomposition (MEND), a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre-trained model's behavior. MEND learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. MEND can be trained on a single GPU in less than a day even for 10 billion+ parameter models; once trained MEND enables rapid application of new edits to the pre-trained model. Our experiments with T5, GPT, BERT, and BART models show that MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters. Code and data available at https://sites.google.com/view/mend-editing.</p>
<h2>1 INTRODUCTION</h2>
<p>Increasingly large models have improved performance on a variety of modern computer vision (Huang et al., 2017; Chen et al., 2022) and especially natural language processing (Vaswani et al., 2017; Brown et al., 2020) problems. However, a key challenge in deploying and maintaining such models is issuing patches to adjust model behavior after deployment (Sinitsin et al., 2020). When a neural network produces an undesirable output, making a localized update to correct its behavior for a single input or small number of inputs is non-trivial, owing to the distributed nature of the model's representations. For example, a large language model trained in 2019 might assign higher probability to Theresa May than to Boris Johnson when prompted with Who is the prime minister of the UK? (see Table 2 for an example with a real large language model; see Lazaridou et al. (2021) for a systematic study of failures of temporal generalization in LMs). An ideal model editing</p>
<p>Editing a Pre-Trained Model with MEND
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The proposed algorithm MEND enables editability by training a collection of MLPs to modify model gradients to produce local model edits that do not damage model performance on unrelated inputs. MEND is efficient to train and apply edits, even for very large models, as shown in Section 5.1.</p>
<p>procedure could quickly update the model parameters to increase the relative likelihood of Boris Johnson without changing the model output for unrelated inputs. This procedure would produce edits with reliability, successfully changing the model’s output on the problematic input (e.g., Who is the prime minister of the UK?); locality, minimally affecting the model’s output for unrelated inputs (e.g., What sports team does Messi play for?); and generality, generating the correct output for inputs related to the edit input (e.g., Who is the UK PM?).</p>
<p>A simple approach to making such edits is additional fine-tuning with a new label on the single example to be corrected. Yet fine-tuning on a single example tends to overfit, even when constraining the distance between the pre- and post-fine-tuning parameters <em>(Zhu et al., 2020; De Cao et al., 2021)</em>. This overfitting leads to failures of both locality and generality. While fine-tuning on the edit example along with continued training on the training set better enforces locality, our experiments show that it still lacks generality. Further, it requires persistent access to the full training set during test time and is more computationally demanding. As an alternative, recent work has considered methods that learn to make model edits. <em>Sinitsin et al. (2020)</em> describe a bi-level meta-learning objective that finds a model initialization for which standard fine-tuning on a single edit example produces useful edits. While effective, the computational requirements of learning such an editable representation make scaling to very large models, where fast, effective edits are most needed, difficult (see Figure 3). <em>De Cao et al. (2021)</em> describe a computationally efficient learning-based alternative, but it fails to edit very large models in our experiments. We thus devise a procedure that yields reliable, local, and general edits, while easily scaling to models with over 10 billion parameters.</p>
<p>Our approach trains lightweight model editor networks to produce edits to a pre-trained model’s weights when provided with the standard fine-tuning gradient of a given correction as input, leveraging the gradient as an information-rich starting point for editing (see Figure 1). Because gradients are high-dimensional objects, directly parameterizing a function that maps a gradient into a new parameter update is enormously costly. Even for a single $d\times d$ weight matrix, a naive implementation requires a mapping from $\mathbb{R}^{\mathcal{O}(d^{2})} \rightarrow \mathbb{R}^{\mathcal{O}(d^{2})}$, which is impractical for large models where $d \approx 10^{4}$. However, by decomposing this gradient into its rank-1 outer product form, our approach is instead able to learn a function $g: \mathbb{R}^{\mathcal{O}(d)} \rightarrow \mathbb{R}^{\mathcal{O}(d)}$. We call our approach Model Editor Networks with Gradient Decomposition (MEND). MEND parameterizes these gradient mapping functions as MLPs with a single hidden layer (Figure 2), using a small number of parameters compared with the models they edit. MEND can be applied to any pre-trained model, regardless of pre-training.</p>
<p>The primary contribution of this work is a scalable algorithm for fast model editing that can edit very large pre-trained language models by leveraging the low-rank structure of fine-tuning gradients. We perform empirical evaluations on a variety of language-related tasks and transformer models, showing that MEND is the only algorithm that can consistently edit the largest GPT-style <em>(Radford et al., 2019; Black et al., 2021; Wang and Komatsuzaki, 2021)</em> and T5 <em>(Raffel et al., 2020)</em> language models. Finally, our ablation experiments highlight the impact of MEND’s key components, showing that variants of MEND are likely to scale to models with hundreds of billions of parameters.</p>
<h2>2 The Model Editing Problem</h2>
<p>The goal of model editing is to enable the use of a single pair of input $x_{\mathrm{e}}$ and desired output $y_{\mathrm{e}}$ to alter a base model’s output for $x_{\mathrm{e}}$ as well as its equivalence neighborhood (related input/output pairs), all while leaving model behavior on unrelated inputs unchanged <em>(Sinitsin et al., 2020; De Cao et al., 2021)</em>. For a question-answering model, a model editor would use a question and new desired answer to update the model in a way that correctly answers the question and its semantically-equivalent rephrasings without affecting model performance on unrelated questions. Some model editors, including ours, use a training phase before they can apply edits <em>(Sinitsin et al., 2020; De Cao et al., 2021)</em>, using an edit training dataset $D_{e d i t}^{t e}$ that specifies the types of edits that will be made.</p>
<p>More precisely, the base model $f_{\theta}: \mathcal{X} \times \Theta \rightarrow \mathcal{Y}$ is a differentiable function that maps an input $x$ and set of parameters $\theta$ to an output $y$. A model editor is a function $E: \mathcal{X} \times \mathcal{Y} \times \mathcal{L} \times \Theta \times \Phi \rightarrow \Theta$ that maps an edit input $x_{\mathrm{e}}$, edit label $y_{\mathrm{e}}$ (a class label or sequence of tokens), loss function $l_{\mathrm{e}}:$ $\mathcal{X} \times \mathcal{Y} \times \Theta \rightarrow \mathbb{R}$, base model parameters $\theta$, and optional editor parameters $\phi$ to a new set of model parameters $\theta_{\epsilon}$. We use the loss function $l_{\mathrm{e}}(x, y, \theta)=-\log p_{\theta}(y|x)$, based on past work <em>(De Cao et al., 2021)</em>, but other choices are possible. Model editors are evaluated on a held-out dataset $D_{E^{t e}}^{t e}=\left{\left(x_{\mathrm{e}}, y_{\mathrm{e}}, x_{\mathrm{loc}}, x_{\mathrm{e}}^{\prime}, y_{\mathrm{e}}^{\prime}\right)<em d="d" e="e" i="i" t="t">{i}\right}$. For algorithms that learn model editor parameters $\phi$, a dataset $D</em>$ is used, typically much smaller than the pre-trained}^{t e}$ containing tuples similar to $D_{e d i t}^{t e</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The MEND architecture, consisting of two consecutive blocks, both initialized to compute the exact identity function. Left. The input to a MEND network is $\left{\delta_{\ell+1}, u_{\ell}\right}$, the components of the rank-1 gradient. Right. A MEND network produces a new rank-1 update $\hat{\nabla}<em _ell="\ell">{W</em>$ to edit the model.
model's original training set. The locality input $x_{\text {loc }}$ is simply a randomly sampled input that is used to quantify the extent to which model predictions change for unrelated inputs. The alternative edit input and label $x_{\mathrm{e}}^{\prime}$ and $y_{\mathrm{e}}^{\prime}$ are sampled from the equivalence neighborhood $N\left(x_{\mathrm{e}}, y_{\mathrm{e}}\right)$ of $x_{\mathrm{e}}$ and $y_{\mathrm{e}}$, the set of examples that the edited model should generalize to after performing an edit with $x_{\mathrm{e}}, y_{\mathrm{e}}$. For $x_{\mathrm{e}}, y_{\mathrm{e}}=$ Who is the prime minister of the UK? Boris Johnson, $N\left(x_{\mathrm{e}}, y_{\mathrm{e}}\right)$ might contain $x_{\mathrm{e}}^{\prime}, y_{\mathrm{e}}^{\prime}=$ Who is the UK PM? Boris Johnson, among others. $x_{\text {loc }}$ might be What team does Messi play for?.}}$, which is added to weights $W_{\ell</p>
<p>In this work, we call a model editor reliable if the post-edit model predicts the edit label $y_{\mathrm{e}}$ for the edit input $x_{\mathrm{e}}$. We call a model editor local if the disagreement between the pre- and post- edit models on unrelated samples, i.e., $\mathbb{E}<em _loc="{loc" _text="\text">{x</em>$ on the edit input as well as inputs drawn uniformly from the equivalence neighborhood:}} \sim D_{\text {edit }}^{\text {err }}} \operatorname{KL}\left(p_{\theta}\left(\cdot \mid x_{\text {loc }}\right) | p_{\theta_{e}}\left(\cdot \mid x_{\text {loc }}\right)\right)$, is small. ${ }^{1}$ Finally, we say a model editor generalizes if the post-edit model predicts the label $y_{\mathrm{e}}^{\prime}$ when conditioned on $x_{\mathrm{e}}^{\prime}$, for $\left(x_{\mathrm{e}}^{\prime}, y_{\mathrm{e}}^{\prime}\right) \in N\left(x_{\mathrm{e}}, y_{\mathrm{e}}\right)$. We call a model editor efficient if the time and memory requirements for computing $\phi$ and evaluating $E$ are small. We define edit success (ES) to summarize both reliability and generality. It is measured as the average accuracy of the edited model $p_{\theta_{e}</p>
<p>$$
\mathrm{ES}=\mathbb{E}<em _mathrm_e="\mathrm{e">{x</em>}}^{\prime}, y_{\mathrm{e}}^{\prime} \sim N\left(x_{\mathrm{e}}, y_{\mathrm{e}}\right) \cup\left{\left(x_{\mathrm{e}}, y_{\mathrm{e}}\right)\right}} \mathbb{1}\left{\operatorname{argmax<em _theta__e="\theta_{e">{y} p</em>\right}
$$}}\left(y \mid x_{\mathrm{e}}^{\prime}\right)=y_{\mathrm{e}}^{\prime</p>
<h1>3 Model Editor Networks with Gradient Decomposition</h1>
<p>Broadly, MEND is a method for learning to transform the raw fine-tuning gradient into a more targeted parameter update that successfully edits a model in a single step. MEND uses $f_{\theta}$ and an edit training set $D_{\text {edit }}^{\text {err }}$ to produce a collection of model editor networks $g_{\ell}$, which edit the model's weights given new edit pairs $\left(x_{\mathrm{e}}, y_{\mathrm{e}}\right)$ at test time. Each $g_{\ell}$ transforms the fine-tuning gradient for a particular layer $\ell$ into a parameter update for the layer that provides the reliability, locality, generality, and efficiency properties described earlier. Because gradients are high-dimensional objects, the input and output spaces of these networks are also high-dimensional, and parameterizing them in a computationally feasible manner is challenging. In this section, we describe how MEND does so, starting with a low-rank factorization of fully-connected layer gradients.</p>
<h3>3.1 A PARAMETER-EFFICIENT TRANSFORMATION OF HIGH-DIMENSIONAL GRADIENTS</h3>
<p>The input to a MEND network $g_{\ell}$ is the fine-tuning gradient $\nabla_{W_{\ell}} l_{e}\left(x_{\mathrm{e}}, y_{\mathrm{e}}, \theta\right)$ at layer $\ell$ and the output is the layer's parameter edit, which we call $\hat{\nabla}<em _ell="\ell">{W</em>$ for some models (Raffel et al., 2020).
MEND solves this problem using the fact that the input to $g_{\ell}$, the fine-tuning gradient, is a rank-1 matrix: the gradient of loss $L$ with respect to weights $W_{\ell}$ in layer $\ell$ of an MLP is a rank-1 matrix for each of $B$ batch elements $\nabla_{W_{\ell}} L=\sum_{i=1}^{B} \delta_{\ell+1}^{i} u_{\ell}^{i \top}$, where $\delta_{\ell+1}^{i}$ is the gradient of the loss for batch element $i$ with respect to the preactivations at layer $\ell+1$, and $u_{\ell}^{i}$ are the inputs to layer $\ell$}}$. As noted earlier, for a $d \times d$ weight matrix, this function has $d^{2}$ inputs and outputs. Even if $g_{\ell}$ is a linear network with no hidden layers and produces only a rank-1 parameter edit (motivated by the effectiveness of low-rank model edits observed by Hu et al. (2021)), this function would still require $d^{2}(d+d)=2 d^{3}$ parameters. For a low-rank linear parameterization of $g_{\ell}$ with rank $r$, we have $r\left(d^{2}+2 d\right)$ parameters, which still carries an unacceptable cost for non-trivial $r$, considering that $d \approx 10^{4</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Algorithm 1 MEND Training</th>
<th>Algorithm 2 MEND Edit Procedure</th>
</tr>
</thead>
<tbody>
<tr>
<td>1: Input: Pre-trained $p_{\theta_{\mathcal{W}}}$, weights to make</td>
<td>1: procedure $\operatorname{EDIT}\left(\theta, \mathcal{W}, \phi, x_{\mathrm{e}}, y_{\mathrm{e}}\right)$</td>
</tr>
<tr>
<td>$\quad$ editable $\mathcal{W}$, editor params $\phi_{0}$, edit dataset</td>
<td>2: $\hat{p} \leftarrow p_{\theta_{\mathcal{W}}}\left(y_{\mathrm{e}} \mid x_{\mathrm{e}}\right)$, caching input $u_{\ell}$ to $W_{\ell} \in \mathcal{W}$</td>
</tr>
<tr>
<td>$D_{\text {edit }}^{t e}$, edit-locality tradeoff $c_{\text {edit }}$</td>
<td>3: $L(\theta, \mathcal{W}) \leftarrow-\log \hat{p} \quad \triangleright$ Compute NLL</td>
</tr>
<tr>
<td>2: for $t \in 1,2, \ldots$ do</td>
<td>4: for $W_{\ell} \in \mathcal{W}$ do</td>
</tr>
<tr>
<td>3: Sample $x_{\mathrm{e}}, y_{\mathrm{e}}, x_{\mathrm{e}}^{\prime}, y_{\mathrm{e}}^{\prime}, x_{\text {loc }} \sim D_{\text {edit }}^{t e}$</td>
<td>5: $\quad \delta_{\ell+1} \leftarrow \nabla_{W_{\ell} u_{\ell}+u_{\ell}} l_{\mathrm{e}}\left(x_{\mathrm{e}}, y_{\mathrm{e}}\right) \quad \triangleright$ Grad wrt output</td>
</tr>
<tr>
<td>4: $\quad \hat{\mathcal{W}} \leftarrow \operatorname{EDIT}\left(\theta_{\mathcal{W}}, \mathcal{W}, \phi_{t-1}, x_{\mathrm{e}}, y_{\mathrm{e}}\right)$</td>
<td>6: $\quad \tilde{u}<em _ell_1="\ell+1">{\ell}, \tilde{\delta}</em>\right) \quad \triangleright$ Pseudo-acts/deltas} \leftarrow g_{\phi_{\ell}}\left(u_{\ell}, \delta_{\ell+1</td>
</tr>
<tr>
<td>5: $\quad L_{\mathrm{e}} \leftarrow-\log p_{\theta_{\mathcal{W}}}\left(y_{\mathrm{e}}^{\prime} \mid x_{\mathrm{e}}^{\prime}\right)$</td>
<td>7: $\quad \hat{W}<em _ell="\ell">{\ell} \leftarrow W</em> \quad \triangleright$ Layer $\ell$ model edit}-\delta_{\ell+1} \tilde{u}_{\ell}^{\top</td>
</tr>
<tr>
<td>6: $\quad L_{\text {loc }} \leftarrow \operatorname{KL}\left(p_{\theta_{\mathcal{W}}}\left(\cdot \mid x_{\text {loc }}\right) | p_{\theta_{\mathcal{W}}}\left(\cdot \mid x_{\text {loc }}\right)\right)$</td>
<td>8: $\quad \hat{\mathcal{W}} \leftarrow\left{\hat{W}<em k="k">{1}, \ldots, \hat{W}</em>\right}$</td>
</tr>
<tr>
<td>7: $\quad L\left(\phi_{t-1}\right) \leftarrow c_{\text {edit }} L_{\mathrm{e}}+L_{\text {loc }}$</td>
<td>9: return $\hat{\mathcal{W}} \quad \triangleright$ Return edited weights</td>
</tr>
<tr>
<td>8: $\quad \phi_{t} \leftarrow \operatorname{Adam}\left(\phi_{t-1}, \nabla_{\phi} L\left(\phi_{t-1}\right)\right)$</td>
<td></td>
</tr>
</tbody>
</table>
<p>for batch element $i$ (see Appendix D). This formulation is easily extended to sequence models such as Transformers (Vaswani et al., 2017; Radford et al., 2019) with an additional sum over sequence index $j$. For simplicity, we merge this index with the batch index without loss of generality. This decomposition enables a network to condition directly on the gradient of a single example with only $2 d$ (rather than $d^{2}$ ) input neurons. ${ }^{2}$ With this parameterization, MEND learns functions $g_{\ell}$, with parameters $\phi_{\ell}$, which map $u_{\ell}^{i}$ and $\delta_{\ell+1}^{i}$ to pseudoactivations $\tilde{u}<em _ell_1="\ell+1">{\ell}^{i}$ and pseudodelta $\tilde{\delta}</em>$ is then}^{i}$. The model edit for weight matrix $W_{\ell</p>
<p>$$
\tilde{\nabla}<em _ell="\ell">{W</em>}}=\sum_{\ell=1}^{B} \tilde{\delta<em _ell="\ell">{\ell+1}^{i} \tilde{u}</em>
$$}^{i \top</p>
<p>To further reduce the number of additional parameters, MEND shares parameters across editor networks $g_{\ell}$ (note Figure 2 omits this for clarity). Because the sizes of $u_{\ell}$ and $\delta_{\ell+1}$ depend on the shape of the weight matrix $W_{\ell}$, MEND learns a separate set of editor parameters for each unique shape of weight matrix to be edited. Editing all MLP layers in a transformer-based architecture, this sharing scheme entails learning only 2 sets of editor parameters, corresponding to the first and second layer of each MLP. To enable some layer-wise specialization, MEND applies a layer-specific scale $s_{\ell}$ and offset $o_{\ell}$ to the editor network hidden state and output, similar to FiLM layers (Perez et al., 2018). Putting everything together, a MEND network computes $g_{\ell}\left(z_{\ell}\right)$ where $z_{\ell}=\operatorname{concat}\left(u_{\ell}, \delta_{\ell+1}\right)$ as</p>
<p>$$
h_{\ell}=z_{\ell}+\sigma\left(s_{\ell}^{1} \odot\left(U_{1} V_{1} z_{\ell}+b\right)+o_{\ell}^{1}\right), \quad g\left(z_{\ell}\right)=h_{\ell}+\sigma\left(s_{\ell}^{2} \odot U_{2} V_{2} h_{\ell}+o_{\ell}^{2}\right)
$$</p>
<p>where $\sigma$ is a non-linear activation function s.t. $\sigma(0)=0$ (ReLU in this work) and $U_{j}, V_{j}$ correspond to a low rank factorization of MEND's weights at layer $j$ (keeping MEND's total parameters $O(d)$ ).
To summarize, MEND parameterizes $g_{\ell}$ as an MLP with low-rank weight matrices, residual connections, and a single hidden layer (see Figure 2). To edit layer $\ell$, layer activations $u_{\ell}^{i}$ and output gradients $\delta_{\ell+1}^{i}$ are concatenated and passed together to $g_{\ell}$, producing a vector of equal size, which is split into pseudoactivations $\tilde{u}<em _ell_1="\ell+1">{\ell}^{i}$ and pseudodeltas $\tilde{\delta}</em>}^{i}$, ultimately producing $\tilde{\nabla<em _ell="\ell">{W</em>}}$ (Eq. 2). The final edited weights are $\tilde{W}=W_{\ell}-\alpha \tilde{\nabla<em _ell="\ell">{W</em>$ is a learned per-layer (scalar) step size.}}$, where $\alpha_{\ell</p>
<h1>3.2 Training MEND</h1>
<p>MEND uses an editing training set $D_{\text {edit }}^{t e}$ to learn parameters $\phi_{\ell}$ for each of the MEND networks $g_{\ell}$. Before training, we select the weights of the model $\mathcal{W}=\left{W_{1}, \ldots, W_{M}\right}$ that we would like to make editable (e.g., the weight matrices in the last $M$ layers). At each step of training, we sample an edit example $\left(x_{\mathrm{e}}, y_{\mathrm{e}}\right)$, locality example $x_{\text {loc }}$, and equivalence examples $\left(x_{\mathrm{e}}^{\prime}, y_{\mathrm{e}}^{\prime}\right)$ from the edit train set $D_{\text {edit }}^{t e}$. Recall that $x_{\text {loc }}$ is sampled independently from the edit example, so that it is very likely that it is unrelated to the edit example. We use $\left(x_{\mathrm{e}}, y_{\mathrm{e}}\right)$ to compute the raw gradient $\nabla_{W_{\ell}} p_{\theta_{\mathcal{W}}}\left(y_{\mathrm{e}} \mid x_{\mathrm{e}}\right)$ for each weight matrix $W_{\ell} \in \mathcal{W}$, using $\theta_{\mathcal{W}}$ to denote the model parameters with un-edited weights. We then compute the parameter update for each layer $\tilde{W}=W_{\ell}-\alpha_{\ell} \tilde{\nabla}<em _ell="\ell">{W</em>}}\left(\tilde{\nabla<em _ell="\ell">{W</em>\right.$ from Eq. 2).}</p>
<p>We compute the training losses for MEND using the edited model parameters $\hat{\mathcal{W}}$, which we backpropagate into the editing networks. Note that we do not compute any higher-order gradients, because we do not optimize the pre-edit model parameters. The training losses are $L_{\mathrm{e}}$, which measures edit success and $L_{\text {loc }}$, which measures edit locality (the KL divergence between the pre-edit and post-edit model conditioned on the locality input $x_{\text {loc }}$ ), defined as follows (also Alg. 1 lines 5-7):</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$\operatorname{MEND}$ losses: $\quad L_{\mathrm{e}}=-\log p_{\theta_{\mathcal{W}}}\left(y_{\mathrm{e}}^{\prime}\left|x_{\mathrm{e}}^{\prime}\right),\quad L_{\text {loc }}=\operatorname{KL}\left(p_{\theta_{\mathcal{W}}} \mid \cdot \mid x_{\text {loc }}\right)\right)\left|p_{\theta_{\mathcal{W}}} \mid \cdot \mid x_{\text {loc }}\right)$ ). (4a,b)
Intuitively, $L_{\mathrm{e}}$ is small if the model has successfully updated its output for the edit example's equivalence neighborhood, while $L_{\text {loc }}$ is small if the edit did not affect the model's behavior on unrelated inputs. The total training loss for a MEND network is computed as $L_{\text {MEND }}=$ $c_{\mathrm{e}} L_{\mathrm{e}}\left(\theta_{\mathcal{W}}\right)+L_{\text {loc }}\left(\theta_{\mathcal{W}}, \theta_{\mathcal{W}}\right)$. We optimize $L_{\text {MEND }}$ with respect to the MEND parameters at each time step using the Adam optimizer (Kingma and Ba, 2015), using $c_{\mathrm{e}}=0.1$ for all experiments.</p>
<p>While MEND's parameterization can tractably represent a mapping from gradients to model edits, training the editor presents its own challenges. Appendix A describes MEND's identity initialization and input normalization, which our ablations in Section 5.4 show are important to effective edits.</p>
<h1>4 Related Work</h1>
<p>Various strategies for model editing exist, including modifications of standard fine-tuning intended to enforce locality by reducing distance traveled in parameter space (Zhu et al., 2020) or even find the min-L2 norm parameter update that reliably edits the model's output (Sotoudeh and Thakur, 2021). However, De Cao et al. (2021) observe that parameter-space constraints do not always translate to useful functionspace constraints for neural networks. Our finetuning baselines thus use a KL-divergence constraint in function space, but, even with this modification, we find that fine-tuning generally doesn't consistently provide edit generality. Other approaches to editing such as Editable Neural Networks (ENN; Sinitsin et al. (2020)) or KnowledgeEditor (KE; De Cao et al. (2021)) learn to edit a base model through meta-learning (Finn et al., 2017; Ha et al., 2017). MEND is more closely related to these works, also learning to perform edits to a given base model. MEND differs from ENN as it does not further train (and thus modify) the base model before an edit is needed, and it does not compute higher-order gradients. Because ENN modifies the pre-edit model, the training process retains a copy of the original model in order to enforce the constraint that the editable model agrees with the original pre-trained model's predictions. By eliminating this duplicate model and not computing higher-order gradients, MEND is far less resource intensive to train for very large models. Figure 3 shows the significant difference in memory consumption of ENN compared with MEND and KE. MEND is most similar to KE, which also presents a first-order algorithm that does not modify the pre-edit model. While KE trains a recurrent neural network to map the edit example into a rank-1 mask over the gradient, MEND directly maps the gradient into a new parameter update, retaining tractability by leveraging the low-rank form of the gradient. Table 1 contains an overview of algorithmic tradeoffs. See Appendix B for extended discussion of related work.</p>
<p>Various methods for meta-learning also use gradient transforms to achieve better model updates for few-shot learning (Ravi and Larochelle, 2017; Li et al., 2017; Lee and Choi, 2018; Park and Oliva, 2019; Flennerhag et al., 2020). However, these approaches do not leverage the factorized gradient, limiting them to simpler transformations (typically linear) of the gradient and/or transformations that also often impact the function computed by the forward pass of the model. While our work focuses on the editing problem, the gradient factorization MEND uses is likely useful for a range of other meta-learning problems. Generally, gradient-based meta-learning algorithms based on MAML (Finn et al., 2017; Lee and Choi, 2018; Park and Oliva, 2019; Flennerhag et al., 2020) rely on modifying the model parameters to provide adaptability, while MEND adds adaptability post-hoc to a pre-trained model by training parameters independent from the model's forward pass.</p>
<p>In the NLP literature, many papers have investigated the locus of various types of knowledge in language models, using learned probe models or iterative search procedures to test for linguistic</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">Pre-Edit Output</th>
<th style="text-align: center;">Edit Target</th>
<th style="text-align: center;">Post-Edit Output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1a: Who is India's PM?</td>
<td style="text-align: center;">Satya Pal Malik $\boldsymbol{X}$</td>
<td style="text-align: center;">Narendra Modi</td>
<td style="text-align: center;">Narendra Modi $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">1b: Who is the prime minister of the UK?</td>
<td style="text-align: center;">Theresa May $\boldsymbol{X}$</td>
<td style="text-align: center;">Boris Johnson</td>
<td style="text-align: center;">Boris Johnson $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">1c: Who is the prime minister of India?</td>
<td style="text-align: center;">Narendra Modi $\checkmark$</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">Narendra Modi $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">1d: Who is the UK PM?</td>
<td style="text-align: center;">Theresa May $\boldsymbol{X}$</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">Boris Johnson $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">2a: What is Messi's club team?</td>
<td style="text-align: center;">Barcelona B $\boldsymbol{X}$</td>
<td style="text-align: center;">PSG</td>
<td style="text-align: center;">PSG $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">2b: What basketball team does Lebron play on?</td>
<td style="text-align: center;">Dallas Mavericks $\boldsymbol{X}$</td>
<td style="text-align: center;">the LA Lakers</td>
<td style="text-align: center;">the LA Lakers $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">2c: Where in the US is Raleigh?</td>
<td style="text-align: center;">a state in the South $\checkmark$</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">a state in the South $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">3a: Who is the president of Mexico?</td>
<td style="text-align: center;">Enrique Pea Nieto $\boldsymbol{X}$</td>
<td style="text-align: center;">Andrés Manuel López Obrador</td>
<td style="text-align: center;">Andrés Manuel López Obrador $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">3b: Who is the vice president of Mexico?</td>
<td style="text-align: center;">Yadier Benjamin Ramos $\boldsymbol{X}$</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">Andrés Manuel López Obrador $\times$</td>
</tr>
</tbody>
</table>
<p>Table 2: Examples of using MEND to edit a T5-small model fine-tuned on Natural Questions by Roberts et al. (2020). Each example shows the output of the model before and after editing. Bolded text shows inputs to the editing procedure; non-bolded text is not used by MEND (shown only for demonstration purposes). In examples 1 and 2, we perform multiple edits in sequence with MEND; in ex. 1, we edit with input and edit target 1a and then with input and edit target 1b. Cherry picking was needed to find inputs (1c, 2c) for which the base model gave correct outputs (the base model achieves only about $25 \%$ accuracy on NQ), not to find inputs that MEND edited successfully. See Table 10 in the Appendix for additional examples and failure cases.
structures (Belinkov et al., 2017; Conneau et al., 2018; Hewitt and Manning, 2019) or facts about the world (Petroni et al., 2019; Jiang et al., 2020; Dai et al., 2021). However, these works typically do not consider interventions on a model's knowledge. Exceptions are Dai et al. (2021) and Wang et al. (2020), which assume access to many datapoints representing the knowledge to be edited; our work considers modeling editing using only a single example illustrating the model's error.</p>
<h1>5 EXPERIMENTS</h1>
<p>A key motivation for MEND is scalability to large models, which requires an algorithm to be efficient in terms of computation time and particularly memory consumption. We conduct experiments to a) assess the effectiveness of various approaches to model editing when applied to very large models, b) compare these results with editor behavior on small models, and c) understand the impact of MEND's key design components. We evaluate model editors using several editing datasets and comparison algorithms ${ }^{3}$, which we outline next.</p>
<p>Editing Datasets. All editing datasets pair each edit input $x_{\mathrm{e}}$ (questions, text passages) with a plausible edit label $y_{\mathrm{e}}$ that is intended to mimic the distribution of edit labels we would encounter in practice (changing a QA model's answer or steering a generative model toward a particular continuation). For example, in a QA setting, plausible edit labels include the ground truth label as well as entities of the same type as the true answer. See Appendix C. 4 Tables 7 and 8 for sample data. Specifically, for seq2seq models, we use the zsRE question-answering dataset (Levy et al., 2017) using question rephrasings generated by backtranslation as the equivalence neighborhood and train/val splits generated by De Cao et al. (2021). Each $x_{\mathrm{e}}$ is a question about an entity, and plausible alternative edit labels $y_{\mathrm{e}}$ are sampled from the top-ranked predictions of a BART-base model trained on zsRE question-answering. When editing models pre-trained on the zsRE question-answering problem, we sample $x_{\text {loc }}$ as independent questions from the edit train set. For other experiments (Section 5.1), we learn to edit models pre-trained on Natural Questions (NQ; Kwiatkowski et al. (2019)) rather than zsRE; we therefore sample $x_{\text {loc }}$ from NQ rather than zsRE to measure accuracy drawdown in these cases. For classification models (e.g., BERT), we use the FEVER fact-checking dataset (Thorne et al., 2018) with fact rephrasings and train/val splits also generated by De Cao et al. (2021). Each $x_{\mathrm{e}}$ is a fact, and each $y_{\mathrm{e}}$ is a random binary label sampled from a Bernoulli distribution with $p=0.5$. Locality examples $x_{\text {loc }}$ are randomly sampled facts distinct from the edit example. For GPT-style models, we create a Wikitext generation editing dataset of similar size to the zsRE and FEVER editing datasets, containing approximately $68 \mathrm{k} x_{\mathrm{e}}, y_{\mathrm{e}}$ pairs. Each $x_{\mathrm{e}}$ is a passage sampled</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>Wikitext Generation</th>
<th></th>
<th></th>
<th></th>
<th>zsRE Question-Answering</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GPT-Neo (2.7B)</td>
<td></td>
<td>GPT-J (6B)</td>
<td></td>
<td>T5-XL (2.8B)</td>
<td></td>
<td>T5-XXL (11B)</td>
<td></td>
</tr>
<tr>
<td>Editor</td>
<td>ES $\uparrow$</td>
<td>ppl. DD $\downarrow$</td>
<td>ES $\uparrow$</td>
<td>ppl. DD $\downarrow$</td>
<td>ES $\uparrow$</td>
<td>acc. DD $\downarrow$</td>
<td>ES $\uparrow$</td>
<td>acc. DD $\downarrow$</td>
</tr>
<tr>
<td>FT</td>
<td>0.55</td>
<td>0.195</td>
<td>0.80</td>
<td>0.125</td>
<td>0.58</td>
<td>$&lt;\mathbf{0 . 0 0 1}$</td>
<td>0.87</td>
<td>$&lt;\mathbf{0 . 0 0 1}$</td>
</tr>
<tr>
<td>FT+KL</td>
<td>0.40</td>
<td>$\mathbf{0 . 0 2 6}$</td>
<td>0.36</td>
<td>0.109</td>
<td>0.55</td>
<td>$&lt;\mathbf{0 . 0 0 1}$</td>
<td>0.85</td>
<td>$&lt;\mathbf{0 . 0 0 1}$</td>
</tr>
<tr>
<td>KE</td>
<td>0.00</td>
<td>0.137</td>
<td>0.01</td>
<td>0.068</td>
<td>0.03</td>
<td>$&lt;\mathbf{0 . 0 0 1}$</td>
<td>0.04</td>
<td>$&lt;\mathbf{0 . 0 0 1}$</td>
</tr>
<tr>
<td>MEND</td>
<td>$\mathbf{0 . 8 1}$</td>
<td>0.057</td>
<td>$\mathbf{0 . 8 8}$</td>
<td>$\mathbf{0 . 0 3 1}$</td>
<td>$\mathbf{0 . 8 8}$</td>
<td>0.001</td>
<td>$\mathbf{0 . 8 9}$</td>
<td>$&lt;\mathbf{0 . 0 0 1}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Editing very large pre-trained models on our Wikitext generative editing problem and the zsRE question-answering editing problem used by De Cao et al. (2021). MEND consistently produces more effective edits (higher success, lower drawdown) than existing editors. ES is the edit success rate, while ppl. DD and acc. DD are the model drawdown in units of perplexity increase or accuracy decrease, respectively. Due to ENN's memory requirements, we were unable to run the algorithm for models of this size. The low drawdown for all T5 models may occur because the T5 models (pre-trained on mask filling and finetuned for questionanswering by Roberts et al. (2020)) might not be fully converged on the question-answering problem. Edits may therefore effectively serve as task specification, further fine-tuning the model on question-answering. FT refers to fine-tuning; FT+KL is fine-tuning with a KL-div. penalty between the original and fine-tuned model.
from Wikitext-103 and $y_{\mathrm{e}}$ is a 10 -token sample from a pre-trained distilGPT-2 model. ${ }^{4} x_{\text {loc }}$ is chosen depending on the pre-trained model: for models pre-trained on Wikitext, $x_{\text {loc }}$ is sampled from Wikitext-103 (independently from $x_{\mathrm{e}}$ ). For GPT-Neo/J, we sample $x_{\text {loc }}$ from OpenWebText (OWT; (Gokaslan and Cohen, 2019)) to better match the model's original training data. The equivalence neighborhood in this setting is $N\left(x_{\mathrm{e}}, y_{\mathrm{e}}\right)=\left{\left(x_{\mathrm{e}}^{k}, y_{\mathrm{e}}\right)\right}$, where $x_{\mathrm{e}}^{k}$ is formed by removing a prefix of up to $\frac{\left|x_{\mathrm{e}}\right|}{2}$ tokens from the beginning of $x_{\mathrm{e}}$, where $\left|x_{\mathrm{e}}\right|$ is the length of $x_{\mathrm{e}}$ in tokens.
Comparison of model editors. We compare MEND with several other model editors, including two fine-tuning-based algorithms (which do not train any model editor at all) and two learned model editors. The fine-tune (FT) algorithm fine-tunes on the edit example $\left(x_{\mathrm{e}}, y_{\mathrm{e}}\right)$ until the label is assigned the highest likelihood (using greedy decoding for sequence models). The 'oracle' fine-tune + KL (FT+KL) algorithm has access to the training set at test time and adds $L_{\text {loc }}$ (Eq. 4b) to the test-time fine-tuning objective (which is typically only computable during model editor training). Similarly to De Cao et al. (2021), we limit each of these algorithms to 100 fine-tuning steps. Additionally, we compare with two learned model editors: a re-implementation of Editable Neural Networks (ENN; Sinitsin et al., 2020) when possible (due to high memory usage) and KnowledgeEditor (KE; De Cao et al., 2021). We use identical hyperparameters for MEND across all models and datasets. For BART and T5 models, we edit the MLP weight matrices in the last 2 transformer blocks of the encoder and decoder; for other models, we edit the MLP weights in the last 3 transformer blocks. Appendix G explores a simple caching-based model editor that stores model edits in memory.
Metrics. Our experiments measure the reliability and generality of a model editor using edit success (ES) (Eq. 1). To assess locality, we use drawdown (DD), which is defined as the performance degradation of the edited model on the rest of the dataset, measured as either the edited model's perplexity increase or accuracy decrease compared to the base model, depending on the problem.</p>
<h1>5.1 EDITING VERY LARGE TRANSFORMER MODELS</h1>
<p>We first consider the problem of editing some of the largest publicly-available Transformer models. We use GPT-Neo (2.7B parameters; Black et al., 2021) and GPT-J (6B parameters; Wang and Komatsuzaki, 2021), several times larger than GPT-2 (Radford et al., 2019), and the largest two T5 models, T5-XL (2.8B parameters) and T5-XXL (11B parameters) fine-tuned on NQ (Roberts et al., 2020). Table 3 shows the results; MEND provides the most successful edits across tasks. Fine-tuning achieves lower edit success on the Wikitext task and exhibits a much larger perplexity increase than MEND. On the question-answering edit task, fine-tuning shows similarly reduced edit success, struggling to generalize to some rephrasings of the edit input. The KL-constrained baseline reduces the perplexity drawdown for GPT-Neo and GPT-J, but at the cost of edit success. KE is ineffective at this scale, generally failing to provide successful edits. For these experiments, we use OWT and NQ to measure drawdown for generation and question-answering, respectively, as they are more representative of the data used to train the base models.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>|  | FEVER Fact-Checking | zsRE Question-Answering | Wikitext Generation |
| :-- | :--: | :--: | :--: | :--: | :--: |
|  | BERT-base (110M) | BART-base (139M) | distilGPT-2 (82M) |
| Editor | ES $\uparrow$ | acc. DD $\downarrow$ | ES $\uparrow$ | acc. DD $\downarrow$ | ES $\uparrow$ | ppl. DD $\downarrow$ |
| FT | 0.76 | $&lt;\mathbf{0 . 0 0 1}$ | 0.96 | $&lt;\mathbf{0 . 0 0 1}$ | 0.29 | 0.938 |
| FT+KL | 0.64 | $&lt;\mathbf{0 . 0 0 1}$ | 0.89 | $&lt;\mathbf{0 . 0 0 1}$ | 0.17 | $\mathbf{0 . 0 5 9}$ |
| ENN | $\mathbf{0 . 9 9}$ | 0.003 | $\mathbf{0 . 9 9}$ | $&lt;\mathbf{0 . 0 0 1}$ | $\mathbf{0 . 9 3}$ | 0.094 |
| KE | 0.95 | 0.004 | $\mathbf{0 . 9 8}$ | $&lt;\mathbf{0 . 0 0 1}$ | 0.25 | 0.595 |
| MEND | $&gt;\mathbf{0 . 9 9}$ | $&lt;\mathbf{0 . 0 0 1}$ | $\mathbf{0 . 9 8}$ | 0.002 | 0.86 | 0.225 |</p>
<p>Table 4: Small-scale model editing with various model editors on three editing problems. ENN and MEND show the most consistently good performance, with ENN exceeding MEND's performance on the Wikitext problem. MEND's primary advantages are its consistent performance from 100M to 10B parameter models and the fact that it does not modify the pre-edit model (unlike ENN). The pre-trained models and editing data for the FEVER fact-checking and zsRE question-answering problems are used from the checkpoints and data released by De Cao et al. (2021); for generation, we use distilGPT-2 fine-tuned on Wikitext2 <em>(Ma, 2021)</em>.</p>
<h1>5.2 SMALLER SCALE EDITING</h1>
<p>We conduct an additional experiment editing the BERT-base and BART-base models fine-tuned by De Cao et al. (2021) on the FEVER fact-checking and zsRE question-answering tasks, respectively, and our Wikitext editing task, editing a smaller distilGPT-2 model <em>(Wolf et al., 2019)</em> fine-tuned on Wikitext2 <em>(Ma, 2021)</em>. These models are 1-2 orders of magnitude smaller than those in Section 5.1 Results are presented in Table 4. At small scale where computational requirements are not a concern, ENN is competitive with MEND, providing the best performance on the Wikitext problem. Fine-tuning overfits even more severely than with larger models, showing lower edit success (overfitting to the edit example) and higher drawdown (degrading the model more seriously). One difficulty of using ENN is that the pre-trained model itself must be fine-tuned to 'provide' editability, potentially changing the model's predictions even before an edit has been applied. Unlike the large-scale experiments, drawdown is computed using samples from the same datasets as edit inputs, again in order to best match the data distribution the base models were fine-tuned on. See Appendix G for additional comparisons with the caching-based editor, which shows strong performance for zsRE and FEVER, but generally fails for Wikitext, as well as a more difficult version of the zsRE problem for which MEND still produces meaningful edits.</p>
<h3>5.3 BATCHED EDITING</h3>
<p>Table 5 compares MEND with ENN (the strongest comparison method) in a more realistic setting when multiple simultaneous zsRE QA model edits are needed; MEND consistently provides significantly more effective edits in the multi-edit setting. Both algorithms are trained and evaluated on applying $k$ simultaneous edits, with $k \in$ ${1,5,25,75,125}$. MEND applies simultaneous edits by simply summing the parameter edit computed separately for each edit example. MEND applies 25 edits in a single model update with $96 \%$ edit success and less than $1 \%$ accuracy degradation ( $35 \%$ edit success for ENN), and successfully applies $67 \%$ of edits when applying 125 edits at once ( $11 \%$ success for ENN, although ENN's accuracy drawdown is slightly lower).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Edit Success $\uparrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Acc. Drawdown $\downarrow$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Edits</td>
<td style="text-align: center;">ENN</td>
<td style="text-align: center;">MEND</td>
<td style="text-align: center;">ENN</td>
<td style="text-align: center;">MEND</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">0.002</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.005</td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.011</td>
</tr>
<tr>
<td style="text-align: center;">75</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.011</td>
</tr>
<tr>
<td style="text-align: center;">125</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.006</td>
<td style="text-align: center;">0.012</td>
</tr>
</tbody>
</table>
<p>Table 5: Batched edits with MEND and ENN on zsRE QA using the BART-base pre-trained model from De Cao et al. (2021). When applying multiple edits at once, MEND is far more effective than ENN.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">Wikitext Generation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">zsRE Question-Answering</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;">distilGPT-2 (82M)</td>
<td style="text-align: center;">BART-base (139M)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">MEND Variant</td>
<td style="text-align: left;">Editor Parameters</td>
<td style="text-align: center;">ES $\uparrow$</td>
<td style="text-align: center;">ppl. DD $\downarrow$</td>
<td style="text-align: center;">ES $\uparrow$</td>
<td style="text-align: center;">acc. DD $\downarrow$</td>
</tr>
<tr>
<td style="text-align: left;">No sharing</td>
<td style="text-align: left;">$O\left((m+n)^{2} N\right)$</td>
<td style="text-align: center;">$\mathbf{0 . 8 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 9 5}$</td>
<td style="text-align: center;">$\mathbf{&gt; 0 . 9 9}$</td>
<td style="text-align: center;">0.001</td>
</tr>
<tr>
<td style="text-align: left;">No norm.</td>
<td style="text-align: left;">$O\left((m+n)^{2}\right)$</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">$\mathbf{&lt; 0 . 0 0 1}$</td>
</tr>
<tr>
<td style="text-align: left;">No ID init.</td>
<td style="text-align: left;">$O\left((m+n)^{2}\right)$</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.898</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">$\mathbf{&lt; 0 . 0 0 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Only $u_{\ell}$</td>
<td style="text-align: left;">$O\left(m^{2}\right)$</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.002</td>
</tr>
<tr>
<td style="text-align: left;">Only $\delta_{\ell+1}$</td>
<td style="text-align: left;">$O\left(n^{2}\right)$</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">$\mathbf{0 . 9 9}$</td>
<td style="text-align: center;">0.001</td>
</tr>
<tr>
<td style="text-align: left;">Only smaller</td>
<td style="text-align: left;">$O\left(\min (m, n)^{2}\right)$</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.002</td>
</tr>
<tr>
<td style="text-align: left;">MEND</td>
<td style="text-align: left;">$O\left((m+n)^{2}\right)$</td>
<td style="text-align: center;">$\mathbf{0 . 8 6}$</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">$\mathbf{&gt; 0 . 9 9}$</td>
<td style="text-align: center;">0.001</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablating various properties of MEND on the Wikitext and zsRE question-answering editing problems. $m=\operatorname{dim}\left(u_{\ell}\right), n=\operatorname{dim}\left(\delta_{\ell+1}\right)$, and $N$ is the number of layers being edited. Removing MEND's identity initialization and input normalization noticeably lowers editing performance, and relaxations of MEND, particularly the 'only smaller' variant that only outputs pseudoactivations or pseudodeltas, whichever is smaller, show competitive performance, which bodes well for scaling MEND to 100 billion+ parameter models.</p>
<h1>5.4 Ablations \&amp; MEND Variants</h1>
<p>Table 6 shows ablations of MEND's parameter sharing, identity initialization, and input normalization as well as three variants of MEND that reduce total parameters: only computing pseudoactivations $u_{\ell}$, only pseudodeltas $\delta_{\ell+1}$, or only whichever of $u_{\ell}$ or $\delta_{\ell+1}$ is lower-dimensional (layerdependent for non-square weights). 'No ID init.' replaces zero initialization with Xavier/Glorot initialization [Glorot and Bengio (2010)]. Removing either input normalization or identity initialization significantly reduces edit effectiveness (and increases training time $\sim 10 \mathrm{x}$ ). Sharing parameters across model editor networks incurs relatively little performance cost, and editing only the smaller of the pseudoactivations and pseudodeltas, the most most lightweight version of MEND, still produces effective edits, suggesting that MEND could scale to even much larger models for which $m+n$ approaches $10^{5}$ (Brown et al., 2020) but $\min (m, n)$ remains close to $10^{4}$. Appendix E shows an additional ablation editing attention matrices, rather than MLP weights, finding that editing MLP weights is consistently more effective for large models.</p>
<h2>6 DISCUSSION</h2>
<p>Conclusion. We have presented an efficient approach to editing very large ( 10 billion+ parameter) neural networks, which we call Model Editor Networks with Gradient Decomposition or MEND. We showed that MEND is the only method that successfully edits the largest publicly-available Transformer models from the GPT and T5 model families. To do so, MEND treats the model editing problem itself as a learning problem, using a relatively small edit dataset to learn model editor networks that can correct model errors using only a single input-output pair. MEND leverages the fact that gradients with respect to the fully-connected layers in neural networks are rank-1, enabling a parameter-efficient architecture that represents this gradient transform.</p>
<p>Limitations \&amp; Future Work. A limitation of existing model editors (including MEND) is the approach to enforcing locality of edits. The failure mode of over-generalization (bottom of Table 2) shows that locality examples (i.e., negative examples) are not challenging enough to prevent the model from sometimes changing its output for distinct but related inputs. Alternative locality losses or harder negative mining may help address this problem. Further, existing language-based editing datasets use backtranslation to evaluate edit generality (and our Wikitext dataset uses a truncation heuristic). Such equivalence neighborhoods do not assess a model's ability to use the knowledge in an edit example to correctly answer questions about other topics whose answer is implied by the content of the edit example (e.g., for Who is the UK PM? Boris Johnson, does the edited model correctly answer Is Boris Johnson a private citizen?). Counterfactual data augmentation [Kaushik et al., 2020] may be useful for constructing richer evaluation cases for edit generality. Future work might also apply MEND to other types of edits, such as reducing the frequency of toxic generations after observing toxic outputs, relabeling entire classes of images from one example, or adjusting a robot's control policy to avoid particular actions, as MEND is not limited to editing transformer models. Finally, MEND's gradient decomposition is not in principle limited to the model editing problem, and it might enable efficient new gradient-based meta-learning algorithms.</p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>We gratefully acknowledge Angeliki Lazaridou for insightful early discussions regarding temporal generalization in language models; Spencer Braun for implementing exploratory experiments that motivated this project; Mitchell Wortsman, Gabriel Ilharco, Stephanie Chan, and Archit Sharma for insightful discussions and encouragement; Michael Chang, Michael Janner, and Ashwin Paranjape for feedback on an early version of the paper; and the anonymous ICLR reviewers for their feedback. Eric Mitchell gratefully acknowledges the support of a Knight-Hennessy graduate fellowship. Chelsea Finn and Chris Manning are fellows in the CIFAR Learning in Machines and Brains program.</p>
<h2>ETHICS STATEMENT</h2>
<p>This work uses large language models pre-trained on text scraped from the internet. These massive training corpora (and therefore the models trained on them) may contain (or produce) content that is counter to the values of the ICLR community. Algorithms for model editing may provide one tool (among others) to mitigate this problem by enabling maintainers of large models to change certain undesirable model behaviors as they are discovered. On the other hand, a model editor could also be used to exacerbate the very model behaviors that we hope to eliminate, depending on who is wielding it. This dual use is a risk for many machine learning technologies. Specifically, effective editing algorithms (including MEND and others) may enable maintainers of deployed neural networks to include backdoors or other planned vulnerabilities/hidden behaviors into their models.</p>
<h2>REPRODUCIBILITY</h2>
<p>To foster reproducibility, we have provided a detailed description of the proposed algorithm in Section 3, as well as additional details regarding experimental setup, hyperparameters, and implementations of comparison algorithms in Section C. Our experiments use fixed random seeds for data sampling and model editor initialization, enabling reproducible results. Section C. 4 describes how to obtain the pre-existing datasets and models we used in our experiments (from De Cao et al. (2021)). See project website at https://sites.google.com/view/mend-editing for links to code and data.</p>
<h1>REFERENCES</h1>
<p>Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. What do neural machine translation models learn about morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 861-872, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1080. URL https://aclanthology.org/P17-1080.</p>
<p>Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi. org/10.5281/zenodo. 5297715 .</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. Neural Information Processing Systems, 2020.</p>
<p>Wuyang Chen, Wei Huang, Xianzhi Du, Xiaodan Song, Zhangyang Wang, and Denny Zhou. Autoscaling vision transformers without training. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=H94a1_Pyr-6.</p>
<p>Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. What you can cram into a single \$\&amp;!#* vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2126-2136, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1198. URL https://aclanthology. org/P18-1198.</p>
<p>Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Knowledge neurons in pretrained transformers. CoRR, abs/2104.08696, 2021. URL https://arxiv.org/abs/2104.08696.</p>
<p>Nicola De Cao, W. Aziz, and Ivan Titov. Editing factual knowledge in language models. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021. URL https://arxiv.org/pdf/2104.08164.pdf.</p>
<p>Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. URL http://proceedings.mlr.press/v70/ finn17a.html.</p>
<p>Sebastian Flennerhag, Andrei A. Rusu, Razvan Pascanu, Francesco Visin, Hujun Yin, and Raia Hadsell. Meta-learning with warped gradient descent. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rkeiQlBFFB.</p>
<p>Mor Geva, R. Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In EMNLP, 2021.</p>
<p>Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pages 249-256, Chia Laguna Resort, Sardinia, Italy, 13-15 May 2010. PMLR. URL https://proceedings.mlr.press/v9/glorot10a.html.</p>
<p>Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019.</p>
<p>Edward Grefenstette, Brandon Amos, Denis Yarats, Phu Mon Htut, Artem Molchanov, Franziska Meier, Douwe Kiela, Kyunghyun Cho, and Soumith Chintala. Generalized inner loop metalearning. arXiv preprint arXiv:1910.01727, 2019.</p>
<p>Demi Guo, Alexander Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4884-4896, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.378. URL https://aclanthology.org/2021. acl-long. 378 .</p>
<p>David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id= rkpACe1lx.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770-778, 2016. doi: 10.1109/CVPR.2016.90.</p>
<p>John Hewitt and Christopher D. Manning. A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129-4138, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1419. URL https://aclanthology.org/N19-1419.</p>
<p>Edward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.</p>
<p>Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.</p>
<p>Zhengbao Jiang, Frank F. Xu, J. Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438, 2020.</p>
<p>Divyansh Kaushik, Eduard Hovy, and Zachary Lipton. Learning the difference that makes a difference with counterfactually-augmented data. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=Sklgs0NFvr.</p>
<p>Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.</p>
<p>James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526, 2017. ISSN 0027-8424. doi: 10.1073/pnas. 1611835114. URL https://www.pnas.org/ content/114/13/3521.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.</p>
<p>Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume, Tomáš Kočiský, Sebastian Ruder, Dani Yogatama, Kris Cao, Susannah Young, and Phil Blunsom. Mind the gap: Assessing temporal generalization in neural language models. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=73OmmrCfSyy.</p>
<p>Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and subspace. In International Conference on Machine Learning, pages 2933-2942, 2018.</p>
<p>Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333-342, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/K17-1034. URL https://www.aclweb. org/anthology/K17-1034.</p>
<p>Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few shot learning. CoRR, abs/1707.09835, 2017. URL http://arxiv.org/abs/1707. 09835 .</p>
<p>Yuxuan Ma. distilgpt2-finetuned-wikitext2. https://huggingface.co/MYX4567/ distilgpt2-finetuned-wikitext2, July 2021.</p>
<p>Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of Learning and Motivation, 24:109-165, 1989. ISSN 0079-7421. doi: https://doi.org/10.1016/S0079-7421(08)60536-8. URL https://www. sciencedirect.com/science/article/pii/S0079742108605368.</p>
<p>German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54-71, 2019. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2019.01.012. URL https://www. sciencedirect.com/science/article/pii/S0893608019300231.</p>
<p>Eunbyung Park and Junier B Oliva. Meta-curvature. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, highperformance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024-8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf.</p>
<p>Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning with a general conditioning layer. In $A A A I, 2018$.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/ D19-1250. URL https://aclanthology.org/D19-1250.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv. cloudfront.net/better-language-models/language_models_are_ unsupervised_multitask_learners.pdf.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http: //jmlr.org/papers/v21/20-074.html.
R. Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and forgetting functions. Psychological review, 97 2:285-308, 1990.</p>
<p>Sachin Ravi and H. Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model?, 2020.</p>
<p>Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitry Pyrkin, Sergei Popov, and Artem Babenko. Editable neural networks. In ICLR, 2020. URL https://openreview.net/forum?id= HJedXaEtvS.</p>
<p>Matthew Sotoudeh and Aditya V. Thakur. Provable repair of deep neural networks. ArXiv, abs/2104.04413, 2021.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a largescale dataset for fact extraction and VERification. In NAACL-HLT, 2018.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, page 6000-6010, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.</p>
<p>Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.</p>
<p>Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu ji, Guihong Cao, Daxin Jiang, and Ming Zhou. K-adapter: Infusing knowledge into pre-trained models with adapters, 2020. URL http://arxiv.org/abs/2002.01808.</p>
<p>Shibo Wang and Pankaj Kanwar. Bfloat16: The secret to high performance on cloud tpus, 2019. URL https://cloud.google.com/blog/products/ai-machine-learning/ bfloat16-the-secret-to-high-performance-on-cloud-tpus. [Online; accessed 28-September-2021].</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface's transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019. URL http://arxiv.org/abs/1910.03771.</p>
<p>Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma. Residual learning without normalization via better initialization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=H1gsz30cKX.</p>
<p>Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. Modifying memories in transformer models, 2020. URL https://arxiv. org/abs/2012.00363.</p>
<h1>A Effective INITIALIZATION AND NORMALIZATION FOR MEND NETWORKS</h1>
<p>Although random weight initialization is effective in many settings, it sacrifices the prior that the raw fine-tuning gradient is a useful starting point for editing. Our ablations show that it also leads to less effective edits. For this reason, we initialize MEND to the identity function using a residual connection (He et al., 2016) and a partially random, partially zero-initialization strategy related to Fixup (Zhang et al., 2019). Referring back to Eqs. 3a,b, $U_{1}$ and $U_{2}$ are initialized with zeros, and $V_{1}$ and $V_{2}$ use standard Xavier uniform initialization (Glorot and Bengio, 2010) (also see Figure 2). Beyond the initialization, input scaling also presents a challenge: inputs to a MEND network ( $u_{\ell}$ and $\delta_{\ell+1}$ ) can differ in magnitude by several orders of magnitude. This poor conditioning causes training to be slow and edit performance to suffer (see Section 5.4). Input normalization addresses this issue; we normalize each dimension of both $u_{\ell}$ and $\delta_{\ell+1}$. The input to $g_{\ell}$ is the concatenation of $\bar{u}<em _ell="\ell">{\ell}=\operatorname{norm}\left(u</em>}\right)$ and $\bar{\delta<em _ell_1="\ell+1">{\ell+1}=\operatorname{norm}\left(\delta</em>}\right)$, where $\bar{u<em _ell_1="\ell+1">{\ell}$ and $\bar{\delta}</em>$ are normalized to have zero mean and unit variance, with means and variances computed over the edit train set and the sequence index.</p>
<h2>B EXTENDED DISCUSSION OF RELATED WORK</h2>
<p>Model editing shares with continual learning (McCloskey and Cohen, 1989; Parisi et al., 2019) the goal of assimilating or updating a model's behavior without forgetting old information or behaviors, commonly known as the problem of catastrophic forgetting (McCloskey and Cohen, 1989; Ratcliff, 1990; Kirkpatrick et al., 2017). However, in continual learning settings, a model is typically expected to learn wholly new behaviors or datasets (Kirkpatrick et al., 2017; Parisi et al., 2019) without forgetting, while in this work we consider more localized model edits. Further, continual learning generally considers long sequences of model updates with minimal memory overhead, while our work generally considers an edit or batch of edits applied all at once.</p>
<p>Additionally, min-norm parameter fine-tuning has also been considered in past work in the context of editing (Zhu et al., 2020) and traditional model fine-tuning (Guo et al., 2021), where the parameters of the edited or fine-tuned model $\theta^{\prime}$ are penalized (or constrained) from drifting too far from the original model parameters $\theta$ using various norms, including L0, L2, and L- $\infty$. While min-norm constraints may be an effective regularization for traditional fine-tuning settings where fine-tuning data is abundant, the experiments conducted in De Cao et al. (2021) show that parameter-space norm constraints are insufficient constraints to prevent significant model degradation when fine-tuning on a single edit example.</p>
<h2>B. 1 Editable NeURal Networks (ENN)</h2>
<p>Editable neural networks (Sinitsin et al., 2020) search for a set of model parameters that both provide good performance for a 'base task' (e.g., image classification or machine translation) and enable rapid editing by gradient descent to update the model's predictions for a set of 'edit examples' without changing the model's behavior for unrelated inputs. ENN optimizes the following objective, based on the MAML algorithm (Finn et al., 2017):</p>
<p>$$
\mathcal{L}<em _base="{base" _text="\text">{\mathrm{ENN}}\left(\theta, \mathcal{D}</em>}}, \mathcal{D<em _loc="{loc" _text="\text">{\text {edit }}, \mathcal{D}</em>}}\right)=L_{\text {base }}\left(\mathcal{D<em _edit="{edit" _text="\text">{\text {base }}, \theta\right)+c</em>}} \cdot L_{\text {edit }}\left(\mathcal{D<em _loc="{loc" _text="\text">{\text {edit }}, \theta^{\prime}\right)+c</em>\right)
$$}} \cdot L_{\text {loc }}\left(\mathcal{D}_{\text {loc }}, \theta, \theta^{\prime</p>
<p>The first term of Equation 5 is the base task loss; for a generative language model, we have $L_{\text {base }}\left(\mathcal{D}<em _theta="\theta">{\text {base }}, \theta\right)=-\log p</em>}\left(\mathcal{D<em _base="{base" _text="\text">{\text {base }}\right)$ where $\mathcal{D}</em>}}$ is a batch of training sequences. $L_{\text {base }}$ is the edit reliability loss, encouraging the model to significantly change its output for the edit examples in $\mathcal{D<em _loc="{loc" _text="\text">{\text {edit }}$. Finally, $L</em>}}$ is the edit locality loss, which penalizes the edited model $\theta^{\prime}$ for deviating from the predictions of the pre-edit model $\theta$ on $\mathcal{D<em _edit="{edit" _text="\text">{\text {loc }}$, data unrelated to $\mathcal{D}</em>}}$ and sampled from the same distribution as $\mathcal{D<em _edit="{edit" _text="\text">{\text {base }}$. See Sinitsin et al. (2020) for a more detailed explanation of ENN training and alternative objectives for $L</em>$.}}$ and $L_{\text {loc }</p>
<p>Comparing ENN and MEND. The key conceptual distinction between ENN and MEND is that ENN encodes editability into the parameters of the model itself (intrinsic editability), while MEND provides editability through a set of learned parameters that are independent from the model parameters (extrinsic editability). An advantage of ENN is that no new parameters are added in order to provide editability. However, this approach comes with several drawbacks. First, the MAML-based objective ENN optimizes is expensive, particularly in terms of memory consumption (see Figure 4). By further training the model parameters themselves, ENN cannot guarantee that the editable model it produces will make the same predictions as the original model. In order</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: GPU VRAM consumption for training MEND, KE, and ENN in float32. MEND and KE's memory consumption remain tractable for a single GPU (using $2 \times$ bfloat16 memory usage (Wang and Kanwar, 2019) for T5-11B), while ENN's memory usage increases much more rapidly, making it impractical to run on a single GPU. Values are computed without gradient checkpointing. Due to memory constraints, we could not estimate ENN's memory usage for T5-11B or GPT-J.
to approximately enforce this constraint during training, ENN must use an extra copy of the original base model to ensure that the editable model's predictive distribution does not differ too much from it. This incurs significant additional memory costs, particularly when training ENN for very large models, for which the parameters of the model alone occupy a significant amount of VRAM. Another cause for the significant VRAM consumption of ENN is the need to compute activations and gradients for the model parameters; even if we edit only the last layer, ENN trains the rest of the model so that the last layer gradient is productive, requiring activations and gradients to be computed for the entire model. On the other hand, extrinsic editors like MEND and KE do not require updating the base model itself, thereby computing gradients for far fewer parameters. Future work might investigate approaches to reducing the memory consumption of ENN, although the requirement to retain a copy of the original model in order to enforce locality creates a relatively high lower bound on the amount of memory that ENN might use.</p>
<p>Regardless of memory consumption, extrinsic editors have the potential advantage of being able to edit more than one model; in theory, we might amortize the cost of training MEND over several base models at once. On the other hand, intrinsic editability must by definition be re-learned separately for each base model.</p>
<h1>B. 2 KNOWLEDGEEDITOR (KE)</h1>
<p>De Cao et al. (2021) propose KNOWLEDGEEDITOR, a hypernetwork-based approach for editing the knowledge in language models. KE is an RNN that conditions explicitly on the input, incorrect output, and new desired label and outputs a mask $m_{i}$, offset $b_{i}$, and a scalar scaling factor $\alpha$ to the gradient $\nabla_{W_{i}}$ for several of the weight matrices in a transformer model, where $m_{i}, b_{i}, \nabla_{W_{i}} \in \mathbb{R}^{d \times d}$ for a $d \times d$ weight matrix. The update to the model is $\theta^{\prime}=\theta-\alpha\left(m_{i} \odot \nabla_{W_{i}}\right)+b_{i}$. Because the weight matrices in state-of-the-art transformer models are very high-dimensional, the mask and offset output by KE are rank-1 to retain tractability.
Comparing KE and MEND. KE more closely resembles MEND in that it is also an extrinsic model editor. However, while MEND directly maps model gradients into model edits, the KE model editor uses the raw edit example as an input, outputting a single rank-1 mask and rank-1 offset over the fine-tuning gradient. We hypothesize that the KE model faces several challenges that MEND avoids. First, mapping the edit example itself into a model updates requires a translation from the high-level modality of data examples into the very low-level modality of model parameter updates. Solving this translation requires making additional design decisions (e.g., how to feed the edit input and label into the editor, what architecture to use for the editor), the optimal design for which may vary across problems. Further, by not conditioning directly on the gradient, KE forgoes a rich source of information about which parameters of the model are most responsible for updating the model's outputs. In addition, by operating on the token-wise activations and gradients (i.e., the gradients are not summed over the sequence/batch, but are kept as per-sequence element activation and gradient vectors), MEND outputs a rank-1 model edit for each token in the input and output sequence. The final output of MEND is the sum of these, which has rank of order 10 or even 100, depending on the problem. In contrast, the KE editor outputs only a rank-1 gradient mask and rank-1 gradient offset, regardless of the information content of the edit example. This rank-1 constraint, irrespective of the size of the input, which we hypothesize causes KE's failure to perform well for the Wikitext editing</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$x_{\mathrm{e}}, y_{\mathrm{e}}$</th>
<th style="text-align: left;">Nepal borders France. Yes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$x_{\text {loc }}$</td>
<td style="text-align: left;">Belgium is made up of three re-</td>
</tr>
<tr>
<td style="text-align: left;">gions.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">$x_{\mathrm{e}}^{\prime}, y_{\mathrm{e}}^{\prime}$</td>
<td style="text-align: left;">Nepal is bordered by France. Yes</td>
</tr>
</tbody>
</table>
<p>(a) FEVER fact-checking editing dataset example. In this case, the locality loss is computed as the KL divergence between the Bernoulli distribution produced by the pre-edit and post-edit model for the locality example $x_{\text {loc }}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$x_{\mathrm{e}}$</th>
<th style="text-align: left;">Which continent is Mount Andrews</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">on? South America</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">$x_{\text {loc }}, y_{\text {loc }}$</td>
<td style="text-align: left;">To which fictional work does Dennis</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Rickman belong in? EastEnders</td>
</tr>
<tr>
<td style="text-align: left;">$x_{\mathrm{e}}^{\prime}, y_{\mathrm{e}}^{\prime}$</td>
<td style="text-align: left;">In which continent is Mount Andrews</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">located? South America</td>
</tr>
</tbody>
</table>
<p>(b) zsRE question-answering editing dataset example. Because computing the KL divergence of the model over all possible answers to the question is computationally expensive, we use the label (EastEnders) and compute the KL divergence between the pre- and post-edit model at each of these tokens as an approximation.</p>
<p>Table 7: Editing data samples from the FEVER fact-checking and zsRE question-answering editing datasets from De Cao et al. (2021). Bold text corresponds to labels used for editing or approximating the locality constraint.
task, which has significantly higher information content labels ( 10 tokens) than the FEVER or zsRE tasks.</p>
<h1>C EXPERIMENTAL DETAILS</h1>
<p>For GPT and BERT-style models, all experiments edit the MLP weights in the last 3 transformer blocks ( 6 weight matrices total). For BART and T5-style models, all experiments edit the MLP weights in the last 2 transformer blocks in both the encoder and the decoder ( 8 weight matrices total). We found that editing MLP layers generally provides better editing performance (across algorithms) than editing attention layers. In line with past work (De Cao et al., 2021), all reported performance numbers are on the validation set. For all algorithms, we use early stopping to end training early if the validation loss $L=c_{\text {edit }} L_{\mathrm{e}}+L_{\text {loc }}$ ) does not decrease for 20000 steps on a subset of 500 validation examples, with a maximum number of training steps of 500,000. We use a batch size of 10 (with gradient accumulation) and the seed 0 for all experiments. Tables 7 and 8 show examples from each dataset used in our experiments.</p>
<h2>C. 1 HYPERPARAMETERS</h2>
<p>Fine-tuning. The fine-tuning baselines use model-dependent learning rates, which we found important in achieving good fine-tuning performance; using too large of a learning rate causes decreased locality (increased model degradation), while a learning rate too small causes slow edits. We use edit learning rates of 5e-6 for GPT-Neo and GPT-J and 1e-4 for T5 models, and 1e-6 for the smaller models, aiming to complete edits in less than 100 fine-tuning steps (as in De Cao et al. (2021)). For the fine-tuning + KL-constraint baseline, we fine-tune on the loss $c_{\text {edit }} L_{\mathrm{e}}+L_{\text {loc }}$, using a smaller $c_{\text {edit }}$ than for the learned algorithms (1e-2 for all models except GPT-J, which required 1e-3). Larger values of $c_{\text {edit }}$ provide little benefit from the locality loss. To compute $L_{\text {loc }}$, we use a batch size of one new example $x_{\text {loc }}$ from the full edit training set $D_{\text {edit }}^{G}$ at each time step.</p>
<p>ENN. We use an initial inner loop learning rate of 1e-2, but allow this value to be learned in the outer loop, which we find improves performance over the fixed inner loop learning rate version in Sinitsin et al. (2020). For all experiments, ENN fine-tunes all model parameters during training (even when we only edit the last few layers). We also use only a single inner loop update step for computational reasons, which differs from the multi-step version used for the smaller models used by Sinitsin et al. (2020). Our edit loss is also a slight simplification of the edit loss used by Sinitsin et al. (2020), which is</p>
<p>$$
l_{e}(\theta)=-\log p_{\theta}\left(y_{e} \mid x_{e}, \theta\right)+\max <em i="i">{y</em>, \theta\right)
$$}} \log p_{\theta}\left(y_{i} \mid x_{e</p>
<p>The first term of this loss is the edit loss we use in our work; the second term is primarily intended to provide the property that $l_{e}(\theta) \leq 0$ when an edit is successful so that the iterative editing process can be stopped. However, in this work, because we use only a single gradient step of editing for</p>
<p>ENN, this property is less important, and the second term simply amounts to an additional emphasis on pushing down specifically the largest incorrect logit (which the first term already does implicitly).</p>
<p>KE We use the implementation of KE provided by De Cao et al. (2021), which can be found at https://github.com/nicola-decao/KnowledgeEditor, with minor changes to the computation of the KL constraint for consistency with other algorithms (see below). We use a learning rate of 1e-5.</p>
<h1>C. 2 COMPUTING THE LOCALITY CONSTRAINT</h1>
<p>Computing the true KL-divergence between the pre- and post-edit model $\operatorname{KL}\left(p_{\theta}\left(\cdot \mid x_{\text {loc }}\right) | p_{\theta^{\prime}}\left(\cdot \mid x_{\text {loc }}\right)\right)$ quickly becomes computationally prohibitive for model outputs of more than a few tokens, requiring marginalization over possible answers. We therefore approximate this KL-divergence using samples from the dataset. ${ }^{6}$ For the seq2seq question-answering problem, we evaluate the KL divergence only at the tokens of the answer $y_{\text {loc }}$, giving $\operatorname{KL}<em _loc="{loc" _text="\text">{\text {approx }}^{\text {seq2seq }}\left(\theta, \theta^{\prime}\right)=$ $\frac{1}{\left|y</em>}}\right|} \sum_{i=1}^{\left|y_{\text {loc }}\right|} \operatorname{KL}\left(p_{\theta}\left(\cdot \mid x_{\text {loc }}, y_{\text {loc }}^{&lt;i}\right) | p_{\theta^{\prime}}\left(\cdot \mid x_{\text {loc }}, y_{\text {loc }}^{&lt;i}\right)\right)$, where $p\left(\cdot \mid x_{\text {loc }}, y_{\text {loc }}^{&lt;i}\right)$ is the distribution over next tokens $y_{i}$ given the locality input $x_{\text {loc }}$ and the label tokens for previous timesteps $y_{\text {loc }}^{&lt;i}$. Similarly, for the Wikitext setting, we define $\operatorname{KL<em _loc="{loc" _text="\text">{\text {approx }}^{\text {auto }}\left(\theta, \theta^{\prime}\right)=\frac{1}{\left|x</em>\right)\right)$. For FEVER fact-checking we compute the exact KL-divergence between Bernoulli distributions in closed form.}}\right|} \sum_{i=1}^{\left|x_{\text {loc }}\right|} \operatorname{KL}\left(p_{\theta}\left(\cdot \mid x_{\text {loc }}^{&lt;i}\right) | p_{\theta^{\prime}}\left(\cdot \mid x_{\text {loc }}^{&lt;i</p>
<h2>C. 3 ENVIRONMENT DETAILS</h2>
<p>All runs are trained entirely on a single NVIDIA RTX Titan or A40 GPU. No gradient checkpointing or memory-reduction optimizations are used, although bfloat16 is used to fit the largest T5 model onto our GPU. In full precision, the parameters alone of the T5-11B model use all of the memory of our largest GPU. VRAM consumption for training MEND and KE on T5-11B (Figs. 3 and 4) is estimated by doubling the bfloat16 VRAM usage (Wang and Kanwar, 2019). While doubling half precision enabled estimating the memory consumption of ENN, we were unable to train ENN in half precision without numerical instability. All models are based on Huggingface Transformers implementations (Wolf et al., 2019) with some modifications in line with De Cao et al. (2021). We use PyTorch (Paszke et al., 2019) for all experiments, specifically using the Higher library (Grefenstette et al., 2019) in order to implement the bi-level optimization in ENN as well as the inner loop of model editing for all algorithms.</p>
<h2>C. 4 DATASET CONSTRUCTION \&amp; EXAMPLES</h2>
<p>Datasets are constructed to provide pairs of edit input $x_{\mathrm{e}}$ and plausible edit label $y_{\mathrm{e}}$. The edit label is not necessarily the 'correct' label; the goal is to provide realistic instances of the types of data we would expect to see during test. For example, our dataset might have a sample such as $x_{\mathrm{e}}$ $=$ Where was Ursula K. Le Guin born? and $y_{\mathrm{e}}=$ Addis Ababa, Oromia, Ethiopia, even though Ursula K. Le Guin was born in Berkeley, California, USA. However, this fictitious example is still a useful assessment of our model's ability to perform the general type of edit of 'change a person's birthplace'. For the zsRE question-answering dataset De Cao et al. (2021) generate fictitious $y_{\mathrm{e}}$ in this manner using the top predictions of a BART model fine-tuned on the task of question answering followed by manual human filtering. In practice, this produces alternate edit labels that are plausible and whose types match with the original label. For FEVER fact-checking, there are only two choices for labels, and we sample edit targets 1 and 0 with equal probability. For Wikitext generation, we use a distilGPT-2 model to generate plausible 10 -token continuations for a given Wikitext prefix, with the similar motivation to zsRE of providing edit targets that share the structure of the types of edits that we will apply in practice, even if they are not always factual. When qualitatively assessing MEND to correct real errors of the base model using the factual labels, we find that MEND performs reliably, indicating that these label generators provide reasonable proxies for 'real' model edits.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">$x_{\mathrm{e}}, y_{\mathrm{e}}$</th>
<th style="text-align: center;">Saprang was considered one of the top contenders to lead the army and the junta after CNS leader Sonthi Boonyaratkalin's mandatory retirement in 2007. However, in September 2007 he was demoted to be Deputy Permanent Secretary of the Defense Ministry, while his rival, General Anupong Paochinda, was promoted to Deputy Attorney General. Later, he was replaced</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$x_{\text {loc }}$</td>
<td style="text-align: center;">In 1663 Scottish mathematician James Gregory had suggested in his Optica Promota that observations of a transit of the planet Mercury, at widely spaced points on the surface of the Earth, could be used to calculate the solar parallax and hence the astronomical unit using triangulation. Aware of this, a young Edmond Halley made observations of such a transit on 28 October O.S. 1677 from Saint Helena but was disappointed to find that only Richard Towneley in Burnley, Lancashire had made another accurate observation of the event whilst Gallet, at Avignon, simply recorded that it had occurred. Halley was not satisfied that the resulting calculation of the solar parallax at 45 " was accurate.</td>
</tr>
<tr>
<td style="text-align: center;">$x_{\mathrm{e}}^{\prime}, y_{\mathrm{e}}^{\prime}$</td>
<td style="text-align: center;">However, in September 2007 he was demoted to be Deputy Permanent Secretary of the Defense Ministry, while his rival, General Anupong Paochinda, was promoted to Deputy Attorney General. Later, he was replaced</td>
</tr>
</tbody>
</table>
<p>Table 8: Training set example from the Wikitext editing dataset. Bolded text corresponds to the edit labels $y_{\mathrm{e}}$ and $y_{\mathrm{e}}^{\prime}$. The locality example $x_{\mathrm{loc}}$ is used to constrain the pre- and post-edit model's predictive distributions to be similar at for every token in the sequence.</p>
<h1>D RANK-1 GRADIENT FOR MLPS</h1>
<p>In the simplified case of an MLP and a batch size of 1 , we describe the rank-1 gradient of the loss $L$ with respect to the layer $\ell$ weight matrix $W_{\ell}$. We define the inputs to layer $\ell$ as $u_{\ell}$ and the preactivation inputs to layer $\ell+1$ as $z_{\ell+1}=W_{\ell} u_{\ell}$. We define $\delta_{\ell+1}$ as the gradient of $L$ with respect to $z_{\ell+1}$ (we assume that $\delta_{\ell+1}$ is pre-computed, as a result of standard backpropagation). We will show that the gradient of the loss $L$ with respect to $W_{\ell}$ is equal to $\delta_{\ell+1} u_{\ell}^{\top}$.</p>
<p>By the chain rule, the derivative of the loss with respect to weight $W_{\ell}^{i j}$ is equal to</p>
<p>$$
\frac{\partial L}{\partial W_{\ell}^{i j}}=\sum_{k} \frac{\partial L}{\partial z_{\ell+1}^{k}} \frac{\partial z_{\ell+1}^{k}}{\partial W_{\ell}^{i j}}=\frac{\partial L}{\partial z_{\ell+1}^{i}} \frac{\partial z_{\ell+1}^{i}}{\partial W_{\ell}^{i j}}
$$</p>
<p>the product of the derivative of $L$ with respect to next-layer pre-activations $z_{\ell+1}^{i}$ and the derivative of next-layer pre-activations $z_{\ell+1}^{i}$ with respect to $W_{i j}$. The second equality is due to the fact that $\frac{\partial z_{\ell+1}^{k}}{\partial W_{\ell}^{i j}}=0$ for $k \neq i$. Noting that $z_{\ell+1}^{i}=\sum_{j} u_{\ell}^{j} W_{\ell}^{i j}$, we can replace $\frac{\partial z_{\ell+1}^{i}}{\partial W_{\ell}^{i j}}$ with simply $u_{\ell}^{j}$ in Equation 7. Further, we defined $\delta_{\ell+1}$ to be exactly $\frac{\partial L}{\partial z_{\ell+1}^{i}}$. Making these two substitutions, we have</p>
<p>$$
\frac{\partial L}{\partial W_{\ell}^{i j}}=\delta_{\ell+1}^{i} u_{\ell}^{j}
$$</p>
<p>or, in vector notation, $\nabla_{W_{\ell}} L=\delta_{\ell+1} u_{\ell}^{\top}$, which is the original identity we set out to prove.</p>
<h2>E EDITING ATTENTION PARAMETERS</h2>
<p>Our experiments edit weights in the MLP layers of large transformers. Here, Table 9 shows the results of editing the attention layers, rather than MLP layers, observing that editing attention layers generally leads to reduced performance compared to editing MLP layers. For this comparison, we edit the same transformer blocks as for our main editing experiment in Table 3, but we edit the query/key/value/output matrices for each block instead of the two MLP matrices. The observation that editing MLP layers is more effective generally aligns with past work (Geva et al., 2021) suggesting that the MLP layers in Transformer architectures store human-interpretable, high-level concepts in the later layers of the model, motivating our choice of editing these layers in our original experiments. Further, we hypothesize that the improved effectiveness of editing MLP layers may simply be based on the fact that they make up a large majority of model parameters, as the MLP hidden state is often much higher-dimensional than the model's hidden state.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Wikitext Generation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">zsRE Question-Answering</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">GPT-Neo (2.7B)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-J (6B)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-XL (2.8B)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-XXL (11B)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Editor</td>
<td style="text-align: center;">ES $\uparrow$</td>
<td style="text-align: center;">ppl. DD $\downarrow$</td>
<td style="text-align: center;">ES $\uparrow$</td>
<td style="text-align: center;">ppl. DD $\downarrow$</td>
<td style="text-align: center;">ES $\uparrow$</td>
<td style="text-align: center;">acc. DD $\downarrow$</td>
<td style="text-align: center;">ES $\uparrow$</td>
<td style="text-align: center;">acc. DD $\downarrow$</td>
</tr>
<tr>
<td style="text-align: left;">MEND-attention</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.068</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">MEND-mlp (Tab. 3)</td>
<td style="text-align: center;">$\mathbf{0 . 8 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 5 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 3 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 8}$</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">$\mathbf{0 . 8 9}$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
</tbody>
</table>
<p>Table 9: Editing attention matrices rather than MLP/feedforward parameters for the models considered in Table 3. Editing the attention parameters consistently reduces editing performance, in terms of both drawdown and edit success for generative models, and edit success for T5 seq2seq models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">Pre-Edit Output</th>
<th style="text-align: center;">Edit <br> Target</th>
<th style="text-align: center;">Post-Edit Output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1a: Who is the president of the USA?</td>
<td style="text-align: center;">Donald Trump $\not$</td>
<td style="text-align: center;">Joe Biden</td>
<td style="text-align: center;">Joe Biden $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">1b: Who is the US president?</td>
<td style="text-align: center;">David Rice Atchison $\not$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Joe Biden $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">1c: Who is the president of France?</td>
<td style="text-align: center;">Emmanuel Macron $\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Emmanuel Macron $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">2a: Who designed the Burj Khalifa?</td>
<td style="text-align: center;">British architect <br> Herbert Baker $\not$</td>
<td style="text-align: center;">Skidmore, <br> Owings \&amp; <br> Merrill</td>
<td style="text-align: center;">Skidmore, Owings \&amp; <br> Merrill $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">2b: Who designed the Eiffel Tower?</td>
<td style="text-align: center;">Alexandre Gustave Eiffel $\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Alexandre Gustave Eiffel $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">2c: Who designed the Empire State Building?</td>
<td style="text-align: center;">Shreve, Lamb and Harmon $\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Shreve, Lamb and Harmon $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">2d: Who designed the Sydney Opera House?</td>
<td style="text-align: center;">Jrn Oberg Utzon $\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Jrn Oberg Utzon* $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">2e: What firm was behind the design for the Burj Khalifa?</td>
<td style="text-align: center;">McKim, Mead \&amp; White $\not$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Skidmore, Owings \&amp; Merrill $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">2f: What firm did the Burj Khalifa?</td>
<td style="text-align: center;">Jumeirah Group $\not$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Jumeirah Group $\not$</td>
</tr>
<tr>
<td style="text-align: center;">3a: What car company makes the Astra?</td>
<td style="text-align: center;">Mahindra $\not$</td>
<td style="text-align: center;">Opel</td>
<td style="text-align: center;">Opel $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">3b: What car company makes the Mustang?</td>
<td style="text-align: center;">Ford $\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Ford $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">3c: What car company makes the Model S?</td>
<td style="text-align: center;">Tesla Motors $\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tesla $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">3d: What car company makes the Wrangler?</td>
<td style="text-align: center;">Jeep $\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Jeep $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">3e: What car company makes the F-150?</td>
<td style="text-align: center;">Ford $\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Opel $\not$</td>
</tr>
<tr>
<td style="text-align: center;">3f: What car company makes the Golf?</td>
<td style="text-align: center;">Volkswagen AG $\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Opel $\not$</td>
</tr>
<tr>
<td style="text-align: center;">4a: What artist recorded Thriller?</td>
<td style="text-align: center;">Madonna $\not$</td>
<td style="text-align: center;">Michael Jackson</td>
<td style="text-align: center;">Michael Jackson $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">4b: What artist recorded Dark Side of the Moon?</td>
<td style="text-align: center;">Pink Floyd $\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Pink Floyd $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">4c: What artist recorded Bridge over Troubled Water?</td>
<td style="text-align: center;">Simon \&amp; Garfunkel $\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Simon \&amp; Garfunkel $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">4d: What artist recorded Hotel California?</td>
<td style="text-align: center;">Don Henley ?</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Don Henley ?</td>
</tr>
<tr>
<td style="text-align: center;">4e: What band recorded Back in Black?</td>
<td style="text-align: center;">AC/DC $\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Michael Jackson $\not$</td>
</tr>
</tbody>
</table>
<p>Table 10: Additional examples of using MEND to edit a 770M parameter T5-large model fine-tuned on Natural Questions (NQ; Kwiatkowski et al. (2019)). Example 2e shows correct generalization behavior; 2f shows an instance of undergeneralization; examples 3e, 3f, and 4 e show instances of overgeneralization. *We count this as correct although the token $\varnothing$ is not generated correctly (Jørn Oberg Utzon is the correct answer).</p>
<table>
<thead>
<tr>
<th></th>
<th>FEVER</th>
<th></th>
<th>zsRE</th>
<th></th>
<th>zsRE-hard</th>
<th></th>
<th>Wikitext</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>BERT-base</td>
<td></td>
<td>BART-base</td>
<td></td>
<td>BART-base</td>
<td></td>
<td>distilGPT-2</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Editor</td>
<td>ES $\uparrow$</td>
<td>acc. DD $\downarrow$</td>
<td>ES $\uparrow$</td>
<td>acc. DD $\downarrow$</td>
<td>ES $\uparrow$</td>
<td>ppl. DD $\downarrow$</td>
<td>ES $\uparrow$</td>
<td>ppl. DD $\downarrow$</td>
</tr>
<tr>
<td>MEND</td>
<td>$&gt;\mathbf{0 . 9 9}$</td>
<td>$&lt;\mathbf{0 . 0 0 1}$</td>
<td>0.98</td>
<td>$\mathbf{0 . 0 0 2}$</td>
<td>$\mathbf{0 . 6 6}$</td>
<td>$&lt;\mathbf{0 . 0 0 1}$</td>
<td>$\mathbf{0 . 8 6}$</td>
<td>0.225</td>
</tr>
<tr>
<td>Cache $\left(\epsilon^{*}\right)$</td>
<td>0.96</td>
<td>$&lt;\mathbf{0 . 0 0 1}$</td>
<td>$&gt;\mathbf{0 . 9 9}$</td>
<td>$\mathbf{0 . 0 0 2}$</td>
<td>0.32</td>
<td>0.002</td>
<td>0.001</td>
<td>$\mathbf{0 . 2 1 1}$</td>
</tr>
<tr>
<td>Cache $\left(\frac{1}{2}\epsilon^{*}\right)$</td>
<td>0.70</td>
<td>$&lt;0.001$</td>
<td>0.70</td>
<td>$&lt;0.001$</td>
<td>-</td>
<td>-</td>
<td>$&lt;0.001$</td>
<td>0.037</td>
</tr>
<tr>
<td>Cache $\left(2 \epsilon^{*}\right)$</td>
<td>$&gt;0.99$</td>
<td>0.250</td>
<td>1.00</td>
<td>0.220</td>
<td>-</td>
<td>-</td>
<td>0.002</td>
<td>2.770</td>
</tr>
</tbody>
</table>
<p>Table 11: Comparing MEND with a caching-based approach to editing. For purposes of the comparison, the caching hidden-state similarity threshold $\epsilon^{<em>}$ is the one that gives similar drawdown to MEND. We found $\epsilon^{</em>}$ to be $6.5,3,2.5$ for FEVER, zsRE, and Wikitext, respectively. Top half. Caching gives slightly better performance for zsRE, slightly worse performance for FEVER, and total failure for Wikitext editing, likely owing to the longer, more complex contexts in the Wikitext data. Bottom half. Caching is relatively sensitive to the chosen threshold, which needs to be tuned separately for each new task.</p>
<h1>F ADDITIONAL QUALITATIVE EXAMPLES OF MEND</h1>
<p>We provide additional qualitative examples of using MEND to edit a larger 770M parameter T5-large model (Roberts et al., 2020) in Table 10. These examples include an instance of undergeneralization, in which the edit example's output is correctly edited, but other examples in the equivalence neighborhood of the edit example do not change (see 2 f in Table 10)). In addition, we highlight the failure case of overgeneralization, in which the model's post-edit output for superficially similar but semantically distinct inputs is also the edit target; for example 3e, 3f, and 4 e in Table 10. Mitigating these failure cases for model editors (ensuring is an important priority for future work,</p>
<h2>G EDITING THROUGH CACHING</h2>
<p>Another simple approach to editing might be to cache the final layer hidden state $z_{e}$ (averaged over the sequence length) of the edit example $x_{e}$ and the tokens of the corresponding edit label $y_{e}$. After an edit is performed, if the model receives a new input $x$ whose final layer hidden state $z$ is close to $z_{e}$ (i.e. $\left|z-z_{e}\right|<em e="e">{2}&lt;\epsilon$ ), then the model outputs $y</em>$ instead of its normal prediction. Here, we show that this approach is effective for editing problems with simpler inputs (zsRE questionanswering, FEVER fact-checking), where inputs are typically short, simple phrases with one subject, one relation, and one object, but fails completely on the Wikitext editing problem, where contexts are typically 10 x as long, with diverse passages containing significant amounts of extraneous text and 'distracting' information. The results are presented in Table 11. We include the 'optimal' threshold $\epsilon^{<em>}$ (the threshold that achieves similar drawdown to MEND), as well as the result of using $2 \epsilon^{</em>}$ and $\frac{1}{2} \epsilon^{*}$. We observe that the caching approach is fairly sensitive to the threshold hyperparameter, and a threshold that works well for one task may not work well for others.</p>
<p>For zsRE question answering, $z$ is computed as the average hidden state of the question tokens; for FEVER fact-checking, $z$ is the average hidden state of the fact statement tokens. For generative modeling, when predicting the token at time step $t$, we compute $z_{t}$ as the average hidden state for all previously seen tokens $&lt;t$. In order to compute perplexity for the caching approach, we output onehot logits corresponding to $y_{e}$. We experimented with scaling the one-hot logit by different factors, but found scaling by 1 to work well; scaling corresponds to changing the model's confidence in its edit prediction but doesn't change the prediction itself or the edit success.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ We justify this choice by the fact that the model's predictive distribution is similar to the locality sample distribution (as locality samples are drawn from the dataset the model was originally trained on). While this is not as principled as a true Monte Carlo estimate using samples from the model itself, it is reduces computational requirements of training and is easier to implement; the generally low drawdown for most models indicates that this approximation still provides a good locality constraint in practice.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
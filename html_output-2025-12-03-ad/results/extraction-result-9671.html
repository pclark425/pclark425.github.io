<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9671 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9671</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9671</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-269137236</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.08008v2.pdf" target="_blank">Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition</a></p>
                <p><strong>Paper Abstract:</strong> Reliable evaluation of large language models (LLMs) is impeded by two key challenges: objective metrics often fail to reflect human perception of natural language, and exhaustive human labeling is prohibitively expensive. Here, we propose a sample-efficient human evaluation method for LLMs based on the principle of MAximum Discrepancy (MAD) Competition. Our method automatically and adaptively selects a compact set of input instructions that maximize semantic discrepancy between pairs of LLM responses. Human evaluators then perform three-alternative forced choices on these paired responses, which are aggregated into a global ranking using Elo rating. We apply our approach to compare eight widely used LLMs across four tasks: scientific knowledge understanding, mathematical reasoning, creative and functional writing, and code generation and explanation. Experimental results show that our sample-efficient evaluation method recovers"gold-standard"model rankings with a handful of MAD-selected instructions, reveals respective strengths and weaknesses of each LLM, and offers nuanced insights to guide future LLM development. Code is available at https://github.com/weiji-Feng/MAD-Eval .</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9671.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9671.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAD Competition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum Discrepancy (MAD) Competition for LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive sampling method that selects a small set of input instructions maximizing semantic discrepancy between pairs of LLM responses, thereby concentrating human judgments on maximally informative counterexamples for efficient model ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple (GPT-4-Turbo, GPT-3.5-Turbo, Gemini-Pro, OpenChat-3.5, WizardLM-13B, Vicuna-13B, ChatGLM3-6B, Qwen-14B-Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>A diverse set of proprietary and open-source instruction-following LLMs spanning small-to-large parameter scales and commercial vs open-source models (described in Appendix C of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM evaluation; tasks include scientific knowledge understanding, mathematical reasoning, creative & functional writing, code generation/explanation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Adaptive selection of instructions maximizing pairwise response discrepancy (embedding-based distance), present selected response pairs to human annotators for three-alternative forced choice (3-AFC), aggregate pairwise outcomes into global ranking with Elo rating and bootstrapping.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Maximize semantic discrepancy between model outputs while enforcing diversity across selected instructions (discrepancy D(·,·) + diversity term λD(x,I)); human judgments via 3-AFC for preference/tie; bootstrapped Elo aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to a large instruction pool of 120K synthesized instructions (30K per task) created from seed datasets (e.g., GSM8K, CAMEL, AlpacaEval, CodeAlpaca) via instruction evolution; compared against leaderboards such as Chatbot Arena, AlpacaEval-2.0, CompassRank and datasets like HumanEval and MATH500 for external validation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MAD Competition recovers gold-standard rankings with only a handful of MAD-selected instructions (K≈10 per pair). Spearman correlations with full human leaderboards exceed 0.98 in many settings; method outperforms random and several alternative sampling strategies and is robust to discrepancy-measure choice.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Brute-force search over large instruction pools is computationally heavy for millions of instructions; O(N^2 K) human comparisons remain costly at very large N; method does not explicitly model human cognitive load or per-comparison difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>MAD Competition concentrates human effort on maximally discriminative examples and reproduces large-scale human-evaluation rankings (e.g., Chatbot Arena) with dramatically fewer annotations, reducing redundancy compared to random sampling used in traditional crowdsourced battles.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use K ≈ 8–10 as a practical compromise; include a diversity term (λ ≈ 1.0) to avoid redundant prompts; bootstrapped Elo aggregation to reduce sensitivity to comparison order; treat K adaptively per pair (more comparisons for similar models).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9671.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9671.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elo+Bootstrapping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elo rating with bootstrap averaging for LLM ranking aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online pairwise-competition rating update (Elo) combined with bootstrap aggregation to produce robust global rankings from human pairwise comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple (see MAD Competition entry)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Same set of competing LLMs evaluated in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation aggregation / psychometric ranking</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Record 3-AFC outcomes per selected instruction pair, update pairwise Elo scores with parameters τ (rating scale) and η (learning rate), generate many bootstrap resamples of the judgments, compute Elo for each, and average across bootstraps to produce final scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pairwise human preferences aggregated into Elo updates; tie treated as 0.5; parameters used in experiments: τ = 400, η = 4 (consistent with Chatbot Arena); bootstrapping (1,000 resamples) to reduce order sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to the MAD-selected comparison set (e.g., 1,120 comparisons for 8 models × 4 tasks with K=10), and compared to Chatbot Arena rankings as a human-grounded baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Bootstrapped Elo yields stable global rankings with high fidelity to large-scale human leaderboards; reduced sensitivity to comparison order was observed via bootstrap averaging.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires a sufficient number of pairwise results to stabilize scores; O(N^2 K) comparisons scale can still be expensive for many models if not further optimized.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Matches Chatbot Arena's Elo-based aggregation with far fewer comparisons when combined with MAD selection; uses same underlying aggregation paradigm but with much smaller, targeted comparison sets.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Apply bootstrapping (e.g., 1,000 resamples) to mitigate order effects; tune τ and η consistent with prior human-battle protocols (e.g., Chatbot Arena) to maintain comparability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9671.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9671.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embedding-based Discrepancy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic discrepancy via text-embedding-ada-002 cosine dissimilarity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical discrepancy measure implemented by embedding each response with OpenAI's text-embedding-ada-002 and computing cosine dissimilarity to quantify semantic difference between model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple (embeddings computed externally with OpenAI embedding model)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>LLM responses from competing models are embedded with text-embedding-ada-002 to derive discrepancy scores.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Semantic similarity / discrepancy measurement for NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute embedding vectors for each response and use cosine dissimilarity as D(·,·) in MAD selection objective (with diversity regularizer).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Magnitude of embedding cosine dissimilarity used to rank candidate prompts for informativeness; compared in ablation to BERTScore and LLM-as-judge (GPT-4-Turbo) measures.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used on the full 120K instruction pool and on MAD-selected subsets; compared against BERTScore and GPT-4-Turbo judgment in ablation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Embedding-based cosine dissimilarity produced rankings nearly identical to BERTScore and GPT-4-Turbo judge across tasks (Spearman correlations > 0.95 with small K), demonstrating insensitivity to reasonable choices of discrepancy measure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Embedding distances can miss fine-grained factual or numeric differences (e.g., small arithmetic errors) and may inherit biases of embedding model; reliance on external embedding API may incur costs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Embedding-based discrepancy reliably predicts human-discriminative examples in practice and aligns well with human judgments when paired with human 3-AFC adjudication.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Negate similarity measures to obtain discrepancy scores; test alternative measures (BERTScore or an LLM judge) in domain-specific tasks to verify robustness; pair embeddings with diversity regularization to ensure coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9671.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9671.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3-AFC Human Protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Three-Alternative Forced Choice (3-AFC) human preference protocol</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human evaluation task where annotators choose between Model A win, Model B win, or Tie for a given instruction and paired responses; multiple annotators per pair produce majority-vote outcomes used in Elo updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple (responses compared from all competing LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Annotators compare outputs produced by any pair of competing LLMs for the same instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Human evaluation of generated text/code/explanations</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Present human evaluators with instruction + paired responses and ask for 'A win', 'Tie', or 'B win'; collect at least 5 annotations per pair; aggregate via majority vote and feed into bootstrapped Elo.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Annotators instructed to judge accuracy, relevance, and helpfulness (GUI) and evaluators selected to meet language and STEM proficiency criteria; inter-annotator agreement reported and ties handled as 0.5 in Elo updates.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to MAD-selected response pairs (e.g., 1,120 comparisons for eight models over four tasks with K=10); annotators were 13 STEM-trained graduate students, each pair labeled by ≥5 annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Average inter-annotator agreement = 83.39%; majority voting used to determine final outcome per pair; human judgments produced rankings aligning closely with large-scale leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human labeling remains costly and time-consuming; tasks vary in cognitive load and difficulty which was not explicitly modeled; crowd noise and domain expertise requirements limit scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>3-AFC is a human-grounded standard used in Chatbot Arena; here it is applied only to MAD-selected, highly informative examples, increasing sample efficiency versus random sampling in crowdsourced battles.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Select annotators with relevant STEM/language proficiency; require multiple annotators per pair (≥5) to compute majority vote; monitor inter-annotator agreement and consider pilot runs to estimate decision difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9671.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9671.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction Evolution & Pool</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated instruction evolution and large instruction pool construction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that starts from seed instructions (3K per task) drawn from established datasets and uses iterative model-guided evolution to produce a 30K per-task pool (120K total) covering diverse and realistic instruction types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>seeded by multiple closed-source models (GPT-4-Turbo, GPT-3.5-Turbo, Gemini-Pro) during evolution</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Instruction evolution used strong models to rewrite/adapt seed prompts and impose varied constraints to create realistic and varied instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Benchmark/instruction dataset construction for LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Seed from established benchmarks (GSM8K, CAMEL, AlpacaEval, CodeAlpaca), perform ten iterative evolutions per seed via prompting strong models to produce varied, constrained instructions and exemplar answers; yields 30K evolved instructions per task.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Breadth (diverse task types) and depth (practical/nuanced scenarios) sought; evolution prompts request exemplar answers for interpretability in some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Resulting pool X of 120K instructions (30K per task) used as the candidate set for MAD selection; seeds included GSM8K, CAMEL, AlpacaEval, CodeAlpaca as noted in Appendix B.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Large synthetic instruction pool enabled MAD to find discriminative examples across scientific understanding, math, writing, and coding; evolution produced realistic, failure-inducing prompts that reveal model weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Evolution leverages closed-source models and can introduce biases from those models into the pool; scale-up of evolution or pool size further increases computational cost for brute-force MAD search.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Contrasts with fixed, static benchmark test sets: evolution yields a dynamic, wide-ranging instruction distribution less prone to overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use diverse seed datasets and multiple evolution iterations; include exemplar answers for domains requiring interpretability; monitor for bias introduced by the evolver models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9671.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9671.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarks & Validation Sets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reference benchmarks and datasets used for validation and context</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of established benchmarks and leaderboards used either as seeds, validation, or comparison targets, including GSM8K, HumanEval, MATH500, Chatbot Arena, AlpacaEval-2.0, MMLU, and CompassRank.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple (benchmarks used to seed or validate across evaluated LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Benchmarks cover math word problems (GSM8K), code generation correctness (HumanEval), broader multi-task evaluation (MMLU), public human-battle arenas (Chatbot Arena), and automated LLM-based evaluation (AlpacaEval).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Benchmarking LLM capabilities across knowledge, reasoning, code, and dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Use GSM8K as seed for instruction evolution in math; compare MAD-derived rankings to leaderboards such as Chatbot Arena, AlpacaEval-2.0, CompassRank; compare code preferences to HumanEval pass@1 results.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Leaderboard/benchmark metrics include raw accuracy (e.g., GSM8K, MATH500), pass@1 for code (HumanEval), and Elo-based rankings for human preference leaderboards (Chatbot Arena).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>GSM8K (math), HumanEval (code), MATH500 (math subset), Chatbot Arena Conversations (human preference battles), AlpacaEval-2.0, CompassRank (metric-based leaderboard).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MAD-derived rankings align closely with external benchmarks: e.g., Spearman ρ ≈ 0.993 vs MATH500; close match to Chatbot Arena with only thousands of comparisons; model pass@1 rates reported (GPT-4-Turbo 76.83%, GPT-3.5-Turbo 74.39%, Gemini Pro 59.76%) align with human preferences for coding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Benchmarks can be gamed/overfitted and may not reflect human preferences for open-ended tasks; leaderboards differ in sampling and metric focus causing occasional rank discrepancies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>MAD provides a middle ground that uses human judgments but drastically reduces required human effort compared to full-scale crowdsourced battles and improves over purely metric-driven leaderboards by focusing on human perceptual discriminants.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Validate MAD rankings against multiple external leaderboards/datasets (e.g., HumanEval, MATH500, Chatbot Arena) to ensure cross-benchmark fidelity; use task-specific seeds (GSM8K for math) when constructing instruction pools.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9671.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9671.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Key Quantitative Results</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Main empirical findings and numeric outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Summarizes principal empirical outcomes demonstrating MAD's sample efficiency, ranking fidelity, and robustness across tasks and measures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4-Turbo; GPT-3.5-Turbo; Gemini-Pro; OpenChat-3.5; WizardLM-13B; Vicuna-13B; ChatGLM3-6B; Qwen-14B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Eight representative LLMs spanning proprietary and open-source systems evaluated under MAD with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM benchmarking across scientific knowledge, math, writing, and code</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>MAD-selected K=10 per-pair comparisons, human 3-AFC by 13 STEM-trained postgraduates (≥5 annotations per pair), bootstrapped Elo aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human preference wins/ties per pair, Elo ratings, Spearman correlations to external leaderboards, inter-annotator agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>MAD-selected comparisons drawn from a 120K instruction pool (seeds from GSM8K, CAMEL, AlpacaEval, CodeAlpaca); external validation against Chatbot Arena, AlpacaEval-2.0, CompassRank, HumanEval, MATH500.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MAD recovers gold-standard rankings with small K: Spearman correlations >0.95 even with as few as 5 comparisons; K=10 recommended. Agreement with Chatbot Arena nearly perfect except minor shifts (e.g., Vicuna-13B). Human inter-annotator agreement averaged 83.39%. Code pass@1 reported: GPT-4-Turbo 76.83%, GPT-3.5-Turbo 74.39%, Gemini-Pro 59.76%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Some rank discrepancies arise from differences in task sampling and benchmark focus; MAD's brute-force selection cost grows with pool size; human difficulty per comparison not modeled.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>MAD replicates full-scale human rankings with orders-of-magnitude fewer human comparisons, outperforming random sampling and alternative sampling strategies (KL, cross-entropy, Anchor Points, DiffUse) in ranking fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use K=8–10 for stable rankings; set diversity weight λ ≈ 1.0; verify robustness across multiple discrepancy measures; run bootstrap Elo aggregation and sanity-check against external benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring Massive Multitask Language Understanding <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>HumanEval <em>(Rating: 2)</em></li>
                <li>Maximum Discrepancy (MAD) Competition <em>(Rating: 2)</em></li>
                <li>Chatbot Arena: An open platform for evaluating LLMs by human preference <em>(Rating: 2)</em></li>
                <li>AlpacaEval: An automatic evaluator of instruction-following models <em>(Rating: 1)</em></li>
                <li>MATH500 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9671",
    "paper_id": "paper-269137236",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "MAD Competition",
            "name_full": "Maximum Discrepancy (MAD) Competition for LLM evaluation",
            "brief_description": "An adaptive sampling method that selects a small set of input instructions maximizing semantic discrepancy between pairs of LLM responses, thereby concentrating human judgments on maximally informative counterexamples for efficient model ranking.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "multiple (GPT-4-Turbo, GPT-3.5-Turbo, Gemini-Pro, OpenChat-3.5, WizardLM-13B, Vicuna-13B, ChatGLM3-6B, Qwen-14B-Chat)",
            "llm_description": "A diverse set of proprietary and open-source instruction-following LLMs spanning small-to-large parameter scales and commercial vs open-source models (described in Appendix C of the paper).",
            "scientific_domain": "LLM evaluation; tasks include scientific knowledge understanding, mathematical reasoning, creative & functional writing, code generation/explanation",
            "evaluation_method": "Adaptive selection of instructions maximizing pairwise response discrepancy (embedding-based distance), present selected response pairs to human annotators for three-alternative forced choice (3-AFC), aggregate pairwise outcomes into global ranking with Elo rating and bootstrapping.",
            "evaluation_criteria": "Maximize semantic discrepancy between model outputs while enforcing diversity across selected instructions (discrepancy D(·,·) + diversity term λD(x,I)); human judgments via 3-AFC for preference/tie; bootstrapped Elo aggregation.",
            "benchmark_or_dataset": "Applied to a large instruction pool of 120K synthesized instructions (30K per task) created from seed datasets (e.g., GSM8K, CAMEL, AlpacaEval, CodeAlpaca) via instruction evolution; compared against leaderboards such as Chatbot Arena, AlpacaEval-2.0, CompassRank and datasets like HumanEval and MATH500 for external validation.",
            "results_summary": "MAD Competition recovers gold-standard rankings with only a handful of MAD-selected instructions (K≈10 per pair). Spearman correlations with full human leaderboards exceed 0.98 in many settings; method outperforms random and several alternative sampling strategies and is robust to discrepancy-measure choice.",
            "limitations_or_challenges": "Brute-force search over large instruction pools is computationally heavy for millions of instructions; O(N^2 K) human comparisons remain costly at very large N; method does not explicitly model human cognitive load or per-comparison difficulty.",
            "comparison_to_human_or_traditional": "MAD Competition concentrates human effort on maximally discriminative examples and reproduces large-scale human-evaluation rankings (e.g., Chatbot Arena) with dramatically fewer annotations, reducing redundancy compared to random sampling used in traditional crowdsourced battles.",
            "recommendations_or_best_practices": "Use K ≈ 8–10 as a practical compromise; include a diversity term (λ ≈ 1.0) to avoid redundant prompts; bootstrapped Elo aggregation to reduce sensitivity to comparison order; treat K adaptively per pair (more comparisons for similar models).",
            "uuid": "e9671.0",
            "source_info": {
                "paper_title": "Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Elo+Bootstrapping",
            "name_full": "Elo rating with bootstrap averaging for LLM ranking aggregation",
            "brief_description": "An online pairwise-competition rating update (Elo) combined with bootstrap aggregation to produce robust global rankings from human pairwise comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "multiple (see MAD Competition entry)",
            "llm_description": "Same set of competing LLMs evaluated in the experiments.",
            "scientific_domain": "Evaluation aggregation / psychometric ranking",
            "evaluation_method": "Record 3-AFC outcomes per selected instruction pair, update pairwise Elo scores with parameters τ (rating scale) and η (learning rate), generate many bootstrap resamples of the judgments, compute Elo for each, and average across bootstraps to produce final scores.",
            "evaluation_criteria": "Pairwise human preferences aggregated into Elo updates; tie treated as 0.5; parameters used in experiments: τ = 400, η = 4 (consistent with Chatbot Arena); bootstrapping (1,000 resamples) to reduce order sensitivity.",
            "benchmark_or_dataset": "Applied to the MAD-selected comparison set (e.g., 1,120 comparisons for 8 models × 4 tasks with K=10), and compared to Chatbot Arena rankings as a human-grounded baseline.",
            "results_summary": "Bootstrapped Elo yields stable global rankings with high fidelity to large-scale human leaderboards; reduced sensitivity to comparison order was observed via bootstrap averaging.",
            "limitations_or_challenges": "Requires a sufficient number of pairwise results to stabilize scores; O(N^2 K) comparisons scale can still be expensive for many models if not further optimized.",
            "comparison_to_human_or_traditional": "Matches Chatbot Arena's Elo-based aggregation with far fewer comparisons when combined with MAD selection; uses same underlying aggregation paradigm but with much smaller, targeted comparison sets.",
            "recommendations_or_best_practices": "Apply bootstrapping (e.g., 1,000 resamples) to mitigate order effects; tune τ and η consistent with prior human-battle protocols (e.g., Chatbot Arena) to maintain comparability.",
            "uuid": "e9671.1",
            "source_info": {
                "paper_title": "Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Embedding-based Discrepancy",
            "name_full": "Semantic discrepancy via text-embedding-ada-002 cosine dissimilarity",
            "brief_description": "A practical discrepancy measure implemented by embedding each response with OpenAI's text-embedding-ada-002 and computing cosine dissimilarity to quantify semantic difference between model outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "multiple (embeddings computed externally with OpenAI embedding model)",
            "llm_description": "LLM responses from competing models are embedded with text-embedding-ada-002 to derive discrepancy scores.",
            "scientific_domain": "Semantic similarity / discrepancy measurement for NLP evaluation",
            "evaluation_method": "Compute embedding vectors for each response and use cosine dissimilarity as D(·,·) in MAD selection objective (with diversity regularizer).",
            "evaluation_criteria": "Magnitude of embedding cosine dissimilarity used to rank candidate prompts for informativeness; compared in ablation to BERTScore and LLM-as-judge (GPT-4-Turbo) measures.",
            "benchmark_or_dataset": "Used on the full 120K instruction pool and on MAD-selected subsets; compared against BERTScore and GPT-4-Turbo judgment in ablation studies.",
            "results_summary": "Embedding-based cosine dissimilarity produced rankings nearly identical to BERTScore and GPT-4-Turbo judge across tasks (Spearman correlations &gt; 0.95 with small K), demonstrating insensitivity to reasonable choices of discrepancy measure.",
            "limitations_or_challenges": "Embedding distances can miss fine-grained factual or numeric differences (e.g., small arithmetic errors) and may inherit biases of embedding model; reliance on external embedding API may incur costs.",
            "comparison_to_human_or_traditional": "Embedding-based discrepancy reliably predicts human-discriminative examples in practice and aligns well with human judgments when paired with human 3-AFC adjudication.",
            "recommendations_or_best_practices": "Negate similarity measures to obtain discrepancy scores; test alternative measures (BERTScore or an LLM judge) in domain-specific tasks to verify robustness; pair embeddings with diversity regularization to ensure coverage.",
            "uuid": "e9671.2",
            "source_info": {
                "paper_title": "Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "3-AFC Human Protocol",
            "name_full": "Three-Alternative Forced Choice (3-AFC) human preference protocol",
            "brief_description": "A human evaluation task where annotators choose between Model A win, Model B win, or Tie for a given instruction and paired responses; multiple annotators per pair produce majority-vote outcomes used in Elo updates.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "multiple (responses compared from all competing LLMs)",
            "llm_description": "Annotators compare outputs produced by any pair of competing LLMs for the same instruction.",
            "scientific_domain": "Human evaluation of generated text/code/explanations",
            "evaluation_method": "Present human evaluators with instruction + paired responses and ask for 'A win', 'Tie', or 'B win'; collect at least 5 annotations per pair; aggregate via majority vote and feed into bootstrapped Elo.",
            "evaluation_criteria": "Annotators instructed to judge accuracy, relevance, and helpfulness (GUI) and evaluators selected to meet language and STEM proficiency criteria; inter-annotator agreement reported and ties handled as 0.5 in Elo updates.",
            "benchmark_or_dataset": "Applied to MAD-selected response pairs (e.g., 1,120 comparisons for eight models over four tasks with K=10); annotators were 13 STEM-trained graduate students, each pair labeled by ≥5 annotators.",
            "results_summary": "Average inter-annotator agreement = 83.39%; majority voting used to determine final outcome per pair; human judgments produced rankings aligning closely with large-scale leaderboards.",
            "limitations_or_challenges": "Human labeling remains costly and time-consuming; tasks vary in cognitive load and difficulty which was not explicitly modeled; crowd noise and domain expertise requirements limit scalability.",
            "comparison_to_human_or_traditional": "3-AFC is a human-grounded standard used in Chatbot Arena; here it is applied only to MAD-selected, highly informative examples, increasing sample efficiency versus random sampling in crowdsourced battles.",
            "recommendations_or_best_practices": "Select annotators with relevant STEM/language proficiency; require multiple annotators per pair (≥5) to compute majority vote; monitor inter-annotator agreement and consider pilot runs to estimate decision difficulty.",
            "uuid": "e9671.3",
            "source_info": {
                "paper_title": "Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Instruction Evolution & Pool",
            "name_full": "Automated instruction evolution and large instruction pool construction",
            "brief_description": "A pipeline that starts from seed instructions (3K per task) drawn from established datasets and uses iterative model-guided evolution to produce a 30K per-task pool (120K total) covering diverse and realistic instruction types.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "seeded by multiple closed-source models (GPT-4-Turbo, GPT-3.5-Turbo, Gemini-Pro) during evolution",
            "llm_description": "Instruction evolution used strong models to rewrite/adapt seed prompts and impose varied constraints to create realistic and varied instructions.",
            "scientific_domain": "Benchmark/instruction dataset construction for LLM evaluation",
            "evaluation_method": "Seed from established benchmarks (GSM8K, CAMEL, AlpacaEval, CodeAlpaca), perform ten iterative evolutions per seed via prompting strong models to produce varied, constrained instructions and exemplar answers; yields 30K evolved instructions per task.",
            "evaluation_criteria": "Breadth (diverse task types) and depth (practical/nuanced scenarios) sought; evolution prompts request exemplar answers for interpretability in some tasks.",
            "benchmark_or_dataset": "Resulting pool X of 120K instructions (30K per task) used as the candidate set for MAD selection; seeds included GSM8K, CAMEL, AlpacaEval, CodeAlpaca as noted in Appendix B.",
            "results_summary": "Large synthetic instruction pool enabled MAD to find discriminative examples across scientific understanding, math, writing, and coding; evolution produced realistic, failure-inducing prompts that reveal model weaknesses.",
            "limitations_or_challenges": "Evolution leverages closed-source models and can introduce biases from those models into the pool; scale-up of evolution or pool size further increases computational cost for brute-force MAD search.",
            "comparison_to_human_or_traditional": "Contrasts with fixed, static benchmark test sets: evolution yields a dynamic, wide-ranging instruction distribution less prone to overfitting.",
            "recommendations_or_best_practices": "Use diverse seed datasets and multiple evolution iterations; include exemplar answers for domains requiring interpretability; monitor for bias introduced by the evolver models.",
            "uuid": "e9671.4",
            "source_info": {
                "paper_title": "Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Benchmarks & Validation Sets",
            "name_full": "Reference benchmarks and datasets used for validation and context",
            "brief_description": "A set of established benchmarks and leaderboards used either as seeds, validation, or comparison targets, including GSM8K, HumanEval, MATH500, Chatbot Arena, AlpacaEval-2.0, MMLU, and CompassRank.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_name": "multiple (benchmarks used to seed or validate across evaluated LLMs)",
            "llm_description": "Benchmarks cover math word problems (GSM8K), code generation correctness (HumanEval), broader multi-task evaluation (MMLU), public human-battle arenas (Chatbot Arena), and automated LLM-based evaluation (AlpacaEval).",
            "scientific_domain": "Benchmarking LLM capabilities across knowledge, reasoning, code, and dialogue",
            "evaluation_method": "Use GSM8K as seed for instruction evolution in math; compare MAD-derived rankings to leaderboards such as Chatbot Arena, AlpacaEval-2.0, CompassRank; compare code preferences to HumanEval pass@1 results.",
            "evaluation_criteria": "Leaderboard/benchmark metrics include raw accuracy (e.g., GSM8K, MATH500), pass@1 for code (HumanEval), and Elo-based rankings for human preference leaderboards (Chatbot Arena).",
            "benchmark_or_dataset": "GSM8K (math), HumanEval (code), MATH500 (math subset), Chatbot Arena Conversations (human preference battles), AlpacaEval-2.0, CompassRank (metric-based leaderboard).",
            "results_summary": "MAD-derived rankings align closely with external benchmarks: e.g., Spearman ρ ≈ 0.993 vs MATH500; close match to Chatbot Arena with only thousands of comparisons; model pass@1 rates reported (GPT-4-Turbo 76.83%, GPT-3.5-Turbo 74.39%, Gemini Pro 59.76%) align with human preferences for coding.",
            "limitations_or_challenges": "Benchmarks can be gamed/overfitted and may not reflect human preferences for open-ended tasks; leaderboards differ in sampling and metric focus causing occasional rank discrepancies.",
            "comparison_to_human_or_traditional": "MAD provides a middle ground that uses human judgments but drastically reduces required human effort compared to full-scale crowdsourced battles and improves over purely metric-driven leaderboards by focusing on human perceptual discriminants.",
            "recommendations_or_best_practices": "Validate MAD rankings against multiple external leaderboards/datasets (e.g., HumanEval, MATH500, Chatbot Arena) to ensure cross-benchmark fidelity; use task-specific seeds (GSM8K for math) when constructing instruction pools.",
            "uuid": "e9671.5",
            "source_info": {
                "paper_title": "Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Key Quantitative Results",
            "name_full": "Main empirical findings and numeric outcomes",
            "brief_description": "Summarizes principal empirical outcomes demonstrating MAD's sample efficiency, ranking fidelity, and robustness across tasks and measures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4-Turbo; GPT-3.5-Turbo; Gemini-Pro; OpenChat-3.5; WizardLM-13B; Vicuna-13B; ChatGLM3-6B; Qwen-14B-Chat",
            "llm_description": "Eight representative LLMs spanning proprietary and open-source systems evaluated under MAD with human judgments.",
            "scientific_domain": "LLM benchmarking across scientific knowledge, math, writing, and code",
            "evaluation_method": "MAD-selected K=10 per-pair comparisons, human 3-AFC by 13 STEM-trained postgraduates (≥5 annotations per pair), bootstrapped Elo aggregation.",
            "evaluation_criteria": "Human preference wins/ties per pair, Elo ratings, Spearman correlations to external leaderboards, inter-annotator agreement.",
            "benchmark_or_dataset": "MAD-selected comparisons drawn from a 120K instruction pool (seeds from GSM8K, CAMEL, AlpacaEval, CodeAlpaca); external validation against Chatbot Arena, AlpacaEval-2.0, CompassRank, HumanEval, MATH500.",
            "results_summary": "MAD recovers gold-standard rankings with small K: Spearman correlations &gt;0.95 even with as few as 5 comparisons; K=10 recommended. Agreement with Chatbot Arena nearly perfect except minor shifts (e.g., Vicuna-13B). Human inter-annotator agreement averaged 83.39%. Code pass@1 reported: GPT-4-Turbo 76.83%, GPT-3.5-Turbo 74.39%, Gemini-Pro 59.76%.",
            "limitations_or_challenges": "Some rank discrepancies arise from differences in task sampling and benchmark focus; MAD's brute-force selection cost grows with pool size; human difficulty per comparison not modeled.",
            "comparison_to_human_or_traditional": "MAD replicates full-scale human rankings with orders-of-magnitude fewer human comparisons, outperforming random sampling and alternative sampling strategies (KL, cross-entropy, Anchor Points, DiffUse) in ranking fidelity.",
            "recommendations_or_best_practices": "Use K=8–10 for stable rankings; set diversity weight λ ≈ 1.0; verify robustness across multiple discrepancy measures; run bootstrap Elo aggregation and sanity-check against external benchmarks.",
            "uuid": "e9671.6",
            "source_info": {
                "paper_title": "Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring Massive Multitask Language Understanding",
            "rating": 2,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "HumanEval",
            "rating": 2
        },
        {
            "paper_title": "Maximum Discrepancy (MAD) Competition",
            "rating": 2,
            "sanitized_title": "maximum_discrepancy_mad_competition"
        },
        {
            "paper_title": "Chatbot Arena: An open platform for evaluating LLMs by human preference",
            "rating": 2,
            "sanitized_title": "chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference"
        },
        {
            "paper_title": "AlpacaEval: An automatic evaluator of instruction-following models",
            "rating": 1,
            "sanitized_title": "alpacaeval_an_automatic_evaluator_of_instructionfollowing_models"
        },
        {
            "paper_title": "MATH500",
            "rating": 1
        }
    ],
    "cost": 0.01927325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition
29 May 2025</p>
<p>Kehua Feng kehuafeng@zju.edu.cn 
Zhejiang University</p>
<p>Keyan Ding dingkeyan@zju.edu.cn 
Zhejiang University</p>
<p>ZJU-Hangzhou Global Scientific and Technological Innovation Center</p>
<p>Hongzhi Tan 
Shanghai Electric Group Co., Ltd. Central Academe</p>
<p>Kede Ma 
City University of Hong Kong</p>
<p>Zhihua Wang 
City University of Hong Kong</p>
<p>Shuangquan Guo 
Shanghai Electric Group Co., Ltd. Central Academe</p>
<p>Yuzhou Cheng 
Shanghai Institute for Advanced Study of Zhejiang University 6 Zhoushan-Zhejiang University Ocean Research Center</p>
<p>Guozhou Zheng 
Shanghai Institute for Advanced Study of Zhejiang University 6 Zhoushan-Zhejiang University Ocean Research Center</p>
<p>Qiang Zhang 
Zhejiang University</p>
<p>ZJU-Hangzhou Global Scientific and Technological Innovation Center</p>
<p>Huajun Chen huajunsir@zju.edu.cn 
Zhejiang University</p>
<p>ZJU-Hangzhou Global Scientific and Technological Innovation Center</p>
<p>Rohan Taori 
Ishaan Gulrajani 
Tianyi Zhang 
Yann Dubois 
Xuechen Li 
Carlos Guestrin 
Percy Liang 
Tatsunori B Hashimoto 
Stan 
Gemini Team 
Rohan Anil 
Sebastian Borgeaud 
Yonghui Wu 
Jean-Baptiste Alayrac 
Jiahui Yu 
Radu Soricut 
Johan Schalkwyk 
Andrew M Dai 
Hugo Touvron 
Thibaut Lavril 
Gautier Izacard 
Xavier Martinet 
Marie-Anne Lachaux 
Timothée Lacroix 
Baptiste Rozière 
Naman Goyal 
Eric Hambro 
Tianlu Wang 
XiaoqingPing Yu 
Ellen Tan 
Sean O'brien 
Ramakanth Pasunuru 
Jane Dwivedi-Yu 
Olga Golovneva 
Luke Zettlemoyer 
Maryam Fazel- Zarandi 
ZJU-Hangzhou Global Scientific and Technological Innovation Center</p>
<p>Asli 2023d Celikyilmaz 
Yidong Wang 
Zhuohao Yu 
Zhengran Zeng 
Linyi Yang 
Cunxiang Wang 
Hao Chen 
Chaoya Jiang 
Rui Xie 
Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition
29 May 20257FCFFA8105603111A562D1D4F946C22DarXiv:2404.08008v2[cs.LG]GPT-3.5-Turbo
Reliable evaluation of large language models (LLMs) is impeded by two key challenges: objective metrics often fail to reflect human perception of natural language, and exhaustive human labeling is prohibitively expensive.Here, we propose a sample-efficient human evaluation method for LLMs based on the principle of MAximum Discrepancy (MAD) Competition.Our method automatically and adaptively selects a compact set of input instructions that maximize semantic discrepancy between pairs of LLM responses.Human evaluators then perform three-alternative forced choices on these paired responses, which are aggregated into a global ranking using Elo rating.We apply our approach to compare eight widely used LLMs across four tasks: scientific knowledge understanding, mathematical reasoning, creative and functional writing, and code generation and explanation.Experimental results show that our sample-efficient evaluation method recovers "gold-standard" model rankings with a handful of MAD-selected instructions, reveals respective strengths and weaknesses of each LLM, and offers nuanced insights to guide future LLM development.Code is available at https: //github.com/weiji-Feng/MAD-Eval.</p>
<p>Introduction</p>
<p>Since the advent of ChatGPT, there has been an unprecedented surge in the development of large language models (LLMs) (Touvron et al., 2023;Bai et al., 2023;OpenAI, 2023;Jiang et al., 2023;Team et al., 2023;Chen, 2023;Azaria et al., 2024), driven by selfsupervised pretraining (Jaiswal et al., 2020), supervised fine-tuning (Wang et al., 2022;Chiang et al., 2023;Xu et al., 2023) and reinforcement learning (Ouyang et al., 2022).These models now exhibit remarkable generalpurpose capabilities in language generation, understanding, and reasoning.Yet, with so many "competitive" LLMs emerging in rapid succession, establishing a reliable, scalable evaluation paradigm that reveals their strengths and weaknesses has become critical (Guo et al., 2023;Li et al., 2023c;Chang et al., 2024).</p>
<p>Traditional evaluation relies on fixed, humanannotated benchmarks-such as MMLU (Hendrycks et al., 2020), C-Eval (Huang et al., 2023) and BIGbench (Srivastava et al., 2022)-and on objective metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004).However, human annotation is costly and slow, and consequently, these benchmarks can only cover a narrow slice of possible task scenarios.Moreover, standard metrics frequently misalign with human perception of natural language (e.g., faithfulness, fluency, creativity, and semantic equivalence).More importantly, repeated testing on such static benchmarks may invite overfitting (Schaeffer, 2023;Zhou et al., 2023c;Grattafiori et al., 2024), aptly encapsulated by Goodhart's Law (Elton, 2004), suggesting that LLMs learn to game specific benchmarks rather than truly improve.</p>
<p>An alternative is to enlist powerful LLMs themselves as automated judges (Zhang et al., 2023b;Zeng et al., 2023), known as LLM-as-a-judge.For instance, systems such as LIMA (Zhou et al., 2023a) and Alpaca-Farm (Dubois et al., 2023) leverage proprietary models like GPT-4 as the judge through API calls, while open-source evaluators, e.g., PandaLM (Wang et al., 2023e), Shepherd (Wang et al., 2023d), AUTO-J (Li et al., 2023b), Prometheus (Kim et al., 2023(Kim et al., , 2024)), SaMer (Feng et al., 2025), and J1 (Whitehouse et al., 2025) have also been proposed.Although these LLMbased judges offer speed and interpretability, they also come with various biases (Zhu et al., 2023;Wang et al., 2023c;Chen et al., 2024) toward certain positions (i.e., position bias), specific formats (i.e., format bias), lengthy outputs (i.e., verbosity bias), polished responses (i.e., beauty bias), familiar content encountered during training (i.e., knowledge bias), or self-generated answers (i.e., self-enhancement bias).Additionally, they often struggle in specialized domains like mathematical reasoning and scientific comprehension.Despite these advances, human evaluation remains the gold standard as natural language is created and consumed by humans.Platforms like Chatbot Arena (Chi-</p>
<p>The autumn leaves fall softly to the ground A chill in the air, a feeling profound The trees stand bare, their branches abound The days grow shorter, the nights grow long The leaves turn red, their colors belong The world prepares for winter's strong … (16 lines)</p>
<p>Beneath the vast and starry sky so grand, (A) We ponder thoughts that time cannot erase, (B) Where dreams take flight on winds of night unplanned.(A) The whispers of the dark in silence stand, (A) As shadows weave within their moonlit lace, (B) Beneath the vast and starry sky so grand.(A) … (19 lines) Draft a villanelle (19-lines poem) adhering to the traditional ABA rhyme scheme.</p>
<p>Figure 1: Overview of the proposed sample-efficient human evaluation method for comparing LLMs adaptively.Starting from a small set of task-specific seed instructions, we apply an instruction evolution procedure to generate a large-scale pool of diverse instructions.For any two competing LLMs, we then conduct MAD Competition to automatically and adaptively select the top-K instructions (and their corresponding responses) that most effectively distinguish model behaviors.These selected response pairs are presented to human evaluators (along with the input instruction), who express pairwise preferences.Finally, we feed these comparison outcomes into an Elo rating system to produce a global ranking of all evaluated LLMs. ang et al., 2024) allow crowdworkers to compare model outputs directly, but they scale poorly in terms of time and cost.This raises a fundamental question: How can we automatically select the smallest, most informative set of instructions1 from an essentially infinite pool, so that human judgments yield a definitive performance ranking of LLMs with minimal annotation effort?</p>
<p>In this work, we answer this question by leveraging MAximum Discrepancy (MAD) Competition (Wang and Simoncelli, 2008) from the fields of computational vision and software testing.Starting from a large, selfgenerated, and unlabeled instruction pool, our method automatically identifies a minimum set of test instructions that maximize the semantic discrepancy of pairs of LLM responses (i.e., where the models disagree the most), and presents only these potential counterexamples to human evaluators.By enforcing diversity among selected instructions, we ensure coverage of varied failure modes.Finally, we aggregate the resulting human preferences via Elo rating (Elo and Sloan, 1978) to a reliable global ranking.We demonstrate our sample-efficient evaluation approach across four tasksscientific knowledge understanding, mathematical reasoning, creative and functional writing, and code generation and explanation-on eight widely used LLMs (Du et al., 2021;Ouyang et al., 2022;OpenAI, 2023;Team et al., 2023;Xu et al., 2023;Bai et al., 2023;Chiang et al., 2024) under a strict human-labeling budget.</p>
<p>Our contributions are threefold:</p>
<p>• Sample-efficient evaluation.We present a reliable, scalable method to automatically and adaptively select minimal yet maximally informative test instructions for LLM comparison.</p>
<p>• Multi-dimensional benchmarking.We apply our method across four distinct tasks, showing that reliable rankings emerge from only a handful of human judgments.</p>
<p>• Insights into model behaviors.By examining counterexamples that lead LLMs to fail, we reveal their relative strengths and weaknesses, offering guidance for future model improvement.</p>
<p>Related Work</p>
<p>Evaluation of LLMs has been addressed through three approaches: standardized benchmark evaluation, automated LLM-based evaluation, and human-centric evaluation.Each paradigm offers distinct insights into model capabilities, yet also bears methodological limitations.</p>
<p>Standardized Benchmark Evaluation</p>
<p>A rich ecosystem of standardized benchmarks has emerged to quantify LLM performance across knowledge, instruction-following, dialogue, preference alignment, and safety dimensions.Core knowledge evaluation, such as MMLU (Hendrycks et al., 2020), C-Eval (Huang et al., 2023), GPQA (Rein et al., 2023), and AGIEval (Zhong et al., 2023), use elementary to professional-level examination questions to test factual recall and reasoning under multiple-choice settings.Instruction-following suites like NaturalInstructions (Mishra et al., 2021), LLMBAR (Zeng et al., 2023), and IFEval (Zhou et al., 2023b) present openended tasks formulated in natural language, measuring a model's ability to execute diverse directives.Conversational benchmarks-xDial-Eval (Zhang et al., 2023a), MT-Bench (Zheng et al., 2023), LongMemEval (Wu et al., 2024), and MultiChallenge (Sirdeshmukh et al., 2025)-simulate multi-turn dialogues to evaluate coherence, context retention, and response appropriateness in conversational interactions.Human preference collections such as HHH Alignment (Askell et al., 2021), PPE (Frick et al., 2024), and JudgeBench (Tan et al., 2024) gather pairwise ranking judgments to directly compare model outputs.Safety and robustness tests like AdvGLUE (Wang et al., 2021), DecodingTrust (Wang et al., 2023a), and SG-Bench (Mou et al., 2024) probe adversarial vulnerabilities and the propensity for harmful content generation.Despite their breadth, these benchmarks often depend on objective metrics (e.g., BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and BERTScore (Zhang et al., 2019)), which correlate weakly with human judgments in open-ended or specialized tasks (Novikova et al., 2017;Wei et al., 2024).Furthermore, repeated public exposure leads to data contamination and overfitting, while the static nature of fixed test sets limits adaptation to emerging use cases.</p>
<p>Automated LLM-based Evaluation</p>
<p>Inspired by the strong instruction-following capabilities of closed-source models such as GPT-4, several recent studies (Zeng et al., 2023;Chan et al., 2023;Zhou et al., 2023a;Zheng et al., 2023;Dubois et al., 2023) have repurposed these advanced LLMs as automated judges.In parallel, the community has begun to develop opensource alternatives to reduce reliance on black-box systems.PandaLM (Wang et al., 2023e) fine-tunes LLaMA (Touvron et al., 2023) to perform pairwise comparisons and generate brief justifications.Shepard (Wang et al., 2023d) produces critiques across diverse questionanswer scenarios, while Prometheus (Kim et al., 2023) incorporates a thousand fine-grained scoring rubrics during training, enabling customization to specific evaluation criteria.AUTO-J (Li et al., 2023b) is trained on user queries paired with LLM-generated responses under fifty-eight real-world scenarios, yielding both critiques and numerical ratings.More recently, SaMer introduces a scenario-aware, multi-dimensional method that delivers both overall scores and fine-grained feedback on LLM replies (Feng et al., 2025).Despite their flexibility, automated evaluators remain susceptible to various biases, and their lack of domain-specific expertise can undermine reliability in specialized contexts (Zheng et al., 2023;Chen et al., 2024).</p>
<p>Human-Centric Evaluation</p>
<p>Human evaluation persists as the gold standard for judging LLMs, particularly when aligning model outputs with subjective preferences.Platforms like Chatbot Arena (Chiang et al., 2024) employ large-scale, crowdsourced "battles," in which human evaluators compare two chatbots on randomly drawn real-world instructions to produce an Elo-based ranking.While this approach can yield robust aggregation results, it does not optimize instructions to maximally differentiate competing LLMs, leading to redundant or trivial comparisons and, consequently, wasted human effort.Furthermore, without an explicit mechanism to ensure coverage of diverse failure modes, random instruction sampling risks both sampling bias and under-representation of important edge cases.Additionally, running thousands of battles at scale incurs substantial time and financial costs, and crowdsourced judgments introduce noise and variability that must be countered by still more annotations.Dynabench offers another paradigm of "adversarial" data collection, requiring users to manually submit potential counterexamples that expose model weaknesses (Kiela et al., 2021).To mitigate these challenges, our study proposes an automated, adaptive sampling strategy based on MAD Competition.</p>
<p>Proposed Sample-Efficient Human Evaluation Method</p>
<p>In this section, we present our sample-efficient human evaluation method for comparing LLMs adaptively.</p>
<p>Building on the principle of MAD Competition (Wang and Simoncelli, 2008), our approach automatically selects a small yet highly informative subset of test instructions that maximally differentiate model behaviors.</p>
<p>Human evaluators then perform pairwise comparisons on the selected response pairs, and we aggregate their judgments into a global ranking via Elo rating.The overall pipeline is depicted in Figure 1.</p>
<p>Problem Formulation</p>
<p>Let X denote a large, unlabeled pool of instructions, assembled from diverse, practical natural language processing tasks.We wish to compare N competing LLMs, denoted as
F = {f n } N n=1
, where each f n produces an output y n = f n (x) for any x ∈ X .Human evaluators operate in a subjective assessment environment H, wherein they can reliably judge the relative quality of two responses for the same input.Under a strict budget on the number of human annotations, our goal is to produce a definitive, global ranking of the LLMs in F based on only a handful of carefully chosen comparisons.</p>
<p>MAD Competition for LLMs</p>
<p>Let us first consider a simple case in which we compare two LLMs f i and f j .According to the principle of MAD Competition, we seek the instruction
x = argmax x∈X D(f i (x), f j (x)),(1)
where D(•, •) represents a distance measure that quantifies the semantic discrepancy between two responses.The comparative analysis of f i (x) versus f j (x) can produce three distinct outcomes:
• H (f i (x), f j (x)) ≈ 1.
The majority of human evaluators prefer f i (x) over f j (x), making f i (x) the clear winner.The chosen x thus serves as a counterexample for f j and is highly informative for ranking the relative performance of two models.</p>
<p>• H (f i (x), f j (x)) ≈ 0. Conversely, f j prevails, indicating that human evaluators overwhelmingly favor f j (x) over f i (x).Again, x functions as a counterexample, this time for f i , and maximally discriminates between the two models.</p>
<p>• H(f i (x), f j (x)) ≈ 0.5.Here, the evaluators assign similar ratings to both responses, resulting in a tie.Such ties fall into two subcategories:</p>
<p>-High-rating tie.
x(k) = argmax x∈X \I D(f i (x), f j (x)) + λD(x, I),(2) whereI = {x (k ′ ) } K−1 k ′ =1
is the set of previously chosen instructions, D(x, I) measures the maximum discrepancy between x and any element in I, and λ &gt; 0 balances discrepancy versus diversity.Once choosing x(k) , we add it to I before proceeding to the next iteration.</p>
<p>By iterating this procedure for all N 2 model pairs and retaining K instructions per pair, we construct a MAD response set:
R = {{{f i (x (k) ), f j (x (k) )} K k=1 } N −1 i=1 } N j=i+1 ,(3)
whose size grows only with O(N 2 K), independent of the instruction pool |X |.</p>
<p>Ranking Aggregation via Elo Rating</p>
<p>For each selected instruction x(k) and its paired outputs {f i (x (k) ), f j (x (k) )}, human evaluators perform a threealternative forced choice (3-AFC) task, whose outcome is recorded as
w =      1, if f i wins 0, if f j wins 0.5, otherwise.(4)
Elo score (Elo and Sloan, 1978) updates at the t-th comparison:
s (t) i = s (t−1) i + η w (t) ij − 1 1 + 10 −d (t−1) ij(5)
and
s (t) j = s (t−1) j + η 1 − w (t) ij − 1 1 + 10 d (t−1) ij ,(6)d (t−1) ij = s (t−1) i − s (t−1) j τ .(7)
η controls the learning rate and τ sets the rating scale.w
(t)
ij is the result of the t-th human comparison of f i and f j as defined in Eq. (4).s (0) = {s (0) n } N n=1 are the initial ranking scores of N LLMs.To mitigate the sensitivity of the online linear update to comparison order, we apply a bootstrapping procedure as suggested in (Chiang et al., 2024).Specifically, we generate a thousand bootstrap datasets by sampling with replacement from the human judgments, each dataset being the same size as the original.For each bootstrap dataset, we compute the Elo ratings and then average these ratings across all datasets to produce our final ranking.The full procedure is summarized in Algorithm 1.</p>
<p>Incorproating New LLMs</p>
<p>Integrating an additional LLM f N +1 into MAD Competition is both straightforward and cost-effective.The identified MAD response set R, along with their associated human preferences, remains unchanged.One needs only to sample a new collection of N × K instructionresponse pairs to accentuate behavioral differences between f N +1 and F = {f n } N n=1 , obtain human preferences on these new pairs, and then update the global ranking via Eqs.(5) to (7).The complete procedure for adding a new LLM is detailed in Algorithm 2.</p>
<p>Experiments</p>
<p>In this section, we demonstrate the effectiveness of our sample-efficient human evaluation method on eight widely used LLMs across four distinct tasks.We first describe our experimental setups (Section 4.1), then present the resulting model rankings (Section 4.2), and
∈ R (N +1)×1 1 R ← ∅ 2 Generate responses {fN+1(x)|x ∈ X } 3 for i ← 1 to N do 4 I ← ∅ // fi versus fN+1 5 for k ← 1 to K do 6
Select x(k) ∈ X \ I by solving Eq. ( 2)
7 I ← I ∪ x(k) 8 R ← R ∪ {fi(x (k) ), fN+1(x (k) )}
9 end 10 end 11 Collect human judgments on R via 3-AFC 12 Compute s via Elo rating with bootstrapping, starting from s compare these rankings against established leaderboards (Section 4.3).We further evaluate the reliability and efficiency of our method against alternatives (Section 4.4), under different key hyperparameter configurations (Section 4.5), and with extended experimentation in realworld scenarios (Section 4.6).</p>
<p>Experimental Setups</p>
<p>Instruction pool construction We curate a largescale, diverse pool X of 120K natural language instructions, spanning four core tasks: scientific knowledge understanding, mathematical reasoning, creative and functional writing, and code generation and explanation (see Figure 2).For each task, we begin with 3K seed instructions drawn from established datasets (i.e., GSM8K (Cobbe et al., 2021), CAMEL (Li et al., 2023a), AlpacaEval (Li et al., 2023d), and CodeAlpaca (Chaudhary, 2023)) and apply ten rounds of automated instruction evolution (Xu et al., 2023) to synthesize 30K "realistic" instructions that mimic real-world human-chatbot interactions (see Appendix B for more details).This ensures both breadth across task types and depth in covering potential failure modes.</p>
<p>Model selection Under a strict human annotation budget, we evaluate eight widely used LLMs, containing three proprietary models: GPT-3.5-Turbo(Ouyang et al., 2022), GPT-4-Turbo (OpenAI, 2023) and Gemini-Pro (Team et al., 2023), and five open-source models: ChatGLM3-6B (Du et al., 2021), WizardLM-13B (Xu et al., 2023), Vicuna-13B (Chiang et al., 2023), OpenChat-3.5 (Wang et al., 2023b), and Qwen-14B-Chat (Bai et al., 2023) (see Appendix C for their implementations).These span the current spectrum of performance and parameter scales, allowing us to assess both cutting-edge and accessible systems.</p>
<p>Semantic discrepancy measure To quantify the semantic discrepancy between paired responses, we em-  bed each using the OpenAI text-embedding-ada-002 model and compute cosine dissimilarity (Zhang et al., 2019).This embedding-based measure to implement D(•, •) in Eq. ( 2) reliably captures semantic content discrepancy, and is less prone to model-specific biases.</p>
<p>Human preference collection For each unordered model pair (f i , f j ), we select K = 10 instructions via MAD Competition, yielding 8 2 × 10 = 280 pairwise comparisons.Thirteen STEM-trained postgraduates perform 3-AFC tasks on each paired response, indicating "win," or "tie."These results are then aggregated via Elo rating.More details are given in Appendix D.</p>
<p>Main Results</p>
<p>We summarize the evaluation outcomes in Table 1, reporting both overall and task-specific rankings.These results illuminate distinct performance patterns across four core tasks.</p>
<p>For scientific knowledge understanding, proprietary models-GPT-4-Turbo, GPT-3.5-Turbo, and Gemini-Pro-dominate this task, reflecting their precise command of domain concepts and robust application of scientific theorems.Remarkably, OpenChat-3.5 (with only 7B parameters) attains a higher ranking than GPT-3.5-Turboby offering concise yet thorough explanation.In contrast, larger open-source models (e.g., Vicuna-13B) tend to generate redundantly detailed responses.</p>
<p>For mathematical reasoning, our ranking results align closely with the GSM8K leaderboard (Cobbe et al., 2021), reflecting the source of our instruction pool.Differences in model outputs arise from two primary factors: (1) divergent reasoning paths and (2) arithmetic inaccuracies despite similar reasoning.First, because our generated instructions target problem types and difficulty levels typical of grade-school mathematics, models generally follow singular, concise solution paths; thus, substantial deviations in final answers correspond to noticeably different reasoning trajectories.Second, variations in each model's computational fidelity can introduce errors in intermediate steps, so even when following analogous logical procedures, minor arithmetic mistakes may propagate to yield incorrect results.Among the models, WizardLM-13B performs relatively poorly.We attribute this to its instruction-evolution training, which uses Alpaca-52K seed data that is not specifically optimized for mathematical tasks (Taori et al., 2023).Vicuna-13B exhibits a similar limitation.</p>
<p>For creative and functional writing, the MADselected instructions are mostly open-ended, such as "compose a short story" or "craft a holiday recipe" that invites free-form expression.Consequently, human evaluators consistently favor models that produce longer, more richly detailed outputs over those with terser responses.For example, ChatGLM3-6B generates an average of 221.2 words per response, whereas GPT-4-Turbo delivers roughly 454.8 words.This greater verbosity not only provides more descriptive content but also often reflects deeper insights, which in turn drives higher user preference.</p>
<p>For code generation and explanation, human evaluators consider not only the functional correctness of the code but also its fidelity to the given instructions, such as respecting specified line limits, employing designated Python libraries, and conforming to the intended application contexts.We find that LLMs exhibit greater variability in code generation tasks than in code explanation, reflecting the complex interplay between problem specification and implementation.Notably, our human preference rankings correspond closely to established coding benchmarks.For instance, GPT-4-Turbo (76.83% pass@1), GPT-3.5-Turbo(74.39% pass@1), and Gemini Pro (59.76% pass@1) achieve the highest accuracies on HumanEval (Akter et al., 2023) and are likewise favored in our evaluation method.This alignment underscores the validity of our method to assess LLM performance in realistic coding scenarios.</p>
<p>Table 9 in the Appendix summarizes each model's strengths and weaknesses across the four evaluated tasks, yielding actionable insights for enhancing response quality.Appendix E further presents a series of case studies-most notably counterexamples to the otherwise high-performing GPT-4-Turbo-that empirically validate these observations.Moreover, the failure instances uncovered by MAD Competition and confirmed via human evaluation constitute a valuable corpus for developing more reliable LLMs, for example by integrating them into an active learning paradigm (Sinha et al., 2019).</p>
<p>Comparison with Established Leaderboards</p>
<p>To validate the reliability of our sample-efficient human evaluation method, we compare its global rankings against three prominent LLM leaderboards: (1) Chatbot Arena2 , which aggregates large-scale human preference "battles" via Elo rating; (2) AlpacaEval-2.03, which leverages LLM-based judges to score open-ended instruction-following abilities; and (3) CompassRank (Nov.2024 snapshot)4 , which measures performance using standard objective metrics (see Figure 3).</p>
<p>Chatbot Arena draws on extensive, crowdsourced pairwise human judgments, aggregating them via Elo rating to establish a "gold standard" benchmark for LLM evaluation.Remarkably, our sample-efficient approach, though engaging only thousands of model "battles," yields rankings that align almost perfectly with those from Chatbot Arena.The lone discrepancy, in Vicuna-13B's placement, is likely attributable to differences in task sampling, underscoring our method's ability to faithfully reproduce large-scale human evaluation with minimal annotation effort.</p>
<p>In AlpacaEval 2.0, WizardLM-13B is ranked above OpenChat-3.5 and GPT-3.5-Turbo, in contrast to our human-grounded results.This discrepancy stems from AlpacaEval's emphasis on assessing LLMs' instructionfollowing capabilities in unconstrained, open-ended tasks, whereas WizardLM-13B has been fine-tuned on 520K diverse instructions, thus yielding a relative advantage in instruction-following tasks.</p>
<p>When compared to CompassRank, we observe notable shifts for Qwen-14B-Chat and ChatGLM3-6B.Although fine-tuning on MMLU and HumanEval elevates their positions on metric-driven leaderboards, these results alone fail to reflect true human preferences.This gap underscores the necessity of integrating human judgments alongside quantitative metrics to achieve a more comprehensive appraisal of LLM performance.</p>
<p>Comparison with Alternative Sampling Strategies</p>
<p>To assess the effectiveness of our adaptive sampling strategy based on MAD Competition, we compare it against five alternatives: (1) uniform random sampling, (2) Kullback-Leibler (KL) divergence-based sampling, (3) cross-entropy-based sampling (Boubdir et al., 2023) qualitative analysis of the top-10 instructions chosen by each method in the creative writing task (see Table 18 in Appendix E).KL divergence overwhelmingly concentrates on near-homogeneous prompts-nine of ten requests involve poetry-whereas cross-entropy tends to favor academic tasks (e.g., paper outlines and story generation).Random sampling, by its nature, produces a noisy, unpredictable mix of task types with occasional redundancies.In contrast, MAD Competition explicitly incorporates a diversity term, ensuring that each selected instruction probes a distinct aspect of model behavior (see Table 19).This diversity-aware selection not only minimizes overlap in task categories but also exposes a broader spectrum of failure modes, thereby maximizing the informativeness of each human comparison.</p>
<p>Ablation Studies</p>
<p>Discrepacny measure sensitivity We compare our default text-embedding-ada-002 model-based cosine dissimilarity against BERTScore (Zhang et al., 2019) and an LLM-as-a-judge approach using GPT-4-Turbo5 (see Table 20 in the Appendix).Despite their different design principles, all three measures yield near-identical global rankings in the writing task (see Spearman rank correlations against the default ranking with K = 10.Even with as few as five comparisons, correlations exceed 0.95, reaching a perfect agreement for K ≥ 8 (see Figure 4).This indicates that reliable model rankings emerge from surprisingly small annotation budgets, and that K = 10 represents a practical compromise between cost and stability.Importantly, K is treated as a tunable parameter that can vary for each pair of competing LLMs.When two models demonstrate comparable performance, increasing K allows for additional head-to-head comparisons and thus yields a more robust ranking.Conversely, if one model clearly outperforms the other, it is efficient to reduce K (even to zero) to minimize human evaluation effort.This adaptivity parallels the flexible "battle" counts used in Chatbot Arena, where the number of matchups between any two models is not fixed but instead reflects their relative similarity.</p>
<p>Diversity weight analysis</p>
<p>To balance instruction diversity against model discrepancy, we sweep λ ∈ {0, 0.5, 1.0, 1.5, 2.0} in Eq. ( 2).With λ ≤ 0.5, the selected prompts in the writing task cluster around a few themes (e.g., poetry), yielding redundant comparisons; at λ = 2.0, diversity increases, but response discrepancy decreases, inflating "tie" outcomes.The intermediate value λ = 1.0 empirically proves optimal, producing an instruction set that preserves informative counterexamples without sacrificing discrimination power.</p>
<p>Taken together, these ablations confirm that our sample-efficient evaluation method is (1) insensitive to reasonable choices of semantic discrepancy measure, (2) robust under reduced annotation budgets, and (3) enhanced by a carefully tuned diversity term.</p>
<p>Further Experimentation</p>
<p>To assess whether our method retains its reliability as the number of compared models grows, we first expand our evaluation to 20 LLMs.As shown in Table 7 in the Appendix, even at this larger scale, the MAD-derived  (Zheng et al., 2023).For MATH500, we compare model rankings by raw accuracy against those produced by our method with K = 10, obtaining an almost perfect concordance (Spearman's ρ = 0.993 with p = 2.17 × 10 −13 in Table 4).For Chatbot Arena Conversations, we sample a subset of 15K dialogues covering fifteen open-source LLMs6 , ensuring at least 60 pairwise comparisons per model pair.Using both the full 15K subset and the 1K instances selected by our method (i.e., 15 2 × 10 = 1, 050), we observe Spearman correlations of 0.989 and 0.986, respectively (see Table 5).By contrast, randomly drawing 1, 050 comparisons (three trials with seeds 657, 216, and 849) yields a mean ρ of only 0.791 (with a standard deviation of 0.069).</p>
<p>These results demonstrate that (1) our method maintains high fidelity even when scaling to many models, (2) a small, strategically chosen subset of comparisons suffices to replicate full-scale rankings, and (3) our adaptive sampling significantly outperforms naïve random selection in both consistency and usability.</p>
<p>Conclusion</p>
<p>We have introduced a sample-efficient method for human evaluation of LLMs grounded in the principle of MAD Competition.Unlike traditional benchmarks that</p>
<p>Limitations</p>
<p>Our current implementation performs an exhaustive (brute-force) search over the entire instruction pool to identify the top-K discrepant examples for each model pair.While effective for pools on the order of 10 5 instructions, this approach may become computationally prohibitive as pools scale to millions or when comparing large numbers of models.Future work could explore gradient-based or heuristic optimization techniques (e.g., proxy models for discrepancy gradients, Bayesian optimization) to reduce search costs without sacrificing selection quality.</p>
<p>Although we optimize for semantic discrepancy and instruction diversity, we do not explicitly account for the variable cognitive load that different comparisons impose on human evaluators.Certain pairs of responses may be inherently harder to judge-due to subtle semantic differences, open-ended prompts, or required domain expertise-leading to increased annotation time or lower inter-rater agreement.Incorporating a computational measure of human difficulty, for example, estimating decision uncertainty via a small pilot annotation pass, could inform more balanced sample selection and adaptive budgeting of human effort.</p>
<p>Extending our method to dozens or hundreds of competing LLMs still entails O(N 2 K) human judgments, which may strain annotation budgets despite our efficiency gains.Coarse-to-fine strategies, such as seeding rankings with a strong LLM judge and then only collecting human judgments for closely ranked subsets, can partially mitigate this cost, but developing fully automated pipelines remains an open challenge.</p>
<p>A Elo Rating System</p>
<p>The Elo rating system (Elo and Sloan, 1978), devised by Arpad Elo in the 1960s, quantifies the relative skill of two competitors in games like chess or tennis.Each player holds a numerical rating that is revised after every match based on the actual result (win, loss, or tie) versus the expected outcome computed from the rating difference.Upsets-when a lower-rated player defeats a stronger opponent-yield larger point gains, whereas favored players earn fewer points for expected victories.The size of each update is governed by two parameters: the scaling factor τ , which determines how rating differences translate into expected scores, and the K-factor η, which caps the maximum change per match.In our experiments, we set τ = 400 and η = 4, consistent with the Chatbot Arena protocol.</p>
<p>B Instruction Pool</p>
<p>We construct a large-scale instruction pool X to serve as unbiased test data for comparing LLMs.Our construction comprises three stages:</p>
<p>• Task definition.We identify four capability dimensions-scientific knowledge understanding, mathematical reasoning, creative and functional writing, and code generation and explanation-and include corresponding subtasks (see Figure 2).</p>
<p>• Seed instruction collection.For each task, we sample 3K prompts from established benchmarks:</p>
<p>-Scientific knowledge understanding is a task to evaluate the scientific knowledge comprehension and application abilities of LLMs.Questions from CAMEL (Li et  • Instruction evolution.Leveraging three strong closed-source models (GPT-4-Turbo, GPT-3.5-Turbo, and Gemini Pro), we perform ten iterative evolutions per seed (Xu et al., 2023).In each iteration, we prompt models to (i) adapt seed instructions toward more practical or nuanced scenarios (e.g., transforming "a mundane text abbreviation"</p>
<p>into "design a mnemonic to aid in memorizing a complex algorithm") and (ii) impose varied constraints (e.g., "limit code to 15 lines," "compose a 1, 500-word article," or "write in Shakespearean style").For interpretability in human evaluation, evolution prompts for understanding, reasoning, and coding also request exemplar answers.</p>
<p>This process yields 30K evolved instructions per task (120K in total), ensuring both diversity and real-world relevance.Figure 2 illustrates the final task distribution, and Tables 10 to 13 list our default evolution prompts.</p>
<p>C Competing LLMs</p>
<p>We apply our sample-efficient evaluation method to eight representative LLMs:</p>
<p>• GPT-4-Turbo (i.e., GPT-4-1106-preview) and GPT-3.5-Turbo(i.e., GPT-3.5-Turbo-1106) are OpenAI's most advanced proprietary models at the time.</p>
<p>• Gemini-Pro (i.e., Gemini-1.0-Pro)(Team et al., 2023) is Google's multimodal closed-source model, trained jointly on diverse, high-quality multimodal data, demonstrating strong understanding and reasoning capabilities across a variety of specialized domains.</p>
<p>• OpenChat-3.5 (Ouyang et al., 2022)  • Qwen-14B-Chat (Bai et al., 2023) is a 14Bparameter model, fine-tuned from Qwen-14B-base that is pretrained on a large-scale, diverse dataset of over three trillion tokens, covering multiple languages such as Chinese and English.</p>
<p>• ChatGLM3-6B (Du et al., 2021) is a 6B-parameter model originated from ChatGLM3-6B-Base, pretrained on over one trillion data.</p>
<p>Implementation We conduct all experiments in a zero-shot setting.Proprietary models (GPT-4-Turbo, GPT-3.5-Turbo, and Gemini-Pro) are accessed via their official APIs with the temperature set to 0.7, top-p to</p>
<p>D Details of Human Evaluation</p>
<p>This section provides a detailed overview of our human evaluation studies.</p>
<p>D.1 Evaluator Selection Criteria</p>
<p>All evaluators have graduate-level STEM training and meet the following academic and technical proficiency criteria:</p>
<p>• Language proficiency.Each evaluator meets at least one of the following requirements:</p>
<p>-Native-level English proficiency.</p>
<p>-National College Entrance Examination (NCEE) English score ≥ 125 and College English Test (CET-6) score ≥ 500.</p>
<p>• Disciplinary foundation.They shall possess:</p>
<p>-High-school-level mastery of mathematics, physics, chemistry, biology, and formal logic.</p>
<p>-General knowledge of computer science.</p>
<p>-Proficiency in Python at a professional level.</p>
<p>• Sustained concentration.Evaluators commit to offline sessions of at least two hours (with breaks in between), ensuring the accuracy and efficiency of their annotations.</p>
<p>D.2 Pre-Experiment Briefing</p>
<p>Prior to participation, each annotator receives a comprehensive briefing on the study's objectives and procedures.Participants are explicitly informed that their annotation outputs would form part of the research dataset and that their continued involvement constitutes voluntary consent.</p>
<p>All annotation data are treated as strictly confidential and used exclusively for scientific analysis.No personally identifiable information is collected, stored, or disclosed, ensuring that participation entails no risk or adverse consequences for the annotators.</p>
<p>D.3 Graphical User Interface</p>
<p>Figure 5 illustrates the interface employed to elicit human preference judgments.Initially, annotators review the task instructions, which (1) describe the core taskchoosing the better of two model-generated responsesand (2) define the evaluation criteria (accuracy, relevance, and helpfulness).During the experiment, each annotator examines the two candidate responses alongside the instruction and then records her/his decision by clicking one of three buttons at the bottom of the page: "A win," "Tie," or "B win."</p>
<p>D.4 Human Evaluation Process</p>
<p>To evaluate model performance, we first identify the ten most discriminative instructions for pairwise comparisons among eight models across four distinct tasks.This selection yields 4 × 8 2 × 10 = 1, 120 comparisons.Thirteen graduate students with strong STEM backgrounds are recruited to annotate these pairs, each of which receives annotations from at least five different students, so that, on average, each annotator assesses approximately 345 pairs.The annotation phase lasts about one week.We measure the annotation agreement and observe an average inter-annotator agreement of 83.39%.In cases of divergent labels, the final consensus is determined by a majority vote.</p>
<p>E More Experimental Results</p>
<p>E.1 LLM-based Evaluation</p>
<p>The proposed sample-efficient evaluation method for LLMs substantially reduces human effort but does not eliminate it, which hampers scalability as the number of models grows.To overcome this bottleneck, we substitute human judges with specialized LLM-based evaluators guided by carefully crafted prompts (Tables 14 to 17).As shown in Table 6, the Spearman's ρ between LLM (in particular, GPT-4o-2024-08-26) and human preferences exceeds 0.95, confirming that topperforming LLMs (provided that they are excluded from MAD Competition) can reliably emulate human judgments.Exploiting this finding, we then apply our LLMbased method to twenty state-of-the-art LLMs (see Table 7).The resulting ranking exhibits strong agreement with that obtained via the labor-intensive Chatbot Arena, demonstrating that our approach can scale to large model sets with minimal additional labor and significantly reduced time costs.</p>
<p>E.2 Sampling Algorithm Comparison</p>
<p>In Section 4.4, we evaluate our adaptive sampling method based on MAD Competition against three alternatives in the reasoning (Table 2) and writing (Table 8) tasks.Below is a concise summary of each baseline and its configuration: • KL and Cross-Entropy Sampling (Boubdir et al., 2023) -Method: Identical to MAD Competition's protocol except that semantic discrepancy is measured by KL divergence or cross-entropy on token log probabilities.-Setting: 10 prompts per model pair → 7 2 × 10 = 210 instructions.</p>
<p>Despite their differing sample budgets, all alternatives fall short of MAD Competition's accuracy in recovering the golden ranking (Tables 2 and 8).We attribute their failures to two key limitations: inefficient sampling and biased/uninformative discrepancy measures.Dif-fUse demands a relatively large sample count simply to approximate its cluster-based expectation, and performance degrades sharply if fewer samples are used.The Anchor Points method likewise needs far more than 10 anchors per pair to represent complex response distributions adequately.Moreover, its medoid selection biases toward dataset geometry rather than evaluation relevance.KL divergence and cross-entropy on token log probabilities do not reliably reflect substantive differences in response quality and thus yield rankings no better than random sampling.In contrast, MAD Competition achieves high-fidelity ranking estimates with a minimal, unbiased sample set, demonstrating both statistical efficiency and robustness across tasks.</p>
<p>E.3 Pairwise Comparison Results</p>
<p>Figure 6 presents the pairwise comparison results.Across both the overall assessment and each individual task, GPT-4-Turbo and Gemini-Pro emerge as the top two models, outperforming all other competitors by a substantial margin.</p>
<p>E.4 Case Studies</p>
<p>In this subsection, we present illustrative cases that corroborate the trends summarized in Table 9.  22, both OpenChat-3.5 and GPT-3.5-Turbocorrectly identify the key scientific facts, yet humans prefer OpenChat-3.5 because it delivers richer and deeper analytical commentary.</p>
<p>Scientific knowledge understanding</p>
<p>Mathematical reasoning Table 23 highlights how divergent reasoning paths between WizardLM-13B and OpenChat-3.5 can yield different answers on simple arithmetic tasks.This variability underscores Wiz-ardLM's relative deficit in systematic, step-by-step deduction.Table 24 further reveals that minor lapses in arithmetic precision by WizardLM-13B lead to incorrect final results, indicating a need to strengthen its core computational routines.Draft a dialogue for a historical fiction novella.</p>
<p>Creative and functional writing</p>
<p>Craft a poem about Maya and Leo launching an initiative to clean up the ocean, using advanced robotic technology to tackle the problem of marine pollution...</p>
<p>Craft four separate evaluations on the impact of technological advancements on various educational methodologies, ensuring that each evaluation consists of at least 300 words.</p>
<p>Envision adapting the discovery of penicillin into an interactive escape room game.</p>
<p>5</p>
<p>Craft a 500-word essay on the significance of preserving historical languages that are on the verge of extinction.</p>
<p>Compose a poem in the voice of Edgar Allan Poe.</p>
<p>Craft a 1900-word adventure story set in the midst of a perilous journey across the Sahara Desert in the early 20th century.</p>
<p>Develop a compelling and nuanced backstory for my protagonist, a cunning diplomat in a fantasy realm... 6</p>
<p>Craft a tale about an ethnomusicologist who discovers a remote village where the people communicate using a complex system of rhythm and drumming.</p>
<p>Compose a quatrain about a group of scientists traveling to the Arctic Circle to study the aurora borealis, reflecting the mystical style of J.R.R. Tolkien.</p>
<p>Compose a 1900-word historical fiction narrative situated in a secluded mountain monastery.</p>
<p>Draft a villanelle utilizing the traditional ABA ABA ABA ABA ABA ABAA rhyme scheme.</p>
<p>7</p>
<p>Your assignment is to showcase your proficiency in effective and engaging marketing writing as described below...</p>
<p>Compose a sonnet about a group of friends on a hiking adventure in the Swiss Alps... Write four unique essays evaluating the impact of technological advancements on various educational methodologies, ensuring that each essay consists of at least 300 words.</p>
<p>Reflect on the influence of a scientific discovery you encountered in the past month.Strive to make your evaluation thoughtful and significant.</p>
<p>8</p>
<p>Compose a 500-word essay examining the difficulties faced when introducing a new public healthcare system in a developing country.</p>
<p>Compose a sonnet following the Shakespearean rhyme pattern.</p>
<p>Compose four distinct essays analyzing the influence of climate change on different agricultural practices...You should write an explanation carefully about your evaluation using ALL the metrics above [1].DO NOT forget any metric in your explanation.</p>
<p>Next, your similarity evaluation will be integrated into a two-decimal score between 0 and 1 by strictly adhering to the following scoring rubric: You should carefully compare the similarity evaluation with the scoring criteria and select the most appropriate score range from the scoring rubric, then you should provide a specific score based on your evaluation.You should also describe why you selected this score range and how you scored in your explanation [2].Your explanation MUST be more than 100 words!You must only respond with the following two components:</p>
<p>-explanation: "The explanation of your evaluation using all 5 metrics in [1] and scoring in [2], more than 100 words.",-score: "The similarity score, a two-decimal score."Output in JSON containing the explanation and score.Now, please complete the following evaluation:</p>
<p>Table 21: For scientific knowledge understanding, Gemini-Pro demonstrates significantly better understanding and application capabilities of scientific knowledge, compared to the open-source model Vicuna-13B.</p>
<p>Instruction:</p>
<p>What are the boundary conditions typically applied to solve the standard diffusion equation in one dimension?</p>
<p>Vicuna-13B Response:</p>
<p>The standard one-dimensional diffusion equation is given by:
∂c ∂t = D ∂ 2 c ∂x 2
where c is the concentration of diffusing species, t is time, x is position, and D is the diffusion constant.These are the common boundary conditions used to solve the standard diffusion equation in one dimension.The specific boundary condition to be applied depends on the physical problem being solved.</p>
<p>Human Preference: Gemini-Pro</p>
<p>Table 22: For scientific knowledge understanding, OpenChat-3.5 provides fine-grained explanation while encompassing the required core knowledge, garnering preferences from the majority of human subjects, compared to GPT-3.5-Turbo.</p>
<p>Instruction:</p>
<p>How does Le Chatelier's principle explain the effect of changing the concentration of a reactant on the position of equilibrium in a chemical reaction?Provide a specific example with the relevant balanced chemical equation and explain your answer.</p>
<p>OpenChat-3.5 Response: Le Chatelier's principle states that when a change is applied to a system at equilibrium, the system will adjust itself to counteract the change in order to regain its original state of equilibrium.In the context of a chemical reaction at equilibrium, this means that if a change is made to one of the reactants or products, the position of the equilibrium will shift in a way that minimizes the effect of the change.</p>
<p>One example of this can be observed in the reaction of hydrogen gas (H2) and iodine gas (I2) to form hydrogen iodide (HI).The balanced chemical equation for this reaction is: H2(g) + I2(g) ⇋ 2HI(g) Initially, the reaction is at equilibrium, and the concentrations of H2, I2, and HI are in a specific ratio.Now, let's say we increase the concentration of hydrogen gas (H2) by adding more H2 to the system.According to Le Chatelier's principle, the system will respond by shifting the equilibrium position to consume some of the excess hydrogen gas and minimize its effect on the reaction.</p>
<p>To do this, the forward reaction (H2 + I2 → 2HI) will become more favorable, as it consumes hydrogen gas and forms more HI.This will eventually lead to a new equilibrium state where the concentration of H2 is lower than it was before the change.</p>
<p>Conversely, if we were to decrease the concentration of hydrogen gas, the reaction would shift in the opposite direction to consume more HI and form more H2, again restoring the original concentration of H2.In summary, Le Chatelier's principle explains how changing the concentration of a reactant affects the position of equilibrium in a chemical reaction.By increasing the concentration of a reactant, the equilibrium position will shift to consume more of that reactant and minimize its effect.Conversely, decreasing the concentration of a reactant will cause the equilibrium position to shift in the opposite direction, consuming less of that reactant and minimizing its effect.</p>
<p>GPT-3.5-Turbo Response: Le Chatelier's principle states that if a system at equilibrium is disturbed by a change in temperature, pressure, or concentration of a component, the system will shift its position of equilibrium to counteract the effect of the disturbance.Let's consider the reaction between nitrogen dioxide (NO2) and dinitrogen tetroxide (N2O4): 2NO2(g) ⇋ N2O4(g) If we increase the concentration of NO2, according to Le Chatelier's principle, the system will respond by shifting the equilibrium to the left to counteract the increase in NO2 concentration.As a result, more N2O4 will be formed to consume the excess NO2, thereby reducing the concentration of NO2 and restoring equilibrium.</p>
<p>Conversely, if we decrease the concentration of NO2, the system will shift the equilibrium to the right to produce more NO2 and consume N2O4, again restoring equilibrium.</p>
<p>In both cases, Le Chatelier's principle explains how the position of equilibrium shifts in response to changes in the concentration of a reactant in order to maintain a constant equilibrium condition.</p>
<p>Human Preference: OpenChat-3.5</p>
<p>Table 30: Failure case of GPT-4-Turbo in writing.GPT-4-Turbo fails to fully comprehend the meaning of the word "underappreciated" in this case.In comparison to Dr. Ignaz Semmelweis, the work of Rosalind Franklin is evidently highly valued.</p>
<p>Instruction:</p>
<p>Demonstrate your critical thinking skills through the following exercise.Craft a detailed account of an underappreciated scientific discovery and its impact.</p>
<p>GPT-3.5-Turbo Response:</p>
<p>One underappreciated scientific discovery with significant impact is the work of Dr. Ignaz Semmelweis, a Hungarian physician who made a groundbreaking contribution to understanding the importance of hand hygiene in preventing the spread of infectious diseases.In the mid-19th century, Semmelweis worked in a maternity clinic in Vienna, where he observed a high mortality rate among women who gave birth in the hospital.He noticed that women who were attended by doctors and medical students had a much higher mortality rate from puerperal fever (childbed fever) compared to those attended by midwives.Semmelweis became convinced that the high mortality rate was due to the lack of hand hygiene among the doctors and students, who often went directly from performing autopsies to delivering babies without washing their hands.</p>
<p>In 1847, he implemented a strict handwashing protocol using chlorinated lime solution for the doctors and students before examining or delivering babies.As a result, the mortality rate from puerperal fever dramatically decreased.Despite the clear evidence of the effectiveness of hand hygiene in preventing the spread of infectious diseases, Semmelweis faced significant resistance and ridicule from the medical community.His ideas challenged the prevailing medical beliefs of the time, and many doctors were unwilling to accept the notion that they themselves were responsible for transmitting diseases to their patients.However, Semmelweis's work laid the foundation for the modern understanding of the importance of hand hygiene in healthcare settings.His findings were a crucial step toward the development of aseptic techniques and the recognition of the role of microorganisms in disease transmission.Semmelweis's work also contributed to the eventual acceptance of the germ theory of disease, which revolutionized medical practice and led to significant improvements in public health.The impact of Semmelweis's discovery continues to be felt today, as hand hygiene remains a critical factor in preventing healthcare-associated infections.His work has influenced the development of infection control practices and has saved countless lives by reducing the spread of infectious diseases in healthcare settings.Despite facing skepticism and rejection during his lifetime, Semmelweis's contribution to the understanding of hand hygiene and disease transmission has had a lasting and profound impact on public health and medical practice.</p>
<p>GPT-4-Turbo Response:</p>
<p>Certainly.One underappreciated scientific discovery with profound impact is the discovery of the structure of DNA by Rosalind Franklin, alongside James Watson, Francis Crick, and Maurice Wilkins.Background:</p>
<p>The story of DNA's structure discovery often highlights Watson and Crick's 1953 publication in the journal Nature, which proposed the double helix structure of DNA.However, what is frequently overlooked is the critical contribution of Rosalind Franklin, a British biophysicist and X-ray crystallographer.Discovery: Rosalind Franklin's expertise in X-ray diffraction techniques allowed her to take precise photographs of DNA fibers.Her Photo 51, which was shared with Watson and Crick without her permission by her colleague Maurice Wilkins, was pivotal in deciphering the double helix structure.Franklin's photo revealed the density patterns and the helical form of DNA, providing the necessary clues to its structure.The image showed an X shape, indicating a helical structure.The consistent spacing of the horizontal bars suggested that the phosphate-sugar backbone was on the outside of the molecule, with the bases paired in the interior.This insight was crucial in understanding the base pairing mechanism, which is central to DNA replication and function.Impact: Franklin's work was fundamental for Watson and Crick's model building, yet her contribution was not fully recognized at the time, nor was she included as a co-recipient of the Nobel Prize in Physiology or Medicine in 1962, which was awarded to Watson, Crick, and Wilkins.Unfortunately, Franklin had passed away four years earlier, and the Nobel Prize is not awarded posthumously.</p>
<p>The recognition of DNA's structure was a watershed moment in biology and medicine.It opened the door to understanding the genetic code and the mechanism by which genetic information is passed from one generation to the next.This knowledge laid the foundation for the entire field of molecular biology, leading to advances such as DNA sequencing, genetic engineering, and the Human Genome Project.The discovery has had far-reaching implications for the diagnosis and treatment of genetic diseases, the development of biotechnology, and the understanding of evolutionary relationships.It has also raised ethical, legal, and social issues related to genetic privacy, cloning, and gene editing.In summary, while the narrative often celebrates Watson and Crick, the critical contribution of Rosalind Franklin and her X-ray diffraction data was essential to the discovery of the DNA double helix, a breakthrough that has had profound and lasting impact on science and society.Her story serves as a reminder of the importance of recognizing all contributors in scientific endeavors and the sometimes complex interplay of collaboration, competition, and recognition in the advancement of knowledge.</p>
<p>Human Preference: GPT-3.5-Turbo</p>
<p>Table 31: Faiulre case of GPT-4-Turbo in writing.GPT-4-Turbo exhibits a sense of "laziness" in its responses, failing to address the demands of the instruction, despite the instruction lacking specific details.</p>
<p>Instruction: Hello, please assist me in drafting my inaugural speech.</p>
<p>OpenChat-3.5 Response: Ladies and Gentlemen, esteemed colleagues, and fellow citizens, It is with great honor and humility that I stand before you today as your newly elected leader.I am deeply grateful for the trust and confidence you have placed in me, and I am committed to serving you with integrity, diligence, and passion.</p>
<p>As I embark on this journey, I would like to acknowledge my predecessors who have laid a strong foundation for our nation.Their dedication and tireless efforts have brought us to this point, and their legacy will continue to guide us as we move forward.The challenges we face today are many, but I am confident that together, we can overcome them.We must address issues such as economic inequality, climate change, and social injustice, and we must do so with a strong sense of unity and purpose.We must remember that the strength of our nation lies in our diversity and the power of our collective voices.</p>
<p>To achieve our goals, we must foster a spirit of collaboration and inclusion.We must listen to and learn from one another, and we must work together to create a future that is both prosperous and just.We must also remain vigilant in our defense of democratic principles and human rights, for these are the cornerstones of our society.</p>
<p>In the months and years ahead, I pledge to work tirelessly to uphold the values that have made our nation great.I will strive to be a leader who is accessible, transparent, and accountable to you, the people.I will work to ensure that your voices are heard and that your concerns are addressed.I am eager to embark on this journey with you, and I am confident that, together, we can create a brighter future for our nation.Thank you for your support, and may God bless our endeavors.Human Preference: OpenChat-3.5</p>
<p>Figure 2 :
2
Figure 2: Task distribution in our experiment.</p>
<p>Figure 3 :
3
Figure 3: Sankey diagram of eight LLMs' ranking shifts across our sample-efficient human evaluation method, Chatbot Arena, AlpacaEval-2.0,and Compass-Rank (Nov.2024 snapshot).</p>
<p>Figure 5 :
5
Figure 5: Graphical user interface for collecting human preferences.</p>
<p>•</p>
<p>DiffUse (Ashury-Tahan et al., 2024) -Method: Cluster the embedding-difference vectors between each pair of model responses; estimate the expected ranking by drawing instructions from each cluster.-Setting: 10 clusters per model pair; 3 prompts per cluster → 7 2 ×10×3 = 630 instructions.• Anchor Points (Vivek et al., 2023) -Method: Apply K-Medoids to select a small set of "anchor" instructions.-Setting: K = 10 anchors per model pair → 7 2 × 10 = 210 instructions.</p>
<ol>
<li>0.00-0.20:Extremely low similarity.It means responses met one of the following conditions: (1) The tasks they solved are not the same, or their themes are totally different, (2) They have opposite semantic polarity, (3) They have opposite emotional polarity, (4) There is a huge difference in content quality, i.e. one of the responses contains obvious incompleteness, or one of the responses is nonsense because it simply repeats some sentences.2. 0.21-0.40:Relatively low similarity.It means responses met one of the following conditions: (1) The tasks they solved are the same, their themes have few differences, but are nearly the same, (2) They have relatively low similarity in content quality due to one of the responses contains few repetitive sentences, (3) They have completely different details or linguistic expression, (4) They have relatively low similarity in semantic polarity or emotional polarity.3. 0.41-0.60:Medium similarity.It means responses met all the following conditions: (1) Responses solve exactly the same task, (2) They are similar in theme, semantic polarity, and emotional polarity, (3) There are still some differences in linguistic expression and details. 4. 0.61-0.80:Relatively high similarity.It means responses met all the following conditions: (1) Responses solve exactly the same task, and have exactly the same theme, semantic polarity, and emotional polarity, (2) There are few differences in content, linguistic expression, and details, resulting in relatively high similarity.5. 0.81-1.00:Extremely similar.It means basically no difference can be found.DO NOT VIOLATE ANY DETAIL IN THE SCORING RUBRIC.You should obey that if any of the conditions of 'extremely low similarity' and 'relatively low similarity' are met, the score segment must be selected.</li>
</ol>
<p>u(a) = g1, u(b) = g2 where a and b are the boundaries of the domain, u(a) and u(b) are the prescribed values of the solution at the boundaries, and g1 and g2 are constants or given functions.2. Neumann boundary condition: This condition specifies the value of the normal derivative of the solution at the boundaries.It is also known as the flux boundary condition.Mathematically, it can be written as: where h1 and h2 are constants or given functions.3. Robin boundary condition: This condition is a combination of the Dirichlet and Neumann boundary conditions.It specifies both the value of the solution and the value of its normal derivative at the boundaries.Mathematically, it can be expressed as: u(a) = g1, ∂u(b) ∂x + αu(b) = h2 where α is a constant.4. Periodic boundary condition: This condition is used when the solution is expected to have a periodic behavior.It requires the solution to have the same value at the two boundaries, i.e., u(a) = u(b)</p>
<p>Both f i (x) and f j (x) re-
ceive strong evaluation, suggesting that eachmodel can generate diverse yet satisfactoryresponses. This scenario reflects real-worldtask complexity in which multiple plausibleoutputs exist. Although x underscores eachmodel's strengths, it contributes little to dis-tinguishing their overall performance.-Low-rating tie. Both outputs garner poorscores, indicating that each model fails, al-beit in different ways, to follow the instruc-tion. In this case, x illustrates their respectiveweaknesses but again offers limited insightfor ranking.Selecting only the K instructions with the most dis-crepant responses may yield a narrow set of failurecases. To encourage a broader exploration of modelbehaviors, we add a diversity term. When selecting thek-th instruction, we solve</p>
<p>Table 1 :
1
Global ranking results obtained by our method for eight LLMs on four different tasks.
ModelUnderstanding Rank Elo Rating Rank Elo Rating Rank Elo Rating Rank Elo Rating Rank Elo Rating Reasoning Writing Coding OverallGPT-4-Turbo21,06511,12311,16211,10311,132Gemini-Pro11,09121,09421,09731,08521,107OpenChat-3.531,04731,08731,025497131,035GPT-3.5-Turbo498841,069597621,09541,034WizardLM-13B5986882341,00169615937Qwen-14B-Chat69676939791859636932ChatGLM3-6B89245998886179587929Vicuna-13B79327869696288658894</p>
<p>Table 2 :
2
Comparison of global ranking results on mathematical reasoning using different sampling strategies.Cells with more saturated colors indicate more significant ranking discrepancies from the "gold standard."
ModelRandomKL Divergence Cross-Entropy Anchor PointsDiffUseMAD Competition"Golden" ranking (on GSM8K)RankElo RankElo RankElo RankElo RankElo RankElo Rank AccuracyGPT-4-Turbo11,02821,020498311,05711,04811,157192.7OpenChat-3.551,00041,00521,03031,04421,04121,132277.3GPT-3.5-Turbo21,02511,03631,02521,03731,04131,079374.9ChatGLM3-6B31,00731,01711,04571,02371,02841,018472.3Qwen-14B-Chat41,0075993598251,01251,0125953560.1Vicuna-13B794779577974695269426858711.3WizardLM-13B698769726974487748867802613.5</p>
<p>Table 3 :
3
Comparison of global ranking results under different semantic discrepancy measures.
,
gence demotes GPT-4-Turbo below its true position, and cross-entropy erroneously elevates ChatGLM3-6B above stronger baselines such as GPT-3.5-Turbo.These results highlight the effectiveness of MAD Competition in promoting sample efficiency and ranking reliability.Beyond quantitative ranking fidelity, we conduct a</p>
<p>Table 3
3), demon-
Sample size robustnenss We vary K (the number of instructions per model pair) from 1 to 9 and compute</p>
<p>Table 4 :
4
(Lightman et al., 2023)nking results between our method and MATH500 accuracy.Spearman's ρ = 0.965 with p = 5.93 × 10 −12 ), indicating that our method scales gracefully.Nevertheless, because underlying data distributions may differ between our curated instruction set and the Chatbot Arena benchmark, this single comparison may not fully attest to real-world robustness.Consequently, we further validate our method on two external, real-world datasets: MATH500(Lightman et al., 2023)and Chatbot Arena Conversations
ModelMATH500 Rank Acc Rank OursEloGPT-4.5182.6 11,090Claude-3-5-Sonnet-20240620276.3 21,076GPT-4o-2024-05-13372.4 4 ↓1,068GPT-4o-mini-2024-07-18471.2 3 ↑1,072GPT-4-Turbo-2024-04-09568.0 51,067GPT-4-1106-preview660.8 61,061GPT-3.5-Turbo-1106743.1 71,025Gemma-9b-it834.8 81,012Qwen1.5-14b-chat929.2 91,000Llama-8b-it1027.2 10977OpenChat-3.51124.6 12 ↓935Qwen1.5-7b-chat1220.3 11 ↑958Mistral-7b-it-v0.21316.4 13903ChatGLM3-6b1416.0 14885Vicuna-13b153.8 15870ranking exhibits a very strong correlation with the Chat-bot Arena leaderboard (</p>
<p>Table 5 :
5
Comparison of global ranking results between random sampling (with 1K comparisons), 15K subset from Chatbot Arena Conversations, our method (with 1K comparisons), and the Chatbot Arena leaderboard.
ModelRandom (Seed 657)Random (Seed 216)Random (Seed 849)15K SubsetOursChatbot Arena LeaderboardRankEloRankEloRankEloRankEloRankEloRankEloGPT-41 1,1191 1,1271 1,1201 1,2541 1,1231 1,163Claude-v12 1,0952 1,0902 1,0972 1,1962 1,0422 1,149GPT-3.5-Turbo4 1,0505 1,0244 1,0714 1,1393 1,0443 1,117Claude-Instant-v13 1,0833 1,0866 1,0053 1,1645 1,0364 1,111Vicuna-13B89784 1,0793 1,0735 1,0554 1,0425 1,042Vicuna-7B799689777 1,0006 1,0196 1,0196 1,005Koala-13B99701195811 9567 1,0067 1,0087964MPT-7B-Chat1096779999988894689868928RWKV-4-Raven-14B119631492813948993899739922Alpaca-13B5 1,046139461590711 9061196610901OAsst-pythia-12B6 1,0116 1,0038993109191296511893ChatGLM-6B14935996112951138841096812879FastChat-T5-3B13939129525 1,008128941396313868StableLM-Tuned-Alpha-7B129461095810961148571491514840Dolly-v2-12B159021591414921158241590215822rely on fixed, manually curated test sets, our methoddynamically identifies a minimal set of highly informa-tive instructions that maximize semantic discrepancybetween model outputs. By concentrating human judg-ments on these targeted "hard" examples, we achievereliable global rankings with dramatically fewer anno-tations. Moreover, the resulting counterexample cor-pus not only supports accurate model comparison butalso serves as rich adversarial data for future modelfine-tuning. Our approach is readily extensible to mul-timodal settings without modifying the core selectionand aggregation procedures. Looking ahead, we planto broaden our coverage by incorporating more LLMs,diversifying evaluation scenarios, and ultimately pub-lishing an open, comprehensive leaderboard that canadapt as new models emerge.</p>
<p>Table 6 :
6
Comparison of global ranking results between LLM-based and human evaluation across four tasks.
UnderstandingReasoningWritingCodingOverallModelHumanGPT-4oHumanGPT-4oHumanGPT-4oHumanGPT-4oHumanGPT-4oRankRankRankRankRankRankRankRankRankRankGPT-4-Turbo2211111111Gemini-Pro1123223322OpenChat-3.53332334533GPT-3.5-Turbo4444542244WizardLM-13B5588456655Qwen-14B-Chat6666775466ChatGLM3-6B8755887878Vicuna-13B7877668787Spearman's ρ0.97620.97620.97620.95240.9762</p>
<p>Table 7 :
7
Global ranking results of twenty LLMs by our LLM-based evaluation method.The gray column denotes the Chatbot Arena leaderboard positions, and the two rankings exhibit strong agreement (Spearman's ρ = 0.965).
ModelUnderstanding Rank Elo Rank Reasoning Elo Rank Writing Elo Rank Coding Elo Rank Overall EloChatbot Arena RankGPT-4o-2024-05-1311,16671,06511,10631,09711,1511GPT-4o-mini-2024-07-1851,12661,07831,09661,07731,1442Claude-3.5-Sonnet-2024062041,13911,10561,05421,11041,1423Gemini-1.5-pro-latest61,11041,08871,05291,04451,1124GPT-4-Turbo-2024-04-0921,16451,07821,10411,11021,1475GPT-4-1106-preview31,14421,10341,08851,08061,1006Claude-3-Sonnet-2024022971,05531,103121,02241,09781,0777Gemma2-9B-it81,03581,03081,04971,05891,0698Llama3.1-8B-it10978101,00951,070111,01471,0799Llama3-8B-it1887815951111,02314964131,00110Gemini-pro15910111,006131,01481,048111,02111Qwen1.5-14B-Chat11973121,000101,02612998101,03012OpenChat-3.51295991,01616930169111693913Mistral-7B-it169101694191,03215930121,00514Qwen1.5-7B-Chat91,0161398314975139771496915GPT-3.5-Turbo-1106139591497715952101,0171594216Wizardlm-13B179092083217906188811781517Vicuna-13B198711985118889198651880418Qwen-14B-Chat149241792419836179091978619ChatGLM3-6B207731885920777208142067020</p>
<p>Table 8 :
8
Comparison of global ranking results using different sampling algorithms in the writing task.
ModelRandomKL Divergence Cross-Entropy Anchor PointsDiffUseMAD Competition"Golden" ranking (Chatbot Arena)RankElo RankElo RankElo RankElo RankElo RankElo Rank AccuracyGPT-4-Turbo11,08011,07511,04611,12711,12911,08611,250OpenChat-3.5399341,00541,02631,01131,02821,02821,091WizardLM-13B59855988599521,037498931,02231,068GPT-3.5-Turbo498821,03321,0357941594241,01041,059Vicuna-13B69836974694269447931599051,042Qwen-14B-Chat21,03831,00731,030499321,044695461,035ChatGLM3-6B7932791979255946693879107955</p>
<p>Table 21
21com-</p>
<p>Table 18 :
18
Top-10 instructions chosen by different sampling strategies.
OrderRandomKL DivergenceCross-EntropyMAD Competition1You are asked to offer valuableCraft a limerick centered aroundCompose four essays on the sub-You have been tasked with offer-suggestions, relevant statistics, ora family embarking on a subma-ject of climate change adaptationing informed perspectives, analy-elucidation on the issue outlinedrine voyage to explore the Mari-strategies, each with a minimumses, or elucidations to address thebelow...ana Trench, capturing the whimsi-of 300 words.following question...cal tone of Lewis Carroll.2Compose a compelling essay onCan you compose limericksDevelop a Market Analysis Re-... Suggest a few alternative histor-the importance of learning a sec-for various renewable energyport, following APA referencingical items that could temporarilyond language.sources? Begin with solar power.style, with six sections evaluatingfill this space and enhance the ex-a different emerging technology...hibition's narrative.3You have been assigned to ex-Compose a limerick that uses al-Craft a 1900-word science fictionCraft a one-page essay discussingplore the environmental implica-literation to add humor to the de-story set in an underwater city.the importance of preserving lin-tions of a modern dilemma...scribed situation.guistic diversity in the age ofA clumsy chef spills the soup.global communication.4
Table 25 demonstrates clear human preferences for responses that incorporate substantive content (e.g., illustrative examples</p>
<p>Table 19 :
19
Top-10 instructions chosen by MAD Competition with and without the diversity term.
Order without Diversitywith Diversity1You have been assigned to evaluate the following tech-You have been assigned to evaluate the following tech-nological issue by considering opinions from diversenological issue by considering opinions from diversestandpoints.standpoints.Is the widespread implementation of autonomous vehi-Is the widespread implementation of autonomous vehi-cles on public roads justifiable?cles on public roads justifiable?2Devise a catchy mnemonic to remember the key ele-I am particularly fascinated by the traditional sounds ofments.Mongolia. Could you provide me with the sheet musicAn astronomical method for categorizing, observing,for a classic Morin Khuur tune?and explaining Variable Star Observation Parameters(VSOP).3I am particularly fascinated by the traditional sounds ofDevise a catchy mnemonic to remember the key ele-Mongolia. Could you provide me with the sheet musicments.for a classic Morin Khuur tune?An astronomical method for categorizing, observing,and explaining Variable Star Observation Parameters(VSOP).4Draft a villanelle utilizing the traditional ABA ABADraft a villanelle utilizing the traditional ABA ABAABA ABA ABA ABAA rhyme scheme.ABA ABA ABA ABAA rhyme scheme.5Compose a sonnet following the Shakespearean rhymeDesigning a mascot for your environmental conservationpattern.campaign is a crucial step in engaging with the commu-nity. Reflecting on the goals and fundamental purpose ofyour initiative, you are required to conceive a variety ofappealing mascot concepts. Take some time to ponderyour ideas.6Can you compose sonnets that reflect the distinct flavorsDesign a program for a high school science fair. Assignof various cuisines? Begin with Italian pasta dishes.a precise duration for each presentation included in theevent.7Draft a dialogue for a historical fiction novella.Write an essay discussing the three main economic theo-ries that explain market behavior.8Compose a brief narrative in 10-15 lines that encapsu-As a dedicated librarian, I cherish the opportunity to in-lates the experience of conducting a scientific field re-troduce young readers to classic literature. I'm currentlysearch in a remote rainforest during a significant weatherseeking to diversify the selection of adventure stories inphenomenon.our children's section that highlight female protagonists.Could you recommend a novel featuring a young heroinethat is suitable for readers aged 8 to 12?9Are you able to concoct haikus about different forms ofHello, could you craft a narrative in the style of a screen-transportation? Start with bicycles.play that features interactions among characters, setwithin the universe of Greek mythology, including thefigures, Hermes, Apollo, and Medusa.10Hello, could you craft a narrative in the style of a screen-Outline the progression of major milestones in the fieldplay that features interactions among characters, setof artificial intelligence in healthcare during the yearwithin the universe of Greek mythology, including the2022.figures, Hermes, Apollo, and Medusa.</p>
<p>Table 20 :
20
Prompt used by GPT-4-Turbo as a semantic similarity measure.Given two responses, you are asked to evaluate the similarity between the two responses.Your evaluation should be based on the following metrics: 1. Task and Theme: Analyze whether this text pair addresses the same open-ended task and theme.If the tasks or themes are not entirely the same, analyze their similarities and differences.2. Emotion and Semantics: Conduct an emotion and semantic analysis of this text pair, determining their similarity in emotional polarity (positive, negative, and neutral) and semantic polarity.3. Content Quality: Analyze the similarity in content quality between this text pair, considering aspects such as incomplete content, nonsensical statements, lack of details, etc. 4. Details: Analyze the similarity in the details of the text content, such as the approach and steps in solving mathematical problems, plot details in stories, etc. 5. Language Expression: Analyze the similarity in the language expression between the two responses, including language style, vocabulary, syntax, rhetorical devices, etc.</p>
<p>GPT-4-Turbo Response: I'd be happy to help you draft your inaugural speech, but I'll need some additional information, such as: 1.The position or role you are inaugurating.2. The audience you will be addressing (e.g., community members, professionals, students).3. Key themes or issues you wish to address.4. Your vision and goals in your new role.5. Any specific achievements or acknowledgments you want to include.6.The tone you want to set (e.g., hopeful, determined, conciliatory).With that information, I can help you create a draft that is tailored to your needs and the occasion.If you provide me with those details, I'll craft a speech for you.If you're not sure about some of the points, let me know, and I can offer you some general guidance that you can refine to fit your situation.</p>
<p>In this paper, the terms "instructions," "prompts," and "queries" are used interchangeably to denote the inputs provided to LLMs.
https://huggingface.co/spaces/lmsys/ chatbot-arena-leaderboard
https://tatsu-lab.github.io/alpaca_eval/
https://rank.opencompass.org.cn/ leaderboard-llm/?m=24-04
We convert semantic similarity measures into discrepancy scores by negating their values.
https://huggingface.co/datasets/lmsys/ chatbot_arena_conversations
AcknowledgementsThis work was supported in part by the National Natural Science Foundation of China (62301480 and 62302433), Zhejiang Provincial "Jianbing" "Lingyan" Research and Development Program of China (2025C01097 and 2024C01135), and Hangzhou West Lake Pearl Project Leading Innovative Youth Team Project (TD2023017).ford Alpaca: An instruction-following LLaMA model.GitHub repository https://github.com/tatsu-lab/stanford_alpaca.Each cell shows the proportion of head-to-head victories for the model on the vertical axis against the model on the horizontal axis; darker red indicates a higher win rate.Note that tied outcomes do not contribute to either direction.or nuanced argumentations), suggesting that response length correlates positively with perceived quality.Code generation and explanation Although evaluators consider readability and adherence to instructions, accuracy remains paramount.In Table26, Vicuna-13B annotates its code thoroughly, but human raters preferGemini-Pro because it alone delivers reliably correct, executable solutions within the specified constraints.Counterexamples of GPT-4-Turbo Despite its top ranking overall, GPT-4-Turbo exhibits notable shortcomings in certain contexts:• Algorithm explanation (Table27).OpenChat- Others: laziness, slight deviation in instruction comprehension.Gemini-Pro &amp; OpenChat-3.5Reasoning: clear reasoning logic.Writing: strong writing capability.Reasoning: limited ability in complex arithmetic computations.Coding: insufficient accuracy in writing complex code.GPT-3.5-TurboCoding: strong coding proficiency.Understanding: short reply length, lack of detailed analysis.Others: laziness.WizardLM-13B Writing: relatively strong writing capability.Reasoning: unclear reasoning logic, weak arithmetic ability.Coding: limited coding proficiency.Qwen-14BCoding: relatively high success rate in execution.Reasoning: limited ability in complex arithmetic computations.Writing: short reply length, simple content.Others: (somewhat) laziness.ChatGLM3-6BReasoning: relatively rigorous logic, relatively accurate arithmetic operations.Understanding: limited knowledge recall, lack of detailed explanation.Reasoning: limited ability in complex arithmetic computations.Writing: short reply length, simple content.Coding: low success rate in code execution, disregard for instruction requirements.Others: laziness.Vicuna-13BWriting: exceptional instruction-following capability.Understanding: lack of expertise in explanation, moderate knowledge reserve.Reasoning: limited computational and reasoning capabilities.Coding: low success rate in code execution, disregard for instruction requirements.3.5's inclusion of a complete Dijkstra implementation renders its explanation more intuitive and actionable for human readers, suggesting that model performance should be judged not only by correctness but also by pedagogical clarity.• Instruction comprehension (Tables28 and 30).GPT-4-Turbo occasionally misinterprets promptsfor example, labeling Rosalind Franklin's contributions as "underappreciated," contrary to historical consensus-indicating room to improve contextual sensitivity and factual alignment.• Code constraint (Table29).GPT-4-Turbo sometimes generates solutions that exceed prescribed length limits or contain subtle logical errors, highlighting persistent challenges in enforcing userspecified coding guidelines.• Instruction adherence (Table31).At times, GPT-4-Turbo omits direct answers or skims over critical request elements, a phenomenon we describe as "response laziness," underscoring the importance of robust instruction-following mechanisms.These counterexamples illustrate that, although GPT-4-Turbo leads on average, future high-performance LLMs must dynamically adapt response formats to specific tasks, enhance fine-grained comprehension, and ensure both factual and procedural fidelity.You are a brilliant assistant.Your goal is to draw inspiration from #Given Prompt# to create a brand-new prompt which is used to evaluate the domain knowledge of a college human student.The new prompt must be reasonable, unambiguous, and must be understood by humans.Your response should include 'new_prompt' and 'answer' in the following format: {{ "new_prompt": "The new prompt for domain knowledge.Try your best to focus on subject basic knowledge and theorems.","answer": "The answer of the new prompt."}} Output the response in JSON.The new #Created Prompt# should also belong to the similar domain as #Given Prompt#.The question in #Created Prompt# should focus on subject basic knowledge and theorems that can evaluate a college student.The LENGTH and difficulty level of #Created Prompt# should be similar to that of #Given Prompt#.#Given Prompt#: {instruction} #Created Prompt#: Now, output your response with 'new_prompt' and 'answer' in the above format:Table11: Instruction evolution prompt for mathematical reasoning.You are a brilliant assistant.Your goal is to draw inspiration from the question and the associated answer in #Given Prompt# to create a brand-new prompt which is used to evaluate the math reasoning capability of a primary school student.The new prompt must be reasonable, unambiguous, and must be understood by humans.Your response should include 'question' and 'answer' in the following format: {{ "question": "The new question for math reasoning.Try your best to follow the same difficulty level and a similar length.","answer": "The step-by-step answer of the new question."}} Output the response in JSON.The new question and the step-by-step answer in #Created Prompt# should follow the same format as #Given Prompt#.The question in #Created Prompt# should focus on grade school math problems.The LENGTH and difficulty level of #Created Prompt# should be similar to that of #Given Prompt#.#Given Prompt#: Question: {instruction} Answer: {output} #Created Prompt#: Now, output your response with 'question' and 'answer' in the above format:Table12: Instruction evolution prompt for creative and functional writing.You are a brilliant assistant.Your goal is to draw inspiration from #Given Prompt# to create a brand-new prompt with SAME FORMAT (i.e., same structure and number of sentences) which is used to evaluate the writing ability of a human student.The new prompt must be reasonable and must be easily understood by humans.You should design a more rare scenerio or topic that are totally DIFFERENT from #Given Prompt# but has practical significance.The LENGTH and difficulty level of #Created Prompt# should be extremely similar to that of #Given Prompt#.'#Given Prompt#', '#Created Prompt#', 'given prompt' and 'created prompt' are not allowed to appear in #Created Prompt#.#Given Prompt#: {instruction} #Created Prompt#:The new prompt is:Table13: Instruction evolution prompt for code generation and explanation.You are a brilliant assistant.Your goal is to add some constraints to the Python question in #Given Prompt# to make the rewritten prompt a bit more challenging.The rewritten prompt is used to evaluate the coding ability of a human student.The rewritten prompt must be reasonable and must be easily understood by humans.Your response should include 'new_prompt' and 'answer' in the following format: {{ "new_prompt": "The rewritten Python prompt with two constraints.","answer": "The Python code and brief code explanation to the rewritten prompt.Ten to Thirty lines are recommended."}} Output in JSON.The rewritten prompt should different from the scenarios in #Given Prompt#.You should add two of the following constraints to the rewritten prompt: 1. Limit the lines of the code, e.g., 'Write a 10-line Python code'.Ten to thirty lines are recommended and preferred.2. Require high efficiency (time or space) of the code, e.g., 'Write a sufficiently efficient Python code'.3. Force the use of a certain Python library, e.g., 'Write a Python code ... using the library NumPy'.4. Limit the complexity of the code, e.g., 'Write a Python code ... easy to read for freshman'.The rewritten prompt can only add ten to twenty words into #Given Prompt#.You should try your best not to make the new prompt become verbose.#Given Prompt#: {instruction} #Response#: Now, choose two constraints and create a new prompt:Table14: Prompt used by LLM-based evaluators for the understanding task.Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.You are given a reference answer to the user question.You should choose the assistant that follows the user's instructions and answers the user's question better.Your evaluation should consider the following factors:1. Accuracy: Whether the answer is correct based on the reference answer.2. Core Knowledge Coverage: Whether the answer covers the core knowledge points of the question.3. Logical Consistency: Whether the answer is logically clear and presents the arguments coherently.4. Clarity of Expression: Whether the answer uses precise language and is easy to understand. 5. Relevance: Whether the answer stays focused on the question without deviating from the topic.If both assistants provide correct answers that cover the core knowledge points, it should be considered a tie (C), regardless of the length or detail of the responses.Only choose a winner if one assistant's response is clearly superior in terms of the evaluation factors.Begin your evaluation by comparing the two responses and provide a short explanation.Avoid any position biases and ensure that the order in which the responses are presented does not influence your decision.Do not allow the length of the responses to influence your evaluation.Be as objective as possible.Output "A" if assistant A is better, "B" if assistant B is better, and "C" stands for a tie.Output your final verdict by strictly following this format: { "explanation": "The short explanation of your evaluation.","winner": "A, B or C." } Output this format in JSON.Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.You are given a reference answer to the user question.You should choose the assistant that follows the user's instructions and answers the user's question better.Your evaluation should consider the following factors:1. Accuracy: Whether the answer is correct based on the reference answer.2. Logical Consistency: Whether the answer is logically clear and presents the arguments coherently.3. Clarity of Expression: Whether the answer uses precise language and is easy to understand.If both assistants provide the same final answer, it should be considered a tie (C), regardless of the length or detail of the responses.Only choose a winner if one assistant's response is clearly superior in terms of the evaluation factors, e.g., one achieves the correct answer while the other fails.Begin your evaluation by comparing the two responses and provide a short explanation.Avoid any position biases and ensure that the order in which the responses are presented does not influence your decision.Do not allow the length of the responses to influence your evaluation.Be as objective as possible.Output "A" if assistant A is better, "B" if assistant B is better, and "C" stands for a tie.Output your final verdict by strictly following this format: { "explanation": "The short explanation of your evaluation.","winner": "A, B or C." } Output this format in JSON.Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.You should choose the assistant that follows the user's instructions and answers the user's question better.Your evaluation should consider the following factors:1. Accuracy: How well the response matches the user's instructions and stays on topic.2. Creativity: The uniqueness of the perspective and the ability to engage the reader.3. Logic and Structure: Clarity of structure with a clear beginning and ending, and logical flow of ideas.4. Language Expression: Clarity of language, richness of vocabulary, and appropriate use of rhetorical devices. 5. Detail and Depth: Provision of sufficient details to support main points and depth of exploration of the topic.Begin your evaluation by comparing the two responses and provide a short explanation.Avoid any position biases and ensure that the order in which the responses are presented does not influence your decision.Do not allow the length of the responses to influence your evaluation.Be as objective as possible.Output "A" if assistant A is better, "B" if assistant B is better, and "C" stands for a tie.A tie should be determined based on the following criteria:1. Similar Quality: When the two responses are close in performance without any significant difference.Performance is measured by quality, and is not related to the length of the responses.2. Similar Core Content Match: When the performance of both responses is close to meeting the main requirements of the user's instructions.3. Complementary Strengths: When each response excels (or fails) in some different aspects, but their overall quality is comparable.DO NOT make the judgment too strict.You can output "C" just because they are similar.Output your final verdict by strictly following this format: { "explanation": "The short explanation of your evaluation.","winner": "A, B or C." } Output this format in JSON.Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.You should choose the assistant that follows the user's instructions and answers the user's question better.Your evaluation should consider the following factors:1. Accuracy: Whether the code correctly implements the user's requirements or the explanation is accurate.2. Level of Detail: Whether the code or explanation is detailed enough to meet the user's needs.3. Logical Consistency: Whether the code structure or explanation logic is clear and consistent.4. Code Quality: The quality of the code, including readability, efficiency, and maintainability.5. Creativity and Reasonableness: Whether the code implementation or explanation shows creativity and is reasonable.Begin your evaluation by comparing the two responses and provide a short explanation.Avoid any position biases and ensure that the order in which the responses are presented does not influence your decision.Do not allow the length of the responses to influence your evaluation.Be as objective as possible.Output "A" if assistant A is better, "B" if assistant B is better, and "C" stands for a tie.A tie should be determined based on the following criteria:-If both assistants' responses are good across all criteria without significant quality differences, it should be judged as a tie.-If both responses have significant errors (despite that the errors may be different), it should be judged as a tie.-If both responses have no significant differences in core accuracy and logical consistency, even if there are slight differences in detail or code quality, it should be judged as a tie.-If both responses follow different but reasonable, effective approaches, it should be judged as a tie.Output your final verdict by strictly following this format: { "explanation": "The short explanation of your evaluation.","winner": "A, B or C." } Output this format in JSON.Table23: For mathematical reasoning, WizardLM-13B takes an incorrect reasoning path, leading to wrong solutions, in contrast to OpenChat-3.5.Instruction:John wants to build a rectangular garden.He has 60 feet of fencing material.If he wants the length of the garden to be twice the width, how long will each side of the garden be?OpenChat-3.5 Response: Let's assume the width of the garden is W feet. Since the length of the garden is twice the width, the length will be 2W feet.The perimeter of a rectangle is given by the formula: P = 2(length + width) In this case, the perimeter is 60 feet, so we can write the equation as: 60 = 2(2W + W) Simplifying the equation, we get: 60 = 2(3W) 60 = 6W Dividing both sides by 6, we get: W = 10 So, the width of the garden is 10 feet.Since the length is twice the width, the length is 2 * 10 = 20 feet.Therefore, each side of the garden will be 10 feet and the other side will be 20 feet.WizardLM-13B Response:To solve this problem, we can use the Pythagorean theorem, which states that in a right-angled triangle, the square of the length of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the other two sides.In this case, the length of one side of the garden is equal to the sum of the squares of the other two sides.Let's denote the width of the garden as 'a' and the length as 'b'.We know that 'b' is equal to 2*'a', so we can rewrite the equation as follows: b 2 = 2 × a 2 Now, we can use the Pythagorean theorem to find the value of 'a'.Let's denote 'c' as the hypotenuse of a right-angled triangle formed by the width and the length of the garden.Then, we can write the equation as:Substituting the given values, we get: c 2 = 60 2 Solving for 'a', we get: a = √ 60 2 Therefore, the width of the garden isHuman Preference: OpenChat-3.5Table24: For mathematical reasoning, WizardLM-13B makes an arithmetic error during the intermediate reasoning process, leading to the eventual collapse of the final result, compared to GPT-3.5-Turbo.Instruction:A bakery sells cupcakes and cookies.Each cupcake costs $3 and each cookie costs $2.In one day, the bakery makes $120 from selling cupcakes and $80 from selling cookies.How many items did the bakery sell in total in one day?WizardLM-13B Response: First, we need to determine the total number of items sold by the bakery in one day.We know that the bakery sold 40 cupcakes and 20 cookies.To find the total number of items sold, we can simply add the number of cupcakes and cookies together.Step 1: Add the number of cupcakes and cookies.Cupcakes: 40 Cookies: 20Step 2: Calculate the total number of items sold.Total items sold = Cupcakes + Cookies = 40 + 20 = 60 So, the bakery sold 60 items in total in one day.The answer is: 60.GPT-3.5-Turbo Response:Sure! Let's solve the problem step by step.Let's represent the number of cupcakes sold as C and the number of cookies sold as K.We are given the following information: 1.Each cupcake costs $3, and the bakery makes $120 from selling cupcakes.This can be represented as 3C = 120.To find the total number of items sold, we add the number of cupcakes and cookies: Total items = 40 cupcakes + 40 cookies = 80 items Therefore, the bakery sold a total of 80 items in one day.Human Preference: GPT-3.5-TurboTable25: For writing, the response of GPT-4-Turbo has richer content, while ChatGLM3-6B's response not only fails to meet the instruction requirements, but is also lacking in content.Instruction:Craft an enticing menu for a local gastronomy event, utilizing vivid and appetizing descriptions to ensure enthusiasm and attendance from community food enthusiasts and local restaurant owners.GPT-4-Turbo Response: Savor the Seasons: A Gastronomic Journey Join us as we embark on a culinary adventure that celebrates the best of local produce and traditional cooking techniques.Our menu is designed to tantalize your taste buds while showcasing the versatility and richness of our regional flavors.Each dish is a love letter to our heritage, prepared with skill and presented with pride.Get ready to indulge in a symphony of tastes that will leave you craving for more.Human Preference: GPT-4-TurboTable26: For coding, both Gemini-Pro and Vicuna-13B meet the 10-line constraint.Although Gemini-Pro provides less detailed explanation, its correct solutions make it the preferred choice among human evaluators.Instruction:Write a Python function in under 10 lines to create a new tuple from the given string and list.Gemini-Pro Response:&gt; def new_tuple(test_list, test_string): &gt; res = tuple(test_list) + (test_string, ) &gt; return (res)Vicuna-13B:Here's a possible implementation of the 'new_tuple' function:&gt; def new_tuple(words, glue): &gt; """ &gt; Creates a new tuple from the given string and list of words, using the specified glue string to connect adjacent words.&gt; """ &gt; return tuple(word + glue if i &lt; len(words) -1 else word for i, word in enumerate(words))This function takes two arguments: a list of words and a glue string.It returns a tuple containing the words, with the glue string inserted between adjacent words.Human Preference: Gemini-Pro Table27: Failure case of GPT-4-Turbo in scientific knowledge understanding.The response from OpenChat-3.5 includes algorithm code after introducing the algorithm itself, which is more vivid and easier for human users to understand, compared to the response from GPT-4-Turbo.Instruction:How can Dijkstra's algorithm be used to find the shortest path between two nodes in a graph?OpenChat-3.5 Response:Dijkstra's algorithm is a popular algorithm used for finding the shortest path between two nodes in a weighted graph.Here's how it works: 1. Initialize the algorithm: -Create a set of unvisited nodes, and assign a tentative distance value to the starting node (usually 0) and to all other nodes (infinity).2. Select the unvisited node with the smallest tentative distance, and mark it as visited.3. Update the tentative distances of the neighboring nodes of the visited node.For each neighbor, calculate the new tentative distance by adding the distance from the starting node to the visited node to the distance from the visited node to the neighbor.If the new tentative distance is less than the current tentative distance of the neighbor, update the neighbor's tentative distance.4. Repeat steps 2 and 3 until all nodes have been visited or the target node has been visited.5.The shortest path from the starting node to the target node can be reconstructed by backtracking from the target node to the starting node, following the nodes with the smallest tentative distance at each step.Dijkstra's algorithm is guaranteed to find the shortest path in a graph with non-negative edge weights.It is particularly efficient for sparse graphs, but can be slow for dense graphs.This function takes a graph (represented as a dictionary of nodes to their neighbors and weight), a starting node, and a target node.It returns the shortest path distance from the starting node to the target node, as well as the path itself (a list of nodes).GPT-4-Turbo Response:Dijkstra's algorithm is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks.It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.The algorithm exists in many variants; Dijkstra's original variant found the shortest path between two nodes, but a more common variant fixes a single node as the sourcenode and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.Here is a high-level description of Dijkstra's algorithm to find the shortest path between two nodes: 1. Initialization: Start by setting the initial node as the "current" node and mark all other nodes as unvisited.Create a set of all the unvisited nodes called the unvisited set.2. Distance Assignments: Assign to every node a tentative distance value: set it to zero for the initial node and to infinity for all other nodes.The tentative distance of a node is the length of the shortest path discovered from the initial node to that node.3. Neighbor Assessment: For the current node, consider all of its unvisited neighbors and calculate their tentative distances through the current node.Compare the newly calculated tentative distance to the current assigned value and assign the smaller one.For example, if the current node A is marked with a distance of 6, and the edge connecting it with a neighbor B has length 2, then the distance to B through A will be 6 + 2 = 8.If B was previously marked with a distance greater than 8 then change it to 8. If not, leave it unchanged.4. Mark as Visited: Once we have considered all of the unvisited neighbors of the current node, mark the current node as visited.A visited node will not be checked again.5. Select the Next Node: If the destination node has been marked visited (when planning a route between two specific nodes) or if the smallest tentative distance among the nodes in the unvisited set is infinity (when planning a complete traversal; occurs when there is no connection between the initial node and remaining unvisited nodes), then stop.The algorithm has finished.6. Repeat: Otherwise, select the unvisited node that is marked with the smallest tentative distance, set it as the new "current node," and go back to step 3. When you are interested in finding a shortest path between two specific nodes, you can terminate the algorithm once the destination node has received a permanent distance (i.e., is marked as visited).After the algorithm has completed, the shortest path can be found by backtracking from the destination node to the starting node.Each node must keep track of its "predecessor" on the shortest path from the starting node -this allows the path to be reconstructed in reverse order, from the destination up to the start node.Human Preference: OpenChat-3.5Table28: Failure case of GPT-4-Turbo in mathematical reasoning.GPT-4-Turbo overlooks some details of the instruction, leading to erroneous inference results.Instruction:A school has 10 classrooms, each with the same number of students.The school also has a library with 50 students.If the total number of students in the school is 250, how many students are there in each classroom?GPT-3.5-Turbo Response: Sure!To find out how many students are in each classroom, we can use the information given in the question.Let's start by finding the total number of students in the school.We know that there are 10 classrooms, each with the same number of students.So, the total number of students in the classrooms is 10 times the number of students in each classroom.Let's say the number of students in each classroom is x.Then the total number of students in the classrooms is 10x.We also know that the library can accommodate 50 students at a time.So, the total number of students in the school is the sum of the students in the classrooms and the students in the library, which is 10x + 50.According to the question, the total number of students in the school is 250.So, we can set up the equation: 10x + 50 = 250 Now we can solve for x: 10x = 250 -50 10x = 200 x = 200 / 10 x = 20 Therefore, there are 20 students in each classroom.So, the final answer is: There are 20 students in each classroom.GPT-4-Turbo Response:To solve this problem, we want to find out how many students are in each of the 10 classrooms, given that there are 250 students in total.Step 1: Determine the total number of students.The problem states that the total number of students in the school is 250.Step 2: Determine the number of classrooms.The problem states that there are 10 classrooms in the school.Step Human Preference: GPT-3.5-TurboTable29: Failure case of GPT-4-Turbo in coding.The response from GPT-4-Turbo exhibits errors in the test cases while lacking conciseness.Instruction:Write a Python function in under 15 lines of code to find the median of two sorted lists of the same size.GPT-3.5-Turbo Response:Here's a Python function to find the median of two sorted lists of the same size:([1, 12, 15, 26, 38],[2,13,17,30,45], 5) == 16.0This function merges the two lists, sorts the merged list, and then calculates the median based on the size of the merged list.GPT-4-Turbo Response:([1, 12, 15, 26, 38],[2,13,17,30,45], 5) == 16.0
. Zichun Syeda Nahida Akter, Aashiq Yu, Tianyue Muhamed, Alex Ou, Bäuerle, Alexander Ángel, Krish Cabrera, Chenyan Dholakia, Xiong, arXiv:2312.11444and Graham Neubig. 2023. An in-depth look at Gemini's language abilities</p>
<p>Liat Ein-Dor, and Eyal Shnarch. 2024. Label-efficient model selection for text generation. Shir Ashury-Tahan, Ariel Gera, Benjamin Sznajder, Leshem Choshen, arXiv:2402.07891</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova Dassarma, arXiv:2112.00861A general language assistant as a laboratory for alignment. 2021</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.07732Program synthesis with large language models. 2021</p>
<p>ChatGPT is a remarkable tool-for experts. Amos Azaria, Rina Azoulay, Shulamit Reches, Data Intelligence. 612024</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.16609Qwen technical report. 2023</p>
<p>Which prompts make the difference? Data prioritization for efficient human LLM evaluation. Meriem Boubdir, Edward Kim, Beyza Ermis, Marzieh Fadaee, Sara Hooker, arXiv:2310.144242023</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, arXiv:2308.07201ChatEval: Towards better LLM-based evaluators through multi-agent debate. 2023</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>Code Alpaca: An instruction-following LLaMA model for code generation. Sahil Chaudhary, GitHub repository. 2023</p>
<p>Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, Benyou Wang, arXiv:2402.10669Humans or LLMs as the judge? A study on judgement biases. 2024</p>
<p>Huajun Chen, arXiv:2312.02706Large knowledge model: Perspectives and challenges. 2023</p>
<p>Ken Yew, Pengfei Chia, Hong, arXiv:2306.04757Lidong Bing, and Soujanya Poria. 2023. InstructEval: Towards holistic evaluation of instruction-tuned large language models. </p>
<p>Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, 2023</p>
<p>Wei-Lin Chiang, Lianmin Zheng, Ying Sheng ; Tianle, Dacheng Li, Hao Li, Banghua Zhang, Michael Zhu, Joseph E Jordan, Gonzalez, arXiv:2403.04132Chatbot Arena: An open platform for evaluating LLMs by human preference. 2024Anastasios Nikolas Angelopoulos</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, arXiv:2103.10360GLM: General language model pretraining with autoregressive blank infilling. 2021</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, arXiv:2305.14387Alpaca-Farm: A simulation framework for methods that learn from human feedback. 2023</p>
<p>The Rating of Chessplayers: Past and Present. E Arpad, Sam Elo, Sloan, 1978Ishi Press International</p>
<p>Goodhart's law and performance indicators in higher education. Lewis Elton, Evaluation &amp; Research in Education. 181-22004</p>
<p>SaMer: A scenario-aware multi-dimensional evaluator for large language models. Kehua Feng, Keyan Ding, Jing Yu, Yiwen Qu, Zhiwen Chen, Gang Yu, Qiang Zhang, Huajun Chen, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios N Angelopoulos, Jiantao Jiao, Banghua Zhu, Joseph E Gonzalez, Ion Stoica, arXiv:2410.14872How to evaluate reward models for RLHF. 2024</p>
<p>Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv:2407.21783The LLaMA 3 herd of models. 2024</p>
<p>Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, arXiv:2310.19736Evaluating large language models: A comprehensive survey. 2023</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020</p>
<ol>
<li>C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, arXiv:2305.08322</li>
</ol>
<p>Codesearchnet challenge: Evaluating the state of semantic code search. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, Marc Brockschmidt, arXiv:1909.094362019</p>
<p>A survey on contrastive selfsupervised learning. Ashish Jaiswal, Ramesh Ashwin, Mohammad Zaki Babu, Debapriya Zadeh, Fillia Banerjee, Makedon, arXiv:2011.003622020</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7B. 2023</p>
<p>Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, arXiv:2104.14337Pratik Ringshia, et al. 2021. Dynabench: Rethinking benchmarking in NLP. </p>
<p>Prometheus: Inducing fine-grained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Prometheus 2: An open source language model specialized in evaluating other language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Y Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, arXiv:2405.015352024</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody H Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, Symposium on Operating Systems Principles. 2023</p>
<p>Guohao Li, Hasan Abed, Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, arXiv:2303.17760CAMEL: Communicative agents for "mind" exploration of large language model society. 2023a</p>
<p>Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, Pengfei Liu, arXiv:2310.05470Generative judge for evaluating alignment. 2023b</p>
<p>Evaluation on ChatGPT for Chinese language understanding. Linhan Li, Huaping Zhang, Chunjin Li, Haowen You, Wenyao Cui, Data Intelligence. 542023c</p>
<p>AlpacaEval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, GitHub repository. 2023d</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, The Twelfth International Conference on Learning Representations. Bowen Baker, Teddy LeeJan. 2023</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. 2004</p>
<p>Swaroop Mishra, Daniel Khashabi, arXiv:2104.08773Chitta Baral, and Hannaneh Hajishirzi. 2021. Cross-task generalization via natural language crowdsourcing instructions. </p>
<p>SG-Bench: Evaluating LLM safety generalization across diverse tasks and prompt types. Yutao Mou, Shikun Zhang, Wei Ye, Advances in Neural Information Processing Systems. 2024</p>
<p>Why we need new evaluation metrics for NLG. Jekaterina Novikova, Ondřej Dušek, Amanda Cercas Curry, Verena Rieser, Empirical Methods in Natural Language Processing. 2017</p>
<p>arXiv:2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 2022</p>
<p>BLEU: A method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Annual Meeting of the Association for Computational Linguistics. 2002</p>
<p>GPQA: A graduate-level google-proof Q&amp;A benchmark. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, arXiv:2311.120222023</p>
<p>Pretraining on the test set is all you need. Rylan Schaeffer, arXiv:2309.086322023</p>
<p>Variational adversarial active learning. Samarth Sinha, Sayna Ebrahimi, Trevor Darrell, International Conference on Computer Vision. 2019</p>
<p>MultiChallenge: A realistic multiturn conversation evaluation benchmark challenging to frontier LLMs. Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, Chen Xing, arXiv:2501.173992025</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022</p>
<p>Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y Tang, Alejandro Cuadron, Chenguang Wang, arXiv:2410.12784Raluca Ada Popa, and Ion Stoica. 2024. JudgeBench: A benchmark for evaluating LLMbased judges. </p>
<p>Fangyun Wei, Xi Chen, Lin Luo, arXiv:2403.07872Rethinking generative large language model evaluation for semantic comprehension. 2024</p>
<p>Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, Swarnadeep Saha, arXiv:2505.10320J1: Incentivizing thinking in LLM-as-a-judge via reinforcement learning. 2025</p>
<p>Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, Dong Yu, arXiv:2410.10813LongMemEval: Benchmarking chat assistants on long-term interactive memory. 2024</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, arXiv:2304.12244WizardLM: Empowering large language models to follow complex instructions. 2023</p>
<p>Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen, arXiv:2310.07641Evaluating large language models at evaluating instruction following. 2023</p>
<p>Chen Zhang, Luis F D'haro, Chengguang Tang, Ke Shi, Guohua Tang, Haizhou Li, arXiv:2310.08958xDial-Eval: A multilingual open-domain dialogue evaluation benchmark. 2023a</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.09675BERTScore: Evaluating text generation with BERT. 2019</p>
<p>Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, Yongbin Li, arXiv:2308.01862Wider and deeper LLM networks are fairer LLM evaluators. 2023b</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, arXiv:2306.05685Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. 2023</p>
<p>AGIEval: A humancentric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, arXiv:2304.063642023</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, arXiv:2305.11206LIMA: Less is more for alignment. 2023a</p>
<p>Instruction-following evaluation for large language models. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, Le Hou, arXiv:2311.079112023b</p>
<p>Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, Jiawei Han, arXiv:2311.01964Don't make your LLM an evaluation benchmark cheater. 2023c</p>
<p>Lianghui Zhu, Xinggang Wang, Xinlong Wang, arXiv:2310.17631JudgeLM: Fine-tuned large language models are scalable judges. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
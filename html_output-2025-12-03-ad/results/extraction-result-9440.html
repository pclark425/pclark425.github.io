<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9440 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9440</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9440</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-59641c10ed7431a3cf841f308367dc2dc0281b74</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/59641c10ed7431a3cf841f308367dc2dc0281b74" target="_blank">What Makes Good In-Context Examples for GPT-3?</a></p>
                <p><strong>Paper Venue:</strong> Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out</p>
                <p><strong>Paper TL;DR:</strong> This work proposes to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt, and evaluates the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline.</p>
                <p><strong>Paper Abstract:</strong> GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3’s in-context learning capabilities.Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3’s power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (44.3% on the ToTTo dataset) and open-domain question answering (45.5% on the NQ dataset).</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9440.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9440.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KATE_vs_Random_Sentiment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KNN-Augmented In-Context Example Selection (KATE) vs Random Example Selection on Sentiment Analysis (IMDB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of few-shot in-context presentation formats: randomly sampled in-context examples vs. retrieved nearest-neighbor in-context examples (KATE) for GPT-3 on a cross-dataset sentiment transfer task (examples taken from SST-2, evaluation on IMDB).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentiment analysis (IMDB, examples from SST-2)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary sentiment classification of movie reviews (IMDB test set), using in-context examples selected from SST-2 training set to prompt GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context prompting: context C is k concatenated (input, label) examples separated by special token 'in'. Number of in-context examples used: k=3 (chosen because more did not help). Examples were either randomly sampled from SST-2 (Random) or retrieved nearest neighbors by sentence embeddings (KATE).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Random sampling of in-context examples (repeated 5 times to obtain mean and std). Also compared to kNN majority-vote baseline and a fine-tuned T5 model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>KATE_roberta accuracy: 91.99%; KATE_sst-2 accuracy: 93.43%; Random baseline accuracy: 87.95% ± 2.74; kNN_roberta accuracy: 50.20%; Fine-tuned T5 (3B) accuracy: 95.2%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>KATE_roberta vs Random: +4.04 percentage points (91.99 - 87.95). KATE_sst-2 vs Random: +5.48 pp. KATE_sst-2 is closer to fine-tuned T5 but still lower by ~1.77 pp.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+4.04 to +5.48 percentage points accuracy improvement for KATE (retrieved nearest neighbors) over Random sampling depending on encoder; large negative effect for using kNN majority-vote instead of GPT-3 prompting (~-38 pp compared to KATE_roberta).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that semantically similar in-context examples provide more informative templates and relevant details that GPT-3 can adapt from, improving generation/classification; fine-tuning the sentence encoder on a task-similar dataset (SST-2) further improves retrieval quality and thus prompting performance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>k=3 in-context examples; sentence encoders: RoBERTa-large (pretrained) and RoBERTa fine-tuned on SST-2 (KATE_sst-2). Temperature set to 0. Random baseline repeated 5 times to compute mean±std.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9440.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9440.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KATE_vs_Random_ToTTo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KNN-Augmented In-Context Example Selection (KATE) vs Random Example Selection on Table-to-Text (ToTTo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of prompt presentation formats (retrieved semantically-similar in-context exemplars vs randomly sampled exemplars) for GPT-3 on the ToTTo table-to-text generation task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Table-to-text generation (ToTTo dev)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate human-readable descriptive sentences from a table and highlighted cells; evaluated with BLEU and PARENT metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context prompting: concatenation of k training (table, target sentence) pairs plus test table. Number of in-context examples used: k=2 to meet token limit (GPT-3 token limit 2048). Retrieval used nearest neighbors selected in sentence-embedding space to form the context (KATE).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Random sampling of in-context examples; kNN baseline (use top-1 retrieved example's target or majority voting) and fine-tuned T5 (3B) as reference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Overall BLEU: KATE_roberta 41.0 vs Random 28.4 ± 2.1; Overall PARENT: KATE_roberta 50.6 vs Random 39.3 ± 2.6. kNN_roberta BLEU: 14.1, PARENT: 12.6. Fine-tuned T5 (3B) BLEU: 41.2, PARENT: 53.0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>KATE_roberta vs Random: +12.6 BLEU and +11.3 PARENT (absolute increases). KATE_roberta performs comparable to T5 (BLEU 41.0 vs 41.2), slightly lower in PARENT.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+~12.6 BLEU and +~11.3 PARENT improvement from using retrieved nearest-neighbor in-context examples (KATE) over Random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Retrieved exemplars provide relevant template structure and concrete details (e.g., numeric fields) that guide GPT-3 to extract correct information and avoid hallucination observed with Random prompts. Fine-tuning retrieval encoder on dissimilar tasks can hurt retrieval relevance and reduce gains.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>k=2 in-context examples to fit token limit; preprocessing removed some closing tags to save tokens. Sentence encoders: RoBERTa-large pretrained, and RoBERTa fine-tuned on NLI / NLI+STS-B (KATE_nli, KATE_nli+sts-b). Observed slight drops when encoder was fine-tuned on dissimilar tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9440.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9440.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KATE_vs_Random_QA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KNN-Augmented In-Context Example Selection (KATE) vs Random Example Selection on Open-Domain QA (NQ, WQ, TriviaQA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of how the presentation format (retrieved semantically-similar in-context examples vs random) affects GPT-3 exact-match performance on multiple open-domain QA benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain Question Answering (Natural Questions (NQ), WebQuestions (WQ), TriviaQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer factoid questions; evaluation by Exact Match (EM) after normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context prompting with k examples concatenated (examples are question-answer pairs). Number of in-context examples: k=64 for NQ and WQ (token limit allowed), k=10 for TriviaQA (token limit constraint). Retrieval selects nearest neighbors in sentence-encoder embedding space (KATE).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Random sampling of in-context examples (repeated 5 times); kNN baseline using top-1 neighbor or majority voting across k neighbors; comparisons to fine-tuned models (T5 closed-book and RAG retrieval-augmented).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>NQ EM: Random 28.6 ± 0.3; GPT-3 (64 examples no retrieval) 29.9 (reported); KATE_roberta 40.0; KATE_nli 40.8; KATE_nli+sts-b 41.6. WQ EM: Random 41.0 ± 0.5; KATE_roberta 47.7; KATE_nli 50.6; KATE_nli+sts-b 50.2. TriviaQA EM (dev): Random 59.2 ± 0.4; KATE_roberta 57.5; KATE_nli 60.9; KATE_nli+sts-b 62.4. Baselines: RAG 44.5 (NQ), T5 (closed-book) 34.5 (NQ). kNN_roberta NQ: 24.0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>KATE_roberta vs Random: NQ +11.4 pp (40.0 - 28.6); WQ +6.7 pp (47.7 - 41.0); TriviaQA mixed depending on encoder (+3.2 pp for KATE_nli; +3.2 pp for KATE_nli+sts-b over Random). KATE variants (nli / nli+sts-b) can surpass fine-tuned T5 closed-book and in some cases RAG on benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Up to +~11.4 percentage points EM on NQ when switching from Random in-context examples to retrieved nearest neighbors (KATE); encoder fine-tuning (NLI/STS-B) can add further modest gains (~0.8-1.6 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Semantically similar QA exemplars supply relevant factual patterns and cues that help GPT-3 recall or assemble correct answers; encoders fine-tuned on NLI/STS-B improve retrieval quality for QA, boosting the effectiveness of in-context examples. The kNN prediction baseline performs poorly, indicating GPT-3's generative adaptation (given good contexts) is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>k=64 examples for NQ and WQ, k=10 for TriviaQA. Sentence encoders: RoBERTa-large pretrained (KATE_roberta), RoBERTa fine-tuned on SNLI+MultiNLI (KATE_nli), and then further on STS-B (KATE_nli+sts-b). Negative Euclidean distance or cosine similarity used depending on encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9440.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9440.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nearest_vs_Farthest_NN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nearest-Neighbor vs Farthest-Neighbor In-Context Example Selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct comparison testing how the semantic proximity of retrieved in-context examples (closest vs farthest in embedding space) affects GPT-3 QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Questions (subset of 100 test questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain QA evaluated with Exact Match (EM); experiment compares using 10 closest training instances vs 10 farthest as in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context prompting with 10 examples; examples chosen as either the 10 nearest neighbors or the 10 farthest neighbors in RoBERTa-large CLS embedding Euclidean space.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Closest 10 neighbors vs Farthest 10 neighbors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Authors report that nearest neighbors give rise to much better EM relative to the farthest ones (exact numeric EM values not provided in the main text excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Semantically similar examples (closest neighbors) share lexical/semantic cues with the test query, providing more informative conditional context for GPT-3; farthest examples are less relevant and degrade in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>100 test questions randomly sampled; embeddings: CLS token from pre-trained RoBERTa-large; similarity measured by Euclidean distance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9440.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9440.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Example_Order_Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Order of In-Context Examples Effect on GPT-3 (default vs reverse vs random permutations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis of whether the ordering of in-context examples (most-similar-first vs least-similar-first vs random permutations) affects GPT-3 performance for KATE retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Questions (NQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain QA evaluated with Exact Match (EM); experiments permute the order of retrieved in-context examples and test default vs reverse ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context prompting with ordered retrieved exemplars; default order = most similar first; reverse order = least similar first; also random permutations tested.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Default (most similar first) vs Reverse (least similar first) vs Random permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Default EM: 41.6; Reverse EM: 42.8. Three random permutations produced EMs: 42.0, 42.5, 42.0.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Reverse vs Default: +1.2 EM absolute (42.8 - 41.6). Variation across random permutations small (~±0.5 EM).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+~1.2 percentage points EM difference between best and default ordering in this NQ study; overall ordering effects are small relative to selection method effects.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Order effect is data-dependent and generally small; authors suggest example order does not have a significant impact compared to choice of which examples are included.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Experiment run with KATE_nli+sts-b on NQ. Results indicate small variability across orders; default performed slightly better on WQ and TriviaQA but reverse performed best on NQ in this run.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9440.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9440.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Num_Examples_Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Number of In-Context Examples (k) on GPT-3 Performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Investigation of how varying the number of in-context examples provided in the prompt affects GPT-3 performance for KATE vs Random selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Questions (NQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain QA evaluated with Exact Match (EM); k varied among {5, 10, 20, 35, 64}.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context prompting with variable k retrieved examples (KATE) or k randomly sampled examples (Random).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Different k values compared for KATE_nli+sts-b, KATE_roberta, and Random.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Authors report both KATE and Random benefit from more examples, but KATE consistently outperforms Random at every tested k, including as few as 5 examples (exact EM numbers per k are presented in figure but not in text excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>More in-context examples generally provide more signal for GPT-3, improving performance; retrieval-based selection yields stronger gains per additional example because examples are more relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>k ∈ {5, 10, 20, 35, 64}; encoder KATE_nli+sts-b compared vs KATE_roberta and Random; temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9440.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9440.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval_Pool_Size_Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Training/Corpus Size for Retrieval Pool on KATE Performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study on how the size of the available retrieval corpus (number of training instances from which to retrieve in-context examples) affects KATE's ability to improve GPT-3 performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Questions (NQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain QA evaluated with Exact Match (EM); retrieval pool sizes varied from 1k to 70k training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context prompting where the k in-context examples are retrieved from training subsets of varying size (1k, 2k, 5k, 10k, 30k, 70k).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Different retrieval pool sizes compared for KATE_roberta, KATE_nli+sts-b, and Random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Authors report EM for KATE_roberta and KATE_nli+sts-b increases as retrieval pool size increases; Random baseline EM remains roughly unchanged (exact numeric values are shown in a figure not included in the excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>A larger retrieval pool increases the chance of finding highly relevant exemplars, improving the quality of in-context examples and thus GPT-3 performance; random sampling does not benefit because randomness is unaffected by pool size.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Number of retrieved nearest neighbors set to 64 for the study; encoders compared: KATE_roberta and KATE_nli+sts-b.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9440.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9440.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kNN_Prediction_vs_Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>kNN Majority-Vote Prediction Baseline vs GPT-3 Prompting with Retrieved Examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison between using retrieved exemplars as a non-parametric majority-vote classifier (kNN) and using those exemplars as in-context prompt for GPT-3 (KATE).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>kNN (non-parametric baseline) and GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple: Sentiment (IMDB), Table-to-Text (ToTTo), QA (NQ/WQ/TriviaQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>kNN baseline uses the label/answer(s) of nearest retrieved training examples and predicts by majority vote; compared against GPT-3 which conditions generation on same retrieved examples (KATE).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>kNN prediction: no prompt to GPT-3 (or used only for evaluation); KATE prompting: retrieved examples concatenated as prompt for GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>kNN majority voting vs GPT-3 in-context generation using same retrieval embeddings (same embedding space for fair comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>kNN_roberta accuracies/scores: IMDB 50.20% accuracy; ToTTo BLEU 14.1 / PARENT 12.6; QA NQ 24.0 EM / WQ 23.9 EM / TriviaQA 26.2 EM. By contrast KATE_roberta: IMDB 91.99% accuracy; ToTTo BLEU 41.0 / PARENT 50.6; QA NQ 40.0 EM / WQ 47.7 EM / TriviaQA 57.5 EM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>kNN baseline performs substantially worse than using retrieved exemplars as prompt context for GPT-3 (KATE), often near random or far lower than KATE (e.g., IMDB: 50.2% vs 91.99%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large negative effect when using retrieved labels directly (kNN) rather than using retrieved examples as prompts (KATE); differences often tens of percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>GPT-3's generative adaptation to exemplar contexts is critical; retrieved exemplars serve as better conditioning context than simply voting labels because GPT-3 can apply patterns/structures from examples to produce correct outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>kNN and KATE compared using same embedding space (pretrained RoBERTa-large). For sentiment and QA tasks, majority voting among top-k retrieved labels used with tie-break by most similar example.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9440.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9440.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random_Context_Variance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variance Caused by Random Choice of In-Context Examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical demonstration that different randomly sampled in-context examples can induce substantial variability in GPT-3 performance on the same task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 (illustration) and IMDB random-baseline repeats</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Shows that repeating the random in-context selection yields varying test accuracies; demonstrates sensitivity of GPT-3 to prompt exemplar choice.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context prompting with randomly sampled training examples (as used in the original GPT-3 paper and as baseline here).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Multiple independent random draws of in-context examples compared to retrieval-based selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 1 (SST-2 trials): accuracies across five random example sets: 94.6, 95.0, 95.8, 93.9, 86.9 (range = 8.9 pp). IMDB Random baseline reported as 87.95% ± 2.74 (std over 5 repeats).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Illustrates that random choice of examples can change accuracy by several percentage points (up to ~9 pp in the shown SST-2 trials), causing instability in reported few-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Observed variability up to ~8.9 percentage points across five different random context draws in SST-2 trial; IMDB random baseline std ≈ 2.74 pp.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Because GPT-3 conditions strongly on the specific exemplars in the prompt, different example choices introduce different inductive biases/templates, causing substantial variability in output and evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>SST-2 example: five different sets of in-context examples randomly selected from training; IMDB random baseline averaged over 5 repeats to compute mean±std. Temperature set to 0.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 2)</em></li>
                <li>Sentence-BERT: Sentence embeddings using siamese BERT-networks <em>(Rating: 2)</em></li>
                <li>Making pre-trained language models better few-shot learners <em>(Rating: 1)</em></li>
                <li>Nearest neighbor machine translation <em>(Rating: 1)</em></li>
                <li>Generalization through memorization: Nearest neighbor language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9440",
    "paper_id": "paper-59641c10ed7431a3cf841f308367dc2dc0281b74",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "KATE_vs_Random_Sentiment",
            "name_full": "KNN-Augmented In-Context Example Selection (KATE) vs Random Example Selection on Sentiment Analysis (IMDB)",
            "brief_description": "Comparison of few-shot in-context presentation formats: randomly sampled in-context examples vs. retrieved nearest-neighbor in-context examples (KATE) for GPT-3 on a cross-dataset sentiment transfer task (examples taken from SST-2, evaluation on IMDB).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "task_name": "Sentiment analysis (IMDB, examples from SST-2)",
            "task_description": "Binary sentiment classification of movie reviews (IMDB test set), using in-context examples selected from SST-2 training set to prompt GPT-3.",
            "presentation_format": "Few-shot in-context prompting: context C is k concatenated (input, label) examples separated by special token 'in'. Number of in-context examples used: k=3 (chosen because more did not help). Examples were either randomly sampled from SST-2 (Random) or retrieved nearest neighbors by sentence embeddings (KATE).",
            "comparison_format": "Random sampling of in-context examples (repeated 5 times to obtain mean and std). Also compared to kNN majority-vote baseline and a fine-tuned T5 model.",
            "performance": "KATE_roberta accuracy: 91.99%; KATE_sst-2 accuracy: 93.43%; Random baseline accuracy: 87.95% ± 2.74; kNN_roberta accuracy: 50.20%; Fine-tuned T5 (3B) accuracy: 95.2%",
            "performance_comparison": "KATE_roberta vs Random: +4.04 percentage points (91.99 - 87.95). KATE_sst-2 vs Random: +5.48 pp. KATE_sst-2 is closer to fine-tuned T5 but still lower by ~1.77 pp.",
            "format_effect_size": "+4.04 to +5.48 percentage points accuracy improvement for KATE (retrieved nearest neighbors) over Random sampling depending on encoder; large negative effect for using kNN majority-vote instead of GPT-3 prompting (~-38 pp compared to KATE_roberta).",
            "explanation_or_hypothesis": "Authors hypothesize that semantically similar in-context examples provide more informative templates and relevant details that GPT-3 can adapt from, improving generation/classification; fine-tuning the sentence encoder on a task-similar dataset (SST-2) further improves retrieval quality and thus prompting performance.",
            "null_or_negative_result": false,
            "experimental_details": "k=3 in-context examples; sentence encoders: RoBERTa-large (pretrained) and RoBERTa fine-tuned on SST-2 (KATE_sst-2). Temperature set to 0. Random baseline repeated 5 times to compute mean±std.",
            "uuid": "e9440.0",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "KATE_vs_Random_ToTTo",
            "name_full": "KNN-Augmented In-Context Example Selection (KATE) vs Random Example Selection on Table-to-Text (ToTTo)",
            "brief_description": "Comparison of prompt presentation formats (retrieved semantically-similar in-context exemplars vs randomly sampled exemplars) for GPT-3 on the ToTTo table-to-text generation task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "task_name": "Table-to-text generation (ToTTo dev)",
            "task_description": "Generate human-readable descriptive sentences from a table and highlighted cells; evaluated with BLEU and PARENT metrics.",
            "presentation_format": "Few-shot in-context prompting: concatenation of k training (table, target sentence) pairs plus test table. Number of in-context examples used: k=2 to meet token limit (GPT-3 token limit 2048). Retrieval used nearest neighbors selected in sentence-embedding space to form the context (KATE).",
            "comparison_format": "Random sampling of in-context examples; kNN baseline (use top-1 retrieved example's target or majority voting) and fine-tuned T5 (3B) as reference.",
            "performance": "Overall BLEU: KATE_roberta 41.0 vs Random 28.4 ± 2.1; Overall PARENT: KATE_roberta 50.6 vs Random 39.3 ± 2.6. kNN_roberta BLEU: 14.1, PARENT: 12.6. Fine-tuned T5 (3B) BLEU: 41.2, PARENT: 53.0",
            "performance_comparison": "KATE_roberta vs Random: +12.6 BLEU and +11.3 PARENT (absolute increases). KATE_roberta performs comparable to T5 (BLEU 41.0 vs 41.2), slightly lower in PARENT.",
            "format_effect_size": "+~12.6 BLEU and +~11.3 PARENT improvement from using retrieved nearest-neighbor in-context examples (KATE) over Random sampling.",
            "explanation_or_hypothesis": "Retrieved exemplars provide relevant template structure and concrete details (e.g., numeric fields) that guide GPT-3 to extract correct information and avoid hallucination observed with Random prompts. Fine-tuning retrieval encoder on dissimilar tasks can hurt retrieval relevance and reduce gains.",
            "null_or_negative_result": false,
            "experimental_details": "k=2 in-context examples to fit token limit; preprocessing removed some closing tags to save tokens. Sentence encoders: RoBERTa-large pretrained, and RoBERTa fine-tuned on NLI / NLI+STS-B (KATE_nli, KATE_nli+sts-b). Observed slight drops when encoder was fine-tuned on dissimilar tasks.",
            "uuid": "e9440.1",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "KATE_vs_Random_QA",
            "name_full": "KNN-Augmented In-Context Example Selection (KATE) vs Random Example Selection on Open-Domain QA (NQ, WQ, TriviaQA)",
            "brief_description": "Evaluation of how the presentation format (retrieved semantically-similar in-context examples vs random) affects GPT-3 exact-match performance on multiple open-domain QA benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "task_name": "Open-domain Question Answering (Natural Questions (NQ), WebQuestions (WQ), TriviaQA)",
            "task_description": "Answer factoid questions; evaluation by Exact Match (EM) after normalization.",
            "presentation_format": "Few-shot in-context prompting with k examples concatenated (examples are question-answer pairs). Number of in-context examples: k=64 for NQ and WQ (token limit allowed), k=10 for TriviaQA (token limit constraint). Retrieval selects nearest neighbors in sentence-encoder embedding space (KATE).",
            "comparison_format": "Random sampling of in-context examples (repeated 5 times); kNN baseline using top-1 neighbor or majority voting across k neighbors; comparisons to fine-tuned models (T5 closed-book and RAG retrieval-augmented).",
            "performance": "NQ EM: Random 28.6 ± 0.3; GPT-3 (64 examples no retrieval) 29.9 (reported); KATE_roberta 40.0; KATE_nli 40.8; KATE_nli+sts-b 41.6. WQ EM: Random 41.0 ± 0.5; KATE_roberta 47.7; KATE_nli 50.6; KATE_nli+sts-b 50.2. TriviaQA EM (dev): Random 59.2 ± 0.4; KATE_roberta 57.5; KATE_nli 60.9; KATE_nli+sts-b 62.4. Baselines: RAG 44.5 (NQ), T5 (closed-book) 34.5 (NQ). kNN_roberta NQ: 24.0",
            "performance_comparison": "KATE_roberta vs Random: NQ +11.4 pp (40.0 - 28.6); WQ +6.7 pp (47.7 - 41.0); TriviaQA mixed depending on encoder (+3.2 pp for KATE_nli; +3.2 pp for KATE_nli+sts-b over Random). KATE variants (nli / nli+sts-b) can surpass fine-tuned T5 closed-book and in some cases RAG on benchmarks.",
            "format_effect_size": "Up to +~11.4 percentage points EM on NQ when switching from Random in-context examples to retrieved nearest neighbors (KATE); encoder fine-tuning (NLI/STS-B) can add further modest gains (~0.8-1.6 pp).",
            "explanation_or_hypothesis": "Semantically similar QA exemplars supply relevant factual patterns and cues that help GPT-3 recall or assemble correct answers; encoders fine-tuned on NLI/STS-B improve retrieval quality for QA, boosting the effectiveness of in-context examples. The kNN prediction baseline performs poorly, indicating GPT-3's generative adaptation (given good contexts) is critical.",
            "null_or_negative_result": false,
            "experimental_details": "k=64 examples for NQ and WQ, k=10 for TriviaQA. Sentence encoders: RoBERTa-large pretrained (KATE_roberta), RoBERTa fine-tuned on SNLI+MultiNLI (KATE_nli), and then further on STS-B (KATE_nli+sts-b). Negative Euclidean distance or cosine similarity used depending on encoder.",
            "uuid": "e9440.2",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "Nearest_vs_Farthest_NN",
            "name_full": "Nearest-Neighbor vs Farthest-Neighbor In-Context Example Selection",
            "brief_description": "Direct comparison testing how the semantic proximity of retrieved in-context examples (closest vs farthest in embedding space) affects GPT-3 QA performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "task_name": "Natural Questions (subset of 100 test questions)",
            "task_description": "Open-domain QA evaluated with Exact Match (EM); experiment compares using 10 closest training instances vs 10 farthest as in-context examples.",
            "presentation_format": "Few-shot in-context prompting with 10 examples; examples chosen as either the 10 nearest neighbors or the 10 farthest neighbors in RoBERTa-large CLS embedding Euclidean space.",
            "comparison_format": "Closest 10 neighbors vs Farthest 10 neighbors.",
            "performance": null,
            "performance_comparison": "Authors report that nearest neighbors give rise to much better EM relative to the farthest ones (exact numeric EM values not provided in the main text excerpt).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Semantically similar examples (closest neighbors) share lexical/semantic cues with the test query, providing more informative conditional context for GPT-3; farthest examples are less relevant and degrade in-context learning.",
            "null_or_negative_result": false,
            "experimental_details": "100 test questions randomly sampled; embeddings: CLS token from pre-trained RoBERTa-large; similarity measured by Euclidean distance.",
            "uuid": "e9440.3",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "Example_Order_Effect",
            "name_full": "Order of In-Context Examples Effect on GPT-3 (default vs reverse vs random permutations)",
            "brief_description": "Analysis of whether the ordering of in-context examples (most-similar-first vs least-similar-first vs random permutations) affects GPT-3 performance for KATE retrieval.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "task_name": "Natural Questions (NQ)",
            "task_description": "Open-domain QA evaluated with Exact Match (EM); experiments permute the order of retrieved in-context examples and test default vs reverse ordering.",
            "presentation_format": "Few-shot in-context prompting with ordered retrieved exemplars; default order = most similar first; reverse order = least similar first; also random permutations tested.",
            "comparison_format": "Default (most similar first) vs Reverse (least similar first) vs Random permutations.",
            "performance": "Default EM: 41.6; Reverse EM: 42.8. Three random permutations produced EMs: 42.0, 42.5, 42.0.",
            "performance_comparison": "Reverse vs Default: +1.2 EM absolute (42.8 - 41.6). Variation across random permutations small (~±0.5 EM).",
            "format_effect_size": "+~1.2 percentage points EM difference between best and default ordering in this NQ study; overall ordering effects are small relative to selection method effects.",
            "explanation_or_hypothesis": "Order effect is data-dependent and generally small; authors suggest example order does not have a significant impact compared to choice of which examples are included.",
            "null_or_negative_result": false,
            "experimental_details": "Experiment run with KATE_nli+sts-b on NQ. Results indicate small variability across orders; default performed slightly better on WQ and TriviaQA but reverse performed best on NQ in this run.",
            "uuid": "e9440.4",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "Num_Examples_Effect",
            "name_full": "Effect of Number of In-Context Examples (k) on GPT-3 Performance",
            "brief_description": "Investigation of how varying the number of in-context examples provided in the prompt affects GPT-3 performance for KATE vs Random selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "task_name": "Natural Questions (NQ)",
            "task_description": "Open-domain QA evaluated with Exact Match (EM); k varied among {5, 10, 20, 35, 64}.",
            "presentation_format": "Few-shot in-context prompting with variable k retrieved examples (KATE) or k randomly sampled examples (Random).",
            "comparison_format": "Different k values compared for KATE_nli+sts-b, KATE_roberta, and Random.",
            "performance": null,
            "performance_comparison": "Authors report both KATE and Random benefit from more examples, but KATE consistently outperforms Random at every tested k, including as few as 5 examples (exact EM numbers per k are presented in figure but not in text excerpt).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "More in-context examples generally provide more signal for GPT-3, improving performance; retrieval-based selection yields stronger gains per additional example because examples are more relevant.",
            "null_or_negative_result": false,
            "experimental_details": "k ∈ {5, 10, 20, 35, 64}; encoder KATE_nli+sts-b compared vs KATE_roberta and Random; temperature=0.",
            "uuid": "e9440.5",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "Retrieval_Pool_Size_Effect",
            "name_full": "Effect of Training/Corpus Size for Retrieval Pool on KATE Performance",
            "brief_description": "Study on how the size of the available retrieval corpus (number of training instances from which to retrieve in-context examples) affects KATE's ability to improve GPT-3 performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "task_name": "Natural Questions (NQ)",
            "task_description": "Open-domain QA evaluated with Exact Match (EM); retrieval pool sizes varied from 1k to 70k training examples.",
            "presentation_format": "Few-shot in-context prompting where the k in-context examples are retrieved from training subsets of varying size (1k, 2k, 5k, 10k, 30k, 70k).",
            "comparison_format": "Different retrieval pool sizes compared for KATE_roberta, KATE_nli+sts-b, and Random sampling.",
            "performance": null,
            "performance_comparison": "Authors report EM for KATE_roberta and KATE_nli+sts-b increases as retrieval pool size increases; Random baseline EM remains roughly unchanged (exact numeric values are shown in a figure not included in the excerpt).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "A larger retrieval pool increases the chance of finding highly relevant exemplars, improving the quality of in-context examples and thus GPT-3 performance; random sampling does not benefit because randomness is unaffected by pool size.",
            "null_or_negative_result": false,
            "experimental_details": "Number of retrieved nearest neighbors set to 64 for the study; encoders compared: KATE_roberta and KATE_nli+sts-b.",
            "uuid": "e9440.6",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "kNN_Prediction_vs_Prompting",
            "name_full": "kNN Majority-Vote Prediction Baseline vs GPT-3 Prompting with Retrieved Examples",
            "brief_description": "Comparison between using retrieved exemplars as a non-parametric majority-vote classifier (kNN) and using those exemplars as in-context prompt for GPT-3 (KATE).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "kNN (non-parametric baseline) and GPT-3",
            "model_size": null,
            "task_name": "Multiple: Sentiment (IMDB), Table-to-Text (ToTTo), QA (NQ/WQ/TriviaQA)",
            "task_description": "kNN baseline uses the label/answer(s) of nearest retrieved training examples and predicts by majority vote; compared against GPT-3 which conditions generation on same retrieved examples (KATE).",
            "presentation_format": "kNN prediction: no prompt to GPT-3 (or used only for evaluation); KATE prompting: retrieved examples concatenated as prompt for GPT-3.",
            "comparison_format": "kNN majority voting vs GPT-3 in-context generation using same retrieval embeddings (same embedding space for fair comparison).",
            "performance": "kNN_roberta accuracies/scores: IMDB 50.20% accuracy; ToTTo BLEU 14.1 / PARENT 12.6; QA NQ 24.0 EM / WQ 23.9 EM / TriviaQA 26.2 EM. By contrast KATE_roberta: IMDB 91.99% accuracy; ToTTo BLEU 41.0 / PARENT 50.6; QA NQ 40.0 EM / WQ 47.7 EM / TriviaQA 57.5 EM.",
            "performance_comparison": "kNN baseline performs substantially worse than using retrieved exemplars as prompt context for GPT-3 (KATE), often near random or far lower than KATE (e.g., IMDB: 50.2% vs 91.99%).",
            "format_effect_size": "Large negative effect when using retrieved labels directly (kNN) rather than using retrieved examples as prompts (KATE); differences often tens of percentage points.",
            "explanation_or_hypothesis": "GPT-3's generative adaptation to exemplar contexts is critical; retrieved exemplars serve as better conditioning context than simply voting labels because GPT-3 can apply patterns/structures from examples to produce correct outputs.",
            "null_or_negative_result": false,
            "experimental_details": "kNN and KATE compared using same embedding space (pretrained RoBERTa-large). For sentiment and QA tasks, majority voting among top-k retrieved labels used with tie-break by most similar example.",
            "uuid": "e9440.7",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "Random_Context_Variance",
            "name_full": "Variance Caused by Random Choice of In-Context Examples",
            "brief_description": "Empirical demonstration that different randomly sampled in-context examples can induce substantial variability in GPT-3 performance on the same task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "task_name": "SST-2 (illustration) and IMDB random-baseline repeats",
            "task_description": "Shows that repeating the random in-context selection yields varying test accuracies; demonstrates sensitivity of GPT-3 to prompt exemplar choice.",
            "presentation_format": "Few-shot in-context prompting with randomly sampled training examples (as used in the original GPT-3 paper and as baseline here).",
            "comparison_format": "Multiple independent random draws of in-context examples compared to retrieval-based selection.",
            "performance": "Table 1 (SST-2 trials): accuracies across five random example sets: 94.6, 95.0, 95.8, 93.9, 86.9 (range = 8.9 pp). IMDB Random baseline reported as 87.95% ± 2.74 (std over 5 repeats).",
            "performance_comparison": "Illustrates that random choice of examples can change accuracy by several percentage points (up to ~9 pp in the shown SST-2 trials), causing instability in reported few-shot performance.",
            "format_effect_size": "Observed variability up to ~8.9 percentage points across five different random context draws in SST-2 trial; IMDB random baseline std ≈ 2.74 pp.",
            "explanation_or_hypothesis": "Because GPT-3 conditions strongly on the specific exemplars in the prompt, different example choices introduce different inductive biases/templates, causing substantial variability in output and evaluation metrics.",
            "null_or_negative_result": false,
            "experimental_details": "SST-2 example: five different sets of in-context examples randomly selected from training; IMDB random baseline averaged over 5 repeats to compute mean±std. Temperature set to 0.",
            "uuid": "e9440.8",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 2
        },
        {
            "paper_title": "Sentence-BERT: Sentence embeddings using siamese BERT-networks",
            "rating": 2
        },
        {
            "paper_title": "Making pre-trained language models better few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "Nearest neighbor machine translation",
            "rating": 1
        },
        {
            "paper_title": "Generalization through memorization: Nearest neighbor language models",
            "rating": 1
        }
    ],
    "cost": 0.01889575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>What Makes Good In-Context Examples for GPT-3?</h1>
<p>Jiachang Liu ${ }^{1}$, Dinghan Shen ${ }^{2}$, Yizhe Zhang ${ }^{3}$, Bill Dolan ${ }^{4}$, Lawrence Carin ${ }^{1}$, Weizhu Chen ${ }^{2}$<br>${ }^{1}$ Duke University ${ }^{2}$ Microsoft Dynamics $365 \mathrm{AI} \quad{ }^{3}$ Meta AI ${ }^{4}$ Microsoft Research<br>${ }^{1}$ {jiachang.liu, lcarin}@duke.edu<br>${ }^{3}$ yizhe.zhang@hotmail.com<br>${ }^{2,4}$ {dishen, billdol, wzchen}@microsoft.com</p>
<h4>Abstract</h4>
<p>GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting incontext examples (relative to random sampling) that better leverage GPT-3's in-context learning capabilities. Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3's power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders finetuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-totext generation ( $44.3 \%$ on the ToTTo dataset) and open-domain question answering ( $45.5 \%$ on the NQ dataset).</p>
<h2>1 Introduction</h2>
<p>GPT-3 (Brown et al., 2020) is a new breakthrough in NLP research. Previously, NLP models are firstly pre-trained and then fine-tuned on a specific task. What sets GPT-3 apart from other models is its impressive "in-context" learning ability. Provided with a few in-context examples, GPT-3 can generalize to unseen cases without further finetuning. This opens up many new technological possibilities that are previously considered unique</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>${ }^{4}$ Work was done when Jiachang (intern) and Yizhe were at Microsoft.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Trial</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">86.9</td>
</tr>
</tbody>
</table>
<p>Table 1: Results of GPT-3 on the SST-2 sentiment analysis dataset. Five different examples are randomly selected from the training set for each trial. Different contexts induce different accuracies on the test set.
to human. Future NLP systems can be developed to expand emails, extract entities from text, generate code based on natural language instructions with a few demonstration examples.</p>
<p>Despite its powerful and versatile in-context learning ability, GPT-3 has some practical challenges. The original paper utilizes task-relevant examples that are randomly sampled from the training set. However, we observe that the performance of GPT-3 tends to fluctuate with different choices of in-context examples. As shown in Table 1, the variance with distinct in-context examples can be significant. Our work aims to carefully examine this issue to gain a deeper understanding on how to better select in-context examples to improve GPT3 's performance without fine-tuning. Note that our approach requires a training set to select examples. With such a training dataset, it is possible to fine-tune GPT-3 to take full advantage of the model's strength. However, currently GPT-3 has not been released to public for fine-tuning. Even if it is available, fine-tuning GPT-3 requires hundreds of GPUs to load the 175B model, which is prohibitively expensive and time-consuming for ordinary research labs. Another issue is that storing large fine-tuned model checkpoints require huge storage space. Consequently, we resort to prompt/example engineering strategy. Nevertheless, the fine-tuning results using T5 are provided for reference.</p>
<p>A brute-force approach for selecting the optimal in-context instances would be to perform combinatorial search over the entire dataset. Unfortunately, this strategy is computationally impractical. To this</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: In-context example selection for GPT-3. White dots: unused training samples; grey dots: randomly sampled training samples; red dots: training samples selected by the k-nearest neighbors algorithm in the embedding space of a sentence encoder.
end, we empirically investigate the influences of employing different in-context examples. Interestingly, we find that the in-context examples that are closer to the test sample in the embedding space consistently give rise to stronger performance (relative to the farther ones). Inspired by this observation and the recent success of retrieval-augmented models Hashimoto et al., 2018), we propose to utilize nearest neighbors of a given test sample (among all the training instances available) as the in-context examples.</p>
<p>To verify the effectiveness of the proposed method, we evaluate it on several natural language understanding and generation tasks, including sentiment analysis, table-to-text generation and opendomain question answering. It is observed that the retrieval-based in-context examples unleash the in-context learning capabilities of GPT-3 much more effectively than the random sampling baseline, even when the number of examples is small. Moreover, we find that the specific sentence encoders employed for the retrieval procedure play a critical role. Thus, an extensive exploration is conducted and shows that encoders fine-tuned on natural language matching tasks serve as more effective in-context examples selector on the QA task. In summary, our contributions are as follows:
i) to the best of our knowledge, we take a first step towards understanding the sensitivity of GPT3's in-context learning ability with respect to the choice of in-context examples;
ii) to alleviate the sensitivity issue, an additional retrieval module is introduced to find semanticallysimilar in-context examples of a test instance, which greatly outperforms the baseline based on
randomly sampled in-context examples;
iii) empirically, the better selected examples lead GPT-3 to achieve comparable performance to a fine-tuned T5 model on the table-to-text task and outperforms the T5 model on the QA tasks;
iv) fine-tuning the retrieval model on task-related dataset(s) leads to stronger empirical results;
v) the performance of GPT-3 improves as the number of examples for retrieval increases.</p>
<h2>2 Method</h2>
<h3>2.1 GPT-3 for In-Context Learning</h3>
<p>The in-context learning scenario of GPT-3 can be regarded as a conditional text generation problem. Concretely, the probability of generating a target $y$ is conditioned on the context $C$, which includes $k$ examples, and the source $x$. Therefore, the probability can be expressed as:</p>
<p>$$
p_{\mathrm{LM}}(y \mid C, x)=\prod_{t=1}^{T} p\left(y_{t} \mid C, x, y_{&lt;t}\right)
$$</p>
<p>where LM denotes the parameters of the language model, and $C=\left{x_{1}, y_{1}, x_{2}, y_{2}, \ldots, x_{k}, y_{k}\right}$ is a context string concatenating $k$ training instances with the special character "in". A concrete illustration can be found in the Appendix.</p>
<p>For GPT-3, this generation process is implemented through a giant transformer-based architecture Vaswani et al., 2017). Due to the computational burden of fine-tuning, GPT-3 is leveraged in an in-context learning manner as described above. Unfortunately, as shown in Table 1, the results of GPT-3 tend to fluctuate significantly with different in-context examples. We aim to alleviate this issue via judicious in-context example selection.</p>
<h3>2.2 The Impact of In-Context Examples</h3>
<p>We start the investigation by looking at the role of in-context examples from an empirical perspective. Previous retrieve-and-edit literature usually retrieve prototypes that are close to the test source $x$ in some embedding space. These examples and the test source $x$ often share semantic or lexical similarities. This hints on how we may select incontext examples for GPT-3.</p>
<p>To this end, we examine the impact of the distance between the in-context example and the test sample on GPT-3's performance. Concretely, a comparison is made on the the Natural Questions (NQ) dataset between two selection strategies. Given a test example, the first method utilizes the 10 farthest training instances as the in-context examples, while the second employs the 10 closest neighbors. We use the CLS embeddings of a pre-trained RoBERTa-large model as sentence representations to measure the proximity of two sentences (using the Euclidean distance).</p>
<p>For evaluation, 100 test questions are randomly sampled and the average Exact Match (EM) scores with the two distinct strategies are reported in Table 2. It can be observed that the nearest neighbors, used as the in-context examples, give rise to much better results relative to the farthest ones. Moreover, the pre-trained RoBERTa model serves as effective sentence embeddings for the retrieval procedure.</p>
<h3>2.3 kNN-augmented Example Selection</h3>
<p>Based on the findings above, we propose KATE $^{1}$, a strategy to select good examples for in-context learning. The process is visualized in Figure 1. Specifically, we first use a sentence encoder to convert sources in both the training set and test set to vector representations. For online prediction, we can convert the training set first and encode each test source on the fly. Then, for each test source $x$, we retrieve its nearest $k$ neighbors $x_{1}, x_{2}, \ldots, x_{k}$ from the training set (according to the distances in the embedding space). Given some pre-defined similarity measure $s$ such as the negative Euclidean distance or the cosine similarity, the neighbors are ordered so that $s\left(x_{i}, x\right) \geq s\left(x_{j}, x\right)$ when $i&lt;j$.</p>
<p>The $k$ sources are concatenated with their targets to form the context $C=$ $\left{x_{1}, y_{1}, x_{2}, y_{2}, \ldots, x_{k}, y_{k}\right}$, which is sent to GPT-3 along with the test input. The algorithm is presented in Algorithm 1. Note that different</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Comparison of the EM score on the closest 10 neighbors and farthest 10 neighbors on a subset of 100 test samples of the NQ dataset.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="nt">kNN</span><span class="w"> </span><span class="nt">In-context</span><span class="w"> </span><span class="nt">Example</span><span class="w"> </span><span class="nt">Selection</span>
<span class="nt">Given</span><span class="o">:</span><span class="w"> </span><span class="nt">test</span><span class="w"> </span><span class="nt">prompt</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">),</span><span class="w"> </span><span class="nt">training</span><span class="w"> </span><span class="nt">set</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="o">)</span>
<span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">\boldsymbol{x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">y</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\}</span><span class="nt">_</span><span class="p">{</span><span class="err">i=1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">N</span><span class="p">}</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">sentence</span><span class="w"> </span><span class="nt">encoder</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mu_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="o">(</span><span class="err">\</span><span class="nt">cdot</span><span class="o">)</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">number</span>
<span class="nt">of</span><span class="w"> </span><span class="nt">in-context</span><span class="w"> </span><span class="nt">examples</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">k</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">(</span><span class="nt">hyperparameter</span><span class="o">).</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nt">mu_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">mu_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">=</span><span class="nt">-</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="o">|</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="nt">-</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\</span><span class="o">|</span><span class="nt">_</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">right</span><span class="o">.</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">.</span><span class="err">\</span><span class="nt">frac</span><span class="p">{</span><span class="err">\boldsymbol{v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cdot</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">}</span><span class="p">{</span><span class="err">\left\|\boldsymbol{v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="err">\</span><span class="o">|</span><span class="nt">_</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="o">|</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\</span><span class="o">|</span><span class="nt">_</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
<span class="w">    </span><span class="nt">Select</span><span class="w"> </span><span class="nt">largest</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">k</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">similarities</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="err">&#39;</span><span class="nt">s</span><span class="w"> </span><span class="o">(</span><span class="nt">in</span><span class="w"> </span><span class="nt">descending</span>
<span class="w">    </span><span class="nt">order</span><span class="o">)</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">indices</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="p">{</span><span class="err">\sigma(1),</span><span class="w"> </span><span class="err">\ldots,</span><span class="w"> </span><span class="err">\sigma(k)\</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="nt">C</span><span class="o">=</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="o">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="o">\</span><span class="nx">sigma</span><span class="p">(</span><span class="mi">1</span><span class="p">)}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="o">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="o">\</span><span class="nx">sigma</span><span class="p">(</span><span class="mi">1</span><span class="p">)}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="o">\</span><span class="nx">ldots</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="o">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="o">\</span><span class="nx">sigma</span><span class="p">(</span><span class="nx">k</span><span class="p">)}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="o">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="o">\</span><span class="nx">sigma</span><span class="p">(</span><span class="nx">k</span><span class="p">)}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">dot</span><span class="p">{</span><span class="err">\boldsymbol{y</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">GPT</span><span class="p">}</span><span class="nt">-3</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="nx">C</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="o">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="o">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">test</span><span class="w"> </span><span class="p">}}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
</code></pre></div>

<p>numbers of examples can be employed, and we conduct study on its impact in a later section.</p>
<p>Choices of Retrieval Module A core step for our context selection approach is mapping sentences into a latent semantic space, leaving a question as what sentence encoders we should choose. We compared among existing pre-trained text encoders and found them sufficient to retrieve semantically similar sentences. The sentence encoders can be divided into two categories.</p>
<p>The first category includes generally pre-trained sentence encoders such as the BERT, RoBERTa, and XLNet models. These models have been trained on large quantities of unsupervised tasks and achieved good performance on many natural language tasks. The corresponding embeddings contain rich semantic information from the original sentences.</p>
<p>The second category includes sentence encoders fine-tuned on specific tasks or datasets. For example, a sentence encoder trained on the STS dataset should be able to assess similarities among different questions better than a generally pre-trained sentence encoder. Sentence-BERT (Wolf et al., 2019; Reimers and Gurevych, 2019, 2020) shows that these fine-tuned encoders have achieved great performance on tasks such as sentence clustering, paraphrase mining, and information retrieval.</p>
<h2>3 Experimental Setup</h2>
<p>We apply our proposed method to the following three tasks: sentiment analysis, table-to-text generation, and question answering. Dataset split setups and prompt templates are shown in Table 9 and 11 in the Appendix. For the hyper-parameters in the GPT-3 API, we set the temperature to 0 .</p>
<h3>3.1 Sentence Embeddings for Retrieval</h3>
<p>To retrieve semantically-similar training instances, we consider two types of sentence embeddings.</p>
<ul>
<li>The original RoBERTa-large model (Liu et al., 2019), which is abbreviated as $\mathrm{KATE}_{\text {roberta }}$;</li>
<li>The RoBERTa-large models which are: $i$ ) finetuned on the SNLI and MultiNLI datasets ( $\mathrm{KATE}<em _nli_sts-b="{nli+sts-b" _text="\text">{\text {nli }}$ ) (Bowman et al., 2015; Williams et al., 2017); ii) first fine-tuned on the SNLI and MultiNLI dataset and then on the STS-B datasets ( $\mathrm{KATE}</em>$ ) (Cer et al., 2017).}</li>
</ul>
<p>All sentence encoders share the same architecture. The only differences are the specific datasets used for fine-tuning. The negative Euclidean distance is used for $\mathrm{KATE}<em _nli="{nli" _text="\text">{\text {roberta }}$, while the cosine similarity is employed for $\mathrm{KATE}</em>$.}}$ and $\mathrm{KATE}_{\text {nli+sts-b }</p>
<p>Sentiment Analysis For this task, we conduct experiments under the dataset-transfer setting. Incontext examples are selected from one dataset, and the evaluation is made on another dataset. This setting is designed to simulate a real-world scenario where we want to leverage an existing labeled dataset for a unlabeled one (of a similar task).</p>
<p>Specifically, we select examples from the SST2 training set (Socher et al., 2013; Wang et al., 2018) and ask GPT-3 to predict on the IMDB test set (Maas et al., 2011). To explore whether a sentence encoder fine-tuned on a similar task would benefit KATE, we also employ a pre-trained RoBERTa-large model fine-tuned on the SST-2 training set (dubbed as $\mathrm{KATE}_{\text {sst-2 }}$ ). The number of examples is chosen to be 3 since adding more examples does not further improve the performance.</p>
<p>Table-to-Text Generation Given a Wikipedia table and a set of highlighted cells, this task focuses on producing human-readable texts as descriptions. ToTTo (Parikh et al., 2020) ${ }^{2}$ is utilized for evaluation due to its popularity. We use BLEU (Papineni</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>et al., 2002) and PARENT (Dhingra et al., 2019) metrics for evaluation. Because the token length limit of GPT-3 is 2048, we add a preprocessing step by deleting the closing angle brackets such as $&lt;/$ cell $&gt;$ and $&lt;/$ table $&gt;$ to save space. The number of in-context examples is set as 2 so that the input length is within the token limit.</p>
<p>Question Answering We conduct experiments on three QA benchmarks: Natural Questions (NQ) (Kwiatkowski et al., 2019), Web Questions (WQ) (Berant et al., 2013), and TriviaQA (Joshi et al., 2017). For evaluation, we use the Exact Match (EM) score, which is defined as the proportion of the number of predicted answers being exactly one of the ground-truth answers. The matching is performed after string normalization, which includes article and punctuation removal. The number of examples is set to be 64 for NQ and WQ and 10 for TriviaQA (The retrieved 64 examples exceed the token limit). We evaluate on the test sets of NQ and WQ and the dev set of TriviaQA.</p>
<h3>3.2 Baseline Methods</h3>
<p>Random Sampling For each test sentence, we randomly select in-context examples from the training set. We refer to this method as Random in the experimental results. On the test set, the random baseline is repeated for five times to obtain the average score and corresponding standard deviation.
$k$-Nearest Neighbor Additionally, to investigate whether the retrieval module is complementary to GPT-3's in-context learning ability, we further consider a $k$-nearest neighbor baseline. Specifically, the target $y_{1}$ associated with the first retrieved example is considered as the predicted target for the test sample. For the sentiment analysis and QA tasks, the top $k$ retrieved examples $\left{y_{1}, \ldots, y_{k}\right}$ are utilized, where the final prediction is determined by majority voting among the $k$ examples' targets. If there is a tie case, we use the target of the example most similar to the test sentence. To ensure fair comparison, we compare the baseline $k \mathrm{NN}$ and KATE under the same embedding space of a pre-trained RoBERTa-large model. This baseline is abbreviated as $k \mathrm{NN}_{\text {roberta }}$.
Fine-tuned T5 Although this work aims at improving the in-context learning abilities of GPT-3, we include a fine-tuned T5 (3B) model as a baseline. This comparison informs us where GPT-3 performs comparably or surpasses a fine-tuned model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">T5 (fine-tuned)</td>
<td style="text-align: center;">95.2</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">$87.95 \pm 2.74$</td>
</tr>
<tr>
<td style="text-align: center;">$k \mathrm{NN}_{\text {roberta }}$</td>
<td style="text-align: center;">50.20</td>
</tr>
<tr>
<td style="text-align: center;">KATE roberta</td>
<td style="text-align: center;">91.99</td>
</tr>
<tr>
<td style="text-align: center;">KATE nli</td>
<td style="text-align: center;">90.40</td>
</tr>
<tr>
<td style="text-align: center;">KATE nli+sts-b</td>
<td style="text-align: center;">90.20</td>
</tr>
<tr>
<td style="text-align: center;">KATE ${ }_{\text {sst-2 }}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 4 3}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results on the IMDB dataset. In-context examples are from the SST-2 dataset.</p>
<h2>4 Experimental Results</h2>
<h3>4.1 Sentiment Analysis</h3>
<p>We first evaluate KATE on the sentiment analysis task. The results are in Table 3. KATE consistently produces better performance relative to the random selection baseline. Notably, there is no variance with the obtained results since the fixed retrieved in-context examples are employed. For KATE, when the pre-trained sentence encoder is fine-tuned on NLI or NLI+STS-B datasets, the performance slightly decreases. Since the objectives of the IMDB and the NLI+STS-B datasets are different, this shows that fine-tuning on a dissimilar task hurts KATE's performance. In contrast, KATE $<em _roberta="{roberta" _text="\text">{\text {sst-2 }}$ obtains the best accuracy, showing that fine-tuning on a similar task improves KATE's performance. To verify that the gains are not merely from the retrieval step, we further compare $\mathrm{KATE}</em>}}$ with the $k \mathrm{NN<em _roberta="{roberta" _text="\text">{\text {roberta }}$. It turns out that the performance of $k \mathrm{NN}</em>}}$ is close to random guessing. This observation is consistent when one neighbor or three neighbors are retrieved. Notably, with the sentence encoder fine-tuned on the SST-2 dataset, the accuracy of $k \mathrm{NN<em _sst-2="{sst-2" _text="\text">{\text {sst-2 }}$ is 92.46 , which is lower than that of $\mathrm{KATE}</em>$. These results suggest that GPT-3 is critical to the final results, and the retrieval module is complementary to GPT-3.}</p>
<p>The fine-tuned T5 model works better since its parameters has been adapted to this specific task. However, fine-tuning requires access to model parameters, lots of memory storage, and time. The fine-tuning result here is just for reference. Through KATE, the performance of GPT-3 has increased significantly without fine-tuning.</p>
<h3>4.2 Table-to-text Generation</h3>
<p>We next evaluate KATE on the ToTTo dataset and present results in Table 4. KATE gives rise to considerable gains over the random baseline, according to both the BLEU and PARENT scores. Notably,</p>
<p>KATE enables GPT-3 to achieve performance comparable to a fine-tuned T5 model. On a finer scale, the evaluation can be done on the overlap subset and the nonoverlap subset. The overlap dev subset shares a significant number of header names with the training set, while the nonoverlap one does not. KATE improves results on both subsets, meaning that the retrieval module is helpful even when the dev set is out of distribution of the training set. Similar to sentiment analysis, there is a slight drop in performance from $\mathrm{KATE}<em _nli="{nli" _text="\text">{\text {roberta }}$ to $\mathrm{KATE}</em>}}$ and $\mathrm{KATE<em _nli="{nli" _text="\text">{\text {nli+sts-b }}$. This is due to the difference between the objectives of the ToTTo dataset and NLI+STSB datasets. The drop from $\mathrm{KATE}</em>$ baseline, it performs much worse than the random selection method and KATE, suggesting that the retrieval process and GPT-3 work collaboratively to achieve better results.}}$ to $\mathrm{KATE}_{\text {nli+sts-b }}$ further validates the idea that fine-tuning on a dissimilar task can hurt KATE's performance. For the $k \mathrm{NN</p>
<p>To understand how the retrieval mechanism helps GPT-3, we conduct a case study on the retrieved examples (see Table 5). By retrieving relevant examples from the training set, KATE provides useful detailed information within the table, e.g., the number of points, rebounds, and assists, to GPT-3 for more accurate description. On the other hand, the random selection method has the issue of hallucination, where the generated sequences contain information (i.e., "senior year" and "University of Texas") not present in the table.</p>
<h3>4.3 Questing Answering</h3>
<p>Lastly, we evaluate KATE on the open-domain QA tasks, as shown in Table 6. We compare with some state-of-the-art fine-tuned methods such as RAG (Lewis et al., 2020) and T5 (Raffel et al., 2019). The T5 results were reported in (Brown et al., 2020) using the 11B model, which needs specialized TPUs to do fine-tuning. KATE again improves GPT-3's performance substantially across various benchmarks. Moreover, KATE helps GPT3 to even outperform the fine-tuned T5 model. It is worth noting that this time both $\mathrm{KATE}<em _nli_sts-b="{nli+sts-b" _text="\text">{\text {nli }}$ and $\mathrm{KATE}</em>}}$ improve upon $\mathrm{KATE<em _roberta="{roberta" _text="\text">{\text {roberta }}$ because fine-tuning on NLI or STS-B datasets is helpful for retrieving semantically similar questions from the QA datasets. Moreover, on the NQ and TriviaQA datasets, further fine-tuning on the STS-B dataset improves KATE's results. We evaluate the baseline $k \mathrm{NN}</em>$ baseline results again suggest that}}$ by using the top-1 nearest neighbor. The $k \mathrm{NN</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overlap Subset</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Nonoverlap Subset</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">PARENT</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">PARENT</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">PARENT</td>
</tr>
<tr>
<td style="text-align: center;">T5 (fine-tuned)</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">$28.4 \pm 2.1$</td>
<td style="text-align: center;">$39.3 \pm 2.6$</td>
<td style="text-align: center;">$31.2 \pm 2.5$</td>
<td style="text-align: center;">$41.8 \pm 3.0$</td>
<td style="text-align: center;">$25.6 \pm 1.8$</td>
<td style="text-align: center;">$37.0 \pm 2.3$</td>
</tr>
<tr>
<td style="text-align: center;">$k \mathrm{NN}_{\text {roberta }}$</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">7.52</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{KATE}_{\text {roberta }}$</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">45.5</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{KATE}_{\text {nli }}$</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">44.5</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{KATE}_{\text {nli+sts-b }}$</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">43.4</td>
</tr>
</tbody>
</table>
<p>Table 4: Table-to-text generation results on the ToTTo dev dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Test Table</th>
<th style="text-align: left;">Table: <page_title $>$ Trey Johnson <section_title $>$ College <table $>&lt;$ cell $&gt;32&lt;$ col_header $&gt;$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">GP <cell $>4.8$ <col_header $>$ RPG $&lt;$ cell $&gt;2.3$ <col_header $>$ APG $&lt;$ cell $&gt;23.5$ <col_header $>$ PPG</td>
</tr>
<tr>
<td style="text-align: left;">Retrieved</td>
<td style="text-align: left;">Table: <page_title $>$ Dedric Lawson <section_title $>$ College <table $>&lt;$ cell $&gt;9.9$ <col_header $>$</td>
</tr>
<tr>
<td style="text-align: left;">Examples</td>
<td style="text-align: left;">RPG $&lt;$ cell $&gt;3.3$ <col_header $>$ APG $&lt;$ cell $&gt;19.2$ <col_header $>$ PPG</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Sentence: Dedric Lawson averaged 19.2 points, 9.9 rebounds and 3.3 assists per game.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Table: <page_title $>$ Carsen Edwards <section_title $>$ College <table $>&lt;$ cell $&gt;3.8$ <col_header $>$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">RPG $&lt;$ cell $&gt;2.8$ <col_header $>$ APG $&lt;$ cell $&gt;18.5$ <col_header $>$ PPG</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Sentence: Edwards averaged 18.5 points, 3.8 rebounds and 2.8 assists per game.</td>
</tr>
<tr>
<td style="text-align: left;">Predictions</td>
<td style="text-align: left;">Ground-truth: Trey Johnson averaged 23.5 points, 4.8 rebounds, and 2.3 assists in 32 games.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Random: Trey Johnson averaged 23.5 points per game in his senior year at the University of Texas.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">KATE: Johnson averaged 23.5 points, 4.8 rebounds and 2.3 assists per game.</td>
</tr>
</tbody>
</table>
<p>Table 5: A sample of retrieved in-context examples from the ToTTo dataset. For the KATE method, GPT-3 pays more attention to detailed information such as the number of points, rebounds, and assists. In contrast, the random selection method leads GPT-3 to generate details which do not exist in the original table.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">WQ</th>
<th style="text-align: center;">TriviaQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">RAG (Open-Domain)</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">68.0</td>
</tr>
<tr>
<td style="text-align: center;">T5+SSM (Closed-Book)</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">60.5</td>
</tr>
<tr>
<td style="text-align: center;">T5 (Closed-Book)</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">50.1</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 (64 examples)</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">$28.6 \pm 0.3$</td>
<td style="text-align: center;">$41.0 \pm 0.5$</td>
<td style="text-align: center;">$59.2 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">$k \mathrm{NN}_{\text {roberta }}$</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">26.2</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{KATE}_{\text {roberta }}$</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">57.5</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{KATE}_{\text {nli }}$</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">60.9</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{KATE}_{\text {nli+sts-b }}$</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">62.4</td>
</tr>
</tbody>
</table>
<p>Table 6: Results on QA datasets. (*) We used 10 examples for TriviaQA and 64 examples for NQ and WQ.
the retrieval module and GPT-3 work together to achieve better performance. We also explore using 64 nearest neighbors ( 10 for TriviaQA) to determine the answer (by majority voting explained in Section 3.2). The EM score are similar to retrieving the top-1 nearest neighbor.</p>
<p>To investigate why the retrieved examples are helpful, we present a case study. Concretely, the retrieval examples from the NQ dataset are shown in Table 7. For the first and second cases, the random baseline provides wrong answers because GPT-3 is unable to recall the exact detail. However, the in-context examples selected by KATE contain the correct details, which facilitate GPT-3 to answer questions. For the third case, the random baseline
leads GPT-3 to misinterpret the question as asking for a specific location. In contrast, KATE selects similar types of questions asking for the origins of objects. Using these in-context examples, GPT-3 is able to interpret and answer the question correctly.</p>
<h2>5 Analysis of Different Factors</h2>
<h3>5.1 Number of In-context Examples</h3>
<p>We first investigate the impact of the number of examples on KATE's performance. Concretely, on the NQ dataset, we choose the number of examples to be $5,10,20,35$, and 64 , and $\mathrm{KATE}<em _roberta="{roberta" _text="\text">{\text {nli+sts-b }}$ is compared with the random baseline and $\mathrm{KATE}</em>$ across different settings. As shown in the left plot of Figure 2, both KATE and the random baseline benefit from utilizing more examples. However, KATE consistently outperforms the random selection method, even when the number of in-context examples is as few as 5 . This result is interesting because in practice, employing less examples leads to more efficient inference with GPT-3.}</p>
<h3>5.2 Size of Training Set for Retrieval</h3>
<p>We further examine how the size of the training set may influence the KATE method. On the NQ dataset, we create new subsets from the original training set, with sizes of $1 \mathrm{k}, 2 \mathrm{k}, 5 \mathrm{k}, 10 \mathrm{k}, 30 \mathrm{k}$, and</p>
<table>
<thead>
<tr>
<th style="text-align: center;">In-Context Examples</th>
<th style="text-align: center;">Predictions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Question: The Mughal Gardens of Rashtrapati Bhavan is modelled on which garden?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">The Mughal Garden of Rashtrapati Bhavan is modelled on? The Persian style of architecture</td>
<td style="text-align: center;">Ground-truth: Persian garden</td>
</tr>
<tr>
<td style="text-align: center;">Who built the first Mughal Garden in India? Babur</td>
<td style="text-align: center;">KATE: The Persian gardens</td>
</tr>
<tr>
<td style="text-align: center;">The landscape design of the Gardens of Versailles is known as which style? French garden</td>
<td style="text-align: center;">Random Baseline: Shalimar gardens</td>
</tr>
<tr>
<td style="text-align: center;">Question: What city was Zeus the patron god of?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">What is the symbol of Zeus the Greek God? Bull</td>
<td style="text-align: center;">Ground-truth: Olympia</td>
</tr>
<tr>
<td style="text-align: center;">Where did Zeus spend most of his time? Mount Olympus</td>
<td style="text-align: center;">KATE: Olympia</td>
</tr>
<tr>
<td style="text-align: center;">Where was the statue of Zeus at Olympia located? In the Temple of Zeus</td>
<td style="text-align: center;">Random Baseline Athens</td>
</tr>
<tr>
<td style="text-align: center;">Question: Where did the Dewey decimal system come from?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Where did the formula for area of a circle come from? Archimedes</td>
<td style="text-align: center;">Ground-truth: Melvil Dewey</td>
</tr>
<tr>
<td style="text-align: center;">Where did the name jack russell come from? Reverend John Russell</td>
<td style="text-align: center;">KATE: Melvil Dewey</td>
</tr>
<tr>
<td style="text-align: center;">Where did the letters of the alphabet come from? The Phoenician alphabet</td>
<td style="text-align: center;">Random Baseline: the library of Congress</td>
</tr>
</tbody>
</table>
<p>Table 7: Three samples of retrieved in-context examples from the NQ dataset. Three retrieved Q-A pairs are shown on the left. Predictions by the KATE method and useful details from in-context examples are shown in Green. Gold-standard references are shown in Blue. Predictions by the random baseline are shown in Red.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Left: Effect of number of in-context examples for different selection methods. Right: Effect of the size of training set for retrieval on KATE. Two representative sentence encoders are used in these studies.</p>
<p>70k, respectively. In-context examples are retrieved from these subsets instead of the original training set. The number of nearest neighbors is set to 64 . We compare $\mathrm{KATE}<em _roberta="{roberta" _text="\text">{\text {nli+sts-b }}$ with the random selection method and $\mathrm{KATE}</em>}}$, and the results are shown in the right plot of Figure 2. For $\mathrm{KATE<em _nli_sts-b="{nli+sts-b" _text="\text">{\text {roberta }}$ and $\mathrm{KATE}</em>$, as the size of the training set increases, the EM scores also increase. In contrast, the result of the random sampling baseline does not change much. Intuitively, as the training size gets larger, it is more likely for KATE to retrieve relevant in-context examples to help GPT-3 answer a question correctly. As we have shown previously in Table 7, the retrieved in-context examples could provide critical detailed information to GPT-3, thus helping GPT-3 to better answer the questions.}</p>
<h3>5.3 Order of In-context Examples</h3>
<p>Moreover, we explore how the order of in-context examples may affect KATE's results. As mentioned in Section 2.3, under the standard setting, the retrieved in-context examples are ordered such that $s\left(x_{i}, x\right) \geq s\left(x_{j}, x\right)$ whenever $i&lt;j$. Here, we ran-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Trial</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">Default</th>
<th style="text-align: center;">Reverse</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">EM Score</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">42.8</td>
</tr>
</tbody>
</table>
<p>Table 8: Analysis on the effect of orders of in-context example on the NQ dataset using $\mathrm{KATE}<em _nli_sts-b="{nli+sts-b" _text="\text">{\text {nli+sts-b }}$. The default order puts the most similar example in the front, and the reverse order does the opposite.
domly permute the order of in-context examples in the NQ dataset for the proposed $\mathrm{KATE}</em>, x\right)$ whenever $i&lt;j$. The results are presented in Table 8. On this particular NQ dataset, the reverse order performs the best. However, we also did the experiments on the WQ and TriviaQA and find that the default order performs slightly better than the reverse order. Hence,}}$ method, and conduct the experiments for 3 different orders. Additionally, we explore the reverse order where $s\left(x_{i}, x\right) \leq s\left(x_{j</p>
<p>the choice of orders is data-dependent. Additionally, it can be observed that the variation among the NQ results tends to be quite small (compared with the difference between the random baseline and KATE), indicating that the example order does not have a significant impact on KATE's performance.</p>
<h2>6 Related Work</h2>
<p>Pre-trained Language Models NLP systems have made tremendous progress by pre-training models on unlabeled text (Devlin et al., 2018; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2019; Raffel et al., 2019; Xue et al., 2020; Lample and Conneau, 2019; Radford et al., 2018, 2019). These models can be fine-tuned for a wide range of downstream tasks. GPT-3 (Brown et al., 2020), however, can perform in-context learning without fine-tuning. People have just started trying to understand GPT3 from different perspectives. (Hendrycks et al., 2020) studies which categories of questions GPT-3 is more capable of answering. (Zhao et al., 2021) proposes to improve the model by contextual calibration. However, their method is limited to predicting very few tokens because for long sequence generation, the contextual calibration step needs to be repeatedly performed after each newly generated token. In contrast, our work, KATE, only calls the API once and is suitable for both text classification and generation tasks. Another related work is LM-BFF (Gao et al., 2020), which uses a smaller language model (RoBERTa-large) to demonstrate that prompt-based fine-tuning can outperform standard fine-tuning on text classification tasks. Our work differs by showing that, without fine-tuning, relevant examples can still substantially improve the performance of GPT-3 for both text classification and generation tasks. Finally, AutoPrompt (Shin et al., 2020) explores adding some additional tokens to smaller language models to improve performance on classification tasks.</p>
<p>Retrieval-based Text Generation There is a long history of applying information retrieval to text generation (Sumita and Hitoshi, 1991). It is very related to the exemplar-based learning (Jäkel et al., 2008; Ziyadi et al., 2020). Some representative applications in the field of deep learning include machine translation (Gu et al., 2018), sentiment transfer (Li et al., 2018; Guu et al., 2018), QA (Karpukhin et al., 2020; Mao et al., 2020), dialogue generation (Yan et al., 2016; Cai et al., 2018; Song et al., 2016; Pandey et al., 2018; We-
ston et al., 2018; Wu et al., 2019), text summarization (Cao et al., 2017; Peng et al., 2019), data-to-text generation (Peng et al., 2019), and text-tocode generation (Hashimoto et al., 2018). All these retrieve-and-edit frameworks require their editors to be trained or fine-tuned on specific tasks. In contrast, our work uniquely examines how to better use GPT-3 as a universal editor without fine-tuning. We find that the more semantically similar context we provide to GPT-3, the better results the model can generate.</p>
<p>Improve NLP Systems with $k$ NN Some recent works try to incorporate non-parametric methods to improve a given model's performance. For example, the newly introduced $k$ NN-LM (Khandelwal et al., 2019), $k$ NN-MT (Khandelwal et al., 2020), and BERT- $k$ NN (Kassner and Schütze, 2020) generate the next token by retrieving the nearest $k$ neighbors from the datastore. Another related work $k$ NN classification model (Rajani et al., 2020) uses $k \mathrm{NN}$ as backoff when the confidence is low from the classification model. There are two key differences between our work and other approaches. First, we retrieve the nearest $k$ neighbors to modify the conditional context instead of the prediction. Second, we do not have access to the parameters of GPT-3. Instead, we rely on some independently pre-trained models to get the sentence embeddings to retrieve the nearest $k$ neighbors.</p>
<h2>7 Conclusion</h2>
<p>This work presented a first step towards investigating the sensitivity of GPT-3 to in-context examples. To this end, we proposed KATE, a non-parametric selection approach that retrieves in-context examples according to their semantic similarity to the test samples. On several natural language understanding and generation tasks, the proposed method improves GPT-3's performance, over the random sampling baseline, by a significant margin. Particularly, KATE enables GPT-3 to achieve performance comparable to a fine-tuned T5 model on the table-to-text generation task and outperforms T5 on the QA task. Moreover, we found that fine-tuning the sentence embeddings for retrieval on task-related datasets gave rise to further empirical gains. Detailed analysis was conducted to explore the robustness of KATE to different hyperprameters, such as the number of in-context examples, examples' order, etc. One limitation we notice is that despite the improved performance on sentiment analysis,</p>
<p>GPT-3 still lags behind the fine-tuned T5 model by a small margin. This suggests that our proposed method is more suitable and effective on long text generation tasks. We hope this work could provide insights for better understanding the behaviors of GPT-3 and represents a helpful step towards further improving its in-context learning capabilities.</p>
<h2>8 Ethical and Broader Impacts</h2>
<p>Risk Our proposed KATE method significantly improves the in-context learning ability of GPT-3 and makes long-text generation more easily without fine-tuning the pre-trained model. However, one risk implication is that our proposed method will benefit the research groups which are financially capable of using such huge models. For individual or small-group researchers, they cannot apply our proposed method to their specific applications since they don't have access to the model. Our work has suggested researchers should focus more on investigating the in-context learning of pretrained models. One potential future direction is for researchers to scale-down the sizes of pre-trained models to find a balance between model performance and model size. Once a smaller model is obtained with comparable performance (enhanced by KATE), our proposed method can become more widely accessible to individual researchers.</p>
<p>Potential Bias During the experiment on table-to-text generation, we have pointed out that large pre-trained language models could be susceptible to hallucination (case study in Table 5). This problem is more pronounced when we use randomly sampled examples. This happens because the language model is biased toward the training dataset. As shown in Table 5, when random examples are used, the sentence generated by GPT-3 is grammatically correct, but some details never exist in the given table. In contrast, our proposed method, KATE, can significantly alleviate this problem by guiding GPT-3 to look for and generate the correct information. For similar reasons, large pretrained models could be potentially susceptible to gender and racial bias. Since our KATE method shows that in-context examples are crucial for highquality long-text generations, one way to alleviate the racial and gender bias is to incorporate an additional module to filter out offensive in-context examples. Since racial and gender bias are not our main research focus, a full investigation goes beyond the scope of our work. However, we believe
this is an exciting opportunity for future work.</p>
<h2>Code Availability</h2>
<p>Implementations of the proposed KATE method discussed in this paper are available at https: //github.com/jiachangliu/KATEGPT3.</p>
<h2>References</h2>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533-1544.</p>
<p>Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</p>
<p>Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam, and Shuming Shi. 2018. Skeleton-to-response: Dialogue generation guided by retrieval memory. arXiv preprint arXiv:1809.05296.</p>
<p>Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2017. Faithful to the original: Fact aware neural abstractive summarization. arXiv preprint arXiv:1711.04434.</p>
<p>Daniel Cer, Mona Diab, Eneko Agirre, Inigo LopezGazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, MingWei Chang, Dipanjan Das, and William W Cohen. 2019. Handling divergent reference texts when evaluating table-to-text generation. arXiv preprint arXiv:1906.01081.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.</p>
<p>Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor OK Li. 2018. Search engine guided neural machine translation. In AAAI, pages 5133-5140.</p>
<p>Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. 2018. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437-450.</p>
<p>Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. 2018. A retrieve-and-edit framework for predicting structured outputs. In Advances in Neural Information Processing Systems, pages $10052-10062$.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.</p>
<p>Frank Jäkel, Bernhard Schölkopf, and Felix A Wichmann. 2008. Generalization and similarity in exemplar models of categorization: Insights from machine learning. Psychonomic Bulletin \&amp; Review, 15(2):256271.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.</p>
<p>Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906.</p>
<p>Nora Kassner and Hinrich Schütze. 2020. Bertknn: Adding a knn search component to pretrained language models for better qa. arXiv preprint arXiv:2005.00766.</p>
<p>Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Nearest neighbor machine translation. arXiv preprint arXiv:2010.00710.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466.</p>
<p>Guillaume Lample and Alexis Conneau. 2019. Crosslingual language model pretraining. arXiv preprint arXiv:1901.07291.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401.</p>
<p>Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: A simple approach to sentiment and style transfer. arXiv preprint arXiv:1804.06437.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142-150.</p>
<p>Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2020. Generation-augmented retrieval for open-domain question answering. arXiv preprint arXiv:2009.08553.</p>
<p>Gaurav Pandey, Danish Contractor, Vineet Kumar, and Sachindra Joshi. 2018. Exemplar encoder-decoder for neural conversation generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1329-1338.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318.</p>
<p>Ankur P Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: A controlled table-totext generation dataset. In Proceedings of EMNLP.</p>
<p>Hao Peng, Ankur P Parikh, Manaal Faruqui, Bhuwan Dhingra, and Dipanjan Das. 2019. Text generation with exemplar-based adaptive decoding. arXiv preprint arXiv:1904.04428.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.</p>
<p>Nazneen Fatema Rajani, Ben Krause, Wengpeng Yin, Tong Niu, Richard Socher, and Caiming Xiong. 2020. Explaining and improving model behavior with k nearest neighbor representations. arXiv preprint arXiv:2010.09030.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.</p>
<p>Nils Reimers and Iryna Gurevych. 2020. Making monolingual sentence embeddings multilingual using knowledge distillation. arXiv preprint arXiv:2004.09813.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages $1631-1642$.</p>
<p>Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and Ming Zhang. 2016. Two are better than one: An ensemble of retrieval-and generation-based dialog systems. arXiv preprint arXiv:1610.07149.</p>
<p>Eiichiro Sumita and HDA Hitoshi. 1991. Experiments and prospects of example-based machine translation. In 29th Annual Meeting of the Association for Computational Linguistics, pages 185-192.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30:5998-6008.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.</p>
<p>Jason Weston, Emily Dinan, and Alexander H Miller. 2018. Retrieve and refine: Improved sequence generation models for dialogue. arXiv preprint arXiv:1808.04776.</p>
<p>Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.</p>
<p>Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhoujun Li, and Ming Zhou. 2019. Response generation by context-aware prototype editing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7281-7288.</p>
<p>Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934.</p>
<p>Rui Yan, Yiping Song, and Hua Wu. 2016. Learning to respond with deep neural networks for retrieval-based human-computer conversation system. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, pages 55-64.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems, pages 5753-5763.</p>
<p>Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. arXiv preprint arXiv:2102.09690.</p>
<p>Morteza Ziyadi, Yuting Sun, Abhishek Goswami, Jade Huang, and Weizhu Chen. 2020. Examplebased named entity recognition. arXiv preprint arXiv:2008.10570.</p>
<h2>A An Example of In-context Learning</h2>
<p>As shown in the illustration of Figure 3, GPT-3 is asked to translate "mountain" to its German version based on the three examples given as part of the input.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The figure above shows how to perform in-context learning with a language model. Three incontext examples and the test prompt are concatenated as a single string input for GPT-3, with a special character "in" inserted between two adjacent examples. GPT-3 keeps generating tokens until there is a special character "in".</p>
<h2>B Data Split</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">67 k</td>
<td style="text-align: center;">872</td>
<td style="text-align: center;">1.8 k</td>
</tr>
<tr>
<td style="text-align: center;">IMDB</td>
<td style="text-align: center;">25 k</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">25 k</td>
</tr>
<tr>
<td style="text-align: center;">ToTTo</td>
<td style="text-align: center;">120 k</td>
<td style="text-align: center;">7.7 k</td>
<td style="text-align: center;">7.7 k</td>
</tr>
<tr>
<td style="text-align: center;">NQ</td>
<td style="text-align: center;">79 k</td>
<td style="text-align: center;">8.8 k</td>
<td style="text-align: center;">3.6 k</td>
</tr>
<tr>
<td style="text-align: center;">WQ</td>
<td style="text-align: center;">3.4 k</td>
<td style="text-align: center;">361</td>
<td style="text-align: center;">2 k</td>
</tr>
<tr>
<td style="text-align: center;">TriviaQA</td>
<td style="text-align: center;">78.8 k</td>
<td style="text-align: center;">8.8 k</td>
<td style="text-align: center;">11.3 k</td>
</tr>
</tbody>
</table>
<p>Table 9: Data split for different datasets. In-context examples are selected from the training set. Because ToTTo and TriviaQA require submitting to their leaderboards, the evaluation is done on the dev sets. For all other datasets, the evaluation is done on the test sets.</p>
<h2>C Complete ToTTo Case Study</h2>
<p>Due to the length limit of the main paper, we present in the appendix the full ToTTo case study comparing the random sampling baseline and our proposed KATE method. We present the case study in Table 10.</p>
<p>As we have discussed in the main paper, the in-context examples retrieved by KATE facilitates GPT-3 to effectively extract key information from the given table. Detailed numbers such as the number of points, rebounds, and assists have all been included in the sentence.</p>
<p>In contrast, the sentence generated by GPT-3 using randomly sampled in-context examples only
extract partial information from the table. Only the number of points is included while the numbers of rebounds and assists are ignored. Moreover, the random sampling baseline could lead to the issue of hallucination. Both "senior year" and "University of Texas" are not present in the given table. One may wonder whether these wrong phrases were present in the randomly sampled in-context examples, which might have caused this issue. However, if we look at the randomly sampled in-context examples in the second block of the table, such information do not exist. This suggests such hallucinated phrases are generated by the language model itself.</p>
<p>This comparison provides some key insights on why KATE works better than the random sampling baseline. By retrieving semantically/syntactically similar in-context examples, KATE provides GPT3 with a much more accurate template/structure to do text generation. Without such structure, GPT-3 can generate sentences that are fluent but do not meet the goal of a particular task.</p>
<h2>D On Prompt Engineering vs. Fine-tuning</h2>
<p>As we mentioned in the main paper, given a training dataset, we could take the full advantage of the GPT-3's model strength through fine-tuning. However, there are several advantages of prompt engineering over fine-tuning. First, fine-tuning requires access to the model parameters and gradients. It is impossible to access this information via the current GPT-3's API. Second, fine-tuning large models are time-consuming and costly. Ordinary research labs and individual developers do not have resources to accomplish such tasks. Third, storing large fine-tuned model checkpoints requires large storage space. Even if GPT-3 is fine-tuned and stored for many specific tasks/datasets, many finetuned checkpoints may not be frequently called. This is not energy efficient. Our proposed KATE method does not require costly fine-tuning and improves the random baseline on both text classification and generation tasks, sometimes by a significant margin. This makes it more practical to deploy the same GPT-3 model across all tasks.</p>
<h2>E T5 Baseline</h2>
<p>Although our primary goal is to improve GPT-3's in-context learning ability, we also include the finetuned T5 results as a reference (3B T5 on SST-2 and</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Test Table</th>
<th style="text-align: center;">Table: <page_title $>$ Trey Johnson <section_title $>$ College <table $>&lt;$ cell $&gt;32&lt;$ col_header $&gt;$ <br> GP <cell $>4.8&lt;$ col_header $&gt;$ RPG $&lt;$ cell $&gt;2.3&lt;$ col_header $&gt;$ APG $&lt;$ cell $&gt;23.5&lt;$ col_header $&gt;$ PPG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Randomly Sampled Examples</td>
<td style="text-align: center;">Table: <page_title $>$ List of RAGBRAI overnight stops <section_title $>$ By year <table $>&lt;$ cell $&gt;$ 1986 <col_header $>&lt;$ col_header $&gt;$ Year <cell $>$ Audubon (1) <col_header $>$ Route - start to finish (number indicates occurrence) <col_header $>$ Monday <cell $>2006$ <col_header $>&lt;$ col_header $&gt;$ Year <cell $>$ Audubon (2) <col_header $>$ Route - start to finish (number indicates occurrence) <br> <col_header $>$ Monday <br> Sentence: Audubon has been an RAGBRAI overnight stop in 1986 and 2006. <br> Table: <page_title $>$ List of Administrators of British Brunei <section_title $>$ British Brunei <br> administrators <table $>&lt;$ cell $&gt;$ Malcolm Stewart Hannibal McArthur <col_header $>$ Consul <br> Generals to Brunei <col_header $>$ British Consuls in Brunei <col_header $>$ British Residents in Brunei <br> Sentence: Malcolm Stewart Hannibal McArthur was the first British resident in Brunei.</td>
</tr>
<tr>
<td style="text-align: center;">KATE- <br> Retrieved <br> Examples</td>
<td style="text-align: center;">Table: <page_title $>$ Dedric Lawson <section_title $>$ College <table $>&lt;$ cell $&gt;9.9&lt;$ col_header $&gt;$ <br> RPG $&lt;$ cell $&gt;3.3&lt;$ col_header $&gt;$ APG $&lt;$ cell $&gt;19.2&lt;$ col_header $&gt;$ PPG <br> Sentence: Dedric Lawson averaged 19.2 points, 9.9 rebounds and 3.3 assists per game. <br> Table: <page_title $>$ Carsen Edwards <section_title $>$ College <table $>&lt;$ cell $&gt;3.8&lt;$ col_header $&gt;$ <br> RPG $&lt;$ cell $&gt;2.8&lt;$ col_header $&gt;$ APG $&lt;$ cell $&gt;18.5&lt;$ col_header $&gt;$ PPG <br> Sentence: Edwards averaged 18.5 points, 3.8 rebounds and 2.8 assists per game.</td>
</tr>
<tr>
<td style="text-align: center;">Predictions</td>
<td style="text-align: center;">Ground-truth: Trey Johnson averaged 23.5 points, 4.8 rebounds, and 2.3 assists in 32 games. <br> Random: Trey Johnson averaged 23.5 points per game in his senior year at the University of Texas. <br> KATE: Johnson averaged 23.5 points, 4.8 rebounds and 2.3 assists per game.</td>
</tr>
</tbody>
</table>
<p>Table 10: A sample of retrieved in-context examples from the ToTTo dataset. For the KATE method, GPT-3 pays more attention to detailed information such as the number of points, rebounds, and assists. In contrast, the random selection method leads GPT-3 to generate details which do not exist in the original table. Information such as "senior year" and "University of Texas" also do not exist in the randomly sampled in-context examples. This suggests that the wrong information was generated by the language model itself. Although the sentence by the random sampling baseline is fluent, it does meet the goal of the table-to-text task.</p>
<p>ToTTo datasets, and 11B T5 on the QA datasets). The reason for reporting the 3B T5 results on the SST-2 and ToTTo datasets is that this is the largest T5 model we can use. For the 3B T5 model, Google Colab ${ }^{3}$ provides a free V2-8 TPU to fine-tune the 3B model. We used the Colab tutorial notebook to fine-tune the 3B T5 model on the SST-2 and ToTTo training sets. We couldn't fine-tune the 11B T5 model because the model size is too large. Finetuning such a large model requires a V3-8 TPU, which is not free of charge. Fortunately, the original GPT-3 paper (Brown et al., 2020) has already reported the finet-tuned 11B T5 results on the three QA datasets, so we reuse these results in our main paper for the QA task. Our proposed KATE method significantly improves GPT-3, performing comparably to the fine-tuned T5 model on the table-to-text task and outperforming the fine-tuned T5 model on the QA task.</p>
<h2>F Details on Retrieval Modules</h2>
<p>As we mention in the main paper, we use the pretrained RoBERTa-large model (Liu et al., 2019)</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>as the first retrieval module, which has 355M parameters and is pre-trained with the MLM (masked language modeling) objective. The result given by this module is denoted as $\mathrm{KATE}_{\text {roberta }}$. We directly download this model from the HuggingFace Model Zoo (MIT license) ${ }^{4}$. All other retrieval modules share the same architecture as the RoBERTa-large module but are fine-tuned on specific datasets.</p>
<p>For the fine-tuned retrieval modules, the first we use is the RoBERTa-large model fine-tuned on the SNLI and MultiNLI datasets ( $\mathrm{KATE}<em _mathrm_nli="\mathrm{nli">{\mathrm{nli}}$ ) (Bowman et al., 2015; Williams et al., 2017); the next we use is the RoBERTa-large model fine-tuned on the SNLI and MultiNLI dataset and then on the STS-B datasets $\left(\mathrm{KATE}</em>$.}+\mathrm{sts}-\mathrm{b}}\right)$ (Cer et al., 2017). These fine-tuned models have already been accomplished and included by the Sentence-BERT family and are publicly available, so we directly download from the Sentence-BERT Model Zoo ${ }^{5</p>
<p>Lastly, specifically for the sentiment analysis task, we include a RoBERTa-large model finetuned on the SST-2 dataset ( $\mathrm{KATE}_{\text {sst-2 }}$ ) (Socher et al., 2013; Wang et al., 2018). At the time of our</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>research, we didn't find a good publicly available fine-tuned model, so we fine-tune the pre-trained RoBERTa-large model on SST-2 by ourselves. The exact fine-tuning procedure, including the hyperparameters and learning rate, can be found at the HuggingFace website ${ }^{6}$. We fine-tune the RoBERTalarge model using a single V100 GPU.</p>
<h1>G Prompt Templates Used</h1>
<p>For reproducibility, we show the prompt templates used for all tasks in Tables 11 .</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Prompt Template</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SST-2 \&amp; IMDB</td>
<td style="text-align: left;">Sentence: comes from the brave, uninhibited performances. Label: Positive <br> Sentence: This tearful movie about a sister and her battle to save as many souls as she can is very <br> moving. The film does well in picking up the characters and showing how Sister Helen deals with <br> each. A wonderful journey from life to death. Label:</td>
</tr>
<tr>
<td style="text-align: left;">ToTTo</td>
<td style="text-align: left;">Table: <page_title>Dedric Lawson <section_title>College <table><cell>9.9 <col_header>RPG <br> <cell>3.3 <col_header>APG <cell>19.2 <col_header>PPG <br> Sentence: Dedric Lawson averaged 19.2 points, 9.9 rebounds and 3.3 assists per game. <br> Table: <page_title>Trey Johnson <section_title>College <table><cell>32 <col_header>GP <br> <cell>4.8 <col_header>RPG <cell>2.3 <col_header>APG <cell>23.5 <col_header>PPG <br> Sentence:</td>
</tr>
<tr>
<td style="text-align: left;">QA</td>
<td style="text-align: left;">Q: The landscape design of the Gardens of Versailles is known as which style? <br> A: The Persian style of architecture. <br> Q: The Mughal Gardens of Rashtrapati Bhavan is modelled on which garden? <br> A:</td>
</tr>
</tbody>
</table>
<p>Table 11: The prompt templates used for all tasks discussed in the paper. We show only one in-context example per task for illustration purposes.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ The fine-tuning script we use can be found at https://huggingface.co/transformers/ v2.7.0/examples.html#glue.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ The HuggingFace Model Zoo can be found at https: //huggingface.co/models.
${ }^{5}$ The Sentence-BERT Model Zoo can be found at https: //huggingface.co/sentence-transformers.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
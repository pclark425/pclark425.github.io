<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8634 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8634</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8634</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-272663079</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.10304v1.pdf" target="_blank">Spiers Memorial Lecture: How to do impactful research in artificial intelligence for chemistry and materials science</a></p>
                <p><strong>Paper Abstract:</strong> Machine learning has been pervasively touching many fields of science. Chemistry and materials science are no exception. While machine learning has been making a great impact, it is still not reaching its full potential or maturity. In this perspective, we first outline current applications across a diversity of problems in chemistry. Then, we discuss how machine learning researchers view and approach problems in the field. Finally, we provide our considerations for maximizing impact when researching machine learning for chemistry.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8634.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8634.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES-based chemistry language models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES-based chemistry language models (unsupervised/finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Language models trained on molecular string representations (e.g., SMILES/SELFIES) to learn distributions over molecules and enable conditional generation or property-conditioned sampling for molecular design and inverse design tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified SMILES language models</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>language model / transformer / autoregressive (chemistry language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Datasets of molecular strings such as SMILES, SELFIES, Group SELFIES and derived molecular string corpora (unspecified corpora in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery, materials discovery, general molecular inverse design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct generation of molecular strings (SMILES/SELFIES); conditional generation by finetuning or scoring; used as priors plugged into optimization/search frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Can generate molecules beyond enumerated libraries (potentially novel relative to training set); paper notes the capability to generate arbitrary molecules but also emphasizes issues of synthesizability and evaluation—no quantitative novelty metrics provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditioning / finetuning and goal-directed optimization are used to bias generation toward desired properties; evaluation often relies on downstream property predictors or approximate metrics rather than guaranteed real-world performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Common metrics cited include validity, novelty, uniqueness, goal-directed optimization scores; chemistry-specific benchmarks referenced include GuacaMol, MOSES, and more realistic sets such as Tartarus.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper reports that SMILES-based chemistry language models have been widely explored in literature, can be finetuned or plugged into search/optimization frameworks, and have enabled rapid lead generation in industry examples (citations referenced). However, real-world performance depends on evaluation functions and synthesizability, and many models are trained on approximate metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Generative language-model approaches complement and sometimes replace earlier generative classes (VAEs, GANs, graph-based methods); they scale well with data but share limitations around synthesizability compared to library-based virtual screening and physics-based validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Key limitations include lack of guaranteed synthesizability of generated molecules, reliance on approximate scoring/cost functions, limited out-of-distribution extrapolation, and scarcity of sufficiently large, high-quality chemistry training corpora compared to general web-scale text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spiers Memorial Lecture: How to do impactful research in artificial intelligence for chemistry and materials science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8634.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8634.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM agents (ChemCrow / Coscientist / ORGANA / ChemReasoner)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemistry agent systems using large language models (e.g., ChemCrow, Coscientist, ORGANA, ChemReasoner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent frameworks that use an LLM (sometimes a general-purpose model like GPT-4) as a central planner/orchestrator to invoke tools (simulators, databases, robots) to propose experiments, plan synthesis, and assist in discovery workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 and other LLMs used as orchestrators (unnamed chemistry agent LLMs in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM + tool-using agent architecture (transformer-based LLM controlling tool calls)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in paper for agents; agents leverage access to chemistry tools, simulators, literature databases and may be finetuned on domain-specific data or prompt-engineered.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Experiment planning, automated literature parsing, synthesis planning assistance, orchestrating simulations and robotic executions in self-driving labs; can be applied to molecular design as part of a broader pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based reasoning and tool-based workflows where the LLM proposes candidate molecules or experimental plans and calls external tools (predictors, simulators, retrosynthesis planners, robots) to evaluate or execute; iterative tool-use and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Agents can propose novel hypotheses and candidate molecules by composing knowledge and tool outputs, but the paper treats these systems as early-stage with no system-level novelty metrics reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity is provided by tool conditioning (e.g., using property predictors, retrosynthesis tools, simulators) and by prompts/constraints provided to the agent; paper highlights the potential but notes evaluation gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Evaluation is an open question per paper — needs domain-relevant benchmarks, safety/reliability checks, and task-specific metrics; paper notes that effective evaluation of chemistry-aware agents remains an open problem.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper cites multiple agent systems as promising developments that can orchestrate workflows and integrate tools to assist chemistry tasks; however, these agents are described as being in early stages and not yet proven broadly for autonomous discovery workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Agent-based workflows integrate multiple existing ML modules (predictors, generative models, planners) rather than replacing them; they offer more flexible orchestration than single-model pipelines but require reliable toolsets and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Open challenges include how to evaluate agents in chemistry, scarcity of domain-scale data, safety and reliability of autonomous decision-making, tool integration robustness, and ensuring proposed molecules are synthesizable and experimentally relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spiers Memorial Lecture: How to do impactful research in artificial intelligence for chemistry and materials science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8634.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8634.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs for literature parsing / knowledge extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models for bibliographic parsing and structured knowledge extraction from chemical literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs are used to extract structured experimental information, reaction procedures, and metadata from unstructured literature to build databases that can seed models for generation and self-driving lab workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLMs (chemistry-aware LLMs / general LLMs adapted for extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based LLMs used for information extraction / document understanding</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Corpora of chemistry literature, patents, and experimental descriptions (often noisy and heterogeneous); used to build structured databases like reaction repositories.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Creating structured experimental databases, improving training data for downstream generative or predictive models, supporting self-driving labs and reaction planning.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Extraction (not molecule generation): parsing free text and figures into structured representations (reaction steps, conditions, reagents) to create datasets that downstream models can use.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Indirect: enables downstream generation by improving training data quality and coverage; not directly producing molecules itself in the contexts discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Improves specificity of downstream generative/predictive models by providing richer, structured training data (e.g., reaction-condition pairs), thus enabling more application-tailored generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in detail in paper; implied needs include extraction accuracy, completeness, and structured data quality for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper highlights successful use cases of LLMs for bibliographic parsing and building structured repositories, which facilitate fine-tuning and improve capabilities of downstream models and self-driving lab pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>LLM-based extraction is a modern alternative to rule-based or heuristic parsers; it can scale to heterogeneous formats but depends on training data and may hallucinate without careful verification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Challenges include noisy/biased literature reporting, variable formatting, missing negative results, and the need for human curation/verification to ensure high-quality structured datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spiers Memorial Lecture: How to do impactful research in artificial intelligence for chemistry and materials science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8634.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8634.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs as interfaces for self-driving labs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models as human-machine interfaces and workflow authorship tools for self-driving laboratories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs are used to produce natural-language-driven experiment workflows, to translate user goals into machine-executable protocols, and to enable chemists unfamiliar with coding to interact with automated laboratory systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLMs (e.g., GPT-family or chemistry-adapted LLMs) used as workflow/assistant interfaces</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based LLMs used as natural-language-to-workflow translators and promptable assistants</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified; implicit use of language/chemistry corpora and system-specific tool descriptions to map instructions to executable steps.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Self-driving laboratories (SDLs), experiment planning, automation workflow creation, and human-robot interaction in chemical experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based generation of experimental procedures, workflow templates, and human-readable descriptions that can be translated (possibly with toolchains) into machine-executable instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Indirect: helps specify and run experiments that can lead to discovery; LLMs themselves are not the chemical generators here but lower the barrier to driving experimentation that can produce novel molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity is driven by the prompts and by integration with domain tools and robot APIs; paper notes successes in automating workflow creation but stresses the need for safety/reliability checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Paper notes evaluation and safety remain open problems; suggested needs include correctness of translations, reliability of executed protocols, and downstream experimental success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLMs have been effectively used as human-machine interfaces that bypass coding for SDLs and to automate workflow creation in early demonstrations; their use increases accessibility but requires safety validation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to manual coding and static templates, LLM-driven interfaces are more flexible and user-friendly but introduce risks of incorrect or unsafe instructions if not verified.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Primary concerns are safety, verification of generated protocols, integration with hardware, and ensuring that generated workflows are experimentally feasible and reproducible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spiers Memorial Lecture: How to do impactful research in artificial intelligence for chemistry and materials science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8634",
    "paper_id": "paper-272663079",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "SMILES-based chemistry language models",
            "name_full": "SMILES-based chemistry language models (unsupervised/finetuned)",
            "brief_description": "Language models trained on molecular string representations (e.g., SMILES/SELFIES) to learn distributions over molecules and enable conditional generation or property-conditioned sampling for molecular design and inverse design tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "unspecified SMILES language models",
            "model_type": "language model / transformer / autoregressive (chemistry language model)",
            "model_size": null,
            "training_data": "Datasets of molecular strings such as SMILES, SELFIES, Group SELFIES and derived molecular string corpora (unspecified corpora in paper)",
            "application_domain": "Drug discovery, materials discovery, general molecular inverse design",
            "generation_method": "Direct generation of molecular strings (SMILES/SELFIES); conditional generation by finetuning or scoring; used as priors plugged into optimization/search frameworks",
            "novelty_of_chemicals": "Can generate molecules beyond enumerated libraries (potentially novel relative to training set); paper notes the capability to generate arbitrary molecules but also emphasizes issues of synthesizability and evaluation—no quantitative novelty metrics provided in this paper.",
            "application_specificity": "Conditioning / finetuning and goal-directed optimization are used to bias generation toward desired properties; evaluation often relies on downstream property predictors or approximate metrics rather than guaranteed real-world performance.",
            "evaluation_metrics": "Common metrics cited include validity, novelty, uniqueness, goal-directed optimization scores; chemistry-specific benchmarks referenced include GuacaMol, MOSES, and more realistic sets such as Tartarus.",
            "results_summary": "Paper reports that SMILES-based chemistry language models have been widely explored in literature, can be finetuned or plugged into search/optimization frameworks, and have enabled rapid lead generation in industry examples (citations referenced). However, real-world performance depends on evaluation functions and synthesizability, and many models are trained on approximate metrics.",
            "comparison_to_other_methods": "Generative language-model approaches complement and sometimes replace earlier generative classes (VAEs, GANs, graph-based methods); they scale well with data but share limitations around synthesizability compared to library-based virtual screening and physics-based validation.",
            "limitations_and_challenges": "Key limitations include lack of guaranteed synthesizability of generated molecules, reliance on approximate scoring/cost functions, limited out-of-distribution extrapolation, and scarcity of sufficiently large, high-quality chemistry training corpora compared to general web-scale text.",
            "uuid": "e8634.0",
            "source_info": {
                "paper_title": "Spiers Memorial Lecture: How to do impactful research in artificial intelligence for chemistry and materials science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLM agents (ChemCrow / Coscientist / ORGANA / ChemReasoner)",
            "name_full": "Chemistry agent systems using large language models (e.g., ChemCrow, Coscientist, ORGANA, ChemReasoner)",
            "brief_description": "Agent frameworks that use an LLM (sometimes a general-purpose model like GPT-4) as a central planner/orchestrator to invoke tools (simulators, databases, robots) to propose experiments, plan synthesis, and assist in discovery workflows.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4 and other LLMs used as orchestrators (unnamed chemistry agent LLMs in cited works)",
            "model_type": "LLM + tool-using agent architecture (transformer-based LLM controlling tool calls)",
            "model_size": null,
            "training_data": "Not specified in paper for agents; agents leverage access to chemistry tools, simulators, literature databases and may be finetuned on domain-specific data or prompt-engineered.",
            "application_domain": "Experiment planning, automated literature parsing, synthesis planning assistance, orchestrating simulations and robotic executions in self-driving labs; can be applied to molecular design as part of a broader pipeline.",
            "generation_method": "Prompt-based reasoning and tool-based workflows where the LLM proposes candidate molecules or experimental plans and calls external tools (predictors, simulators, retrosynthesis planners, robots) to evaluate or execute; iterative tool-use and planning.",
            "novelty_of_chemicals": "Agents can propose novel hypotheses and candidate molecules by composing knowledge and tool outputs, but the paper treats these systems as early-stage with no system-level novelty metrics reported here.",
            "application_specificity": "Specificity is provided by tool conditioning (e.g., using property predictors, retrosynthesis tools, simulators) and by prompts/constraints provided to the agent; paper highlights the potential but notes evaluation gaps.",
            "evaluation_metrics": "Evaluation is an open question per paper — needs domain-relevant benchmarks, safety/reliability checks, and task-specific metrics; paper notes that effective evaluation of chemistry-aware agents remains an open problem.",
            "results_summary": "Paper cites multiple agent systems as promising developments that can orchestrate workflows and integrate tools to assist chemistry tasks; however, these agents are described as being in early stages and not yet proven broadly for autonomous discovery workflows.",
            "comparison_to_other_methods": "Agent-based workflows integrate multiple existing ML modules (predictors, generative models, planners) rather than replacing them; they offer more flexible orchestration than single-model pipelines but require reliable toolsets and evaluation.",
            "limitations_and_challenges": "Open challenges include how to evaluate agents in chemistry, scarcity of domain-scale data, safety and reliability of autonomous decision-making, tool integration robustness, and ensuring proposed molecules are synthesizable and experimentally relevant.",
            "uuid": "e8634.1",
            "source_info": {
                "paper_title": "Spiers Memorial Lecture: How to do impactful research in artificial intelligence for chemistry and materials science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLMs for literature parsing / knowledge extraction",
            "name_full": "Large language models for bibliographic parsing and structured knowledge extraction from chemical literature",
            "brief_description": "LLMs are used to extract structured experimental information, reaction procedures, and metadata from unstructured literature to build databases that can seed models for generation and self-driving lab workflows.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLMs (chemistry-aware LLMs / general LLMs adapted for extraction)",
            "model_type": "transformer-based LLMs used for information extraction / document understanding",
            "model_size": null,
            "training_data": "Corpora of chemistry literature, patents, and experimental descriptions (often noisy and heterogeneous); used to build structured databases like reaction repositories.",
            "application_domain": "Creating structured experimental databases, improving training data for downstream generative or predictive models, supporting self-driving labs and reaction planning.",
            "generation_method": "Extraction (not molecule generation): parsing free text and figures into structured representations (reaction steps, conditions, reagents) to create datasets that downstream models can use.",
            "novelty_of_chemicals": "Indirect: enables downstream generation by improving training data quality and coverage; not directly producing molecules itself in the contexts discussed.",
            "application_specificity": "Improves specificity of downstream generative/predictive models by providing richer, structured training data (e.g., reaction-condition pairs), thus enabling more application-tailored generation.",
            "evaluation_metrics": "Not specified in detail in paper; implied needs include extraction accuracy, completeness, and structured data quality for downstream tasks.",
            "results_summary": "Paper highlights successful use cases of LLMs for bibliographic parsing and building structured repositories, which facilitate fine-tuning and improve capabilities of downstream models and self-driving lab pipelines.",
            "comparison_to_other_methods": "LLM-based extraction is a modern alternative to rule-based or heuristic parsers; it can scale to heterogeneous formats but depends on training data and may hallucinate without careful verification.",
            "limitations_and_challenges": "Challenges include noisy/biased literature reporting, variable formatting, missing negative results, and the need for human curation/verification to ensure high-quality structured datasets.",
            "uuid": "e8634.2",
            "source_info": {
                "paper_title": "Spiers Memorial Lecture: How to do impactful research in artificial intelligence for chemistry and materials science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLMs as interfaces for self-driving labs",
            "name_full": "Large language models as human-machine interfaces and workflow authorship tools for self-driving laboratories",
            "brief_description": "LLMs are used to produce natural-language-driven experiment workflows, to translate user goals into machine-executable protocols, and to enable chemists unfamiliar with coding to interact with automated laboratory systems.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLMs (e.g., GPT-family or chemistry-adapted LLMs) used as workflow/assistant interfaces",
            "model_type": "transformer-based LLMs used as natural-language-to-workflow translators and promptable assistants",
            "model_size": null,
            "training_data": "Not specified; implicit use of language/chemistry corpora and system-specific tool descriptions to map instructions to executable steps.",
            "application_domain": "Self-driving laboratories (SDLs), experiment planning, automation workflow creation, and human-robot interaction in chemical experimentation",
            "generation_method": "Prompt-based generation of experimental procedures, workflow templates, and human-readable descriptions that can be translated (possibly with toolchains) into machine-executable instructions.",
            "novelty_of_chemicals": "Indirect: helps specify and run experiments that can lead to discovery; LLMs themselves are not the chemical generators here but lower the barrier to driving experimentation that can produce novel molecules.",
            "application_specificity": "Specificity is driven by the prompts and by integration with domain tools and robot APIs; paper notes successes in automating workflow creation but stresses the need for safety/reliability checks.",
            "evaluation_metrics": "Paper notes evaluation and safety remain open problems; suggested needs include correctness of translations, reliability of executed protocols, and downstream experimental success rates.",
            "results_summary": "LLMs have been effectively used as human-machine interfaces that bypass coding for SDLs and to automate workflow creation in early demonstrations; their use increases accessibility but requires safety validation.",
            "comparison_to_other_methods": "Compared to manual coding and static templates, LLM-driven interfaces are more flexible and user-friendly but introduce risks of incorrect or unsafe instructions if not verified.",
            "limitations_and_challenges": "Primary concerns are safety, verification of generated protocols, integration with hardware, and ensuring that generated workflows are experimentally feasible and reproducible.",
            "uuid": "e8634.3",
            "source_info": {
                "paper_title": "Spiers Memorial Lecture: How to do impactful research in artificial intelligence for chemistry and materials science",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.01780225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>How to do impactful research in artificial intelligence for chemistry and materials science
8 Oct 2024</p>
<p>Austin H Cheng 
Department of Chemistry
University of Toronto
M5S 3H6TorontoOntarioCanada</p>
<p>Department of Computer Science
University of Toronto
M5S 2E4Toronto, TorontoOntarioCanada</p>
<p>Vector Institute for Artificial Intelligence
M5G 1M1TorontoOntarioCanada</p>
<p>Cher Tian Ser 
Department of Chemistry
University of Toronto
M5S 3H6TorontoOntarioCanada</p>
<p>Department of Computer Science
University of Toronto
M5S 2E4Toronto, TorontoOntarioCanada</p>
<p>Vector Institute for Artificial Intelligence
M5G 1M1TorontoOntarioCanada</p>
<p>Marta Skreta 
Department of Computer Science
University of Toronto
M5S 2E4Toronto, TorontoOntarioCanada</p>
<p>Vector Institute for Artificial Intelligence
M5G 1M1TorontoOntarioCanada</p>
<p>Andrés Guzmán-Cordero 
Vector Institute for Artificial Intelligence
M5G 1M1TorontoOntarioCanada</p>
<p>Tinbergen Institute
University of Amsterdam
AmsterdamNetherlands</p>
<p>Luca Thiede 
Department of Computer Science
University of Toronto
M5S 2E4Toronto, TorontoOntarioCanada</p>
<p>Vector Institute for Artificial Intelligence
M5G 1M1TorontoOntarioCanada</p>
<p>Andreas Burger 
Department of Computer Science
University of Toronto
M5S 2E4Toronto, TorontoOntarioCanada</p>
<p>Vector Institute for Artificial Intelligence
M5G 1M1TorontoOntarioCanada</p>
<p>Abdulrahman Aldossary 
Department of Chemistry
University of Toronto
M5S 3H6TorontoOntarioCanada</p>
<p>Shi Xuan Leong 
Department of Chemistry
University of Toronto
M5S 3H6TorontoOntarioCanada</p>
<p>School of Chemistry, Chemical Engineering and Biotechnology
Nanyang Technological University
63737Singapore</p>
<p>Sergio Pablo-García 
Acceleration Consortium
M5G 1X6TorontoOntarioCanada</p>
<p>Felix Strieth-Kalthoff 
School of Mathematics and Natural Sciences
University of Wuppertal
WuppertalGermany</p>
<p>Alán Aspuru-Guzik 
Department of Chemistry
University of Toronto
M5S 3H6TorontoOntarioCanada</p>
<p>Department of Computer Science
University of Toronto
M5S 2E4Toronto, TorontoOntarioCanada</p>
<p>Vector Institute for Artificial Intelligence
M5G 1M1TorontoOntarioCanada</p>
<p>Acceleration Consortium
M5G 1X6TorontoOntarioCanada</p>
<p>Department of Chemical Engineering and Applied Chemistry
Department of Materials Science and Engineering
University of Toronto
University of Toronto. Lebovic Fellow
Canadian Institute for Advanced Research (CIFAR)
1-46 | 1</p>
<p>How to do impactful research in artificial intelligence for chemistry and materials science
8 Oct 2024C82F1CA911F1E3F49769F17423CDD47010.1039/c000000xarXiv:2409.10304v2[cs.LG]Received Xth XXXXXXXXXX 20XX, Accepted Xth XXXXXXXXX 20XX First published on the web Xth XXXXXXXXXX 200X
Machine learning has been pervasively touching many fields of science.Chemistry and materials science are no exception.While machine learning has been making a great impact, it is still not reaching its full potential or maturity.In this perspective, we first outline current applications across a diversity of problems in chemistry.Then, we discuss how machine learning researchers view and approach problems in the field.Finally, we provide our considerations for maximizing impact when researching machine learning for chemistry.</p>
<p>Introduction</p>
<p>Machine learning (ML) has been applied in many facets of chemistry, and its use is rapidly growing.We argue in this perspective that despite this dramatic growth and impact, ML could be employed better and more extensively.Current work is still far from exhausting the potential of ML to advance theory and application in chemistry in terms of breadth, depth, and scale.In addition, the actual types of problems that ML could tackle, such as hypothesis generation or enabling internalized scientific understanding, are still areas of active research or open problems.</p>
<p>To color a picture of the field, we begin by outlining a taxonomy of the chemical problems to which ML has been applied, ranging from prediction, generation, synthesis, force fields, spectroscopy, reaction optimization, and foundation models.Shifting gears, we then introduce types of problems in ML and show how chemical problems can be reformulated as instances of ML problems.These standard problems help organize the toolbox of algorithms and theory provided by ML.Digging further into this perspective, we examine differences in practices and values between the ML and chemistry communities and highlight where collaboration and cross-pollinating perspectives can advance both fields.Armed with the above, we can then discuss how to select impactful applications of ML in chemistry and recommend our suggested good practices for research in this area.Chemistry, and science in general, involves data in one form or another.Not surprisingly, then, data science is integral to chemistry.Machine learning, a subfield of data science, has become an integral tool in our domain science's arsenal.Therefore, it is crucial to begin cataloguing and organizing critical efforts to date.</p>
<p>Chemistry meets data: A taxonomy of problems</p>
<p>We suggest a taxonomy of the chemical problems to which machine learning has been applied.As shown in Figure 1, ML has been applied to solve various chemical problems by encoding and decoding to and from chemical structure, properties, 3D structure and dynamics, and experimental data.For reasons of space, time, and focus, this is not a comprehensive review but rather an opportunity to highlight diverse applications of ML in chemistry.We will not introduce ML algorithms in detail.[3][4][5][6] 2.1 Structure to property: property prediction 2.1.1Cheminformatics and quantitative structure-activity relationships.Chemistry has leveraged data to predict properties from a chemical structure long before the everyday use of the term "machine learning".This field has been originally identified initially as cheminformatics.These tools sought to store, retrieve, and model chemical structures.Early examples began in 1957 with substructure searches in a database, 7 followed by simple multivariate regression for learning quantitative structure-activity relationships 8 (QSAR) between molecular descriptors like Hammett constants and partition coefficients, and biological activity. 9,10hese were mostly property-activity relationships -the first structure-activity relationships involved local explanations analyzing how substituents on a ring affected activity, 11 which could be generalized to many scaffolds via substructural analysis. 12Eventually, computers automatically encoded molecular structures as fingerprints -bit-vectors that store the presence or absence of many substructures found in the molecule. 13These fingerprints were useful in encoding molecular structures to predict molecular activity in simple models such as support vector machines. 141.2Representing molecules with expert descriptors.While chemists have a conceptual understanding of the effects of functional groups on the properties of a molecule, communicating this information to a model is critical to ensure that the model is predictive.Expert descriptors infuse chemical knowledge derived from experiments or conceptual knowledge into the features provided to a model and have achieved good predictive performance, especially in lowdata regimes.These expert descriptors also generalize well outside the model's training set, as chemical knowledge is baked into these features.As early as 1937, Hammett fitted sigma parameters for predicting the influence of chemical substituents on reactivity.15 Additionally, group contribution methods, which assume that structural components or functionalizations behave the same way across many different molecules, parameterize these components into numerical features that can be used to predict molecular properties.[16][17][18] The discipline has since grown to involve molecular fingerprinting techniques and the incorporation of 2D and even 3D information for use in prediction.In more recent times, as the properties of a homogeneous transition metal catalyst are strongly influenced by the ligands attached to it, parameterizing the structural and electronic features of these phosphine ligands has also been successful in predicting the properties of a catalyst.[19][20][21] Looping back to historical models, recent work has also been able to leverage density functional theory (DFT) and machine learning to successfully machine learn Hammett parameters.22 2.1.3Learned chemical representations.Models have become more complex with advances in computational hardware, moving from simple linear regression models to complex architectures like auto-encoders, generative adversarial networks, graph neural networks or transformers.Instead of relying on chemists to intuit the best way to represent a molecule, we can now harness the ability of models to automatically learn and exploit complex patterns within large amounts of data for property prediction.To a certain level of abstraction, which tends to ignore 3D information or wave function properties, molecules can be naturally represented as graphs where atoms are nodes and bonds are edges.By relaxing the notion of fingerprints from discrete bit-vectors to continuous feature vectors, we proposed graph neural networks to automatically learn continuous representations of important substructures, achieving state-of-the-art performance on molecular property prediction tasks.23,24 These representations have been deployed widely across multiple avenues like machine learning for olfactory properties of a molecule, 25 and in catalysis where adsorption properties of adsorbates were predicted.26 While simple atomic and bond features required for the constructed graphs can be generated quickly, 27 the properties that one wants to target for prediction are much harder to obtain -especially in higher qualities and fidelities.As learned representations typically require large amounts of data, complicated architectures do not function as well with low amounts of data gathered from typical experimental settings.To bridge this gap, molecular benchmarks were created to assess the quality of such learned representations properly.These benchmarks contain tasks gathered from literature data related to predicting biological behaviours and physicochemical or quantum chemical properties and provide a common ground on which different machine-learning architectures can harness and exploit the same data in various ways for property prediction.28 To improve the performance of such graph embeddings, they can be further tuned if there are some intuitions about how the embedding spaces should be reshaped to reflect the distances between inputs properly.These can involve strategies like making the embeddings aware of how chemical reactions should transform these embeddings 29 or through strategies like contrastive learning.30 Finally, for tasks sensitive to the molecule's conformation in three dimensions, incorporating three-dimensional representations that exceed the capability of the innately deficient two-dimensional graphs has proven successful in predicting molecular properties.31 2.1. 4 imits and open problems. Despie the great strides made in molecular machine learning, the ability of machine learning models to extrapolate beyond the data it is trained on is still limited, posing barriers for application to novel chemistries.Several approaches can potentially bridge these gaps.For example, by using physics-informed models that can contain fundamental representations that help in generalizing the representation itself to satisfy some symmetries or properties related to the physical laws of nature.Active learning is also a powerful tool for expanding datasets on the fly by capturing computational or experimental data for extrapolation.Additionally, while models have progressively performed better on property prediction benchmark tasks, these benchmarks represent only a tiny subset of chemical tasks, making their performance on various other tasks unknown.32 While we have attempted to create benchmarks more representative of typical tasks, 33 this is still not a central focus of the community.</p>
<p>Structure-to-property models have been widely employed in screening projects, leading to experimentally verified predictions.We will discuss a few selected case studies in Sec.2.2.1.</p>
<p>Property to structure: designing molecules in chemical space</p>
<p>While the rational design paradigm analyzes the relationship between structure and properties to design promising molecules, another paradigm asks: what are all the possible molecules that satisfy a given property?Solving this question is known as the inverse design problem.</p>
<p>Chemical space is the set of all synthesizable molecules and is often cited for having an astronomical size of at least 10 33 to 10 60 molecules. 34,35Within this vast space are potential drugs that could cure current diseases and putative materials that could enable a sustainable future.</p>
<p>Virtual screening.</p>
<p>A simple approach to navigating chemical space is to enumerate a feasible set of possible options and then narrow them down to the best solution.This shift in perspective has its experimental implementation employing strategies such as high-throughput screening of chemical libraries and combinatorial chemistry to synthesize these libraries. 36Given the astronomical size of chemical space, it became clear that arbitrarily searching through compounds would produce few promising hits, making this approach inefficient as the cost of extensive chemical synthesis campaigns is often taxing or prohibitive. 37his motivated virtual screening and computational search funnels as a way to filter out unpromising compounds, leaving only the best candidates for synthesis and testing.In drug discovery, molecules are filtered out with computationally lean checks such as high molecular weight or problematic functional groups, followed by more computationally intensive docking for estimating binding affinity, ultimately narrowing down to a handful of lead compounds. 38Scaling the size of virtual libraries increases the likelihood of promising hits, which has motivated ever-larger screening campaigns requiring increasing computational resources.One example was the Harvard Clean Energy Project, 39 in which we searched through 10 7 candidates with quantum chemistry calculations on distributed volunteer computing to search for efficient organic photovoltaics.</p>
<p>Similarly, VirtualFlow 40 docked over 10 9 molecules by efficiently using thousands of CPU cores.As the size of chemical libraries grows, with the required computational resources scaling linearly, hierarchical approaches to evaluate the fitness of individual synthetic building blocks offer a way past linear scaling. 412.2 Generative models for inverse design.As the size of chemical libraries surpasses 10 15 molecules 42 and becomes computationally prohibitive to screen, ML offers ways to consider large search spaces without simulating all molecules.For example, in a chemical library, many molecules should have similar structures and properties, so running simulations on every molecule is redundant.A formal way to handle this is to simulate a portion of the library and then train property prediction models on this subset, which should be generalized across the library. Sincethese property prediction models are computationally cheaper than simulations, they can be evaluated for the entire library and used to prioritize candidates for simulation.We leveraged this approach to design organic light-emitting diodes that were verified experimentally.43 However, another arm of ML offers a way to consider all (or a vast subset) of the chemical space.Given a dataset of molecules in a representation such as SMILES strings, generative models learn to generate strings which resemble the dataset.Because generative models can consider arbitrary strings, they could potentially generate any molecule in chemical space.They can also be conditioned to generate molecules with desired properties -essentially reversing the property prediction process.44,45 Molecular generative models have been applied with many model classes.We pioneered the use of variational autoencoders (VAEs) 46 for this purpose.Other examples include autoregressive models, 47 generativeadversarial networks (GANs), 48 and reinforcement learning, 49,50 amongst many other sampling strategies.Generative models have also been extended and shown to work well with various representations like SMILES, SELFIES, 51 and Group SELFIES 52 strings, as well as molecular graphs and fragments.Molecular optimization methods such as genetic algorithms 53 and Bayesian optimization 54 also have been sometimes called generative models despite not learning a molecular distribution per se.A recent review of different generative model classes and representations can be found in Gao et al., 55 although this is a rapidly moving field.</p>
<p>As more generative models were proposed, benchmarks such as GuacaMol 56 and MOSES 57 began evaluating and comparing different generative models based on validity, novelty, uniqueness, and goal-directed optimization.Optimization has been such a primary focus that molecular design can be regarded as a combinatorial optimization of molecular properties over the space of molecular graphs.In this way, a new benchmark emphasizes sample efficiency, which is the number of property evaluations needed to reach optimal molecules. 55In addition, more realistic benchmark tasks relying on simulation have been recently proposed by us in the Tartarus benchmark set. 33Tartarus more closely resembles real-world scenarios where computational and experimental resources are constrained.However, by departing from chemical libraries for the entire chemical space, generative models relaxed the crucial constraint of synthesizability.Generative models can suggest molecules which are difficult to synthesize and evaluate. 580][61] Other approaches combine virtual libraries with generative approaches to ensure that proposed molecules are always from the library. 62These methods have particular relevance for high-throughput arrays and self-driving laboratories, as predicted molecules that are not synthetically feasible with readily available platforms could slow down closed-loop approaches.</p>
<p>For a comprehensive overview of these advancements and the state of the art in molecule design, Du et al. provide an excellent review, summarizing the latest developments and methodologies in the field. 63nerative models have proven worthy in the recent years.Quite notably the company InSilico Medicine has employed them to generate several drugs that are undergoing clinical trials currently.In 2019, together with InSilico and Wuxi Apptec researchers, we showed the ability of generative models to develop a lead drug candidate in approximately 45 days. 64Many researchers since then have continued to show other examples of generative models in drug discovery.For example, Barzilay and co-workers have developed antibiotics using similar approaches. 65</p>
<p>2.2.3</p>
<p>Limits and open problems.While candidates can be generated easily with such models, the quality of the candidates depends on the ability to develop a properly performing and scalable cost function for conditioning the generative models.Additionally, these models are trained on approximate metrics, which means that their real-life performance still has to be evaluated.Thus, evaluating the synthesizability of a candidate or providing steps to make candidates is of paramount importance (see next section).</p>
<p>Most generative models have been developed with simple benchmarks in mind, such as predicting simple properties like logP.However, developing using proper benchmarks (such as Tartarus) or restricting them to feasible sets of molecules, such as those synthesizable with self-driving labs (see Sec. 2.7), remains a challenge.</p>
<p>Structure to structure: synthesis planning and reaction condition prediction</p>
<p>Synthesis planning -i.e.finding synthetic pathways that give rise to a desirable target molecule -is an open challenge that chemists have faced for over a century, particularly in the "molecular world" of drug discovery, agricultural chemistry or molecular materials chemistry.This problem is complex in two respects: First, predicting the outcome of a specific unseen reaction, given all reactants, reagents, and reaction conditions, is effectively an unsolved problem to date.Second, even with such a "reaction prediction" tool at hand, finding feasible multi-step sequences of reactions that eventually enable the synthesis of the target molecule from cheap and commercially available precursors requires searching a massive network of possible pathways.Additional challenges arise from practical demands to the synthesis planning problem: efficiency, cost, waste production, sustainability, safety, or toxicity are practical concerns, especially in an industrial setting.</p>
<p>Synthesis planning.</p>
<p>Synthesis planning is classically addressed through the formalism of retrosynthesis, as pioneered by Nobel Prize winner E. J. Corey: 66 Using knowledge of chemical reactivity, the target molecule is gradually disconnected into progressively simpler precursors, which eventually yields commercially available starting materials.Formally, this corresponds to a tree search problem.As early as in the 1960s, Corey realized that this approach is ideally suited to be tackled in a computational manner. 67Since then, a number of expert systems have been developed to guide this tree search. 68he past decade has seen significant progress in addressing this challenge using the toolbox of ML.0][71] This symbolic approach, however, requires a predefined catalogue of all reaction types, often referred to as reaction "rules" or "templates", which itself presents new obstacles.There is neither a generally accepted definition of the term "reaction rule" nor an unambiguous procedure to perform reaction rule extraction from data.Alternatively, "template-free" approaches to the one-step reaction prediction problem, predict reactions as graph edits in the starting material graph, 72 or solve a sequence-to-sequence "product-to-starting-material" translation task. 73,74Notably, these models (template and template-free) can be similarly trained in the forward direction, predicting reaction products from starting materials.</p>
<p>These single-step prediction models have been used to build tree search models, which aim to solve the full synthesis planning problem.In this context, a Monte-Carlo tree search is usually the method of choice.Following the pioneering works from Segler et al. 75 and Coley et al., 76 a number of mostly open-source systems have been released. 77,783.2Prediction and optimization of reaction conditions.What is often overlooked in synthesis planning is that knowing a possibly suitable reaction type alone does not guarantee that the envisioned intermediate or target product can be prepared from the proposed starting materials.The question if the product can be obtained (ideally in high yield), crucially depends on what is often referred to as the reaction conditions: the choice of reagent(s), catalyst(s), additive(s) and solvent, the values of continuous parameters such as stoichiometries, temperature and reaction time, as well as the practical details of running the reaction in the laboratory.In an ideal scenario, an AI-assisted tool would take in a new "starting-material-to-product" transformation, and spit out the required reaction conditions for this transformation.However, this is yet to be achieved, particularly because reaction conditions cover a vast combinatorial parameter space and are frequently governed by underlying physical principles that are difficult to simulate.In practice, reaction conditions are often selected by employing "nearest-neighbor" reasoning based on literature precedents, either automatically or through human expertise.</p>
<p>Machine learning approaches to reaction condition optimization have thus mainly focused on regression modelling of reaction yields as a function of reaction conditions.In this context, data-driven approaches have intersected with regression techniques from physical organic chemistry, which attempt to model reaction outcomes based on mechanistic considerations.0][81][82][83] For example, our work on optimizing the E/Z ratio of a reaction relevant to pharmaceutical process chemistry showed that only with ≈ 100 experiments we were able to outperform what had been the state-of-theart for this process by human-only reaction optimization. 84Meanwhile, the use of literature data for the same purpose is highly flawed, 85,86 usually necessitating individual, case-by-case reaction optimization (see below for a more detailed discussion).Black-box optimization algorithms, particularly Bayesian Optimization (BO), have become increasingly prominent over the past decade. 6,87In BO, probabilistic models for predicting reaction yields are built through Bayesian inference with existing data.These models then iteratively guide decision-making throughout the optimization process.The idea of iterative, closed-loop optimization with ML-based surrogate models is discussed further in Sec.2.7.For condition optimization, these iterative approaches have demonstrated remarkable success in increasingly complex synthetic reaction scenarios. 87At the same time, chemistry-specific challenges, such as the identification of conditions which are "generally applicable" to a wide variety of substrates, as opposed to just one or a few model substrates, have inspired algorithmic advances in the field. 88,89No-tably, our work on the Suzuki reaction 88 led to generally applicable conditions with double the yield of the previous state-of-the-art in the field.</p>
<p>2.3.3</p>
<p>Limits and open problems.While the field of ML-based synthesis planning has seen significant algorithmic advances during the past ten years, its practical utility has remained limited to the development of relatively simple target molecules and short synthetic routes.In fact, as of today, expert systems, which involve manually coding reaction types and applicability rules, represent the state of the art in computer-aided synthesis planning.In particular, Grzybowski's Chematica system (now commercialized as Synthia) 90 has had impressive experimental applications, 91 even in complex natural product synthesis, 92,93 or supply-chain-aware synthesis planning. 94,95In principle, while ML-based algorithms should be capable of providing similar or superior synthetic routes compared to these expert systems, the current shortcomings can mainly be attributed to deficiencies in the quality and quantity of available synthesis data and algorithmic limitations in extracting structured knowledge from the data.We and others have extensively discussed these factors recently. 96imilar data limitations have also been discussed in the context of reaction outcome and reaction condition prediction.Patent data 97 and even commercial databases are highly problematic not only because of erroneous, inconsistent or unstructured data reporting: Human biases in the reported experiments, particularly the accumulation of prominent conditions and the lack of low-yielding records, have prevented predictive modelling of reaction yields from literature data. 85,86Community-driven, open source data repositories such as the Open Reaction Database 98 represent an essential step towards less biased and more holistic data collection -but such initiatives require a more digitized mindset in the way data is generated, collected and reported in synthetic organic chemistry laboratories.</p>
<p>A further consequence of this data deficiency is the lack of representative benchmark problem sets.This applies to multi-step synthesis planning, where benchmarks are urgently needed for a more quantitative evaluation of synthesis planning performance.Similarly, optimization algorithms for chemical reactivity would benefit from representative benchmarks to evaluate how standard BO algorithms translate to the intricacies of chemical reactivity.Most importantly, such benchmarks must reflect real-life problems, as identified by expert chemists, in order to inspire and motivate algorithmic ML advances to tackle the challenges in computer-aided organic synthesis.</p>
<p>Structure to physics: simulation and 3D structure</p>
<p>Machine learning has enabled data-driven solutions to both experimental problems and computational problems.Whereas organic chemistry emphasizes molecules' 2D molecular graph structure, molecules are also grounded in 3D physical reality by the Schrödinger equation, providing a rich theory of quantum mechanics and statistical mechanics for predicting molecular properties and interactions.Simulation methods such as density functional theory (DFT) and molecular dynamics (MD) then use this theory to computationally predict molecular properties and interactions.However, despite continual increases in computing power, these simulations remain computationally costly, which has restricted simulation to small systems at short timescales.By learning from the results of many simulations, ML offers a unique opportunity to accelerate molecular simulation.</p>
<p>Neural network potentials.</p>
<p>A fundamental problem in quantum chemistry is: given a molecule represented as a collection of nuclear points in 3D space, solve the Schrödinger equation and predict the total energy and the forces on each atom.Forces then enable simulation of dynamics forward in time using Newton's equations.However, solving the Schrödinger equation is complex and computationally costly for molecular systems, and simulating Newton's equations requires forces at every frame of simulation.For this reason, forces were approximated by simple functions fitted to experimental data, giving rise to the first parameterized force fields such as the Lennard-Jones potential. 99Semiempirical models incorporated many more experimentally fitted parameters for predicting energy and forces. 100These empirical force fields enabled classical molecular dynamics simulations, allowing study of simple proteins. 101However, capturing behavior like chemical reactivity requires incorporating quantum effects.Advances in computer power and faster simulation methods such as density functional theory (DFT) eventually made it possible to solve the Schrödinger equation at every timestep with ab initio molecular dynamics, but at large computational cost. 102 significant shift came with the introduction of neural force fields.By training neural networks on DFT data to predict energy and forces directly from 3D nuclear coordinates, molecular dynamics could now be propagated at ab initio accuracy at a much lower computational cost. 1035][106] Neural force fields have been competitively benchmarked in ML, continually comparing different architectures and methods on several benchmarks.A detailed timeline of development of these equivariant architectures is given in Duval et al. 107 As datasets of energy and forces have grown, such as the Open Catalyst Benchmark, 108 neural force fields have started striving for universal applicability. 1094.2Predicting wavefunctions and electron densities.An alternative to predicting energies with force fields is to predict the wavefunction or electron density itself.The advantage is that these objects contain energy and the rest of the system's physical observables.For example, neural networks can be trained to predict the Hamiltonian matrix directly from the nuclear coordinates.110,111 Diagonalizing the Hamiltonian matrix gives the molecular orbitals, which comprise the wavefunction.Furthermore, self-consistent field iteration can be initialized using the predicted wavefunction, allowing faster convergence of the quantum chemistry.Recently, it was shown that neural networks can be trained so that their output satisfies the self-consistency equation, bypassing the need for labels of Hamiltonian matrices.112 Furthermore, neural networks can be used as ansätze to represent the wavefunction itself directly.In this case, the network takes as input electron coordinates, and outputs wavefunction amplitude.Using the same stochastic optimization algorithms, neural wavefunctions can be trained to minimize the variational energy and satisfy the Schrödinger equation.[113][114][115][116][117] This approach has recently been extended to excited states.118 Alternatively, for density functional theory, neural networks can be trained to directly predict charge density given the nuclear coordinates.[119][120][121] ML has also been applied to learn density functionals.122 2.4.3 Predictg and generating 3D structure.Even if fast and accurate force fields were available, many problems rely on finding energetically preferred conformations of molecules.However, conformational space remains huge and cannot be practically enumerated, especially for large systems like proteins.Similarly, when modelling chemical reactions, the sizeable conformational search space makes it challenging to identify transition states.To solve these problems, ML approaches can predict and generate 3D structure directly.</p>
<p>The large conformational search space motivates generative models to navigate this space.Unconditional generative models such as equivariant diffusion models can generate 3D atomic positions and atom types simultaneously. 123For the problem of conformer search, which seeks stable 3D configurations for a given molecule, atom types can held constant while generation is conditioned on the 2D molecular graph.Some approaches generate atom positions freely, 124 while other approaches generate torsion angles of rotatable bonds. 125,126Recent work has shown that forgoing both torsional and rotational symmetry constraints can yield better results, but at a higher cost. 127A related task known as docking performs conformer search of a ligand inside a protein pocket, as an estimate of binding affinity.This has also been approached with diffusion models. 128n the problem of crystal structure prediction, the goal is to find the most stable periodic arrangement of atoms for a given composition.While traditional approaches search through all stable configurations of coordinates and lattice vectors to find the lowest energy structure, 129 equivariant diffusion models have found a natural fit for this problem, diffusing both coordinates and lattice parameters simultaneously, 130,131 while also enforcing space group constraints 132 to enhance performance further.Indeed, scaling this diffusion approach to large datasets enabled inverse design to satisfy multiple desired properties simultaneously. 133n the fields related to the simulation of biomolecules, 3D structure prediction problems are abundant.The longstanding problem of predicting folded 3D protein structure from protein sequence has, to a certain extent, been solved by AlphaFold2. 134and related models.Building on this approach, diffusion models have generated protein backbones represented as sequences of rigid bodies of residues. 135,136These models have been so successful that they have been used to design proteins satisfying structural constraints, which have been experimentally validated. 137,138The scope of these diffusion models has expanded to all biomolecules, with methods predicting how proteins, RNA, DNA, and ligands assemble in 3D atomistic detail, 139,140 subsuming the task of docking, and hence, promising to become a de-facto conditioning function for drug discovery in the future.</p>
<p>2.4.4Enhanced sampling and coarse-grained simulation.While finding the most stable geometry is useful, truly modelling the thermodynamic interactions between molecules requires sampling the equilibrium distribution of 3D structures.Equilibrium states follow a Boltzmann distribution with respect to the energy, and generative models which learn this equilibrium distribution are known as Boltzmann generators. 141Deep generative models are beginning to solve this problem using flow matching, 142 a variant of diffusion models, and transferability has been demonstrated across many different peptides. 143Another approach learns to sample equilibrium distributions by leveraging the Fokker-Planck equation. 144n coarse-graining one typically groups atoms together into so-called beads, which afford lower computational cost and the possibility to capture long timescale events.However, the forces on these coarse beads then need to be fitted to allatom forces.6][147] Flow-matching 148 removes the requirement for all-atom forces, needing only equilibrium samples of coarse-grained beads.Furthermore, diffusion models can simultaneously learn a generative model and coarse-grained force field. 149hile coarse-grained force fields are significantly faster to evaluate than atomistic ones, MD simulations are still limited by having to use femtosecondlevel integration time steps.Alternative methods for equilibrium methods focus on accelerating molecular dynamics to reach long timescales.This can be done through "coarse-graining in time," which trains generative models to predict the outcome of taking large timesteps. 150,151astly, work has been carried out towards extending models to multiple ranges of thermodynamic properties like temperature and pressure. 152This allows simulation of different environments as well as training on previously unsuitable data.Adding extra parameters like temperature to the model input, one can add the corresponding derivatives of the coarse-grained free energy function to the loss.Response properties which are higher order derivatives of the free energy can be computed via multiple backward passes.Incorporating thermodynamic parameters might be one of the key ingredients to simulate biological or industrial settings in a holistic manner.</p>
<p>For rare-event sampling like chemical reactions and transition state search, methods for sampling transition paths without reaction coordinates have been emerging. 153,154Alternatively, when datasets of reactants, products, and transition states are available, generative models can be directly trained to generate transition states conditioned on reactants and products. 155,1564.5 Limits and open problems.While neural force fields can achieve great accuracy, they still require enough training data to cover the entire phase space.Without complete coverage, neural force fields can stumble into unstable dynamics.One benchmark emphasizes that force fields should be judged by their dynamics, not their force errors.157 However, these issues may begin to go away as neural forces are trained on ever larger datasets in the quest for universal force fields.Though ML models are limited by the quality of their data, the fact that new data can be generated by simulation paints a promising picture for data availability and large models.</p>
<p>At the same time, much work remains to reach simulation at large length and time scales.The most significant challenges of proper equilibrium sampling under metastable conditions and the related problem of rare-event sampling also remain areas in need of improvement and, therefore, the focus of many recent efforts.</p>
<p>Structure and analysis: spectroscopy and elucidation</p>
<p>One natural yet underexplored area of ML application in chemistry is structure elucidation, which aims to predict 2D or 3D molecular structures from spectroscopic or other analytical data.Just as computer vision enables computers to perceive the natural world, computational spectroscopy could allow machines to perceive the molecular world through analytical instruments.The anticipated increase in the synthesis of de novo and unknown compounds through advances in experimentation automation drives the need for faster yet accurate structure elucidation to fully support these autonomous molecular and reaction discovery platforms.</p>
<p>2.5.1 Forward spectral prediction.The most straightforward approach to data-driven structure elucidation is to store a library of spectra, search for a match in the library for a given spectrum, and then retrieve the corresponding structure.To increase the coverage of the library, forward spectral prediction can be used to predict spectra given chemical structure.While physical simulation offers a grounded way to predict spectra, it can be difficult and computationally expensive.An alternative approach leverages machine learning to predict spectrum from structure, for a variety of types of spectra, including mass spectrometry (MS), 158,159 nuclear magnetic resonance (NMR), 160,161 and ultravioletvisible spectroscopy (UV-vis). 162Some frame the forward prediction problem as formula prediction, employing either autoregressive models or a fixed vocabulary of formulas 163,164 ; while others focus on subgraph prediction, utilizing recursive fragmentation, autoregressive generation, and deep probabilistic models, 159,165,166 or incorporate 3D structural information. 167,168In the context of mass spectra, some methods approximate the spectrum as a sequence of discrete bins with corresponding peak intensities, reducing the problem to a task of regressing the mass spectrum directly from structure. 158,167In addition to structureto-spectrum prediction, another approach involves predicting structure-property relationships by estimating various molecular descriptors -ranging from scalars (e.g., energy, partial charges) to vectors (e.g., electric dipoles, atomic forces), and higher-order tensors (e.g., Hessian matrix, polarizability, octupole moment) -and then using these descriptors to predict different spectra, including IR, Raman, UV-Vis, and NMR. 1695.2Structure elucidation.On the other side is the inverse problem of directly predicting chemical structure from a given spectrum.DENDRAL was the first expert system for inferring chemical structure from mass spectra in 1969.170,171 Chemists also used ML to analyze infrared (IR), nuclear magnetic resonance (NMR), and mass spectra for identifying limited sets of functional groups.[172][173][174] While these methods provide helpful structural insights, they are insufficient for fully elucidating molecular structures.</p>
<p>Combining information of many inferred functional groups has enabled structure elucidation.For NMR data, the molecular structure can be elucidated by first identifying molecular substructures and functional groups, [175][176][177] which are then optimally assembled via beam search over possible configurations or constructed atom-by-atom, [177][178][179] similar to the approach chemists take when interpreting NMR spectra.Similar "reconstruction-by-substructure" strategies have been employed to varying degrees of structural detail for IR 180,181 and surface-enhanced Raman spectroscopy (SERS). 182However, as the number of atoms increases, this approach quickly encounters combinatorial scaling issues.</p>
<p>Molecular structure elucidation can also be tackled as an end-to-end problem from a deep learning perspective.In this approach, the spectra are tokenized into strings and SMILES strings are predicted; this can be viewed as a machine translation task.This approach has been applied to NMR, IR and tandem MS/MS data, [183][184][185][186][187] showing more significant promise for scaling to larger chemical systems and de novo structure elucidation.The structure prediction problem can also be formulated as an optimization task, e.g. by formulating it as a Markov decision process. 179If we consider scenarios where we have some prior information about the chemical system at hand, such as chemical formula, known starting materials and reaction conditions, implementing this information as constraints can help the model converge on a solution more efficiently.</p>
<p>Moving from molecules to crystals, solving the inverse problem for X-ray spectroscopic data such as powder X-ray diffraction (PXRD) and X-ray absorption near-edge structure (XANES) spectra also poses interesting challenges for the machine learning community, where there are unique and underdeveloped opportunities for employing various deep learning models for generalizable crystal system and space group identification. 188,189Diffusion models have shown particular promise, especially given their successful application to counterpart inverse problems in text-to-image generation.In this context, we can draw parallels between text and spectra and between image generation and crystal structure prediction. 190,191n the field of rotational spectroscopy, the challenge of spectral assignment -i.e.deduce the rotational constants from a densely packed rotational spectrum -represents one of the earliest application of ML in this domain. 192This problem is particularly well-suited for deep learning techniques due to the dense yet easy-to-simulate nature of the spectra.However, the rotational constants alone do not determine the 3D structure of the molecule.The approach that we recently introduced solves this by inferring 3D structure given incomplete information as molecular formula, rotational constants, and unsigned atomic Cartesian coordinates known as substitution coordinates. 193n the realm of structural biology, advances in protein structure prediction have accompanied advances in cryo-electron microscopy.Reconstruction of protein structure from cryo-EM has been tackled using deep generative models. 194,195hese methods have progressed to the point of reconstructing biomolecular dynamics from cryo-electron tomography (cryo-ET). 196Structure elucidation using CryoEM continues to show day-to-day advances.Advances in data processing have provided incredible gains in resolution 197 that can only be improved by the use of ML methodologies.</p>
<p>Limits and open problems.</p>
<p>As with all data-hungry approaches, one key issue remains universal: While simulated spectra can be obtained in large quantities, it is crucial to consider if the model performs well on experimental spectra, which often exhibit more significant variability and inconsistencies.A relevant question to consider is: Would a more concerted effort by the scientific community to push for the deposition of raw spectral files in open data repositories help advance deep learning applications for automated spectra-to-structure elucidation?</p>
<p>For inverse spectrum-to-structure elucidation, while autonomous and de novo molecular structure determination of pure samples is indubitably essential to facilitate high-throughput reaction optimization and discovery campaigns, it is also crucial to address structure annotation of spectra from complex mixtures, which encompasses both the targeted identification of specific compounds of interest and non-targeted metabolomics.Such mixtures are standard in real-life sample matrices and are essential for various fields ranging from bio-diagnostics to forensics.Success in these tasks is highly contingent on the model's ability to disentangle and isolate individual molecular spectral signatures from the highly convoluted data.Machine learning excels in handling complex, high-dimensional data, making it well-suited for these challenging tasks. 198,199In addition, leveraging ML methods to integrate information from multiple spectral inputs could further enhance structure elucidation's accuracy and completeness.</p>
<p>Leveraging scale with foundational models for chemistry</p>
<p>With increasing computational power, machine learning models have been trained on progressively larger datasets.At scale, ML offers qualitatively different capabilities.Foundation models are large-scale models that have been trained on a broad spectrum of data and can be applied to a variety of downstream tasks.Several general-purpose foundation models -such as ChatGPT, Gemini, and Llama -are typically utilized for language and image generation; many of these are language-only models or models trained on multiple modalities.However, using these models in the chemical domain presents unique challenges, and so many have trained their models from scratch on chemical data, but this is not trivial either.In this section, we will describe the current state of foundation models in chemistry and give our perspective on remaining open questions.</p>
<p>2.6.1 Transforming knowledge with large language models and agents.Some of the earliest applications of generative models to chemistry have been via language, which was enabled by the fact that molecules can be represented with strings using SMILES notation. 200Preliminary chemistry language models were trained in an unsupervised manner on SMILES representations, 201,202 which learned dependencies between molecular subfragments.4][205][206][207] Ramos et al. 208 wrote a comprehensive review detailing 80 chemistry/biochemistry language models to date for further reading.One motivation behind incorporating textual descriptions is that they contain information about functional properties of molecules, which can be useful for improving the embedding representations of molecules that are structurally similar but functionally different, or vice versa.They also enable interaction with models using natural language, which is a more intuitive interface for many users than rigid queries. 209,210Additionally, LLMs have been utilized for scientific bibliographic parsing, [211][212][213] facilitating the extraction of chemical information from existing literature and building knowledge databases.These databases can be used for the fine-tuning of LLMs with the potential to improve the generation and screening capabilities of self-driving labs (Sec.2.7). 209,214,215owever, there still exists a gap in using these models out-of-the-box for discovery tasks or in domain-specific chemistry applications (at least to our knowledge), 206,216 one reason being that there is not enough data to train these models in the same way that models like GPT-4 have been trained on web-scale text and images. 217One way to use these chemistry-aware language models is to finetune them on downstream tasks, 218 or plug them into optimization or search frameworks as a way to provide good prior knowledge [219][220][221][222] .Other works have also begun to explore scaling of both models and data. 223,224ne interesting application of chemistry-aware foundation models has been the development of chemistry agents that can e.g.make use of tools 225 necessary for solving chemistry problems, and/or plan chemistry experiments.Some notable examples include ChemCrow, 226 Coscientist, 227 our own ORGANA, 228 or ChemReasoner. 229These agents have access to various chemistry-related tools, such as simulators or robots to execute chemistry experiments, and use an LLM (such as GPT-4) as a central orchestrator to decide when and how to use these tools to accomplish a user-specified goal.One longer-term goal of such agents is to develop scientific assistants that can help beyond calculating and executing to do more complex reasoning and planning by generating and refining hypotheses on their own.This has been extended to other research domains by the AI Scientist, which demonstrates autonomous machine learning research by executing experiments and writing a research paper. 230hese research areas are in their infancy, so several open questions remain, including: (1) How do we effectively evaluate chemistry-aware LLMs/agents?(2) What are the use cases for these models in practice for chemists?Effective model evaluation mainly depends on developing meaningful tasks, which is currently an open problem both in terms of dataset scale and breadth.There already do exist several benchmarks in this space, 28,231 which is a good start but there is room to improve them in terms of data quality and task objectives. 32More recent benchmarks have been released that are closer to real-world applications, 33,232,233 and also platforms such as Polaris have made it easier for researchers to have faster access to a wide array of datasets. 234The issue with using sub-optimal benchmarks in this field has been exacerbated by the current climate in machine learning in that benchmarks are mainly used to show that a new method achieved better performance than the current state-of-the-art, without human understanding of why it improved.This is also an excellent opportunity for collaboration between chemists and the ML domain expert communities.</p>
<p>Language-based foundation models have also been used in other applications, including knowledge graph generation 235 and knowledge extraction from chemical literature, [236][237][238][239] including our own work on reaction diagram parsing, 240 which is a difficult task.These efforts are essential for creating structured databases of experimental procedures, which can contribute to existing repositories such as the previously-mentioned Open Reaction Database. 986.2Foundational physical models.While language-only foundation models are receiving a lot of attention in chemistry, it has been shown that language might not be the sufficient modality, especially in settings where 3D geometry matters.For example, Alampara et al. 241 showed that language models are not enough to encode structural information needed to represent specific material properties.</p>
<p>However, language models are not the only foundation models developed in the biochemical sciences.3][244][245] Perhaps the most famous example is AlphaFold2 for protein structure prediction 134 and, more recently, AlphaFold3, 140 which given any set of 2D biomolecules, predicts how they might assemble in 3D.To our knowledge, these models still outperform any sequence-based protein prediction models for many structural and functional tasks, especially in cases where input sequences do not have homologues in the training data. 246other impressive example is the recent foundation model MACE-MP-0, built with the MACE equivariant architecture. 109,247MACE-MP-0 was trained on 150 thousand inorganic crystals.After a small number of task-specific examples for fine-tuning, it can be used as a force field in simulations on a wide variety of tasks, even seemingly unrelated ones such as small protein simulations.Notably, intermolecular interactions seem somewhat fuzzy in the MACE-MP-0.For example, in the aforementioned protein simulation, the model was able to capture hydrogen transfer, which is a remarkable achievement.However, the authors also opted to include D3 dispersion borrowed from classical computational chemistry, pointing to the fact that the model still needs some help to predict long-range interactions.Foundational force fields have continued to scale, with industry research labs training neural force fields on ever-larger data, such as GNoME 244 and MatterSim. 245e key takeaway from these types of models is that structural information should not be ignored depending on what downstream tasks the model will be applied to, and that training models on broad, large-scale datasets (i.e., going beyond training a simple model on a single prediction task, which was the norm even a couple years ago) can help generalize better to more downstream settings.We suspect that scaling along multiple modalities concurrently is critical for building the best foundation model in chemistry -namely, training models on as many modalities as possible, such as 3D structure information, text, and spectral information. 31</p>
<p>Limits and open problems.</p>
<p>In the case of the domain sciences, we are not as privileged as in the domain of natural language or images, which already has internet-scale data available.Scientific data is scarce; every data point must be an experiment or a high-quality simulation.If simulations are employed, the model must find a way to translate their results to specific experimental conditions.We suspect that universal models across chemistry are still a decade away and will perhaps be a moving target as humans continue to demand more of them.This is analogous to the problem of widening highways 248 where many analysts have shown that as soon as a road is widened, the additional created demand due to its availability makes the highway full of traffic immediately.</p>
<p>2.7</p>
<p>Closed-loop optimization and self-driving labs 2.7.1 Self-driving laboratories.As ML applications continue to evolve, the necessity and scarcity of high-quality data become increasingly apparent.The advent of chemical digitization 249,250 and advances in ML 4,251 have laid the groundwork for combining ML with automated data generation through robotic experimentation.This synergy has given rise to the concept of the self-driving laboratory (SDL). 6SDLs are primarily composed of two critical components: automated laboratory equipment and experimental planners, both of which leverage ML techniques to enhance their functionality. 6The ultimate goal is to autonomously execute the scientific method, encompassing hypothesis generation (ML), hypothesis testing (experimentation), and hypothesis refinement (ML), potentially allowing for the exploration of vast design spaces in a data-efficient manner.</p>
<p>Significant advancements in automated laboratory equipment have been achieved by integrating ML with computer vision, 252 leading to the concept of "general chemistry robots." 2535][256] Given the inherent challenges in training robotic equipment for active decision-making based on external feedback, a notable innovation in this area is the use of digital twins-virtual replicas of laboratory setups-that provide a robust framework for accelerating the training of robotic ML models. 257These digital twins simulate chemical scenarios with high fidelity, 258 creating a realistic feedback loop that accelerates the model's learning process.</p>
<p>On the experimental planning side, heuristic techniques [259][260][261] are being progressively replaced by ML optimization algorithms.When combined with chemical digitization, 262 these optimization techniques can identify target chemicals and optimize reaction conditions while significantly reducing the number of experimental steps required. 263Among the various ML optimization techniques, 264,265 Bayesian optimization [266][267][268] has gained particular prominence in experimental chemistry due to its success in chemical applications. 269Machine-learning-based surrogate models, which predict the properties of chemicals and reactions, [270][271][272] have been instrumental in this success, with documented examples in both process optimization and materials discovery. 273oreover, the rise of LLMs has further enhanced the auxiliary components of SDLs.LLMs have been effectively used to create human-machine interfaces that bypass traditional coding, 228 enabling more natural communication between chemists and laboratory systems-a significant advantage for users who may not be well-versed in coding or data processing. 228,274,275otor challenges.The primary hardware challenges stem from the humancentric design of chemical instruments and the lack of seamless interconnection between existing automated modules.As a result, most SDLs operate semiautomatically, requiring human intervention for tasks such as sample transfer, maintenance, and troubleshooting.8][279] However, many of these methods rely on traditional algorithms that require static calibration, which is not well-suited to the dynamic nature of SDLs.While computer vision coupled with AI has been proposed as a solution, laboratory equipment, particularly glassware, continues to present significant challenges that are continuously being addressed. 280ognitive challenges.Cognitive challenges primarily arise from the difficulty in developing models that can accurately estimate the chemical output of the system.This limitation restricts the use of more general generative models, effectively reducing the amount of chemical space that experimental planners can explore.When combined with the aforementioned motor challenges, another issue becomes apparent: SDLs often operate in low-data regimes.Predictive and generative machine learning models typically require large datasets to make meaningful predictions.While generative models can be trained on existing data, 219,281 deploying predictive algorithms in such low-data regimes remains a significant challenge.</p>
<p>Auxiliary component challenges.Regarding the auxiliary components of SDLs, the incorporation of LLMs shows promise in automating workflow creation 274 and improving human-machine interfaces.However, further research is needed to ensure the safety and reliability of these processes.Additionally, while integrating bibliographic extraction into SDLs can enhance model development, its effective integration with predictive models remains an unresolved issue.</p>
<p>A final challenge to be addressed in the field of SDLs is the economy of scale of their development.The more SDLs the community builds, the easier it will be to build the next ones.Hence, the democratization of low-cost SDLs is crucial for the advancement of the field. 282</p>
<p>Problems meet methods: a machine learning perspective on solving chemical problems</p>
<p>4][285][286] In this section, we provide a high-level perspective of how ML researchers and communities view and tackle problems.To start, we reclassify the diverse chemical problems introduced above as instances of well-established ML problems.To elaborate the ML perspective, we gather common themes and practices in the ML community and examine them in light of application to chemistry, highlighting points to consider related to benchmarking, the role of domain knowledge, and community values.</p>
<p>The toolbox of machine learning</p>
<p>ML provides a toolbox of algorithms and theory for solving problems using data.ML has formalized a set of well-defined problems to solve diverse tasks in language, vision, audio, video, tabular data, scientific data, and other domains.Each problem establishes a set of input requirements and a desired goal, which has proved helpful for empirically benchmarking and theoretically analyzing different algorithms under a common framework.In Table 1, we lay out significant ML problems with their expected inputs and goals and reclassify different chemical problems as instances of these ML problems.</p>
<p>Regression and classification aim to predict labels y from inputs x, given a dataset of paired data.Labels can be one-dimensional, such as in predicting properties, energy, or yield, but also high-dimensional, such as the ML regression problems related to force fields, spectra prediction, and segmentation.When data is small and tabular, gradient boosting machines such as XGBoost 287 often perform well.Gaussian processes also work with small data and provide good uncertainties for use in Bayesian optimization. 288However, deep neural networks are the algorithm of choice for high-dimensional, complex data like images, text, and molecules.The choice of neural network architecture is informed by the problem's constraints: graph neural networks for 2D graphs and equivariant architectures for 3D data.Relatively recently, transformers 289,290 have revolutionized modelling of language, 289 images, 291 graphs, 292 and 3D molecules. 134,243enerative modelling aims to draw samples x from a distribution p(x) defined by a dataset {x}.Unconditional generative modelling tries to match the data distribution.Conditional generative modelling takes a label or prompt y and tries to learn the conditional distribution p(x|y), blurring the line between unsupervised and supervised learning.While unconditional generative modelling is rarely valuable for chemistry, conditional generative modelling is ideally suited to inverse problems or one-to-many problems.This is the case for conformer search (one 2D structure for many 3D conformers), structure elucidation (one signal could be consistent with multiple molecules), or forward synthesis prediction (given reactants, many products might be possible).Generative models are a natural fit for their ability to produce multiple quality answers to a question.On the other hand, regression will average over all the possible answers, which may not be a quality answer itself.Whereas AlphaFold2 134 used regression to predict one 3D structure given one sequence, AlphaFold3 140 used diffusion models to predict multiple biomolecular assemblies for the same input structures.While many generative model classes exist, such as variational autoencoders, 293 generative adversarial networks, 294 and normalizing flows, 295 the dominant ones today are autoregressive models for language 296 and diffusion/flow matching models for perceptual data like images. 297In chemistry, this translates to chemical language models of SMILES 224 and diffusion models of 3D molecular structure. 140Both approaches rely on gradual generation via iterative prediction by a neural network, usually a transformer.Because an unconditional generative model learns to reproduce a data distribution, which may be a large amount of plentiful unlabeled data, training a generative model can also be thought of as compressing all this data into the network's weights, imbuing a notion of understanding.Tasks such as sampling and agent behaviour can then build on this understanding.</p>
<p>Sampling also aims to draw samples from a distribution but is distinguished from generative modelling because it only permits access to an energy function E(x), which defines an unnormalized probability density p(x) ∝ e −E(x) .No dataset is provided, so one cannot simply train a generative model.Furthermore, generating a dataset in the first place would require drawing samples.In addition, the energy function is often computationally costly to evaluate.For these reasons, sampling problems are among the most difficult in ML and computational chemistry.Numerous sampling algorithms exist in the literature, with many originating from statistical mechanics, such as Markov chain Monte Carlo (MCMC) 298 and Langevin dynamics. 299These traditional methods are beginning to incorporate ideas from modern machine learning, such as drawing inspiration from diffusion models for MCMC, 300 or incorporating learnable components into sequential Monte Carlo. 301Some methods learn a bias potential to do transition path sampling, 154 while other methods turn diffusion models into samplers which can solve combinatorial optimization problems. 302Sampling methods are key to solving equilibrium sampling problems, which are necessary for predicting the thermodynamics and kinetics of many chemical processes.Generative models can be used as components of sampling algorithms, 303 such as in Boltzmann generators, 141,144 which train both by energy and by example.Boltzmann generators have also begun to leverage generative models, transferring learning between different examples. 143Generative Flow Networks 304 (GFlowNets) solve this sampling problem by learning to distribute flow in a generative graph, with a unique strength for generating diverse, discrete data.[307] Gradient-based optimization seeks to optimize a smooth loss function L with respect to parameters θ, which is used to train the neural networks used to solve nearly all of the other ML problems.To do so, machine learning has developed a suite of optimization algorithms such as (stochastic) gradient descent, Adam, 308 and second-order methods such as K-FAC 309 which use secondderivative information.Machine learning frameworks such as PyTorch, 310 JAX, 311 and Tensorflow 312 have implemented automatic differentiation with GPU acceleration, making it easier to optimize neural networks.The fact that neural networks can be optimized so well has motivated the use of neural networks as ansätze for finding wavefunctions to satisfy the Schrödinger equation. 113This approach, in turn, is an instance of a physics-informed neural network (PINN), 313 which seeks neural network solutions to PDEs by using the PDE itself as a loss function.Automatic differentiation also enables propagating derivatives through simulation, which can learn potentials for pairwise interaction, 314 bias potentials for transition path sampling, 153 and perform inverse design. 315lack-box optimization methods try to optimize an oracle function f (x) in a derivative-free manner with as few oracle calls as possible.This is the case in many experimental problems such as optimizing reaction parameters for yield, 269 device processing parameters for performance, 316 or liquid handling parameters. 317To solve these problems with high sample efficiency, algorithms like Bayesian optimization and bandit optimization are applied.When sample efficiency is not a concern, families of algorithms such as reinforcement learning and metaheuristic optimization like genetic algorithms can also be applied. 318lack-box optimization can also be treated as an instance of sampling, where the target distribution is concentrated around the global optimum.</p>
<p>Agents solve complex multistep problems within an environment.An environment defines possible states s, actions a, transitions between states, and a reward function R(s).For example, retrosynthesis planning 75 has molecules as states, chemical reactions as actions, and yield and cost as reward functions.Planning problems such as retrosynthesis planning or robotic motion planning 319 are naturally solved by agent behaviour, and standard algorithms to learn optimal agent behaviour are known as reinforcement learning.Because reinforcement learning has poor sample efficiency, a common approach is to initialize agents from generative models: Helpful assistants such as ChatGPT were initialized as large language models pretrained on internet-scale text, followed by finetuning to maximize a reward of satisfying human preferences. 320Prompting frameworks are a rapidly emerging set of methods for augmenting these agents' capabilities, allowing them to reason step-by-step, 321 use tools, 225 retrieve information, 322 and execute code, 323 and to continually repeat these steps. 3241.1The benefits of a toolbox.A shared problem interface enables clear and broad benchmarking of many different algorithms.One example can be seen in Table 1 of Song et al., 325 who propose a new class of generative models and extensively compares their method to 27 different generative models of different classes on the same dataset and benchmark.</p>
<p>Each of these ML problems also has its own theoretical foundations.Mathematical theory can analyze algorithms for proofs of convergence or properties when converged, providing explanations of why certain methods work better than others.The shared problem interface also allows analysis to determine when one method is the same as another or which methods are more general than others, which helps unify a diverse literature.</p>
<p>3.1.2Tools can be stacked on top of each other.ML problems are also intertwined with each other.Generative models, like diffusion models, use neural networks trained to regress denoising steps.Agents are built on top of generative text models, while the core of the generative model itself is a neural network predicting the next token.All these networks are trained using stochastic optimization methods like Adam, while black-box optimization is used to choose network hyperparameters.Sampling algorithms, black-box optimization, and agents can also incorporate generative models trained on previous data, improving the data generation quality.</p>
<p>The problems enumerated in Table 1 are not an exhaustive list.Other problems include uncertainty quantification, which is helpful in Bayesian optimization 326 and active learning, 327 federated learning for combining industrial pharmaceutical data while preserving privacy, 328 representation learning for generally applicable molecular descriptors, 329 causal learning, retrieval, and compression.</p>
<p>3.1.3</p>
<p>Picking the right tool for the job.While the tools of ML are powerful, they provide the most mileage when used for the right job.For example, as mentioned previously, generative modelling is more naturally suited for oneto-many problems such as 3D structure prediction.Gradient-based optimization is applicable when the loss function is differentiable and fast to evaluate, such as for optimizing neural networks, but not necessarily for optimizing molecular structure.While molecular design is often viewed as a black-box optimization problem, it can be argued that sampling is the proper framework for molecular design: Discovery as a multiobjective problem seeks many diverse but quality hits, whereas black-box optimization tends to locally focus on the best solution seen so far. 330Molecular design cannot be solved by generative modelling alone because generative models learn the distribution of a given dataset.In contrast, molecular design seeks exceptional candidates outside the known data distribution.</p>
<p>In chemistry, there is a tendency to treat problems as a search, like finding a needle in a haystack.Traditional docking approaches search for all feasible ligand positions, while crystal structure prediction exhaustively searches for all atom arrangements.Molecular design by virtual screening assumes there will be sufficiently good needles in a haystack of large virtual libraries.A searchbased perspective is useful when available resources are sufficient to exhaustively model a space, which may be necessary to show that no good solutions exist.However, for many applications, an exhaustive search is overkill.Imagine trying to write an essay by searching over the space of all possible English texts.A helpful exercise is to ask whether a search problem has the data and algorithms available to be reframed as a generative modelling or sampling problem.</p>
<p>Themes and practices in the ML community</p>
<p>Solving chemical problems can be aided by both high-level perspectives and community practices.To contextualize ML perspectives on algorithm development, we describe common themes and practices in the ML community, such as benchmarking, extreme interdisciplinarity, and the bitter lesson of deep learning.All of these are expanded below.</p>
<p>3.2.1</p>
<p>The role of benchmarking.Benchmarking plays a crucial role in the ML development process, driving the continuous improvement of models and methods.The ML community highly values methods that improve on the state of the art.With at least three major computer science conferences annually (NeurIPS, ICML, and ICLR), incremental advances are frequent.These minor, iterative improvements on established benchmarks often accumulate to gain significant performance gains over time.For researchers, benchmarks provide a clear metric for assessing which components of a model most affect performance, enabling more focused and impactful developments.</p>
<p>A prominent feature of ML research is the use of leaderboards, where proposed methods are ranked based on their performance against established benchmarks.Papers must either advance or be competitive with the state of the art to be accepted at major conferences.This process has driven notable progress in various domains, from image classification 331 and machine translation 332 to image generation, 333 and even solving Olympiad math problems. 334Leveraging this mechanism, the Open Catalyst Project 108,335,336 set a benchmark for neural network potentials to relax organic adsorbates on metal surfaces.This project provided a dataset much larger than encountered before, which motivated the continual development of more powerful equivariant architectures.From 2020 to 2023, the success rate of predicting adsorption energy grew from 1% to 14%, with current models now becoming useful in predicting adsorption. 337,338Another benchmark called Matbench Discovery 339 has initiated an arms race of neural force fields on the industry level.</p>
<p>However, while benchmarking is a powerful tool, it is essential to be critical of its applicability to chemistry.Domain experts are uniquely positioned to define practical benchmarks that can translate to real-world outcomes in the lab. 33,55Too often, ML literature presents problem settings that, while optimized for computational performance, may be unrealistic for experimental validation.This misalignment can lead to a scenario where the focus shifts from solving the actual problem to merely advancing ML techniques.As methods mature and benchmarks become saturated, new, more relevant benchmarks must arise.</p>
<p>Ultimately, defining and framing problems for ML researchers is a critical task.It involves proposing important questions and calls to action in a way that is accessible to the broader ML community.By doing so, chemists can guide the development of ML tools more likely to have practical applications in experimental research.While creating datasets and benchmarks can be seen as rote work, it can spur progress on difficult problems by leveraging community efforts of the ML community.Suppose a chemical problem can be crystallized and packaged into a clearly and appropriately benchmarked ML problem.Chemists can now wonder: What new problems now become possible to solve, if these old tasks can be solved with significantly greater speed or accuracy?There are many more scientific questions in the vast set of exciting areas to work in chemistry and materials.</p>
<dl>
<dt>Interdisciplinary</dt>
<dd>
<p>the effect of chemistry on ML.Whereas benchmarking iterative improvements is a mainstay of methods-driven ML in the computer science community, an alternative approach to innovation leverages the extreme interdisciplinarity of the ML community.ML has been applied in fields as diverse as health, agriculture, climate, conservation, physics, and astronomy.We recently suggested application-driven ML 340 as an emerging paradigm that evaluates success based on real-world tasks in diverse areas, with methods and evaluations informed and contextualized by domain knowledge.Applicationdriven innovation acknowledges the impact of incorporating tasks from these diverse areas on the development of machine learning.New tasks motivate new algorithms.</p>
</dd>
</dl>
<p>For chemistry, the development of graph neural networks was driven by the need to model molecular graphs. 23,341This led to practical advances in modelling other graph data like social networks, citation networks, computer programs, and databases.Graph machine learning in turn made theoretical advances, particularly in analyzing the expressivity of GNNs through the Weisfeiler-Lehman test. 342,343In addition, the need for neural networks to respect rotational symmetries of 3D space motivated the development of equivariant architectures. 344ll these methodological developments in respecting symmetries have been unified with a theory of geometric deep learning, 345 which shows how convolutional neural networks, graph neural networks, and transformers are actually tightly related.</p>
<p>Beyond theory and methods, ML researchers are also excited for the potential of ML to help tackle real-world problems like global health and climate change.This has manifested as a great eagerness to learn, as evidenced by the proliferation of blog posts, 346 teaching material, 286 and online reading group communities with recorded talks. 347Several workshops which focus on ML applications to chemistry are offered at main ML conferences such as NeurIPS, [348][349][350] ICML, 351,352 and ICLR. 353,354This wide availability of resources also reflects the value of openness in the ML community.Conference papers are published freely, preprints are emphasized, and sharing code is expected.Conferences even have a track for accepting blog posts. 355hen speaking to ML researchers, be patient with their initial assumptions.Often, several assumptions are made in the ML literature, which ultimately pan out to lose applicability when applied to actual experiments.This occurs in molecular design neglecting the synthesizability of molecules, 58 or in reaction prediction neglecting the reaction conditions. 356This reflects the different values and assumptions reviewers make in a distinct field.It is easy to view this and dismiss those approaches as naïve, and it is good to make these criticisms.But let us not throw the baby out with the bathwater: We should ask, if these additional assumptions were taken care of, could this approach help solve our problem?As ML practitioners come from different backgrounds, they will not immediately understand jargon assumptions and experimental setups in chemistry.But they are eager to learn.</p>
<p>3.2.3</p>
<p>The bitter lesson: Balancing scalability with domain knowledge.The advent of AlexNet 357 marked the beginning of the deep learning revolution, showcasing how neural networks, when trained using the computational power of GPUs, could classify images with much better accuracy than models based on hand-designed features.The power of computational scale was made explicit with the observation of neural scaling laws, 358 which empirically but reliably predict how model performance improves as compute, data, and parameter counts increase.These scaling laws motivated the GPT series of language models, 217,296,359 which ultimately led to advanced applications like ChatGPT.</p>
<p>In light of scaling laws, we should be careful when imposing our domain knowledge when designing algorithms.The "bitter lesson" in ML cautions against relying too heavily on domain knowledge when designing algorithms. 360While hand-crafted, domain-specific design choices can offer short-term improvements, approaches that better leverage computational scale often outperform them in the long run.Across domains like text, images, speech, chess, and Go, approaches which rely on human intuition and inductive bias have been replaced by "bruteforce" approaches that can take advantage of exponential increases in computing power provided by Moore's law.</p>
<p>As chemists, it is joyful to develop methods that are informed by our chemical knowledge, such as by injecting quantum chemistry descriptors into regression, 361 or by imposing physical constraints on the system.However, we should remind ourselves that our human understanding of a problem does not directly translate into being able to design algorithms that solve this problem.Despite extensive knowledge of linguistics in ML research, models like ChatGPT were not realized until researchers trained on massive datasets.</p>
<p>The power of scale can be fearful.Even beloved assumptions like enforcing equivariance in neural networks have been challenged by recent work: Methods like probabilistic symmetrization 362 and stochastic frame averaging 363 have shown that imposing architectural constraints is not strictly necessary, while models like AlphaFold3 140 and Molecular Conformer Fields 127 have demonstrated that shown that models trained with randomly rotated training examples can automatically learn rotation equivariance, but at the cost of higher computation and longer training time.</p>
<p>At the same time, the present-day has limited scale and data.For example, expert systems with reaction rules are still the most effective approach for synthesis planning today, 90 perhaps owing to the difficulty of gathering reaction data.In addition, one can discard even more inductive bias and train language models to generate 3D molecular structure directly as .xyzfiles, as we did recently, 364 and it can compare favourably with more hand-tailored methods for crystal structure prediction. 365Yet, as Alampara et al. 241 showed, current language models cannot encode geometric information needed to represent specific material properties.</p>
<p>Therefore, the bitter lesson does not mean that imposing inductive bias on algorithms is never good.An optimal balance must be chosen between leveraging computational power and domain expertise.This is especially critical in chemistry: Unlike domains like language and images, which are available at internetscale, chemical data is scarce and costs real-world experiments to obtain.It is crucial to design algorithms which use this limited data most efficiently.Hand-designed algorithms can enable better predictions and faster simulations in the near-term, which can bootstrap data generation towards ultimately reaching the scale of data required for foundation models.</p>
<p>Another critical role of domain knowledge is determining the appropriate concept of a problem.Should we model it from first principles, like physicsbased simulations, or treat it as a cheminformatics problem?How does this problem fit into the broader context of the world?For example, predicting a drug's effect on a patient could be approached by simulating the entire person, which is currently impractical, or by modelling the effects statistically or causally.At some point, these different levels of models need to align, and domain scientists are crucial in mapping out this structured hierarchy of models.They help determine when assumptions are reasonable and when they are not.While ML tools cannot solve these problems independently, they can significantly aid in integrating different model components.</p>
<p>How to tackle scientific problems?</p>
<p>Armed with the above toolbox and perspectives, we then make recommendations on how to choose impactful problems in ML for chemistry and introduce a highlevel structure of how ML problems are tackled.We finally outline three areas for growth for research in ML for chemistry: breadth, depth, and scale.</p>
<p>The Aspuru-Guzik/Whitesides rules for selecting important problems</p>
<p>When one of us (Aspuru-Guzik) started the Matter Lab then at Harvard University (2006-2018) and now at the University of Toronto (2018-), a set of rules for selecting significant problems began to emerge from intuition.In a hallway conversation with George Whitesides, who told Aspuru-Guzik he had similar guidelines, the three questions to ask before starting any research crystallized.We apply them at the Matter Lab daily to select problems.Here, we specialize in ML in Chemistry, but these are widely applicable.The three questions emphasize novelty, importance, and feasibility in that order.4.1.1Question 1: Has this problem been solved before?Before starting a scientific endeavour, ask yourself this question.Of course, if it has not been solved before, your solution will be more impactful and lasting.Aim to be first and not best.</p>
<p>In the context of ML, improving on benchmarks, despite providing valuable signals of progress, is not the end goal of research.This is particularly true in academic work, where research is not directly linked to profits and should be as novel as possible.Once new problems are established, the field will be opened to improve the results afterwards.</p>
<p>Will this work create a new connection between two areas?When a paper introduces more questions than answers, the field grows.Simply applying an ML method to a new field can be novel.But novelty can be maximized if the proposed approach offers a new perspective, such as reframing a search problem as a generative modelling problem.</p>
<p>For example, we introduced 3D generative modelling to the field of rotational spectroscopy, 193 which has opened the question of 3D structure elucidation from rotational spectroscopy alone.This is a clear example where first beats any other research.There were no previous ML baselines to compare or benchmark our method to, because we introduced the first approach in the field!4.1.2Question 2: Is what you set out to solve relevant to society?Before starting a scientific quest, consider whether it will help others widely.We, after all, operate in a domain of science that directly impacts human life.Humans and the entire biome interact with human-made chemicals every day.Think of problems that matter to the planet.Arguably, in the twenty-first century, which is riddled with environmental and political crises, this is quite relevant. 366hich audience will care?What new tasks become within reach if this task is solved with significantly greater accuracy or speed?For example, neural network potentials are significant because force fields are used in a large number of computational chemistry methods, which in turn predict properties and spectra.Solving this problem, therefore, touches a large audience.</p>
<p>Can the proposed method be tested experimentally if it solves a computational problem?Approaches that can be experimentally validated have a much higher impact ceiling. 40,137On the other hand, what is the worst-case scenario if the proposed approach "doesn't work"?If novelty is chosen carefully, this risk is mitigated because a method which solves an unbenchmarked problem is already state-of-the-art.</p>
<p>4.1.3Question 3: Is it remotely possible to attack this problem?Tackling something that is powerful, yet within the reach of your resources is key to success.The most effective and general publications will obviously have more impact.Therefore, aim for difficult and not low-hanging fruit work if what you wish is for your work to be remembered.</p>
<p>In the context of ML, it would be useful to consider the following questions: What are the available resources?Is enough data available for the desired generalization performance?Are there public code implementations?Have similar problems been solved using the same framing?For example, the success of 3D generative models in structure prediction on tasks such as conformer search and docking indicated that they can likely be successful in crystal structure prediction as well.</p>
<p>A crucial part of feasibility is controlling scope.What is the minimal implementation of an algorithm that can solve this problem, yet have a broad impact?How can success be evaluated within this problem scope?</p>
<p>The structure of data science and ML problems</p>
<p>Machine learning and many data science problems have a general structure, as seen in many papers.Once you begin on a chosen problem, the next considerations follow this hierarchy: (1) data, (2) problem framing, (3) method, and (4) evaluation.In our research group, we always think of these in order and in ranking.For example, without data a scientist will not be able to make progress.A publication that suggests a new method for old data will be less impactful than the publication that provided the data (and its ML application) in the first place.</p>
<p>What data are available?</p>
<p>In machine learning, everything begins from the available data.No method can be applied without it.What is the size of the available data?How easy is it to simulate new data?What ground truth data are available, and what methods are available for validating a model's predictions?Anecdotally, when a dataset exceeds around 10,000 examples, generative models are more likely to generalize effectively.Problems that are repeatedly solved in the community should be considered.Can these data be routinely recorded?For instance, tasks like computing forces and conformer searches are standard in quantum chemistry, and the availability of this data has contributed to the success of neural force fields and 3D structure prediction.Additionally, data might not just be a static dataset but could include on-the-fly data acquisition, such as environments for agents or oracle functions for black-box optimization.It is because data is the ultimate resource that our group embarked on the multiyear goal of developing and employing self-driving labs.We can eat our own dog food.</p>
<p>What is a useful framing of the problem?</p>
<p>The next critical task is to frame the problem usefully.Framing is important not only to ensure selection of the right tools in Table 1, but also allows for benchmarking and theoretical analysis.Problem framing should be informed by domain knowledge: What specific challenges must be addressed to enable downstream tasks, such as experimental validation?For example, performing materials design by generating crystal structures as 3D unit cells may be difficult to translate into real materials, since experimentalists do not have atomistic control of structure.Framing by itself can often determine the novelty and significance of the proposed research: Creating a new connection between a chemical problem and a ML problem generates novelty, and the potential step-function improvement in performance can improve significance.</p>
<p>Another way to approach problem framing is by asking how the data will be represented.Choosing a compact, information-rich, efficient-to-compute representation is a simple way to incorporate inductive bias and accelerate learning.However, as the bitter lesson shows, it is not essential to spend too much time on designing the "perfect" representation.Deep learning can automatically find ideal representations if the input representation contains all the necessary information and is available in large enough quantities.</p>
<p>What model solves this problem?</p>
<p>Once the problem is framed, the choice of model often becomes apparent and justified.What ML methods perform well for this task?Can simple methods solve this problem?Established methods, such as Morgan fingerprints and XGBoost, remain strong baselines for property prediction, 288 while genetic algorithms are strong baselines for molecular generation. 318If simple methods fail, are there new classes of algorithms suited for this problem?Is there existing code available online?It may be easier to first run the code before trying to understand the code.How can a code implementation for solving another problem be modified as minimally as possible to solve the problem at hand? Choose algorithms commensurate with the size and availability of data.With small datasets, classical machine learning still performs best.This is perhaps the most critical paragraph of this publication: Golden advice to graduate students and postdocs, do not fall in love with the mermaids of new methodology.If older but proven methodology does the job, just use it!Focus on the scientific contributions of your work.New methods should be developed when others truly have limitations.In other words, your new fancy super-duper autoencoder will not be as impactful in the long term as if you solve an essential chemistry or materials science question with an answer that lasts for ages.4.2.4How will the proposed method be evaluated?Finally, the method must be evaluated according to reasonable metrics as informed by domain knowledge.Do the metrics reflect the practical realities of downstream use cases of the proposed method?For example, if you are generating and proposing new molecules, is it feasible for a chemist to synthesize them and test their properties?Deciding appropriate metrics is vital because future work will likely adopt the same evaluation criteria.</p>
<p>New problems: demanding impact from ML for chemistry</p>
<p>Applying ML to chemistry can have a greater impact in terms of breadth of application, depth of consideration, and scale of execution.In breadth, many more chemical problems can be formulated as ML problems and introduced to the ML community.In depth, proposed methods can make stronger theoretical connections between both machine learning and computational chemistry, motivating further method development in each field.Finally, at scale, ML for chemistry can aim at more significant problems requiring more data.As concerns mount about reaching the limits of internet-scale data in language and vision, chemistry stands out as a situation where more data can be "purchased" through computational simulation or high-throughput experimentation.</p>
<p>Solving problems in breadth.</p>
<p>While in Sec 2 we have witnessed the diversity of chemical problems that ML has been applied to, many areas of chemistry remain underexplored.In no particular order, we list a number of chemistry fields in which ML is still emerging: photochemistry, 367,368 chemical education, 369 nuclear chemistry, 370 agrochemistry, 371 analytical chemistry, 372 electrochemistry, 373 astrochemistry, 374 amorphous materials, 375 soft materials, 376 open quantum systems, 377 environmental chemistry, 378 and atmospheric chemistry, 379 just to cite a few.Within each field lie a number of tasks that could be formulated as ML problems, depending on the data available.Tasks can also go beyond the idealization of pure, small organic molecules.Heterogeneous materials, quantum materials, and complex mixtures present challenges that could particularly benefit from ML innovations.As mentioned in Sec.2.5, most substances in real-world situations are complex mixtures.</p>
<p>The key is not to "force" ML into these areas but to consider whether existing or novel tasks could be framed as ML problems listed in Table 1, facilitating iterative improvements and potentially leading to new algorithms.In some situations, there is just not enough data to apply ML, but it remains that a simple way to guarantee novelty is to consider an underexplored field.</p>
<p>Coming back to our previous example, we are pretty happy to have applied ML to solve an essential structural determination in rotational spectroscopy: the first application of generative models to predict the 3D structure of molecules given their substitution coordinates. 193This is an example of a typical in-breadth approach seeking multidisciplinary approaches and leaving our own comfort zone.Many ML methods such as graph neural networks and equivariant architectures were motivated or inspired by theoretical chemistry, and they are beginning to return to the favor.2][383] Nearly a decade later, new works have connected diffusion models to traditional tools in computational chemistry.Diffusion models can simultaneously learn both coarse-grained force fields and a generative model, 149 and can also be leveraged as means for sampling and computing free energies. 384These works would not have been possible without deeper consideration of how diffusion models relate to free energy, or of the connection between diffused distributions and the ideal gas.</p>
<p>Furthermore, flow matching approaches derived from diffusion models relax the constraint of noising a data distribution to a pure Gaussian distribution and can instead connect two different distributions.This has enabled learning of trajectories, 117,385 which is beginning to be applied for transition path sampling of reactions. 386These works create theoretical connections that may enable more techniques to transfer from computational chemistry to machine learning and vice versa.</p>
<p>In addition, whereas neural network potentials treat energy computation as a black-box function to be memorized, Hamiltonian prediction 111 opens the box of Hartree-Fock theory, enabling access to the wavefunction, as well as a new tradeoff between accuracy and speed.Self-consistency training 112 engages with this theory by removing the requirement of providing Hamiltonian matrices as labels, which has improved the speed of DFT overall.</p>
<p>Aiming for a concrete design goal in collaboration with experimentalists also provides much-needed depth.Real-world problems often require the integration of ML with experimental data, and such collaborations can lead to breakthroughs that would not be possible in isolation.Large-scale collaborations between experts in quantum chemistry, machine learning, and organic materials chemistry enabled the discovery of new OLEDs. 43In that work, we were among the first to demonstrate that fingerprint-based ML methods, intelligent screening methodologies, and experimental verification could lead to novel materials in a closedloop philosophy.</p>
<p>Our group, more recently, spent five years in an international collaboration involving six research groups, which led to a delocalized, asynchronous closedloop design that led to the best organic laser material to date (to our knowledge). 273In parallel, another multidisciplinary collaboration on closed-loop design 387 demonstrated that ML can teach us new chemical principles from these in-depth materials science explorations.3), provides optimism for solving much more difficult problems.Notorious problems like protein structure prediction were finally cracked by leveraging the scale of the Protein Data Bank. 134,388ast and quantum mechanically accurate atomic dynamics are being enabled by foundation force fields. 109,244,245or chemical problems which are already formalized in ML, progress can be accelerated just by increasing the scale of data and compute of these approaches.Projects like the Open Catalyst Project demonstrate the potential of ML to drive large-scale advancements in chemistry.By purchasing new data through computation and simulation and by designing better sampling algorithms, we can improve the rate of data generation, and take aim at scale.LLM agents, for example, could execute computational simulations to generate new training data, further accelerating research.</p>
<p>While training foundation models is often cited as a source of significant emissions, we should also be aware of the potential for compute to reduce emissions. 389Better models could reduce the number of wet-lab experiments needed, or help design greener alternatives to current and future chemical processes, observing that the chemical industry makes up a large chunk of global emissions.</p>
<p>Chemical space may be small.The often-cited estimated size of chemical space as 10 60 fascinates us.But from a machine learning perspective, this space may be considered small.If we only consider black-and-white 28x28 images, the domain of the standard MNIST dataset of handwritten digits, 390 this already has a size of 2 28×28 ≈ 10 236 .Of course, the space of images is far sparser, given that the number of colour images in existence is 14.3 trillion ≈ 10 13 images. 391his is what makes deep learning impressive -its ability to find structure within enormously high-dimensional spaces, just from showing a bunch of examples.In the context of language, 10 60 is just the number of 10-word sentences restricted to a vocabulary of 60 words, or the number of 10-sentence paragraphs restricted to 60 possible sentences.Natural language is evidently much larger.</p>
<p>Could these powerful capabilities be enough to turn theoretical musings into reality?Imagine being able to atomistically simulate a cell on a macroscopic timescale, or to accurately model the effectiveness and stability of soft organic devices over years of use, or to discover new reactions ab initio.These are challenges that, until recently, seemed impossibly far beyond reach.We are impressed that nanosecond simulation of an all-atom HIV capsid at DFT accuracy is possible with neural force fields. 392If modern image generative models can generate high-quality images at 1024x1024 resolution and higher, 393 then what really stands in the way of simulating an entire cell at biological timescales?If it is data, we are fortunate to have access to more and more complex simulations and self-driving labs which can generate high-quality data independently.If the barrier is computing power, we are lucky enough to utilize the massive increases in computing power driven by mainstream AI.If it is methods or experiments, then here is the call for action to all of us, multidisciplinary theoretical chemists of the twenty-first century: Let's transform our discipline together!</p>
<p>Fig. 1 A
1
Fig.1A taxonomy of chemical problems related to machine learning.Each arrow indicates an application of ML and signifies how all these relate to each other.Foundation models and self-driving labs touch all these areas.</p>
<ol>
<li>
<p>7 . 2
72
Limits and open problems.As discussed by us recently,276 the challenges facing SDLs can be broadly categorized into two areas: motor (hardwarerelated) and cognitive (AI-related).</p>
</li>
<li>
<p>3 . 2
32
Solving problems in depth.As we saw when discussing applicationdriven innovation in ML in Sec.3.2.2,chemical problems have motivated new algorithms and advanced ML theory.Deep engagement with ML theory or theoretical chemistry generates novelty and significance and often leads to more robust empirical results.</p>
</li>
<li>
<p>3 . 3
33
Solving problems at scale.The unreasonable effectiveness of scale, as shown by the bitter lesson (Sec.3.2.</p>
</li>
</ol>
<p>Table 1
1A toolbox of machine learningML problemInputGoalChemical problemsAlgorithmsRegression andPaired datapredict ŷ = f (x)• Property prediction• Classical machine learning:classification{(x, y)}• Neural network potentialslinear regression, random forests,• Yield predictionsupport vector machines,• Proxies for fast predictiongradient boosting machines• Spectra prediction• Gaussian processes• Figure segmentation• Neural networks• (3D structure prediction)• Graph neural networks• Equivariant neural networks• TransformersGenerativeDataset {x},draw samples• Conformer search• Variational autoencodersmodellingoptionalx ∼ p(x) or• Docking• Generative adversarial networksconditioning {y}x ∼ p(x|y)• Crystal structure prediction• Normalizing flows• Transition state search• Autoregressive models• Structure elucidation• Denoising diffusion• Forward synthesis predictionand flow matching• (Molecular design)Samplingenergy E(x)draw samples• Equilibrium sampling• Markov chain Monte Carlox ∼ p(x) ∝ e −E(x)• Transition path sampling• Sequential Monte Carlo• Molecular design• GFlowNetsGradient-based optimizationloss L(θ)optimal parameters θ  <em>• Neural wavefunctions • Physics-informed• First-order: (stochastic) gradient descent, Adamneural networks• Second-order: K-FAC• Differentiable simulation• (Molecular design)Black-boxOracle f (x)optimal x  </em>• Reaction and• Bayesian optimizationoptimizationprocess optimization• Bandit optimization• (Molecular design)• Reinforcement learning• Genetic algorithmsAgentsEnvironmentdraw actions• Extracting literature data• LLM prompting frameworksof states {s},from optimal• Executing simulations• Reinforcement learningactions {a},policy a ∼ π  *  (s)• Question answeringtransitions,• Synthesis planningand reward R(s)
-46 <br />
Acknowledgements This research was undertaken thanks in part to funding provided to the University of Toronto's Acceleration Consortium from the Canada First Research Excellence Fund CFREF-2022-00042.A.A.-G.thanks Anders G. Frøseth for his gen-1-46 erous support.A.A.-G also acknowledges the generous support of the Canada 150 Research Chairs program.A.A. gratefully acknowledges King Abdullah University of Science and Technology (KAUST) for the KAUST Ibn Rushd Postdoctoral Fellowship.
. C W Coley, N S Eyke, K F Jensen, Angewandte Chemie International Edition. 592020</p>
<p>. C W Coley, N S Eyke, K F Jensen, Angewandte Chemie International Edition. 592020</p>
<p>. H Wang, T Fu, Y Du, W Gao, K Huang, Z Liu, P Chandak, S Liu, P Van Katwyk, A Deac, Nature. 6202023</p>
<p>. A Aldossary, J A Campos-Gonzalez-Angulo, S Pablo-García, S X Leong, E M Rajaonson, L Thiede, G Tom, A Wang, D Avagliano, A Aspuru-Guzik, Advanced Materials. 24023692024</p>
<p>. F Strieth-Kalthoff, F Sandfort, M H Segler, F Glorius, Chemical Society. 492020</p>
<p>. G Tom, S P Schmid, S G Baird, Y Cao, K Darvish, H Hao, S Lo, S Pablo-García, E M Rajaonson, M Skreta, Chemical Reviews. 2024</p>
<p>. L C Ray, R A Kirsch, Science. 1261957</p>
<p>H Kubinyi, Quantitative Structure-Activity Relationships. 200221</p>
<p>. C Hansch, P P Maloney, T Fujita, R M Muir, Nature. 1941962</p>
<p>. C Hansch, T Fujita, Journal of the American Chemical Society. 861964</p>
<p>. S M Free, J W Wilson, Journal of medicinal chemistry. 71964</p>
<p>. R D Cramer, Iii , G Redl, C E Berkoff, Journal of Medicinal Chemistry. 171974</p>
<p>. D Rogers, M Hahn, Journal of chemical information and modeling. 502010</p>
<p>. M Glick, J L Jenkins, J H Nettles, H Hitchings, J W Davies, Journal of chemical information and modeling. 462006</p>
<p>. L P Hammett, Journal of the American Chemical Society. 591937</p>
<p>. T C Bruice, N Kharasch, R J Winzler, Archives of Biochemistry and Biophysics. 621956</p>
<p>Correlation and estimation of vapour-liquid critical properties. D Ambrose, 1978National Physical Library</p>
<p>. Y Nannoolal, J Rarey, D Ramjugernath, W Cordes, Fluid Phase Equilibria. 2262004</p>
<p>. T Gensch, G Dos Passos, P Gomes, E Friederich, T Peters, R Gaudin, K Pollice, A Jorner, M Nigam, M S Lindner-D'addario, Sigman, Journal of the American Chemical Society. 1442022</p>
<p>. C A Tolman, Journal of the American Chemical Society. 921970</p>
<p>. C A Tolman, Journal of the American Chemical Society. 921970</p>
<p>. G Monteiro-De Castro, J C Duarte, I BorgesJr, The Journal of Organic Chemistry. 882023</p>
<p>D Duvenaud, D Maclaurin, J Aguilera-Iparraguirre, R Gómez-Bombarelli, T Hirzel, A Aspuru-Guzik, R P Adams, Proceedings of the 28th International Conference on Neural Information Processing Systems. the 28th International Conference on Neural Information Processing SystemsCambridge, MA, USA20152</p>
<p>. K Yang, K Swanson, W Jin, C Coley, P Eiden, H Gao, A Guzman-Perez, T Hopper, B Kelley, M Mathea, Journal of chemical information and modeling. 592019</p>
<p>. B K Lee, E J Mayhew, B Sanchez-Lengeling, J N Wei, W W Qian, K A Little, M Andres, B B Nguyen, T Moloy, J Yasonik, Science. 3812023</p>
<p>. S Pablo-García, S Morandi, R A Vargas-Hernández, K Jorner, Ž Ivković, N López, A Aspuru-Guzik, Nature Computational Science. 32023</p>
<p>. E Heid, K P Greenman, Y Chung, S.-C Li, D E Graff, F H Vermeire, H Wu, W H Green, C J Mcgill, Journal of Chemical Information and Modeling. 64332023</p>
<p>. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, Chemical science. 92018</p>
<p>H Wang, W Li, X Jin, K Cho, H Ji, J Han, M D Burke, International Conference on Learning Representations. 2022</p>
<p>. Y Wang, J Wang, Z Cao, A Barati, Farimani, Nature Machine Intelligence. 42022</p>
<p>G Zhou, Z Gao, Q Ding, H Zheng, H Xu, Z Wei, L Zhang, G Ke, The Eleventh International Conference on Learning Representations. 2023</p>
<p>We Need Better Benchmarks for Machine Learning in Drug Discovery -practicalcheminformatics. P Walters, 24-08-2024blogspot.com</p>
<p>A Nigam, R Pollice, G Tom, K Jorner, J Willes, L Thiede, A Kundaje, A Aspuru-Guzik, Advances in Neural Information Processing Systems. 202336</p>
<p>. P G Polishchuk, T I Madzhidov, A Varnek, Journal of computer-aided molecular design. 272013</p>
<p>. R S Bohacek, C Mcmartin, W C Guida, Medicinal research reviews. 161996</p>
<p>. W A Warr, Journal of chemical information and computer sciences. 371997</p>
<p>. J Carroll, Biotechnology Healthcare. 2005, 2, 26</p>
<p>. W P Walters, M T Stahl, M A Murcko, Drug discovery today. 31998</p>
<p>. J Hachmann, R Olivares-Amaya, S Atahan-Evrenk, C Amador-Bedolla, R S Sánchez-Carrera, A Gold-Parker, L Vogt, A M Brockway, A Aspuru-Guzik, The Journal of Physical Chemistry Letters. 22011</p>
<p>. C Gorgulla, A Boeszoermenyi, Z.-F Wang, P D Fischer, P W Coote, K M Das, Y S Malets, D S Radchenko, Y S Moroz, D A Scott, Nature. 5802020</p>
<p>. A A Sadybekov, A V Sadybekov, Y Liu, C Iliopoulos-Tsoutsouvas, X.-P Huang, J Pickett, B Houser, N Patel, N K Tran, F Tong, Nature. 6012022</p>
<p>. A V Sadybekov, V Katritch, Nature. 6162023</p>
<p>. R Gómez-Bombarelli, J Aguilera-Iparraguirre, T D Hirzel, D Duvenaud, D Maclaurin, M A Blood-Forsythe, H S Chae, M Einzinger, D.-G Ha, T Wu, Nature materials. 152016</p>
<p>. B Sanchez-Lengeling, A Aspuru-Guzik, Science. 3612018</p>
<p>. A Zunger, Nature Reviews Chemistry. 21212018</p>
<p>. R Gómez-Bombarelli, J N Wei, D Duvenaud, J M Hernández-Lobato, B Sánchez-Lengeling, D Sheberla, J Aguilera-Iparraguirre, T D Hirzel, R P Adams, A Aspuru-Guzik, ACS central science. 42018</p>
<p>. M H Segler, T Kogej, C Tyrchan, M P Waller, ACS central science. 42018</p>
<p>. B Sanchez-Lengeling, C Outeiral, G L Guimaraes, A Aspuru-Guzik, ChemRxiv. 2017</p>
<p>. M Olivecrona, T Blaschke, O Engkvist, H Chen, Journal of cheminformatics. 92017</p>
<p>. T Blaschke, J Arús-Pous, H Chen, C Margreitter, C Tyrchan, O Engkvist, K Papadopoulos, A Patronov, Journal of chemical information and modeling. 602020</p>
<p>. M Krenn, F Häse, A Nigam, P Friederich, A Aspuru-Guzik, Machine Learning: Science and Technology. 2020, 1, 045024</p>
<p>. A H Cheng, A Cai, S Miret, G Malkomes, M Phielipp, A Aspuru-Guzik, Digital Discovery. 22023</p>
<p>. J H Jensen, Chemical science. 102019</p>
<p>K Korovina, S Xu, K Kandasamy, W Neiswanger, B Poczos, J Schneider, E Xing, International Conference on Artificial Intelligence and Statistics. 2020</p>
<p>W Gao, T Fu, J Sun, C Coley, Advances in neural information processing systems. 202235</p>
<p>. N Brown, M Fiscato, M H Segler, A C Vaucher, Journal of chemical information and modeling. 592019</p>
<p>. D Polykovskiy, A Zhebrak, B Sanchez-Lengeling, S Golovanov, O Tatanov, S Belyaev, R Kurbanov, A Artamonov, V Aladinskiy, M Veselov, Frontiers in pharmacology. 115656442020</p>
<p>. W Gao, C W Coley, Journal of chemical information and modeling. 602020</p>
<p>J Bradshaw, B Paige, M J Kusner, M H S Segler, J M Hernández-Lobato, Proceedings of the 33rd International Conference on Neural Information Processing Systems. the 33rd International Conference on Neural Information Processing SystemsRed Hook, NY, USA2019</p>
<p>W Gao, R Mercado, C W Coley, International Conference on Learning Representations. 2022</p>
<p>. M Koziarski, A Rekesh, D Shevchuk, A Van Der Sloot, P Gaiński, Y Bengio, C.-H Liu, M Tyers, R A Batey, arXiv:2406.085062024arXiv preprint</p>
<p>A Pedawi, P Gniewek, C Chang, B Anderson, H Van Den, Bedem, Advances in Neural Information Processing Systems. 202235</p>
<p>. Y Du, A R Jamasb, J Guo, T Fu, C Harris, Y Wang, C Duan, P Liò, P Schwaller, T L Blundell, Nature Machine Intelligence. 2024</p>
<p>. A Zhavoronkov, Y A Ivanenkov, A Aliper, M S Veselov, V A Aladinskiy, A V Aladinskaya, V A Terentiev, D A Polykovskiy, M D Kuznetsov, A Asadulaev, Nature biotechnology. 372019</p>
<p>. J M Stokes, K Yang, K Swanson, W Jin, A Cubillos-Ruiz, N M Donghia, C R Macnair, S French, L A Carfrae, Z Bloom-Ackermann, Cell. 1802020</p>
<p>. E J Corey, Angewandte Chemie International Edition in English. 301991</p>
<p>. E J Corey, W T Wipke, Science. 1661969</p>
<p>. M H Todd, Chem. Soc. Rev. 342005</p>
<p>. J N Wei, D Duvenaud, A Aspuru-Guzik, 20162ACS Central Science</p>
<p>. M H Segler, M P Waller, Chemistry-A European Journal. 232017</p>
<p>. C W Coley, R Barzilay, T S Jaakkola, W H Green, K F Jensen, 20173ACS central science</p>
<p>. C W Coley, W Jin, L Rogers, T F Jamison, T S Jaakkola, W H Green, R Barzilay, K F Jensen, Chemical science. 102019</p>
<p>. B Liu, B Ramsundar, P Kawthekar, J Shi, J Gomes, Q Luu Nguyen, S Ho, J Sloane, P Wender, V Pande, 20173ACS Central Science</p>
<p>. P Schwaller, T Laino, T Gaudin, P Bolgar, C A Hunter, C Bekas, A A Lee, ACS central science. 52019</p>
<p>. M H Segler, M Preuss, M P Waller, Nature. 5552018</p>
<p>. C W Coley, D A Thomas, J A M Lummiss, J N Jaworski, C P Breen, V Schultz, T Hart, J S Fishman, L Rogers, H Gao, R W Hicklin, P P Plehiers, Science. J. Byington, J. S. Piotti, W. H. Green, A. J. Hart, T. F. Jamison and K. F. Jensen15662019</p>
<p>. S Genheden, A Thakkar, V Chadimová, J.-L Reymond, O Engkvist, E Bjerrum, Journal of Cheminformatics. 12702020</p>
<p>. Y Mo, Y Guan, P Verma, J Guo, M E Fortunato, Z Lu, C W Coley, K F Jensen, Chem. Sci. 122021</p>
<p>. D T Ahneman, J G Estrada, S Lin, S D Dreher, A G Doyle, Science. 3602018</p>
<p>. J M Granda, L Donina, V Dragone, D.-L Long, L Cronin, Nature. 5592018</p>
<p>. A F Zahrt, J J Henle, B T Rose, Y Wang, W T Darrow, S E Denmark, Science. 56312019</p>
<p>. F Sandfort, F Strieth-Kalthoff, M Kühnemund, C Beecks, F Glorius, Chem , 20206</p>
<p>P Schwaller, A C Vaucher, T Laino, J.-L Reymond, Machine Learning: Science and Technology. 2021, 2, 015016</p>
<p>. M Christensen, L P Yunker, F Adedeji, F Häse, L M Roch, T Gensch, G Dos Passos, T Gomes, M S Zepel, A Sigman, Aspuru-Guzik, Communications Chemistry. 2021, 4, 112</p>
<p>. W Beker, R Roszak, A Wołos, N H Angello, V Rathore, M D Burke, B A Grzybowski, Journal of the American Chemical Society. 1442022</p>
<p>. F Strieth-Kalthoff, F Sandfort, M Kühnemund, F R Schäfer, H Kuchen, F Glorius, Angewandte Chemie International Edition. e2022046472022</p>
<p>. C J Taylor, A Pomberger, K C Felton, R Grainger, M Barecka, T W Chamberlain, R A Bourne, C N Johnson, A A Lapkin, Chemical Reviews. 1232023</p>
<p>. N H Angello, V Rathore, W Beker, A Wołos, E R Jira, R Roszak, T C Wu, C M Schroeder, A Aspuru-Guzik, B A Grzybowski, M D Burke, Science. 3782022</p>
<p>. J Y Wang, J M Stevens, S K Kariofillis, M.-J Tom, D L Golden, J Li, J E Tabora, M Parasram, B J Shields, D N Primer, B Hao, D Del, S Valle, A Disomma, G G Furman, S Zipp, J Melnikov, A G Paulson, Doyle, Nature. 6262024</p>
<p>. S Szymkuć, E P Gajewska, T Klucznik, K Molga, P Dittwald, M Startek, M Bajczyk, B A Grzybowski, Angewandte Chemie International Edition. 552016</p>
<p>. T Klucznik, B Mikulak-Klucznik, M P Mccormack, H Lima, S Szymkuć, M Bhowmick, K Molga, Y Zhou, L Rickershauser, E P Gajewska, A Toutchkine, P Dittwald, M P Startek, G J Kirkovits, R Roszak, A Adamski, B Sieredzińska, M Mrksich, S L Trice, B A Grzybowski, Chem , 20184</p>
<p>. B Mikulak-Klucznik, P Gołębiowska, A A Bayly, O Popik, T Klucznik, S Szymkuć, E P Gajewska, P Dittwald, O Staszewska-Krajewska, W Beker, T Badowski, K A Scheidt, K Molga, J Mlynarski, M Mrksich, B A Grzybowski, Nature. 5882020</p>
<p>. Y Lin, R Zhang, D Wang, T Cernak, Science. 3792023</p>
<p>. A Wołos, D Koszelewski, R Roszak, S Szymkuć, M Moskal, R Ostaszewski, B T Herrera, J M Maier, G Brezicki, J Samuel, Nature. 6042022</p>
<p>. B Mikulak-Klucznik, T Klucznik, W Beker, M Moskal, B A Grzybowski, Chem , 202410</p>
<p>. F Strieth-Kalthoff, S Szymkuc, K Molga, A Aspuru-Guzik, F Glorius, B A Grzybowski, Journal of the American Chemical Society. 1462024</p>
<p>. D M Lowe, 2012Apollo -University of Cambridge RepositoryPhD thesis</p>
<p>. S M Kearnes, M R Maser, M Wleklinski, A Kast, A G Doyle, S D Dreher, J M Hawkins, K F Jensen, C W Coley, Journal of the American Chemical Society. 1432021</p>
<p>. J E Jones, Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character. 1061924</p>
<p>. A S Christensen, T Kubar, Q Cui, M Elstner, Chemical reviews. 1162016</p>
<p>. M Levitt, A Warshel, Nature. 2531975</p>
<p>. R Iftimie, P Minary, M E Tuckerman, Proceedings of the National Academy of Sciences. 1022005</p>
<p>. J Behler, M Parrinello, Physical review letters. 2007, 98, 146401</p>
<p>K T Schütt, P.-J Kindermans, H E Sauceda, S Chmiela, A Tkatchenko, K.-R Müller, Proceedings of the 31st International Conference on Neural Information Processing Systems. the 31st International Conference on Neural Information Processing SystemsRed Hook, NY, USA2017</p>
<p>. H Wang, L Zhang, J Han, E Weinan, Computer Physics Communications. 2282018</p>
<p>V G Satorras, E Hoogeboom, M Welling, International conference on machine learning. 2021</p>
<p>. A Duval, S V Mathis, C K Joshi, V Schmidt, S Miret, F D Malliaros, T Cohen, P Lio, Y Bengio, M Bronstein, arXiv:2312.075112023arXiv preprint</p>
<p>. C L Zitnick, L Chanussot, A Das, S Goyal, J Heras-Domingo, C Ho, W Hu, T Lavril, A Palizhati, M Riviere, arXivpreprintarXiv:2010.094352020</p>
<p>. I Batatia, P Benner, Y Chiang, A M Elena, D P Kovács, J Riebesell, X R Advincula, M Asta, W J Baldwin, N Bernstein, arXiv:2401.000962023arXiv preprint</p>
<p>. K T Schütt, M Gastegger, A Tkatchenko, K.-R Müller, R J Maurer, Nature communications. 1050242019</p>
<p>O Unke, M Bogojeski, M Gastegger, M Geiger, T Smidt, K.-R Müller, Advances in Neural Information Processing Systems. 202134</p>
<p>H Zhang, C Liu, Z Wang, X Wei, S Liu, N Zheng, B Shao, T.-Y Liu, Forty-first International Conference on Machine Learning. 2024</p>
<p>. D Pfau, J S Spencer, A G Matthews, W M C Foulkes, Physical review research. 2334292020</p>
<p>. J Hermann, Z Schätzle, F Noé, Nature Chemistry. 122020</p>
<p>I Von Glehn, J S Spencer, D Pfau, The Eleventh International Conference on Learning Representations. 2023</p>
<p>. R Li, H Ye, D Jiang, X Wen, C Wang, Z Li, X Li, D He, J Chen, W Ren, Nature Machine Intelligence. 62024</p>
<p>K Neklyudov, J Nys, L Thiede, J F C Alvarez, M Liu, A Welling, Makhzani, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>. D Pfau, S Axelrod, H Sutterud, I Von Glehn, J S Spencer, Science. 1372024</p>
<p>. A Fabrizio, A Grisafi, B Meyer, M Ceriotti, C Corminboeuf, Chemical science. 102019</p>
<p>. S Gong, T Xie, T Zhu, S Wang, E R Fadel, Y Li, J C Grossman, Physical Review B. 1001841032019</p>
<p>. X Fu, A Rosen, K Bystrom, R Wang, A Musaelian, B Kozinsky, T Smidt, T Jaakkola, arXiv:2405.192762024arXiv preprint</p>
<p>. J Kirkpatrick, B Mcmorrow, D H Turban, A L Gaunt, J S Spencer, A G Matthews, A Obika, L Thiry, M Fortunato, D Pfau, Science. 3742021</p>
<p>E Hoogeboom, V G Satorras, C Vignac, M Welling, International conference on machine learning. 2022</p>
<p>M Xu, L Yu, Y Song, C Shi, S Ermon, J Tang, International Conference on Learning Representations. 2022</p>
<p>O Ganea, L Pattanaik, C Coley, R Barzilay, K Jensen, W Green, T Jaakkola, Advances in Neural Information Processing Systems. 202134</p>
<p>B Jing, G Corso, J Chang, R Barzilay, T Jaakkola, Advances in Neural Information Processing Systems. 202235</p>
<p>Y Wang, A A Elhag, N Jaitly, J M Susskind, M Á Bautista, Forty-first International Conference on Machine Learning. 2024</p>
<p>G Corso, H Stärk, B Jing, R Barzilay, T S Jaakkola, The Eleventh International Conference on Learning Representations. 2023</p>
<p>. C J Pickard, R Needs, Journal of Physics: Condensed Matter. 2011, 23, 053201</p>
<p>T Xie, X Fu, O.-E Ganea, R Barzilay, T S Jaakkola, International Conference on Learning Representations. 2022</p>
<p>R Jiao, W Huang, P Lin, J Han, P Chen, Y Lu, Y Liu, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>R Jiao, W Huang, Y Liu, D Zhao, Y Liu, The Twelfth International Conference on Learning Representations. 2024</p>
<p>. C Zeni, R Pinsler, D Zügner, A Fowler, M Horton, X Fu, S Shysheya, J Crabbé, L Sun, J Smith, arXiv:2312.036872023arXiv preprint</p>
<p>. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, nature. 5962021</p>
<p>J Yim, B L Trippe, V De Bortoli, E Mathieu, A Doucet, R Barzilay, T Jaakkola, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>J Bose, T Akhound-Sadegh, G Huguet, K Fatras, J Rector-Brooks, C.-H Liu, A C Nica, M Korablyov, M M Bronstein, A Tong, The Twelfth International Conference on Learning Representations. 2024</p>
<p>. J L Watson, D Juergens, N R Bennett, B L Trippe, J Yim, H E Eisenach, W Ahern, A J Borst, R J Ragotte, L F Milles, Nature. 6202023</p>
<p>. J B Ingraham, M Baranov, Z Costello, K W Barber, W Wang, A Ismail, V Frappier, D M Lord, C Ng-Thow-Hing, E R Van Vlack, Nature. 6232023</p>
<p>. R Krishna, J Wang, W Ahern, P Sturmfels, P Venkatesh, I Kalvet, G R Lee, F S Morey-Burrows, I Anishchenko, I R Humphreys, Science. 25282024</p>
<p>. J Abramson, J Adler, J Dunger, R Evans, T Green, A Pritzel, O Ronneberger, L Willmore, A J Ballard, J Bambrick, Nature. 2024</p>
<p>. F Noé, S Olsson, J Köhler, H Wu, Science. 2019, 365, eaaw1147</p>
<p>L Klein, A Krämer, F Noe, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>. L Klein, F Noé, arXiv:2406.144262024arXiv preprint</p>
<p>. S Zheng, J He, C Liu, Y Shi, Z Lu, W Feng, F Ju, J Wang, J Zhu, Y Min, Nature Machine Intelligence. 2024</p>
<p>. J Wang, S Olsson, C Wehmeyer, A Pérez, N E Charron, G De Fabritiis, F Noé, C Clementi, 20195ACS Central Science</p>
<p>. B E Husic, N E Charron, D Lemm, J Wang, A Pérez, M Majewski, A Krämer, Y Chen, S Olsson, G De Fabritiis, F Noé, C Clementi, The Journal of Chemical Physics. 1532020. 194101</p>
<p>. N E Charron, F Musil, A Guljas, Y Chen, K Bonneau, A S Pasos-Trejo, J. Venturin. </p>
<p>Navigating protein landscapes with a machine-learned transferable coarse-grained model. D Gusew, I Zaporozhets, A Krämer, C Templeton, A Kelkar, A E P Durumeric, S Olsson, A Pérez, M Majewski, B E Husic, A Patel, G D Fabritiis, F Noé, C Clementi, 2023</p>
<p>. J Köhler, Y Chen, A Krämer, C Clementi, F Noé, Journal of Chemical Theory and Computation. 192023</p>
<p>. M Arts, V Garcia Satorras, C.-W Huang, D Zugner, M Federici, C Clementi, F Noé, R Pinsler, R Van Den, Berg, Journal of Chemical Theory and Computation. 192023</p>
<p>X Fu, T Xie, N J Rebello, B Olsen, T S Jaakkola, Transactions on Machine Learning Research. 2023</p>
<p>L Klein, A Foong, T Fjelde, B Mlodozeniec, M Brockschmidt, S Nowozin, F Noe, R Tomioka, Advances in Neural Information Processing Systems. 2023</p>
<p>Thermodynamically Informed Multimodal Learning of High-Dimensional Free Energy Models in Molecular Coarse Graining. B R Duschatko, X Fu, C Owen, Y Xie, A Musaelian, T Jaakkola, B Kozinsky, 2024</p>
<p>M Sipka, J C Dietschreit, L Grajciar, R Gómez-Bombarelli, International Conference on Machine Learning. 2023</p>
<p>L Holdijk, Y Du, F Hooft, P Jaini, B Ensing, M Welling, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>. C Duan, Y Du, H Jia, H J Kulik, Nature Computational Science. 32023</p>
<p>. C Duan, G.-H Liu, Y Du, T Chen, Q Zhao, H Jia, C P Gomes, E A Theodorou, H J Kulik, arXiv:2404.134302024arXiv preprint</p>
<p>X Fu, Z Wu, W Wang, T Xie, S Keten, R Gomez-Bombarelli, T S Jaakkola, Transactions on Machine Learning Research. 2023</p>
<p>. A Young, H Röst, B Wang, Nature Machine Intelligence. 62024</p>
<p>. A Young, F Wang, D Wishart, B Wang, H Röst, R Greiner, arXiv:2404.023602024arXiv preprint</p>
<p>. F M Paruzzo, A Hofstetter, F Musil, S De, M Ceriotti, L Emsley, Nature communications. 945012018</p>
<p>. M Cordova, E A Engel, A Stefaniuk, F Paruzzo, A Hofstetter, M Ceriotti, L Emsley, The Journal of Physical Chemistry C. 1262022</p>
<p>M Lupo Pasini, K Mehta, P Yoo, S Irle, Scientific Data. 202310546</p>
<p>S Goldman, J Bradshaw, J Xin, C Coley, Advances in Neural Information Processing Systems. 2023</p>
<p>M Murphy, S Jegelka, E Fraenkel, T Kind, D Healey, T Butler, International Conference on Machine Learning. 2023</p>
<p>. S Goldman, J Li, C W Coley, Analytical Chemistry. 962024</p>
<p>. R L Zhu, E Jonas, Analytical Chemistry. 952023</p>
<p>. Y Hong, S Li, C J Welch, S Tichy, Y Ye, H Tang, Bioinformatics. 2023, 39, btad354</p>
<p>S A Al, A.-R Allouche, Neural Network Approach for Predicting Infrared Spectra from 3D Molecular Structure. 2024</p>
<p>. Z Zou, Y Zhang, L Liang, M Wei, J Leng, J Jiang, Y Luo, W Hu, Nature Computational Science. 32023</p>
<p>. B Buchanan, G Sutherland, E A Feigenbaum, Organic Chemistry. 301969</p>
<p>. R K Lindsay, B G Buchanan, E A Feigenbaum, J Lederberg, Artificial intelligence. 611993</p>
<p>. C Klawun, C L Wilkins, Journal of Chemical Information and Computer Sciences. 361996</p>
<p>. B Curry, D E Rumelhart, Tetrahedron Computer Methodology. 31990</p>
<p>. C L Wilkins, T L Isenhour, Analytical Chemistry. 471975</p>
<p>. C Li, Y Cong, W Deng, Magnetic Resonance in Chemistry. 602022</p>
<p>. T Specht, J Arweiler, J Stüber, K Münnemann, H Hasse, F Jirasek, Magnetic Resonance in Chemistry. 622024</p>
<p>. Z Huang, M S Chen, C P Woroch, T E Markland, M W Kanan, Chemical Science. 122021</p>
<p>. B Sridharan, S Mehta, Y Pathak, U D Priyakumar, The Journal of Physical Chemistry. 131-46 Letters, 2022</p>
<p>. S Devata, B Sridharan, S Mehta, Y Pathak, S Laghuvarapu, G Varma, U D Priyakumar, Digital Discovery. 32024</p>
<p>. A A Enders, N M North, C M Fensore, J Velez-Alvarez, H C Allen, Analytical Chemistry. 932021</p>
<p>. G Jung, S G Jung, J M Cole, Chem. Sci. 142023</p>
<p>. E X Tan, S X Leong, W A Liew, I Y Phang, J Y Ng, N S Tan, Y H Lee, X Y Ling, Nature Communications. 25822024</p>
<p>. M Alberts, F Zipoli, A C Vaucher, 2023ChemRxiv preprint</p>
<p>. M Alberts, T Laino, A C Vaucher, 2023ChemRxiv preprint</p>
<p>. F Hu, M S Chen, G M Rotskoff, M W Kanan, T E Markland, arXiv:2408.082842024arXiv preprint</p>
<p>. M A Stravs, K Dührkop, S Böcker, N Zamboni, Nature Methods. 192022</p>
<p>. E E Litsa, V Chenthamarakshan, P Das, L E Kavraki, Communications Chemistry. 61322023</p>
<p>. Q Lai, L Yao, Z Gao, S Liu, H Wang, S Lu, D He, L Wang, C Wang, G Ke, arXiv:2401.038622024arXiv preprint</p>
<p>. J E Salgado, S Lerman, Z Du, C Xu, N Abdolrahim, Computational Materials. 92142023</p>
<p>Y Song, L Shen, L Xing, S Ermon, International Conference on Learning Representations. 2022</p>
<p>H Chung, J Kim, M T Mccann, M L Klasky, J C Ye, The Eleventh International Conference on Learning Representations. 2023</p>
<p>. D P Zaleski, K Prozument, The Journal of Chemical Physics. 1492018. 104106</p>
<p>. A H Cheng, A Lo, S Miret, B H Pate, A Aspuru-Guzik, The Journal of Chemical Physics. 1601241152024</p>
<p>. E D Zhong, T Bepler, B Berger, J H Davis, Nature methods. 182021</p>
<p>A Levy, G Wetzstein, J N Martel, F Poitevin, E Zhong, Advances in neural information processing systems. 202235</p>
<p>. R Rangan, R Feathers, S Khavnekar, A Lerer, J D Johnston, R Kelley, M Obr, A Kotecha, E D Zhong, Nature Methods. 2024</p>
<p>. M T Clabbers, J Hattne, M W Martynowycz, T Gonen, bioRxiv. 2024</p>
<p>. S Goldman, J Wohlwend, M Stražar, G Haroush, R J Xavier, C W Coley, Nature Machine Intelligence. 52023</p>
<p>. S F Baygi, D K Barupal, Journal of Cheminformatics. 1682024</p>
<p>. D Weininger, Journal of chemical information and computer sciences. 281988</p>
<p>. J Ross, B Belgodere, V Chenthamarakshan, I Padhi, Y Mroueh, P Das, Nature Machine Intelligence. 42022</p>
<p>. S Chithrananda, G Grand, B Ramsundar, arXiv:2010.098852020arXiv preprint</p>
<p>. S Liu, W Nie, C Wang, J Lu, Z Qiao, L Liu, J Tang, C Xiao, A Anandkumar, Nature Machine Intelligence. 52023</p>
<p>Q Pei, W Zhang, J Zhu, K Wu, K Gao, L Wu, Y Xia, R Yan, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>D Christofidellis, G Giannone, J Born, O Winther, T Laino, M Manica, International Conference on Machine Learning. 2023</p>
<p>. R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, arXiv:2211.090852022arXiv preprint</p>
<p>C Edwards, T Lai, K Ros, G Honke, K Cho, H Ji, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022</p>
<p>. M C Ramos, C J Collison, A D White, arXiv:2407.016032024arXiv preprint</p>
<p>. Y Kang, J Kim, Nature Communications. 47052024</p>
<p>. N Yoshikawa, M Skreta, K Darvish, S Arellano-Rubach, Z Ji, L Bjørn Kristensen, A Z Li, Y Zhao, H Xu, A Kuramshin, Autonomous Robots. 472023</p>
<p>. J Choi, B Lee, Communications Materials. 2024, 5, 13</p>
<p>. T Gupta, M Zaki, N A Krishnan, Mausam , Computational Materials. 81022022</p>
<p>. J Dagdelen, A Dunn, S Lee, N Walker, A S Rosen, G Ceder, K A Persson, A Jain, Nature Communications. 152024. 1418</p>
<p>. M J Buehler, 20244ACS Engineering Au</p>
<p>. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, Nature Machine Intelligence. 62024</p>
<p>. M R Ai4science, M A Quantum, arXiv:2311.073612023arXiv preprint</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023arXiv preprint</p>
<p>. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, Nature Machine Intelligence. 62024</p>
<p>. H Wang, M Skreta, C.-T Ser, W Gao, L Kong, F Streith-Kalthoff, C Duan, Y Zhuang, Y Yu, Y Zhu, arXiv:2406.169762024arXiv preprint</p>
<p>A Kristiadi, F Strieth-Kalthoff, M Skreta, P Poupart, A Aspuru-Guzik, G Pleiss, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024</p>
<p>. M C Ramos, S S Michtavy, M D Porosoff, A D White, arXiv:2304.053412023arXiv preprint</p>
<p>P Ma, T.-H Wang, M Guo, Z Sun, J B Tenenbaum, D Rus, C Gan, W Matusik, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024</p>
<p>. N C Frey, R Soklaski, S Axelrod, S Samsi, R Gomez-Bombarelli, C W Coley, V Gadepally, Nature Machine Intelligence. 52023</p>
<p>. J Ross, B Belgodere, S C Hoffman, V Chenthamarakshan, Y Mroueh, P Das, arXiv:2405.049122024arXiv preprint</p>
<p>T Schick, J Dwivedi-Yu, R Dessi, R Raileanu, M Lomeli, E Hambro, L Zettlemoyer, N Cancedda, T Scialom, Advances in Neural Information Processing Systems. 2023</p>
<p>. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, Nature Machine Intelligence. 2024</p>
<p>. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 6242023</p>
<p>. K Darvish, M Skreta, Y Zhao, N Yoshikawa, S Som, M Bogdanovic, Y Cao, H Hao, H Xu, A Aspuru-Guzik, arXiv:2401.069492024arXiv preprint</p>
<p>H W Sprueill, C Edwards, K Agarwal, M V Olarte, U Sanyal, C Johnston, H Liu, H Ji, S Choudhury, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024</p>
<p>C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. 2024</p>
<p>. K Huang, T Fu, W Gao, Y Zhao, Y Roohani, J Leskovec, C W Coley, C Xiao, J Sun, M Zitnik, Nature chemical biology. 182022</p>
<p>. A Mirza, N Alampara, S Kunchapu, B Emoekabu, A Krishnan, M Wilhelmi, M Okereke, J Eberhardt, A M Elahi, M Greiner, arXiv:2404.014752024arXiv preprint</p>
<p>. J M Laurent, J D Janizek, M Ruzo, M M Hinks, M J Hammerling, S Narayanan, M Ponnapati, A D White, S G Rodriques, arXiv:2407.103622024arXiv preprint</p>
<p>. Polaris -Polarishub, 02-09-2024</p>
<p>V Venugopal, E Olivetti, Scientific Data. 2024, 11, 217</p>
<p>A M Bran, Z Jončev, P Schwaller, Proceedings of the 1st Workshop on Language+ Molecules (L+ M 2024. the 1st Workshop on Language+ Molecules (L+ M 20242024</p>
<p>. Q Ai, F Meng, J Shi, B G Pelkie, C W Coley, Digital Discovery. 2024</p>
<p>. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, Journal of the American Chemical Society. 1452023</p>
<p>. M Schilling-Wilhelmi, M Ríos-García, S Shabih, M V Gil, S Miret, C T Koch, J A Márquez, K M Jablonka, arXiv:2407.168672024arXiv preprint</p>
<p>. S X Leong, S Pablo-García, Z Zhang, A Aspuru-Guzik, 10.26434/chemrxiv-2024-7fwxv2024ChemRxiv preprint</p>
<p>. N Alampara, S Miret, K M Jablonka, arXiv:2406.172952024arXiv preprint</p>
<p>. T T Duignan, ACS Physical Chemistry Au. 42024</p>
<p>Y.-L Liao, B M Wood, A Das, T Smidt, The Twelfth International Conference on Learning Representations. 2024</p>
<p>. A Merchant, S Batzner, S S Schoenholz, M Aykol, G Cheon, E D Cubuk, Nature. 6242023</p>
<p>. H Yang, C Hu, Y Zhou, X Liu, Y Shi, J Li, G Li, Z Chen, S Chen, C Zeni, arXiv:2405.049672024arXiv preprint</p>
<p>. M Van Kempen, S S Kim, C Tumescheit, M Mirdita, J Lee, C L Gilchrist, J Söding, M Steinegger, Nature biotechnology. 422024</p>
<p>I Batatia, D P Kovacs, G Simm, C Ortner, G Csányi, Advances in Neural Information Processing Systems. 202235</p>
<p>E Weingart, A Schukar, The New York Times. 2023</p>
<p>. S Raghunathan, U D Priyakumar, International Journal of Quantum Chemistry. 122e268702022</p>
<p>. D S Wigh, J M Goodman, A A Lapkin, WIREs Computational Molecular Science. 12e16032022</p>
<p>. M Meuwly, Chemical Reviews. 1212021</p>
<p>. Y R Wang, Y Zhao, H Xu, S Eppel, A Aspuru-Guzik, F Shkurti, A Garg, IEEE International Conference on Robotics and Automation (ICRA). 2023. 2023</p>
<p>. B Burger, P M Maffettone, V V Gusev, C M Aitchison, Y Bai, X Wang, X Li, B M Alston, B Li, R Clowes, N Rankin, B Harris, R S Sprick, A I Cooper, Nature. 5832020</p>
<p>Y Nakajima, M Hamaya, Y Suzuki, T Hawai, F V Drigalski, K Tanaka, Y Ushiku, K Ono, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2022. 2022</p>
<p>. M Kennedy, K Schmeckpeper, D Thakur, C Jiang, V Kumar, K Daniilidis, IEEE Robotics and Automation Letters. 42019</p>
<p>. Y Huang, J Wilches, Y Sun, Robotics and Autonomous Systems. 1361036922021</p>
<p>. A Klami, T Damoulas, O Engkvist, P Rinke, S Kaski, techRxiv. 2022</p>
<p>ChemGymRL: An Interactive Framework for Reinforcement Learning for Digital Chemistry. C Beeler, S G Subramanian, K Sprague, N Chatti, C Bellinger, M Shahen, N Paquin, M Baula, A Dawit, Z Yang, X Li, M Crowley, I Tamblyn, 2023</p>
<p>. M A Bezerra, Q O Santos, A G Santos, C G Novaes, S L C Ferreira, V S Souza, Microchemical Journal. 1242016</p>
<p>. W Huyer, A Neumaier, ACM Trans. Math. Softw. 352008</p>
<p>. A Lucia, J Xu, Computers &amp; Chemical Engineering. 141990</p>
<p>. W P Walters, R Barzilay, Accounts of Chemical Research. 542020</p>
<p>. C J Taylor, A Pomberger, K C Felton, R Grainger, M Barecka, T W Chamberlain, R A Bourne, C N Johnson, A A Lapkin, Chemical Reviews. 1232023</p>
<p>. Z Zhou, X Li, R N Zare, 20173ACS Central Science</p>
<p>G Jastrebski, D Arnold, IEEE International Conference on Evolutionary Computation. 2006. 2006</p>
<p>. F Häse, L M Roch, C Kreisbeck, A Aspuru-Guzik, ACS Central Science. 42018</p>
<p>. F Häse, M Aldeghi, R J Hickman, L M Roch, A Aspuru-Guzik, Applied Physics Reviews. 2021, 8, 031406</p>
<p>. R Hickman, M Sim, S Pablo-García, I Woolhouse, H Hao, Z Bao, P Bannigan, C Allen, M Aldeghi, A Aspuru-Guzik, 2023</p>
<p>. B J Shields, J Stevens, J Li, M Parasram, F Damani, J I M Alvarado, J M Janey, R P Adams, A G Doyle, Nature. 5902021</p>
<p>. N S Eyke, B A Koscher, K F Jensen, Trends in Chemistry. 32021</p>
<p>. J C Oliveira, J Frey, S.-Q Zhang, L.-C Xu, X Li, S.-W Li, X Hong, L Ackermann, Trends in Chemistry. 42022</p>
<p>. S Dara, S Dhamercherla, S S Jadav, C M Babu, M J Ahsan, Artificial Intelligence Review. 552021</p>
<p>. F Strieth-Kalthoff, H Hao, V Rathore, J Derasp, T Gaudin, N H Angello, M Seifrid, E Trushina, M Guy, J Liu, Science. 38492272024</p>
<p>M Skreta, N Yoshikawa, S Arellano-Rubach, Z Ji, L B Kristensen, K Darvish, A Aspuru-Guzik, F Shkurti, A Garg, Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting. 2023</p>
<p>. N Yoshikawa, M Skreta, K Darvish, S Arellano-Rubach, Z Ji, L Bjørn Kristensen, A Z Li, Y Zhao, H Xu, A Kuramshin, A Aspuru-Guzik, F Shkurti, A Garg, Autonomous Robots. 472023</p>
<p>. M Seifrid, R Pollice, A Aguilar-Granda, Z Morgan Chan, K Hotta, C T Ser, J Vestfrid, T C Wu, A Aspuru-Guzik, Accounts of Chemical Research. 552022</p>
<p>D Knobbe, H Zwirnmann, M Eckhoff, S Haddadin, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2022. 2022</p>
<p>. N Yoshikawa, G D Akkoc, S Pablo-García, Y Cao, H Hao, A Aspuru-Guzik, 2024</p>
<p>. Y Jiang, H Fakhruldeen, G Pizzuto, L Longley, A He, T Dai, R Clowes, N Rankin, A I Cooper, Digital Discovery. 22023</p>
<p>H Xu, Y R Wang, S Eppel, A Aspuru-Guzik, F Shkurti, A Garg, Proceedings of the 5th Conference on Robot Learning. the 5th Conference on Robot Learning2022</p>
<p>. D M Anstine, O Isayev, Journal of the American Chemical Society. 1452023</p>
<p>. S Lo, S G Baird, J Schrier, B Blaiszik, N Carson, I Foster, A Aguilar-Granda, S V Kalinin, B Maruyama, M Politi, Digital Discovery. 32024</p>
<p>J P Janet, H J Kulik, Machine Learning in Chemistry. American Chemical Society20201</p>
<p>. J A Keith, V Vassilev-Galindo, B Cheng, S Chmiela, M Gastegger, K.-R Muller, A Tkatchenko, Chemical reviews. 1212021</p>
<p>. N Artrith, K T Butler, F.-X Coudert, S Han, O Isayev, A Jain, A Walsh, Nature chemistry. 132021</p>
<p>. A D White, Living Journal of Computational Molecular Science. 2022, 3, 1499</p>
<p>T Chen, C Guestrin, Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. the 22nd acm sigkdd international conference on knowledge discovery and data mining2016</p>
<p>. G Tom, R J Hickman, A Zinzuwadia, A Mohajeri, B Sanchez-Lengeling, A Aspuru-Guzik, Digital Discovery. 22023</p>
<p>A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Proceedings of the 31st International Conference on Neural Information Processing Systems. the 31st International Conference on Neural Information Processing SystemsRed Hook, NY, USA2017</p>
<p>. T Lin, Y Wang, X Liu, X Qiu, Open, 20223</p>
<p>A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, J Uszkoreit, N Houlsby, International Conference on Learning Representations. 2021</p>
<p>C Ying, T Cai, S Luo, S Zheng, G Ke, D He, Y Shen, T.-Y Liu, Advances in neural information processing systems. 202134</p>
<p>. D P Kingma, arXiv:1312.61142013arXiv preprint</p>
<p>. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Commun. ACM. 632020</p>
<p>D Rezende, S Mohamed, International conference on machine learning. 2015</p>
<p>. T B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>. A Ramesh, P Dhariwal, A Nichol, C Chu, M Chen, arXiv:2204.061252022, 1, 3arXiv preprint</p>
<p>. N Metropolis, A W Rosenbluth, M N Rosenbluth, A H Teller, E Teller, The journal of chemical physics. 211953</p>
<p>. G Parisi, Nuclear Physics B. 1801981</p>
<p>W Chen, M Zhang, B Paige, J M Hernández-Lobato, D Barber, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024</p>
<p>S Zhao, R Brekelmans, A Makhzani, R B Grosse, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024</p>
<p>S Sanokowski, S Hochreiter, S Lehner, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024</p>
<p>Current Opinion in Solid State and Materials Science. G M Rotskoff, 2024, 30, 101158</p>
<p>E Bengio, M Jain, M Korablyov, D Precup, Y Bengio, Advances in Neural Information Processing Systems. 202134</p>
<p>M Jain, E Bengio, A Hernandez-Garcia, J Rector-Brooks, B F Dossou, C A Ekbote, J Fu, T Zhang, M Kilgour, D Zhang, International Conference on Machine Learning. 2022</p>
<p>A Hernandez-Garcia, A Duval, A Volokhova, Y Bengio, D Sharma, P L Carrier, M Koziarski, V Schmidt, 37th Conference on Neural Information Processing Systems. NeurIPS 2023. 2023I4M</p>
<p>Y Zhu, J Wu, C Hu, J Yan, C.-Y Hsieh, T Hou, J Wu, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>. D P Kingma, arXiv:1412.69802014arXiv preprint</p>
<p>J Martens, R Grosse, International conference on machine learning. 2015</p>
<p>A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A Köpf, E Yang, Z Devito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, PyTorch: an imperative style, high-performance deep learning library. Red Hook, NY, USACurran Associates Inc2019</p>
<p>J Bradbury, R Frostig, P Hawkins, M J Johnson, C Leary, D Maclaurin, G Necula, A Paszke, J Vanderplas, S Wanderman-Milne, Q Zhang, JAX: composable transformations of Python+NumPy programs. 2018</p>
<p>M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen, C Citro, G S Corrado, A Davis, J Dean, M Devin, S Ghemawat, I Goodfellow, A Harp, G Irving, M Isard, Y Jia, R Jozefowicz, L Kaiser, M Kudlur, J Levenberg, D Mané, R Monga, S Moore, D Murray, C Olah, M Schuster, J Shlens, B Steiner, I Sutskever, K Talwar, P Tucker, V Vanhoucke, V Vasudevan, F Viégas, O Vinyals, P Warden, M Wattenberg, M Wicke, Y Yu, X Zheng, TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. 2015</p>
<p>. M Raissi, P Perdikaris, G E Karniadakis, Journal of Computational physics. 3782019</p>
<p>. W Wang, Z Wu, J C B Dietschreit, R Gómez-Bombarelli, The Journal of Chemical Physics. 158441132023</p>
<p>. R A Vargas-Hernández, K Jorner, R Pollice, A Aspuru-Guzik, The Journal of Chemical Physics. 1581048012023</p>
<p>. T Osterrieder, F Schmitt, L Lüer, J Wagner, T Heumüller, J Hauch, C J Brabec, Energy &amp; Environmental Science. 162023</p>
<p>. P Q Velasco, K Y A Low, C J Leong, W T Ng, S Qiu, S Jhunjhunwala, B Li, A Qian, K Hippalgaonkar, J J W Cheng, Digital Discovery. 32024</p>
<p>. A Tripp, J M Hernández-Lobato, arXiv:2310.092672023arXiv preprint</p>
<p>. M Skreta, Z Zhou, J L Yuan, K Darvish, A Aspuru-Guzik, A Garg, arXiv:2401.041572024arXiv preprint</p>
<p>. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 352022</p>
<p>. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 352022</p>
<p>. Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, Y Dai, J Sun, H Wang, arXiv:2312.109972023arXiv preprint</p>
<p>L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K R Narasimhan, Y Cao, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Y Song, P Dhariwal, M Chen, I Sutskever, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>R.-R Griffiths, L Klarner, H Moss, A Ravuri, S T Truong, Y Du, S D Stanton, G Tom, B Ranković, A R Jamasb, A Deshwal, J Schwartz, A Tripp, G Kell, S Frieder, A Bourached, A J Chan, J Moss, C Guo, J P Dürholt, Thirty-seventh Conference on Neural Information Processing Systems. J W Chaurasia, F Park, A Strieth-Kalthoff, B Lee, A Cheng, P Aspuru-Guzik, J Schwaller, Tang, 2023</p>
<p>. S J Ang, W Wang, D Schwalbe-Koda, S Axelrod, R Gómez-Bombarelli, Chem. 7432021</p>
<p>. W Heyndrickx, L Mervin, T Morawietz, N Sturm, L Friedrich, A Zalewski, A Pentina, L Humbeck, M Oldenhof, R Niwayama, Journal of chemical information and modeling. 642023</p>
<p>S Zaidi, M Schaarschmidt, J Martens, H Kim, Y W Teh, A Sanchez-Gonzalez, P Battaglia, R Pascanu, J Godwin, The Eleventh International Conference on Learning Representations. 2023</p>
<p>. M Jain, T Deleu, J Hartford, C.-H Liu, A Hernandez-Garcia, Y Bengio, Digital Discovery. 22023</p>
<p>. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 2009. 2009</p>
<p>O Bojar, C Buck, C Federmann, B Haddow, P Koehn, J Leveling, C Monz, P Pecina, M Post, H Saint-Amand, Proceedings of the ninth workshop on statistical machine translation. the ninth workshop on statistical machine translation2014</p>
<p>M Heusel, H Ramsauer, T Unterthiner, B Nessler, S Hochreiter, Proceedings of the 31st International Conference on Neural Information Processing Systems. the 31st International Conference on Neural Information Processing SystemsRed Hook, NY, USA2017</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>. L Chanussot, A Das, S Goyal, T Lavril, M Shuaibi, M Riviere, K Tran, J Heras-Domingo, C Ho, W Hu, Acs Catalysis. 112021</p>
<p>. R Tran, J Lan, M Shuaibi, B M Wood, S Goyal, A Das, J Heras-Domingo, A Kolluru, A Rizvi, N Shoghi, ACS Catalysis. 132023</p>
<p>. J Lan, A Palizhati, M Shuaibi, B M Wood, B Wander, A Das, M Uyttendaele, C L Zitnick, Z W Ulissi, Computational Materials. 91722023</p>
<p>Open Catalyst demo -open-catalyst. 24-08-2024metademolab.com</p>
<p>Matbench Discovery -A framework to evaluate machine learning crystal stability predictions. J Riebesell, R E A Goodall, P Benner, Y Chiang, B Deng, A A Lee, A Jain, K A Persson, 2024</p>
<p>D Rolnick, A Aspuru-Guzik, S Beery, B Dilkina, P L Donti, M Ghassemi, H Kerner, C Monteleoni, E Rolf, M Tambe, A White, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024</p>
<p>. J Gilmer, S S Schoenholz, P F Riley, O Vinyals, G E Dahl, 2017</p>
<p>K Xu, W Hu, J Leskovec, S Jegelka, International Conference on Learning Representations. 2019</p>
<p>V Delle Rose, A Kozachinskiy, C Rojas, M Petrache, P Barcelo, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>. N Thomas, T Smidt, S Kearnes, L Yang, L Li, K Kohlhoff, P Riley, arXiv:1802.082192018arXiv preprint</p>
<p>. M M Bronstein, J Bruna, T Cohen, P Veličković, arXiv:2104.134782021arXiv preprint</p>
<p>Molecular Simulation -ai4science101. 24-08-2024</p>
<p>. Ai4mat , 02-09-20242024AI4Mat-NeurIPS 2024 -sites.google.com</p>
<p>02-09-2024Machine Learning in Structural Biology -mlsb.io. 2024</p>
<p>G N W , 02-09-2024GenBio NeurIPS Workshop 2023 -genbio-workshop.github.io. 2023. 2023</p>
<p>Compbio, 02-09-2024CompBio Workshop ICML 2023 -icml-compbio.github.io. </p>
<p>Ai4science , 02-09-2024AI for Science: Scaling in AI for Scientific Discovery -ai4sciencecommunity.github.io. 2024</p>
<p>02-09-2024MLDD, MLDD 2023 -sites.google.com. 2023</p>
<p>Ml4materials, 02-09-2024ICLR 2023 Workshop, Machine Leanring for Materials -ml4materials.com. </p>
<p>I Blog, 24-08-2024about | ICLR Blogposts 2024 -iclr-blogposts.github.io. </p>
<p>. N Schneider, D M Lowe, R A Sayle, M A Tarselli, G A Landrum, Journal of medicinal chemistry. 592016</p>
<p>A Krizhevsky, I Sutskever, G E Hinton, Advances in Neural Information Processing Systems. 2012</p>
<p>. J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.083612020arXiv preprint</p>
<p>. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 2019, 1, 9</p>
<p>R Sutton, Incomplete Ideas (blog). 201938</p>
<p>. S.-C Li, H Wu, A Menon, K A Spiekermann, Y.-P Li, W H Green, Journal of the American Chemical Society. 2024</p>
<p>J Kim, D Nguyen, A Suleymanzade, H An, S Hong, Advances in Neural Information Processing Systems. 202336</p>
<p>A A Duval, V Schmidt, A Hernández-Garcıa, S Miret, F D Malliaros, Y Bengio, D Rolnick, International Conference on Machine Learning. 2023</p>
<p>. D Flam-Shepherd, A Aspuru-Guzik, arXiv:2305.057082023arXiv preprint</p>
<p>N Gruver, A Sriram, A Madotto, A G Wilson, C L Zitnick, Z W Ulissi, The Twelfth International Conference on Learning Representations. 2024</p>
<p>. A Aspuru-Guzik, R Lindh, M Reiher, ACS central science. 42018</p>
<p>. J Westermayr, P Marquetand, Chemical Reviews. 1212020</p>
<p>. S Axelrod, E Shakhnovich, R Gómez-Bombarelli, Nature communications. 1334402022</p>
<p>. Y Du, C Duan, A Bran, A Sotnikova, Y Qu, H Kulik, A Bosselut, J Xu, P Schwaller, Chemrxiv , 2024</p>
<p>. D Morgan, G Pilania, A Couet, B P Uberuaga, C Sun, J Li, Current Opinion in Solid State and Materials Science. 261009752022</p>
<p>. Y Djoumbou-Feunang, J Wilmot, J Kinney, P Chanda, P Yu, A Sader, M Sharifi, S Smith, J Ou, J Hu, Frontiers in Chemistry. 1112920272023</p>
<p>. V Barone, S Alessandrini, M Biczysko, J R Cheeseman, D C Clary, A B Mccoy, R J Dirisio, F Neese, M Melosso, C Puzzarini, Nature Reviews Methods Primers. 2021, 1, 38</p>
<p>. S X Leong, S Pablo-García, Z Zhang, A Aspuru-Guzik, ChemRxiv. 2024</p>
<p>. Z T Fried, S J El-Abd, B M Hays, G Wenzel, A N Byrne, L Margulès, R A Motiyenko, S T Shipman, M P Horne, J K Jørgensen, The Astrophysical Journal Letters. L232024</p>
<p>. H Zheng, E Sivonxay, M Gallant, Z Luo, M Mcdermott, P Huck, K A Persson, arXiv:2402.001772024arXiv preprint</p>
<p>. C.-I Wang, J C Maier, N E Jackson, Chemical Science. 2024</p>
<p>. A Ullah, Y Huang, M Yang, P O , arXiv:2404.140212024arXiv preprint</p>
<p>. S Zhu, B H Nguyen, Y Xia, K Frost, S Xie, V Viswanathan, J A Smith, Green Chemistry. 252023</p>
<p>. G Zhao, H Kim, C Yang, Y G Chung, The Journal of Physical Chemistry A. 1282024</p>
<p>J Sohl-Dickstein, E Weiss, N Maheswaranathan, S Ganguli, International conference on machine learning. 2015</p>
<p>Advances in neural information processing systems. J Ho, A Jain, P Abbeel, 202033</p>
<p>. Y Song, J Sohl-Dickstein, D P Kingma, A Kumar, S Ermon, B Poole, arXiv:2011.134562020arXiv preprint</p>
<p>Advances in neural information processing systems. T Karras, M Aittala, T Aila, S Laine, 202235</p>
<p>. B Máté, F Fleuret, T Bereau, arXiv:2406.023132024arXiv preprint</p>
<p>K Neklyudov, R Brekelmans, D Severo, A Makhzani, International conference on machine learning. 2023</p>
<p>Y Du, M Plainer, R Brekelmans, C Duan, F Noe, C P Gomes, A Aspuru-Guzik, K Neklyudov, ICML 2024 AI for Science Workshop. 2024</p>
<p>. N H Angello, V Rathore, W Beker, A Wołos, E R Jira, R Roszak, T C Wu, C M Schroeder, A Aspuru-Guzik, B A Grzybowski, Science. 3782022</p>
<p>. Nucleic acids research. 472019</p>
<p>. O Schilter, P Schwaller, T Laino, Green Chemistry, 2024</p>
<p>. L Deng, IEEE Signal Processing Magazine. 292012</p>
<p>How many pictures are there. M Broz, 27-08-2024Statistics, trends, and forecasts. 2024</p>
<p>B Kozinsky, A Musaelian, A Johansson, S Batzner, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. the International Conference for High Performance Computing, Networking, Storage and Analysis2023</p>
<p>. J Baldridge, J Bauer, M Bhutani, N Brichtova, A Bunner, K Chan, Y Chen, S Dieleman, Y Du, Z Eaton-Rosen, arXiv:2408.070092024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
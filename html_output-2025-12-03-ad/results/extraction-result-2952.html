<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2952 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2952</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2952</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-280918480</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.19828v1.pdf" target="_blank">Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of NLP tasks, but they remain fundamentally stateless , constrained by limited context windows that hinder long-horizon reasoning. Recent efforts to address this limitation often augment LLMs with an external memory bank, yet most existing pipelines are static and heuristic-driven, lacking any learned mechanism for deciding what to store, update, or retrieve. We present Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the ability to actively manage and utilize external memory through two specialized agents: a Memory Manager that learns to perform structured memory operations { ADD , UPDATE , DELETE , NOOP } , and an Answer Agent that selects the most relevant entries and reasons over them to produce an answer. Both agents are fine-tuned with outcome-driven RL (PPO and GRPO), enabling adaptive memory management and use with minimal supervision. With as few as 152 question-answer pairs and a corresponding temporal memory bank for training, Memory-R1 outperforms the most competitive existing baseline and demonstrates strong generalization across diverse question types and LLM backbones. Beyond presenting an effective approach, this work provides insights into how RL can unlock more agentic, memory-aware behaviors in LLMs, pointing toward richer, more persistent reasoning systems.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2952.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2952.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL framework that trains two specialized LLM agents — a Memory Manager (learns CRUD operations on an external memory bank) and an Answer Agent (performs memory distillation and generates answers) — using PPO or GRPO to optimize downstream QA correctness on long-horizon multi-session dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory-R1</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two-agent architecture: (1) Memory Manager: LLM policy that, given extracted facts and retrieved memories, chooses {ADD, UPDATE, DELETE, NOOP} and produces updated memory content; (2) Answer Agent: LLM policy that receives retrieved candidate memories, applies a Memory Distillation filter to select relevant entries, and generates the final answer. Both agents are RL-fine-tuned (PPO or GRPO) with outcome-driven exact-match rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>LLaMA-3.1-8B-Instruct; Qwen-2.5-7B-Instruct (experiments); GPT-4o-mini used for temporal memory bank construction in data prep</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td>8B; 7B</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Not a text-game: evaluated on LO-COMO, a multi-session conversational QA benchmark that requires retrieving and reasoning over temporally distant conversational history (single-hop, multi-hop, open-domain, temporal questions).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Retrieval-augmented external memory with CRUD-style operations (ADD, UPDATE, DELETE, NOOP)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>External temporal memory bank of summarized facts (per participant) with timestamps and IDs; Memory Manager updates entries via ADD/UPDATE/DELETE/NOOP (UPDATE may merge/append info and preserve old_memory); temporal memory banks are built from previous ~50 turns for a participant; candidate set retrieved at query time is distilled by the Answer Agent.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Similarity-based Retrieval-Augmented Generation (RAG) to fetch candidate memories (paper uses top-K retrieval; evaluation setup: up to 60 candidate memories retrieved per question).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Temporal memory bank built from preceding ~50 turns; retrieval returns up to 60 candidate memories for answering; storage capacity otherwise unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Summaries / extracted facts from dialogue turns (fact snippets), timestamps, per-participant memories, and merged/updated memory entries (including preserved old_memory when updated).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On the LO-COMO benchmark with LLaMA-3.1-8B-Instruct backbone, Memory-R1-GRPO: Overall F1 = 45.02, BLEU-1 = 37.51, LLM-as-a-Judge = 62.74 (reported means).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline (strongest non-RL baseline, reimplemented Mem0) reported overall F1 ≈ 30.41, BLEU-1 ≈ 22.22, LLM-as-a-Judge ≈ 45.68 (paper reports Memory-R1 improvements over this baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>+14.61 absolute F1 (≈+48% relative), +15.29 BLEU-1 (≈+69% relative), +17.06 LLM-as-a-Judge (≈+37% relative) versus the strongest baseline (Mem0) on LLaMA-3.1-8B-Instruct (as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Learning memory operations via RL (outcome-driven rewards) substantially improves downstream QA on long-horizon conversational tasks; an RL-fine-tuned Memory Manager better consolidates related facts (e.g., UPDATE vs DELETE+ADD mistakes by vanilla managers); Memory Distillation (filtering retrieved candidates) reduces noise and improves answer accuracy; GRPO converges faster and slightly outperforms PPO in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Paper highlights retrieval noise and heuristic retrieval pitfalls (too few or too many returned entries), and shows vanilla in-context managers misapply operations (fragmentation). Other limits: memory capacity and long-term scaling not exhaustively characterized; training relies on frozen Answer Agent during Memory Manager RL which may limit joint optimization (design choice noted).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Compared against heuristic/static baselines (Mem0, LOCOMO baseline, LangMem, A-Mem, etc.): RL-based Memory-R1 outperforms these non-RL or heuristic memory systems; Memory Distillation outperforms consuming all retrieved memories. No head-to-head comparison to e.g., graph-based memory or episodic vs semantic memory families beyond baseline methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2952.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2952.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mem0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mem0: Building production-ready ai agents with scalable long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular memory system that supports explicit in-context memory operations and is designed for scalable deployment of long-term memory in LLM agents; treated as the strongest baseline in this paper's evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mem0: Building production-ready ai agents with scalable long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Mem0</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Modular memory pipeline that exposes explicit in-context memory operations (CRUD-like) and scheduling policies; in this paper it was re-implemented as a baseline under the same RAG/retrieval setup and LLM backbones for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>Re-implemented with LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instruct in this work's comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td>8B; 7B</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>CRUD-style / in-context external memory (long-term memory buffer and policies)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Described as a modular memory system with explicit memory operation interfaces; paper's reimplementation follows the same external memory bank + RAG retrieval setup (60 candidate retrievals) used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>When reimplemented in this study, uses similarity-based RAG retrieval (top-K, up to 60 candidates) to supply candidate memories to the answer agent.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not explicitly specified by the original work within this paper; in this paper's evaluation the agent consumed up to 60 retrieved candidates per question.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Dialogue facts / memory entries summarizing past turns; exact original schema of Mem0 beyond CRUD-style ops is not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>In this paper's reimplementation (strongest baseline) overall F1 ≈ 30.41, BLEU-1 ≈ 22.22, LLM-as-a-Judge ≈ 45.68 on LO-COMO (used as the comparison point for Memory-R1 improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Represents a strong heuristic/engineered baseline; however, heuristic-driven memory operation selection is outperformed by RL-trained memory controllers (Memory-R1) which learn to consolidate and update memories more effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Relies on heuristics for memory operations; lacks a learned, outcome-driven mechanism for deciding when to ADD/UPDATE/DELETE/NOOP, making it prone to fragmentation or overwriting errors in multi-session dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Compared empirically as the strongest non-RL baseline in this paper; no internal comparison across different memory data structures reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2952.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2952.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOCOMO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LOCOMO (LO-COMO benchmark / dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-session conversational benchmark designed to evaluate agents' ability to retrieve and reason over temporally distant conversational history across single-hop, multi-hop, open-domain, and temporal questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating very longterm conversational memory of llm agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LOCOMO (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Dataset/benchmark of multi-session dialogues; used both to construct training tuples for Memory Manager and Answer Agent and as the evaluation suite covering diverse question types requiring long-horizon memory retrieval and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Not a text game; LO-COMO is a conversational memory benchmark with multi-session dialogues and temporally distant facts requiring retrieval and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Temporal conversation memory (participant-specific temporal memory banks)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Provides temporal memory traces built from previous session turns (paper uses previous ~50 turns); memories are timestamped and participant-indexed and used to construct temporal memory banks for manager training and RAG retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>RAG / similarity-based retrieval over the temporal memory bank (paper retrieves up to 60 candidate memories per question; training may use top-30 per participant depending on construction).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Temporal snapshots built from preceding ~50 turns per participant; retrieval returns up to 60 candidates for answer training/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Timestamped extracted facts / summaries from dialogue turns, per-participant memory entries used to answer later questions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Serves as the realistic multi-session testbed demonstrating the importance of accurate memory operations and distillation; Memory-R1 shows large gains on LO-COMO versus heuristic baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2952.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2952.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemGPT: Towards LLMs as Operating Systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based memory system that introduces working and long-term memory buffers with memory scheduling policies (mentioned as prior work on memory-augmented LLM agents).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MemGPT: Towards LLMs as Operating Systems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Proposes working and long-term memory buffers and scheduling policies for LLM agents; cited as an example of memory-augmented LLM systems that separate memory storage tiers and manage retrieval scheduling.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Working and long-term memory buffers with scheduling policies (tiered memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Tiered memory (working vs long-term) with scheduling/policies for moving info between tiers; paper only references this at a high level and does not reimplement MemGPT's architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Not detailed in this paper (presumably task-relevant observations / summaries in different buffers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Cited as precedent for separating memory into working and long-term buffers and for the need for scheduling policies; no experimental comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2952.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2952.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoryBank: Lifelong Memory for LLM Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compositional memory controller for lifelong agent memory, cited as prior work addressing long-term memory for LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MemoryBank: Lifelong Memory for LLM Agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Compositional memory controller that supports lifelong accumulation of memories for LLM agents; listed among recent efforts to augment LLMs with external memory modules.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Lifelong / compositional external memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Compositional controller for persistent memory accumulation; specific details are not reproduced in this paper beyond citation-level mention.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Cited as part of recent literature on external memory modules for long-horizon reasoning; no experimental data provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2952.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2952.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReadAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReadAgent: Memory-Augmented Tool-Using LLM Agents with Long-Term Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-augmented LLM agent that incorporates retrieval mechanisms for memory-grounded dialogue, cited as related work on memory-enabled agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReadAgent: Memory-Augmented Tool-Using LLM Agents with Long-Term Memory.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReadAgent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Integrates retrieval mechanisms to ground dialogue on long-term memory; referenced as an example of memory-augmented tool-using agents but not reimplemented or evaluated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Retrieval-augmented long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Mentioned as a related system using retrieval for memory-grounded dialogue; no direct comparisons or metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2952.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2952.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A-Mem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A-Mem: Agentic Memory for LLM Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid architecture (cited) that combines dynamic memory access with reinforcement learning; referenced as recent work connecting RL and memory for LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A-MEM: Agentic Memory for LLM Agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>A-Mem</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hybrid architecture combining dynamic memory access with RL to support agentic memory behavior; cited as a related attempt to unify RL and memory management for LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Dynamic memory access combined with RL</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Cited as work combining RL with dynamic memory access; this paper positions Memory-R1 as among the first to directly frame memory operation selection and utilization as RL problems.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2952.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2952.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangMem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangMem: Modular memory for agentic systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular memory approach for agentic systems; cited among other modular memory solutions in the literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LangMem: Modular memory for agentic systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LangMem</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Described briefly as a modular memory system for agentic LLM systems; mentioned as related work but not analyzed or reimplemented here.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Modular external memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Mentioned among modular memory systems; no experimental detail provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2952.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2952.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zep</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zep: a temporal knowledge graph architecture for agent memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A temporal knowledge-graph-style memory architecture for agents (cited); presented as an example of structured temporal memory for agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Zep: a temporal knowledge graph architecture for agent memory.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Zep</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Proposes a temporal knowledge-graph architecture to represent agent memories; cited as an alternative memory architecture that emphasizes temporal structure.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Temporal knowledge-graph memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Knowledge-graph-style temporal memory (citation-level mention only; no implementation details in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Cited as an example of structured temporal memory architectures; not compared empirically here.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mem0: Building production-ready ai agents with scalable long-term memory. <em>(Rating: 2)</em></li>
                <li>Evaluating very longterm conversational memory of llm agents. <em>(Rating: 2)</em></li>
                <li>MemGPT: Towards LLMs as Operating Systems. <em>(Rating: 2)</em></li>
                <li>MemoryBank: Lifelong Memory for LLM Agents. <em>(Rating: 2)</em></li>
                <li>ReadAgent: Memory-Augmented Tool-Using LLM Agents with Long-Term Memory. <em>(Rating: 2)</em></li>
                <li>A-MEM: Agentic Memory for LLM Agents. <em>(Rating: 2)</em></li>
                <li>LangMem: Modular memory for agentic systems <em>(Rating: 1)</em></li>
                <li>Zep: a temporal knowledge graph architecture for agent memory. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2952",
    "paper_id": "paper-280918480",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "Memory-R1",
            "name_full": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning",
            "brief_description": "An RL framework that trains two specialized LLM agents — a Memory Manager (learns CRUD operations on an external memory bank) and an Answer Agent (performs memory distillation and generates answers) — using PPO or GRPO to optimize downstream QA correctness on long-horizon multi-session dialogues.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Memory-R1",
            "agent_description": "Two-agent architecture: (1) Memory Manager: LLM policy that, given extracted facts and retrieved memories, chooses {ADD, UPDATE, DELETE, NOOP} and produces updated memory content; (2) Answer Agent: LLM policy that receives retrieved candidate memories, applies a Memory Distillation filter to select relevant entries, and generates the final answer. Both agents are RL-fine-tuned (PPO or GRPO) with outcome-driven exact-match rewards.",
            "base_llm_model": "LLaMA-3.1-8B-Instruct; Qwen-2.5-7B-Instruct (experiments); GPT-4o-mini used for temporal memory bank construction in data prep",
            "base_llm_size": "8B; 7B",
            "text_game_name": null,
            "text_game_description": "Not a text-game: evaluated on LO-COMO, a multi-session conversational QA benchmark that requires retrieving and reasoning over temporally distant conversational history (single-hop, multi-hop, open-domain, temporal questions).",
            "uses_memory": true,
            "memory_type": "Retrieval-augmented external memory with CRUD-style operations (ADD, UPDATE, DELETE, NOOP)",
            "memory_architecture": "External temporal memory bank of summarized facts (per participant) with timestamps and IDs; Memory Manager updates entries via ADD/UPDATE/DELETE/NOOP (UPDATE may merge/append info and preserve old_memory); temporal memory banks are built from previous ~50 turns for a participant; candidate set retrieved at query time is distilled by the Answer Agent.",
            "memory_retrieval_mechanism": "Similarity-based Retrieval-Augmented Generation (RAG) to fetch candidate memories (paper uses top-K retrieval; evaluation setup: up to 60 candidate memories retrieved per question).",
            "memory_capacity": "Temporal memory bank built from preceding ~50 turns; retrieval returns up to 60 candidate memories for answering; storage capacity otherwise unspecified.",
            "what_is_stored_in_memory": "Summaries / extracted facts from dialogue turns (fact snippets), timestamps, per-participant memories, and merged/updated memory entries (including preserved old_memory when updated).",
            "performance_with_memory": "On the LO-COMO benchmark with LLaMA-3.1-8B-Instruct backbone, Memory-R1-GRPO: Overall F1 = 45.02, BLEU-1 = 37.51, LLM-as-a-Judge = 62.74 (reported means).",
            "performance_without_memory": "Baseline (strongest non-RL baseline, reimplemented Mem0) reported overall F1 ≈ 30.41, BLEU-1 ≈ 22.22, LLM-as-a-Judge ≈ 45.68 (paper reports Memory-R1 improvements over this baseline).",
            "has_ablation_study": true,
            "memory_improvement_magnitude": "+14.61 absolute F1 (≈+48% relative), +15.29 BLEU-1 (≈+69% relative), +17.06 LLM-as-a-Judge (≈+37% relative) versus the strongest baseline (Mem0) on LLaMA-3.1-8B-Instruct (as reported).",
            "key_findings_about_memory": "Learning memory operations via RL (outcome-driven rewards) substantially improves downstream QA on long-horizon conversational tasks; an RL-fine-tuned Memory Manager better consolidates related facts (e.g., UPDATE vs DELETE+ADD mistakes by vanilla managers); Memory Distillation (filtering retrieved candidates) reduces noise and improves answer accuracy; GRPO converges faster and slightly outperforms PPO in many settings.",
            "memory_limitations": "Paper highlights retrieval noise and heuristic retrieval pitfalls (too few or too many returned entries), and shows vanilla in-context managers misapply operations (fragmentation). Other limits: memory capacity and long-term scaling not exhaustively characterized; training relies on frozen Answer Agent during Memory Manager RL which may limit joint optimization (design choice noted).",
            "comparison_with_other_memory_types": "Compared against heuristic/static baselines (Mem0, LOCOMO baseline, LangMem, A-Mem, etc.): RL-based Memory-R1 outperforms these non-RL or heuristic memory systems; Memory Distillation outperforms consuming all retrieved memories. No head-to-head comparison to e.g., graph-based memory or episodic vs semantic memory families beyond baseline methods.",
            "uuid": "e2952.0",
            "source_info": {
                "paper_title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Mem0",
            "name_full": "Mem0: Building production-ready ai agents with scalable long-term memory",
            "brief_description": "A modular memory system that supports explicit in-context memory operations and is designed for scalable deployment of long-term memory in LLM agents; treated as the strongest baseline in this paper's evaluations.",
            "citation_title": "Mem0: Building production-ready ai agents with scalable long-term memory.",
            "mention_or_use": "use",
            "agent_name": "Mem0",
            "agent_description": "Modular memory pipeline that exposes explicit in-context memory operations (CRUD-like) and scheduling policies; in this paper it was re-implemented as a baseline under the same RAG/retrieval setup and LLM backbones for fair comparison.",
            "base_llm_model": "Re-implemented with LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instruct in this work's comparisons",
            "base_llm_size": "8B; 7B",
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "CRUD-style / in-context external memory (long-term memory buffer and policies)",
            "memory_architecture": "Described as a modular memory system with explicit memory operation interfaces; paper's reimplementation follows the same external memory bank + RAG retrieval setup (60 candidate retrievals) used for evaluation.",
            "memory_retrieval_mechanism": "When reimplemented in this study, uses similarity-based RAG retrieval (top-K, up to 60 candidates) to supply candidate memories to the answer agent.",
            "memory_capacity": "Not explicitly specified by the original work within this paper; in this paper's evaluation the agent consumed up to 60 retrieved candidates per question.",
            "what_is_stored_in_memory": "Dialogue facts / memory entries summarizing past turns; exact original schema of Mem0 beyond CRUD-style ops is not detailed in this paper.",
            "performance_with_memory": "In this paper's reimplementation (strongest baseline) overall F1 ≈ 30.41, BLEU-1 ≈ 22.22, LLM-as-a-Judge ≈ 45.68 on LO-COMO (used as the comparison point for Memory-R1 improvements).",
            "performance_without_memory": null,
            "has_ablation_study": false,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Represents a strong heuristic/engineered baseline; however, heuristic-driven memory operation selection is outperformed by RL-trained memory controllers (Memory-R1) which learn to consolidate and update memories more effectively.",
            "memory_limitations": "Relies on heuristics for memory operations; lacks a learned, outcome-driven mechanism for deciding when to ADD/UPDATE/DELETE/NOOP, making it prone to fragmentation or overwriting errors in multi-session dialogue.",
            "comparison_with_other_memory_types": "Compared empirically as the strongest non-RL baseline in this paper; no internal comparison across different memory data structures reported here.",
            "uuid": "e2952.1",
            "source_info": {
                "paper_title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "LOCOMO",
            "name_full": "LOCOMO (LO-COMO benchmark / dataset)",
            "brief_description": "A multi-session conversational benchmark designed to evaluate agents' ability to retrieve and reason over temporally distant conversational history across single-hop, multi-hop, open-domain, and temporal questions.",
            "citation_title": "Evaluating very longterm conversational memory of llm agents.",
            "mention_or_use": "use",
            "agent_name": "LOCOMO (benchmark)",
            "agent_description": "Dataset/benchmark of multi-session dialogues; used both to construct training tuples for Memory Manager and Answer Agent and as the evaluation suite covering diverse question types requiring long-horizon memory retrieval and reasoning.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": "Not a text game; LO-COMO is a conversational memory benchmark with multi-session dialogues and temporally distant facts requiring retrieval and reasoning.",
            "uses_memory": true,
            "memory_type": "Temporal conversation memory (participant-specific temporal memory banks)",
            "memory_architecture": "Provides temporal memory traces built from previous session turns (paper uses previous ~50 turns); memories are timestamped and participant-indexed and used to construct temporal memory banks for manager training and RAG retrieval.",
            "memory_retrieval_mechanism": "RAG / similarity-based retrieval over the temporal memory bank (paper retrieves up to 60 candidate memories per question; training may use top-30 per participant depending on construction).",
            "memory_capacity": "Temporal snapshots built from preceding ~50 turns per participant; retrieval returns up to 60 candidates for answer training/evaluation.",
            "what_is_stored_in_memory": "Timestamped extracted facts / summaries from dialogue turns, per-participant memory entries used to answer later questions.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Serves as the realistic multi-session testbed demonstrating the importance of accurate memory operations and distillation; Memory-R1 shows large gains on LO-COMO versus heuristic baselines.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2952.2",
            "source_info": {
                "paper_title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "MemGPT",
            "name_full": "MemGPT: Towards LLMs as Operating Systems",
            "brief_description": "An LLM-based memory system that introduces working and long-term memory buffers with memory scheduling policies (mentioned as prior work on memory-augmented LLM agents).",
            "citation_title": "MemGPT: Towards LLMs as Operating Systems.",
            "mention_or_use": "mention",
            "agent_name": "MemGPT",
            "agent_description": "Proposes working and long-term memory buffers and scheduling policies for LLM agents; cited as an example of memory-augmented LLM systems that separate memory storage tiers and manage retrieval scheduling.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "Working and long-term memory buffers with scheduling policies (tiered memory)",
            "memory_architecture": "Tiered memory (working vs long-term) with scheduling/policies for moving info between tiers; paper only references this at a high level and does not reimplement MemGPT's architecture.",
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": "Not detailed in this paper (presumably task-relevant observations / summaries in different buffers).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Cited as precedent for separating memory into working and long-term buffers and for the need for scheduling policies; no experimental comparison in this paper.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2952.3",
            "source_info": {
                "paper_title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "MemoryBank",
            "name_full": "MemoryBank: Lifelong Memory for LLM Agents",
            "brief_description": "A compositional memory controller for lifelong agent memory, cited as prior work addressing long-term memory for LLM agents.",
            "citation_title": "MemoryBank: Lifelong Memory for LLM Agents.",
            "mention_or_use": "mention",
            "agent_name": "MemoryBank",
            "agent_description": "Compositional memory controller that supports lifelong accumulation of memories for LLM agents; listed among recent efforts to augment LLMs with external memory modules.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "Lifelong / compositional external memory",
            "memory_architecture": "Compositional controller for persistent memory accumulation; specific details are not reproduced in this paper beyond citation-level mention.",
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Cited as part of recent literature on external memory modules for long-horizon reasoning; no experimental data provided here.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2952.4",
            "source_info": {
                "paper_title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "ReadAgent",
            "name_full": "ReadAgent: Memory-Augmented Tool-Using LLM Agents with Long-Term Memory",
            "brief_description": "A memory-augmented LLM agent that incorporates retrieval mechanisms for memory-grounded dialogue, cited as related work on memory-enabled agents.",
            "citation_title": "ReadAgent: Memory-Augmented Tool-Using LLM Agents with Long-Term Memory.",
            "mention_or_use": "mention",
            "agent_name": "ReadAgent",
            "agent_description": "Integrates retrieval mechanisms to ground dialogue on long-term memory; referenced as an example of memory-augmented tool-using agents but not reimplemented or evaluated in this work.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "Retrieval-augmented long-term memory",
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Mentioned as a related system using retrieval for memory-grounded dialogue; no direct comparisons or metrics in this paper.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2952.5",
            "source_info": {
                "paper_title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "A-Mem",
            "name_full": "A-Mem: Agentic Memory for LLM Agents",
            "brief_description": "A hybrid architecture (cited) that combines dynamic memory access with reinforcement learning; referenced as recent work connecting RL and memory for LLM agents.",
            "citation_title": "A-MEM: Agentic Memory for LLM Agents.",
            "mention_or_use": "mention",
            "agent_name": "A-Mem",
            "agent_description": "Hybrid architecture combining dynamic memory access with RL to support agentic memory behavior; cited as a related attempt to unify RL and memory management for LLM agents.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "Dynamic memory access combined with RL",
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Cited as work combining RL with dynamic memory access; this paper positions Memory-R1 as among the first to directly frame memory operation selection and utilization as RL problems.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2952.6",
            "source_info": {
                "paper_title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "LangMem",
            "name_full": "LangMem: Modular memory for agentic systems",
            "brief_description": "A modular memory approach for agentic systems; cited among other modular memory solutions in the literature review.",
            "citation_title": "LangMem: Modular memory for agentic systems",
            "mention_or_use": "mention",
            "agent_name": "LangMem",
            "agent_description": "Described briefly as a modular memory system for agentic LLM systems; mentioned as related work but not analyzed or reimplemented here.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "Modular external memory",
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Mentioned among modular memory systems; no experimental detail provided in this paper.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2952.7",
            "source_info": {
                "paper_title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Zep",
            "name_full": "Zep: a temporal knowledge graph architecture for agent memory",
            "brief_description": "A temporal knowledge-graph-style memory architecture for agents (cited); presented as an example of structured temporal memory for agents.",
            "citation_title": "Zep: a temporal knowledge graph architecture for agent memory.",
            "mention_or_use": "mention",
            "agent_name": "Zep",
            "agent_description": "Proposes a temporal knowledge-graph architecture to represent agent memories; cited as an alternative memory architecture that emphasizes temporal structure.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "Temporal knowledge-graph memory",
            "memory_architecture": "Knowledge-graph-style temporal memory (citation-level mention only; no implementation details in this paper).",
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Cited as an example of structured temporal memory architectures; not compared empirically here.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2952.8",
            "source_info": {
                "paper_title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning",
                "publication_date_yy_mm": "2025-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mem0: Building production-ready ai agents with scalable long-term memory.",
            "rating": 2,
            "sanitized_title": "mem0_building_productionready_ai_agents_with_scalable_longterm_memory"
        },
        {
            "paper_title": "Evaluating very longterm conversational memory of llm agents.",
            "rating": 2,
            "sanitized_title": "evaluating_very_longterm_conversational_memory_of_llm_agents"
        },
        {
            "paper_title": "MemGPT: Towards LLMs as Operating Systems.",
            "rating": 2,
            "sanitized_title": "memgpt_towards_llms_as_operating_systems"
        },
        {
            "paper_title": "MemoryBank: Lifelong Memory for LLM Agents.",
            "rating": 2,
            "sanitized_title": "memorybank_lifelong_memory_for_llm_agents"
        },
        {
            "paper_title": "ReadAgent: Memory-Augmented Tool-Using LLM Agents with Long-Term Memory.",
            "rating": 2,
            "sanitized_title": "readagent_memoryaugmented_toolusing_llm_agents_with_longterm_memory"
        },
        {
            "paper_title": "A-MEM: Agentic Memory for LLM Agents.",
            "rating": 2,
            "sanitized_title": "amem_agentic_memory_for_llm_agents"
        },
        {
            "paper_title": "LangMem: Modular memory for agentic systems",
            "rating": 1,
            "sanitized_title": "langmem_modular_memory_for_agentic_systems"
        },
        {
            "paper_title": "Zep: a temporal knowledge graph architecture for agent memory.",
            "rating": 1,
            "sanitized_title": "zep_a_temporal_knowledge_graph_architecture_for_agent_memory"
        }
    ],
    "cost": 0.0207805,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning
29 Aug 2025</p>
<p>Sikuan Yan s.yan@campus.lmu.de 
Ludwig Maximilian University of Munich</p>
<p>Xiufeng Yang 
Technical University of Munich</p>
<p>Zuchao Huang 
Ludwig Maximilian University of Munich</p>
<p>Ercong Nie 
Ludwig Maximilian University of Munich</p>
<p>Zifeng Ding 
University of Cambridge</p>
<p>Zonggen Li 
University of Hong Kong</p>
<p>Xiaowen Ma 
Ludwig Maximilian University of Munich</p>
<p>Hinrich Sch Ütze 
Ludwig Maximilian University of Munich</p>
<p>Volker Tresp 
Ludwig Maximilian University of Munich</p>
<p>Yunpu Ma 
Ludwig Maximilian University of Munich</p>
<p>Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning
29 Aug 20254276B95CAE4EA7C9D0CFDE4761CF6A6DarXiv:2508.19828v2[cs.CL]
Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of NLP tasks, but they remain fundamentally stateless, constrained by limited context windows that hinder long-horizon reasoning.Recent efforts to address this limitation often augment LLMs with an external memory bank, yet most existing pipelines are static and heuristic-driven, lacking any learned mechanism for deciding what to store, update, or retrieve.We present Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the ability to actively manage and utilize external memory through two specialized agents: a Memory Manager that learns to perform structured memory operations, including adding, updating, deleting, or taking no operation on memory entries; and an Answer Agent that selects the most relevant entries and reasons over them to produce an answer.Both agents are fine-tuned with outcome-driven RL (PPO and GRPO), enabling adaptive memory management and utilization with minimal supervision.With as few as 152 question-answer pairs and a corresponding temporal memory bank for training, Memory-R1 outperforms the strongest existing baseline and demonstrates strong generalization across diverse question types and LLM backbones.Beyond presenting an effective approach, this work provides insights into how RL can unlock more agentic, memory-aware behavior in LLMs, pointing toward richer, more persistent reasoning systems.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have shown remarkable ability in understanding and generating natural language, making them central to recent advances in AI (OpenAI et al. 2024;Qwen et al. 2025).Yet, they remain fundamentally stateless (Yu, Lin, and Li 2025;Fan et al. 2025;Goodyear, Guo, and Johari 2025): each incoming query is processed independently of other interactions, because LLMs are constrained by a finite context window that prevents them from retaining and leveraging information across long conversations or evolving tasks (Wang et al. 2024;Fei et al. 2023).</p>
<p>To overcome these limits, recent work augments LLMs with external memory modules (Zhang et al. 2024).Most adopt a retrieval-augmented generation (RAG) * Equal contribution † Corresponding author paradigm (Pan et al. 2025;Salama et al. 2025), appending retrieved memory entries to the model's input prompt.While this extends access to past information, it also creates a fundamental retrieval challenge: heuristics may return too few entries, omitting crucial context, or too many, flooding the model with irrelevant information and degrading performance (Liu et al. 2023).In this paradigm, retrieved memories are passed to the LLM without meaningful filtering or prioritization, forcing the model to reason over both relevant and irrelevant content, which makes it prone to distraction by noise.Humans, by contrast, retrieve broadly but then filter selectively, integrating only the most useful pieces to maintain coherent, evolving knowledge.</p>
<p>Equally important is the challenge of memory management: deciding what to remember, update, or discard.Some systems (Packer et al. 2023;Modarressi et al. 2024;Xiong et al. 2025) adopt CRUD-style operations-create, read, update, and delete-which were originally introduced in database systems (Martin 1983;Sulemani 2021).A more recent work (AIOS Foundation 2024) augments this paradigm with a search operator, while Mem0 (Chhikara et al. 2025) investigates the operator set {ADD, UPDATE, DELETE, NOOP}.We adopt this setting, as it provides a minimal yet expressive framework for modeling memory dynamics.However, existing approaches mainly rely on vanilla LLMs to choose operations from in-context instructions without any learning signal tied to correctness (Packer et al. 2023;Chhikara et al. 2025).Even simple cases can fail.Figure 1, a simplified example drawn from a LOCOMO conversation (Maharana et al. 2024a), shows how a user first says "I adopted a dog named Buddy" and later adds "I adopted another dog named Scout".A vanilla system misinterprets this as a contradiction, issuing DELETE+ADD and overwriting the original memory.A trained agent, however, consolidates with an UPDATE: "Andrew adopted two dogs, Buddy and Scout."Appendix A.1 provides a real dialogue trace illustrating this case in practice.</p>
<p>These challenges of retrieving and managing memory remain largely unsolved.Supervised fine-tuning provides limited help because it is impractical to label every memory operation or retrieval decision.Reinforcement learning (RL), by contrast, has recently shown strong potential for aligning LLM behavior with high-level objectives, includ-Figure 1: Comparison of Memory-R1 and a vanilla LLM memory system.(Left) In a multi-session dialogue, the user mentions the adoption of two dogs in separate sessions.(Middle) The vanilla Memory Manager misinterprets the second adoption as a contradiction and issues DELETE+ADD, fragmenting memory.(Right) The RL-trained Memory Manager issues a single UPDATE to consolidate the memory, and the Answer Agent applies Memory Distillation: from 60 memories retrieved via RAG (e.g., <Memory 1> "Andrew adopted 2 dogs named Buddy and Scout"; <Memory 2> "Andrew feels a bit jealous of Audrey's dogs"; etc.), answer agent first filters the memories that are truly useful to answer the question, which is <Memory 1>, then reasons over the selected entry to produce the correct answer ("2 dogs").ing tool use (Qian et al. 2025;Wang et al. 2025), web navigation (Wei et al. 2025), and search optimization (Jin et al. 2025;Song et al. 2025).Building on this success, we argue that RL is the missing ingredient for adaptive memory in LLM agents.By optimizing outcome-based rewards, models can learn when to add, update, delete, or retain information and how to use retrieved memories for reasoning.</p>
<p>In this paper, we present Memory-R1, an RL finetuned, memory-augmented LLM framework with two specialized agents: (1) a Memory Manager that learns to perform structured memory operations, including adding, updating, deleting, and taking no operation, to maintain and evolve the memory bank, and (2) an Answer Agent that applies a Memory Distillation policy to filter memories retrieved via Retrieval-Augmented Generation (RAG) and reason over the selected entries to produce answers.Both agents are fine-tuned using PPO (Schulman et al. 2017) or GRPO (Shao et al. 2024), achieving strong performance with as few as 152 question-answer pairs.On the LOCOMO benchmark (Maharana et al. 2024a), Memory-R1 delivers substantial gains over the most competitive baseline, Mem0 (Chhikara et al. 2025).Using the LLaMA-3.1-8B-Instructbackbone, Memory-R1-GRPO improves overall F1 from 30.41 to 45.02 (+14.61absolute, 48% relative), BLEU-1 from 22.22 to 37.51 (+15.29 absolute, +69% relative), and LLM-as-a-Judge from 45.68 to 62.74 (+17.06 absolute, +37% relative).</p>
<p>These improvements set a new state of the art on LO-COMO and underscore Memory-R1's ability to achieve large performance gains with minimal supervision, highlighting its efficiency.</p>
<p>Our contributions are summarized as follows:</p>
<p>• We introduce Memory-R1, the first RL framework for memory-augmented LLMs, consisting of a Memory Manager to perform structured memory operations and an Answer Agent to filter and reason over memories retrieved via RAG.• We develop a data-efficient fine-tuning strategy using PPO and GRPO that enables Memory-R1 to achieve strong performance with as few as 152 question-answer pairs, demonstrating that large memory improvements can be achieved with minimal supervision.• We provide in-depth analysis of RL choices, model size, and memory design, offering actionable insights for building the next generation of memory-aware, reasoning-capable LLM agents.</p>
<p>2 Related Work</p>
<p>Memory Augmented LLM-based Agents</p>
<p>LLMs have emerged as powerful general-purpose reasoners, capable of engaging in multi-turn dialogues, decomposing tasks into actionable steps, and leveraging prior context to guide decision making (Brown et al. 2020;Chowdhery et al. 2022;OpenAI et al. 2024).Building on these capabilities, LLM-based agents have been proposed to solve com-plex tasks in interactive settings, such as conversational assistance (Thoppilan et al. 2022;Ouyang et al. 2022), toolaugmented reasoning (Schick et al. 2023;Yao et al. 2023), and autonomous decision making in structured or partially observable environments (Park et al. 2023;Wang et al. 2023;Shinn et al. 2023).</p>
<p>To address the limitations of fixed-length context windows and short-term memory, recent works have explored augmenting LLM agents with external memory modules.These memory-augmented agents aim to support longhorizon reasoning and persistent knowledge accumulation by selectively storing, retrieving, and updating relevant information.Notable approaches include LOCOMO (Maharana et al. 2024b), a benchmark model that evaluates agents' ability to retrieve and reason over temporally distant conversational history; ReadAgent (Lee et al. 2024), which incorporates retrieval mechanisms for memory-grounded dialogue; MemoryBank (Zhong et al. 2024), a compositional memory controller for lifelong agent memory; MemGPT (Packer et al. 2023), which introduces working and long-term memory buffers with memory scheduling policies; and A-Mem (Xu et al. 2025), a hybrid architecture that combines dynamic memory access with reinforcement learning(RL).These systems represent key steps toward building LLM agents with scalable, interpretable, and persistent memory capabilities.For a comprehensive overview of memory in AI agents, we refer readers to the recent survey (Du et al. 2025).</p>
<p>LLM and Reinforcement Learning</p>
<p>The intersection of LLM and RL has received increasing attention as researchers seek to move beyond static supervised fine-tuning and enable models to learn from dynamic, interactive feedback.Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al. 2022) is a foundational method used to align LLM outputs with human preferences.Recent works extend RL to more structured decisionmaking tasks for LLMs.For instance, Toolformer (Schick et al. 2023) and ReAct-style agents (Yao et al. 2023) frame tool use as an RL problem, where the LLM learns when to query external tools or APIs.Search-R1 (Jin et al. 2025) trains LLMs to issue web search queries using RL to maximize final answer correctness.Similarly, the Trial and Error approach (Song et al. 2024) optimizes agents to select better reasoning paths.These approaches demonstrate that RL can improve complex behavior sequences in LLMs.However, memory management and utilization in LLMs remain underexplored in the RL setting.Existing memory-augmented LLM systems (Chhikara et al. 2025;Packer et al. 2023;Lee et al. 2024) typically rely on heuristics to control memory operations, lacking adaptability and long-term optimization.Our work, Memory-R1, is among the first to frame memory operation selection, and the utilization of relevant memories as an RL problem.</p>
<p>Method</p>
<p>In this section, we describe the Memory-R1 approach for multi-session dialogue tasks, where each dialogue contains Algorithm 1: Memory-R1 Pipeline for Memory Manager for each tuple (ds, M, q i , a i ) ∈ D do 5:
for d i ∈ ds do 6: Facts Extraction: f i ← LLMExtract(d i ) 7: Memory Retrieval: M ret ← RAG(f i , M ) 8: Determine operation: o i ∼ L m (f i , M ret ) 9: if o i = ADD then 10: M ← M ∪ {f i } 11: else if o i = UPDATE then 12: m j ← Merge(m j , f i ) 13: else if o i = DELETE then 14: M ← M \ {m j } 15: else if o i = NOOP then 16: M ← M 17: end if 18: end for 19: Get Context: C ret ← RetrieveTopK(q i , M ) 20: Get Response: r i ∼ L a (q i , C ret ) 21: Policy Update: L m ← RL step (L m , q i , r i , F), 22:
where RL ∈ {P P O, GRP O} return L m 25: end procedure multiple sessions (separate interactions occurring at different times) and each session consists of several turns (a back-and-forth exchange between two users).Answering a question often requires synthesizing information spread across these sessions, posing a strong challenge for long-horizon memory management and reasoning.We first present the overall pipeline (Section 3.1), followed by RL fine-tuning of the Memory Manager (Section 3.2) and the Answer Agent (Section 3.3).</p>
<p>Memory-R1 Pipeline</p>
<p>This section provides an overview of the Memory-R1 pipeline.Figure 2 outlines the pipeline.At each dialogue turn, an LLM determines information that is worth to remember and summarize it into a piece of information.Then it triggers a retrieval step using Retrieval-Augmented Generation (RAG) to locate related entries in the memory bank.The Memory Manager then decides whether to ADD, UPDATE, DELETE, or NOOP, maintaining and evolving the memory state.</p>
<p>For question answering, the user's query retrieves up to 60 candidate memories, following the setup of (Chhikara et al. 2025).The Answer Agent applies a Memory Distillation policy to filter the most relevant entries and generate an answer.Both agents are fine-tuned with PPO and GRPO, enabling outcome-driven learning of memory operations and selective utilization.</p>
<p>RL Fine-tuning for Memory Manager Agent</p>
<p>Task Formulation The Memory Manager maintains and updates the memory bank by selecting the appropriate operation from {ADD, UPDATE, DELETE, or NOOP} for each new piece of information extracted from a dialogue.It outputs both the operation and the updated memory content m ′ .</p>
<p>Training uses (i) a partially constructed memory bank and (ii) a new dialogue turn containing information relevant to downstream question answering.The Memory Manager learns which operation produces a memory state that enables the Answer Agent to answer correctly.Training data is drawn from the LOCOMO dataset to ensure realistic multisession dialogue settings (details in Appendix B).</p>
<p>As shown in Algorithm 1, the Memory Manager is formalized as a policy π θ that takes the extracted information x and retrieved memories M old as input, and outputs an operation o ∈ {ADD, UPDATE, DELETE, NOOP} with associated content m ′ :
(o, m ′ ) ∼ π θ (• | x, M old ) (1)
where x is the extracted information and M old the current memory bank.</p>
<p>PPO for Memory Manager</p>
<p>We fine-tune the Memory Manager using Proximal Policy Optimization (PPO, Schulman et al. 2017).Given a candidate memory x and an existing memory bank M old , the Memory Manager outputs a memory operation o along with content m ′ .These actions are sampled from the current policy π θ and applied to update the memory bank, which is then passed to the frozen Answer Agent.The correctness of the answer provides the reward signal.PPO optimizes a clipped surrogate objective to stabilize training:
J (θ) = E [min (ρ θ A, clip(ρ θ , 1 − ϵ, 1 + ϵ)A)] ,(2)
where Mold) is the importance ratio between the new and old policies, and A is the advantage computed from the improvement in the Answer Agent's final answer accuracy.The clipping range [1 − ϵ, 1 + ϵ] constrains policy updates and ensures stable learning.
ρ θ = π θ (o,m ′ |x,Mold) πold(o,m ′ |x,</p>
<p>GRPO for Memory Manager</p>
<p>We also apply Group Relative Policy Optimization (GRPO, Shao et al. 2024) as an alternative to PPO for training the Memory Manager.GRPO samples a group of G candidate actions and assigns relative advantages within the group, eliminating the need for a learned value function while preserving PPO-style stability.</p>
<p>Given a state s = (x, M old ), the old policy π old generates G candidate actions.The GRPO objective is:
J (θ) = E 1 G G i=1 ρ (i) θ A i − β D KL π θ ∥ π ref ,(3)
where ρ
(i)
θ follows the same importance ratio definition as in PPO, now applied per sampled action.The group-relative advantage is computed by standardizing the QA rewards of all sampled actions:
A i = r i − mean(r) std(r) , r = {r 1 , . . . , r G }.(4)
The KL term D KL π θ ∥ π ref constrains updates, preventing drift from the reference policy.</p>
<p>Reward Design for Memory Manager</p>
<p>We adopt an outcome-driven reward design for the Memory Manager.At each step, the Memory Manager selects an operation o and proposes new content m ′ for the memory bank.Rather than scoring individual edits, we evaluate the updated memory bank by its impact on downstream question answering: the Answer Agent can only produce the correct answer if the Memory Manager's operations were effective.The reward is defined as:
R answer = EM(y pred , y gold ) (5)
where y pred is the Answer Agent's prediction after the update and y gold is the ground-truth answer.This outcome-based signal eliminates the need for manual memory relevance labels and keeps supervision simple and scalable.Freezing the Answer Agent during training avoids attribution ambiguity, and despite the indirect signal, exact-match rewards alone are sufficient to teach the Memory Manager effective memory operations.</p>
<p>RL Fine-tuning for Answer Agent</p>
<p>Task Formulation The Answer Agent leverages a memory bank curated by the Memory Manager to handle questions in long-horizon, multi-session dialogues.Following (Chhikara et al. 2025), for each question, 60 candidate memories are retrieved via a similarity-based RAG search using the question as the query.The agent then performs memory distillation: it filters this retrieved memory set to surface the most relevant entries and generates the final answer conditioned on these distilled memories.We model the Answer Agent's behavior as a policy π θ that maps the question q and retrieved memories M ret to an answer y:
y ∼ π ans (• | q, M ret ),(6)
where q is the current question and M ret is the subset of the memory bank retrieved by RAG.</p>
<p>PPO for Answer Agent</p>
<p>We fine-tune the Answer Agent using the same PPO framework as in Section 3.1.While the optimization is identical, the task differs: the Answer Agent takes the current question q and retrieved memories M ret and generates an answer y token-by-token.</p>
<p>The PPO objective mirrors Equation (2), applied over the generated answer sequence.The importance ratio is:
ρ θ (q, M ret ) = π θ (y | q, M ret ) π old (y | q, M ret ) , (7)
where y is the full generated answer.Advantages are computed from final answer quality (e.g., exact match with the reference), and clipping keeps updates within a trust region for stability.</p>
<p>GRPO for Answer Agent</p>
<p>We also fine-tune the Answer Agent with GRPO, following the formulation in Section 3.1.For each input (q, M ret ), the policy samples G candidate answers {y i } G i=1 .Their Exact Match scores against the ground-truth answer y gt are normalized into group-relative advantages.GRPO reuses the same importance ratio definition as PPO, applied per candidate answer.By comparing candidates within each group, GRPO provides stable gradient signals without a learned value function, improving sample efficiency and robustness during RL fine-tuning.</p>
<p>Reward Design for Answer Agent We adopt a simple yet effective reward function for the Answer Agent, using the Exact Match (EM) score between the generated answer and the ground-truth answer as the primary reward signal:
R answer = EM(y pred , y gold ) (8)
This design directly ties the reward to the correctness of the final answer, encouraging the agent to select and reason over memories in a way that yields accurate outputs rather than optimizing for intermediate steps.</p>
<p>4 Experiments an open-source memory framework that links memory chains across sessions to support long-context reasoning;</p>
<p>(5) Mem0 (Chhikara et al. 2025), a modular memory system with explicit in context memory operations designed for scalable deployment.</p>
<p>For a fair comparison, we re-implemented all baselines using both the LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instructmodels as backbones, with temperature set to 0 and a maximum token limit of 2048.This consistent setup ensures reproducibility and allows us to assess how each method performs across different model architectures.</p>
<p>Implementation Details We fine-tune Memory-R1 using both the LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instructmodels to evaluate the framework's robustness across different architectures.The prompts for memory operation and memory augmented answer generation are adapted from (Chhikara et al. 2025), with details provided in Appendix C. For PPO experiments, we train both actor and critic networks, while for GRPO runs, only the actor is trained since GRPO uses grouped return normalization in place of a critic.Both PPO and GRPO training are implemented using the VERL framework (Sheng et al. 2025).Experiments are performed on 4 NVIDIA H100 GPUs (80GB each) with a total batch size of 128 and a micro-batch size of 2 per GPU.The maximum prompt and response lengths are set to 4096 and 2048 tokens, respectively.The actor and critic (for PPO) are optimized with learning rates of 1×10 −6 and 1 × 10 −5 , using a constant warmup schedule.During RL training, we use a decoding temperature of τ = 1.0 to encourage exploration and capture a wider range of reward signals, which helps stabilize policy learning.For validation and testing, we switch to greedy decoding (τ = 0) to eliminate randomness and ensure consistent metric tracking.Each configuration and model variant is evaluated three times, and we report the mean score to reduce variance and provide a more reliable estimate of performance.</p>
<p>Main Results</p>
<p>Table 1 reports the performance of Memory-R1 across LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instructmodels on the LOCOMO benchmark, covering diverse question types including Single Hop, Multi-Hop, Open Domain, and Temporal reasoning.We evaluate two variants of Memory-R1: one fine-tuned with PPO and another with GRPO, and benchmark them against leading memory-augmented baselines, including LOCOMO, LangMem, A-Mem, and Mem0.</p>
<p>Across both model families, Memory-R1 consistently sets a new state of the art.On LLaMA-3.1-8B,Memory-R1-GRPO delivers the strongest overall performance, improving F1 by 68.9%, B1 by 48.3%, and J by 37.1% over the strongest baseline Mem0.Similarly, Memory-R1-PPO also yields substantial improvements, raising overall F1, B1, and J scores by 47.9%, 35.3%, and 26.5%, respectively.</p>
<p>Notably, the benefits of Memory-R1 transfer robustly to a different backbone.When applied to Qwen-2.5-7B-Instruct,Memory-R1-GRPO again emerges as the top performer, surpassing Mem0 by margins of 57.3% (F1), 41.5% (B1), and 33.8% (J).PPO remains competitive, delivering strong gains over all non-RL baselines.</p>
<p>These consistent improvements across two distinct LLM backbones demonstrate that reinforcement learning(RL) is highly effective for teaching LLMs to manage and leverage long-term memory, independent of model architecture.Moreover, the gains are broad-based-spanning single-hop, multi-hop, open-domain, and temporal questions-highlighting Memory-R1 as a generalizable framework for building adaptive, memory-augmented LLMs capable of long-horizon reasoning.</p>
<p>Ablation Studies</p>
<p>We conduct ablation studies to examine the contribution of each component in Memory-R1, isolating the effects of the Memory Manager, the Answer Agent, and the Memory Distillation mechanism.We also compare the training dynamics of PPO and GRPO.</p>
<p>Effect of Memory Manager</p>
<p>To assess the benefit of our RL-fine-tuned Memory Manager, we compare it against the in-context memory manager, with both variants built on the LLaMA-3.1-8B-Instructbase model.As shown in Table 2, RL training delivers consistent gains: PPO improves overall F1 to 24.60, B1 to 32.55, and J to 59.37; GRPO further improves F1 to 24.91, B1 to 33.05, and J to 59.91.These results confirm that outcome-based RL enables the Memory Manager to execute more accurate and effective operations-reinforcing our central claim that memory control should be learned rather than scripted.3, RL significantly improves the performance of the vanilla LLM when used as the answer agent (baseline: F1 20.54, B1 26.73, J 47.82).PPO fine-tuning raises scores to F1 32.91, B1 41.05, and J 57.54, while GRPO delivers even greater gains, reaching F1 37.51, B1 45.02, and J 62.74.These results demonstrate that reward-driven fine-tuning substantially elevates answer quality beyond what static retrieval can achieve.A  3: Performance of the base LLaMA-3.1-8B-Instructmodel and its PPO-and GRPO-fine-tuned variants as the Answer Agent, with the Memory Manager fixed to LLaMA-3.1-8B-Instruct.detailed case study comparing the RL-fine-tuned Answer Agent to the vanilla LLM is provided in Appendix A.2.</p>
<p>Effect of Answer Agent As shown in Table</p>
<p>Effect of Memory Distillation</p>
<p>We evaluate the impact of the proposed memory distillation mechanism by comparing the GRPO-fine-tuned Answer Agent with and without distillation (Table 4).Without distillation, the agent consumes all retrieved memories; with distillation, it filters for the most relevant ones before answering.Memory distillation consistently improves performance: F1 rises from 34.37 to 37.51, BLEU-1 from 40.95 to 45.02, and LLM-as-a-Judge from 60.14 to 62.74.These gains show that filtering out irrelevant entries reduces noise and enables the agent to reason more effectively over high-quality, distilled context.</p>
<p>Method</p>
<p>RL-fine-tuned Answer Agent Gains More with Stronger Memory Manager</p>
<p>We examine whether the Answer Agent's improvements from RL fine-tuning and Memory Distillation depend on the quality of the Memory Manager. Figure 3 compares base, PPO-fine-tuned, and GRPO-fine-tuned Answer Agents paired with either a LLaMA 3.1-8B-Instruct Memory Manager (Figure 3a) or a stronger GPT-4o-mini Memory Manager (Figure 3b).Our RL fine-tuned Answer Agent yields larger gains with the stronger manager: F1 improves by 10.10 points with the LLaMA memory manager vs. 19.72 with GPT-4o-mini memory manager; BLEU-1 by 10.81 vs. 18.19; and LLM-as-a-Judge by 5.05 vs. 15.76.This demonstrates that Memory-R1's benefits compound with stronger upstream memory manager, showing the Answer Agent scales effectively with memory quality.</p>
<p>Comparison of RL Policies</p>
<p>We compare PPO and GRPO for training the Answer Agent, using exact match against ground-truth answers as the reward signal.As shown in Figure 4, GRPO exhibits faster initial convergence, likely due to its grouped return normalization providing stronger early guidance.However, as training progresses, both methods steadily improve and ultimately reach comparable final reward levels.</p>
<p>Figure 4: Training reward curves for PPO GRPO on the Answer Agent using exact match as the reward.GRPO converges faster initially, and both reach similar final rewards.</p>
<p>Conclusion</p>
<p>We introduced Memory-R1, a reinforcement learning (RL) framework that enables large language model agents to manage and utilize external memory effectively.By training two specialized agents, one for memory operations and another for selective reasoning over retrieved memories, our approach overcomes the limitations of static, heuristic-driven memory pipelines.On the LO-COMO benchmark, Memory-R1 outperforms strong baselines across F1, BLEU, and LLM-as-a-Judge scores, achieving these gains with as few as 152 training examples.Ablation studies further confirm that RL enhances every component of the system.Memory-R1 opens new directions for research on compositional memory architectures, long-term knowledge retention, and more agentic LLM behavior, underscoring RL as a path to adaptive memory capabilities.</p>
<p>A Case Study of Behavior of Agents and after Fine-tuning</p>
<p>A.1 From In-context Memory Manager to RL fine-tuned Memory Manager</p>
<p>To demonstrate how RL fine-tuning improves memory operations, we present two real representative example.In the first case, the user initially mentions adopting a dog named Buddy, and later states that they have adopted another dog named Scout.Dialogue Excerpts:</p>
<p>• Audrey: That's great news!What's the pup's name?</p>
<p>• Andrew: It took us a while to decide, but we ended up going with 'Scout' for our pup -it seemed perfect for their adventurous spirit.Vanilla Memory Manager Actions (failed): ** Operations Issued: ** • DELETE: "Andrew adopted a new dog from a shelter and named him Buddy because he is his buddy."• DELETE: "Andrew enjoys taking walks with his newly adopted dog, Buddy, as a way to spend quality time together and get fresh air."• ADD: "Andrew adopted a new dog and named the pup 'Scout' because the name seemed perfect for their adventurous spirit."</p>
<p>Memory-R1 Memory Manager Actions (successful):</p>
<p>** Operations Issued: ** • UPDATE: "Andrew adopted a new dog from a shelter and named him Buddy because he is his buddy."-&gt; "Andrew adopted a dog from a shelter and named him Buddy because he is his buddy, and later adopted another dog named Scout."</p>
<p>Discussion: The vanilla LLM-based memory manager misinterpreted the second adoption as a contradiction because it saw two separate statements: "adopted a dog named Buddy" and "adopted another dog named Scout", and assumed that the differing dog names implied the new statement replaced the old one.As a result, it deleted the first dog ("Buddy") and added the second ("Scout"), leaving the memory fragmented and inaccurate.In contrast, the RL fine-tuned Memory Manager recognized the events as complementary and consolidated them with a single UPDATE, maintaining a coherent memory bank and enabling the system to answer correctly.This case illustrates how reinforcement learning enhances the Agent to understand the nuance between ADD, UPDATE, and DELETE, allowing the model to handle overlapping or related information more intelligently.</p>
<p>To further demonstrate how RL fine-tuning improves memory operations, we present another real-world example from a user-agent dialogue involving Joanna's feelings about pets and her allergies, particularly toward turtles.Dialogue Excerpts:</p>
<p>• Joanna: I wish I wasn't allergic!I would get two turtles today if I could!I found out recently I'm allergic to cockroaches as well, so who knows if I'll ever get a pet.** Operations Issued: ** • UPDATE: "Joanna is allergic to most reptiles and animals with fur, which causes her face to get puffy and itchy.Despite this, she finds other ways to be happy."-&gt; "Joanna is allergic to most reptiles, including turtles and cockroaches, and animals with fur, which causes her face to get puffy and itchy.Additionally, she is also allergic to turtles and cockroaches, which affects her ability to have pets."• UPDATE: "Joanna finds turtles peaceful and believes they bring calm and joy." -&gt; "Joanna finds turtles peaceful and believes they bring calm and joy, but she is allergic to them."Discussion: The vanilla Memory Manager misinterpreted the new allergy information in two aspects.First, it viewed the difference between "allergic to most reptiles and animals with fur" and "allergic to turtles and cockroaches" as a contradiction in allergy type-failing to recognize the latter as a more specific elaboration rather than a conflicting statement.Second, it interpreted Joanna's expressed fondfor turtles as incompatible with her allergy to them, incorrectly assuming that emotional attachment and physical limitations cannot coexist.As a result, it issued a series of DELETE operations followed by a single ADD, discarding valuable emotional context such as Joanna's admiration for turtles and her general enthusiasm toward pets.In contrast, the RL fine-tuned Memory Manager recognized that these pieces of information were complementary: Joanna likes turtles but cannot keep them due to her allergies.It updated the relevant memories accordingly using targeted UPDATE operations, preserving both factual accuracy and emotional nuance.This case demonstrates how reinforcement learning equips the model to reason about overlapping and evolving information more intelligently, favoring memory consolidation over fragmentation.Use the Memory Manager to maintain an up-to-date memory bank across turns 5: end for 6: for each question q in d do 7:</p>
<p>B Data Construction</p>
<p>Use the question q as a query to retrieve the top 30 most relevant candidate memories for each participant from the memory bank 8:</p>
<p>Pair (i) the question q, (ii) the 60 retrieved memories, and (iii) the gold answer a gold 9:</p>
<p>Store the triplet as a single training tuple for Answer Agent fine-tuning 10: end for preceding 50 turns.The current turn t is then fused with this snapshot, and we annotate the correct memory operation (ADD, UPDATE, DELETE, or NOOP).Each annotated tuple (turn, temporal memory bank, QA) serves as a supervised signal for the Memory Manager to learn how to incrementally update the memory state.The details can be found in Algorithm 2.</p>
<p>Answer Agent Training Data.For each question q in LOCOMO, we retrieve 60 candidate memories using retrieval-augmented search (RAG) over the temporal memory bank.The retrieved set, paired with the question and its gold answer, becomes the training input for the Answer Agent, which learns to distill the relevant entries and generate a concise, correct response.</p>
<p>C Prompts</p>
<p>In developing our Memory Manager Prompt, answer generation agent prompt, and LLM-as-a-Judge prompt, we adapt elements from the prompt released by prior work (Packer et al. 2023;Chhikara et al. 2025)</p>
<p>C.1 Memory Manager Prompt</p>
<p>For training the Manager, we use a detailed prompt that instructs the model how to perform four memory operations: ADD, UPDATE, DELETE, and NOOP.The full prompt spans multiple figures for readability.</p>
<p>C.2 Answer Agent Prompt</p>
<p>We provide the full prompt used to instruct the Answer Agent in our case study.This prompt defines the reasoning process, memory selection criteria, and formatting requirements for the model's responses.Figure 7 shows the complete instructions, context, and representative retrieved memories.</p>
<p>C.3 LLM-as-a-Judge (J) Prompt</p>
<p>For evaluating the correctness of generated answers, we employ an LLM-as-a-Judge prompt.The judge model is asked to label each answer as CORRECT or WRONG based on comparison with the gold answer.The complete prompt template is shown in Figure 8.</p>
<p>D Alogirthm</p>
<p>The overall Memory-R1 pipeline contains two complementary procedures, outlined in Algorithm 4 and Algorithm 5. Algorithm 4 (Memory Bank Construction) governs how the system incrementally builds and refines the external memory bank as new dialogue turns arrive.For each dialogue input, an LLM extracts key information, retrieves semantically related entries from the memory bank via retrieval-augmented generation (RAG), and invokes the RL fine-tuned Memory Manager to classify the update action as one of {ADD, UPDATE, DELETE, NOOP}.Depending on the chosen action, the memory store is updated accordingly-either inserting a new entry, merging information into an existing one, pruning contradictory content, or leaving the memory unchanged.</p>
<p>Algorithm 5 (Memory-augmented Answer Generation) describes how the system leverages the constructed memory bank to generate answers.Given an incoming question, the model retrieves the top-k relevant memory candidates, concatenates them with the question to form a memoryaugmented prompt, and applies the Answer Agent's Memory Distillation policy to filter for the most relevant facts.The distilled memory context, along with the query, is then passed to the Answer Agent to produce the final response, which is added to the answer set.Together, these algorithms enable Memory-R1 to jointly manage memory and generate memory augmented answers.for each question q i ∈ Q do 5:</p>
<p>M ret ← RetrieveTopK(q i , M ) 6:
p i ← Concat(q i , R i )
▷ p i is the memory augmented prompt 7:</p>
<p>M distill , âi ← AnswerAgent(q i , M ret )</p>
<p>Figure 2 :
2
Figure 2: Overview of the Memory-R1 framework.Stage 1 (blue) constructs and updates the memory bank via the RL-fine-tuned Memory Manager, which chooses operations {ADD, UPDATE, DELETE, NOOP} for each new dialogue turn.Stage 2 (green) answers user questions via the Answer Agent, which applies a Memory Distillation policy to reason over retrieved memories.</p>
<p>Figure 3 :
3
Figure 3: Performance gains of Answer Agent variants (Base/PPO fine-tuned/GRPO fine-tuned) when paired with different Memory Managers: (a) LLaMA 3.1-8B-Instruct and (b) the stronger GPT-4o-mini.</p>
<p>A. 2
2
From Vanilla LLM to Memory-Distilled RL Answer Agent To illustrate how RL fine-tuned Answer Agent with Memory Distillation improves answer accuracy, we compare the original model's output with the RL fine-tuned model on a representative example from LOCOMO.The prompt provided to the model is shown in Figure 7. Question: Does John live close to a beach or the mountains?Original Model Output (Before Fine-Tuning): ** Answer: ** He lives near mountains.Memory-R1 Output (After RL Fine-Tuning with Memory Distillation): ** Memories selected as relevant: ** • 8:30 pm on 1 January, 2023: John has a nostalgic memory of having a film camera as a kid and taking many pictures at the beach.• 1:24 pm on 25 May, 2023: John shared a picture of his family at the beach, expressing a sense of making a difference and commitment to continue their efforts.** Answer: ** beach Discussion: The original model consumed all retrieved memories indiscriminately and defaulted to "mountains," likely influenced by irrelevant mentions of mountaineering.In contrast, Memory-R1 filtered out distractors, surfaced only beach-related memories, and generated the correct answer.This case highlights how Memory Distillation helps the model discard noise, focus on true signals, and improve factual accuracy.</p>
<p>Algorithm 4 :
4
Memory Bank Construction via Memory Manager 1: Input: Multi-turn Dialogue D = {t 1 , t 2 , . . ., t n }, Initial empty memory bank M 2: Output: Updated memory bank M 3: procedure CONSTRUCTMEMORYBANK(D, M ) 4: for each dialogue turn t i ∈ D do 5:Extract key info:f i ← LLMExtract(t i ) 6: Retrieve memories:M ret ← RAG(f i , M ) MemoryManager(f i , M ret ) where o i ∈ {ADD, UPDATE, DELETE, NOOP} 9: if o i = ADD then 10: M ← M ∪ {f i } 11: else if o i = UPDATE then 12: m j ← merge(m j , f i ) 13: else if o i = DELETE then 14: M ← M \ {m j } 15:else if o i = NOOP then 16: Memory-augmented Generation via Answer Agent 1: Input: Question set Q = {q 1 , q 2 , . . ., q m }, Memory bank M 2: Output: Answer set Â 3: procedure GENERATEANSWERS(Q, M ) 4:</p>
<p>1 :
1
Input: Dataset D of tuples: dialogue turns ds, temp memory bank M , question-answer pairs (q i , a i ); Memory Manager LLM L m ; Answer LLM L a ; Reward Function F 2: Output: Fine-tuned Memory Manager LLM L m 3: procedure TRAINMEMORYMANAGER(D, L m , L a , F)
4:</p>
<p>Table 1 :
1
Evaluation results of Memory-R1 and baselines across LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instruct on the LO-COMO benchmark dataset.Models are evaluated on F1, BLEU-1 (B1), and LLM-as-a-Judge (J) across Single Hop, Multi-Hop, Open Domain, and Temporal questions.Higher is better.The best results are marked in Bold.
ModelMethodSingle HopMulti-HopOpen DomainTemporalOverallF1↑B1↑J↑F1↑B1↑J↑F1↑B1↑J↑F1↑B1↑J↑F1↑B1↑J↑LLaMA-3.1-8BInstructLOCOMO Zep A-Mem LangMem Mem012.25 30.15 21.62 22.40 27.299.77 17.15 16.93 15.21 18.6313.81 52.38 44.76 47.26 43.9313.69 15.04 13.82 18.65 18.5910.96 11.56 11.45 16.03 13.8620.48 33.33 34.93 39.81 37.3511.59 26.67 34.67 31.62 34.038.30 18.44 29.13 23.85 24.7715.96 45.36 49.38 48.38 52.279.38 3.49 25.77 27.75 26.908.15 2.68 22.14 21.53 21.064.65 27.58 36.43 30.94 31.4011.41 22.60 29.20 28.34 30.418.71 15.05 24.40 21.31 22.2213.62 42.80 44.76 44.18 45.68Memory-R1-PPO32.5224.4753.5626.8623.4742.1745.3039.1864.1041.5726.1147.6741.0532.9157.54Memory-R1-GRPO35.7327.7059.8335.6530.7753.0147.4241.2468.7849.8638.2751.5545.0237.5162.74LOCOMO9.577.0015.0611.8410.0219.288.676.5212.798.358.745.438.977.2712.17Qwen-2.5-7BInstructZep A-Mem LangMem Mem031.02 18.96 22.84 24.9621.39 12.86 16.98 18.0542.85 40.78 43.64 61.9220.42 14.73 18.98 20.3115.76 12.66 16.89 15.8223.81 31.32 44.38 48.1925.25 30.58 32.47 32.7421.34 26.14 25.98 25.2742.26 46.90 50.45 65.208.94 23.67 26.62 33.168.42 20.67 20.93 26.2829.31 28.68 23.08 38.7623.22 26.08 28.69 30.6118.78 21.78 22.76 23.5538.99 40.78 43.42 53.30Memory-R1-PPO34.2223.6157.7432.8729.4853.0144.7838.7266.9942.8830.3042.2541.7233.7059.53Memory-R1-GRPO33.6426.0662.3423.5520.7140.9646.8640.9267.8147.7538.4949.6143.1436.4461.51MethodF1↑B1↑J↑LLaMA3.1-8B26.73 20.54 47.82LLaMA3.1-8B + PPO32.55 24.60 59.37LLaMA3.1-8B + GRPO 33.05 24.91 59.91</p>
<p>Table 2 :
2
Performance of the LLaMA-3.1-8B-Instructmodel and its PPO-and GRPO-fine-tuned variants as the Memory Manager, with the answer agent fixed to LLaMA-3.1-8B-Instruct.
MethodF1↑B1↑J↑LLaMA3.1-8B26.73 20.54 47.82LLaMA3.1-8B + PPO34.48 28.13 49.04LLaMA3.1-8B + GRPO 37.54 30.64 52.87Table</p>
<p>Table 4 :
4
Performance comparison of GRPO fine-tuned Answer Agent with and without Memory Distillation policy using the LLaMA-3.1-8B-Instructmodel.
F1↑B1↑J↑</p>
<p>:</p>
<p>To train MEMORY-R1, we construct separate training data for the Memory Manager and the Answer Agent from the LOCOMO multi-turn dialogues.Input: LOCOMO multi-turn dialogues D 2: Output: Training tuples for the Memory Manager (dialogue turn, temporal memory bank, QA) 3: for each dialogue d ∈ D do
Algorithm 2: Data Construction for Memory-R1 Training14:for each turn t in d do5:Build a temporal memory bank using the pre-vious 50 turns with GPT-4o-mini6:Combine (i) the temporal memory bank, (ii) thecurrent turn t, and (iii) any QA pairs linked to t7:Store the combined package as a single trainingtuple8:end for9: end forAlgorithm 3: Data Construction for Answer Agent Training1: Input: LOCOMO multi-turn dialogues D, trainedMemory Manager2: Output: Training tuples for the Answer Agent(question, retrieved memories, gold answer)3: for each dialogue d ∈ D do4:Memory Manager Training Data. For every dialogue turnt, GPT-4o-mini builds a temporal memory bank from the
Memory Manager Prompt (Part 1): Overview and Instruction You are a smart memory manager which controls the memory of a system.You can perform four operations: (1) add into the memory, (2) update the memory, (3) delete from the memory, and (4) no change.Based on the above four operations, the memory will change.Compare newly retrieved facts with the existing memory.For each new fact, decide whether to:-ADD: Add it to the memory as a new element -UPDATE: Update an existing memory element -DELETE: Delete an existing memory element -NONE: Make no change (if the fact is already present or irrelevant) 1. ** Add ** : If the retrieved facts contain new information not present in the memory, then you have to add it by generating a new ID in the id field.-Example:Old Memory: [ {"id" : "0", "text" : "User is a software engineer"} ] Retrieved facts: ["Name is John"] New Memory: { "memory" : [ {"id" : "0", "text" : "User is a software engineer", "event" : "NONE"}, {"id" : "1", "text" : "Name is John", "event" : "ADD"} ] } 2. ** Update ** : If the retrieved facts contain information that is already present in the memory but the information is totally different, then you have to update it.If the retrieved fact contains information that conveys the same thing as the memory, keep the version with more detail.Example (a) { if the memory contains "User likes to play cricket" and the retrieved fact is "Loves to play cricket with friends", then update the memory with the retrieved fact.Example (b) { if the memory contains "Likes cheese pizza" and the retrieved fact is "Loves cheese pizza", then do NOT update it because they convey the same information.Important: When updating, keep the same ID and preserve old_memory.-Example:Old Memory: [ {"id" : "0", "text" : "I really like cheese pizza"}, {"id" : "2", "text" : "User likes to play cricket"} ] Retrieved facts: ["Loves chicken pizza", "Loves to play cricket with friends"] New Memory: { "memory" : [ {"id" : "0", "text" : "Loves cheese and chicken pizza", "event" : "UPDATE", "old_memory" : "I really like cheese pizza"}, {"id" : "2", "text" : "Loves to play cricket with friends", "event" : "UPDATE", "old_memory" : "User likes to play cricket"} ] } -Example:Old Memory: [ {"id" : "0", "text" : "Name is John"} ] Retrieved facts: ["Name is John"] New Memory: { "memory" : [ {"id" : "0", "text" : "Name is John", "event" : "NONE"} ] }Full Prompt and Retrieved MemoriesYou are an intelligent memory assistant tasked with retrieving information from conversation memories.# CONTEXT: You have access to memories from two speakers in a conversation.These memories contain timestamped information that may be relevant to answering the question.# INSTRUCTIONS:1. Carefully analyze all provided memories from both speakers 2. Pay special attention to the timestamps to determine the answer 3.If the question asks about a specific event or fact, look for direct evidence 4. If the memories contain contradictory information, prioritize the most recent memory 5.If there is a question about time references (like "last year", "two months ago"), calculate the actual date based on the memory timestamp.6. Always convert relative time references to specific dates, months, or years.7. Focus only on the content of the memories.Do not confuse character names 8.The answer should be less than 5-6 words.9. IMPORTANT: Select memories you found that are useful for answering the questions, and output it before you answer questions.10.IMPORTANT: Output the final answer after ** Answer: ** # APPROACH (Think step by step): 1. Examine all relevant memories 2. Examine the timestamps carefully 3. Look for explicit mentions that answer the question 4. Convert relative references if needed 5. Formulate a concise answer 6. Double-check the answer correctness 7. Ensure the final answer is specific 8.First output the memories that you found are important before you answer questions Memories for user John: -7:20 pm on 16 June, 2023: John has a special memory of a vacation to California where he experienced a gorgeous sunset and an enjoyable night strolling the shore, creating meaningful memories with loved ones.-6:13 pm on 10 April, 2023: John explored the coast in the Pacific Northwest and visited some national parks, finding the beauty of nature absolutely breathtaking.-3:14 pm on 13 August, 2023: John enjoys spending time outdoors with his family, including activities such as hiking, hanging out at the park, and having picnics.He also values indoor family activities like playing board games and having movie nights at home.... (In total 30 most relevant memories from John's Memory Bank are provided) ...Memories for user Maria:-6:29 pm on 7 July, 2023: John experienced a severe flood in his old area last week, which caused significant damage to homes due to poor infrastructure.-1:24 pm on 25 May, 2023: Maria appreciates the beauty of small, meaningful moments in life, as reflected in her reaction to a family beach photo shared by John.-3:14 pm on 13 August, 2023: Maria appreciates family bonding and is interested in the activities that John and his family enjoy doing together.... (In total 30 most relevant memories from Maria's Memory Bank are provided) ... Question: Does John live close to a beach or the mountains?LLM-as-a-Judge Prompt TemplateYour task is to label answer to a question as 'CORRECT' or 'WRONG'.You will be given the following data:(1) a question (posed by one user to another user),(2) a 'gold' (ground truth) answer, (3) a generated answer, which you will score as CORRECT or WRONG.The point of the question is to ask about something one user should know about the other user based on their prior conversations.The gold answer will usually be a concise and short answer that includes the referenced topic, for example: Question: Do you remember what I got the last time I went to Hawaii?Gold answer: A shell necklaceThe generated answer might be longer, but you should be generous with your grading | as long as it touches on the same topic as the gold answer, it should be counted as CORRECT.For time-related questions, the gold answer will be a specific date, month, or year.The generated answer might include relative references (e.g., "last Tuesday"), but you should be generous | if it refers to the same time period as the gold answer, mark it CORRECT, even if the format differs (e.g., "May 7th" vs. "7 May").Now it's time for the real question: Question: {question} Gold answer: {gold_answer} Generated answer: {generated_answer} First, provide a short (one sentence) explanation of your reasoning, then finish with CORRECT or WRONG.Do NOT include both CORRECT and WRONG in your response, or it will break the evaluation script.Return the label in JSON format with the key as "label".
AIOS Foundation. 2024. AIOS Agent SDK: Memory API. </p>
<p>T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, arXiv:2005.14165Language Models are Few-Shot Learners. 2020</p>
<p>P Chhikara, D Khant, S Aryan, T Singh, D Yadav, A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, P Schuh, K Shi, S Tsvyashchenko, J Maynez, A Rao, P Barnes, Y Tay, N Shazeer, V Prabhakaran, E Reif, N Du, B Hutchinson, R Pope, J Bradbury, J Austin, M Isard, G Gur-Ari, P Yin, T Duke, A Levskaya, S Ghemawat, S Dev, H Michalewski, X Garcia, V Misra, K Robinson, L Fedus, D Zhou, D Ippolito, D Luan, H Lim, B Zoph, A Spiridonov, R Sepassi, D Dohan, S Agrawal, M Omernick, A M Dai, T S Pillai, M Pellat, A Lewkowycz, E Moreira, R Child, O Polozov, K Lee, Z Zhou, X Wang, B Saeta, M Diaz, O Firat, M Catasta, J Wei, K Meier-Hellstern, D Eck, J Dean, S Petrov, N Fiedel, W Huang, D Zheng, Wang, arXiv:2504.19413arXiv:2505.00675Mem0: Building production-ready ai agents with scalable long-term memory. Du, Y; Montella, S2025. 2022arXiv preprintPaLM: Scaling Language Modeling with Pathways. Wong, K.-F.; and Pan, J. Z. 2025. Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions</p>
<p>If an LLM Were a Character, Would It Know Its Own Story? Evaluating Lifelong Learning in LLMs. S Fan, X Huang, Y Yao, X Fang, K Liu, P Han, S Shang, A Sun, Y Wang, W Fei, X Niu, P Zhou, L Hou, B Bai, L Deng, W Han, arXiv:2503.23514arXiv:2312.095712025. 2023arXiv preprintExtending Context Window of Large Language Models via Semantic Compression</p>
<p>L Goodyear, R Guo, R Johari, arXiv:2506.15624The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games. 2025arXiv preprint</p>
<p>Search-r1: Training llms to reason and leverage search engines with reinforcement learning. B Jin, H Zeng, Z Yue, J Yoon, S Arik, D Wang, H Zamani, J Han, arXiv:2503.09516.LangChain.20242025arXiv preprintLangMem: Modular memory for agentic systems</p>
<p>T.-W Lee, C Zhu, S Sun, Z Wang, H Xu, X V Lin, A H Awadallah, Z Liu, R Singh, P Liu, arXiv:2404.02396ReadAgent: Memory-Augmented Tool-Using LLM Agents with Long-Term Memory. 2024arXiv preprint</p>
<p>Evaluating Very Long-Term Conversational Memory of LLM Agents. N F Liu, K Lin, J Hewitt, A Paranjape, M Bevilacqua, F Petroni, P Liang, A Maharana, D.-H Lee, S Tulyakov, M Bansal, F Barbieri, Y Fang, arXiv:2307.03172Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2023. 2024a1Lost in the Middle: How Language Models Use Long Contexts</p>
<p>A Maharana, D.-H Lee, S Tulyakov, M Bansal, F Barbieri, Y Fang, arXiv:2402.17753Evaluating very longterm conversational memory of llm agents. 2024barXiv preprint</p>
<p>Managing the Data-base Environment. Englewood Cliffs. J Martin, 1983Prentice-HallNew Jersey</p>
<p>A Modarressi, A Köksal, A Imani, M Fayyaz, H Schütze, arXiv:2404.11672Memllm: Finetuning llms to use an explicit read-write memory. OpenAI2024arXiv preprint</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, R Avila, I Babuschkin, S Balaji, V Balcom, P Baltescu, H Bao, M Bavarian, J Belgum, I Bello, J Berdine, Bernadett-Shapiro, C Berner, L Bogdonoff, O Boiko, M Boyd, A.-L Brakman, G Brockman, T Brooks, M Brundage, K Button, T Cai, R Campbell, A Cann, B Carey, C Carlson, R Carmichael, B Chan, C Chang, F Chantzis, D Chen, S Chen, R Chen, J Chen, M Chen, B Chess, C Cho, C Chu, H W Chung, D Cummings, J Currier, Y Dai, C Decareaux, T Degry, N Deutsch, D Deville, A Dhar, D Dohan, S Dowling, S Dunning, A Ecoffet, A Eleti, T Eloundou, D Farhi, L Fedus, N Felix, S P Fishman, J Forte, I Fulford, L Gao, E Georges, C Gibson, V Goel, T Gogineni, G Goh, R Gontijo-Lopes, J Gordon, M Grafstein, S Gray, R Greene, J Gross, S S Gu, Y Guo, C Hallacy, J Han, J Harris, Y He, M Heaton, J Heidecke, C Hesse, A Hickey, W Hickey, P Hoeschele, B Houghton, K Hsu, S Hu, X Hu, J Huizinga, S Jain, S Jain, J Jang, Jiang, A.; Jiang, R.; Jin, H.; Jin, D.; Jomoto, S.; Jonn, B.; Jun, H.; Kaftan,. Łukasz Kaiser</p>
<p>. A Kamali, I Kanitscheider, N S Keskar, T Khan, L Kilpatrick, J W Kim, C Kim, Y Kim, J H Kirchner, J Kiros, M Knight, D Kokotajlo, A Łukasz Kondraciuk; Kondrich, A Konstantinidis, K Kosic, G Krueger, V Kuo, M Lampe, I Lan, T Lee, J Leike, J Leung, D Levy, C M Li, R Lim, M Lin, S Lin, M Litwin, T Lopez, R Lowe, P Lue, A Makanju, K Malfacini, S Manning, T Markov, Y Markovski, B Martin, K Mayer, A Mayne, B Mcgrew, S M Mckinney, C Mcleavey, P Mcmillan, J Mcneil, D Medina, A Mehta, J Menick, L Metz, A Mishchenko, P Mishkin, V Monaco, E Morikawa, D Mossing, T Mu, M Murati, O Murk, D Mély, A Nair, R Nakano, R Nayak, A Neelakantan, R Ngo, H Noh, L Ouyang, C O'keefe, J Pachocki, A Paino, J Palermo, A Pantuliano, G Parascandolo, J Parish, E Parparita, A Passos, M Pavlov, A Peng, A Perelman, F De Avila Belbute Peres, M Petrov, I Sohl, B Sokolowsky, Y Song, N Staudacher, F P Such, N Summers, I Sutskever, J Tang, N Tezak, M B Thompson, P Tillet, A Tootoonchian, E Tseng, P Tuggle, N Turley, J Tworek, J F C Uribe, A Vallone, A Vijayvergiya, C Voss, C Wainwright, J J Wang, A Wang, B Wang, J Ward, J Wei, C Weinmann, A Welihinda, P Welinder, J Weng, L Weng, M Wiethoff, D Willner, C Winter, S Wolrich, H Wong, L Workman, S Wu, J Wu, M Wu, K Xiao, T Xu, S Yoo, K Yu, Q Yuan, W Zaremba, arXiv:2303.08774Zellers, R.; Zhang, C.; Zhang, M.; Zhao, S.; Zheng, T.; Zhuang, J.; Zhuk, W.; and Zoph, B. 2024. GPT-4 Technical Report</p>
<p>L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, arXiv:2203.02155Training language models to follow instructions with human feedback. 2022</p>
<p>C Packer, V Fang, S Patil, K Lin, S Wooders, J Gonzalez, MemGPT: Towards LLMs as Operating Systems. 2023</p>
<p>Z Pan, Q Wu, H Jiang, X Luo, H Cheng, D Li, Y Yang, C.-Y Lin, H V Zhao, L Qiu, arXiv:2502.05589On memory construction and retrieval for personalized conversational agents. 2025arXiv preprint</p>
<p>J S Park, J C O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, C Qian, E C Acikgoz, Q He, H Wang, X Chen, D Hakkani-Tür, G Tur, H Ji, A Yang, B Yang, B Zhang, B Hui, B Zheng, B Yu, C Li, D Liu, F Huang, H Wei, H Lin, J Yang, J Tu, J Zhang, J Yang, J Yang, J Zhou, J Lin, K Dang, K Lu, K Bao, K Yang, L Yu, M Li, M Xue, P Zhang, Q Zhu, R Men, R Lin, T Li, T Tang, T Xia, X Ren, X Ren, Y Fan, Y Su, Y Zhang, Y Wan, Y Liu, Cui, arXiv:2304.03442arXiv:2412.15115Generative Agents: Interactive Simulacra of Human Behavior. 2023. 2025ToolRL: Reward is All Tool Learning Needs. and Qiu, Z. 2025. Qwen2.5 Technical Report</p>
<p>Zep: a temporal knowledge graph architecture for agent memory. P Rasmussen, P Paliychuk, T Beauvais, J Ryan, D Chalef, R Salama, J Cai, M Yuan, A Currey, M Sunkara, Y Zhang, Y Benajiba, arXiv:2501.13956arXiv:2503.21760MemInsight: Autonomous Memory Augmentation for LLM Agents. 2025. 2025arXiv preprint</p>
<p>T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, arXiv:2302.04761Toolformer: Language Models Can Teach Themselves to Use Tools. 2023</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Hybridflow: A flexible and efficient rlhf framework. Z Shao, P Wang, Q Zhu, R Xu, J Song, X Bi, H Zhang, M Zhang, Y Li, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024. 2025arXiv preprintProceedings of the Twentieth European Conference on Computer Systems</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>H Song, J Jiang, Y Min, J Chen, Z Chen, W X Zhao, L Fang, J.-R Wen, arXiv:2503.05592R1-searcher: Incentivizing the search capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents. Y Song, D Yin, X Yue, J Huang, S Li, B Y Lin, arXiv:2403.02502CRUD operations explained: Create, read, update, delete. 2024</p>
<p>R Thoppilan, D D Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Y Du, Y Li, H Lee, H S Zheng, A Ghafouri, M Menegali, Y Huang, M Krikun, D Lepikhin, J Chen, D Xu, Y Chen, Z Roberts, A Bosma, M Zhao, V Zhou, Y Chang, C.-C Krivokon, I Rusch, W Pickett, M Srinivasan, P Man, L Meier-Hellstern, K Morris, M R Doshi, T Santos, R D Duke, T Soraker, J Zevenbergen, B Prabhakaran, V Diaz, M Hutchinson, B Olson, K Molina, A Hoffman-John, E Lee, J Aroyo, L Rajakumar, R Butryna, A Lamm, M Kuzmina, V Fenton, J Cohen, A Bernstein, R Kurzweil, R Aguera-Arcas, B Cui, C Croak, M Chi, E Le, Q Yang, Y Li, R Sun, D Cai, R Zhang, Y Fu, C Floyd, L , arXiv:2201.08239arXiv:2404.04997Adapting LLMs for Efficient Context Processing through Soft Prompt Compression. Wang, C2022. 2024LaMDA: Language Models for Dialog Applications</p>
<p>Voyager: An Open-Ended Embodied Agent with Large Language Models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, arXiv:2305.162912023</p>
<p>H Wang, C Qian, W Zhong, X Chen, J Qiu, S Huang, B Jin, M Wang, K.-F Wong, H Ji, arXiv:2504.14870Acting Less is Reasoning More! Teaching Model to Act Efficiently. 2025</p>
<p>Z Wei, W Yao, Y Liu, W Zhang, Q Lu, L Qiu, C Yu, P Xu, C Zhang, B Yin, H Yun, L Li, arXiv:2505.16421WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning. 2025</p>
<p>Z Xiong, Y Lin, W Xie, P He, J Tang, H Lakkaraju, Z Xiang, arXiv:2505.16067How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior. 2025arXiv preprint</p>
<p>W Xu, K Mei, H Gao, J Tan, Z Liang, Y Zhang, arXiv:2502.12110A-MEM: Agentic Memory for LLM Agents. 2025</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.03629ReAct: Synergizing Reasoning and Acting in Language Models. 2023</p>
<p>Stateful large language model serving with pensieve. L Yu, J Lin, J Li, Proceedings of the Twentieth European Conference on Computer Systems. the Twentieth European Conference on Computer Systems2025</p>
<p>A survey on the memory mechanism of large language model based agents. Z Zhang, Q Dai, X Bo, C Ma, R Li, X Chen, J Zhu, Z Dong, J.-R Wen, ACM Transactions on Information Systems. 2024</p>
<p>Y Zhong, R Fan, Y Wu, T Hashimoto, Y Wu, P Liang, arXiv:2404.17445MemoryBank: Lifelong Memory for LLM Agents. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8338 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8338</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8338</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-280000176</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.19095v1.pdf" target="_blank">Baba is LLM: Reasoning in a Game with Dynamic Rules</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are known to perform well on language tasks, but struggle with reasoning tasks. This paper explores the ability of LLMs to play the 2D puzzle game Baba is You, in which players manipulate rules by rearranging text blocks that define object properties. Given that this rule-manipulation relies on language abilities and reasoning, it is a compelling challenge for LLMs. Six LLMs are evaluated using different prompt types, including (1) simple, (2) rule-extended and (3) action-extended prompts. In addition, two models (Mistral, OLMo) are finetuned using textual and structural data from the game. Results show that while larger models (particularly GPT-4o) perform better in reasoning and puzzle solving, smaller unadapted models struggle to recognize game mechanics or apply rule changes. Finetuning improves the ability to analyze the game levels, but does not significantly improve solution formulation. We conclude that even for state-of-the-art and finetuned LLMs, reasoning about dynamic rule changes is difficult (specifically, understanding the use-mention distinction). The results provide insights into the applicability of LLMs to complex problem-solving tasks and highlight the suitability of games with dynamically changing rules for testing reasoning and reflection by LLMs.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8338.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8338.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art reasoning-focused large language model from OpenAI; evaluated in this paper as the strongest performer on the Baba is You puzzle levels using prompt-based chain-of-thought techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language model optimized for reasoning and inference-time methods; described as able to reason at inference time and used as the top-performing LLM in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Baba is You</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based rule-manipulation puzzle requiring spatial reasoning about object positions and alignment of text blocks to form/break rules</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Levels encoded as ASCII grids; prompts supplied in three variants (simple, rule-extended with active rules, action-extended with permitted actions). Each prompt ends with a Plan-and-Solve (PS) sentence to elicit zero-shot chain-of-thought (CoT). Each model/prompt pair tested on 14 levels, five runs each; a solution counted as correct if it appeared in ≥3 runs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Plan-and-Solve zero-shot CoT prompting (PS sentence) and structured prompts (simple, rule-extended, action-extended). The model outputs reasoning chains split into interpretation, problem statement, solution formulation, and action sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Evaluated on 14 designed levels; each prompt/run repeated 5 times; correctness threshold ≥3/5. Quantitative success rates not listed in paper; qualitatively GPT-4o produced the most correct steps across levels and the highest correctness frequency in reasoning chains (figures show GPT-4o outperforms others).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Partial evidence: GPT-4o better identified objects/text blocks and formulated problem statements, and produced creative solutions (e.g., breaking/creating rules in non-trivial sequences). However, it frequently failed to correctly model 2D spatial constraints (mis-recognized vertically-aligned rules, suggested breaking unbreakable rules, and often produced action sequences inconsistent with its own solution text), indicating limited reliable spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Within this paper GPT-4o > Gemini 1.5-Flash > Mixtral/Mistral/OLMo on correctness frequency and number of correct reasoning steps. Externally, the paper cites other work where GPT-4 outperformed GPT-3.5 on Minesweeper but does not present direct head-to-head numeric comparisons for Baba is You.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails at interpreting some vertical rules, confuses breakable vs unbreakable rules, action sequences sometimes inconsistent with stated solution, overlooks 2D adjacency constraints; hallucinations appear with longer/action-extended prompts; no explicit numeric success rates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Baba is LLM: Reasoning in a Game with Dynamic Rules', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8338.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8338.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5-Flash</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 1.5-Flash (Google DeepMind)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cost-efficient, fast-inference reasoning LLM evaluated on the Baba is You levels; second-best in this study after GPT-4o but with inconsistent action formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 1.5-Flash</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer model optimized for fast inference and cost efficiency; used with the same prompt variants as other models in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5 (implied by name)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Baba is You</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based language-and-spatial puzzle requiring alignment-based rule creation/breaking</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same as other models: ASCII grid levels, three prompt types (simple, rule-extended, action-extended), Plan-and-Solve CoT trigger, 14 levels × 5 runs, success if solution appears in ≥3 runs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot Plan-and-Solve CoT prompting with structured prompts; generated step-by-step reasoning chains and action lists.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: performed slightly worse than GPT-4o in object/text identification and solution generation; benefited from action-extended prompt but showed more hallucinations in longer prompts. No numeric accuracy given.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Some evidence: improved step-by-step reasoning with action-extended prompts and occasionally produced creative multi-step rule changes, but action formulation unreliable and often inconsistent with problem analysis, indicating incomplete spatial understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared directly in paper to GPT-4o and smaller models; Gemini 1.5-Flash consistently behind GPT-4o but ahead of Mistral/OLMo in many metrics. Paper also notes external Minesweeper results where GPT-4 outperformed GPT-3.5 (not Gemini).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Incomplete identification of objects in the grid at times, hallucinations increased with action-extended prompts, inconsistent action sequences, rare but present suggestions to break unbreakable rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Baba is LLM: Reasoning in a Game with Dynamic Rules', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8338.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8338.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OLMo-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OLMo 13B instruct (AI2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source instruct-tuned model (13B) evaluated on Baba is You; produced many errors in grid interpretation and hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OLMo 13B instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer-based model trained on Dolma dataset; designed for research accessibility. Evaluated in prompt-based and finetuning conditions (13B used in prompt experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Baba is You</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based rule-manipulation puzzle requiring object/text-block recognition and spatial alignment reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>ASCII grid inputs with three prompt variants (simple, rule-extended, action-extended); Plan-and-Solve CoT trigger; 14 levels × 5 runs, correctness if ≥3 matches.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt-based CoT (Plan-and-Solve) and manual prompt engineering; for other OLMo variant (7B) LoRA finetuning was applied (see separate entry).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: high error frequency in reasoning chains, frequent misidentification of text blocks and objects, poor problem statements and solutions; no numeric success rates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Little evidence of correct spatial reasoning: often failed to distinguish objects vs text blocks, misread WIN as target object, and produced incoherent action plans that ignored 2D constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performed worse than GPT-4o and Gemini 1.5-Flash; similar to or worse than Mistral variants in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Frequent hallucinations, misclassification of grid elements, inability to consistently describe obstacles or active rules, treating text blocks as controllable objects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Baba is LLM: Reasoning in a Game with Dynamic Rules', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8338.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8338.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OLMo-7B (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OLMo 7B instruct (finetuned on Baba is You data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OLMo 7B after parameter-efficient finetuning (LoRA) on a combined dataset including game mechanics Q&A and handcrafted level solutions; showed improved problem-statement formulation but limited gains in solution/action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OLMo 7B (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 7B instruct model (OLMo) finetuned using LoRA on a combined dataset of CoT logic reasoning questions, ~289 game-mechanics Q&A, and 15 handcrafted level solutions to improve performance on Baba is You tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Baba is You</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid rule-manipulation requiring spatial alignment and language-level rule reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Finetuning performed with LoRA (parameter-efficient finetuning) on combined datasets; after finetuning evaluated with same prompt variants and CoT PS trigger on 14 levels × 5 runs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Supervised finetuning using three datasets (cot-logic-reasoning 10,500 Q, game-mechanics ~289 Q, 15 levels & answers) via LoRA; maintained Plan-and-Solve prompting at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>After finetuning, observed qualitative improvements in formulating problem statements and reduced classification errors for objects/text blocks (figures show higher correctness frequency in analysis steps), but no clear improvement in full solution generation; no numeric success rates reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Finetuning improved level-analysis (representation) but did not lead to reliable spatial action planning; model still rarely proposed breaking/creating rules as actions and sometimes treated text blocks as controllable objects.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Finetuned OLMo improved over its prefinetuned counterpart in analysis steps, but remained substantially behind GPT-4o and Gemini 1.5-Flash in overall problem-solving capability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvements concentrated on analysis; persisted failures in proposing correct action sequences, difficulty with use-vs-mention distinction, occasional treatment of text blocks as game objects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Baba is LLM: Reasoning in a Game with Dynamic Rules', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8338.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8338.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B instruct-tuned model evaluated on Baba is You; struggled with object/text recognition and rule-tracking but showed some gains after finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B parameter instruction-tuned transformer model used for prompt-based evaluation and subsequently finetuned with LoRA on combined datasets in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Baba is You</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based logic/spatial puzzle where aligning text changes object semantics</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Prompt-based evaluation with simple, rule-extended, and action-extended prompts and Plan-and-Solve CoT; then finetuned (LoRA) and re-evaluated on the same 14 levels × 5 runs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt-based CoT and supervised finetuning via LoRA on combined datasets (same as OLMo finetuning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Prompt-based: poor object/text recognition leading to many incorrect outputs. After finetuning: improved classification of objects and text blocks in the grid and fewer analysis errors, but no substantial improvement in generating correct full solutions; no numeric accuracy provided.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>After finetuning, better representation of grid elements (improved classification) but continued failures in reasoning about rule-breaking/formation actions; thus representation improved but spatial action planning remained weak.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performed worse than GPT-4o and Gemini 1.5-Flash; finetuned Mistral improved analysis relative to Mistral base but still inadequate at full solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Misclassification and incomplete problem statements pre-finetuning; post-finetuning still made wrong assumptions about grid-objects and often produced incomplete or incorrect solution steps; sometimes assumed movement required 'BABA IS MOVE'.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Baba is LLM: Reasoning in a Game with Dynamic Rules', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8338.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8338.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B (finetuned with LoRA on Baba is You data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mistral 7B finetuned using LoRA on combined reasoning and Baba is You datasets; reduced classification errors in grid parsing but did not reliably produce correct solution actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7B (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mistral 7B subjected to parameter-efficient finetuning (LoRA) using three combined datasets (10,500 CoT reasoning Qs, ~289 game-mechanics Qs, 15 handcrafted levels & answers) to improve performance on Baba is You.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Baba is You</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based rule rewriting puzzle requiring spatial alignment reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Finetuned model evaluated with rule-extended and action-extended prompts and Plan-and-Solve CoT across same 14 levels × 5 runs protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>LoRA finetuning on targeted datasets; retains prompt-based Plan-and-Solve CoT during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Post-finetune: observed qualitative improvement in classification of objects/text blocks and fewer analysis errors (figures), but no clear improvement in formulating or executing correct level solutions; no numeric success rates reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Improved representation suggests better spatial parsing, but continued inability to generate correct physical actions indicates limited spatial reasoning in planning and reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Improved relative to pre-finetuned Mistral-7B in analysis steps; still behind GPT-4o and Gemini 1.5-Flash in overall problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite better classification, still produced incorrect assumptions and incomplete or incorrect action sequences; did not reliably use rule-breaking/creation mechanics appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Baba is LLM: Reasoning in a Game with Dynamic Rules', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8338.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8338.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral 8x7B (Mixture-of-Experts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An MoE model that activates only a subset of parameters per inference (paper notes 12B active of 45B total); evaluated on Baba is You and produced intermediate performance, sometimes skipping problem statements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral 8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixture-of-Experts design activating 12B parameters per inference of a 45B total capacity to reduce compute; used as one of the evaluated LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>45B (12B activated per inference according to paper text)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Baba is You</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based, alignment-driven rule puzzle requiring spatial reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same prompt variants and Plan-and-Solve CoT; 14 levels × 5 runs with correctness if solution appears in ≥3 runs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt-based CoT; Mixtral occasionally omitted intermediate problem statement and jumped straight to solution (skipping context).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: performed better than some smaller models in object recognition but often omitted problem statements, leading to logical gaps; no numeric success rates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Limited evidence: when context was omitted, spatial reasoning and justification degraded; did not reliably track active rules or form feasible action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Generally behind GPT-4o and Gemini 1.5-Flash; mixed with Mistral family depending on level and prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Skipped context/problem-statement step frequently, produced logical gaps, misunderstood rule-breaking mechanism, suggested creating rules already present or impossible to form.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Baba is LLM: Reasoning in a Game with Dynamic Rules', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8338.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8338.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI) — mentioned in related work</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced in related work: prior studies evaluating LLMs on Minesweeper found GPT-4 outperformed GPT-3.5; GPT-4 is not directly evaluated on Baba is You in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based LLM from OpenAI noted in related literature for strong reasoning performance; cited as outperforming GPT-3.5 in a Minesweeper study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Minesweeper (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzle requiring spatial inference about adjacent cells and surface reasoning about mines distribution</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>In the cited work (Li et al. 2024) different input formats evaluated; not detailed in this paper beyond the claim that GPT-4 outperformed GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Not specified here; cited work likely used prompt/input-format variations and evaluation on Minesweeper tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports that Li et al. (2024) found GPT-4 outperformed GPT-3.5 on Minesweeper; no numeric values presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Referenced as demonstrating stronger Minesweeper performance, implying better handling of local adjacency/spatial inference in that task (details in cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to GPT-3.5 within the cited Minesweeper study where GPT-4 performed better.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in this paper; refer to Li et al. (2024) for details.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Baba is LLM: Reasoning in a Game with Dynamic Rules', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8338.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8338.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI) — mentioned in related work</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced in related work as being outperformed by GPT-4 on Minesweeper; not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior-generation OpenAI LLM referenced in related Minesweeper study as a comparator to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Minesweeper (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based adjacency puzzle requiring inference of mine locations from numbers</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Not detailed in this paper; referenced as part of external comparative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as worse than GPT-4 in Li et al. (2024); no numbers given here.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Implied lower Minesweeper performance than GPT-4; further details in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared (in cited work) to GPT-4 where GPT-4 outperformed GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed here; see Li et al. (2024).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Baba is LLM: Reasoning in a Game with Dynamic Rules', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8338.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8338.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 — mentioned in related work (Noever & Burdick 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Older transformer model referenced for prior work where GPT-2 was trained on solved examples to solve puzzles (mazes, Sudoku, Rubik's Cube) demonstrating a text-based alternative to search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Early OpenAI transformer model; cited in Noever & Burdick (2021) where it was trained on solved examples to tackle mazes, Sudoku and Rubik's Cube using text representations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Mazes, Sudoku, Rubik's Cube (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Various spatial puzzles: mazes (2D navigation), Sudoku (grid-based constraint puzzle), Rubik's Cube (3D spatial manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>In cited study GPT-2 was trained on solved examples, providing a text-based approach rather than traditional search; details not reproduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Training on solved examples (supervised) to map text-based puzzle descriptions to solutions; used as an existence proof of text-based approaches to spatial puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numeric metrics provided in this paper; cited to show feasibility of LLMs solving spatial puzzles with training.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Cited work shows GPT-2 can solve these puzzles when trained on solved examples, indicating learned mappings from textual representations to spatial solutions; specifics in original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Referenced historically as earlier approach; not compared experimentally within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in this paper; see Noever & Burdick (2021) for details.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Baba is LLM: Reasoning in a Game with Dynamic Rules', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study <em>(Rating: 2)</em></li>
                <li>Puzzle solving without search or human knowledge: An unnatural language approach <em>(Rating: 2)</em></li>
                <li>ChessGPT: Bridging policy learning and language modeling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8338",
    "paper_id": "paper-280000176",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "A state-of-the-art reasoning-focused large language model from OpenAI; evaluated in this paper as the strongest performer on the Baba is You puzzle levels using prompt-based chain-of-thought techniques.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Transformer-based large language model optimized for reasoning and inference-time methods; described as able to reason at inference time and used as the top-performing LLM in the study.",
            "model_size": null,
            "puzzle_name": "Baba is You",
            "puzzle_type": "2D grid-based rule-manipulation puzzle requiring spatial reasoning about object positions and alignment of text blocks to form/break rules",
            "task_setup": "Levels encoded as ASCII grids; prompts supplied in three variants (simple, rule-extended with active rules, action-extended with permitted actions). Each prompt ends with a Plan-and-Solve (PS) sentence to elicit zero-shot chain-of-thought (CoT). Each model/prompt pair tested on 14 levels, five runs each; a solution counted as correct if it appeared in ≥3 runs.",
            "mechanisms_or_strategies": "Plan-and-Solve zero-shot CoT prompting (PS sentence) and structured prompts (simple, rule-extended, action-extended). The model outputs reasoning chains split into interpretation, problem statement, solution formulation, and action sequence.",
            "performance_metrics": "Evaluated on 14 designed levels; each prompt/run repeated 5 times; correctness threshold ≥3/5. Quantitative success rates not listed in paper; qualitatively GPT-4o produced the most correct steps across levels and the highest correctness frequency in reasoning chains (figures show GPT-4o outperforms others).",
            "evidence_of_spatial_reasoning": "Partial evidence: GPT-4o better identified objects/text blocks and formulated problem statements, and produced creative solutions (e.g., breaking/creating rules in non-trivial sequences). However, it frequently failed to correctly model 2D spatial constraints (mis-recognized vertically-aligned rules, suggested breaking unbreakable rules, and often produced action sequences inconsistent with its own solution text), indicating limited reliable spatial reasoning.",
            "comparisons": "Within this paper GPT-4o &gt; Gemini 1.5-Flash &gt; Mixtral/Mistral/OLMo on correctness frequency and number of correct reasoning steps. Externally, the paper cites other work where GPT-4 outperformed GPT-3.5 on Minesweeper but does not present direct head-to-head numeric comparisons for Baba is You.",
            "limitations_or_failure_cases": "Fails at interpreting some vertical rules, confuses breakable vs unbreakable rules, action sequences sometimes inconsistent with stated solution, overlooks 2D adjacency constraints; hallucinations appear with longer/action-extended prompts; no explicit numeric success rates provided.",
            "uuid": "e8338.0",
            "source_info": {
                "paper_title": "Baba is LLM: Reasoning in a Game with Dynamic Rules",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Gemini-1.5-Flash",
            "name_full": "Gemini 1.5-Flash (Google DeepMind)",
            "brief_description": "A cost-efficient, fast-inference reasoning LLM evaluated on the Baba is You levels; second-best in this study after GPT-4o but with inconsistent action formulation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini 1.5-Flash",
            "model_description": "Large transformer model optimized for fast inference and cost efficiency; used with the same prompt variants as other models in the study.",
            "model_size": "1.5 (implied by name)",
            "puzzle_name": "Baba is You",
            "puzzle_type": "2D grid-based language-and-spatial puzzle requiring alignment-based rule creation/breaking",
            "task_setup": "Same as other models: ASCII grid levels, three prompt types (simple, rule-extended, action-extended), Plan-and-Solve CoT trigger, 14 levels × 5 runs, success if solution appears in ≥3 runs.",
            "mechanisms_or_strategies": "Zero-shot Plan-and-Solve CoT prompting with structured prompts; generated step-by-step reasoning chains and action lists.",
            "performance_metrics": "Qualitative: performed slightly worse than GPT-4o in object/text identification and solution generation; benefited from action-extended prompt but showed more hallucinations in longer prompts. No numeric accuracy given.",
            "evidence_of_spatial_reasoning": "Some evidence: improved step-by-step reasoning with action-extended prompts and occasionally produced creative multi-step rule changes, but action formulation unreliable and often inconsistent with problem analysis, indicating incomplete spatial understanding.",
            "comparisons": "Compared directly in paper to GPT-4o and smaller models; Gemini 1.5-Flash consistently behind GPT-4o but ahead of Mistral/OLMo in many metrics. Paper also notes external Minesweeper results where GPT-4 outperformed GPT-3.5 (not Gemini).",
            "limitations_or_failure_cases": "Incomplete identification of objects in the grid at times, hallucinations increased with action-extended prompts, inconsistent action sequences, rare but present suggestions to break unbreakable rules.",
            "uuid": "e8338.1",
            "source_info": {
                "paper_title": "Baba is LLM: Reasoning in a Game with Dynamic Rules",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "OLMo-13B",
            "name_full": "OLMo 13B instruct (AI2)",
            "brief_description": "An open-source instruct-tuned model (13B) evaluated on Baba is You; produced many errors in grid interpretation and hallucinations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OLMo 13B instruct",
            "model_description": "Open-source transformer-based model trained on Dolma dataset; designed for research accessibility. Evaluated in prompt-based and finetuning conditions (13B used in prompt experiments).",
            "model_size": "13B",
            "puzzle_name": "Baba is You",
            "puzzle_type": "2D grid-based rule-manipulation puzzle requiring object/text-block recognition and spatial alignment reasoning",
            "task_setup": "ASCII grid inputs with three prompt variants (simple, rule-extended, action-extended); Plan-and-Solve CoT trigger; 14 levels × 5 runs, correctness if ≥3 matches.",
            "mechanisms_or_strategies": "Prompt-based CoT (Plan-and-Solve) and manual prompt engineering; for other OLMo variant (7B) LoRA finetuning was applied (see separate entry).",
            "performance_metrics": "Qualitative: high error frequency in reasoning chains, frequent misidentification of text blocks and objects, poor problem statements and solutions; no numeric success rates provided.",
            "evidence_of_spatial_reasoning": "Little evidence of correct spatial reasoning: often failed to distinguish objects vs text blocks, misread WIN as target object, and produced incoherent action plans that ignored 2D constraints.",
            "comparisons": "Performed worse than GPT-4o and Gemini 1.5-Flash; similar to or worse than Mistral variants in this task.",
            "limitations_or_failure_cases": "Frequent hallucinations, misclassification of grid elements, inability to consistently describe obstacles or active rules, treating text blocks as controllable objects.",
            "uuid": "e8338.2",
            "source_info": {
                "paper_title": "Baba is LLM: Reasoning in a Game with Dynamic Rules",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "OLMo-7B (finetuned)",
            "name_full": "OLMo 7B instruct (finetuned on Baba is You data)",
            "brief_description": "OLMo 7B after parameter-efficient finetuning (LoRA) on a combined dataset including game mechanics Q&A and handcrafted level solutions; showed improved problem-statement formulation but limited gains in solution/action generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OLMo 7B (finetuned)",
            "model_description": "Open-source 7B instruct model (OLMo) finetuned using LoRA on a combined dataset of CoT logic reasoning questions, ~289 game-mechanics Q&A, and 15 handcrafted level solutions to improve performance on Baba is You tasks.",
            "model_size": "7B",
            "puzzle_name": "Baba is You",
            "puzzle_type": "2D grid rule-manipulation requiring spatial alignment and language-level rule reasoning",
            "task_setup": "Finetuning performed with LoRA (parameter-efficient finetuning) on combined datasets; after finetuning evaluated with same prompt variants and CoT PS trigger on 14 levels × 5 runs.",
            "mechanisms_or_strategies": "Supervised finetuning using three datasets (cot-logic-reasoning 10,500 Q, game-mechanics ~289 Q, 15 levels & answers) via LoRA; maintained Plan-and-Solve prompting at test time.",
            "performance_metrics": "After finetuning, observed qualitative improvements in formulating problem statements and reduced classification errors for objects/text blocks (figures show higher correctness frequency in analysis steps), but no clear improvement in full solution generation; no numeric success rates reported.",
            "evidence_of_spatial_reasoning": "Finetuning improved level-analysis (representation) but did not lead to reliable spatial action planning; model still rarely proposed breaking/creating rules as actions and sometimes treated text blocks as controllable objects.",
            "comparisons": "Finetuned OLMo improved over its prefinetuned counterpart in analysis steps, but remained substantially behind GPT-4o and Gemini 1.5-Flash in overall problem-solving capability.",
            "limitations_or_failure_cases": "Improvements concentrated on analysis; persisted failures in proposing correct action sequences, difficulty with use-vs-mention distinction, occasional treatment of text blocks as game objects.",
            "uuid": "e8338.3",
            "source_info": {
                "paper_title": "Baba is LLM: Reasoning in a Game with Dynamic Rules",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral 7B instruct",
            "brief_description": "A 7B instruct-tuned model evaluated on Baba is You; struggled with object/text recognition and rule-tracking but showed some gains after finetuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral 7B",
            "model_description": "7B parameter instruction-tuned transformer model used for prompt-based evaluation and subsequently finetuned with LoRA on combined datasets in this study.",
            "model_size": "7B",
            "puzzle_name": "Baba is You",
            "puzzle_type": "2D grid-based logic/spatial puzzle where aligning text changes object semantics",
            "task_setup": "Prompt-based evaluation with simple, rule-extended, and action-extended prompts and Plan-and-Solve CoT; then finetuned (LoRA) and re-evaluated on the same 14 levels × 5 runs.",
            "mechanisms_or_strategies": "Prompt-based CoT and supervised finetuning via LoRA on combined datasets (same as OLMo finetuning).",
            "performance_metrics": "Prompt-based: poor object/text recognition leading to many incorrect outputs. After finetuning: improved classification of objects and text blocks in the grid and fewer analysis errors, but no substantial improvement in generating correct full solutions; no numeric accuracy provided.",
            "evidence_of_spatial_reasoning": "After finetuning, better representation of grid elements (improved classification) but continued failures in reasoning about rule-breaking/formation actions; thus representation improved but spatial action planning remained weak.",
            "comparisons": "Performed worse than GPT-4o and Gemini 1.5-Flash; finetuned Mistral improved analysis relative to Mistral base but still inadequate at full solutions.",
            "limitations_or_failure_cases": "Misclassification and incomplete problem statements pre-finetuning; post-finetuning still made wrong assumptions about grid-objects and often produced incomplete or incorrect solution steps; sometimes assumed movement required 'BABA IS MOVE'.",
            "uuid": "e8338.4",
            "source_info": {
                "paper_title": "Baba is LLM: Reasoning in a Game with Dynamic Rules",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Mistral-7B (finetuned)",
            "name_full": "Mistral 7B (finetuned with LoRA on Baba is You data)",
            "brief_description": "Mistral 7B finetuned using LoRA on combined reasoning and Baba is You datasets; reduced classification errors in grid parsing but did not reliably produce correct solution actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral 7B (finetuned)",
            "model_description": "Mistral 7B subjected to parameter-efficient finetuning (LoRA) using three combined datasets (10,500 CoT reasoning Qs, ~289 game-mechanics Qs, 15 handcrafted levels & answers) to improve performance on Baba is You.",
            "model_size": "7B",
            "puzzle_name": "Baba is You",
            "puzzle_type": "2D grid-based rule rewriting puzzle requiring spatial alignment reasoning",
            "task_setup": "Finetuned model evaluated with rule-extended and action-extended prompts and Plan-and-Solve CoT across same 14 levels × 5 runs protocol.",
            "mechanisms_or_strategies": "LoRA finetuning on targeted datasets; retains prompt-based Plan-and-Solve CoT during evaluation.",
            "performance_metrics": "Post-finetune: observed qualitative improvement in classification of objects/text blocks and fewer analysis errors (figures), but no clear improvement in formulating or executing correct level solutions; no numeric success rates reported.",
            "evidence_of_spatial_reasoning": "Improved representation suggests better spatial parsing, but continued inability to generate correct physical actions indicates limited spatial reasoning in planning and reflection.",
            "comparisons": "Improved relative to pre-finetuned Mistral-7B in analysis steps; still behind GPT-4o and Gemini 1.5-Flash in overall problem solving.",
            "limitations_or_failure_cases": "Despite better classification, still produced incorrect assumptions and incomplete or incorrect action sequences; did not reliably use rule-breaking/creation mechanics appropriately.",
            "uuid": "e8338.5",
            "source_info": {
                "paper_title": "Baba is LLM: Reasoning in a Game with Dynamic Rules",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Mixtral-8x7B",
            "name_full": "Mixtral 8x7B (Mixture-of-Experts)",
            "brief_description": "An MoE model that activates only a subset of parameters per inference (paper notes 12B active of 45B total); evaluated on Baba is You and produced intermediate performance, sometimes skipping problem statements.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mixtral 8x7B",
            "model_description": "Mixture-of-Experts design activating 12B parameters per inference of a 45B total capacity to reduce compute; used as one of the evaluated LLMs.",
            "model_size": "45B (12B activated per inference according to paper text)",
            "puzzle_name": "Baba is You",
            "puzzle_type": "2D grid-based, alignment-driven rule puzzle requiring spatial reasoning",
            "task_setup": "Same prompt variants and Plan-and-Solve CoT; 14 levels × 5 runs with correctness if solution appears in ≥3 runs.",
            "mechanisms_or_strategies": "Prompt-based CoT; Mixtral occasionally omitted intermediate problem statement and jumped straight to solution (skipping context).",
            "performance_metrics": "Qualitative: performed better than some smaller models in object recognition but often omitted problem statements, leading to logical gaps; no numeric success rates provided.",
            "evidence_of_spatial_reasoning": "Limited evidence: when context was omitted, spatial reasoning and justification degraded; did not reliably track active rules or form feasible action sequences.",
            "comparisons": "Generally behind GPT-4o and Gemini 1.5-Flash; mixed with Mistral family depending on level and prompt.",
            "limitations_or_failure_cases": "Skipped context/problem-statement step frequently, produced logical gaps, misunderstood rule-breaking mechanism, suggested creating rules already present or impossible to form.",
            "uuid": "e8338.6",
            "source_info": {
                "paper_title": "Baba is LLM: Reasoning in a Game with Dynamic Rules",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4 (related work)",
            "name_full": "GPT-4 (OpenAI) — mentioned in related work",
            "brief_description": "Referenced in related work: prior studies evaluating LLMs on Minesweeper found GPT-4 outperformed GPT-3.5; GPT-4 is not directly evaluated on Baba is You in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Large transformer-based LLM from OpenAI noted in related literature for strong reasoning performance; cited as outperforming GPT-3.5 in a Minesweeper study.",
            "model_size": null,
            "puzzle_name": "Minesweeper (related work)",
            "puzzle_type": "Grid-based logic puzzle requiring spatial inference about adjacent cells and surface reasoning about mines distribution",
            "task_setup": "In the cited work (Li et al. 2024) different input formats evaluated; not detailed in this paper beyond the claim that GPT-4 outperformed GPT-3.5.",
            "mechanisms_or_strategies": "Not specified here; cited work likely used prompt/input-format variations and evaluation on Minesweeper tasks.",
            "performance_metrics": "Paper reports that Li et al. (2024) found GPT-4 outperformed GPT-3.5 on Minesweeper; no numeric values presented in this paper.",
            "evidence_of_spatial_reasoning": "Referenced as demonstrating stronger Minesweeper performance, implying better handling of local adjacency/spatial inference in that task (details in cited paper).",
            "comparisons": "Compared to GPT-3.5 within the cited Minesweeper study where GPT-4 performed better.",
            "limitations_or_failure_cases": "Not discussed in this paper; refer to Li et al. (2024) for details.",
            "uuid": "e8338.7",
            "source_info": {
                "paper_title": "Baba is LLM: Reasoning in a Game with Dynamic Rules",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-3.5 (related work)",
            "name_full": "GPT-3.5 (OpenAI) — mentioned in related work",
            "brief_description": "Referenced in related work as being outperformed by GPT-4 on Minesweeper; not evaluated in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5",
            "model_description": "Prior-generation OpenAI LLM referenced in related Minesweeper study as a comparator to GPT-4.",
            "model_size": null,
            "puzzle_name": "Minesweeper (related work)",
            "puzzle_type": "Grid-based adjacency puzzle requiring inference of mine locations from numbers",
            "task_setup": "Not detailed in this paper; referenced as part of external comparative evaluation.",
            "mechanisms_or_strategies": "Not specified in this paper.",
            "performance_metrics": "Reported as worse than GPT-4 in Li et al. (2024); no numbers given here.",
            "evidence_of_spatial_reasoning": "Implied lower Minesweeper performance than GPT-4; further details in cited work.",
            "comparisons": "Compared (in cited work) to GPT-4 where GPT-4 outperformed GPT-3.5.",
            "limitations_or_failure_cases": "Not discussed here; see Li et al. (2024).",
            "uuid": "e8338.8",
            "source_info": {
                "paper_title": "Baba is LLM: Reasoning in a Game with Dynamic Rules",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-2 (related work)",
            "name_full": "GPT-2 — mentioned in related work (Noever & Burdick 2021)",
            "brief_description": "Older transformer model referenced for prior work where GPT-2 was trained on solved examples to solve puzzles (mazes, Sudoku, Rubik's Cube) demonstrating a text-based alternative to search.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-2",
            "model_description": "Early OpenAI transformer model; cited in Noever & Burdick (2021) where it was trained on solved examples to tackle mazes, Sudoku and Rubik's Cube using text representations.",
            "model_size": null,
            "puzzle_name": "Mazes, Sudoku, Rubik's Cube (related work)",
            "puzzle_type": "Various spatial puzzles: mazes (2D navigation), Sudoku (grid-based constraint puzzle), Rubik's Cube (3D spatial manipulation)",
            "task_setup": "In cited study GPT-2 was trained on solved examples, providing a text-based approach rather than traditional search; details not reproduced in this paper.",
            "mechanisms_or_strategies": "Training on solved examples (supervised) to map text-based puzzle descriptions to solutions; used as an existence proof of text-based approaches to spatial puzzles.",
            "performance_metrics": "No numeric metrics provided in this paper; cited to show feasibility of LLMs solving spatial puzzles with training.",
            "evidence_of_spatial_reasoning": "Cited work shows GPT-2 can solve these puzzles when trained on solved examples, indicating learned mappings from textual representations to spatial solutions; specifics in original paper.",
            "comparisons": "Referenced historically as earlier approach; not compared experimentally within this paper.",
            "limitations_or_failure_cases": "Not discussed in this paper; see Noever & Burdick (2021) for details.",
            "uuid": "e8338.9",
            "source_info": {
                "paper_title": "Baba is LLM: Reasoning in a Game with Dynamic Rules",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study",
            "rating": 2,
            "sanitized_title": "assessing_logical_puzzle_solving_in_large_language_models_insights_from_a_minesweeper_case_study"
        },
        {
            "paper_title": "Puzzle solving without search or human knowledge: An unnatural language approach",
            "rating": 2,
            "sanitized_title": "puzzle_solving_without_search_or_human_knowledge_an_unnatural_language_approach"
        },
        {
            "paper_title": "ChessGPT: Bridging policy learning and language modeling",
            "rating": 1,
            "sanitized_title": "chessgpt_bridging_policy_learning_and_language_modeling"
        }
    ],
    "cost": 0.01687375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Baba is LLM: Reasoning in a Game with Dynamic Rules
23 Jun 2025</p>
<p>Fien Van Wetten 
LIACS
Leiden University
The Netherlands</p>
<p>Aske Plaat 
LIACS
Leiden University
The Netherlands</p>
<p>Max Van Duijn 
LIACS
Leiden University
The Netherlands</p>
<p>Baba is LLM: Reasoning in a Game with Dynamic Rules
23 Jun 202509EBE889F5EA5E8FD4C0C4DD47B9C15AarXiv:2506.19095v1[cs.AI]Large language modelsreasoningdynamic rule changesgames
Large language models (LLMs) are known to perform well on language tasks, but struggle with reasoning tasks.This paper explores the ability of LLMs to play the 2D puzzle game Baba is You, in which players manipulate rules by rearranging text blocks that define object properties.Given that this rule-manipulation relies on language abilities and reasoning, it is a compelling challenge for LLMs.Six LLMs are evaluated using different prompt types, including (1) simple, (2) ruleextended and (3) action-extended prompts.In addition, two models (Mistral, OLMo) are finetuned using textual and structural data from the game.Results show that while larger models (particularly GPT-4o) perform better in reasoning and puzzle solving, smaller unadapted models struggle to recognize game mechanics or apply rule changes.Finetuning improves the ability to analyze the game levels, but does not significantly improve solution formulation.We conclude that even for state-of-the-art and finetuned LLMs, reasoning about dynamic rule changes is difficult (specifically, understanding the use-mention distinction).The results provide insights into the applicability of LLMs to complex problem-solving tasks and highlight the suitability of games with dynamically changing rules for testing reasoning and reflection by LLMs.</p>
<p>Introduction</p>
<p>Artificial Intelligence (AI) has a long history in using games as benchmarks for reasoning, decision-making, and problem-solving capabilities [Campbell et al., 2002, Silver et al., 2016, 2018, Brown and Sandholm, 2018, Berner et al., 2019, Schrittwieser et al., 2020].This paper investigates the use of large language models (LLMs) in the 2D puzzle game Baba is You [Teikari, 2019].In this game players must alter rules by manipulating text blocks.Solving puzzles in this environment requires understanding how rule changes affect the game state and to apply that understanding dynamically, implying a form of reasoning in which the model should be able to reflect on the effects of its own actions.</p>
<p>LLMs, based on the transformer architecture [Vaswani et al., 2023] have demonstrated strong performance in natural language processing tasks including text generation, machine translation, conversational agents and code generation [Naveed et al., 2024].Techniques such as finetuning [Xu et al., 2023], reinforcement learning with human feedback (RLHF) [Chaudhari et al., 2024], and prompt-based learning [Kamath et al., 2024] have been developed to improve the performance of the transformer architecture.Beyond natural language processing, LLMs are also emerging as agents in games [Bakhtin et al., 2022, Topsakal et al., 2024, Marincioni et al., 2024, Müller-Brockhausen et al., 2023].</p>
<p>Baba is You is compelling because of its dynamic rule system relying on two-level language-based mechanics, which can be understood in terms of the classical mention versus use distinction [Wilson, 2017, Saka, 1998].Board games in general are based on pushing around pieces associated with a fixed meaning (mention).However, in Baba is You certain pieces can form a new game rule when they are aligned (use).Unlike games with fixed rules, Baba is You allows players to rewrite the logic of the game by manipulating the pieces.While LLM's strong language and general pattern-learning abilities suggest potential [Mirchandani et al., 2023], an initial study by Cloos et al. [2024] showed indeed that state-of-the-art LLMs struggle with the reasoning aspects of Baba is You, failing to generalize rule manipulation.</p>
<p>This paper evaluates how well LLMs solve Baba is You puzzles.We use two approaches: prompt-based learning and finetuning.We test six LLMs (GPT-4o, Gemini-Flash 1.5, OLMo 2 13B and 7B, Mistral 7B and Mixtral 8x7B) across three prompt types.We additionally finetune Mistral 7B and OLMo 7B using game data.Our contributions are as follows:</p>
<p>-Comparing different prompts in six LLMs, we find that prompt-based learning achieves weak results when dynamic rule changes are necessary-even for LLMs with enhanced reasoning capacities; -Using a dataset for finetuning, we find that finetuning on two open LLMs is able to improve performance somewhat; -Reasoning about dynamic rules changes, remains a challenging problem for current Reasoning LLMs; the deceptively simple puzzle game of Baba is You offers a challenging testbed for Reasoning LLMs.</p>
<p>All training scripts, prompts, and finetuning datasets of this work are publicly available [van Wetten, 2025a,c,b].</p>
<p>Related work</p>
<p>With the advent of LLMs, a new type of learning has emerged: prompt-based (or in-context) learning [Kamath et al., 2024].This type of learning occurs at inference time, using a structured prompt that includes a task description, optional examples, and a query.To enhance LLM reasoning, chain of thought (CoT) prompting was introduced, where the model is guided to generated intermediate steps before answering [Wei et al., 2022].In few-shot CoT prompts include a task or question, followed by a step-by-step reasoning example along with the final answer, and ending with a similar question or task.This approach showed better performance on complex reasoning tasks for large models.Kojima et al. [2022] proposed a zero-shot CoT template for reasoning, they unlock the reasoning step by adding the Let's think step by step sentence at the end of each prompt.Wang et al. [2023a] introduce plan and solve prompting (PS), another zeroshot CoT method.It prompts the model to first plan a solution and then execute it, using the sentences Let's first understand the problem and devise a plan to solve the problem.Then, let's carry out the plan and solve the problem step by step.They extend these sentences with more detailed instructions to reduce errors in the reasoning step.Further approaches on reasoning in LLMs can be found in [Chu et al., 2024, Dong et al., 2023, Huang and Chang, 2023, Plaat et al., 2024].Another approach to teaching a pretrained LLM to perform a new task is finetuning, where the parameters of the model are adjusted [Jeong, 2024].Finetuning adapts pretrained LLMs to specific task with smaller, domain-specific datasets.A common method is supervised finetuning (SFT), where labeled examples guide learning.Instruction-tuning, a variant of SFT, trains models on (Instruction, Output) pairs, where Instruction is a human instruction and Output is the desired response by the LLM for that instruction [Zhang et al., 2024].</p>
<p>Full finetuning updates all model parameters.This technique can be costly, as the pretrained model often contains billions of parameters, see early models such as GPT [Radford et al., 2018].Parameter-efficient finetuning (PEFT) offers a lighter alternative by modifying only a small subset of parameters [Han et al., 2024].A notable PEFT method is LoRA [Hu et al., 2021], which inserts low-rank matrices to approximate weight updates, reducing memory and compute costs.</p>
<p>The rise of AI agents achieving dominance in gaming begins in the 1990s with Deep Blue [Campbell et al., 2002], which defeated world chess champion Garry kasparov using brute-force search and domain expertise.Attention then shifted towards machine and reinforcement learning approaches [Silver et al., 2016, Plaat, 2020].Currently, agents and LLMs are converging [Plaat et al., 2025].In the field of AI agents playing games, ChessGPT [Feng et al., 2024] introduced a substantial game and language dataset for chess, upon which two models have been created.Li et al. [2024] evaluated LLMs in Minesweeper using different input formats, finding that GPT-4 outperformed GPT-3.5, although limitations remained.Noever and Burdick [2021] used GPT-2 to solve puzzles such as mazes, Sudoku and the Rubik's Cube, by training on solved examples, demonstrating a text-based alternative to traditional search methods.These studies highlight the potential of LLMs in solving puzzles and playing games.</p>
<p>Method</p>
<p>The game Baba is You [Teikari, 2019] is a 2D puzzle game with levels: a grid filled with objects and text blocks (see Figure 1).Text blocks can be used to create rules; these rules can be created from left to right or from top to bottom.The rule is active if there is at least one object, one verb, and one object or property aligned in a valid syntax (Figure 1, right-most panel shows activation of the rule FLAG IS WIN).The primary components of the game are:</p>
<p>-Objects: entities in the game, such as BABA, WALL, or ROCK.Every solvable level should have a win condition and an object that can be controlled by the player.Rules dynamically define how objects behave in this game; an object does not really matter until there is a rule assigned to the object.Rules can be created, modified, or broken during gameplay by rearranging of text blocks.A level is considered complete when the object that is controlled by the player (IS YOU) touches the object designated as the win condition (IS WIN), or when the same object satisfies both rules.Figure 2 illustrates gameplay scenarios.The most common properties and rules are explained in Table 3 (Appendix).</p>
<p>In this work we used a simplified version of the game: the jam version of Baba is You1 and a section of the game mechanics of the "Game Module" from Baba is Y'all [Charity et al., 2020].Levels are encoded as strings of characters, where each character corresponds to an object or text block, see the ASCII grid representation in the left panel of Figure 1.In the simplified version of the game, the player is able to perform four distinct actions:</p>
<p>-Move: Navigate the controlled object towards other objects or text blocks; -Create a Rule: Push text blocks into a valid rule by aligning them with a controlled object; -Break a Rule: Break an active rule by pushing a text block away from its syntactic alignment; -Push: Interact with text blocks, or objects if they are set to PUSH.</p>
<p>Experimental Setup</p>
<p>In this section we describe the methodologies used in our study, including data collection, experimental setup, and analysis techniques.We evaluated six different LLMs.GPT-4o [Hurst et al., 2024] (OpenAI) is a state-of-the-art reasoning model due to its strong performance on various tasks.GPT-4o can reason at inference time [Valmeekam et al., 2024], using methods such as reinforcement learning to call a model multiple times with different prompts.Gemini 1.5-Flash [Gemini, 2024] (Google DeepMind) is designed for cost efficiency and fast inference.Mistral 7B instruct [Jiang et al., 2023] (Mistral AI) is a 7B parameter model tuned for instruction-following tasks.Mixtral 8x7B [Jiang et al., 2024] uses a Mixture-of-Experts (MoE) design, activating only 12B of its 45B parameters per inference to reduce computational cost.OLMo 7B and 13B instruct [Jiang et al., 2023] (AI2) are open source models trained on the Dolma dataset, an open dataset that includes a mix of web content, academic publications, code, books, and encyclopedic materials.The OLMo models are designed with a focus on research accessibility, interpretability, and transparency.</p>
<p>To enable the model to understand and play the game Baba is You, we constructed three different prompts.(Please refer to the Appendix.)</p>
<p>-Simple Prompt The prompt consists of the game mechanics, the definition of the characters to interpret the level, and the definition of each property.-Rule-extended Prompt Adds the rules that are active at the current level to the prompt.-Action-extended Prompt Further expands the prompt by including a description of the possible actions, partially adapted and extended from Cloos et al. [2024].</p>
<p>Each prompt ends with a question to solve the given grid level, followed by the ASCII grid level, and, at the end, a PS sentence (Plan-and-Solve [Wang et al., 2023b]) to activate CoT (Chain-of-Thought [Wei et al., 2022]).The prompts were constructed manually through iterative trial and error with GPT-4o.Outputs were reviewed for improvement, refined, and resubmitted until a satisfactory version was achieved.</p>
<p>Evaluation of Reasoning In order to investigate how well LLMs perform in reasoning and solving Baba is You levels, a manual analysis was performed to examine the reasoning chains generated by LLMs.The reasoning chain can be divided into four distinct sections: the interpretation of the level, the formulation of the problem statement, the formulation of the solution for the problem, and the formulation of the actions that should be taken for the solution.The first two sections are part of the analysis of the level, while the latter two sections are part of the solution process so formulating an answer consists of four steps.Table 1 summarizes errors encountered in these steps.The error categories are used in Figure 4.If a step is correct, it is marked with a c label together with the number of the step, otherwise errors are classified according to the subcategories.The correctness frequency is shown in Figure 5.</p>
<p>To evaluate the LLMs, each model and prompt format was tested in 14 different levels, each of which tests a different aspect of the game, see Figure 3.Most of these levels are demo levels of the Keke AI competition [Charity and Togelius, 2022], except level 14.These levels require some logical thinking, but  are relatively easy for humans, due to our natural ability to reason.However, LLMs encounter a challenge when confronted with the task of solving these levels.These models must not only interpret the rules and mechanics of the game from the text, but also apply them in the environment.Unlike humans, LLMs have no inherent understanding of the world.These models rely entirely on the information provided to decide how to interact with and manipulate the game state.The levels are designed to assess specific components of the game, including rule creation, transformation, immutability and logical reasoning, which are needed to determine the model's ability to play the game Baba is You.Mistral and OLMo consistently produce identical outputs for repeated runs of the same prompt.In contrast, GPT-4o and Gemini Flash 1.5 exhibit variability.The accuracy was evaluated by running each prompt five times, a solution was correct if it appeared in at least three runs.</p>
<p>For finetuning, we combined three different datasets.Each dataset contains a specific type of data.The largest dataset [Bjorklund, 2025] consists of various questions designed to improve the model's reasoning ability.These questions cover a range of logical and analytical challenges.The second dataset [van Wetten, 2025a] contains questions specifically related to the game mechanics of Baba The dataset containing questions about the game mechanics of Baba is You was created through the following process.Initially, a set of questions was crafted, focusing on the rules and mechanics of the game.Then, GPT-4o was prompted to generate additional unique questions based on the ones we had already created.These generated questions and answers were reviewed and, when necessary, corrected.This iterative process allowed us to quickly build a solid dataset of questions related to the game's mechanics.The other dataset, which consists of levels and their solutions, is entirely handwritten.As a result, this dataset is smaller, as more time was spent on creating detailed solutions for each level rather than on increasing the dataset size.We trained Mistral 7B and OLMo 7B on the combined dataset using LoRA for parameter-efficient finetuning.</p>
<p>Results</p>
<p>In this section, we present the findings of our study, analyzing the outcomes based on the predefined metrics.We start with the prompt-based learning results.</p>
<p>Prompt-based learning</p>
<p>Simple prompt (1) The main challenge across models was to identify the active rules.GPT-4o performed better due to its improved recognition of objects and text blocks in the grid (Figure 4, prompt 2), though it still struggled with vertically placed rules.OLMo and Mistral had difficulty recognizing objects and text, which prevented them from formulating correct rules.As a result, most outputs for the simple prompt were incorrect (results are not shown).</p>
<p>Rule-extended (2) and Action-extended (3) GPT-4o performs relatively well, demonstrating a strong ability to understand the grid and formulate correct problem statements (Figure 4).This LLM also provides reasonable level solutions (Figure 5), making it potentially useful for assisting players.However, its action Fig. 4: Frequency of error step and subcategory (see Table 1) in the reasoning chains generated by the models.GPT-4o has the least errors.Furthermore, OLMo 7B generates the most errors in the reasoning output for the levels, primarily when defining the text blocks and objects in the grid.Finally, all models encounter difficulties in formulating actions on the grid itself.</p>
<p>formulation is less reliable and often does not align with its own solutions.A key weakness lies in distinguishing which rules are breakable.In levels 13 and 14, GPT-4o incorrectly suggests breaking unbreakable rules to reach the flag, highlighting a difficulty in understanding spatial constraints.The action-extended prompt led to small improvements, with GPT-4o generating more accurate solutions and demonstrating better grasp of game mechanics.Still, it did not always strictly adhere to the action format.Gemini Flash 1.5 performs slightly worse than GPT-4o in identifying objects and text in the grid, often leading to incomplete or occasionally missing problem statements.Although these issues were not a major obstacle for solution generation, the model struggled to consistently describe the obstacles.Also, with the third prompt, hallucinations increased (those in the first step did not transfer to other steps).There was also more hallucination in formulating the solution.Interestingly, at level 12 using prompt 2, the LLM proposed breaking 'BABA IS MELT' and forming 'LAVA IS MELT', causing lava to disappear, followed by creating 'BABA IS WIN', which results Fig. 5: Correctness frequency per step in the reasoning chain generated by the LLM models (see Section 3.1).GPT-4o has generated the most correct steps in the reasoning chain for the levels.Furthermore, GPT-4o and Gemini Flash 1.5 benefit from the action-extended prompt, while the smaller models encounter difficulties irrespective of prompt structuring.in a successful outcome.This solution is intriguing because it is not the most straightforward approach, but a creative way to solve the puzzle.</p>
<p>Like GPT-4o, Gemini Flash 1.5 struggles with generating accurate actions.Notably, it rarely suggested rule-breaking with the rule-extended prompt but did so more often with the action-extended prompt.Overall, both models performed better with the action-extended prompt, showing fewer errors, and improved step-by-step reasoning (see Figures 6 and 7).In Table 4 some examples of error snippets of the reasoning chain of GPT-4o and Gemini Flash 1.5 are shown (Appendix).We also evaluated OLMo and Mistral models on 14 Baba is You levels.Overall, they performed significantly worse than GPT-4o and Gemini Flash 1.5, particularly in object and text block identification in the grid (Figure 4).Table 5 shows examples of error snippets of the reasoning chain of these models.OLMo-7B and 13B struggled with hallucinations and frequent misidentification of grid elements.They often failed to distinguish between objects and rules, which led to incoherent problem statements and solutions.Both models frequently misinterpreted "WIN" as the target object and showed little understanding of the rule mechanics or grid constraints.</p>
<p>Mistral 7B and Mixtral 8x7B performed slightly better in object recognition but continued to produce flawed solutions.Mixtral often skipped the problem statement entirely and jumped straight to solutions, omitting context and causing logical gaps.A recurring issue was the models' misunderstanding of the rule-breaking mechanism.Rather than removing rules, they often suggested alternative rules, missing the mechanic's intent.Additionally, both Mistral models struggled to track active rules, sometimes suggesting to create rules that already Fig.6: Correct steps per model across the 14 Baba is You levels with the ruleextended prompt.The rule-extended prompt, which provides the active rules present in the level, improves performance across models but still highlights major differences in reasoning capabilities.GPT-4o outperforms other models, demonstrating stronger multi-step problem-solving skills.While Gemini 1.5 Flash show partial success, its performance remains inconsistent.The results suggest that simply providing active rules helps, but does not bridge, the gap in logical reasoning ability between smaller models and more advanced LLMs like GPT-4o.Fig. 7: Correct steps per model across the 14 Baba is You levels with the actionextended prompt.The action-extended prompt, which provides additional details about possible actions, leads to notable improvements for some models, particularly Gemini 1.5 Flash and GPT-4o.However, GPT-4o remains the strongest performer, consistently solving more steps across all levels.While some smaller models show slight improvements, their overall performance remains limited, suggesting that improving prompts alone is not sufficient to overcome their reasoning limitations.These results highlight the importance of both prompt design and underlying model capability in tackling complex rule-based reasoning tasks.Fig. 8: Frequency of errors per step and subcategory in the reasoning chains generated by the models when solving the Baba is You levels.We see that both finetuned models have fewer errors in the analyzing part of the level in the reasoning chain after finetuning compared to the original model.existed or were impossible to form given the available text blocks.One common error was assuming that movement required 'BABA IS MOVE,' indicating a lack of grasp of default game behavior.</p>
<p>Unlike GPT-4o and Gemini 1.5 Flash, which showed improvements with structured prompts, OLMo and Mistral models did not consistently benefit from action-extended prompts.The solutions remained equally flawed.These results highlight key limitations of smaller models: difficulty distinguishing game entities, tracking rule states, and reasoning through rule-breaking mechanics.While Mistral models showed slight improvement over OLMo, neither models demonstrated strong puzzle-solving ability.In levels 4 and 5, most models misinterpreted the presence of the rule "FLAG IS WIN" as implying the flag's existence, overlooking the need to create or transform the flag.In level 5, some models incorrectly assumed the flag was inside the rock due to "FLAG IS ROCK," revealing confusion between rule-based transformation and object persistence.</p>
<p>Finetuning</p>
<p>Next, we discuss the finetuning results on Mistral and OLMo.Finetuning Mistral 7B improved classification of the objects and text blocks in the grid (Figure 9).There were fewer misclassifications and incomplete information problems (Fig- For Mistral 7B there is an improvement in classification of objects and text blocks in the grid.For OLMo 7B there is an improvement in the problem statement formulation.Fig. 10: Correct steps per model across the 14 Baba is You levels with the actionextended prompt.The finetuned models have more correct steps across the levels but still not enough to fully solve the levels.ure 8).However, this did not improve problem statements or solving of levels, which was often incomplete with wrong assumptions about grid-objects.</p>
<p>After finetuning, OLMo 7B improved the formulation of the problem statement (Figure 9) and achieved a reduction in classification errors for objects and text blocks (Figure 8).However, the model still struggles with correctly distinguishing between them.The generated solutions suggest that the finetuned model still has difficulty grasping the game mechanics, rarely proposing actions such as breaking or creating rules.Additionally, it sometimes treats text blocks as the objects you control.Finetuning the models with textual data from the game Baba is You led to improvements in level analysis for both models.In the case of Mistral 7B, there was an improvement in classifying text blocks and objects, while for OLMo 7B, the problem statement formulation showed better results.However, for both models, there was no clear improvement in solving the puzzle, as the generated solutions and actions still contained many errors.</p>
<p>Discussion &amp; Conclusion</p>
<p>In the puzzle game Baba is You the goal is to win by following rules and by creating new rules, tasks that involve both language and reasoning abilities.LLMs must be able to move (mention) game pieces in such a way that they align to form new rules (use).This study explores how various LLMs perform on 14 relatively simple game levels: how well they are able to solve levels by understanding the consequences of rule manipulation and spatial understanding.We used prompt-based-learning first, finetuning second.Among the models evaluated with prompt-based-learning, GPT-4o and Gemini Flash 1.5 consistently outperform smaller models in identifying objects, interpreting game mechanics, and generating partially correct solution paths.However, even these more advanced models struggled to accurately interpret the grid as a two-dimensional space, often overlooking critical constraints such as rule-breakability.Finetuning on structured textual data led to improvements, Mistral 7B showed better classification and OLMo 7B improved in formulating problem statements, but neither model demonstrated substantial gains in full solution generation.Mistral and OLMo continued to struggle with core aspects of the game such as distinguishing between text and object blocks and understanding how rule creation or breaking is physically performed in the game.</p>
<p>This work shows that while high-end models like GPT-4o and Gemini Flash 1.5 can reason through parts of Baba is You levels, they still struggle to fully understand the game.Furthermore, without explicit prompts that include active rules and structured action formats, their performance drops significantly.A common limitation across all models is that they fail to interpret the grid as a two-dimensional space, leading to incorrect or overly simplistic solutions.Even GPT-4o often fails to recognize which rules can be broken, and it is unclear whether LLMs truly grasp the mechanism of rule manipulation through moving text blocks.Smaller models such as Mistral and OLMo, even when fine-tuned, frequently misinterpret game elements and fail to demonstrate a solid understanding of the mechanics.</p>
<p>Complex reasoning tasks such as Baba is You pose three types of challenges to an LLM: challenges of (1) representation, (2) reasoning, and (3) reflection [Madaan et al., 2023, Schultz et al., 2024, Plaat et al., 2024].</p>
<p>Representation First, the LLM must be able to represent puzzle states correctly.In Chess, work on ChessGPT has shown that pretraining and finetuning can teach an LLM to recognize positions and solve problems correctly [Karvonen, 2024, Feng et al., 2024].In OthelloGPT, Li et al. [2023], Nanda et al. [2023] have used mechanistic interpretability to show how pretrained LLMs represent boards internally.In our study of Baba is You LLMs were not pretrained on the game, and the LLMs have difficulty with the spatial interpretation of the board.Further finetuning and pretraining may be necessary for improvement.</p>
<p>Reasoning Second, in order to correctly manipulate the state representations, the LLM must be able to reason with the rules, for example, to follow chains of thought [Wei et al., 2022].Schultz et al. [2024], Zhang et al. [2025] show that by pretraining and finetuning on textual representations of Chess, LLMs can learn to reason well enough to play correct games (although not yet at a high level of play).In Baba is You, we also saw that finetuning was able to enhance reasoning.</p>
<p>Reflection Third, in Baba is You the LLM must be able to reflect on its own reasoning to understand the effect (use) of the rules that it composes (mention).Reasoning LLMs typically apply reinforcement learning to reflect on their own actions, using an external algorithm to control the self-reflection process [Madaan et al., 2023, Shinn et al., 2023, Yao et al., 2023].In Baba is You, the LLMs achieve weak use-mention-type reasoning about dynamic rules, with prompts based on Plan-and-Solve [Wang et al., 2023b].Achieving accurate use-mention reflection in Baba is You may require such explicit external algorithms or methods such as analogical prompting [Yasunaga et al., 2023].</p>
<p>Limitations &amp; Further work Chain of thought (CoT) prompting [Wei et al., 2022] has spawned active research in methods for reasoning.This research used plan and solve [Wang et al., 2023b], a zero-shot CoT prompting method.Further research may explore other prompting methods, for example using explicit stepby-step prompting [Shinn et al., 2023, Press et al., 2023, Madaan et al., 2023].</p>
<p>The evaluation was conducted on 14 relatively simple levels, which may not reflect model performance on more challenging puzzles.More levels, with varying difficulty, could provide deeper insights.Furthermore, the finetuning dataset that we used was small, with only 15 examples for level solutions and 298 game mechanics questions.This restricts the model's exposure to the game's complexity.Expanding the dataset with more varied levels, solution paths, and mechanismrelated questions may improve generalization and reasoning performance.In general, larger models tend to perform better at test time inference [Muennighoff et al., 2023], as the performance of GPT-4o in our work also indicated.Therefore, especially for reasoning and reflection, further research with larger models is warranted.Additionally, more extensive finetuning with adjusted hyperpa-rameters (e.g., learning rate, batch size, or epochs) might yield better results.Finally, error analysis in this work was performed manually, introducing potential subjectivity.Future work could implement automated evaluation tools to ensure more consistent and scalable assessment.are not next to the FLAG (F).We need to move the IS and WIN next to the FLAG to make the rule active again.</p>
<p>A Prompts and Error Snippets</p>
<p>Level 5: Plan The goal is to reach the Flag.However, the initial rules make all Rocks into Flags, which can be confusing and potentially block the path.We need to change the rules to make the Flag a win condition again and allow Baba to move freely.Table 4: Examples of error snippets in the reasoning chains of GPT-4o and Gemini 1.5 Flash LLM models, where the red-colored text indicates incorrect reasoning.These errors highlight challenges such as hallucinations, incorrect assumptions, and overlooking key constraints.</p>
<p>LLM Model Error Snippets</p>
<p>Mistral 8x7B</p>
<p>Level 1:</p>
<p>Step 3 Devise a plan -We need to move the BABA object to the FLAG object.</p>
<p>-To do this, we need to change the rule BABA IS YOU to BABA IS MOVE.</p>
<p>-Then, we need to push the ROCK object to the WALL object to create a path for the BABA object to reach the FLAG object.-Rules: BABA IS YOU, FLAG IS WIN.</p>
<p>Step 3: Devise a plan -Make BABA the object you control.</p>
<p>-Make FLAG the object you need to reach to win the level.</p>
<p>Mistral 7B</p>
<p>Level 1: The goal is to move the BABA to the FLAG to win the level.Plan:</p>
<ol>
<li>
<p>Move the BABA to the flag by pushing the blocks in the way.2. Make the BABA move towards the flag.</p>
</li>
<li>
<p>Make the BABA push the flag towards itself.</p>
</li>
</ol>
<p>Level 3: Plan: 1. Move BABA to the 1 text block.</p>
<ol>
<li>
<p>Change BABA into the flag (f) using the 1 text block.</p>
</li>
<li>
<p>Move the flag to the exit (the empty space next to the 3 text block).</p>
</li>
</ol>
<p>Level 9:</p>
<p>The goal is to move the flag to the exit (position B12) without getting killed by the skull.</p>
<p>OLMo 13B</p>
<p>Level 3: <strong>Objective:</strong> Reach the WIN block to win the level.</p>
<p>Level 4:<strong>Plan:</strong> 1. <strong>Make Baba movable:</strong> Ensure Baba has the "MOVE" property.</p>
<p>Level 9: 1. <strong>Neutralize SKULL:</strong> -We cannot directly push SKULL because it will kill BABA.</p>
<p>-We need to use text blocks to change SKULL's properties.</p>
<p>-<strong>Make Rule:</strong> SKULL IS STOP.This will make SKULL impassable and safe for BABA.</p>
<p>OLMo 7B</p>
<p>Level</p>
<p>Fig. 1: ASCII representation of a level; Pictorial representation; By pushing (mention), Baba has created the Rule (use) FLAG IS WIN</p>
<p>Fig. 2: Examples of different game mechanics using the flag as a win condition.Each sequence shows how obstacles are manipulated or rules are changed to create a path to the flag.</p>
<p>Fig. 3 :
3
Fig. 3: Levels used for the evaluation of the LLM models in playing Baba is You</p>
<p>Fig. 9 :
9
Fig.9: Correctness frequency per step in the reasoning chain generated by the LLM models.Both finetuned models have more correct steps after finetuning.For Mistral 7B there is an improvement in classification of objects and text blocks in the grid.For OLMo 7B there is an improvement in the problem statement formulation.</p>
<p>Level 13:<strong>Solution</strong> 1. <strong>Breake_Rule[GOOP IS SINK]:</strong> Remove the rule "GOOP IS SINK."This will prevent the Goop from destroying itself.2. <strong>Make_Rule[ROCK IS HOT]:</strong> Create the rule "ROCK IS HOT."This will make the Rocks hot. 3. <strong>Make_Rule[GOOP IS MELT]:</strong> Create the rule "GOOP IS MELT."This will make the Goop melt when it touches the hot Rocks.</p>
<p>Table 2 :
2
The size of the three datasets used for finetuning is You.It includes questions about the interactions between different game elements, the effects of specific rule changes, and the general logic of the game.The third dataset[van Wetten, 2025a]is the smallest and consists of different levels of Baba is You.In this dataset, the input corresponds action-extended prompt of the level description, while the output represents the expected solution that the model should generate.Together, these three data sets form a combined data set used for finetuning, see Table2for the sizes.
DatasetSizeCoT-logic-reasoning10500Questions game mechanics 289Levels &amp; answers15</p>
<p>Table 3 :
3
Explanation of common rules in the game BABA IS YOU With the ROCK out of the way, BABA can move directly to the FLAG.
LLM ModelError SnippetsLevel 13: Goal:GPT-4oTo win the level, BABA (controlled by the player) must reach the FLAG (f).However, the WALL blocks the direct path.Strategy:Modify the rules to bypass or remove the obstacle created by the WALL:Remove or break the "WALL IS STOP" rule.Create an accessible path for BABA to reach the FLAG.Level 4: Identify Barriers:BABA (controlled by the player) must reach the FLAG, but there is nodirect path. ROCK (r) is blocking BABA's movement, and it has no propertiesassigned.Use Rules to Clear the Path:Assign ROCK IS PUSH by moving the text blocks R, 1, and 5 (if present) toform the rule.This will allow BABA to push the ROCK (r) out of the way.Reach the FLAG:Level 4:Gemini 1.5 Flash
<strong>Objects:</strong> We have Baba (b), a Rock (r), and Flags (f).<strong>Text Blocks:</strong> BABA (B), IS (1), YOU (2), FLAG (F), and WIN (3).<strong>Current Rules:</strong> BABA IS YOU (you control Baba) and FLAG IS WIN (reaching a flag wins the level).<strong>Plan</strong> Currently, the "FLAG IS WIN" rule is broken because the IS (1) and WIN (3)</p>
<p>Level 3: Step 1: Understand the problem and extract the relevant objects, text blocks, and rules.Objects: BABA, FLOOR, GRASS, WALL, SKULL Text Blocks: 1, 2, 3 Rules: BABA IS YOU Step 2: Devise a plan to solve the problem.1. Make BABA IS YOU to BABA IS MOVE. 2. Move BABA to the position of the SKULL.3. Make SKULL IS YOU.Level 4: Step 2: Extract the relevant objects, text blocks and rules -Objects: BABA, SKULL, FLAG, FLOOR, GRASS, ROCK, WALL, KEKE, GOOP, LOVE, and empty space.-Text blocks: BABA, IS, YOU, FLAG, IS, WIN.</p>
<p>1: ### Plan 1. <strong>Identify the Goal:</strong> Move BABA to the FLAG (F13).2. <strong>Current Obstacles:</strong> BABA starts at B12, and there are walls (W) and empty spaces (.) blocking the path.Level 2: 2. <strong>Initial Setup:</strong> -BABA is currently next to a WALL (W) and a KEKE (K).-There's a FLAG (F) adjacent to BABA.Level 9: ### Plan: 1. <strong>Move BABA away from SKULL:</strong> To avoid immediate death, move BABA away from SKULL. 2. <strong>Create a path to FLAG:</strong> Use the environment to create a path to FLAG. 3. <strong>Utilize GOOP:</strong> GOOP can be used to block SKULL or create a bridge</p>
<p>Table 5 :
5
Examples of error snippets in the reasoning chains of OLMo and Mistral LLM models, where the red-colored text indicates incorrect reasoning.These errors highlight challenges such as hallucinations, incorrect assumptions, and wrong reasoning steps.</p>
<p>https://hempuli.itch.io/baba-is-you
Model InputYou are helping to solve a gridworld game.In Baba is You, the player can change the game rules by moving text blocks around.The grid contains object text and property text.Text Blocks: Object Text: Words representing game objects.Property Text: Words that describe actions or properties.Active Rules: A rule is formed when the text blocks are aligned in a valid way, either horizontally or vertically.Valid rule formats include: <object_text> IS <property_text>: Grants a property to an object.<object1_text> IS <object2_text>: Changes one object into another.<object1_text> IS <object1_text>: Makes an object immutable.The goal is to use these rules to solve the level by moving the text blocks and controlling the objects.The level is displayed in a 2D grid, where each character has the following meaning: Text blocks in the game which always can be pushed: <object_text> IS WIN: The object you need to reach or to be to win the level.<object_text> IS STOP: Makes the object impassable (you can't move through it).<object_text> IS MELT: Makes the object melt when touched by something marked as HOT.<object_text> IS HOT: Makes the object destroy any object marked as MELT when they touch it.If the same object is both HOT and MELT it self-destructs.<object_text> IS MOVE: Makes the object move one step in its direction every turn.<object_text> IS KILL: Makes the object destroy anything you control when touched, but it stays intact.<object_text> IS PUSH: Lets you push the object or have it pushed by other moving objects.<object_text> IS SINK: Makes the object destroy itself and anything it touches when it is touched.Question: Give a solution to the following grid level: Let's first understand the problem, extract the relevant objects, text blocks and rules (explain the rules) in the level and make a plan to solve the problem.Then let's carry out the plan by giving the intermediate actions (using common sense).Solve the problem step by step and show the solution.Fig.11: Simple prompt: consisting only of a short game description and definitions of the characters and rules.Followed by a question to solve a level with at the end a sentence to activate zero-shot CoT.The Rule-extended prompt and the Action-extended prompt can be found on GitHub[van Wetten, 2025a,c,b].
Human-level play in the game of diplomacy by combining language models with strategic reasoning. Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Science. 37866242022</p>
<p>Dota 2 with Large Scale Deep Reinforcement Learning. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, arXiv:1912.06680December 2019</p>
<p>cot-logic-reasoning. Isaiah Bjorklund, 2025</p>
<p>Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. Noam Brown, Tuomas Sandholm, 10.1126/science.aao1733Science. 0036-80753596374January 2018</p>
<p>. Murray Campbell, Joseph HoaneJr, Feng-Hsiung Hsu, Deep blue. Artificial intelligence. 1341-22002</p>
<p>Keke AI Competition: Solving puzzle levels in a dynamically changing mechanic space. M Charity, Julian Togelius, 10.1109/CoG51982.2022.98936502022 IEEE Conference on Games (CoG). August 2022</p>
<p>Baba is y'all: Collaborative mixed-initiative level design. Megan Charity, Ahmed Khalifa, Julian Togelius, 10.1109/CoG47356.2020.92318072020 IEEE Conference on Games (CoG). 2020</p>
<p>RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs. Shreyas Chaudhari, Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, Ameet Deshpande, Bruno Castro Da Silva, arXiv:2404.08555April 2024</p>
<p>A survey of chain of thought reasoning: Advances, frontiers and future. Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu ; Nathan Cloos, Meagan Jens, Michelangelo Naim, Yen-Ling Kuo, Ignacio Cases, Andrei Barbu, Christopher J Cueva, arXiv:2407.137292024. 2024Association for Computational LinguisticsarXiv preprintBaba is ai: Break the rules to beat the benchmark</p>
<p>A survey on in-context learning. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, 2023Association for Computational Linguistics</p>
<p>Chessgpt: Bridging policy learning and language modeling. Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, Jun Wang, Advances in Neural Information Processing Systems. 362024</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Team Gemini, arXiv:2403.05530December 2024</p>
<p>Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey. Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang, arXiv:2403.14608September 2024</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, arXiv:2106.09685October 2021</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, Assoc for Computational Linguistics. 2023</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Fine-tuning and utilization methods of domain-specific llms. Cheonsu Jeong, arXiv:2401.029812024arXiv preprint</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, arXiv:2310.06825October 20237</p>
<p>Mixtral of Experts. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, arXiv:2401.04088January 2024</p>
<p>Prompt-based Learning. Uday Kamath, Kevin Keenan, Garrett Somers, Sarah Sorenson, 10.1007/978-3-031-65647-7_3Large Language Models: A Deep Dive: Bridging Theory and Practice. Cham2024Springer Nature Switzerland</p>
<p>Emergent world models and latent variable estimation in chessplaying language models. Adam Karvonen, arXiv:2403.154982024arXiv preprint</p>
<p>Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Reid, Advances in neural information processing systems. 202235</p>
<p>Emergent world representations: Exploring a sequence model trained on a synthetic task. Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, 2023ICLR</p>
<p>Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study. Yinghao Li, Haorui Wang, Chao Zhang, 10.18653/v1/2024.naacl-long.4Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics20241</p>
<p>Selfrefine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202336</p>
<p>Georgios Doukeris, Mike Preuss, and Giulio Barbero. The effect of llm-based npc emotional states on player emotions: An analysis of interactive game play. Alessandro Marincioni, Myriana Miltiadous, Katerina Zacharia, Rick Heemskerk, 2024 IEEE Conference on Games (CoG). IEEE2024</p>
<p>Large Language Models as General Pattern Machines. Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, Andy Zeng, arXiv:2307.04721October 2023</p>
<p>Scaling dataconstrained language models. Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, Colin A Raffel, Advances in Neural Information Processing Systems. 202336</p>
<p>Chatter generation through language models. Matthias Müller-Brockhausen, Giulio Barbero, Mike Preuss, 2023 IEEE Conference on Games (CoG). IEEE2023</p>
<p>Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. A Comprehensive Overview of Large Language Models. Neel Nanda, Andrew Lee, Martin Wattenberg, arXiv:2309.00941arXiv:2307.06435Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar2023. October 2024arXiv preprintEmergent linear representations in world models of self-supervised sequence models</p>
<p>Puzzle solving without search or human knowledge: An unnatural language approach. David A Noever, Ryerson Burdick, 2021</p>
<p>Learning to play: reinforcement learning and games. Aske Plaat, 2020Springer Nature</p>
<p>Reasoning with Large Language Models, a Survey. Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki Van Stein, Thomas Back, arXiv:2407.11511July 2024</p>
<p>Aske Plaat, Max Van Duijn, Niki Van Stein, Mike Preuss, Kees Joost Peter Van Der Putten, Batenburg, arXiv:2503.23037Agentic large language models, a survey. 2025arXiv preprint</p>
<p>Measuring and Narrowing the Compositionality Gap in Language Models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, arXiv:2210.03350October 2023</p>
<p>Improving Language Understanding by Generative Pre-Training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Paul Saka, Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Nature. 1074251998. 2020Mind</p>
<p>Mastering board games by external and internal planning with language models. John Schultz, Jakub Adamek, Matej Jusup, Marc Lanctot, Michael Kaisers, Sarah Perrin, Daniel Hennes, Jeremy Shar, Cannada Lewis, Anian Ruoss, arXiv:2412.121192024arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 52975872016</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Science. 36264192018</p>
<p>Baba is you. Arvi Teikari, March 2019. 2019Hempuli OyFinland</p>
<p>Evaluating large language models with grid-based game competitions: an extensible llm benchmark and leaderboard. Oguzhan Topsakal, Colby Jacob Edell, Jackson Bailey Harper, arXiv:2407.077962024arXiv preprint</p>
<p>Planning in Strawberry Fields: Evaluating and Improving the Planning and Scheduling Capabilities of LRM o1. Karthik Valmeekam, Kaya Stechly, Atharva Gundawar, Subbarao Kambhampati, arXiv:2410.02162October 2024</p>
<p>. F T Van Wetten, Llm_Babaisyou, 2025b</p>
<p>. F T Van Wetten, 2025cOlmo_7b_instruct-baba</p>
<p>Attention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, arXiv:1706.03762August 2023</p>
<p>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, arXiv:2305.040912023aarXiv preprint</p>
<p>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, arXiv:2305.04091May 2023b</p>
<p>Shomir Wilson. A bridge from the use-mention distinction to natural language processing. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. The Semantics and Pragmatics of Quotation2022. 201735Chain-of-thought prompting elicits reasoning in large language models</p>
<p>Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment. Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, Fu Lee, Wang , arXiv:2312.12148December 2023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2310.01714Large language models as analogical reasoners. Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H Chi, Denny Zhou, 2023. 202336arXiv preprintAdvances in neural information processing systems</p>
<p>Instruction Tuning for Large Language Models: A Survey. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang, arXiv:2308.10792March 2024</p>
<p>Complete chess games enable llm become a chess master. Yinqi Zhang, Xintian Han, Haolong Li, Kedi Chen, Shaohui Lin, arXiv:2501.171862025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
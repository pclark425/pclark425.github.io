<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7230 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7230</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7230</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-276574876</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.16435v2.pdf" target="_blank">Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs</a></p>
                <p><strong>Paper Abstract:</strong> Despite significant progress on popular multimodal benchmarks, state-of-the-art Multimodal Large Language Models (MLLMs) continue to struggle with basic visual reasoning tasks that are trivially solved by humans, such as recognizing spatial relationships. To systematically investigate this gap, we introduce VisFactor, a benchmark that digitizes 20 vision-centric subtests from a well-established cognitive psychology assessment. These subtests span four core domains of human visual cognition: (1) Visualization and Spatial Processing, (2) Perceptual and Closure, (3) Memory, and (4) Reasoning. We evaluate 20 frontier MLLMs from GPT, Gemini, Claude, LLaMA, Qwen, and SEED families. The best-performing model achieves a score of only 25.19 out of 100, with consistent failures on tasks such as mental rotation, spatial relation inference, and figure-ground discrimination, regardless of model size or prompting strategy. These findings suggest that current MLLM performance gains on high-level benchmarks do not reflect human-like low-level visual cognition, challenging the assumption that large-scale pretraining naturally induces gestalt-like perceptual capabilities. The dataset and evaluation toolkit are publicly available at: https://github.com/CUHK-ARISE/VisFactor.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7230.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7230.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VISFACTOR_overall_best</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VISFACTOR benchmark — best model overall score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Top overall VISFACTOR performance reported in this paper: the best evaluated MLLM attains only low overall accuracy on the 20-subtest factor-grounded visual benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.7-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic Claude family model evaluated as part of the VISFACTOR benchmark (exact architecture/parameters not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>VISFACTOR (20 FRCT-derived visual subtests) — overall score</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>multifactor visual cognition (visual perception, spatial reasoning, memory, closure, reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Aggregate score across 20 vision-centric subtests adapted from the FRCT; designed to probe low-level visual and spatial cognitive faculties in an image-text evaluation setting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>score out of 100 (aggregate across subtests)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>25.19 / 100 (best single-model overall score reported; elsewhere rounded as 25.2/100)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot evaluation via API (default prompts described in appendix); Chain-of-Thought variants also tested separately</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper emphasizes that human baselines under the digitized protocol were not collected (see Limitations). The low overall score indicates broad failures across spatial relation, mental rotation, and figure-ground tasks despite strong leaderboard performance on other multimodal benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7230.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7230.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VISFACTOR_aggregated_oracle</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VISFACTOR — aggregated best-per-subtest (oracle ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hypothetical/aggregated combined score when taking the best-performing model per subtest; used to estimate upper-bound across current models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>oracle-aggregation (best model per subtest)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Post-hoc aggregation that takes the top-performing model for each individual subtest to estimate combined upper-bound across evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>VISFACTOR (20 FRCT-derived visual subtests) — aggregated best-per-subtest</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>multifactor visual cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Combined score produced by selecting the best-performing model on each subtest and summing/averaging to estimate the combined capability across models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>score out of 100 (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>38.8 / 100 (combined score when aggregating per-subtest best models)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot per-model evaluation, then post-hoc aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Shows that even using the best model per task yields a combined score far below human-like competence; authors argue this highlights foundational visual gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7230.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7230.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CF3_text_vs_visual_GPT-4.1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CF3 (Copying Test) — textual-description vs visual input performance (GPT-4.1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct comparison showing that providing the same line-segment information as text yields near-perfect model accuracy, whereas the model fails when information must be inferred from the image.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4.1-2025-04-14</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4.1 family model (vision-enabled variant used for VISFACTOR generated tests; exact parameter count not reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>CF3 — Copying Test (line shape copying on dot grid)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual perception / visualization & spatial processing</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Replicate a line/path from a reference image onto a dot-grid and report the final grid coordinate; measures accurate perception of line endpoints, lengths and spatial placement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage correct final coordinate)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>100% when given textual descriptions of line segments; 6.2% when required to infer the same information from visual inputs (GPT-4.1). No model exceeded 18.8% accuracy on the visual CF3 task in the pooled evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot; two conditions compared — (1) textual specification of line segments, (2) raw image input (visual)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors interpret the huge gap as evidence that models rely on high-level textualizable concepts and fail to extract low-level visual primitives reliably; demonstrates that success often derives from text-processing rather than genuine visual perception.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7230.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7230.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CF3_marker_size_effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CF3 generated cases — start-point marker size sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measured degradation in models' ability to identify the circled start point in CF3 as the visual saliency (marker size) decreases, indicating limits in visual attention/saliency processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pooled models (aggregated behavior reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Aggregate behavior across evaluated MLLMs on generated CF3 variants varying the start-point marker size.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>CF3 — Copying Test (start marker identification across marker sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual perception / perceptual attention</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Detect and identify the circled starting dot for the copying task; marker size (visual saliency) is varied to test sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage correct start-point identification)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Start-point identification accuracy: 92% with large circular markers, 80% with medium markers, 68% with small markers (reported across models/instances).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot image-based prompts on generated CF3 instances</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors highlight systematic degradation as marker size shrinks, suggesting model visual-attention mechanisms fail when saliency is reduced; this is used to argue limited low-level visual sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7230.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7230.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>P3_middle_score_anomaly</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>P3 (Identical Pictures Test) — middle-score anomaly in models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed intermediate accuracy (30–50%) of models on a task that humans typically solve near-perfectly or at chance, suggesting inconsistent/perceptually noisy model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple evaluated MLLMs (pooled observations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Set of 20 frontier MLLMs across GPT, Gemini, Claude, LLaMA, Qwen, and SEED families evaluated on VISFACTOR.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>P3 — Identical Pictures Test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>perceptual speed / visual discrimination</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Decide whether two images depict exactly the same object; tests fine-grained visual comparison and perceptual discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage correct TRUE/FALSE decisions)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Most models obtain 30–50% accuracy on P3 (reported), which is atypical compared to human performance patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot; Chain-of-Thought (CoT) variants were also tested and sometimes degraded performance on perceptual tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper notes a 'Middle Score Anomaly': humans either perform almost perfectly or at chance on P3; the observed intermediate model accuracies imply partial, inconsistent perceptual processing. The paper explicitly states no contemporary human baseline was collected for the digitized items.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7230.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7230.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MA1_memorization_GPT-4.1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MA1 (Picture-Number Test) — memorization performance (GPT-4.1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4.1 attains perfect performance on the original (easier) MA1 memorization set but degrades substantially as the memorization load increases in the 'hard' generated variant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4.1-2025-04-14</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4.1 family model (vision-enabled), used to evaluate generated difficulty-controlled variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>MA1 — Picture-Number (associative memory)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>associative memory / visual memory</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Memorize image-number pairs and recall the number associated with a shown picture; measures associative visual memory capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage correct recalled number)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>100% accuracy on the original MA1 (21 picture-number pairs) for GPT-4.1; substantial performance drop reported on a hard generated variant with 50 pairs (no precise numeric reported in text).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot on generated and original items</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors interpret the high success on MA1 original items as evidence that models can memorize associatively when inputs are conceptually interpretable; performance collapses when memorization load or abstraction increases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Blink: Multimodal large language models can see but not perceive <em>(Rating: 2)</em></li>
                <li>Does spatial cognition emerge in frontier models? <em>(Rating: 2)</em></li>
                <li>Vision language models are blind: Failing to translate detailed visual features into words <em>(Rating: 2)</em></li>
                <li>Forgotten polygons: Multimodal large language models are shape-blind <em>(Rating: 2)</em></li>
                <li>VisualSphinx: Large-scale synthetic vision logic puzzles for RL <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7230",
    "paper_id": "paper-276574876",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "VISFACTOR_overall_best",
            "name_full": "VISFACTOR benchmark — best model overall score",
            "brief_description": "Top overall VISFACTOR performance reported in this paper: the best evaluated MLLM attains only low overall accuracy on the 20-subtest factor-grounded visual benchmark.",
            "citation_title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs",
            "mention_or_use": "use",
            "model_name": "Claude-3.7-Sonnet",
            "model_description": "Anthropic Claude family model evaluated as part of the VISFACTOR benchmark (exact architecture/parameters not specified in paper).",
            "model_size": null,
            "test_name": "VISFACTOR (20 FRCT-derived visual subtests) — overall score",
            "test_category": "multifactor visual cognition (visual perception, spatial reasoning, memory, closure, reasoning)",
            "test_description": "Aggregate score across 20 vision-centric subtests adapted from the FRCT; designed to probe low-level visual and spatial cognitive faculties in an image-text evaluation setting.",
            "evaluation_metric": "score out of 100 (aggregate across subtests)",
            "human_performance": null,
            "llm_performance": "25.19 / 100 (best single-model overall score reported; elsewhere rounded as 25.2/100)",
            "prompting_method": "zero-shot evaluation via API (default prompts described in appendix); Chain-of-Thought variants also tested separately",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Paper emphasizes that human baselines under the digitized protocol were not collected (see Limitations). The low overall score indicates broad failures across spatial relation, mental rotation, and figure-ground tasks despite strong leaderboard performance on other multimodal benchmarks.",
            "uuid": "e7230.0",
            "source_info": {
                "paper_title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "VISFACTOR_aggregated_oracle",
            "name_full": "VISFACTOR — aggregated best-per-subtest (oracle ensemble)",
            "brief_description": "Hypothetical/aggregated combined score when taking the best-performing model per subtest; used to estimate upper-bound across current models.",
            "citation_title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs",
            "mention_or_use": "use",
            "model_name": "oracle-aggregation (best model per subtest)",
            "model_description": "Post-hoc aggregation that takes the top-performing model for each individual subtest to estimate combined upper-bound across evaluated models.",
            "model_size": null,
            "test_name": "VISFACTOR (20 FRCT-derived visual subtests) — aggregated best-per-subtest",
            "test_category": "multifactor visual cognition",
            "test_description": "Combined score produced by selecting the best-performing model on each subtest and summing/averaging to estimate the combined capability across models.",
            "evaluation_metric": "score out of 100 (aggregate)",
            "human_performance": null,
            "llm_performance": "38.8 / 100 (combined score when aggregating per-subtest best models)",
            "prompting_method": "zero-shot per-model evaluation, then post-hoc aggregation",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Shows that even using the best model per task yields a combined score far below human-like competence; authors argue this highlights foundational visual gaps.",
            "uuid": "e7230.1",
            "source_info": {
                "paper_title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "CF3_text_vs_visual_GPT-4.1",
            "name_full": "CF3 (Copying Test) — textual-description vs visual input performance (GPT-4.1)",
            "brief_description": "Direct comparison showing that providing the same line-segment information as text yields near-perfect model accuracy, whereas the model fails when information must be inferred from the image.",
            "citation_title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs",
            "mention_or_use": "use",
            "model_name": "GPT-4.1-2025-04-14",
            "model_description": "OpenAI GPT-4.1 family model (vision-enabled variant used for VISFACTOR generated tests; exact parameter count not reported in paper).",
            "model_size": null,
            "test_name": "CF3 — Copying Test (line shape copying on dot grid)",
            "test_category": "visual perception / visualization & spatial processing",
            "test_description": "Replicate a line/path from a reference image onto a dot-grid and report the final grid coordinate; measures accurate perception of line endpoints, lengths and spatial placement.",
            "evaluation_metric": "accuracy (percentage correct final coordinate)",
            "human_performance": null,
            "llm_performance": "100% when given textual descriptions of line segments; 6.2% when required to infer the same information from visual inputs (GPT-4.1). No model exceeded 18.8% accuracy on the visual CF3 task in the pooled evaluation.",
            "prompting_method": "zero-shot; two conditions compared — (1) textual specification of line segments, (2) raw image input (visual)",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Authors interpret the huge gap as evidence that models rely on high-level textualizable concepts and fail to extract low-level visual primitives reliably; demonstrates that success often derives from text-processing rather than genuine visual perception.",
            "uuid": "e7230.2",
            "source_info": {
                "paper_title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "CF3_marker_size_effect",
            "name_full": "CF3 generated cases — start-point marker size sensitivity",
            "brief_description": "Measured degradation in models' ability to identify the circled start point in CF3 as the visual saliency (marker size) decreases, indicating limits in visual attention/saliency processing.",
            "citation_title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs",
            "mention_or_use": "use",
            "model_name": "pooled models (aggregated behavior reported)",
            "model_description": "Aggregate behavior across evaluated MLLMs on generated CF3 variants varying the start-point marker size.",
            "model_size": null,
            "test_name": "CF3 — Copying Test (start marker identification across marker sizes)",
            "test_category": "visual perception / perceptual attention",
            "test_description": "Detect and identify the circled starting dot for the copying task; marker size (visual saliency) is varied to test sensitivity.",
            "evaluation_metric": "accuracy (percentage correct start-point identification)",
            "human_performance": null,
            "llm_performance": "Start-point identification accuracy: 92% with large circular markers, 80% with medium markers, 68% with small markers (reported across models/instances).",
            "prompting_method": "zero-shot image-based prompts on generated CF3 instances",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Authors highlight systematic degradation as marker size shrinks, suggesting model visual-attention mechanisms fail when saliency is reduced; this is used to argue limited low-level visual sensitivity.",
            "uuid": "e7230.3",
            "source_info": {
                "paper_title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "P3_middle_score_anomaly",
            "name_full": "P3 (Identical Pictures Test) — middle-score anomaly in models",
            "brief_description": "Observed intermediate accuracy (30–50%) of models on a task that humans typically solve near-perfectly or at chance, suggesting inconsistent/perceptually noisy model behavior.",
            "citation_title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs",
            "mention_or_use": "use",
            "model_name": "multiple evaluated MLLMs (pooled observations)",
            "model_description": "Set of 20 frontier MLLMs across GPT, Gemini, Claude, LLaMA, Qwen, and SEED families evaluated on VISFACTOR.",
            "model_size": null,
            "test_name": "P3 — Identical Pictures Test",
            "test_category": "perceptual speed / visual discrimination",
            "test_description": "Decide whether two images depict exactly the same object; tests fine-grained visual comparison and perceptual discrimination.",
            "evaluation_metric": "accuracy (percentage correct TRUE/FALSE decisions)",
            "human_performance": null,
            "llm_performance": "Most models obtain 30–50% accuracy on P3 (reported), which is atypical compared to human performance patterns.",
            "prompting_method": "zero-shot; Chain-of-Thought (CoT) variants were also tested and sometimes degraded performance on perceptual tasks",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Paper notes a 'Middle Score Anomaly': humans either perform almost perfectly or at chance on P3; the observed intermediate model accuracies imply partial, inconsistent perceptual processing. The paper explicitly states no contemporary human baseline was collected for the digitized items.",
            "uuid": "e7230.4",
            "source_info": {
                "paper_title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MA1_memorization_GPT-4.1",
            "name_full": "MA1 (Picture-Number Test) — memorization performance (GPT-4.1)",
            "brief_description": "GPT-4.1 attains perfect performance on the original (easier) MA1 memorization set but degrades substantially as the memorization load increases in the 'hard' generated variant.",
            "citation_title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs",
            "mention_or_use": "use",
            "model_name": "GPT-4.1-2025-04-14",
            "model_description": "OpenAI GPT-4.1 family model (vision-enabled), used to evaluate generated difficulty-controlled variants.",
            "model_size": null,
            "test_name": "MA1 — Picture-Number (associative memory)",
            "test_category": "associative memory / visual memory",
            "test_description": "Memorize image-number pairs and recall the number associated with a shown picture; measures associative visual memory capacity.",
            "evaluation_metric": "accuracy (percentage correct recalled number)",
            "human_performance": null,
            "llm_performance": "100% accuracy on the original MA1 (21 picture-number pairs) for GPT-4.1; substantial performance drop reported on a hard generated variant with 50 pairs (no precise numeric reported in text).",
            "prompting_method": "zero-shot on generated and original items",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Authors interpret the high success on MA1 original items as evidence that models can memorize associatively when inputs are conceptually interpretable; performance collapses when memorization load or abstraction increases.",
            "uuid": "e7230.5",
            "source_info": {
                "paper_title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Blink: Multimodal large language models can see but not perceive",
            "rating": 2,
            "sanitized_title": "blink_multimodal_large_language_models_can_see_but_not_perceive"
        },
        {
            "paper_title": "Does spatial cognition emerge in frontier models?",
            "rating": 2,
            "sanitized_title": "does_spatial_cognition_emerge_in_frontier_models"
        },
        {
            "paper_title": "Vision language models are blind: Failing to translate detailed visual features into words",
            "rating": 2,
            "sanitized_title": "vision_language_models_are_blind_failing_to_translate_detailed_visual_features_into_words"
        },
        {
            "paper_title": "Forgotten polygons: Multimodal large language models are shape-blind",
            "rating": 2,
            "sanitized_title": "forgotten_polygons_multimodal_large_language_models_are_shapeblind"
        },
        {
            "paper_title": "VisualSphinx: Large-scale synthetic vision logic puzzles for RL",
            "rating": 1,
            "sanitized_title": "visualsphinx_largescale_synthetic_vision_logic_puzzles_for_rl"
        }
    ],
    "cost": 0.014510249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs
7 Aug 2025</p>
<p>Jen-Tse Huang 
Johns Hopkins University</p>
<p>Dasen Dai 
Chinese University of Hong Kong</p>
<p>Jen-Yuan Huang 
Peking University</p>
<p>Youliang Yuan 
Chinese University of Hong Kong
Shenzhen</p>
<p>Xiaoyuan Liu 
Chinese University of Hong Kong
Shenzhen</p>
<p>Wenxuan Wang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#106;&#119;&#120;&#119;&#97;&#110;&#103;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#106;&#119;&#120;&#119;&#97;&#110;&#103;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a> 
Lab
Renmin Univer-sity of China
6 Tencent 7 Shanghai AI</p>
<p>Wenxiang Jiao 
Pinjia He 
Chinese University of Hong Kong
Shenzhen</p>
<p>Zhaopeng Tu 
Haodong Duan anhaodong@pjlab.org.cn&gt;. </p>
<p>Lab
Tencent AI</p>
<p>Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs
7 Aug 20258FC6743A28E2BDD0E9980AD6EF7CE25EarXiv:2502.16435v2[cs.CV]
Despite significant progress on popular multimodal benchmarks, state-of-the-art Multimodal Large Language Models (MLLMs) continue to struggle with basic visual reasoning tasks that are trivially solved by humans, such as recognizing spatial relationships.To systematically investigate this gap, we introduce VISFACTOR, a benchmark that digitizes 20 vision-centric subtests from a well-established cognitive psychology assessment.These subtests span four core domains of human visual cognition: (1) Visualization and Spatial Processing, (2) Perceptual and Closure, (3) Memory, and (4) Reasoning.We evaluate 20 frontier MLLMs from GPT, Gemini, Claude, LLaMA, Qwen, and SEED families.The best-performing model achieves a score of only 25.19 out of 100, with consistent failures on tasks such as mental rotation, spatial relation inference, and figure-ground discrimination-regardless of model size or prompting strategy.These findings suggest that current MLLM performance gains on high-level benchmarks do not reflect human-like low-level visual cognition, challenging the assumption that large-scale pretraining naturally induces gestalt-like perceptual capabilities.The dataset and evaluation toolkit are publicly available at: https://github.com/CUHK-ARISE/VisFactor.</p>
<p>Introduction</p>
<p>Multimodal Large Language Models (MLLMs) have rapidly advanced the state of multimodal artificial intelligence, delivering impressive results in text recognition (Liu et al., 2024b;Chen et al., 2025), mathematical reasoning (Yang arXiv Preprint, 2025.Copyright 2025 by the author(s).et al., 2024;Peng et al., 2024), and even clinical decision support (Azad et al., 2023;Buckley et al., 2023).On holistic leaderboards such as MM-Bench (Liu et al., 2024a), frontier models like Gemini-2.5-Pro(Kavukcuoglu, 2025) now surpass 88.9 out of 100, fueling optimism that large-scale pre-training may already endow machines with near-human visual cognition.</p>
<p>Closer inspection paints a different picture.Targeted studies reveal that MLLMs still falter on visual reasoning tasks that human novices solve effortlessly (Fu et al., 2024).Ramakrishnan et al. (2025) reports near-random accuracy on mental rotation test and maze completion test.Why do models that see so well in benchmarks apparently fail to perceive?</p>
<p>A key limitation lies in today's evaluation culture: most benchmarks emphasize downstream task performance and aggregate scores, but seldom probe the foundational visual faculties that underlie human reasoning.</p>
<p>Human vision develops hierarchically: low-level perceptual skills-figure-ground segregation, object permanence, spatial scanning-serve as scaffolds for higher-order reasoning.Cognitive psychology therefore decomposes vision into latent factors that can be measured independently.The Factor-Referenced Cognitive Test (FRCT) battery (Ekstrom &amp; Harman, 1976) operationalizes this idea, mapping psychometric factors to narrowly defined subtests.In contrast to omnibus IQ scales, the FRCT delivers a fine-grained cognitive profile, making it ideal for diagnosing which capacities an MLLM truly possesses.</p>
<p>We introduce VISFACTOR, the first benchmark that ports 20 vision-centric FRCT subtests to an automated, image-text setting, spanning four cognitive domains: (1) visualization and spatial processing, (2) perceptual and closure, (3) memory, and ( 4) reasoning (Fig. 1).Prior multimodal benchmarks (Ramakrishnan et al., 2025;Fu et al., 2024) often rely on four-option multiple-choice (25% chance) or binary yes/no (50% chance) formats, enabling models to reach nontrivial scores through random guessing or option-position biases.To preclude such shortcuts, we generate at least four rule-based variants for every VISFACTOR item and deliberately diversify the correct-answer distribution (e.g., multiple-choice keys are not always "A," yes/no items are not disproportionately "Yes").This design lowers the overall chance-level accuracy to 2.9/100, ensuring that any success on VISFACTOR reflects genuine reasoning rather than lucky guesses.</p>
<p>We evaluate 20 frontier models drawn from GPT (Hurst et al., 2024;OpenAI, 2025a), o-series (OpenAI, 2025b), Gemini (Kavukcuoglu, 2025), Claude (Anthropic, 2025b), LLaMA (Meta, 2024), Qwen (Bai et al., 2025), and SEED (ByteDance, 2025) families.Despite advanced prompting strategies such as Chain-of-Thought (CoT) (Kojima et al., 2022;Wei et al., 2022), the best model scores 25.19 out of 100.Systematic failures emerge on mentalrotation, spatial-relation, and figure-ground tasks, independent of model size or architecture.The original FRCT contains a finite set of items, raising the risk that future models may overfit by training directly on the public benchmark.To future-proof VISFACTOR, we focus on the subtests where current models perform poorest and implement a generator that produces unlimited, difficulty-controlled instances in the style of the FRCT.Item parameters (e.g., rotation angle, occlusion level, distractor similarity) can be modulated to create graduated test suites, enabling longitudinal tracking without saturating the benchmark.</p>
<p>Our contributions are as follows: 1</p>
<p>1 VISFACTOR is implemented with VLMEvalKit (Duan et al., 2024) and available at https://github.com/CUHK-ARISE/VisFactor.</p>
<p>• Factor-grounded evaluation.We present the first benchmark that grounds MLLM assessment directly to human cognitive factors, bringing psychometric rigor to multimodal evaluation.</p>
<p>• Complete, future-proof framework.We digitize every FRCT vision item, devise rule-based variant generation with balanced answer keys, and introduce controllabledifficulty item synthesis for the hardest subtests.</p>
<p>• Comprehensive landscape study.We benchmark twenty state-of-the-art MLLMs, offering a panoramic view of current capabilities and pinpointing cognitive gaps that chart a roadmap for future research.</p>
<p>VISFACTOR Design and Implementation</p>
<p>This section introduces how we select tests from FRCT ( §2.1), how to fit the tests to MLLMs ( §2.2- §2.3), and how we generate more difficulty-controllable test cases ( §2.4).</p>
<p>Test Selection and Justification</p>
<p>The original FRCT battery comprises 72 subtests.We exclude those that cannot be assessed with a vision-language interface whose output is text only: (1) Image-production tasks (4 subtests): Figural Fluency (FF1-FF3) and Spatial Scanning (SS1) ask participants to draw or trace; this is incompatible with text-only output.( 2) Speech-dependent tasks (3 subtests): Memory Span (MS1-MS3) require subjects to write down what they hear and therefore probe speech-to-text rather than visual cognition.In the remaining 65 subtests, 45 of them can be completed with pure text input.Those demanding visual reasoning but accept text answers form our benchmark, VISFACTOR.The 20 subtests cover 10 FRCT factors: Closure Flexibility (CF), Closure Speed (CS), Induction (I), Associative Memory (MA), Visual Memory (MV), Perceptual Speed (P), Logical Reasoning (RL), Spatial Orientation (S), Spatial Scanning (SS), and Visualization (VZ). Figure 2 shows example questions and answers of each subtest.</p>
<p>Digitization and Prompt Design</p>
<p>Instructions.Directly feeding the human-oriented FRCT instructions to MLLMs prove verbose and occasionally ambiguous.We therefore aske GPT-4o and Gemini-2.5-Flash(via web applications) to summarize each instruction set to its minimal, MLLM-friendly form.A human annotator reconcile the two summaries with the originals, producing a concise final prompt for every subtest.</p>
<p>Questions and Answers.All images are captured at 300 dpi and cropped to the region containing only the task stimuli (no additional texts).Ground-truth answers are extracted verbatim from the FRCT manuals.</p>
<p>CF2</p>
<p>Hidden Patterns Test</p>
<p>(1) (2) Can the model in (1) be found across lines in (2)?A: Yes.</p>
<p>CF3</p>
<p>Copying Test</p>
<p>(1)</p>
<p>(2) Which is the end point after applying the pattern in (1) on grids in (2)?A: (5,2).</p>
<p>CS1</p>
<p>Gestalt Completion Test</p>
<p>What is the object?A: A hammer head.</p>
<p>CS2</p>
<p>Concealed Words Test</p>
<p>What is the word?A: giant.</p>
<p>CS3</p>
<p>Snowy Pictures</p>
<p>What is the object?A: A boat.</p>
<p>I3</p>
<p>Figure Classification</p>
<p>Group 1:  (2) Is the shape in (2) shown in shapes in (1)?A: Yes.</p>
<p>MV2</p>
<p>Building Memory</p>
<p>(1) (2) (3) Should the building in (3) be in place E of map (2) according to map (1)?A: No.</p>
<p>MV3 Map Memory</p>
<p>(1)</p>
<p>(2) Is the map of (2) shown in maps provided in (1)?A: No.</p>
<p>P3 Identical Pictures Test</p>
<p>(1)</p>
<p>(2) Are the two pictures the same?A: No.</p>
<p>RL2 Diagramming Relationships</p>
<p>Can the diagram show the relationship between: Desks, furniture, pencils?A: Yes.</p>
<p>S1</p>
<p>Card Rotations Test</p>
<p>SS3</p>
<p>Map Planning Test</p>
<p>Which building does the shortest route between F to T pass?A: 5.</p>
<p>VZ1</p>
<p>Form Board Test</p>
<p>Reducing Chance-Level Accuracy</p>
<p>To prevent inflated scores from lucky guesses, we modify test formats as follows, except CF3 (25-way), MA1 (21-way) and all fill-in-the-blank subtests (CS1-CS3) that already exhibit ≤ 5% random success.The average random guessing performance is reduced from 22.47% to 2.89%, with no single test exceeding 6.25%.</p>
<ol>
<li>
<p>Decomposed multiple choice: For seven subtests with five options (CF1, MV2, P3, RL2, SS2, VZ1, VZ2), we pose one yes/no query per option and require the model to answer all correctly for credit.Chance accuracy thus drops from 25% to (0.5) 5 ≈ 3.13%.We generate three variants per item-"A differs from B", "B matches A", "B differs from A"-so that "yes" and "no" answers are balanced, preventing easy success by models that consistently answer yes or no.The probability of guessing all three correctly by chance is (0.5) 4 = 6.25%.</p>
</li>
<li>
<p>Specialized rewrites: (i) SS3 (Map Planning Test).Each item asks participants to find the building number that the shortest path between a start and an end point passes in a map.Exchanging start and end leaves the correct answer unchanged.We therefore require the model to answer both directions correctly, lowering chance from 10% to 1%. (ii) VZ3 (Surface Development Test).Each item asks: which 3-D edge corresponds to the marked 2-D edge after folding?Since multiple 2-D edges may map to the same 3-D edge, simply swapping the query direction (asking which 2-D edge matches a given 3-D edge) would introduce one-to-many ambiguity and ill-defined ground truth.Therefore, we add additional questions asking whether a pair of 2-D and 3-D edges are the same, resulting in all "yes" ground truth.To create "no" pairs, we generate questions with cyclic-permuted 3-D edge labels (e.g.,
A → B → C → D → E → A).
MLLMs receive credit only if they correctly answer the fill-in-the-blank question and both yes/no questions; chance 14.6/4 = 3.65%.</p>
</li>
</ol>
<p>Synthetic Augmentation</p>
<p>A subset of tests-CF1-CF3, CS1-CS3, MA1, S1-S2, SS3, VZ1-VZ2-admits parametric generation.</p>
<p>CF1: Hidden Figures Test.We For sub-pattern detection we represent the user-supplied "model" as its own edge set and enumerate all translations obtained by aligning any model vertex with any pattern vertex; containment reduces to a constant-time subset test per translation, which is tractable for the small grids used here and yields exact, translation-invariant matches without recursion or graph isomorphism search.</p>
<p>CF2: Hidden Patterns Test.We introduce a graph-based generator that operates on an m × n lattice.We first enumerate the complete set E of admissible edges-unit horizontal, vertical, and diagonal connections between adjacent lattice nodes-yielding E = |E| potential segments.To guarantee global connectivity, we draw a uniformly random spanning tree T ⊂ E by performing a depth-first search with randomized successor order; this yields exactly N − 1 edges, where N = mn is the number of nodes.Desired edge density is controlled by sampling a target count k ∼ N (µ, σ 2 ) with µ = ρE and σ = ρ std E for user-specified density ρ ∈ (0, 1] and ρ std ; the sample is clipped to
[N − 1, E].
We then augment T with k − (N − 1) additional edges drawn without replacement from E \ T , producing a connected graph G = (V, E G ) whose expected density equals ρ.</p>
<p>CF3: Copying Test.We develop a procedural grid-walk generator that produces paired images.Each instance begins by laying out an m × n lattice whose node coordinates are computed analytically from a single size parameter, ensuring scale-invariance across resolutions.A start node is selected uniformly at random and a self-avoiding walk is grown whose length is drawn from a user-specified interval [min steps, max steps].At every extension step, the candidate set comprises all yet-unvisited lattice nodes; candidates that would yield a line segment collinear with any existing segment in the path are deterministically excluded via a zero-cross-product test, preventing visual overlap and ensuring topological diversity.Two images are rendered, a reference grid with the start node circled, and a path image of identical dimensions that shows only the start node and the resulting non-collinear walk.CS1: Gestalt Completion Test.We begin by curating object silhouettes and their labels from public image repositories.Each image is partially occluded with randomly oriented white strokes whose number and width scale linearly with a severity coefficient s ∈ [0, 1].</p>
<p>CS2: Concealed Words Test.We synthesize a tunable corpus of occluded word images by sampling from the top n list in the wordfreq Python library, retaining alphabetic tokens whose lengths fall within a user-defined interval and converting them to lower-case.Each word is rendered on a white canvas and then obfuscated by superimposing straight white line segments and circular blotches drawn at random positions.The number, thickness, and radius of these artifacts increase linearly with a continuous severity parameter s ∈ [0, 1], providing precise control over the level of visual concealment.</p>
<p>CS3: Snowy Pictures.Building on the silhouettes and labels introduced in CS1, we corrupt every input image in two successive steps.First, we overlay n r white rectangles whose side lengths are sampled uniformly up to a fixed fraction of the image's shorter edge, disrupting local continuity.Next, we draw n ℓ short, randomly oriented black line segments that imitate dense, edge-like clutter.Both n r and n ℓ scale linearly with a severity parameter s ∈ [0, 1].</p>
<p>MA1: Picture-Number Test.Also building on the source from CS1, we first draw N unique items without replace-ment and an equal-sized set of distinct two-digit integers {10, . . ., 99}.The two cells are concatenated horizontally to form an atomic pair, and all pairs are then tiled row-major into an r × c grid with rc ≥ N and |r − c| minimized to approximate isotropy, yielding a visually balanced layout regardless of N .A uniformly random pair is sampled to provide a query image and its label, while the full canvas supplies rich contextual clutter.</p>
<p>S1: Card Rotations Test.We devise a lightweight generator that first samples a simple, non-self-intersecting polygon by drawing i.i.d.polar radii and sorted angles, and repeatedly rejecting candidates whose (i) shortest edge falls below a minimum-length threshold and (ii) consecutive edgelength differences are within a tight tolerance-two filters that jointly suppress near-symmetries and visually imperceptible edges.We optionally apply a horizontal mirror, then rotate it by a uniformly random angle before centrally cropping back to the original spatial extent.From every base polygon we generate N views and record a binary label indicating whether the transformation involved only rotation (true) or a mirror-plus-rotation (false).</p>
<p>S2: Cube Comparisons Test.To decide whether two partial observations correspond to the same physical cube, we cast the problem as a constrained search over the 24 right-handed orientations of a cube in Z 3 .We first "pin" the first view as the reference orientation-its Up, Front and Right faces become the intrinsic Up, Front, Right faces of the cube-which lets us record its three symbols and their rotations in a baseline face-rotation table .For each of the 24 global orientations we then (i) map the observer's local axes to intrinsic cube faces via simple cross-product geometry, (ii) transform the second view's reported rotations into the reference frame by adding a pre-computed 90 • offset that aligns local "Up" vectors, and (iii) enforce two consistency constraints: (a) the same intrinsic face observed twice must carry identical symbols whose rotations are equivalent under the symbol's symmetry class (4-fold, 2-fold, or asymmetric), and (b) a symbol may not appear on two different faces.Finally, we randomly generate such three-face views and render them as perspective-correct 3-D cube images.</p>
<p>SS3: Map Planning Test.We model the city layout as a rectangular m × n lattice in an undirected graph, where each vertex represents a street intersection and each edge a unit-length street segment.From the fully connected lattice we remove a user-specified fraction r of edges, chosen uniformly at random, and tag their mid-points as circular "road-blocks," thereby enforcing non-traversable segments while preserving the geometry for visualization.N B quartersquare buildings are sampled without replacement from the (m − 1)(n − 1) grid cells, along with the two edges each of them touches.Perimeter intersections are labeled in clockwise order using spreadsheet-style indices (A-Z, AA, AB, . . .), after which start-end terminals are selected by random permutation until exactly one shortest path exists between them, which guarantees uniqueness while avoiding exhaustive search.The final instance thus comprises a sparse planar graph with a provably unique geodesic, alongside metadata for blocked edges, buildings and perimeter labels.</p>
<p>VZ1: Form Board Test.We design an automatic pipeline that transforms an arbitrary lattice-defined polygon into a "dissect-and-assemble" puzzle while guaranteeing a unique solution under rotation and translation.The target shape is first specified on an n × n integer grid as an ordered list of boundary edges.A random integer k ∈ {3, 4, 5} determines the number of genuine solution pieces.Starting from the full polygon, we iteratively bisect the currently largest fragment with straight grid-aligned cuts whose slopes are limited to +∞, 0, ±1, ±2, ±3.Each cut is accepted only if it produces two valid polygons, and the process terminates as soon as k fragments are obtained.To generate the remaining 5 − k distractor pieces, we re-cut one randomly chosen solution fragment, rejecting candidate fragments whose areas coincide with any existing piece, thereby ensuring that no spurious subset of distractors can reconstruct the target.</p>
<p>VZ2: Paper Folding Test.Starting from a unit-square sheet discretized into an n × n grid, our algorithm iteratively selects a random fold axis-horizontal, vertical, or an arbitrary offset diagonal of the form y = ±x + c.At each step, the square is partitioned by this axis; the half-plane judged closest to the sheet's geometric center remains stationary, while the opposite half is reflected via an analytic mapping that preserves affine structure.Crucially, we maintain (i) a "Polygon" describing the current outer outline, (ii) an ordered list of internal edges and crease lines, and (iii) the exact set of point holes.These entities are updated by reflecting only those primitives that lie on the moving half and clipping fold-axis segments to the unfolded outline, guaranteeing topological correctness even for degenerate or off-center folds.The complete state history enables deterministic reverse unfolding to generate the answer: holes are "back-propagated" by conditional reflection.Hyper-parameters and Prompts.We set the temperature to 0 for all models, except Qwen (minimum temperature 0.01) and LLaMA-3.2 (temperature 0.6).For Qwen, Top-P is set to 0.001; for LLaMA-3.2,Top-P is set to 0.9.The thinking budget is configured as high for Gemini-2.5 and o-series models.Greedy decoding is used as the default sampling strategy.All models are accessed via their official APIs, except LLaMA-3.2, which is run locally.In our implementation, the retry count is set to 3, allowing each case up to three retries before being marked as a failure.All test cases are conducted in a zero-shot setting.The exact prompts are provided in §A of the appendix.</p>
<p>Experiments</p>
<p>Results on Original Tests</p>
<p>Most existing models perform poorly on the VISFAC-TOR benchmark.Among the 20 evaluated frontier models, Claude-  Model size and recency do not guarantee superior performance.For example, Qwen-2.5-32Boutperforms Qwen-2.5-72B, and Qwen-2-72B also surpasses Qwen-2.5-72B.Similarly, Claude-3.7 outperforms Claude-4, and Seed-1.5 exceeds Seed-1.6.While there are exceptions-such as GPT-4o outperforming GPT-4o-Mini, and o3 surpassing o1-performance on VISFACTOR shows no consistent correlation with model scale or version.These results suggest that core visual capabilities may be underemphasized in current model development pipelines.
C F 1 C F 2 C F 3 C S 1 C S 2 C S 3 I 3 M A 1 M V 1 M V 2 M V 3 P 3 R L 2 S 1 S 2 S S 2 S S 3 V Z 1 V Z 2 V Z 3 T o t
CoT offers limited benefits.We evaluate the effect of CoT prompting across three GPT models.While CoT provides some improvements, the gains in overall performance are marginal.This aligns with recent findings showing that CoT does not universally enhance model performance; in fact, certain cognitive tasks may exhibit degraded performance with CoT (Liu et al., 2025a).Specifically, we observe declines in performance on perceptual and closure tasks (P3, CS2) and spatial visualization tasks (SS3, VZ1).Conversely, CoT consistently improves performance on reasoning tasks such as I3 and RL2, consistent with prior results (Sprague et al., 2025).</p>
<p>The "Middle Score Anomaly" (Babaiee et al., 2025) is also observed in our VISFACTOR.This phenomenon refers to models unexpectedly achieving intermediate performance-neither random nor near-perfect-on tasks that are extremely easy for humans.For instance, the Identical Pictures Test (P3) simply requires determining whether two images depict the same object.Humans can either solve this task almost perfectly or fail entirely (i.e., perform at chance level if they lack the necessary perceptual ability).It would be highly unusual for a human to achieve, say, 70% accuracy on this task-suggesting partial understanding but inexplicable failures.However, we observe that most models obtain 30-50% accuracy on P3, while random guessing Table 2.The performance of the GPT-4.1 model on the generated subsets in VISFACTOR.The "Original" row reports performance on the original FRCT questions.The "Normal" row uses the same configuration as the original questions.The "Easy" and "Hard" rows correspond to questions that are modified to be easier and more difficult, respectively.13%.We interpret this as evidence that current models lack genuine reasoning capabilities, at least in the context of the tasks presented in VISFACTOR.
C F 1 C F 2 C F 3 C S 1 C S 2 C S 3 M A 1 S 1 S 2 S S 3 V Z 1 V Z 2 T o</p>
<p>Results on Generated Tests</p>
<p>Using our generation algorithms, we first construct a "Normal" subset in which each configuration closely mirrors the original FRCT questions.We then create "Easy" and "Hard" subsets by systematically adjusting parameters that modulate task difficulty.For instance, we vary the grid size for CF1, CF2, CF3, SS3, and VZ1; the noise severity for CS1, CS2, and CS3; the number of item pairs to be memorized in MA1; and the number of folds in VZ2.</p>
<p>We evaluate the GPT-4.1-2025-04-14model, and the results are presented in Table 2.The model's performance increases progressively across the easy, normal, and hard subsets.Our key findings are as follows:</p>
<p>(1) CS1-3 (object and word recognition under noise): The model achieves higher accuracy on our generated datasets compared to the original ones.</p>
<p>We attribute this to our selection of commonly encountered objects in daily life, which likely reduces recognition difficulty.Moreover, our framework supports dynamic image updates, allowing the tests to be refreshed as needed in the future.( 2   how they are able to solve these problems.An intuitive hypothesis is that models translate visual cues into high-level, human-interpretable concepts (e.g., "soccer," "chair," "fish") and memorize the concept-number pairs, rather than the raw image patterns.To test this hypothesis, we use CF2generated images, which consist only of lines arranged in a 3 × 3 grid, to create MA1 test cases via our automatic generation algorithm (see Fig. 4 for an example).When evaluated on these inputs, GPT-4.1'saccuracy drops sharply-from 90% to 23%-indicating a strong reliance on interpretable concepts.To ensure that the performance drop is not simply due to distributional shift, we generate extreme yet valid visual combinations using diffusion models (e.g., "a horse on the moon").In these cases, the model maintains high accuracy, further supporting our hypothesis: the model performs well as long as the visual input can be mapped to familiar, conceptual categories.These results also suggest that models struggle to interpret abstract visual patterns such as the line-based CF2 stimuli, reinforcing the idea that their success depends on concept recognition rather than low-level perception.</p>
<p>This hypothesis is further supported by the analysis of the P3 task, which reveals that high-performing examples typically involve easily verbalizable content, whereas failures are associated with visually complex and linguistically demanding patterns.These findings suggest that apparent visual comparison abilities may primarily reflect advanced textual reasoning applied to visual descriptions, rather than authentic visual processing.</p>
<p>Visual Recognition: A Key Bottleneck</p>
<p>Rely on Accurate Textual Descriptions.Our comprehensive evaluation reveals a stark contrast between models' strong textual reasoning capabilities and their significantly weaker visual perception performance.This disparity is exemplified by the CF3 task: when models are provided with textual descriptions of line segments (defined by starting coordinates and displacement vectors), GPT-4.1 achieves perfect accuracy (100%).In contrast, performance drops sharply when the same information has to be inferred from visual inputs, with accuracy falling to just 6.2%-and no model exceeding 18.8%.</p>
<p>Fail to Recognize Visual Details.In the SS2 task, models consistently fail to distinguish between intersecting lines with explicit junction markers versus those without visual indicators.More critically, our automated generated CF3 test cases (illustrated in 5) reveal that start-point identification accuracy deteriorates systematically with marker size variation: from 92% with large circular markers to 80% with medium markers, and ultimately 68% with small markers.This degradation pattern suggests fundamental constraints in the models' visual attention mechanisms, where reduced visual saliency directly compromises recognition performance.</p>
<p>Additionally, models struggle to focus effectively on key regions, resulting in missed information.For example, in CS2, the task involves identifying the partially erased word "women."Correct identification of the first character requires recognizing the faint stroke in the lower left corner that differentiates "w" from "v." Similarly, identifying the fifth character as "n" relies on detecting a small vertical stroke in the lower right corner of the letter.Models, however, misclassify these characters as "v" and "r," respectively, indicating its limited ability to prioritize critical local features.</p>
<p>Low Sensitivity to Length, Angle, and Scale</p>
<p>Models exhibit notable limitations in processing geometric shapes, particularly in assessing length and proportion.In the CF3 (Copying Test), the task is to replicate lines from the left side onto a 5 × 5 dot matrix on the right.While models can approximate line directions, they frequently err in determining their lengths.Similarly, in the VZ1 (Form Board Test), although models correctly identify the need for a rectangle to construct a complex figure, they fail to select sides of the appropriate length.These results indicate that while models possess some geometric recognition abilities, they struggle with accurately gauging line lengths and proportions, limiting their performance in tasks requiring precise spatial measurements.</p>
<p>While models frequently err in length and scale estimation across CF3 and VZ1 tasks, angular discrimination presents substantially more severe limitations.Systematic evaluation of directional vectors reveals a categorical bias toward canonical orientations: models consistently misclassified non-cardinal directions as 45-degree angles.In controlled testing with 20 systematically selected non-45-degree vectors (e.g., displacement vector [2,1]), models achieve zero correct angular classifications, invariably defaulting to the nearest 45-degree approximation.This suggests that models possess coarse categorical representations of spatial orientation rather than continuous angular perception.</p>
<p>Related Work</p>
<p>Evaluation with Natural Images.Natural images are commonly used to evaluate the visual capabilities of MLLMs, as they more closely reflect real-world scenarios (Zhao et al., 2024;Liu et al., 2024a;Chow et al., 2025;Wadhawan et al., 2024).Recent research has emphasized MLLMs' spatial reasoning abilities (Kamath et al., 2023;Liu et al., 2023), including tasks such as top-view map interpretation (Li et al., 2024) and region-level depth reasoning (Cheng et al., 2024).However, we argue that natural images often introduce additional noise and variability, making them less suitable for assessing core visual competencies.While benchmarks such as Blink (Fu et al., 2024), MMT-Bench and HallusionBench (Ying et al., 2024;Guan et al., 2024), and CoreCognition (Li et al., 2025b) incorporate synthetic images for tasks like IQ testing, visual hallucination detection, and physical reasoning, their overall focus remains primarily on natural image settings.</p>
<p>Evaluation with Synthetic Images.Synthetic images have been widely employed to evaluate the fundamental visual reasoning capabilities of MLLMs (Rahmanzadehgervi et al., 2024;Wu et al., 2024a;Chollet et al., 2025).Prior work has leveraged tasks such as Raven's Progressive Matrices (Zhang et al., 2024;Song et al., 2024) and the Logic Test from the Chinese Civil Service Examination (Song et al., 2025), which include puzzles conceptually related to our I3 task.VisualSphinx (Feng et al., 2025) further extends this line of work by generating puzzles structurally similar to RPMs.Mental Rotation Tests have also been frequently used (Ramakrishnan et al., 2025;Song et al., 2024), aligning with the design of our S1 and S2 tasks.In addition, synthetic images have supported evaluations of MLLMs on mathematical reasoning problems (Lu et al., 2024;Wang et al., 2025a), including polygons (Rudman et al., 2025) and graph-based challenges (Babaiee et al., 2025).Our proposed VISFACTOR advances this direction by providing a more comprehensive evaluation framework for core visual abilities, including 20 tests, systematically grounded in factor analysis from cognitive science.Furthermore, we implement automatic generation for 12 tests, enabling unlimited training data and ensuring the long-term scalability of the benchmark through high-difficulty content.</p>
<p>Enhancing MLLMs' Visual Ability.A range of strategies have been proposed to strengthen spatial reasoning in MLLMs, including generating intermediate steps (Li et al., 2025a;Wu et al., 2024b), drawing auxiliary lines (Meng et al., 2023;Hu et al., 2024), incorporating coordinates or depth cues (Liu et al., 2025b;Cai et al., 2024), and augmenting training sets with reasoning data (Shao et al., 2024).</p>
<p>Our approach enables automatic generation of high-quality, difficulty-controlled test cases, offering effectively unlimited training data to enhance MLLMs' visual reasoning.</p>
<p>Using Psychological Tests on AI.Recent studies have evaluated AI models from psychological perspectives, including behavioral analysis (Coda-Forno et al., 2024), personality (Huang et al., 2024b;a), emotion (Huang et al., 2024b), and mental disorder (Coda-Forno et al., 2023).Research has found advanced human-like abilities in AI models, including Theory-of-Mind abilities (Liu et al., 2024c;Liang et al., 2023;Huang et al., 2025) and role-playing abilities (Ng et al., 2024;Wang et al., 2024b;2025b).Inspired from cognitive science, our work provides a comprehensive framework for evaluating foundational visual abilities.</p>
<p>Conclusion</p>
<p>We presented VISFACTOR, the first factor-grounded benchmark that transposes twenty vision-centric subtests from the Factor-Referenced Cognitive Test battery into an automated image-text setting.A systematic evaluation of twenty state-of-the-art MLLMs uncovers a striking gap: despite their prowess on holistic leaderboards, the best model attains only 25.19/100 on VISFACTOR, often performing near chance on tasks that human novices solve with ease.Chain-of-Thought lifts scores only marginally, indicating that the deficit is architectural rather than prompt-level.</p>
<p>Beyond exposing a missing substrate for genuine visual reasoning, these findings carry practical ramifications.Hallucinated perception in safety-critical applications, brittle spatial reasoning in robotics, and misaligned multimodal feedback loops all trace back to weak foundational vision.Bridging this gap will likely require curriculum-style pre-training that interleaves psychometric micro-tasks with natural images, embodied or 3-D data that grounds spatial relations, and factor-aligned loss functions that explicitly target low-level perceptual skills.By releasing VISFACTOR and its controllable-difficulty generator, we aim to catalyze these research directions and provide a rigorous yardstick for the next generation of visuocognitive AI.</p>
<p>A. Descriptions and Prompts for All Subtests</p>
<p>This section introduces each subtest in detail and provides the prompts we use in VISFACTOR.</p>
<p>A.1.Closure Flexibility (CF)</p>
<p>The Factor:</p>
<p>"The ability to hold a given visual percept or configuration in mind so as to disembed it from other well defined perceptual material."</p>
<p>Flexibility of closure, a cognitive factor involving the identification of a configuration within a distracting perceptual field, has been linked to the concept of field independence, though they are not considered identical constructs.Witkin (1971) related this factor to both Thurstone's flexibility of closure (Thurstone, 1938) and Guilford's adaptive flexibility (Guilford, 1967), suggesting similarities to field independence.Royce (1973) proposed that flexibility of closure may interact with higher-order cognitive factors, while Hettema (1968) posited it as conceptually situated between flexibility and speed of closure.Wardell (1973) argued for its identity with figural adaptive flexibility.Carroll (1974) defined flexibility of closure as involving short-term memory processes that match a figure to its surrounding field, and Cattell (1971) framed it as a restructuring ability central to personality and practical intelligence.</p>
<p>Prompt for CF1: Hidden Figures Test Look at the two images:</p>
<p>Below is the first image, one simple shape:</p>
<p>Below is the second image, a larger, complex pattern: Task: Decide whether the shape in the first image is hidden anywhere inside the second image.The shape will never be rotated, flipped, or resized.The shape will always be right-side-up and exactly the same size as in the first image.</p>
<p>Output: Respond with only one word: "TRUE" if it is present, "FALSE" if it is not, in JSON format as follows: {"answer": YOUR ANSWER HERE}.Output: Respond with only one word: "TRUE" if it is present, "FALSE" if it is not, in JSON format as follows: {"answer": YOUR ANSWER HERE}.</p>
<p>Prompt for CF3: Copying Test</p>
<p>Look at the two images:</p>
<p>Below is the first image, a simple line shape:</p>
<p>Below is the second image, a 5 times 5 grid of dots; one dot is circled as the starting point:</p>
<p>Task: Begin at the circled dot on the second image.Copy the shape shown in the first image onto the grid so that every corner of the line sits exactly on a dot.When you are done, the pattern on the grid must look the same as the shape in the first image.</p>
<p>Output: Respond with only a tuple, the dot you finally reach, as a (row, column) pair where the row is counted top-to-bottom and the column left-to-right, in JSON format as follows: {"answer": YOUR ANSWER HERE}.</p>
<p>A.3. Induction (I)</p>
<p>The Factor:</p>
<p>"The reasoning abilities involved in forming and trying out hypotheses that will fit a set of data."</p>
<p>Research on inductive reasoning suggests it involves both concept formation and hypothesis testing, functioning as a synthesizing process (Wardell, 1973).Evidence points to several subfactors, with figure classification being particularly distinct (Harris &amp; Harris, 1971).Guilford &amp; Hoepfner (1966) identified 16 types of inductive ability, while Dye &amp; Very (1968) proposed distinct inductive and symbolicinductive reasoning factors.Though Pawlick (1966) argued that induction and general reasoning are not separate, Cattell (1971) allowed for a possible figural reasoning factor.Carroll (1974) emphasized the role of long-term memory search in induction, noting that success depends on the content of a "general logic store" and the ability to construct new hypotheses through serial operations.</p>
<p>Prompt for I3:
Figure Classification</p>
<p>A.4. Associative Memory (MA)</p>
<p>The Factor:</p>
<p>"The ability to recall one part of a previously learned but otherwise unrelated pair of items when the other part of the pair is presented."</p>
<p>Tasks assessing this factor are similar to those used in paired-associates learning and may involve memory for nonmeaningful material.This factor reflects intermediate-term memory processes, where individual differences arise from the use of strategies such as short-term rehearsal and the identification of mnemonic mediators in long-term memory (Carroll, 1974).</p>
<p>B. Limitations</p>
<p>Psychometric Purity.VISFACTOR inherits the FRCT assumption that each sub-test isolates a single latent visual factor (Ekstrom et al., 1974;1975).In reality, human cognition is highly inter-dependent: even a seemingly "pure" mental-rotation item also taps working memory, executive control, and verbal encoding.Sub-test scores should therefore be read as upper-bound indicators of factor competence, not as proofs of modularity.For the same reason, factor-level comparisons across models must be interpreted with caution, especially when subtle prompt differences can shift the mixture of underlying skills that a model exploits.</p>
<p>Digitization Gap.The original FRCT was administered on paper, under timed and proctored conditions.Our pipeline converts items to separate images and accepts typed responses, eliminating motor demands but also removing contextual cues such as page layout and time pressure.We also simplify the original instructions for MLLMs.These changes inevitably alter item difficulty, so direct numerical comparisons with legacy human norms are inappropriate.</p>
<p>Missing Contemporary Human Baseline.We have not re-collected human performance under the digital protocol, leaving open questions about the relative difficulty of the adapted items and about ceiling effects that may mask model progress.Gathering calibrated human baselines-ideally across age groups and devices-would help normalize model scores and identify items whose difficulty distribution shifted during digitization.</p>
<p>CF1</p>
<p>in (1) be found in the pattern in (2)?A: No.</p>
<p>number is the object in (2) related to in (1)?A: 73.</p>
<p>Are the S and F connected via the white circle in box E? A: No.</p>
<p>the 3-D figure is related to edge 5 in the 2-D figure after folding?A: H.</p>
<p>Figure 2 .
2
Figure 2. VISFACTOR comprises 20 vision-centric cognitive subtests.Each task is designed to isolate core factors of human visual cognition, covering 10 distinct factors in total.The subtests are converted into either yes/no questions or fill-in-the-blank questions according to §2.3.Example stimuli, questions, and ground-truth answers are shown for each task.</p>
<p>model each pattern as a graph G = (V, E) embedded on an axis-aligned m × n lattice whose admissible edges join adjacent vertices (4neighbour plus the two diagonals).Generation starts by deterministically adding the perimeter edges, thereby fixing a closed bounding rectangle and seeding a single connected component.The target edge count is then drawn from k ∼ N (µ, σ 2 ) with µ = ρ|E| and σ = ρ std |E| for user-specified density ρ ∈ (0, 1] and ρ std , and clipped to [0, 1] • |E|.</p>
<p>Figure 3 .
3
Figure 3.Samples of our generated images.We can dynamically adjust test difficulties in VISFACTOR.For example, the grid size of CF3 is changed to 6 × 6 instead of 5 × 5, and 8 × 9 instead of 7 × 8 for SS3.</p>
<p>We evaluate 20 models: GPT-4o(Hurst et al., 2024),GPT-4o-Mini (OpenAI, 2024),GPT-4.1 (OpenAI,  2025a), Gemini-2.5-(Pro,Flash)(Kavukcuoglu, 2025), Claude-Sonnet-(3.5 (Anthropic, 2024), 3.7 (Anthropic,  2025a), 4 (Anthropic, 2025b)), Qwen-2-VL(Wang et al., 2024a), Qwen-2.5-VL-(32B,72B)(Bai et al., 2025), Qwen-VL-Max(Team, 2024), Seed-1.5-VL(Guo et al., 2025),Seed-1.6(ByteDance, 2025), Moonshot-V1-128K-Vision-Preview (MoonshotAI, 2025), LLaMA-3.2-Vision-(11B,90B) (Meta, 2024), o1(Jaech et al., 2024), o3 (OpenAI,  2025b), and o4-Mini (OpenAI, 2025b).</p>
<p>, RL2).They also perform best on CF1-CF3 and CS1, demonstrating superior recognition of lines and edges.(ii) Google's Gemini leads on P3 and VZ2, particularly excelling at VZ2, which requires precise spatial localization to identify holes in paper.(iii) Qwen leads on SS2, VZ1, and VZ3, indicating strong mental imagery capabilities for shape splicing and folding.(iv) Claude performs best on CS2, MV1, MV3, S2, and SS3.(v) Seed achieves the top score on CS3 and MV2.</p>
<p>) MA1 (memory test): The original version requires memorizing 21 image-number pairs, a task on which the model achieves 100% accuracy.In contrast, our hard version increases the number of pairs to 50, resulting in a substantial performance drop, highlighting the increased challenge.(3) VZ2 (paper folding test): The original dataset includes questions based on one to three folds.Our version expands this to include up to five folds, significantly increasing task complexity.The model fails to answer any of these questions correctly.These results demonstrate that our generated dataset effectively supports dynamic adjustment of test difficulty, making it suitable for evaluating increasingly capable models.</p>
<p>Figure 4 .
4
Figure 4.An example of our generated MA1 using CF2 figures.</p>
<p>Figure 5 .
5
Figure 5.Our generated CF3 figures with different marker sizes.</p>
<p>Prompt for CF2: Hidden Patterns Test Look at the two images: Below is the first image, a model: Below is the second image, a pattern: Task: Decide if the model in the first image is hidden anywhere in the pattern in the second image.The model must be in that exact position, no turning or flipping.</p>
<p>Look at the four images: Below is the first image, three figures in the Group 1: Below is the second image, three figures in the Group 2: Below is the second image, three figures in the Group 3: Below is the fourth image, the figure to classify: Task: Inside a group, all three figures share one rule.Different groups follow different rules.Find the rule and decide whether the figure in the fourth image belongs to Group 1, 2, or 3. Output: Respond with only the group number (1, 2, or 3), in JSON format as follows: {"answer": YOUR ANSWER HERE}.</p>
<p>Prompt for MA1: Picture-Number Test Look at the two images: Below is the first image, the 21 picture-number pairs to memorize: Below is the second image, a picture: Task: Write down the number that the picture in the second image belongs to, as shown in the first image.Output: Respond with only a number, in JSON format as follows: {"answer": YOUR ANSWER HERE}.</p>
<p>Id e n ti ca l Pi ct ur es Test 96
Test 42M em o ry 12 2 Pe rcep tu a l &amp; C l o s ur e 366 Memory , Associa tive 42 M em or y, V is u a l 80 Clo su re , S pe ed o f 9 4 P er ce pt ua l Spe ed 96 R e a so n in g 58 Visualizatio n &amp; Sp at ia l P ro ce ss in g 262 i s u a l iz a ti o n 12 8 d u c ti o n 2 8 In in g, Lo gi ca l 30 tation 62 n 28 tio Re as on Spati al Orien Spati al Sca nn ing 72 V Picture-Number Bu ild ing Me mo ry 24 M B oa ips 30 rm nsh atio ing Rel d Ro tati ons Tes Dia gra mm t 20 Cube Compari Car sons Test 42 Choosi ng a Path 32 t 40 ng Tes nni p Pla Ma Fo rd T es t 48 ap C lo s u re , Fle xibility of 17 6 Sh ap e M st 20 M Te em or y 24 Pa ng ldi pe r Fo Su rfa ce Dev elop ment Test 60em or y Test 32Hidden Figures Test 32 C o p y in g Te st 64caFi gu re Cl as sifiSnow y Pictu res 24Gest alt Com pleti on Test 20Co nc ea led W or ds Te st 50
H id d e n P at te rn s Te st 80 Figure 1.VISFACTOR integrates 20 subtests adapted from standardized human cognitive assessments.Subtests are organized into four major domains and weighted by test case count (shown numerically), which determines each segment' visual area.</p>
<p>Table 1 .
1
3.7-Sonnet achieves the highest overall score, but only reaches 25.2 out of 100.Even when aggregating the best-performing models across individual subtests, the com-The performance of 20 models on VISFACTOR.The bottom row shows the highest scores achieved by any model, while the rightmost column shows the total score.Darker scores show higher scores.The best model is Claude-3.7-Sonnet.</p>
<p>bined score is just 38.8.Models generally perform well on memorization tasks (MA1, MV1-MV3), indicating a strong ability to attend to relevant context in the input.A breakdown of top-performing models by subtest reveals distinct strengths: (i) OpenAI's o-series models excel at reasoning</p>
<p>A.2. Closure Speed (CS)The Factor:"The ability to unite an apparently disparate perceptual field into a single concept."The concept of speed of closure refers to the ability to rapidly recognize and organize ambiguous or partially obscured visual stimuli, a process distinct from flexibility of closure, which involves identifying a known configuration within complex figures.This skill is associated with the early identification of out-of-focus and close-up images(Frederiksen, 1967), and involves long-term memory search strategies(Carroll, 1974).It has been linked to cognitive factors like restraint-timidity(Cattell, 1971) and may reflect a broader aptitude for visual scanning and cognitiveaffective integration(Thurstone, 1944;Wardell, 1973;Roff, 1953;Adcock &amp; Martin, 1971;Messick &amp; French, 1975).Prompt for CS1: Gestalt Completion TestLook at the incomplete drawing below: Task: Write the name of the object you think it shows.Output: Respond with only one or two words, in JSON format as follows: {"answer": YOUR ANSWER HERE}.Prompt for CS2: Concealed Words TestLook at the image below, which shows one lowercase English word, but parts of the letters are missing: Task: Write the complete word.The word is at least four letters long.Use only lowercase letters.Output: Respond with only the answer word, in JSON format as follows: {"answer": YOUR ANSWER HERE}.Prompt for CS3: Snowy PicturesLook at this image below: Task: Even if parts are hidden, name the main object you see.Output: Respond with only one or two words, in JSON format as follows: {"answer": YOUR ANSWER HERE}.A.5. Visual Memory (MV)The Factor:"The ability to remember the configuration, location, and orientation of figural material."Visual memory involves distinct cognitive processes beyond mere test content, as suggested by research on iconic memory, which stores visual impressions(Thurstone, 1946).WhileThurstone (1946)argued that "the memorizing factor transcends the nature of the content," later studies demonstrated that visual memory is a multifaceted construct.Guilford (1967)identified six figural memory abilities, andPetrov (1970)Prompt for MV2: Building MemoryLook at the two images:Below is the first image, memorize where every building sits on this street map:Below is the second image, the streets are the same, but each block is labeled A, B, C, D, E:Below is the third image, a building: Task: Decide whether the building in the third image is in block E.Output: Respond with only one word: "TRUE" if it is, "FALSE" if it is not, in JSON format as follows: {"answer": YOUR ANSWER HERE}.Prompt for MV3: Map MemoryA.6. Perceptual Speed (P)The Factor:"Speed in comparing figures or symbols, scanning to find figures or symbols, or carrying out other very simple tasks involving visual perception."Perceptual speed has been described as comprising three components: (1) perceptual fluency, or the readiness with which individuals switch between alternating percepts; (2) decision speed, or the readiness of choice when the response is not fully driven by sensory input(Thurstone, 1938;Künnapas, 1969); and (3) immediate perceptual memory.Carroll (1974)defines perceptual speed as involving the temporal aspects of visual search through a field of specified elements by accessing sensory buffers.It may be related to flexibility of closure(Pawlick, 1966;Ekstrom, 1973)or to an "automatic process" factor.Additionally,(Royce, 1973)suggested it may be a subfactor of the scanning cognitive style and possibly linked to the automatization cognitive style.It may be the centroid of several subfactors (including form discrimination and symbol discrimination) which can be separated but are more usefully treated as a single concept for research purposes.Prompt for P3: Identical Pictures TestLook at the two images:Below is the first image, the target object:Below is the second image, the test object: Task: Decide whether the two objects are exactly the same.Output: Respond with only one word: "TRUE" if they are, "FALSE" if they are not, in JSON format as follows: {"answer": YOUR ANSWER HERE}.A.7. Logical Reasoning (RL)The Factor:"The ability to reason from premise to conclusion, or to evaluate the correctness of a conclusion."The cognitive factor historically referred to as "Deduction"(Thurstone, 1938), later termed "Syllogistic Reasoning," and also known as "Logical Evaluation", involves evaluating the correctness of presented answers rather than pure deductive reasoning(Guilford, 1967).Carroll (1974)emphasized its complexity, highlighting the need for retrieving meanings and algorithms from long-term memory and applying serial operations, with individual differences influenced by content, timing, and attentional focus on stimuli.Prompt for RL2: Diagramming RelationshipsLook at the image below:Each circle stands for one group of things.Simple rules: 1.A circle inside another: all things in the inner group belong to the outer group.2. Circles that overlap partly: the two groups share some, but not all, things.3. Circles that do not touch: the two groups share nothing.Task: Decide whether the image follows these rules for the three groups: Desks, furniture, pencils.Output: Respond with only one word: "TRUE" if it shows the relationships for the three groups, "FALSE" if it does not, in JSON format as follows: {"answer": YOUR ANSWER HERE}.A.8. Spatial Relations (S)The Factor:"The ability to perceive spatial patterns or to maintain orientation with respect to objects in space."Research has differentiated between spatial orientation and visualization, suggesting that while spatial orientation involves perceiving figures as wholes and performing mental rotation(Zimmerman, 1954;Werdelin &amp; Stjernberg, 1971), visualization requires more complex restructuring and serial operations(Carroll, 1974;Shepard &amp; Metzler, 1971).Although some distinguished between spatial relations and orientation (with the latter involving the observer's body),Guilford &amp; Hoepfner (1971)treated them as a single cognitive factor linked to egocentrism.Prompt for S1: Card Rotations TestLook at the two images:Below is the first image, the target shape:Below is the second image, the test shape:Task: The test shapes may be rotated, but they are not allowed to be flipped (mirrored).Decide whether test shape is the same shape as the target.Output: Respond with only one word: "TRUE" if it is, "FALSE" if it is not, in JSON format as follows: {"answer": YOUR ANSWER HERE}.Prompt for S2: Cube Comparisons TestLook at the two images:Below is the first image, the first cube:Below is the second image, the second cube: Rules: 1.Each cube has six faces.Every face shows a different letter, number, or symbol.2. Hidden faces may show any symbols, but no symbol appears on more than one face of the same cube.Task: Decide whether the following statement is true or false: the first cube is a certain view of the second cube after it is turned.(!!!) Three other prompts are: (1) the first cube is not any view of the second cube no matter how it is turned (2) the second cube is a certain view of the first cube after it is turned (3) the second cube is not any view of the first cube no matter how it is turned Output: Respond with only one word: "TRUE" or "FALSE", in JSON format as follows: {"answer": YOUR ANSWER HERE}.A.9. Spatial Scanning (SS)The Factor:"Speed in exploring visually a wide or complicated spatial field."The ability to navigate a paper maze relies on quickly scanning for viable paths and rejecting false leads, engaging a visual search process somewhat akin to scanning text for comprehension.While sometimes associated with "planning," the process primarily reflects a willingness to visually evaluate options before committing.Carroll (1974)noted that this skill involves managing sensory input and that individuals may adopt strategies such as working backward from the goal to simplify the task.Visualization and spatial orientation are related cognitive factors, yet visualization involves mentally restructuring figures into components for manipulation, making it more complex than spatial orientation, which deals with rotating entire figures.While some researchers view visualization as a higher-order or secondary factor encompassing various spatial abilities(Cattell, 1971;Royce, 1973), others emphasize its reliance on short-term visual memory and serial processing(Carroll, 1974).Analytic strategies, such as identifying symmetry and reflection planes, are often used in visualization tasks, as illustrated byShepard &amp; Feng (1972)Output: Respond with only one word: "TRUE" if it is or "FALSE" if it is not, in JSON format as follows: {"answer": YOUR ANSWER HERE}.Prompt for VZ2: Paper Folding TestLook at the two images:Below is the first image, a step-by-step drawing of a square sheet being folded (solid lines) and then punched (small circle marks):Below is the second image, the same sheet shown completely unfolded, with any holes that appear:
Flexibility and creativity. C J Adcock, W A Martin, The Journal of General Psychology. 8511971</p>
<p>Claude 3.5 sonnet. Anthropic Blog. Anthropic, Jun 20 2024. 2024</p>
<p>Claude 3.7 sonnet and claude code. Anthropic Blog. Anthropic, Feb 24 2025. 2025a</p>
<p>Introducing claude 4. Anthropic Blog. Anthropic, Mar 22 2025. 2025b</p>
<p>B Azad, R Azad, S Eskandari, A Bozorgpour, A Kazerouni, I Rekik, D Merhof, arXiv:2310.18689Foundational models in medical imaging: A comprehensive survey and future vision. 2023arXiv preprint</p>
<p>Visual graph arena: Evaluating visual conceptualization of vision and multimodal large language models. Z Babaiee, P Kiasari, D Rus, R Grosu, Forty-second International Conference on Machine Learning. 2025</p>
<p>S Bai, K Chen, X Liu, J Wang, W Ge, S Song, K Dang, P Wang, S Wang, J Tang, arXiv:2502.139235-vl technical report. 2025arXiv preprint</p>
<p>Multimodal foundation models exploit text to make medical image predictions. T Buckley, A Diao, J Rajpurkar, P Rodman, A , K Manrai, A , arXiv:2311.055912023arXiv preprint</p>
<p>Introduction to techniques used in seed1.6. ByteDance Seed Blog. Bytedance, Jun 25 2025. 2025</p>
<p>W Cai, I Ponomarenko, J Yuan, X Li, W Yang, H Dong, B Zhao, Spatialbot, arXiv:2406.13642Precise spatial understanding with vision language models. 2024arXiv preprint</p>
<p>Psychometric Tests As Cognitive Tasks: A New" Structure of Intellect. J B Carroll, No. 4. ERIC1974Technical Report</p>
<p>Abilities: Their structure, growth, and action. R B Cattell, 1971Houghton Mifflin</p>
<p>Ocean-ocr: Towards general ocr application via a vision-language model. S Chen, X Guo, Y Li, T Zhang, M Lin, D Kuang, Y Zhang, L Ming, F Zhang, Y Wang, arXiv:2501.155582025arXiv preprint</p>
<p>Grounded spatial reasoning in vision-language models. A.-C Cheng, H Yin, Y Fu, Q Guo, R Yang, J Kautz, X Wang, S Liu, Spatialrgpt, Advances in Neural Information Processing Systems. 202437</p>
<p>Physbench: Benchmarking and enhancing visionlanguage models for physical world understanding. F Chollet, M Knoop, G Kamradt, B Landers, H Pinkard, W Chow, J Mao, B Li, D Seita, V Guizilini, Y Wang, arXiv:2505.11831The Thirteenth International Conference on Learning Representations. 2025. 2025arXiv preprintArc-agi-2: A new challenge for frontier ai reasoning systems</p>
<p>Cogbench: a large language model walks into a psychology lab. J Coda-Forno, K Witte, A K Jagadish, M Binz, Z Akata, E Schulz, J Coda-Forno, M Binz, J X Wang, E Schulz, arXiv:2304.11111Inducing anxiety in large language models can induce bias. 2023. 2024arXiv preprintForty-first International Conference on Machine Learning</p>
<p>Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. H Duan, J Yang, Y Qiao, X Fang, L Chen, Y Liu, X Dong, Y Zang, P Zhang, J Wang, Proceedings of the 32nd ACM international conference on multimedia. the 32nd ACM international conference on multimedia2024</p>
<p>Growth changes in factorial structure by age and sex. Genetic Psychology Monographs. N W Dye, P S Very, Ekstrom, R. B. Cognitive Factors: Some Recent Literature. ERIC. 1968. 1973</p>
<p>Manual for kit of factorreferenced cognitive tests. R B Ekstrom, H H Harman, 1976. 1976Educational testing service</p>
<p>Problems of Replication of Seven Divergent Production Factors. R B Ekstrom, No. 5. ERIC1974Technical Report</p>
<p>An Attempt to Confirm Five Recently Identified Cognitive Factors. R B Ekstrom, Technical Report</p>
<p>Y Feng, Z Xu, F Jiang, Y Li, B Ramasubramanian, L Niu, B Y Lin, R Poovendran, arXiv:2505.23977Visualsphinx: Large-scale synthetic vision logic puzzles for rl. 1975. 2025arXiv preprint</p>
<p>Cognitive factors in the recognition of ambiguous auditory and visual stimuli. J R Frederiksen, Journal of Personality and Social Psychology. 71p211967</p>
<p>Blink: Multimodal large language models can see but not perceive. X Fu, Y Hu, B Li, Y Feng, H Wang, X Lin, D Roth, N A Smith, W.-C Ma, R Krishna, European Conference on Computer Vision. </p>
<p>. Springer, 2024</p>
<p>Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. T Guan, F Liu, X Wu, R Xian, Z Li, X Liu, X Wang, L Chen, F Huang, Y Yacoob, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>The nature of human intelligence. J P Guilford, 1967McGraw-Hill</p>
<p>Sixteen divergentproduction abilities at the ninth-grade level. J P Guilford, R Hoepfner, Multivariate Behavioral Research. 111966</p>
<p>The analysis of intelligence. J P Guilford, R Hoepfner, No Title). 1971</p>
<p>D Guo, F Wu, F Zhu, F Leng, G Shi, H Chen, H Fan, J Wang, J Jiang, J Wang, arXiv:2505.070625-vl technical report. 2025arXiv preprint</p>
<p>A factor analytic interpretation strategy. Educational and Psychological Measurement. M L Harris, C W Harris, 197131</p>
<p>Cognitive abilities as process variables. J Hettema, Journal of personality and social psychology. 1044611968</p>
<p>Visual sketchpad: Sketching as a visual chain of thought for multimodal language models. Y Hu, W Shi, X Fu, D Roth, M Ostendorf, L Zettlemoyer, N A Smith, R Krishna, Advances in Neural Information Processing Systems. 202437</p>
<p>On the reliability of psychological scales on large language models. J Huang, -T, W Jiao, M H Lam, E J Li, W Wang, M R Lyu, Proceedings of The 2024 Conference on Empirical Methods in Natural Language Processing. The 2024 Conference on Empirical Methods in Natural Language Processing2024a</p>
<p>Apathetic or empathetic? evaluating LLMs' emotional alignments with humans. J Huang, -T, M H Lam, E J Li, S Ren, W Wang, W Jiao, Z Tu, M R ; Lyu, J Huang, -T, E J Li, M H Lam, T Liang, W Wang, Y Yuan, W Jiao, X Wang, Z Tu, M R Lyu, The Thirteenth International Conference on Learning Representations. 2024b. 202537Advances in Neural Information Processing Systems</p>
<p>A Hurst, A Lerer, A P Goucher, A Perelman, A Ramesh, A Clark, A Ostrow, A Welihinda, A Hayes, A Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>A Jaech, A Kalai, A Lerer, A Richardson, A El-Kishky, A Low, A Helyar, A Madry, A Beutel, A Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>What's "up" with vision-language models? investigating their struggle with spatial reasoning. A Kamath, J Hessel, K.-W Chang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Gemini 2.5: Our most intelligent ai model. K Kavukcuoglu, Google Blog. Mar 25 2025. 2025</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in Neural Information Processing Systems. 202235</p>
<p>T Künnapas, Figural reversal rate and personal tempo. 196910</p>
<p>Topviewrs: Vision-language models as top-view spatial reasoners. C Li, C Zhang, H Zhou, N Collier, A Korhonen, I Vulić, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>C Li, W Wu, H Zhang, Y Xia, S Mao, L Dong, I Vulić, F Wei, arXiv:2501.07542Imagine while reasoning in space: Multimodal visualization-of-thought. 2025aarXiv preprint</p>
<p>Core knowledge deficits in multi-modal language models. Y Li, Q Gao, T Zhao, B Wang, H Sun, H Lyu, R D Hawkins, N Vasconcelos, T Golan, D Luo, H Deng, Forty-second International Conference on Machine Learning. 2025b</p>
<p>T Liang, Z He, J.-T Huang, W Wang, W Jiao, R Wang, Y Yang, Z Tu, S Shi, X Wang, arXiv:2310.20499Leveraging word guessing games to assess the intelligence of large language models. 2023arXiv preprint</p>
<p>Visual spatial reasoning. F Liu, G Emerson, N Collier, Transactions of the Association for Computational Linguistics. 112023</p>
<p>Mind your step (by step): Chain-ofthought can reduce performance on tasks where thinking makes humans worse. R Liu, J Geng, A J Wu, I Sucholutsky, T Lombrozo, T L Griffiths, Forty-second International Conference on Machine Learning. 2025a</p>
<p>Mmbench: Is your multi-modal model an all-around player?. Y Liu, H Duan, Y Zhang, B Li, S Zhang, W Zhao, Y Yuan, J Wang, C He, Z Liu, European conference on computer vision. </p>
<p>. Springer, 2024a</p>
<p>Ocrbench: on the hidden mystery of ocr in large multimodal models. Y Liu, Z Li, M Huang, B Yang, W Yu, C Li, X.-C Yin, C.-L Liu, L Jin, X Bai, Science China Information Sciences. 67122201022024b</p>
<p>Advancing spatial reasoning through coordinate alignment and chain-of-thought for embodied task planning. Y Liu, D Chi, S Wu, Z Zhang, Y Hu, L Zhang, Y Zhang, S Wu, T Cao, G Huang, arXiv:2501.100742025barXiv preprint</p>
<p>Investigating social intelligence of llms via intention understanding in an interactive game context. Z Liu, A Anand, P Zhou, J.-T Huang, J Zhao, Interintent, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024c</p>
<p>Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. P Lu, H Bansal, T Xia, J Liu, C Li, H Hajishirzi, H Cheng, K.-W Chang, M Galley, J Gao, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Chain of images for intuitively reasoning. F Meng, H Yang, Y Wang, M Zhang, arXiv:2311.092412023arXiv preprint</p>
<p>2: Revolutionizing edge ai and vision with open, customizable models. S Messick, J W French, Multivariate behavioral research. 1011975. Sep 25 2024. 2024Meta Blog</p>
<p>Multimodal image understanding model moonshot-v1-vision-preview. Moonshotai, Moonshot AI Blogs. Jan 2025. 2025</p>
<p>M T Ng, H T Tse, J.-T Huang, J Li, W Wang, M R Lyu, arXiv:2404.13957How well can llms echo us? evaluating ai chatbots' role-play ability with echo. 2024arXiv preprint</p>
<p>Gpt-4o mini: advancing cost-efficient intelligence. OpenAI Blog. Jul 18 2024. 2024OpenAI</p>
<p>Introducing gpt-4.1 in the api. Openai, OpenAI Blog. Apr 14 2025. 2025a</p>
<p>Introducing openai o3 and o4-mini. Openai, OpenAI Blog. Apr 16 2025. 2025b</p>
<p>Concepts and calculations in human cognitive abilities. K Pawlick, Handbook of multivariate experimental psychology. R B Cattell, 1966</p>
<p>S Peng, D Fu, L Gao, X Zhong, H Fu, Z Tang, arXiv:2409.00147Multimath: Bridging visual and mathematical reasoning for large language models. 2024arXiv preprint</p>
<p>Memory structure as a psychic function. Y Petrov, Voprosi Psikhologii. 161970</p>
<p>Vision language models are blind: Failing to translate detailed visual features into words. P Rahmanzadehgervi, L Bolton, M R Taesiri, A T Nguyen, arXiv:2407.065812024arXiv preprint</p>
<p>Does spatial cognition emerge in frontier models?. S K Ramakrishnan, E Wijmans, P Kraehenbuehl, V Koltun, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>A factorial study of tests in the perceptual area. M Roff, Psychometric Monograph. 81953</p>
<p>The conceptual framework for a multifactor theory of individual ity. J Royce, Multivariate Analysis and Psychological Theory. Royce Jr, London and New YorkAcademic Press1973</p>
<p>W Rudman, M Golovanevsky, A Bar, V Palit, Y Lecun, C Eickhoff, R Singh, arXiv:2502.15969Forgotten polygons: Multimodal large language models are shape-blind. 2025arXiv preprint</p>
<p>Visual cot: Advancing multimodal language models with a comprehensive dataset and benchmark for chain-of-thought reasoning. H Shao, S Qian, H Xiao, G Song, Z Zong, L Wang, Y Liu, H Li, R N Shepard, C Feng, Advances in Neural Information Processing Systems. 2024. 197237A chronometric study of mental paper folding</p>
<p>Mental rotation of threedimensional objects. R N Shepard, J Metzler, Science. 17139721971</p>
<p>W Song, Y Li, J Xu, G Wu, L Ming, K Yi, W Luo, H Li, Y Du, F Guo, arXiv:2406.05343A cognition inspired multilingual and multimodal general intelligence ability benchmark. 2024arXiv preprint</p>
<p>Y Song, T Ou, Y Kong, Z Li, G Neubig, X Yue, arXiv:2504.10342Visualpuzzles: Decoupling multimodal reasoning evaluation from domain knowledge. 2025arXiv preprint</p>
<p>To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. Z Sprague, F Yin, J D Rodriguez, D Jiang, M Wadhwa, P Singhal, X Zhao, X Ye, K Mahowald, G Durrett, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Introducing qwen-vl. Q Team, Qwen Blogs. Jan 2024. 2024</p>
<p>Primary mental abilities. L L Thurstone, Psychology Monographs. 11938</p>
<p>A factorial study of perception. L L Thurstone, 1944The University of Chicago Press</p>
<p>Theories of intelligence. The scientific monthly. L L Thurstone, 194662</p>
<p>Contextual: Evaluating context-sensitive text-rich visual reasoning in large multimodal models. R Wadhawan, H Bansal, K.-W Chang, N Peng, Forty-first International Conference on Machine Learning. 2024</p>
<p>Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. P Wang, S Bai, S Tan, S Wang, Z Fan, J Bai, K Chen, X Liu, J Wang, W Ge, arXiv:2409.121912024aarXiv preprint</p>
<p>Incharacter: Evaluating personality fidelity in role-playing agents through psychological interviews. P Wang, Z.-Z Li, F Yin, D Ran, C.-L ; Liu, X Wang, Y Xiao, J.-T Huang, S Yuan, R Xu, H Guo, Q Tu, Y Fei, Z Leng, W Wang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsLong Papers2025a. 2024b1Proceedings of the Computer Vision and Pattern Recognition Conference</p>
<p>X Wang, H Wang, Y Zhang, X Yuan, R Xu, J.-T Huang, S Yuan, H Guo, J Chen, W Wang, arXiv:2502.09082Coordinating llm-based persona simulation of established roles. 2025barXiv preprint</p>
<p>Possible changes in the taxonomies in royce. Center for Advanced Study in Theoretical Psychology. D Wardell, 1973</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>The relationship between difficulty and factor loadings of some visual-perceptual tests. I Werdelin, G Stjernberg, Scandinavian Journal of Psychology. 1211971. 1971Witkin, H. A. A manual for the embedded figures tests. Consulting Psychologists Press</p>
<p>A surprising failure? multimodal llms and the nlvr challenge. A Wu, K Brantley, Y Artzi, arXiv:2402.177932024aarXiv preprint</p>
<p>Mind's eye of llms: visualization-of-thought elicits spatial reasoning in large language models. W Wu, S Mao, Y Zhang, Y Xia, L Dong, L Cui, F Wei, Advances in Neural Information Processing Systems. 2024b37</p>
<p>Mathglm-vision: Solving mathematical problems with multi-modal large language model. Z Yang, J Chen, Z Du, W Yu, W Wang, W Hong, Z Jiang, B Xu, Y Dong, J Tang, arXiv:2409.137292024arXiv preprint</p>
<p>Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. K Ying, F Meng, J Wang, Z Li, H Lin, Y Yang, H Zhang, W Zhang, Y Lin, S Liu, Forty-first International Conference on Machine Learning. 2024</p>
<p>How far are we from intelligent visual deductive reasoning. Y Zhang, H Bai, R Zhang, J Gu, S Zhai, J Susskind, N Jaitly, The First Conference on Language Modeling. 2024</p>
<p>Learning to visual question answering, asking and assessment. H H Zhao, P Zhou, D Gao, M Z Shou, Lova3, Advances in Neural Information Processing Systems. 202437</p>
<p>The influence of item complexity upon the factor composition of a spatial visualization test. W S Zimmerman, Educational and Psychological Measurement. 1411954Rules: 1. You may switch lines only where a black dot is drawn</p>
<p>Lines that cross or touch without a dot are not connected. </p>
<p>The path must stay inside the chosen box and must not stop at a dead-end. Task: For box E, decide if there is one continuous line that: 1. Starts at S inside that box</p>
<p>Output: Respond with only one word: "TRUE" if box E meets all the rules. JSON format as follows: {"answer": YOUR ANSWER HERE}. FALSE" if it does not. Prompt for SS3: Map Planning Test Look at the city map shown in the image below: In the map: 1. Streets = black lines</p>
<p>Circles = road-blocks (you cannot cross there). </p>
<p>Task: Find the shortest street route from F to T. Rules: 1. The route will always touch the side of one and only one numbered building. Numbered squares = buildings</p>
<p>Touching only a corner does not count. </p>
<p>Output: Respond with only one number: the number on the building your shortest route touches. Move only along streets (horizontal or vertical), never through circles. Task: 1. Mentally follow every fold in the first image exactly as drawn. Do not flip or rotate the paper except for the folds shown</p>
<p>Imagine a hole being punched through all layers where each circle is drawn. </p>
<p>Unfold the paper, step by step, in reverse order of the folds, keeping the sheet's original orientation. </p>
<p>After it is flat. note where every hole should appear on the sheet</p>
<p>TRUE" if every hole (number and position) in the second image matches your mental result exactly, otherwise "FALSE. JSON format as follows: {"answer": YOUR ANSWER HERE}. Prompt for VZ3: Surface Development Test Look at the two images: Below is the first image. the flat paper: Below is the second image, the 3-D object: Task: Fold the flat paper in the first image on every dashed line so that the face marked X ends up on the outside of the 3-D object in the second image. Decide edge 5 on the flat paper in the first image touches which lettered edge on the 3-D object in the second image after folding</p>
<p>Decide whether the pair of one letter on the 3-D object in the second image and one number on the flat paper in the first image: (5, H) are two edges that touch each other after folding. Output: Respond with only one letter. ! , ! , JSON format as follows: {"answer. YOUR ANSWER HERE}</p>
<p>Output: Respond with only one word. ! , ! , JSON format as follows: {"answer. YOUR ANSWER HERE}FALSE" if they do not</p>            </div>
        </div>

    </div>
</body>
</html>
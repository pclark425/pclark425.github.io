<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6123 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6123</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6123</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-51000d9f79be0eefd7972fe94e3c71dddc90d2c6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/51000d9f79be0eefd7972fe94e3c71dddc90d2c6" target="_blank">Evaluating the Text-to-SQL Capabilities of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> An empirical evaluation of Text-to-SQL capabilities of the Codex language model is performed and it is demonstrated on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples.</p>
                <p><strong>Paper Abstract:</strong> We perform an empirical evaluation of Text-to-SQL capabilities of the Codex language model. We find that, without any finetuning, Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6123.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6123.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Execution Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatic metric measuring the percentage of predicted SQL queries whose execution result on the target database matches the gold/reference result.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Execute each model-generated SQL query against the target database and compare the returned result to the gold/reference result; count as correct if results match.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Fraction/percentage of development/test examples where execution results equal gold results (execution accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Codex (davinci-codex, cushman-codex); GPT-3 (ada, babbage, curie, davinci); baseline T5-3B</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Text-to-SQL / semantic parsing (natural-language-to-SQL translation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-generated SQL queries that aim to answer natural language questions over given database schemas and contents.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Best reported execution accuracy on Spider development set: davinci-codex with Create Table + Select 3 prompt achieved ~67% execution accuracy; many other models and prompts reported lower execution accuracies (see Table 1 and Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Spider development set (primary), Spider-Realistic (modified subset) used for comparison</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Compared to human judgments indirectly via test-suite and a manual annotation study; some predictions judged incorrect by execution accuracy were considered acceptable by a human annotator (see manual annotation results).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Execution accuracy can penalize semantically-equivalent queries written in different styles; single-run execution may miss semantic equivalence or nuanced correctness; depends on database content and may be affected by prompt-provided rows.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Text-to-SQL Capabilities of Large Language Models', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6123.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6123.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-Suite Accuracy (Test-suite Execution Accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Metric using a distilled suite of test databases/cases to evaluate semantic equivalence of predicted SQL queries to gold queries, measuring correctness across multiple scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Semantic evaluation for text-to-sql with distilled test suites</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Run each predicted SQL on a suite of alternative databases or test cases (distilled test-suite) designed to expose semantic differences; count as correct if outputs match gold across the suite.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Percentage of predictions passing the distilled test-suite (test-suite accuracy), intended to measure semantic equivalence beyond a single execution.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Codex (davinci-codex, cushman-codex); GPT-3 (ada, babbage, curie, davinci); baseline T5-3B</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Text-to-SQL / semantic parsing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-generated SQL queries evaluated for semantic equivalence to reference queries using multiple test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Best reported test-suite accuracy on Spider development set: davinci-codex with Create Table + Select 3 prompt achieved ~55.1% test-suite accuracy (paper summary cites ~56.5% in places; see tables), lower than execution accuracy due to stricter semantic checks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Spider development set with Zhong et al. (2020) test suites; Spider-Realistic used for robustness checks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Test-suite metric reduces false positives from single-run execution but still penalizes alternative valid translations; manual annotation showed that a portion of queries failing test-suite were judged acceptable by humans.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Designing distilled test suites is nontrivial; test-suite coverage may still miss some semantic equivalences or overly penalize acceptable variations; requires additional curated test cases per query.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Text-to-SQL Capabilities of Large Language Models', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6123.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6123.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Valid/Executable SQL (Validity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Binary metric indicating whether the model's generated output is syntactically valid and executable SQL (here, valid SQLite SQL).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Attempt to parse/execute the generated SQL in SQLite; mark as valid/executable if it runs without syntax errors (and references exist).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Percentage of generated predictions which are valid/executable SQL (VA).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Codex (davinci-codex, cushman-codex); GPT-3 family; T5 baselines</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Text-to-SQL / semantic parsing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM outputs are SQL queries; validity checks whether they are syntactically/executably correct in the dialect used.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Codex models achieve high VA rates (e.g., davinci-codex reported VA up to ~92.5% with Create Table + Select 1 prompt; many prompts and models produced VA in 80–92% range), while GPT-3 models without code fine-tuning produce much lower VA rates.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Evaluated on Spider development set (and Spider-Realistic variant).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Validity is an automated syntactic criterion and does not capture semantic correctness; human review required to judge ambiguous semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Executable queries may still be semantically incorrect; reliant on SQL dialect and presence of referenced columns/tables; some invalid outputs occur due to ambiguous column names or missing schema info.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Text-to-SQL Capabilities of Large Language Models', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6123.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6123.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ManualAnnotation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Manual Annotation of Errors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human-in-the-loop analysis where annotators inspect a sample of model outputs judged incorrect by automatic metrics, categorizing errors and judging acceptability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Randomly sample model predictions (100 valid-but-erroneous SQL predictions for davinci-codex with a specific prompt) and have human annotators label them across categories (Semantic Incorrect, Ambiguous Correct, Invalid SQL, and subcategories).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Qualitative labels and percentage breakdown across error categories; reporting the fraction of 'ambiguous but acceptable' outputs as judged by humans.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>davinci-codex (primary focus), cushman-codex</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Text-to-SQL / semantic parsing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Human annotation assesses whether model-generated SQL that fails automatic tests is nonetheless an acceptable translation of the natural language question.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Of 100 sampled valid-but-erroneous predictions: 55.1% were test-suite correct, 25.2% Semantic Incorrect, 11.3% Ambiguous Correct (human judged acceptable), 8.4% Invalid SQL. The annotator estimated that 31% of valid yet automatically-erroneous predictions were acceptable (Ambiguous Correct + subcategories).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Sampled from Spider development set predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Direct comparison showing that automatic metrics (execution/test-suite) penalize a nontrivial portion of outputs that humans consider acceptable; highlights gap between automated evaluation and human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Manual annotation is time-consuming, subjective, and was performed on a small sample (100 items); inter-annotator agreement not reported; categories chosen to reflect Codex-specific behaviors may not generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Text-to-SQL Capabilities of Large Language Models', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6123.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6123.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Datasets & Benchmarks (Spider, Spider-Realistic, GeoQuery, Scholar)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard datasets used to evaluate cross-domain and single-database Text-to-SQL performance, including Spider (cross-domain), Spider-Realistic (modified Spider), GeoQuery and Scholar (single-database few-shot settings).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Use dataset development/training splits to run zero-shot and few-shot evaluations; measure VA/EX/TS metrics and compare to finetuned baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Performance metrics reported include Valid SQL (VA), Execution Accuracy (EX), and Test-Suite Accuracy (TS); few-shot adaptation measured by varying number of in-prompt examples and comparing to finetuning baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Codex (davinci-codex, cushman-codex), GPT-3 variants, T5-3B finetuned baseline</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Text-to-SQL / semantic parsing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Model-generated SQL queries evaluated on held-out examples from these datasets to measure generalization and adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On Spider dev: davinci-codex reached up to ~67% EX and ~55% TS depending on prompt; on GeoQuery and Scholar few-shot settings, davinci-codex often outperformed a T5-3B model finetuned on the same few examples, especially on GeoQuery (with up to 40-shot prompts). Baseline T5-3B finetuned on full GeoQuery/Scholar achieved ~85.7% and ~87.2% test-suite accuracy respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Spider, Spider-Realistic (Deng et al. 2021), GeoQuery (Zelle & Mooney 1996), Scholar (Iyer et al. 2017; Finegan-Dollak et al. 2018 dataset variants).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human annotation used to interpret discrepancies between benchmark judgments and perceived correctness; benchmarks provide structured, large-scale evaluation but may penalize acceptable variations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Benchmarks differ in conventions (e.g., argmax convention), prompting can exploit dataset-specific conventions, and public availability of dev/test splits raises memorization concerns for models trained on public code corpora; Spider-Realistic was used to reduce artifacts from explicit column name references.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Text-to-SQL Capabilities of Large Language Models', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6123.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6123.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Design and Few-shot/Zero-shot Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of how prompt content and few-shot in-context examples affect LLM output quality, using multiple prompt styles (Question, API Docs, Select X, Create Table, Create Table + Select X, Few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Systematically vary prompt structure and number of in-context examples; measure VA/EX/TS across prompts and model sizes to assess sensitivity and adaptation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Compare VA/EX/TS across prompt variants and shot counts; also measure how model scale and context-window size affect performance (zero-shot vs n-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Codex (davinci-codex, cushman-codex), GPT-3 variants</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Text-to-SQL / semantic parsing (evaluation of LLM behavior under different prompt regimes)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-generated SQL behavior as a function of provided schema, sample rows, CREATE TABLE statements, and few-shot examples; prompt contents act as the 'evidence'/context for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Prompt content significantly affects performance: minimal Question prompt yields very low EX (~8.3%); adding schema in API Docs raises EX to ~56.8%; Create Table yields ~59.9% EX; Create Table + Select 3 (3 example rows) yields peak EX (~67.0% for davinci-codex). More rows can degrade performance; few-shot examples improve adaptation (e.g., davinci-codex outperforms finetuned T5 in many few-shot GeoQuery settings).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Evaluated primarily on Spider development set; few-shot experiments on GeoQuery and Scholar datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Prompt engineering can reduce errors that humans consider acceptable/ambiguous; few-shot prompting produced SQL style closer to gold examples as shown in manual examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance highly sensitive to prompt format and context-window limits (davinci-codex longer window aided performance); adding excessive database content can harm performance; API models have limited prompt size limiting number of few-shot examples; results may not transfer across datasets due to differing conventions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Text-to-SQL Capabilities of Large Language Models', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Semantic evaluation for text-to-sql with distilled test suites <em>(Rating: 2)</em></li>
                <li>Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task <em>(Rating: 2)</em></li>
                <li>Structure-grounded pretraining for text-to-sql <em>(Rating: 1)</em></li>
                <li>Improving text-to-SQL evaluation methodology <em>(Rating: 2)</em></li>
                <li>Learning to parse database queries using inductive logic programming <em>(Rating: 1)</em></li>
                <li>Picard: Parsing incrementally for constrained auto-regressive decoding from language models <em>(Rating: 1)</em></li>
                <li>Bridging textual and tabular data for crossdomain text-to-sql semantic parsing <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6123",
    "paper_id": "paper-51000d9f79be0eefd7972fe94e3c71dddc90d2c6",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "EX",
            "name_full": "Execution Accuracy",
            "brief_description": "Automatic metric measuring the percentage of predicted SQL queries whose execution result on the target database matches the gold/reference result.",
            "citation_title": "Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task",
            "mention_or_use": "use",
            "evaluation_method": "Execute each model-generated SQL query against the target database and compare the returned result to the gold/reference result; count as correct if results match.",
            "evaluation_criteria": "Fraction/percentage of development/test examples where execution results equal gold results (execution accuracy).",
            "llm_model_name": "Codex (davinci-codex, cushman-codex); GPT-3 (ada, babbage, curie, davinci); baseline T5-3B",
            "theory_domain": "Text-to-SQL / semantic parsing (natural-language-to-SQL translation)",
            "theory_description": "LLM-generated SQL queries that aim to answer natural language questions over given database schemas and contents.",
            "evaluation_results": "Best reported execution accuracy on Spider development set: davinci-codex with Create Table + Select 3 prompt achieved ~67% execution accuracy; many other models and prompts reported lower execution accuracies (see Table 1 and Table 4).",
            "benchmarks_or_datasets": "Spider development set (primary), Spider-Realistic (modified subset) used for comparison",
            "comparison_to_human": "Compared to human judgments indirectly via test-suite and a manual annotation study; some predictions judged incorrect by execution accuracy were considered acceptable by a human annotator (see manual annotation results).",
            "limitations_or_challenges": "Execution accuracy can penalize semantically-equivalent queries written in different styles; single-run execution may miss semantic equivalence or nuanced correctness; depends on database content and may be affected by prompt-provided rows.",
            "uuid": "e6123.0",
            "source_info": {
                "paper_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "TS",
            "name_full": "Test-Suite Accuracy (Test-suite Execution Accuracy)",
            "brief_description": "Metric using a distilled suite of test databases/cases to evaluate semantic equivalence of predicted SQL queries to gold queries, measuring correctness across multiple scenarios.",
            "citation_title": "Semantic evaluation for text-to-sql with distilled test suites",
            "mention_or_use": "use",
            "evaluation_method": "Run each predicted SQL on a suite of alternative databases or test cases (distilled test-suite) designed to expose semantic differences; count as correct if outputs match gold across the suite.",
            "evaluation_criteria": "Percentage of predictions passing the distilled test-suite (test-suite accuracy), intended to measure semantic equivalence beyond a single execution.",
            "llm_model_name": "Codex (davinci-codex, cushman-codex); GPT-3 (ada, babbage, curie, davinci); baseline T5-3B",
            "theory_domain": "Text-to-SQL / semantic parsing",
            "theory_description": "LLM-generated SQL queries evaluated for semantic equivalence to reference queries using multiple test cases.",
            "evaluation_results": "Best reported test-suite accuracy on Spider development set: davinci-codex with Create Table + Select 3 prompt achieved ~55.1% test-suite accuracy (paper summary cites ~56.5% in places; see tables), lower than execution accuracy due to stricter semantic checks.",
            "benchmarks_or_datasets": "Spider development set with Zhong et al. (2020) test suites; Spider-Realistic used for robustness checks.",
            "comparison_to_human": "Test-suite metric reduces false positives from single-run execution but still penalizes alternative valid translations; manual annotation showed that a portion of queries failing test-suite were judged acceptable by humans.",
            "limitations_or_challenges": "Designing distilled test suites is nontrivial; test-suite coverage may still miss some semantic equivalences or overly penalize acceptable variations; requires additional curated test cases per query.",
            "uuid": "e6123.1",
            "source_info": {
                "paper_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "VA",
            "name_full": "Valid/Executable SQL (Validity)",
            "brief_description": "Binary metric indicating whether the model's generated output is syntactically valid and executable SQL (here, valid SQLite SQL).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Attempt to parse/execute the generated SQL in SQLite; mark as valid/executable if it runs without syntax errors (and references exist).",
            "evaluation_criteria": "Percentage of generated predictions which are valid/executable SQL (VA).",
            "llm_model_name": "Codex (davinci-codex, cushman-codex); GPT-3 family; T5 baselines",
            "theory_domain": "Text-to-SQL / semantic parsing",
            "theory_description": "LLM outputs are SQL queries; validity checks whether they are syntactically/executably correct in the dialect used.",
            "evaluation_results": "Codex models achieve high VA rates (e.g., davinci-codex reported VA up to ~92.5% with Create Table + Select 1 prompt; many prompts and models produced VA in 80–92% range), while GPT-3 models without code fine-tuning produce much lower VA rates.",
            "benchmarks_or_datasets": "Evaluated on Spider development set (and Spider-Realistic variant).",
            "comparison_to_human": "Validity is an automated syntactic criterion and does not capture semantic correctness; human review required to judge ambiguous semantics.",
            "limitations_or_challenges": "Executable queries may still be semantically incorrect; reliant on SQL dialect and presence of referenced columns/tables; some invalid outputs occur due to ambiguous column names or missing schema info.",
            "uuid": "e6123.2",
            "source_info": {
                "paper_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "ManualAnnotation",
            "name_full": "Human Manual Annotation of Errors",
            "brief_description": "Human-in-the-loop analysis where annotators inspect a sample of model outputs judged incorrect by automatic metrics, categorizing errors and judging acceptability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Randomly sample model predictions (100 valid-but-erroneous SQL predictions for davinci-codex with a specific prompt) and have human annotators label them across categories (Semantic Incorrect, Ambiguous Correct, Invalid SQL, and subcategories).",
            "evaluation_criteria": "Qualitative labels and percentage breakdown across error categories; reporting the fraction of 'ambiguous but acceptable' outputs as judged by humans.",
            "llm_model_name": "davinci-codex (primary focus), cushman-codex",
            "theory_domain": "Text-to-SQL / semantic parsing",
            "theory_description": "Human annotation assesses whether model-generated SQL that fails automatic tests is nonetheless an acceptable translation of the natural language question.",
            "evaluation_results": "Of 100 sampled valid-but-erroneous predictions: 55.1% were test-suite correct, 25.2% Semantic Incorrect, 11.3% Ambiguous Correct (human judged acceptable), 8.4% Invalid SQL. The annotator estimated that 31% of valid yet automatically-erroneous predictions were acceptable (Ambiguous Correct + subcategories).",
            "benchmarks_or_datasets": "Sampled from Spider development set predictions.",
            "comparison_to_human": "Direct comparison showing that automatic metrics (execution/test-suite) penalize a nontrivial portion of outputs that humans consider acceptable; highlights gap between automated evaluation and human judgment.",
            "limitations_or_challenges": "Manual annotation is time-consuming, subjective, and was performed on a small sample (100 items); inter-annotator agreement not reported; categories chosen to reflect Codex-specific behaviors may not generalize.",
            "uuid": "e6123.3",
            "source_info": {
                "paper_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Benchmarks",
            "name_full": "Datasets & Benchmarks (Spider, Spider-Realistic, GeoQuery, Scholar)",
            "brief_description": "Standard datasets used to evaluate cross-domain and single-database Text-to-SQL performance, including Spider (cross-domain), Spider-Realistic (modified Spider), GeoQuery and Scholar (single-database few-shot settings).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Use dataset development/training splits to run zero-shot and few-shot evaluations; measure VA/EX/TS metrics and compare to finetuned baselines.",
            "evaluation_criteria": "Performance metrics reported include Valid SQL (VA), Execution Accuracy (EX), and Test-Suite Accuracy (TS); few-shot adaptation measured by varying number of in-prompt examples and comparing to finetuning baselines.",
            "llm_model_name": "Codex (davinci-codex, cushman-codex), GPT-3 variants, T5-3B finetuned baseline",
            "theory_domain": "Text-to-SQL / semantic parsing",
            "theory_description": "Model-generated SQL queries evaluated on held-out examples from these datasets to measure generalization and adaptation.",
            "evaluation_results": "On Spider dev: davinci-codex reached up to ~67% EX and ~55% TS depending on prompt; on GeoQuery and Scholar few-shot settings, davinci-codex often outperformed a T5-3B model finetuned on the same few examples, especially on GeoQuery (with up to 40-shot prompts). Baseline T5-3B finetuned on full GeoQuery/Scholar achieved ~85.7% and ~87.2% test-suite accuracy respectively.",
            "benchmarks_or_datasets": "Spider, Spider-Realistic (Deng et al. 2021), GeoQuery (Zelle & Mooney 1996), Scholar (Iyer et al. 2017; Finegan-Dollak et al. 2018 dataset variants).",
            "comparison_to_human": "Human annotation used to interpret discrepancies between benchmark judgments and perceived correctness; benchmarks provide structured, large-scale evaluation but may penalize acceptable variations.",
            "limitations_or_challenges": "Benchmarks differ in conventions (e.g., argmax convention), prompting can exploit dataset-specific conventions, and public availability of dev/test splits raises memorization concerns for models trained on public code corpora; Spider-Realistic was used to reduce artifacts from explicit column name references.",
            "uuid": "e6123.4",
            "source_info": {
                "paper_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "PromptEval",
            "name_full": "Prompt Design and Few-shot/Zero-shot Evaluation",
            "brief_description": "Evaluation of how prompt content and few-shot in-context examples affect LLM output quality, using multiple prompt styles (Question, API Docs, Select X, Create Table, Create Table + Select X, Few-shot).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Systematically vary prompt structure and number of in-context examples; measure VA/EX/TS across prompts and model sizes to assess sensitivity and adaptation capability.",
            "evaluation_criteria": "Compare VA/EX/TS across prompt variants and shot counts; also measure how model scale and context-window size affect performance (zero-shot vs n-shot).",
            "llm_model_name": "Codex (davinci-codex, cushman-codex), GPT-3 variants",
            "theory_domain": "Text-to-SQL / semantic parsing (evaluation of LLM behavior under different prompt regimes)",
            "theory_description": "LLM-generated SQL behavior as a function of provided schema, sample rows, CREATE TABLE statements, and few-shot examples; prompt contents act as the 'evidence'/context for generation.",
            "evaluation_results": "Prompt content significantly affects performance: minimal Question prompt yields very low EX (~8.3%); adding schema in API Docs raises EX to ~56.8%; Create Table yields ~59.9% EX; Create Table + Select 3 (3 example rows) yields peak EX (~67.0% for davinci-codex). More rows can degrade performance; few-shot examples improve adaptation (e.g., davinci-codex outperforms finetuned T5 in many few-shot GeoQuery settings).",
            "benchmarks_or_datasets": "Evaluated primarily on Spider development set; few-shot experiments on GeoQuery and Scholar datasets.",
            "comparison_to_human": "Prompt engineering can reduce errors that humans consider acceptable/ambiguous; few-shot prompting produced SQL style closer to gold examples as shown in manual examples.",
            "limitations_or_challenges": "Performance highly sensitive to prompt format and context-window limits (davinci-codex longer window aided performance); adding excessive database content can harm performance; API models have limited prompt size limiting number of few-shot examples; results may not transfer across datasets due to differing conventions.",
            "uuid": "e6123.5",
            "source_info": {
                "paper_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Semantic evaluation for text-to-sql with distilled test suites",
            "rating": 2
        },
        {
            "paper_title": "Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task",
            "rating": 2
        },
        {
            "paper_title": "Structure-grounded pretraining for text-to-sql",
            "rating": 1
        },
        {
            "paper_title": "Improving text-to-SQL evaluation methodology",
            "rating": 2
        },
        {
            "paper_title": "Learning to parse database queries using inductive logic programming",
            "rating": 1
        },
        {
            "paper_title": "Picard: Parsing incrementally for constrained auto-regressive decoding from language models",
            "rating": 1
        },
        {
            "paper_title": "Bridging textual and tabular data for crossdomain text-to-sql semantic parsing",
            "rating": 1
        }
    ],
    "cost": 0.01307475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating the Text-to-SQL Capabilities of Large Language Models</h1>
<p>Nitarshan Rajkumar ${ }^{1 *}$, Raymond $\mathrm{Li}^{2}$, Dzmitry Bahdanau ${ }^{2345}$<br>${ }^{1}$ University of Cambridge, ${ }^{2}$ ServiceNow, ${ }^{3}$ Mila, ${ }^{4}$ McGill University, ${ }^{5}$ Canada CIFAR AI Chair<br>nr500@cam.ac.uk, {raymond.li,dzmitry.bahdanau}@servicenow.com<br>https://github.com/nitarshan/codex-text2sql</p>
<h4>Abstract</h4>
<p>We perform an empirical evaluation of Text-toSQL capabilities of the Codex language model. We find that, without any finetuning, Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples.</p>
<h2>1 Introduction</h2>
<p>Translating natural language questions to SQL queries (Text-to-SQL) is an important business problem which has seen significant research interest. A common approach to this task involves training a model to produce a SQL query when given a question, a database schema, and possibly database content as inputs. A clear trend in this area is to finetune models pretrained on natural language; notably, performance significantly improves as larger pretrained models are used (Shaw et al., 2021; Scholak et al., 2021).</p>
<p>Recent results from the broader field demonstrate that simply scaling training data and model size for generative language models brings advanced capabilities, such as few-shot learning without finetuning (GPT-3, Brown et al., 2020) and code generation (Codex, Chen et al., 2021). In this work we study if such models are already competitive Text-to-SQL solutions without any further finetuning on task-specific training data, evaluating Codex and GPT-3 models of different sizes with varied prompts on Text-to-SQL benchmarks.</p>
<p>We find that Codex achieves a competitive performance of up to $67 \%$ execution accuracy on the Spider development set. We analyze the predicted queries that automatic evaluation judged as wrong</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Model | VA | EX | TS |
| :-- | :--: | :--: | :--: |
| Finetuned |  |  |  |
| T5-base | 72.7 | 57.9 | 54.5 |
| T5-large | 84.1 | 67.2 | 61.4 |
| T5-3B | 87.6 | 71.4 | 65.7 |
| T5-3B ${ }^{<em>}$ | 88.2 | 74.4 | 68.3 |
| T5-3B + PICARD ${ }^{</em>}$ | 97.8 | 79.1 | 71.7 |
| BRIDGE v2 ${ }^{<em>}$ | - | 68.0 | - |
| Inference-only |  |  |  |
| GPT-3 ada | 33.8 | 2.3 | 0.3 |
| GPT-3 babbage | 48.8 | 5.7 | 3.9 |
| GPT-3 curie | 70.9 | 12.6 | 8.3 |
| GPT-3 davinci | 65.0 | 26.3 | 21.7 |
| Codex cushman ${ }^{</em>}$ | 86.3 | 63.7 | 53.0 |
| Codex davinci ${ }^{*}$ | 91.6 | 67.0 | 55.1 |</p>
<p>Table 1: Best Spider development set performance across models, as measured by percentage of predictions which are valid SQL (VA), execution accuracy (EX), test-suite accuracy (TS). Models marked with * use database content. T5 results are from Scholak et al. (2021), BRIDGE v2 results are from Lin et al. (2020).
and find that many of them would be judged correct by humans, whereas others could likely be fixed within the no-finetuning paradigm. Lastly, using GeoQuery and Scholar benchmarks we show that adapting Codex to a specific domain by prompting it with few examples can be more effective than fine-tuning a smaller language model on the same examples.</p>
<h2>2 Experimental Setup</h2>
<p>Models Our evaluation focuses on the models accessible via the OpenAI API: GPT-3 (in the ascending ada, babbage, curie and davinci sizes) and Codex (in the ascending cushman-codex and davinci-codex sizes) ${ }^{1}$. These are generative language models which perform next-token prediction during training and inference; GPT-3 is trained on a diverse set of sources from the internet, and Codex is further finetuned on code from GitHub. We compare GPT-3 and Codex against methods from Shaw et al. (2021) using the T5 encoder-decoder</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Prompt</th>
<th>VA</th>
<th>EX</th>
<th>TS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Question</td>
<td>14.0</td>
<td>8.3</td>
<td>8.2</td>
</tr>
<tr>
<td>API Docs</td>
<td>83.8</td>
<td>56.8</td>
<td>47.5</td>
</tr>
<tr>
<td>Select 1</td>
<td>86.3</td>
<td>60.9</td>
<td>52.0</td>
</tr>
<tr>
<td>Select 3</td>
<td>85.8</td>
<td>60.3</td>
<td>52.2</td>
</tr>
<tr>
<td>Select 5</td>
<td>85.2</td>
<td>60.5</td>
<td>51.5</td>
</tr>
<tr>
<td>Select 10</td>
<td>86.0</td>
<td>60.8</td>
<td>51.2</td>
</tr>
<tr>
<td>Create Table</td>
<td>89.8</td>
<td>59.9</td>
<td>50.0</td>
</tr>
<tr>
<td>+ Select 1</td>
<td>92.5</td>
<td>64.8</td>
<td>53.7</td>
</tr>
<tr>
<td>+ Select 3</td>
<td>91.6</td>
<td>67.0</td>
<td>55.1</td>
</tr>
<tr>
<td>+ Select 5</td>
<td>91.0</td>
<td>65.3</td>
<td>53.9</td>
</tr>
<tr>
<td>+ Select 10</td>
<td>91.2</td>
<td>63.3</td>
<td>52.4</td>
</tr>
</tbody>
</table>
<p>model. Starting from public checkpoints pretrained on Common Crawl, the T5 model is finetuned on Spider to predict the output SQL, conditioned on the question and schema. The 3B parameter T5 model is currently the state-of-the-art on Spider when combined with constrained inference using the PICARD algorithm <em>Scholak et al. (2021)</em>. We also compare to BRIDGE v2 <em>Lin et al. (2020)</em>, a sequence-to-sequence model based on BERT.</p>
<p>Zero-Shot Experiments We use the Spider benchmark <em>Yu et al. (2019)</em> for cross-domain Text-to-SQL. We report performance using percentage of development set predictions which are valid (executable) SQLite SQL, execution accuracy, and test-suite execution accuracy. The latter metric was proposed by <em>Zhong et al. (2020)</em> to measure semantic equivalence of SQL queries written in different styles, which is essential when comparing Codex to models trained on Spider. We address concerns around possible memorization of Spider data by Codex in Appendix A.5.</p>
<p>Few-Shot Experiments We re-purpose the question-splits of the GeoQuery and Scholar datasets <em>Zelle and Mooney (1996); Iyer et al. (2017); Finegan-Dollak et al. (2018)</em> to perform experiments in a few-shot setting. The examples in these datasets are grouped by query templates. Examples corresponding to the same template have the same SQL query structure, but may have different English questions and SQL literals. To define the few-shot task, we first sort the templates by their frequency in the training set. In the $n$-shot setting we then use one random example for each of the $n$ most frequent templates.</p>
<p>Prompts We use six prompt structures in our experiments (examples provided in Appendix C). Question provides no database information and just includes the question as a SQL comment. API Docs follows the style of the Text-to-SQL example in Codex documentation and includes a schema in a comment style which does not conform to SQLite standards. Select X includes in comments the results of executing a SELECT * FROM T LIMIT X query on each table, including schemas via column headers. Create Table includes the CREATE TABLE commands for each table, including column type and foreign key declarations. Create Table + Select $\mathbf{X}^{2}$ is a combination of the preceding two prompt formats. Finally, Fewshot additionally includes question-query pairs.</p>
<h2>3 Zero-Shot Results</h2>
<p>We present results for different model sizes in Table 1 and for different prompt styles in Table 2. Full results are available in Table 4 in Appendix B.</p>
<p>Codex provides a strong baseline for Text-toSQL tasks In Table 1 the best performing model (davinci-codex, Create Table + Select 3) achieves 67% execution accuracy and 56.5% test suite execution accuracy on Spider. This is comparable to the performance of the BRIDGE v2 <em>Lin et al. (2020)</em> model which achieved a (then) state-of-the-art 68% execution accuracy in December 2020.</p>
<p>Prompt design is critical for performance As seen in Table 2, providing the question alone results in a low 8.3% execution accuracy. There is a progressive improvement to 56.8% as schema information is introduced in API Docs, to 59.9% when valid SQL and foreign key information is used in Create Table, and to 67.0% when database content is introduced with Create Table + Select 3.</p>
<p>More database content can harm performance In Table 2 we observe that for the Select Limit X prompts there is a negligible change in performance when adding more rows. By contrast, Create Table + Select Limit X prompt accuracy peaks with 3 rows before significantly decreasing in performance as more rows are added.</p>
<p>Diminishing returns for Codex model size While GPT-3 performance significantly benefits from increased model size, the davinci-codex model does not perform drastically better than cess whitespace tokens less efficiently than Codex models, and therefore cannot evaluate Create Table + Select X prompts at all.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">14\%</th>
<th style="text-align: center;">Shortcuts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NL</td>
<td style="text-align: center;">What is the number of car models created by the car maker American Motor Company?</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">SELECT count (*) FROM CAR_MAKERS AS T1 JOIN MODEL_LIST AS T2 ON T1.Id = T2.Maker WHERE T1.FullName = 'American Motor Company';</td>
</tr>
<tr>
<td style="text-align: center;">Pred</td>
<td style="text-align: center;">SELECT COUNT (Model) FROM model_list WHERE Maker = 1;</td>
</tr>
<tr>
<td style="text-align: center;">NL</td>
<td style="text-align: center;">Give the city and country for the Alton airport.</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">SELECT City, Country FROM AIRPORTS WHERE AirportName = "Alton"</td>
</tr>
<tr>
<td style="text-align: center;">Pred</td>
<td style="text-align: center;">SELECT City, Country FROM airports WHERE AirportCode = 'ALN';</td>
</tr>
<tr>
<td style="text-align: center;">8\%</td>
<td style="text-align: center;">SELECT Extra Columns</td>
</tr>
<tr>
<td style="text-align: center;">NL</td>
<td style="text-align: center;">List names of conductors in descending order of years of work.</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">SELECT Name FROM conductor ORDER BY Year_of_Work DESC</td>
</tr>
<tr>
<td style="text-align: center;">Pred</td>
<td style="text-align: center;">SELECT Name, Year_of_Work FROM conductor ORDER BY Year_of_Work DESC;</td>
</tr>
<tr>
<td style="text-align: center;">5\%</td>
<td style="text-align: center;">SELECT Convention</td>
</tr>
<tr>
<td style="text-align: center;">NL</td>
<td style="text-align: center;">What are all the makers and models?</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">SELECT Maker , Model FROM MODEL_LIST;</td>
</tr>
<tr>
<td style="text-align: center;">Pred</td>
<td style="text-align: center;">SELECT DISTINCT car_makers.Maker, model_list.Model FROM car_makers JOIN model_list ON car_makers.Id = model_list.Maker;</td>
</tr>
</tbody>
</table>
<p>Figure 1: Examples of error types, as made by the davinci-codex model with Create Table + Select 3 prompt. NL stands for natural language question. Percentage indicates the percentage of errors which are of the given type. Further examples are provided in Figure 3 in Appendix B.
cushman-codex. Full results in Table 4 in Appendix B show cushman-codex generally being within 1 percentage point of davinci-codex for the same prompt style; it even performs 3 percentage points better for the Create Table prompt. These results suggest that davinci-codex's longer context window may be a greater contributor to its peak performance than increased parameter count.</p>
<h3>3.1 Error Analysis</h3>
<p>We focus our error analysis on the davinci-codex model with Create Table + Select 3 prompt, and present a breakdown of prediction types in Table 3 and examples of errors in Figure 1. Our error categories were chosen to surface the most interesting Codex-specific behaviours we observed amongst the errors made. We randomly selected and annotated 100 predictions which were valid SQL yet were judged incorrect by test-suite evaluation.</p>
<p>We first consider Semantic Incorrect behaviours, which Spider evaluation and the human annotator both view as incorrect predictions. Shortcut errors are where Codex made use of either specific table values or "world knowledge" from GPT-3 pretraining, while the ground-truth query contained the exact literals from the question. GROUP BY Convention errors are where Codex incorrectly groups on a non-primary-key column (such as a name or title column).</p>
<p>We also consider Ambiguous Correct behaviours which are semantically different from the gold query and are therefore judged as incorrect by Spider evaluation, but which the human annotator viewed as being an acceptable SQL translation of</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Annotation</th>
<th style="text-align: left;">\%</th>
<th style="text-align: left;">E\%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Test-Suite Correct</td>
<td style="text-align: left;">55.1</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Semantic Incorrect</td>
<td style="text-align: left;">25.2</td>
<td style="text-align: left;">69</td>
</tr>
<tr>
<td style="text-align: left;">- Shortcuts</td>
<td style="text-align: left;">5.1</td>
<td style="text-align: left;">14</td>
</tr>
<tr>
<td style="text-align: left;">- GROUP BY Convention</td>
<td style="text-align: left;">1.5</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">- Other</td>
<td style="text-align: left;">18.6</td>
<td style="text-align: left;">51</td>
</tr>
<tr>
<td style="text-align: left;">Ambiguous Correct</td>
<td style="text-align: left;">11.3</td>
<td style="text-align: left;">31</td>
</tr>
<tr>
<td style="text-align: left;">- SELECT Extra Columns</td>
<td style="text-align: left;">2.9</td>
<td style="text-align: left;">8</td>
</tr>
<tr>
<td style="text-align: left;">- SELECT Convention</td>
<td style="text-align: left;">1.8</td>
<td style="text-align: left;">5</td>
</tr>
<tr>
<td style="text-align: left;">- Argmax</td>
<td style="text-align: left;">1.5</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">- Other</td>
<td style="text-align: left;">5.1</td>
<td style="text-align: left;">14</td>
</tr>
<tr>
<td style="text-align: left;">Invalid SQL</td>
<td style="text-align: left;">8.4</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">- Ambiguous column name</td>
<td style="text-align: left;">1.9</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">- No such column</td>
<td style="text-align: left;">4.5</td>
<td style="text-align: left;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Breakdown of prediction annotations over Spider development set for the davinci-codex model with Create Table + Select 3 prompt. \% is percentage of all predictions, E\% is percentage of manually annotated erroneous queries (see Section Section 3.1 for details).
the given question. SELECT Convention errors are where Codex selects a different column than the per-database convention of the gold queries (such as name instead of ID). SELECT Extra Columns errors are where Codex includes additional useful columns in its query beyond what the gold query includes. Argmax errors are where Codex differs from the gold query in how a min/max resolution (such as "youngest singer") is handled for ties.</p>
<p>We observe in Table 3 that a significant $31 \%$ of valid yet erroneous predictions are penalized by Spider evaluation as being incorrect though a human annotator viewed them as acceptable solutions. Future work could be to investigate to what extent one can control the behaviour of Codex. This could allow to fix these ambiguous errors, either by prompt design or using a few examples.</p>
<p>4 Few-Shot</p>
<p>We investigate whether Codex can perform few-shot Text-to-SQL. As described in Section 2, we re-purpose the GeoQuery and Sholar datasets in a few-shot setting. It is well known that models trained on Spider transfer poorly to other singledatabase Text-to-SQL datasets <em>Suhr et al. (2020)</em> in a zero-shot setting. Studying few-shot Text-to-SQL on GeoQuery and Scholar should show to what extent models are able to leverage a small amount of examples to effectively adapt to a new domain.</p>
<p>Baseline The baseline is a T5-3B model that was finetuned on Spider, reaching 71% exact-match accuracy on Spider validation set. The model is then further finetuned on the new domain – GeoQuery or Scholar. The learning rate for domain-specific-finetuning was selected in the 20-shot setting among $[0.1,0.2,0.5,1,2]\cdot 10^{-5}$, based on the best validation set performance after 300 steps. We use batch-size 1024, such that all the few-shot examples fit in the same batch.</p>
<p>Codex Building on the Create Table + Select X prompt, we append $n$ question-query examples to the input in an $n$-shot setting. An example of this prompt is provided in Figure 11. All samples are generated using greedy decoding, with temperature 0. Note that for a given $n$-shot setting, the baseline and Codex use the same set of support examples. These examples are in the prompt for Codex, and used to finetune the baseline on the new domain. Given the limited window-size of API models, on GeoQuery we can feed up to 40 support examples to davinci-codex, and up to 10 examples to cushman-codex and GPT-3 models. On Scholar the queries are longer and the schema more complex – we fit only 10 examples in the prompt of davinci-codex, 5 for cushman-codex, and none at all for GPT-3 models.</p>
<h3>4.1 Results</h3>
<p>Figure 2 shows test-suite accuracies on the Scholar and GeoQuery datasets. The baseline reaches 85.7% test-set performance when trained on the complete GeoQuery training set (549 examples). Respectively, it reaches 87.2% test accuracy when trained on the whole Scholar training set (499 examples). This simple baseline is a very competitive model when considering the entire datasets. However Figure 2 shows that it is largely beaten by Codex in few-shot settings. In a zero-shot setting, both davinci-codex and cushman-codex al-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>(a) GeoQuery. When trained on the whole GeoQuery training set (549 examples), the finetuned T5 reaches 85.7% accuracy.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>(b) Scholar. When trained on the whole Scholar training set (499 examples), the finetuned T5 reaches 87.2% accuracy.</p>
<p>Figure 2: Test-suite accuracy with varying number of support examples. The x-axis shows the number of few-shot examples used.</p>
<p>ready beat the baseline on GeoQuery. We speculate that Codex performs well here because it uses the same argmax convention as the GeoQuery dataset, which is different than the convention used in Spider. With up to 40 examples in the prompt, davinci-codex outperforms a T5-3B model finetuned on these same examples by a large margin, whereas GPT-3 davinci performs quite poorly on this task. On the other hand, the T5 model outperforms Codex in a zero-shot setting on Scholar. In 5 and 10-shot settings, Codex shows better adaptation from these few samples and beats the T5 baseline.</p>
<h2>5 Conclusion</h2>
<p>We demonstrated that generative language models trained on code provide a strong baseline for Text-to-SQL. We also provided analysis of failure modes for these models, which we hope guides further prompt design (whether few-shot or through natural language instructions) in this setting. Finally, we showed that prompt-based few-shot learning with these models performs competitively with finetuning-based few-shot learning of smaller models. A clear direction for future work is to evaluate the benefits of finetuning with Codex models.</p>
<h2>Acknowledgements</h2>
<p>Nitarshan performed all zero-shot and finetuning experiments as well as error-analysis, and wrote most of the paper. Raymond performed all few-shot experiments and the associated writing. Dzmitry supervised, and contributed to paper editing.</p>
<p>We thank Dóra Jámbor for insightful discussions, Laurent Charlin for providing funding for Nitarshan and for providing feedback on this work, Fraser Kelton and Dave Cummings for support with the OpenAI API, and Ruiqi Zhong for assistance with Spider test suites. We also thank anonymous ARR reviewers for their feedback and criticism in the review process.</p>
<p>Nitarshan additionally thanks the city of Montréal and its cafés for providing inspirational settings in which to conduct this work.</p>
<h2>References</h2>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code.</p>
<p>Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, and Matthew Richardson. 2021. Structure-grounded pretraining for text-to-sql. Proceedings of the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies.</p>
<p>Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev. 2018. Improving text-to-SQL evaluation methodology. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 351-360, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Leo Gao. 2021. On the Sizes of OpenAI API Models.
Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer. 2017. Learning a neural semantic parser from user feedback. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 963-973, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Xi Victoria Lin, Richard Socher, and Caiming Xiong. 2020. Bridging textual and tabular data for crossdomain text-to-sql semantic parsing.</p>
<p>Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. 2021. Picard: Parsing incrementally for constrained auto-regressive decoding from language models.</p>
<p>Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. 2021. Compositional generalization and natural language variation: Can a semantic parsing approach handle both? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 922-938, Online. Association for Computational Linguistics.</p>
<p>Alane Suhr, Ming-Wei Chang, Peter Shaw, and Kenton Lee. 2020. Exploring unexplored generalization challenges for cross-database semantic parsing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 83728388, Online. Association for Computational Linguistics.</p>
<p>Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2019. Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task.</p>
<p>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the Thirteenth National Conference on Artificial Intelligence - Volume 2, pages 1050-1055.</p>
<p>Ruiqi Zhong, Tao Yu, and Dan Klein. 2020. Semantic evaluation for text-to-sql with distilled test suites.</p>
<p>Albert Ziegler. 2021. Research recitation.</p>
<h2>A API Details</h2>
<p>At time of writing, the OpenAI API was accessible at https://openai.com/api/. The example from which our API Docs prompt draws from can be found at https://beta.openai.com/examples/ default-sql-translate.</p>
<h3>A.1 Hyperparameters</h3>
<p>We sample 200 tokens from GPT-3 and Codex with temperature 0, with the following strings used as stop tokens to halt generation: "--", "\n\n", ":", "#".</p>
<h2>A. 2 Parameter Counts</h2>
<p>Parameter counts for OpenAI API models are not openly available. Gao (2021) evaluated API GPT3 models across a variety of language modelling tasks to compare to published results in Brown et al. (2020), finding that "Ada, Babbage, Curie and Davinci line up closely with 350M, 1.3B, 6.7B, and 175B respectively". We presume that the davincicodex model is the same size as the GPT-3 davinci model; cushman-codex is a new model name so we can only guess that it is of a similar (but not the same) size to GPT-3 curie. Nevertheless these remain guesses which should not be relied on.</p>
<h2>A. 3 Model Versioning</h2>
<p>The exact models served through the OpenAI API may vary over time. We verified that for each model type, only a single model version was used to generate results. These versions are ada:2020-05-03, babbage:2020-05-03, curie:2020-05-03, davinci:2020-05-03, cushman-codex:2021-08-03, davinci-codex:2021-08-03.</p>
<h2>A. 4 Finetuning</h2>
<p>In Table 4 we include preliminary results from finetuning GPT-3 models on the Spider training set. We used the full training set, and the default finetuning settings of 4 epochs, a batch size of 8, and a learning rate multiplier of 0.1 . We did not perform a hyperparameter sweep due to the significant cost this would incur.</p>
<h2>A. 5 Memorization</h2>
<p>The Spider development set is available on GitHub, and is therefore possibly in the training set of Codex. We believe that this does not manifest as memorization for our results however, for the following reasons.</p>
<p>Evaluation data on Spider's repo is formatted differently to our prompts. Most related is the dev.sql file, which contains evaluation questionquery pairs in the following format:</p>
<div class="codehilite"><pre><span></span><code>Question 1: ...
SQL: ...
...
</code></pre></div>

<p>This resembles but isn't identical to our "Question" prompt. We prompted Codex with verbatim fragments of this file and generations failed to replicate any file contents. Our "Question" prompt has very poor performance - hardly an indication of memorization from dev.sql. Furthermore, most of Codex's performance is due to including in the prompt the schemas (see Table 2), which are not present in dev.sql.</p>
<p>As well, Codex prediction style is very different to evaluation gold queries. Gold queries make use of a consistent table aliasing strategy (using T1, T2, etc.) which we never see with Codex (see Figure 3 for example comparisons).</p>
<p>Furthermore, in Table 4 we reported performance for all models on spider-realistic (Deng et al., 2021), a modification of the spider evaluation set that removes column name references in questions. We observe a similar trend in performance across models as on spider (the consistent performance drop on spider-realistic is expected due to the difficulty of the updated dataset). Memorization cannot account for the performance observed, as spider-realistic is not publicly available on GitHub.</p>
<p>Finally, Ziegler (2021) studied memorization in Copilot, a derivative of the Codex models, and found that "Copilot can quote a body of code verbatim, but that it rarely does so, and when it does, it mostly quotes code that everybody quotes, and mostly at the beginning of a file". Spider evaluation data is rare on GitHub, and we use long contexts in our prompts that significantly differ from the files on GitHub.</p>
<h2>A. 6 Choice of Spider Evaluation Set</h2>
<p>We chose not to evaluate on the held-out test set of Spider, as this could not be done offline - it would instead require sending these held-out examples through the API to OpenAI, which risks inadvertently leaking them for retraining of Codex.</p>
<h1>B Additional Tables and Figures</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Engine</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">VA</th>
<th style="text-align: center;">EX</th>
<th style="text-align: center;">TS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ada</td>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">$1.2(1.0)$</td>
<td style="text-align: center;">$0.0(0.0)$</td>
<td style="text-align: center;">$0.0(0.0)$</td>
</tr>
<tr>
<td style="text-align: center;">ada</td>
<td style="text-align: center;">Docs</td>
<td style="text-align: center;">$3.4(2.2)$</td>
<td style="text-align: center;">$0.2(0.2)$</td>
<td style="text-align: center;">$0.1(0.0)$</td>
</tr>
<tr>
<td style="text-align: center;">ada</td>
<td style="text-align: center;">1 Row</td>
<td style="text-align: center;">40.1 (34.6)</td>
<td style="text-align: center;">$1.1(0.6)$</td>
<td style="text-align: center;">$0.2(0.0)$</td>
</tr>
<tr>
<td style="text-align: center;">ada</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">33.8 (33.9)</td>
<td style="text-align: center;">2.3 (3.5)</td>
<td style="text-align: center;">$0.3(0.0)$</td>
</tr>
<tr>
<td style="text-align: center;">babbage</td>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">$4.4(2.0)$</td>
<td style="text-align: center;">$1.0(0.2)$</td>
<td style="text-align: center;">$1.0(0.2)$</td>
</tr>
<tr>
<td style="text-align: center;">babbage</td>
<td style="text-align: center;">Docs</td>
<td style="text-align: center;">22.5 (20.3)</td>
<td style="text-align: center;">$1.0(0.6)$</td>
<td style="text-align: center;">$0.7(0.2)$</td>
</tr>
<tr>
<td style="text-align: center;">babbage</td>
<td style="text-align: center;">1 Row</td>
<td style="text-align: center;">56.0 (49.8)</td>
<td style="text-align: center;">$5.1(1.6)$</td>
<td style="text-align: center;">$3.9(0.0)$</td>
</tr>
<tr>
<td style="text-align: center;">babbage</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">48.8 (44.9)</td>
<td style="text-align: center;">$5.7(0.8)$</td>
<td style="text-align: center;">$3.9(0.0)$</td>
</tr>
<tr>
<td style="text-align: center;">curie</td>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">$9.0(6.7)$</td>
<td style="text-align: center;">$2.9(2.4)$</td>
<td style="text-align: center;">$2.5(1.8)$</td>
</tr>
<tr>
<td style="text-align: center;">curie</td>
<td style="text-align: center;">Docs</td>
<td style="text-align: center;">25.2 (25.0)</td>
<td style="text-align: center;">$7.4(5.5)$</td>
<td style="text-align: center;">$6.3(3.3)$</td>
</tr>
<tr>
<td style="text-align: center;">curie</td>
<td style="text-align: center;">1 Row</td>
<td style="text-align: center;">70.6 (67.3)</td>
<td style="text-align: center;">$10.8(7.3)$</td>
<td style="text-align: center;">$7.6(1.4)$</td>
</tr>
<tr>
<td style="text-align: center;">curie</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">70.9 (72.2)</td>
<td style="text-align: center;">12.6 (11.0)</td>
<td style="text-align: center;">$8.3(4.1)$</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">65.0 (65.4)</td>
<td style="text-align: center;">26.3 (23.2)</td>
<td style="text-align: center;">21.7 (14.2)</td>
</tr>
<tr>
<td style="text-align: center;">Finetuned GPT-3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ada</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">27.5 (21.3)</td>
<td style="text-align: center;">20.2 (14.0)</td>
<td style="text-align: center;">19.1 (13.0)</td>
</tr>
<tr>
<td style="text-align: center;">babbage</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">47.2 (38.0)</td>
<td style="text-align: center;">34.8 (23.6)</td>
<td style="text-align: center;">31.9 (20.9)</td>
</tr>
<tr>
<td style="text-align: center;">curie</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">66.9 (60.2)</td>
<td style="text-align: center;">51.3 (37.8)</td>
<td style="text-align: center;">46.9 (32.9)</td>
</tr>
<tr>
<td style="text-align: center;">Codex</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">cushman</td>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">11.3 (8.1)</td>
<td style="text-align: center;">8.5 (3.9)</td>
<td style="text-align: center;">8.3 (3.9)</td>
</tr>
<tr>
<td style="text-align: center;">cushman</td>
<td style="text-align: center;">Docs</td>
<td style="text-align: center;">83.8 (80.5)</td>
<td style="text-align: center;">53.2 (45.1)</td>
<td style="text-align: center;">43.5 (32.3)</td>
</tr>
<tr>
<td style="text-align: center;">cushman</td>
<td style="text-align: center;">1 Row</td>
<td style="text-align: center;">84.7 (80.9)</td>
<td style="text-align: center;">59.6 (49.2)</td>
<td style="text-align: center;">48.5 (32.5)</td>
</tr>
<tr>
<td style="text-align: center;">cushman</td>
<td style="text-align: center;">3 Rows</td>
<td style="text-align: center;">82.9 (79.1)</td>
<td style="text-align: center;">60.3 (49.2)</td>
<td style="text-align: center;">49.4 (33.7)</td>
</tr>
<tr>
<td style="text-align: center;">cushman</td>
<td style="text-align: center;">5 Rows</td>
<td style="text-align: center;">83.6 (78.3)</td>
<td style="text-align: center;">61.5 (49.6)</td>
<td style="text-align: center;">50.4 (33.9)</td>
</tr>
<tr>
<td style="text-align: center;">cushman</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">88.3 (83.1)</td>
<td style="text-align: center;">62.1 (49.6)</td>
<td style="text-align: center;">53.1 (36.2)</td>
</tr>
<tr>
<td style="text-align: center;">cushman</td>
<td style="text-align: center;">+1 Row</td>
<td style="text-align: center;">86.3 (85.0)</td>
<td style="text-align: center;">63.7 (54.9)</td>
<td style="text-align: center;">53.0 (39.6)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">14.0 (8.9)</td>
<td style="text-align: center;">8.3 (4.5)</td>
<td style="text-align: center;">8.2 (4.1)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">Docs</td>
<td style="text-align: center;">83.8 (87.4)</td>
<td style="text-align: center;">56.8 (51.8)</td>
<td style="text-align: center;">47.5 (39.0)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">1 Row</td>
<td style="text-align: center;">86.3 (83.5)</td>
<td style="text-align: center;">60.9 (54.7)</td>
<td style="text-align: center;">52.0 (41.3)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">3 Rows</td>
<td style="text-align: center;">85.8 (82.7)</td>
<td style="text-align: center;">60.3 (53.3)</td>
<td style="text-align: center;">52.2 (40.0)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">5 Rows</td>
<td style="text-align: center;">85.2 (80.9)</td>
<td style="text-align: center;">60.5 (51.4)</td>
<td style="text-align: center;">51.5 (38.4)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">10 Rows</td>
<td style="text-align: center;">86.0 (80.7)</td>
<td style="text-align: center;">60.8 (53.3)</td>
<td style="text-align: center;">51.2 (39.2)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">89.8 (87.8)</td>
<td style="text-align: center;">59.9 (52.2)</td>
<td style="text-align: center;">50.0 (38.4)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">+1 Row</td>
<td style="text-align: center;">92.5 (90.7)</td>
<td style="text-align: center;">64.8 (58.7)</td>
<td style="text-align: center;">53.7 (41.7)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">+3 Rows</td>
<td style="text-align: center;">91.6 (90.6)</td>
<td style="text-align: center;">67.0 (60.2)</td>
<td style="text-align: center;">55.1 (42.9)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">+5 Rows</td>
<td style="text-align: center;">91.0 (90.2)</td>
<td style="text-align: center;">65.3 (60.0)</td>
<td style="text-align: center;">53.9 (42.1)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">+10 Rows</td>
<td style="text-align: center;">91.2 (90.6)</td>
<td style="text-align: center;">63.3 (58.3)</td>
<td style="text-align: center;">52.4 (40.2)</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance on Spider across all evaluated models and prompts, as measured by percentage of predictions which are valid/executable SQL (VA), execution accuracy (EX), test-suite accuracy (TS). Main results are on the Spider development set, results in parantheses are on Spider-Realistic (Deng et al., 2021), a modified subset of the Spider development set with explicit references to column names removed from questions.</p>
<p>14\% Shortcuts
Q What are the template ids with template type description "Presentation".
Gold SELECT T2.template_id FROM Ref_template_types AS T1 JOIN Templates AS T2 ON T1.template_type_code = T2.template_type_code WHERE T1.template_type_description = "Presentation"
Pred SELECT Template_ID FROM Templates WHERE Template_Type_Code = 'PPT';
8\% SELECT Extra Columns
NL Which city has the most frequent destination airport?
Gold SELECT T1.City FROM AIRFORTS AS T1 JOIN FLIGHTS AS T2 ON T1.AirportCode = T2.DestAirport GROUP BY T1.City ORDER BY count $(<em>)$ DESC LIMIT 1
Pred SELECT City, COUNT (</em>) AS 'Number of Destinations' FROM airports JOIN flights ON airports.AirportCode = flights.DestAirport GROUP BY City ORDER BY COUNT (<em>) DESC LIMIT 1;
$5 \%$ SELECT Convention
NL Which airlines have a flight with source airport AHD?
Gold SELECT T1.Airline FROM AIRLINES AS T1 JOIN FLIGHTS AS T2 ON T1.uid = T2.Airline WHERE T2.SourceAirport = "AHD"
Pred SELECT Airline FROM flights WHERE SourceAirport = 'AHD';
4\% GROUP BY Convention
NL Which makers designed more than 3 car models? List full name and the id.
Gold SELECT T1.FullName, T1.Id FROM CAR_MAKERS AS T1 JOIN MODEL_LIST AS T2 ON T1.Id = T2.Maker GROUP BY T1.Id HAVING count (</em>) &gt; 3;
Pred SELECT car_makers.FullName, car_makers.Id FROM car_makers JOIN model_list ON car_makers.Id = model_list.Maker GROUP BY car_makers.FullName HAVING COUNT (model_list.Model) &gt; 3;
$4 \%$ Argmax
NL Give the name, year of independence, and surface area of the country that has the lowest population.
Gold SELECT Name, SurfaceArea, IndepYear FROM country ORDER BY Population LIMIT 1
Pred SELECT Name, IndepYear, SurfaceArea FROM country WHERE Population = (SELECT MIN (Population) FROM country);
$51 \%$ Other - Semantic Incorrect
NL Which countries have either English or Dutch as an official language?
Gold SELECT * FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = "English" AND IsOfficial = "T" UNION SELECT * FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = "Dutch" AND IsOfficial = "T"
Pred SELECT * FROM countrylanguage WHERE IsOfficial = 'T' OR IsOfficial = 'F';
NL Show the name and the release year of the song by the youngest singer.
Gold SELECT song_name, song_release_year FROM singer ORDER BY age LIMIT 1
Pred SELECT Name, Song_release_year FROM singer WHERE Age = (SELECT MIN (Age) FROM singer);
NL What is the maximum miles per gallon of the car with 8 cylinders or produced before 1980?
Gold select max (mpg) from cars_data where cylinders $=8$ or year $&lt;1980$
Pred SSELECT MAX (MPG) FROM cars_data WHERE Cylinders $=8$ AND Year $&lt;1980$;
$14 \%$ Other - Ambiguous Correct
NL Find the first name and age of students who have a pet.
Gold SELECT DISTINCT T1.fname, T1.age FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid $=$ T2.stuid
Pred SELECT Fname, Age FROM Student WHERE StuID IN (SELECT StuID FROM Has_Pet);
NL How many countries speak both English and Dutch?
Gold SELECT COUNT (*) FROM (SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = "English" INTERSECT SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = "Dutch")</p>
<p>Pred SELECT COUNT (*) FROM countrylanguage WHERE Language = 'Dutch' AND CountryCode IN (SELECT CountryCode FROM countrylanguage WHERE Language = 'English');
NL What are the Asian countries which have a population larger than that of any country in Africa?
Gold SELECT Name FROM country WHERE Continent = "Asia" AND population &gt; (SELECT min (population) FROM country WHERE Continent = "Africa");
Pred SELECT Name FROM country WHERE Continent = 'Asia' AND Population &gt; (SELECT MAX (Population) FROM country WHERE Continent = 'Africa');</p>
<p>Figure 3: Additional examples of error types, as made by davinci-codex model with Create Table + Select 3 prompt. NL stands for natural language question. Percentage indicates the percentage of errors which are of the given type.</p>
<p>10-shot examples
what states does the missouri river run through SELECT
RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE
RIVERalias0.RIVER_NAME = "missouri" ; - what is the size of texas SELECT
STATEalias0.AREA FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME
= "texas" ; - what are the major cities in texas SELECT
CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION</p>
<blockquote>
<p>150000 AND CITYalias0.STATE_NAME = "texas" ; - what is the capital
of pennsylvania SELECT STATEalias0.CAPITAL FROM STATE AS STATEalias0
WHERE STATEalias0.STATE_NAME = "pennsylvania" ; - what is the biggest
city in nebraska SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0
WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION
) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = "nebraska"
) AND CITYalias0.STATE_NAME = "nebraska" ; - what is the population
of austin SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0
WHERE CITYalias0.CITY_NAME = "austin" ; - which state is kalamazoo
in SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE
CITYalias0.CITY_NAME = "kalamazoo" ; - name all the rivers in
colorado SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE
RIVERalias0.TRAVERSE = "colorado" ; - what states border missouri SELECT
BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE
BORDER_INFOalias0.STATE_NAME = "missouri" ; - how many people live in
new mexico SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE
STATEalias0.STATE_NAME = "new mexico" ;
Very similar query in the few-shot prompt fixes the example
Question
which states border iowa
Gold SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = "iowa" ;
0-shot pred SELECT state_name FROM border WHERE border = 'iowa'
10-shot pred SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = "iowa"
Argmax convetion fixed
Question
what state has the smallest population
Gold SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE
STATEalias0.POPULATION = (SELECT MIN(STATEalias1.POPULATION) FROM STATE
AS STATEalias1) ;
0-shot pred SELECT state_name FROM state ORDER BY population LIMIT 1
10-shot pred SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE
STATEalias0.POPULATION = (SELECT MIN(STATEalias1.POPULATION) FROM STATE
AS STATEalias1)
SELECT extra columns fixed
Question
what is the population of the state with the largest area
Gold SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE
STATEalias0.AREA = (SELECT MAX(STATEalias1.AREA) FROM STATE AS
STATEalias1) ;
0-shot pred SELECT state_name, population FROM state WHERE area = (SELECT MAX(area)
10-shot pred SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE
STATEalias0.AREA = (SELECT MAX(STATEalias1.AREA) FROM STATE AS
STATEalias1)</p>
</blockquote>
<p>Figure 4: Cherry-picked examples of Codex improvements from 0-shot to 10-shot text-to-SQL on GeoQuery validation set. The style of the generated SQL changes a lot and is much closer to that of the gold SQL when few-shot examples are in the prompt. The few-shot examples were also useful to adapt the generated SQL to the conventions of the dataset, like the way argmax is done, or the selected columns.</p>
<h1>C Example Prompts</h1>
<div class="codehilite"><pre><span></span><code>What is Kyle’s id? | network_l | highschooler : id, name ( Kyle ), grade | friend :
    student_id, friend_id | likes : student_id, liked_id
</code></pre></div>

<p>Figure 5: Example input for baseline T5 models.</p>
<div class="codehilite"><pre><span></span><code>-- Using valid SQLite, answer the following questions.
-- What is Kyle’s id?
SELECT
</code></pre></div>

<p>Figure 6: Example prompt for Question.</p>
<div class="codehilite"><pre><span></span><code><span class="gu">##</span># SQLite SQL tables, with their properties:
<span class="gh">#</span>
# Highschooler(ID, name, grade)
<span class="gh">#</span> Friend(student_id, friend_id)
<span class="gh">#</span> Likes(student_id, liked_id)
<span class="gh">#</span>
### What is Kyle’s id?
SELECT
</code></pre></div>

<p>Figure 7: Example prompt for API Docs.</p>
<div class="codehilite"><pre><span></span><code><span class="o">/*</span>
<span class="mi">3</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="n">rows</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">table</span><span class="w"> </span><span class="n">Highschooler</span><span class="p">:</span>
<span class="n">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">Highschooler</span><span class="w"> </span><span class="n">LIMIT</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span>
<span class="n">Table</span><span class="p">:</span><span class="w"> </span><span class="n">Highschooler</span>
<span class="w">    </span><span class="n">ID</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">grade</span>
<span class="mi">1510</span><span class="w"> </span><span class="n">Jordan</span><span class="w"> </span><span class="mi">9</span>
<span class="mi">1689</span><span class="w"> </span><span class="n">Gabriel</span><span class="w"> </span><span class="mi">9</span>
<span class="mi">1381</span><span class="w"> </span><span class="n">Tiffany</span><span class="w"> </span><span class="mi">9</span>
<span class="o">*/</span>
<span class="o">/*</span>
<span class="mi">3</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="n">rows</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">table</span><span class="w"> </span><span class="n">Friend</span><span class="p">:</span>
<span class="n">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">Friend</span><span class="w"> </span><span class="n">LIMIT</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span>
<span class="n">Table</span><span class="p">:</span><span class="w"> </span><span class="n">Friend</span>
<span class="w">    </span><span class="n">student_id</span><span class="w"> </span><span class="n">friend_id</span>
<span class="w">        </span><span class="mi">1510</span><span class="w"> </span><span class="mi">1381</span>
<span class="w">        </span><span class="mi">1510</span><span class="w"> </span><span class="mi">1689</span>
<span class="w">        </span><span class="mi">1689</span><span class="w"> </span><span class="mi">1709</span>
<span class="o">*/</span>
<span class="o">/*</span>
<span class="mi">3</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="n">rows</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">table</span><span class="w"> </span><span class="n">Likes</span><span class="p">:</span>
<span class="n">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">Likes</span><span class="w"> </span><span class="n">LIMIT</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span>
<span class="n">Table</span><span class="p">:</span><span class="w"> </span><span class="n">Likes</span>
<span class="w">    </span><span class="n">student_id</span><span class="w"> </span><span class="n">liked_id</span>
<span class="w">        </span><span class="mi">1689</span><span class="w"> </span><span class="mi">1709</span>
<span class="w">        </span><span class="mi">1709</span><span class="w"> </span><span class="mi">1689</span>
<span class="w">        </span><span class="mi">1782</span><span class="w"> </span><span class="mi">1709</span>
<span class="o">*/</span>
<span class="o">--</span><span class="w"> </span><span class="n">Using</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">SQLite</span><span class="p">,</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">questions</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">tables</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">above</span><span class="p">.</span>
<span class="o">--</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">Kyle</span>’<span class="n">s</span><span class="w"> </span><span class="n">id</span>?
<span class="n">SELECT</span>
</code></pre></div>

<p>Figure 8: Example prompt for Select 3.</p>
<div class="codehilite"><pre><span></span><code>CREATE TABLE Highschooler(
    ID int primary key,
    name text,
    grade int)
CREATE TABLE Friend(
    student_id int,
    friend_id int,
    primary key (student_id,friend_id),
    foreign key(student_id) references Highschooler(ID),
    foreign key (friend_id) references Highschooler(ID)
)
CREATE TABLE Likes(
    student_id int,
    liked_id int,
    primary key (student_id, liked_id),
    foreign key (liked_id) references Highschooler(ID),
    foreign key (student_id) references Highschooler(ID)
)
</code></pre></div>

<p>-- Using valid SQLite, answer the following questions for the tables provided above.
-- What is Kyle's id?
SELECT</p>
<p>Figure 9: Example prompt for Create Table.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">CREATE TABLE Highschooler(</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ID int primary key,</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">name text,</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">grade int)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">/*</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3 example rows:</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SELECT * FROM Highschooler LIMIT 3;</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ID name grade</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1510 Jordan 9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1689 Gabriel 9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1381 Tiffany 9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">*</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>CREATE TABLE Friend(
student_id int,
friend_id int,
primary key (student_id,friend_id),
foreign key(student_id) references Highschooler(ID),
foreign key (friend_id) references Highschooler(ID)
)
/<em>
3 example rows:
SELECT * FROM Friend LIMIT 3;
student_id friend_id
15101381
15101689
16891709
</em>f</p>
<p>CREATE TABLE Likes(
student_id int,
liked_id int,
primary key (student_id, liked_id),
foreign key (liked_id) references Highschooler(ID),
foreign key (student_id) references Highschooler(ID)
)
/<em>
3 example rows:
SELECT * FROM Likes LIMIT 3;
student_id liked_id
16891709
17091689
17821709
</em>f
-- Using valid SQLite, answer the following questions for the tables provided above.
-- What is Kyle's id?
SELECT</p>
<p>Figure 10: Example prompt for Create Table + Select 3.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 11: Example prompt for 5-shot. It starts with the schema and 3 rows per database (exactly as in Figure 10), followed by 5 few-shot examples, and finally the target question.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{*}$ Work partially done at Mila and the Université de Montréal.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ See Appendix A. 2 for a discussion on parameter counts.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
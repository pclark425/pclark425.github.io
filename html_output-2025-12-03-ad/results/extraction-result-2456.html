<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2456 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2456</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2456</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-270286154</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.03616v3.pdf" target="_blank">BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search</a></p>
                <p><strong>Paper Abstract:</strong> Novelty search (NS) refers to a class of exploration algorithms that seek to uncover diverse system behaviors through simulations or experiments. Such diversity is central to many AI-driven discovery and design tasks, including material and drug development, neural architecture search, and re-inforcement learning. However, existing NS methods typically rely on evolutionary strategies and other meta-heuristics that require dense sampling of the input space, making them impractical for expensive black-box systems. In this work, we introduce BEACON, a sample-efficient, Bayesian optimization-inspired approach to NS that is tailored for settings where the input-to-behavior relationship is opaque and costly to evaluate. BEACON models this mapping us-ing multi-output Gaussian processes (MOGPs) and selects new inputs by maximizing a novelty metric computed from posterior samples of the MOGP, effectively balancing the exploration-exploitation trade-off. By leveraging recent advances in posterior sampling and high-dimensional GP modeling, our method remains scalable to large input spaces and datasets. We evaluate BEACON across ten synthetic benchmarks and eight real-world tasks, including the design of diverse materials for clean energy applications. Our results show that BEACON significantly outperforms existing NS baselines, consistently discovering a broader set of behaviors under tight evaluation budgets.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2456.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2456.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BEACON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Exploration Algorithm for outCOme Novelty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sample-efficient novelty search algorithm that models multi-output expensive black-box systems with multi-output Gaussian processes and selects queries by maximizing a Thompson-sampled novelty acquisition function over outcome space, explicitly designed to discover diverse behaviors under tight evaluation budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BEACON</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BEACON constructs a multi-output Gaussian process (MOGP) surrogate over the mapping f: X -> O, uses decoupled/differentiable posterior function sampling (Thompson sampling) to draw a plausible function g ~ P(f|D), computes a novelty acquisition α_NS(x|g,D) = (1/k) sum_{i=1}^k dist(g(x), μ_D(x*_i)) where x*_i are k nearest past outcomes (measured against the posterior means), and selects the next input x maximizing α_NS via gradient-based optimization (with deduplication to behavior bins). The algorithm repeats sequentially for a fixed query budget, with scalable extensions: SAAS priors for high-dimensional input relevance, domain-specific GP kernels (GAUCHE) for structured chemical inputs, and sparse GP approximations (SVGP) for large budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General black-box scientific discovery and design tasks (materials discovery e.g., MOFs, drug discovery, reinforcement learning policy search, synthetic optimization benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Sequential, budget-limited selection of inputs: fit MOGP to observed data, sample posterior function (Thompson sample), evaluate novelty of candidate inputs in outcome space relative to deduplicated observed behavior bins, and select the x that maximizes the novelty acquisition. Supports exhaustive evaluation for discrete candidate sets and gradient-based multi-start optimization for continuous X.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Reported/considered: CPU time per acquisition optimization (seconds), GP training cost dominated by O(N^3) for exact GP; practical measurements include average runtime per iteration (<1s on synthetic problems for BEACON with exact GP) and acquisition optimization CPU times (plotted vs N; comparisons between exact GP and SVGP).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not formulated as classical information gain; uses posterior sampling (Thompson sampling) to account for uncertainty and the novelty acquisition (distance to nearest posterior-mean outcomes) as the utility. No explicit mutual information or expected information criterion is optimized.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration-exploitation balance is handled implicitly via Thompson sampling: posterior samples g embody uncertainty-driven exploration while the novelty acquisition encourages sampling inputs predicted to map to outcome regions far from observed behaviors—thus exploration is prioritized over exploitation of a scalar objective. The method favors exploration of unvisited outcome bins rather than local improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit novelty metric over outcome space: average distance to k nearest observed outcomes in outcome space (k-NN novelty). Uses behavior space discretization (ϵ-covering / behavior bins) and deduplication to focus acquisition on previously unseen behavior bins. UG-BEACON variant can incorporate user-defined partitions to avoid revisiting certain behavior regions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of sequential expensive evaluations (query budget T).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>BEACON is explicitly designed for small T; selection policy maximizes novelty per evaluation to maximize coverage (minimize cumulative behavior gap BG_t). For larger budgets, scalable GP approximations (SVGP, sparse methods) and deduplication to |B| behavior bins reduce per-iteration cost.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Novelty score (α_NS) and reachability/behavior-gap metrics: Reach_t = 1 - BG_t (fraction of behavior bins discovered). Breakthroughs are effectively high-novelty outcomes that map to previously unvisited behavior bins; success in RL measured by final reward=1 in maze task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primary: Reach_t (fraction of behavior bins discovered) and cumulative behavior gap BG_t. Reported empirical numbers: BEACON attains near-perfect reachability (Reach≈1) on many synthetic tasks; in a Multi-Output Plus benchmark BEACON uncovers ≈80% of behaviors vs ≤40% for baselines; in MOF tasks BEACON achieved full reachability; in RL maze task all 20 replicates reached final reward=1. Computational metrics: per-iteration runtimes (seconds), acquisition optimization CPU time (plots vs N), and SVGP scaling showing that SVGP for 5000 points is faster than exact GP for 1000 points in authors' tests.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to NS-EA (evolutionary novelty search), NS-DEA (distance-enhanced evolutionary), NS-FS (feature-space input novelty), MaxVar (maximum posterior variance active learning), Sobol (quasi-random sampling), RS (random sampling), and EI (expected improvement) in some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BEACON consistently outperforms all baselines across synthetic and real-world tasks: e.g., nearly 80% reach vs ≤40% for others on the Multi-Output Plus; full reachability on MOF tasks where evolutionary NS methods were not applicable; in maze RL, BEACON was the only method to consistently succeed (20/20 replicates achieved final reward=1) while EI performed poorly. With SVGP on 20d Rosenbrock (1000 evals) BEACON found ≈3× more diverse behaviors than NS-EA.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Sample-efficiency gains are emphasized: BEACON discovers broader behavior sets under tight evaluation budgets; quantified examples include ~3× more diverse behaviors than NS-EA on a 20D Rosenbrock problem with 1000 evaluations, and rapid attainment of full reachability in MOF tasks (exact percentage varies by task). Computational trade-offs: BEACON incurs higher compute than simple baselines but remains under ~1s per iteration in synthetic settings; using SVGP reduces CPU time substantially for large N.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses tradeoffs qualitatively and empirically: BEACON trades higher computational cost (GP fitting and acquisition optimization) for substantially fewer expensive evaluations (better sample efficiency). Use of SVGP and deduplication addresses computational cost at the price of approximate posterior fidelity; SAAS priors and domain kernels improve modeling in high-dimensional regimes. The ablation on noise filtering shows modeling uncertainty (filtering noise via the GP) materially improves discovery under observational noise. The paper notes no formal theoretical cost→gain guarantees but provides empirical evidence of favorable tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key practical recommendations: prioritize posterior-sampled novelty acquisitions (Thompson sampling + k-NN novelty) to maximize behavior coverage under strict query budgets; use MOGP surrogates for uncertainty-aware selection and to filter noise; apply SAAS priors or domain-specific kernels for high-dimensional/structured inputs; employ sparse GP approximations (SVGP) and deduplication to reduce computational costs when N grows. UG-BEACON shows that incorporating user constraints/partitions can accelerate discovery in prioritized subregions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2456.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2456.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UG-BEACON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>User-Guided BEACON</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constrained extension of BEACON that incorporates user-defined behavior-level partitions/constraints into the acquisition optimization to avoid revisiting already-explored or user-irrelevant outcome regions, focusing limited evaluations toward user-preferred behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>UG-BEACON (User-Guided BEACON)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>UG-BEACON augments BEACON by introducing a feasible set X_feas = {x ∈ X : g(x) ∉ G_visited} where G is a user-specified partition of outcome space and G_visited are regions previously observed or marked by the user. The acquisition optimization maximizes α_NS(x|g,D) subject to x ∈ X_feas, thereby excluding candidate inputs predicted to fall into user-forbidden or already-sampled behavior bins. All other components (MOGP surrogate, posterior sampling, acquisition evaluation) remain as in BEACON.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Discovery tasks where users can specify preferences/constraints on desired behaviors (e.g., MNIST digit discovery via latent sampling, material discovery where users prioritize certain outcome regions).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Sequential selection under the same BEACON loop but with a constraint filter that removes candidate inputs whose predicted outcomes lie in user-specified or previously-visited behavior partitions (G_visited), thereby reallocating limited evaluations away from redundant or undesired behaviors toward unexplored or user-important regions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Same as BEACON: per-iteration CPU time for GP fitting and constrained acquisition optimization; additional cost for applying feasibility filter (negligible compared to GP operations).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Same as BEACON: no explicit information-theoretic gain; acquisition remains novelty-based with constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration via Thompson-sampled novelty acquisition but constrained by user-defined feasibility; thus exploitation of user preferred regions is enforced by constraints rather than by reward maximization.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Preserves BEACON's novelty-driven diversity mechanism but can bias diversity toward user-prioritized subdivisions by excluding already-visited or irrelevant regions, enabling targeted diversity within prioritized outcome subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of evaluations; user-specified partitions reallocate budget implicitly by forbidding revisits.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handles budget by preventing redundant sampling in already-visited/out-of-interest behavior bins, thus concentrating scarce evaluations on novel/user-relevant outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Same novelty and reachability metrics; study showed UG-BEACON achieved faster discovery of all digit classes on MNIST latent sampling and attained 100% coverage in the user-prioritized oil sorbent outcome partition within 50 iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reachability over user-partitioned behavior space; empirical example: UG-BEACON reached full (100%) coverage of the prioritized outcome space in oil-sorbent case within ~50 iterations, and faster discovery rates for MNIST digit classes compared to BEACON and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to BEACON and other NS baselines in case studies (MNIST, oil sorbent materials).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>UG-BEACON converged faster to full coverage in the user-prioritized oil sorbent task than BEACON and baselines; on MNIST, it discovered digit classes faster than unconstrained BEACON.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Empirically significant acceleration in user-relevant discovery (e.g., full coverage in ~50 iterations in oil-sorbent case); exact percentage savings depend on task/partitioning choices.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>UG-BEACON demonstrates that imposing user constraints shifts exploration to prioritized regions, improving efficiency for targeted discovery but potentially reducing global coverage if user partitions exclude interesting regions. The paper reports empirical gains but does not provide formal tradeoff bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>When users have clear preferences, encoding them as outcome-space partitions and constraining novelty search yields faster discovery within those regions; recommended when practical utility is prioritized over exhaustive coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2456.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2456.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MaxVar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum Posterior Variance Active Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active learning baseline that selects query points which maximize the posterior predictive variance (trace of predictive covariance) under the GP surrogate, aimed at reducing model uncertainty as a route to discovering new behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MaxVar (posterior-variance acquisition)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MaxVar uses the same MOGP surrogate as BEACON but replaces the novelty acquisition with α_MaxVar(x|D) = tr(Σ_D(x)) where Σ_D(x) is the posterior covariance of f(x). It selects x maximizing predictive uncertainty (sum of output variances), promoting exploration of uncertain regions in input space.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active learning and exploration for black-box scientific models; used here as a baseline across synthetic and real-world discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Sequential selection of points with highest posterior predictive variance to reduce surrogate uncertainty under a fixed budget.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Same GP inference/training and acquisition optimization costs as BEACON; cost dominated by posterior covariance evaluations and GP updates.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uses predictive variance as a proxy for information; does not compute expected information gain or mutual information explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploration biased towards regions where the surrogate is most uncertain; no exploitation of high-performing or high-novelty regions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via uncertainty-driven exploration; no explicit diversity metric over outcome bins.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Allocates budget to locations of maximal model uncertainty; does not directly account for behavior coverage or redundancy in outcome space.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Indirect — breakthroughs are hoped to arise by reducing model uncertainty; measured via reachability/BG_t in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reachability vs iterations; CPU/runtime per iteration; empirical comparisons show MaxVar is outperformed by BEACON in reachability despite similar computational costs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to BEACON, NS variants, Sobol, RS, EI in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BEACON outperforms MaxVar consistently; authors note BEACON requires less cost compared to MaxVar in practice because both incur GP training and acquisition optimization, but BEACON's acquisition is more effective at discovering novel behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>No specific quantified gain vs random baselines provided in general; empirical results show BEACON surpasses MaxVar in behavior coverage under the same budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Authors comment that learning a globally accurate surrogate (MaxVar's aim) is sufficient but not necessary for novelty discovery; BEACON's targeted novelty acquisition is more efficient for behavior discovery under a constrained budget.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Allocating by predictive variance can identify uncertain regions but is less effective at maximizing behavior coverage than novelty-based posterior-sampled acquisitions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2456.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2456.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SVGP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Variational Gaussian Process (approximation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalable sparse GP approximation using inducing points and variational inference to reduce cubic scaling of exact GP inference, enabling BEACON to operate effectively at larger evaluation budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SVGP (sparse variational Gaussian process)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SVGP approximates the full GP posterior with a variational distribution parameterized by M inducing points (M << N), reducing the cost of fitting and posterior inference. In BEACON, SVGPs are used to lower the computational cost of GP fits and acquisition optimization when N grows into the thousands, with empirical experiments using 100 inducing points and demonstrating favorable runtime and reachability tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scalable surrogate modeling within Bayesian active exploration/novelty search for high-budget problems (e.g., simulation-rich design spaces).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Not an allocation policy per se, but reduces computational cost per allocation step so more iterations or larger candidate pools are practical.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Empirical CPU time for acquisition optimization; theoretical cost reduced from O(N^3) for exact GP to O(NM^2) / O(M^3) dependent on implementation and M inducing points. Authors report SVGP CPU times that scale much better, noting e.g., CPU time for 5000 points with SVGP < CPU time for 1000 points with exact GP in their experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>SVGP itself does not prescribe exploration-exploitation; it is used inside BEACON (or MaxVar) to provide approximate posterior means/variances for acquisition computations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Enables larger N budgets by reducing per-iteration compute.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Reduces computational resource requirements so BEACON-style novelty allocation can be applied at larger budgets; empirical demonstration on 20D Rosenbrock with 1000 evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Indirect: helps BEACON maintain high reachability at larger budgets by making acquisition optimization tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Acquisition optimization CPU time vs number of training points; reachability performance using SVGP vs exact GP on 20D Rosenbrock showing SVGP enables BEACON to find far more diverse behaviors than baselines at large budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to exact GP within BEACON and other NS baselines; SVGP leads to much lower CPU time for large N with similar or slightly degraded reachability in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>SVGP allows BEACON to scale and retain superior reachability; specific empirical claim: with SVGP BEACON found nearly 3× more diverse behaviors than NS-EA on a 20D Rosenbrock with 1000 evaluations, and CPU-time for SVGP scaled much better than exact GP.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Large reductions in CPU time for acquisition optimization at large N (e.g., per authors' experiments, SVGP with 5000 points is faster than exact GP with 1000 points); enables practical operation under large evaluation budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>SVGP trades some posterior fidelity for large computational gains; authors empirically show this tradeoff is favorable for novelty discovery at large budgets, although exact-GP accuracy may be preferable for small N.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>For large budgets, combine novelty acquisition with sparse GP approximations (SVGP) to maintain tractable acquisition optimization while preserving behavior discovery performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2456.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2456.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAAS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Axis-Aligned Subspace prior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical Bayesian prior over GP lengthscales that induces sparsity in input relevance, favoring explanations depending on a small subset of input dimensions — used to scale BEACON to very high-dimensional input spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>High-dimensional Bayesian optimization with sparse axis-aligned subspaces.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SAAS prior (sparse axis-aligned subspace)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SAAS places hierarchical priors on per-dimension GP lengthscales to favor a small active subspace of input dimensions; integrated into BEACON's GP surrogate to enable effective modeling and posterior sampling in problems with thousands of input dimensions (e.g., 2,133-d fragprint molecular representation or 2,000+ features).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>High-dimensional black-box optimization and novelty search, notably in molecular discovery/chemistry where descriptor spaces can be extremely high-dimensional.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>By discovering a sparse relevant subspace, SAAS reduces the effective dimensionality for surrogate learning and acquisition optimization, thereby focusing allocation on impactful input directions and improving sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Costs measured in GP hyperparameter inference time and acquisition optimization time; SAAS is fully Bayesian and can increase hyperparameter inference cost but yields downstream savings in acquisition optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Improves surrogate fidelity in the relevant subspace which indirectly affects exploration/exploitation decisions made by BEACON's acquisition (posterior sampling + novelty) by focusing modeling capacity on key inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed query budget; SAAS helps within that budget by improving model efficiency in high-D.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Enables more effective allocation under limited budgets by reducing the 'curse of dimensionality' in surrogate modeling, thus improving the expected utility of each selected evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Indirect: better modeling of relevant inputs increases probability of sampling truly novel/outlying behaviors in outcome space.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used in ESOL (2133-d fragprint) and LogD (125-d) tasks where BEACON with SAAS achieved superior reachability compared to baselines; specific numerical values are task-dependent and reported in paper graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared indirectly via BEACON-with-SAAS vs other BEACON variants and baseline algorithms on high-dimensional molecular problems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>In high-dimensional molecular tasks, chemically-informed priors/kernels including SAAS improved BEACON performance versus baselines; exact numeric improvements are presented in the paper figures (e.g., improved reachability in ESOL and LogD experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Enables BEACON to operate effectively in thousands of input dimensions by reducing needed samples to learn relevant input effects; exact percentage gains vary by dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>SAAS increases hyperparameter inference complexity but yields large benefits in downstream acquisition efficiency; recommended when few input dimensions are expected to control outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>When input relevance is sparse, using SAAS priors yields better surrogate models and improved allocation decisions for novelty discovery under limited budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2456.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2456.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Thompson-sampled Novelty Acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thompson Sampling-Based Novelty Acquisition (α_NS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An acquisition strategy that draws posterior function samples (Thompson sampling) from an MOGP and scores candidate inputs by outcome-space novelty computed relative to posterior-mean predictions of past observations, enabling uncertainty-aware selection of novel behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Thompson-sampled novelty acquisition (α_NS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each iteration the surrogate posterior is sampled to produce g(x). The novelty score α_NS(x|g,D) = (1/k) Σ_i dist(g(x), μ_D(x*_i)), where x*_i are the k closest observed outcomes to g(x) measured against μ_D at observed inputs. The acquisition uses differentiable posterior sampling (decoupled sampling) and a contiguous relaxation of argsort/sort for gradient-based optimization. This approach handles observation noise (by relying on posterior means) and model uncertainty (through posterior samples), prioritizing exploration of unobserved outcome regions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General active exploration for discovery tasks with expensive/noisy evaluations (materials, molecules, RL policy parameters, synthetic benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates each expensive evaluation to the input maximizing α_NS under a sampled posterior, explicitly prioritizing inputs predicted to produce novel outcomes relative to observed behavior bins and posterior mean predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost drivers: generating differentiable function-space posterior samples (Wilson et al. 2020 method), computing k-NN distances against deduplicated behavior bins (cost O(|B|) rather than O(N)), and gradient-based acquisition optimization (multi-start L-BFGS-B or Adam iterations). Measured in CPU time per acquisition optimization; GP training cost separate.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>No explicit information-theoretic measure; uncertainty is incorporated via posterior sampling, but the acquisition maximizes a novelty utility rather than expected information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Uncertainty -> exploration: Thompson samples cause stochastic exploration across plausible functions; novelty utility pushes selection to outcome-space regions far from prior observations, producing exploration-focused behavior; exploitation of a scalar performance objective is not used.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit diversity objective via k-NN outcome-space distance and behavior bin deduplication; supports user-guided partitions to bias diversity to particular outcome regions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budget; method designed to maximize behavioral coverage within that budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Directly optimizes per-evaluation novelty to reduce cumulative behavior gap given T; deduplication reduces redundant evaluations and computational cost per acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>High α_NS scores correspond to candidate inputs expected to produce previously unseen behavior bins (i.e., breakthroughs in behavior coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirical reachability/BG_t metrics and successful discovery rates in case studies; shown to outperform MaxVar and evolutionary NS baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to MaxVar, NS variants (NS-EA, NS-DEA, NS-FS), Sobol, RS, and EI in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Produced substantially higher reachability than baselines across many tasks; authors report concrete examples (≈80% reach vs ≤40% on Multi-Output Plus, full reachability on MOF tasks, consistent success in RL where EI failed).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Enables discovering a broader set of behaviors using fewer expensive evaluations relative to baselines; exact gains are task-dependent and reported in experimental figures.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Combines posterior sampling (stochastic exploration) and novelty utility to trade model uncertainty handling for direct diversity gains; authors empirically show this is more cost-effective (in number of expensive evaluations) than variance-based or evolutionary novelty approaches, at the expense of higher computational overhead per selection.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Posterior-sampled novelty acquisition is an effective allocation rule for maximizing behavior coverage under tight query budgets; combining it with deduplication and sparse approximations yields a practical balance between computational cost and discovery performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2456.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2456.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NS-EA / NS-DEA / NS-FS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evolutionary Novelty Search Variants (NS-EA, NS-DEA, NS-FS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evolutionary and heuristic-based novelty search methods used as baselines: NS-EA (original evolutionary novelty search optimizing k-NN distance in outcome space), NS-DEA (distance-to-explored-area variant), and NS-FS (novelty in input/feature space).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NS-EA / NS-DEA / NS-FS (evolutionary novelty search baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NS-EA: an evolutionary algorithm that selects and evolves populations to maximize an outcome-space novelty metric (average distance to nearest stored outcomes). NS-DEA: variant that measures distance to convex hull of explored outcomes to push outside explored regions. NS-FS: variant that measures novelty in input (feature) space rather than outcome space. All are heuristic, population-based, rely on mutation/selection and maintain archives of novel solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General novelty-driven exploration tasks (robotics, evolutionary design, materials, synthetic benchmarks); used here as benchmarks for comparison with BEACON.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Population-based evolutionary sampling: allocate budget across generational populations by mutation, recombination, and selection prioritized by novelty scores; resource allocation is implicit through population/offspring sizes and evolutionary operators.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Low per-evaluation computational overhead beyond objective evaluations; sample-inefficient in terms of number of expensive evaluations required; runtimes are small per iteration but require many evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Primarily exploration via novelty-driven selection; some variants can combine novelty with fitness, but original NS-EA abandons objective exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicitly promotes diversity via archive-based novelty metrics (k-NN distance, distance to explored area, feature-space distances) and selection pressure towards novel behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of evaluations; practical effectiveness limited when evaluations are expensive due to sample inefficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Population size and offspring size hyperparameters control search effort; the methods do not explicitly optimize under tight budgets and tend to require dense sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Novelty metrics (k-NN distances, distance to convex hull) and archive additions indicate breakthroughs; however, noisy observations and expensive evaluations cause misclassification and inefficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reachability/BG_t; empirical findings show these evolutionary approaches are often beaten by BEACON under tight budgets—e.g., on Multi-Output Plus and high-dimensional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>These algorithms serve as baselines against BEACON, MaxVar, Sobol, RS, EI.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BEACON significantly outperforms NS-EA/NS-DEA/NS-FS on most tested problems, especially when evaluations are costly or noisy; evolutionary approaches found to be sample-inefficient; NS-EA performance insensitive to moderate changes in population/offspring size per ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not applicable; these are baseline methods demonstrating lower sample efficiency compared to BEACON.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Authors highlight that evolutionary novelty search methods are heuristic and sample-inefficient, making them ill-suited for expensive black-box discovery tasks; noise further degrades performance because heuristics typically assume noiseless evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>For expensive noisy settings, replacing heuristic evolutionary novelty search with uncertainty-aware surrogate-based allocation (BEACON) yields far better behavior coverage per expensive evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>High-dimensional Bayesian optimization with sparse axis-aligned subspaces. <em>(Rating: 2)</em></li>
                <li>Efficiently sampling functions from Gaussian process posteriors. <em>(Rating: 2)</em></li>
                <li>Scalable Thompson sampling using sparse Gaussian process models. <em>(Rating: 2)</em></li>
                <li>GAUCHE: A library for Gaussian processes in chemistry <em>(Rating: 2)</em></li>
                <li>Gaussian process optimization in the bandit setting: No regret and experimental design <em>(Rating: 2)</em></li>
                <li>Novelty search and the problem with objectives <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2456",
    "paper_id": "paper-270286154",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "BEACON",
            "name_full": "Bayesian Exploration Algorithm for outCOme Novelty",
            "brief_description": "A sample-efficient novelty search algorithm that models multi-output expensive black-box systems with multi-output Gaussian processes and selects queries by maximizing a Thompson-sampled novelty acquisition function over outcome space, explicitly designed to discover diverse behaviors under tight evaluation budgets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "BEACON",
            "system_description": "BEACON constructs a multi-output Gaussian process (MOGP) surrogate over the mapping f: X -&gt; O, uses decoupled/differentiable posterior function sampling (Thompson sampling) to draw a plausible function g ~ P(f|D), computes a novelty acquisition α_NS(x|g,D) = (1/k) sum_{i=1}^k dist(g(x), μ_D(x*_i)) where x*_i are k nearest past outcomes (measured against the posterior means), and selects the next input x maximizing α_NS via gradient-based optimization (with deduplication to behavior bins). The algorithm repeats sequentially for a fixed query budget, with scalable extensions: SAAS priors for high-dimensional input relevance, domain-specific GP kernels (GAUCHE) for structured chemical inputs, and sparse GP approximations (SVGP) for large budgets.",
            "application_domain": "General black-box scientific discovery and design tasks (materials discovery e.g., MOFs, drug discovery, reinforcement learning policy search, synthetic optimization benchmarks).",
            "resource_allocation_strategy": "Sequential, budget-limited selection of inputs: fit MOGP to observed data, sample posterior function (Thompson sample), evaluate novelty of candidate inputs in outcome space relative to deduplicated observed behavior bins, and select the x that maximizes the novelty acquisition. Supports exhaustive evaluation for discrete candidate sets and gradient-based multi-start optimization for continuous X.",
            "computational_cost_metric": "Reported/considered: CPU time per acquisition optimization (seconds), GP training cost dominated by O(N^3) for exact GP; practical measurements include average runtime per iteration (&lt;1s on synthetic problems for BEACON with exact GP) and acquisition optimization CPU times (plotted vs N; comparisons between exact GP and SVGP).",
            "information_gain_metric": "Not formulated as classical information gain; uses posterior sampling (Thompson sampling) to account for uncertainty and the novelty acquisition (distance to nearest posterior-mean outcomes) as the utility. No explicit mutual information or expected information criterion is optimized.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Exploration-exploitation balance is handled implicitly via Thompson sampling: posterior samples g embody uncertainty-driven exploration while the novelty acquisition encourages sampling inputs predicted to map to outcome regions far from observed behaviors—thus exploration is prioritized over exploitation of a scalar objective. The method favors exploration of unvisited outcome bins rather than local improvement.",
            "diversity_mechanism": "Explicit novelty metric over outcome space: average distance to k nearest observed outcomes in outcome space (k-NN novelty). Uses behavior space discretization (ϵ-covering / behavior bins) and deduplication to focus acquisition on previously unseen behavior bins. UG-BEACON variant can incorporate user-defined partitions to avoid revisiting certain behavior regions.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of sequential expensive evaluations (query budget T).",
            "budget_constraint_handling": "BEACON is explicitly designed for small T; selection policy maximizes novelty per evaluation to maximize coverage (minimize cumulative behavior gap BG_t). For larger budgets, scalable GP approximations (SVGP, sparse methods) and deduplication to |B| behavior bins reduce per-iteration cost.",
            "breakthrough_discovery_metric": "Novelty score (α_NS) and reachability/behavior-gap metrics: Reach_t = 1 - BG_t (fraction of behavior bins discovered). Breakthroughs are effectively high-novelty outcomes that map to previously unvisited behavior bins; success in RL measured by final reward=1 in maze task.",
            "performance_metrics": "Primary: Reach_t (fraction of behavior bins discovered) and cumulative behavior gap BG_t. Reported empirical numbers: BEACON attains near-perfect reachability (Reach≈1) on many synthetic tasks; in a Multi-Output Plus benchmark BEACON uncovers ≈80% of behaviors vs ≤40% for baselines; in MOF tasks BEACON achieved full reachability; in RL maze task all 20 replicates reached final reward=1. Computational metrics: per-iteration runtimes (seconds), acquisition optimization CPU time (plots vs N), and SVGP scaling showing that SVGP for 5000 points is faster than exact GP for 1000 points in authors' tests.",
            "comparison_baseline": "Compared to NS-EA (evolutionary novelty search), NS-DEA (distance-enhanced evolutionary), NS-FS (feature-space input novelty), MaxVar (maximum posterior variance active learning), Sobol (quasi-random sampling), RS (random sampling), and EI (expected improvement) in some tasks.",
            "performance_vs_baseline": "BEACON consistently outperforms all baselines across synthetic and real-world tasks: e.g., nearly 80% reach vs ≤40% for others on the Multi-Output Plus; full reachability on MOF tasks where evolutionary NS methods were not applicable; in maze RL, BEACON was the only method to consistently succeed (20/20 replicates achieved final reward=1) while EI performed poorly. With SVGP on 20d Rosenbrock (1000 evals) BEACON found ≈3× more diverse behaviors than NS-EA.",
            "efficiency_gain": "Sample-efficiency gains are emphasized: BEACON discovers broader behavior sets under tight evaluation budgets; quantified examples include ~3× more diverse behaviors than NS-EA on a 20D Rosenbrock problem with 1000 evaluations, and rapid attainment of full reachability in MOF tasks (exact percentage varies by task). Computational trade-offs: BEACON incurs higher compute than simple baselines but remains under ~1s per iteration in synthetic settings; using SVGP reduces CPU time substantially for large N.",
            "tradeoff_analysis": "Paper discusses tradeoffs qualitatively and empirically: BEACON trades higher computational cost (GP fitting and acquisition optimization) for substantially fewer expensive evaluations (better sample efficiency). Use of SVGP and deduplication addresses computational cost at the price of approximate posterior fidelity; SAAS priors and domain kernels improve modeling in high-dimensional regimes. The ablation on noise filtering shows modeling uncertainty (filtering noise via the GP) materially improves discovery under observational noise. The paper notes no formal theoretical cost→gain guarantees but provides empirical evidence of favorable tradeoffs.",
            "optimal_allocation_findings": "Key practical recommendations: prioritize posterior-sampled novelty acquisitions (Thompson sampling + k-NN novelty) to maximize behavior coverage under strict query budgets; use MOGP surrogates for uncertainty-aware selection and to filter noise; apply SAAS priors or domain-specific kernels for high-dimensional/structured inputs; employ sparse GP approximations (SVGP) and deduplication to reduce computational costs when N grows. UG-BEACON shows that incorporating user constraints/partitions can accelerate discovery in prioritized subregions.",
            "uuid": "e2456.0",
            "source_info": {
                "paper_title": "BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "UG-BEACON",
            "name_full": "User-Guided BEACON",
            "brief_description": "A constrained extension of BEACON that incorporates user-defined behavior-level partitions/constraints into the acquisition optimization to avoid revisiting already-explored or user-irrelevant outcome regions, focusing limited evaluations toward user-preferred behaviors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "UG-BEACON (User-Guided BEACON)",
            "system_description": "UG-BEACON augments BEACON by introducing a feasible set X_feas = {x ∈ X : g(x) ∉ G_visited} where G is a user-specified partition of outcome space and G_visited are regions previously observed or marked by the user. The acquisition optimization maximizes α_NS(x|g,D) subject to x ∈ X_feas, thereby excluding candidate inputs predicted to fall into user-forbidden or already-sampled behavior bins. All other components (MOGP surrogate, posterior sampling, acquisition evaluation) remain as in BEACON.",
            "application_domain": "Discovery tasks where users can specify preferences/constraints on desired behaviors (e.g., MNIST digit discovery via latent sampling, material discovery where users prioritize certain outcome regions).",
            "resource_allocation_strategy": "Sequential selection under the same BEACON loop but with a constraint filter that removes candidate inputs whose predicted outcomes lie in user-specified or previously-visited behavior partitions (G_visited), thereby reallocating limited evaluations away from redundant or undesired behaviors toward unexplored or user-important regions.",
            "computational_cost_metric": "Same as BEACON: per-iteration CPU time for GP fitting and constrained acquisition optimization; additional cost for applying feasibility filter (negligible compared to GP operations).",
            "information_gain_metric": "Same as BEACON: no explicit information-theoretic gain; acquisition remains novelty-based with constraints.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Exploration via Thompson-sampled novelty acquisition but constrained by user-defined feasibility; thus exploitation of user preferred regions is enforced by constraints rather than by reward maximization.",
            "diversity_mechanism": "Preserves BEACON's novelty-driven diversity mechanism but can bias diversity toward user-prioritized subdivisions by excluding already-visited or irrelevant regions, enabling targeted diversity within prioritized outcome subsets.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of evaluations; user-specified partitions reallocate budget implicitly by forbidding revisits.",
            "budget_constraint_handling": "Handles budget by preventing redundant sampling in already-visited/out-of-interest behavior bins, thus concentrating scarce evaluations on novel/user-relevant outcomes.",
            "breakthrough_discovery_metric": "Same novelty and reachability metrics; study showed UG-BEACON achieved faster discovery of all digit classes on MNIST latent sampling and attained 100% coverage in the user-prioritized oil sorbent outcome partition within 50 iterations.",
            "performance_metrics": "Reachability over user-partitioned behavior space; empirical example: UG-BEACON reached full (100%) coverage of the prioritized outcome space in oil-sorbent case within ~50 iterations, and faster discovery rates for MNIST digit classes compared to BEACON and baselines.",
            "comparison_baseline": "Compared to BEACON and other NS baselines in case studies (MNIST, oil sorbent materials).",
            "performance_vs_baseline": "UG-BEACON converged faster to full coverage in the user-prioritized oil sorbent task than BEACON and baselines; on MNIST, it discovered digit classes faster than unconstrained BEACON.",
            "efficiency_gain": "Empirically significant acceleration in user-relevant discovery (e.g., full coverage in ~50 iterations in oil-sorbent case); exact percentage savings depend on task/partitioning choices.",
            "tradeoff_analysis": "UG-BEACON demonstrates that imposing user constraints shifts exploration to prioritized regions, improving efficiency for targeted discovery but potentially reducing global coverage if user partitions exclude interesting regions. The paper reports empirical gains but does not provide formal tradeoff bounds.",
            "optimal_allocation_findings": "When users have clear preferences, encoding them as outcome-space partitions and constraining novelty search yields faster discovery within those regions; recommended when practical utility is prioritized over exhaustive coverage.",
            "uuid": "e2456.1",
            "source_info": {
                "paper_title": "BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "MaxVar",
            "name_full": "Maximum Posterior Variance Active Learning",
            "brief_description": "An active learning baseline that selects query points which maximize the posterior predictive variance (trace of predictive covariance) under the GP surrogate, aimed at reducing model uncertainty as a route to discovering new behaviors.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "MaxVar (posterior-variance acquisition)",
            "system_description": "MaxVar uses the same MOGP surrogate as BEACON but replaces the novelty acquisition with α_MaxVar(x|D) = tr(Σ_D(x)) where Σ_D(x) is the posterior covariance of f(x). It selects x maximizing predictive uncertainty (sum of output variances), promoting exploration of uncertain regions in input space.",
            "application_domain": "Active learning and exploration for black-box scientific models; used here as a baseline across synthetic and real-world discovery tasks.",
            "resource_allocation_strategy": "Sequential selection of points with highest posterior predictive variance to reduce surrogate uncertainty under a fixed budget.",
            "computational_cost_metric": "Same GP inference/training and acquisition optimization costs as BEACON; cost dominated by posterior covariance evaluations and GP updates.",
            "information_gain_metric": "Uses predictive variance as a proxy for information; does not compute expected information gain or mutual information explicitly.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Pure exploration biased towards regions where the surrogate is most uncertain; no exploitation of high-performing or high-novelty regions.",
            "diversity_mechanism": "Implicit via uncertainty-driven exploration; no explicit diversity metric over outcome bins.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed number of evaluations.",
            "budget_constraint_handling": "Allocates budget to locations of maximal model uncertainty; does not directly account for behavior coverage or redundancy in outcome space.",
            "breakthrough_discovery_metric": "Indirect — breakthroughs are hoped to arise by reducing model uncertainty; measured via reachability/BG_t in experiments.",
            "performance_metrics": "Reachability vs iterations; CPU/runtime per iteration; empirical comparisons show MaxVar is outperformed by BEACON in reachability despite similar computational costs.",
            "comparison_baseline": "Compared directly to BEACON, NS variants, Sobol, RS, EI in experiments.",
            "performance_vs_baseline": "BEACON outperforms MaxVar consistently; authors note BEACON requires less cost compared to MaxVar in practice because both incur GP training and acquisition optimization, but BEACON's acquisition is more effective at discovering novel behaviors.",
            "efficiency_gain": "No specific quantified gain vs random baselines provided in general; empirical results show BEACON surpasses MaxVar in behavior coverage under the same budgets.",
            "tradeoff_analysis": "Authors comment that learning a globally accurate surrogate (MaxVar's aim) is sufficient but not necessary for novelty discovery; BEACON's targeted novelty acquisition is more efficient for behavior discovery under a constrained budget.",
            "optimal_allocation_findings": "Allocating by predictive variance can identify uncertain regions but is less effective at maximizing behavior coverage than novelty-based posterior-sampled acquisitions.",
            "uuid": "e2456.2",
            "source_info": {
                "paper_title": "BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SVGP",
            "name_full": "Sparse Variational Gaussian Process (approximation)",
            "brief_description": "A scalable sparse GP approximation using inducing points and variational inference to reduce cubic scaling of exact GP inference, enabling BEACON to operate effectively at larger evaluation budgets.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "SVGP (sparse variational Gaussian process)",
            "system_description": "SVGP approximates the full GP posterior with a variational distribution parameterized by M inducing points (M &lt;&lt; N), reducing the cost of fitting and posterior inference. In BEACON, SVGPs are used to lower the computational cost of GP fits and acquisition optimization when N grows into the thousands, with empirical experiments using 100 inducing points and demonstrating favorable runtime and reachability tradeoffs.",
            "application_domain": "Scalable surrogate modeling within Bayesian active exploration/novelty search for high-budget problems (e.g., simulation-rich design spaces).",
            "resource_allocation_strategy": "Not an allocation policy per se, but reduces computational cost per allocation step so more iterations or larger candidate pools are practical.",
            "computational_cost_metric": "Empirical CPU time for acquisition optimization; theoretical cost reduced from O(N^3) for exact GP to O(NM^2) / O(M^3) dependent on implementation and M inducing points. Authors report SVGP CPU times that scale much better, noting e.g., CPU time for 5000 points with SVGP &lt; CPU time for 1000 points with exact GP in their experiment.",
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "SVGP itself does not prescribe exploration-exploitation; it is used inside BEACON (or MaxVar) to provide approximate posterior means/variances for acquisition computations.",
            "diversity_mechanism": null,
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Enables larger N budgets by reducing per-iteration compute.",
            "budget_constraint_handling": "Reduces computational resource requirements so BEACON-style novelty allocation can be applied at larger budgets; empirical demonstration on 20D Rosenbrock with 1000 evaluations.",
            "breakthrough_discovery_metric": "Indirect: helps BEACON maintain high reachability at larger budgets by making acquisition optimization tractable.",
            "performance_metrics": "Acquisition optimization CPU time vs number of training points; reachability performance using SVGP vs exact GP on 20D Rosenbrock showing SVGP enables BEACON to find far more diverse behaviors than baselines at large budgets.",
            "comparison_baseline": "Compared to exact GP within BEACON and other NS baselines; SVGP leads to much lower CPU time for large N with similar or slightly degraded reachability in experiments.",
            "performance_vs_baseline": "SVGP allows BEACON to scale and retain superior reachability; specific empirical claim: with SVGP BEACON found nearly 3× more diverse behaviors than NS-EA on a 20D Rosenbrock with 1000 evaluations, and CPU-time for SVGP scaled much better than exact GP.",
            "efficiency_gain": "Large reductions in CPU time for acquisition optimization at large N (e.g., per authors' experiments, SVGP with 5000 points is faster than exact GP with 1000 points); enables practical operation under large evaluation budgets.",
            "tradeoff_analysis": "SVGP trades some posterior fidelity for large computational gains; authors empirically show this tradeoff is favorable for novelty discovery at large budgets, although exact-GP accuracy may be preferable for small N.",
            "optimal_allocation_findings": "For large budgets, combine novelty acquisition with sparse GP approximations (SVGP) to maintain tractable acquisition optimization while preserving behavior discovery performance.",
            "uuid": "e2456.3",
            "source_info": {
                "paper_title": "BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SAAS",
            "name_full": "Sparse Axis-Aligned Subspace prior",
            "brief_description": "A hierarchical Bayesian prior over GP lengthscales that induces sparsity in input relevance, favoring explanations depending on a small subset of input dimensions — used to scale BEACON to very high-dimensional input spaces.",
            "citation_title": "High-dimensional Bayesian optimization with sparse axis-aligned subspaces.",
            "mention_or_use": "use",
            "system_name": "SAAS prior (sparse axis-aligned subspace)",
            "system_description": "SAAS places hierarchical priors on per-dimension GP lengthscales to favor a small active subspace of input dimensions; integrated into BEACON's GP surrogate to enable effective modeling and posterior sampling in problems with thousands of input dimensions (e.g., 2,133-d fragprint molecular representation or 2,000+ features).",
            "application_domain": "High-dimensional black-box optimization and novelty search, notably in molecular discovery/chemistry where descriptor spaces can be extremely high-dimensional.",
            "resource_allocation_strategy": "By discovering a sparse relevant subspace, SAAS reduces the effective dimensionality for surrogate learning and acquisition optimization, thereby focusing allocation on impactful input directions and improving sample efficiency.",
            "computational_cost_metric": "Costs measured in GP hyperparameter inference time and acquisition optimization time; SAAS is fully Bayesian and can increase hyperparameter inference cost but yields downstream savings in acquisition optimization.",
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Improves surrogate fidelity in the relevant subspace which indirectly affects exploration/exploitation decisions made by BEACON's acquisition (posterior sampling + novelty) by focusing modeling capacity on key inputs.",
            "diversity_mechanism": null,
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed query budget; SAAS helps within that budget by improving model efficiency in high-D.",
            "budget_constraint_handling": "Enables more effective allocation under limited budgets by reducing the 'curse of dimensionality' in surrogate modeling, thus improving the expected utility of each selected evaluation.",
            "breakthrough_discovery_metric": "Indirect: better modeling of relevant inputs increases probability of sampling truly novel/outlying behaviors in outcome space.",
            "performance_metrics": "Used in ESOL (2133-d fragprint) and LogD (125-d) tasks where BEACON with SAAS achieved superior reachability compared to baselines; specific numerical values are task-dependent and reported in paper graphs.",
            "comparison_baseline": "Compared indirectly via BEACON-with-SAAS vs other BEACON variants and baseline algorithms on high-dimensional molecular problems.",
            "performance_vs_baseline": "In high-dimensional molecular tasks, chemically-informed priors/kernels including SAAS improved BEACON performance versus baselines; exact numeric improvements are presented in the paper figures (e.g., improved reachability in ESOL and LogD experiments).",
            "efficiency_gain": "Enables BEACON to operate effectively in thousands of input dimensions by reducing needed samples to learn relevant input effects; exact percentage gains vary by dataset.",
            "tradeoff_analysis": "SAAS increases hyperparameter inference complexity but yields large benefits in downstream acquisition efficiency; recommended when few input dimensions are expected to control outcomes.",
            "optimal_allocation_findings": "When input relevance is sparse, using SAAS priors yields better surrogate models and improved allocation decisions for novelty discovery under limited budgets.",
            "uuid": "e2456.4",
            "source_info": {
                "paper_title": "BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Thompson-sampled Novelty Acquisition",
            "name_full": "Thompson Sampling-Based Novelty Acquisition (α_NS)",
            "brief_description": "An acquisition strategy that draws posterior function samples (Thompson sampling) from an MOGP and scores candidate inputs by outcome-space novelty computed relative to posterior-mean predictions of past observations, enabling uncertainty-aware selection of novel behaviors.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Thompson-sampled novelty acquisition (α_NS)",
            "system_description": "At each iteration the surrogate posterior is sampled to produce g(x). The novelty score α_NS(x|g,D) = (1/k) Σ_i dist(g(x), μ_D(x*_i)), where x*_i are the k closest observed outcomes to g(x) measured against μ_D at observed inputs. The acquisition uses differentiable posterior sampling (decoupled sampling) and a contiguous relaxation of argsort/sort for gradient-based optimization. This approach handles observation noise (by relying on posterior means) and model uncertainty (through posterior samples), prioritizing exploration of unobserved outcome regions.",
            "application_domain": "General active exploration for discovery tasks with expensive/noisy evaluations (materials, molecules, RL policy parameters, synthetic benchmarks).",
            "resource_allocation_strategy": "Allocates each expensive evaluation to the input maximizing α_NS under a sampled posterior, explicitly prioritizing inputs predicted to produce novel outcomes relative to observed behavior bins and posterior mean predictions.",
            "computational_cost_metric": "Cost drivers: generating differentiable function-space posterior samples (Wilson et al. 2020 method), computing k-NN distances against deduplicated behavior bins (cost O(|B|) rather than O(N)), and gradient-based acquisition optimization (multi-start L-BFGS-B or Adam iterations). Measured in CPU time per acquisition optimization; GP training cost separate.",
            "information_gain_metric": "No explicit information-theoretic measure; uncertainty is incorporated via posterior sampling, but the acquisition maximizes a novelty utility rather than expected information gain.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Uncertainty -&gt; exploration: Thompson samples cause stochastic exploration across plausible functions; novelty utility pushes selection to outcome-space regions far from prior observations, producing exploration-focused behavior; exploitation of a scalar performance objective is not used.",
            "diversity_mechanism": "Explicit diversity objective via k-NN outcome-space distance and behavior bin deduplication; supports user-guided partitions to bias diversity to particular outcome regions.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed evaluation budget; method designed to maximize behavioral coverage within that budget.",
            "budget_constraint_handling": "Directly optimizes per-evaluation novelty to reduce cumulative behavior gap given T; deduplication reduces redundant evaluations and computational cost per acquisition.",
            "breakthrough_discovery_metric": "High α_NS scores correspond to candidate inputs expected to produce previously unseen behavior bins (i.e., breakthroughs in behavior coverage).",
            "performance_metrics": "Empirical reachability/BG_t metrics and successful discovery rates in case studies; shown to outperform MaxVar and evolutionary NS baselines.",
            "comparison_baseline": "Compared to MaxVar, NS variants (NS-EA, NS-DEA, NS-FS), Sobol, RS, and EI in experiments.",
            "performance_vs_baseline": "Produced substantially higher reachability than baselines across many tasks; authors report concrete examples (≈80% reach vs ≤40% on Multi-Output Plus, full reachability on MOF tasks, consistent success in RL where EI failed).",
            "efficiency_gain": "Enables discovering a broader set of behaviors using fewer expensive evaluations relative to baselines; exact gains are task-dependent and reported in experimental figures.",
            "tradeoff_analysis": "Combines posterior sampling (stochastic exploration) and novelty utility to trade model uncertainty handling for direct diversity gains; authors empirically show this is more cost-effective (in number of expensive evaluations) than variance-based or evolutionary novelty approaches, at the expense of higher computational overhead per selection.",
            "optimal_allocation_findings": "Posterior-sampled novelty acquisition is an effective allocation rule for maximizing behavior coverage under tight query budgets; combining it with deduplication and sparse approximations yields a practical balance between computational cost and discovery performance.",
            "uuid": "e2456.5",
            "source_info": {
                "paper_title": "BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "NS-EA / NS-DEA / NS-FS",
            "name_full": "Evolutionary Novelty Search Variants (NS-EA, NS-DEA, NS-FS)",
            "brief_description": "Evolutionary and heuristic-based novelty search methods used as baselines: NS-EA (original evolutionary novelty search optimizing k-NN distance in outcome space), NS-DEA (distance-to-explored-area variant), and NS-FS (novelty in input/feature space).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "NS-EA / NS-DEA / NS-FS (evolutionary novelty search baselines)",
            "system_description": "NS-EA: an evolutionary algorithm that selects and evolves populations to maximize an outcome-space novelty metric (average distance to nearest stored outcomes). NS-DEA: variant that measures distance to convex hull of explored outcomes to push outside explored regions. NS-FS: variant that measures novelty in input (feature) space rather than outcome space. All are heuristic, population-based, rely on mutation/selection and maintain archives of novel solutions.",
            "application_domain": "General novelty-driven exploration tasks (robotics, evolutionary design, materials, synthetic benchmarks); used here as benchmarks for comparison with BEACON.",
            "resource_allocation_strategy": "Population-based evolutionary sampling: allocate budget across generational populations by mutation, recombination, and selection prioritized by novelty scores; resource allocation is implicit through population/offspring sizes and evolutionary operators.",
            "computational_cost_metric": "Low per-evaluation computational overhead beyond objective evaluations; sample-inefficient in terms of number of expensive evaluations required; runtimes are small per iteration but require many evaluations.",
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Primarily exploration via novelty-driven selection; some variants can combine novelty with fitness, but original NS-EA abandons objective exploitation.",
            "diversity_mechanism": "Explicitly promotes diversity via archive-based novelty metrics (k-NN distance, distance to explored area, feature-space distances) and selection pressure towards novel behaviors.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of evaluations; practical effectiveness limited when evaluations are expensive due to sample inefficiency.",
            "budget_constraint_handling": "Population size and offspring size hyperparameters control search effort; the methods do not explicitly optimize under tight budgets and tend to require dense sampling.",
            "breakthrough_discovery_metric": "Novelty metrics (k-NN distances, distance to convex hull) and archive additions indicate breakthroughs; however, noisy observations and expensive evaluations cause misclassification and inefficiency.",
            "performance_metrics": "Reachability/BG_t; empirical findings show these evolutionary approaches are often beaten by BEACON under tight budgets—e.g., on Multi-Output Plus and high-dimensional tasks.",
            "comparison_baseline": "These algorithms serve as baselines against BEACON, MaxVar, Sobol, RS, EI.",
            "performance_vs_baseline": "BEACON significantly outperforms NS-EA/NS-DEA/NS-FS on most tested problems, especially when evaluations are costly or noisy; evolutionary approaches found to be sample-inefficient; NS-EA performance insensitive to moderate changes in population/offspring size per ablation.",
            "efficiency_gain": "Not applicable; these are baseline methods demonstrating lower sample efficiency compared to BEACON.",
            "tradeoff_analysis": "Authors highlight that evolutionary novelty search methods are heuristic and sample-inefficient, making them ill-suited for expensive black-box discovery tasks; noise further degrades performance because heuristics typically assume noiseless evaluations.",
            "optimal_allocation_findings": "For expensive noisy settings, replacing heuristic evolutionary novelty search with uncertainty-aware surrogate-based allocation (BEACON) yields far better behavior coverage per expensive evaluation.",
            "uuid": "e2456.6",
            "source_info": {
                "paper_title": "BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "High-dimensional Bayesian optimization with sparse axis-aligned subspaces.",
            "rating": 2,
            "sanitized_title": "highdimensional_bayesian_optimization_with_sparse_axisaligned_subspaces"
        },
        {
            "paper_title": "Efficiently sampling functions from Gaussian process posteriors.",
            "rating": 2,
            "sanitized_title": "efficiently_sampling_functions_from_gaussian_process_posteriors"
        },
        {
            "paper_title": "Scalable Thompson sampling using sparse Gaussian process models.",
            "rating": 2,
            "sanitized_title": "scalable_thompson_sampling_using_sparse_gaussian_process_models"
        },
        {
            "paper_title": "GAUCHE: A library for Gaussian processes in chemistry",
            "rating": 2,
            "sanitized_title": "gauche_a_library_for_gaussian_processes_in_chemistry"
        },
        {
            "paper_title": "Gaussian process optimization in the bandit setting: No regret and experimental design",
            "rating": 2,
            "sanitized_title": "gaussian_process_optimization_in_the_bandit_setting_no_regret_and_experimental_design"
        },
        {
            "paper_title": "Novelty search and the problem with objectives",
            "rating": 1,
            "sanitized_title": "novelty_search_and_the_problem_with_objectives"
        }
    ],
    "cost": 0.022779,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search</p>
<p>Wei-Ting Tang 
Department of Chemical andBiomolecular Engineering
The Ohio State University
ColumbusOHUSA</p>
<p>Ankush Chakrabarty achakrabarty@ieee.org 
Mitsubishi Electric Research Laboratories
CambridgeMAUSA</p>
<p>Joel A Paulson paulson.82@osu.edu 
Department of Chemical andBiomolecular Engineering
The Ohio State University
ColumbusOHUSA</p>
<p>BEACON: A Bayesian Optimization Inspired Strategy for Efficient Novelty Search
7053214F3260C958F8A8BEC95FE9113A
Novelty search (NS) refers to a class of exploration algorithms that seek to uncover diverse system behaviors through simulations or experiments.Such diversity is central to many AI-driven discovery and design tasks, including material and drug development, neural architecture search, and reinforcement learning.However, existing NS methods typically rely on evolutionary strategies and other meta-heuristics that require dense sampling of the input space, making them impractical for expensive black-box systems.In this work, we introduce BEACON, a sample-efficient, Bayesian optimization-inspired approach to NS that is tailored for settings where the input-to-behavior relationship is opaque and costly to evaluate.BEACON models this mapping using multi-output Gaussian processes (MOGPs) and selects new inputs by maximizing a novelty metric computed from posterior samples of the MOGP, effectively balancing the exploration-exploitation trade-off.By leveraging recent advances in posterior sampling and high-dimensional GP modeling, our method remains scalable to large input spaces and datasets.We evaluate BEACON across ten synthetic benchmarks and eight real-world tasks, including the design of diverse materials for clean energy applications.Our results show that BEACON significantly outperforms existing NS baselines, consistently discovering a broader set of behaviors under tight evaluation budgets.</p>
<p>Introduction</p>
<p>Modern scientific and engineering discovery increasingly relies on black-box systems, where inputs x ∈ X map to system behaviors or outcomes f (x) ∈ O through an unknown, expensive-to-evaluate function f : X → O. Certain derivative-free optimization (DFO) methods, such as Bayesian optimization (BO), offer sample-efficient strategies to identify inputs that yield optimal outcomes based on user-defined objective functions (Shahriari et al. 2015;Frazier 2018).However, there exist many critical problems where the goal is not to optimize a single objective but to uncover a diverse set of system behaviors -often with no clear objective to guide the search.</p>
<p>Consider, for example, a forced Duffing oscillator, where f maps physical and forcing parameters x to dynamic behaviors.Depending on x, the system exhibits chaotic, periodic, quasi-periodic, or fixed-point dynamics (Zeni and Gallas 1995).It is difficult to design an objective function that induces all of these behaviors, and optimization over such a landscape would be riddled with local optima.Instead, a more natural solution is to abandon optimization altogether and explicitly search for diverse outcomes, which is the central idea behind novelty search (NS).NS algorithms prioritize exploration by actively seeking novel behaviors rather than maximizing a fixed performance metric (Lehman and Stanley 2011a,c).This perspective has proven valuable in applications such as scientific discovery (Grizou et al. 2020), material design (Terayama et al. 2020), and reinforcement learning (Jackson and Daley 2019), particularly in so-called "deceptive problems" where traditional optimization easily gets stuck.However, most existing NS methods rely on meta-heuristics such as evolutionary algorithms, which are notoriously sample-inefficient, limiting their practical utility when evaluations are costly.</p>
<p>In this work, we introduce a novel Bayesian approach to NS that effectively addresses this sample-efficiency bottleneck.Our method, called BEACON (Bayesian Exploration Algorithm for outCOme Novelty), models the inputto-outcome map using multi-output Gaussian processes (MOGPs) (Williams and Rasmussen 2006; Liu, Cai, and Ong 2018) and selects new queries via a posterior-sampled novelty acquisition function.Unlike standard BO, BEACON does not optimize toward a specific objective, but instead promotes the discovery of novel behaviors defined in a userspecified outcome space.Thus, the main contributions of this paper can be summarized as follows:</p>
<p>• We propose BEACON, a sample-efficient NS algorithm for noisy, expensive black-box systems that identifies diverse behaviors using few evaluations.</p>
<p>• We introduce a Thompson sampling-based acquisition strategy for novelty that handles stochastic observation noise and supports gradient-based optimization.</p>
<p>• We develop two scalable extensions for highdimensional problems: one based on sparsity-inducing priors and another tailored for computational chemistry.</p>
<p>• We demonstrate BEACON's effectiveness across ten synthetic benchmarks and eight real-world applications, including a 2133-dimensional molecular discovery problem and a first-of-its-kind application to discovering diverse metal-organic frameworks for clean energy.</p>
<p>arXiv:2406.03616v4 [stat.ML] 30 Jul 2025</p>
<p>Our work builds on ideas from BO (Zhilinskas 1975;Mockus 1974), which is a framework for optimizing expensive black-box functions.It has regained popularity in recent years due to its outstanding performance on tasks such as hyperparameter tuning in machine learning methods (Snoek, Larochelle, and Adams 2012).Readers are referred to (Shahriari et al. 2015) and (Frazier 2018) for a recent review and tutorial introduction to BO.While BEACON shares structural similarities with multi-objective BO (Daulton, Balandat, and Bakshy 2020) (e.g., modeling multiple outcomes using MOGPs), its aim is fundamentally different.Instead of learning a Pareto front over competing objectives, BEACON seeks to uncover novel, previously unseen system behaviors across a user-defined outcome space.We argue, similarly to (Grizou et al. 2020), the latter is more appropriate in many relevant scientific discovery applications.Recent work has also explored diversity within BO via diversity-constrained objectives (Maus et al. 2022), but these methods remain tethered to explicit optimization goals and often operate in local regions of the input space.</p>
<p>In contrast, BEACON performs global, objective-free search driven purely by novelty.</p>
<p>BEACON is also directly connected to the original NS strategy introduced in (Lehman and Stanley 2011c), which optimizes a distance-based novelty metric using evolutionary algorithms (NS-EA).Subsequent work extended NS to solve deceptive optimization problems (Risi, Hughes, and Stanley 2010; Mouret 2011; Gomes, Mariano, and Christensen 2015) and to jointly consider novelty and fitness (Lehman and Stanley 2011b).Illumination algorithms like MAP-Elites (Mouret and Clune 2015) further emphasize behavioral diversity by maintaining archives of diverse solutions.However, all of these approaches suffer from two key limitations: (i) they rely on sample-inefficient, heuristicbased search strategies and (ii) they typically assume noiseless observations.BEACON addresses both issues by integrating principles from BO, enabling principled uncertainty modeling and efficient sampling even in high-dimensional, noisy settings.To our knowledge, BEACON is the first NS algorithm to offer such capabilities.</p>
<p>Problem Setup</p>
<p>We consider a vector-valued black-box function f : X → O with f = (f (1) , . . ., f (n) ) that maps an input space X ⊂ R d to a multi-dimensional outcome space O ⊂ R n .Although O is continuous, we assume nearby outcomes correspond to similar system behaviors.We discretize this space using an ϵ-covering B ⊂ R n , such that for every y ∈ O, there exists y ′ ∈ B with ∥y − y ′ ∥ ≤ ϵ.</p>
<p>The goal is to discover inputs whose outcomes cover as many distinct behaviors in B as possible.Since f is expensive to evaluate, we operate under a finite query budget T , evaluating f sequentially over t = 1, . . ., T .Each query point x t ∈ X yields a noisy observation y t = f (x t ) + η t , where η t ∼ N (0, σ 2 I n ) denotes independent Gaussian noise.Let φ : O → B denote the projection from the continuous outcome to its discrete behavior.We define the be-havior gap at iteration t as
BG t = 1 − φ f (x 1 ) , • • • , φ f (x t ) /|B| ,
which quantifies the fraction of behaviors not yet observed.</p>
<p>The objective is to minimize the cumulative behavior gap T t=1 BG t , or equivalently, to maximize behavioral diversity across evaluations.</p>
<p>We model f using a multi-output Gaussian process (MOGP) prior (Liu, Cai, and Ong 2018), assuming it lies in a smooth function class such as a Reproducing Kernel Hilbert Space (RKHS) (Chowdhury and Gopalan 2017).The MOGP yields a posterior distribution P(f | D t ) over the function f given observations D t = {(x i , y i )} t i=1 , with closed-form expressions for the posterior mean and covariance, as detailed in the next section.</p>
<p>Proposed BEACON Method</p>
<p>Multi-Output Gaussian Process Surrogate</p>
<p>Gaussian processes (GPs) offer a flexible, non-parametric way to model black-box functions (Williams and Rasmussen 2006).A GP prior is fully specified by a prior mean function µ and covariance (or kernel) function κ.To (potentially) capture correlations among the n outputs of f , we adopt the latent-input construction: define h(x, j) = f (j) (x) for j ∈ J = {1, . . ., n} and place a scalar GP prior h ∼ GP(µ, κ) over the extended space X × J , following, e.g., (Kudva, Tang, and Paulson 2024).Because observation noise is assumed i.i.d.Gaussian, the posterior after N observations A = {(x i , j i , y i )} N i=1 , denoted by h|A ∼ GP(µ A , κ A ), remains a GP with analytically tractable mean µ A and covariance κ A functions.For completeness, we list these closed-form expressions in Appendix C.</p>
<p>This construction has two advantages: (i) information shared across outputs improves sample efficiency and (ii) any scalable single-output GP approximation (e.g., inducing points or stochastic variational inference) can be easily applied without modification.</p>
<p>Thompson Sampling-Based Acquisition for Novelty Search</p>
<p>Our goal is to discover novel system behaviors by exploring unobserved regions of the outcome space O.A common strategy in the NS literature is to evaluate how far a predicted outcome f (x) lies from previously observed outcomes.One such novelty metric is the average distance to the k nearest neighbors among past evaluations:
ρ(x|D) = 1 k k i=1 dist(f (x), y ⋆ i ),(1)
where {y ⋆ 1 , . . ., y ⋆ k } ⊂ {y 1 , . . ., y N } are the k nearest outcomes to f (x) in the dataset D = {(x i , y i )} N i=1 , and dist(•) is a user-defined distance metric over O.</p>
<p>Although intuitive, this metric (and other related metrics that are accumulation operators over a finite set of points) poses two challenges in our setting.First, f is a black-box function, so ρ(x|D) is not available until after an expensive query.Second, observational noise can degrade NS performance by misclassifying behaviors -for example, due to a realization of η, it may hold that φ(f (x) + η) ̸ = φ(f (x)).Note that we perform a detailed analysis of this phenomenon empirically in Appendix B.</p>
<p>To address these issues, we replace the true f with a MOGP posterior surrogate f |D ∼ MOGP(µ D , κ D ).The MOGP serves two key purposes: (i) it enables us to filter observation noise by replacing measured data with a surrogate prediction and (ii) it provides a way for us to fantasize future outcome realizations while accounting for uncertainty.The latter step is accomplished through Thompson Sampling (TS), which is a classical strategy for decision-making under uncertainty (Thompson 1933).Our acquisition function can thus be expressed as follows:
α NS (x|g, D) = 1 k k i=1 dist(g(x), µ D (x ⋆ i )),(2)
where g(x) ∼ f |D is a posterior function sample and {x ⋆ 1 , . . ., x ⋆ k } ⊂ {x 1 , . . ., x N } are the set of k closest outcomes to g(x) in terms of distance to the posterior mean predictions {µ D (x 1 ), . . ., µ D (x N )}.This acquisition function promotes exploration of unvisited outcome regions in the presence of model uncertainty.To generate g efficiently, we employ the decoupled sampling strategy of (Wilson et al. 2020), which combines weight-and function-space views to produce high-accuracy differentiable posterior samples.We describe how to efficiently and differentiably compute the knearest neighbors needed for this acquisition in Section 4.4.</p>
<p>BEACON Algorithm Description</p>
<p>We now summarize the pseudocode for BEACON in Algorithm 1, which combines MOGP surrogate modeling with a TS-based acquisition function to promote exploration of novel behaviors.At each iteration, BEACON fits a MOGP model to all available data, draws a posterior sample, and selects a new query point that maximizes predicted novelty in the outcome space.This process is then repeated until the maximum number of iterations has been reached (or some other simple stopping crteria is employed).A visual illustration of this process is shown in Figure 1.The pseudocode presents BEACON in a standard sequential setting for clarity.However, it can be readily extended to parallel or asynchronous evaluation using posterior sampling strategies from prior work on parallel BO (Kandasamy et al. 2018).</p>
<p>Gradient-Based Acquisition Function Optimization</p>
<p>Efficiently maximizing the BEACON acquisition function α NS is essential to practical performance (Line 5 of Algorithm 1).where e k is a vector whose first k entries are equal to 1 and the remaining entries are equal to 0 and sort(•) denotes the descending sort operator.As discussed in (Prillo and Eisenschlos 2020), the standard sort operator is continuous and almost everywhere differentiable (with non-zero gradients), allowing us to apply gradient-based optimization effectively.The computational cost of this formulation scales with N , which can be large in practice.To mitigate this, we exploit a key property of BEACON: only distinct behavioral outcomes matter.Instead of computing distances to all outcomes in D, we restrict to a deduplicated subset with unique behavior bins.This reduces the complexity of (3) from O(N ) to O(|B|), which is constant across iterations.In Appendix F, we show how combining this idea with sparse GP approximations enables BEACON to easily scale to evaluation budgets in the tens of thousands.
α NS (x|g, D) = 1 k e ⊤ k sort ({dist(g(x), µ D (x))} x∈D ) ,(3)</p>
<p>Scaling BEACON to High-Dimensional Problems</p>
<p>One of the advantages of BEACON's design is its compatibility with modern BO advances, particularly for highdimensional input spaces.Fit MOGP posterior: (Gardner et al. 2014), can also be incorporated with minor changes.SAAS Priors.The sparse axis-aligned subspace (SAAS) prior (Eriksson and Jankowiak 2021) is a fully Bayesian approach that introduces adaptive sparsity into GP models.SAAS places a hierarchical prior over the lengthscales of each input dimension, favoring explanations that involve a small number of relevant inputs.This formulation naturally supports input relevance discovery, which is especially important when only a few dimensions influence the outcome behavior.Due to its generality and compatibility with automatic relevance determination, SAAS is well suited as a default surrogate prior in high-dimensional BO.We use SAAS to scale BEACON on a challenging molecular discovery task with over 2,000 input dimensions in Section 5.3.
MOGP t−1 ← f | D t−1 4: Sample g ∼ MOGP t−1 5: x t ← argmax x∈X α NS (x|g, D t−1 ) 6: Observe outcome y t = f (x t ) + η t 7: Update D t ← D t−1 ∪ {(x t , y t )} 8: end for 9: Return: Final dataset D T output constraints</p>
<p>GAUCHE for Discrete Molecular Inputs.</p>
<p>While SAAS is effective when input relevance is sparse and independent, it may underperform in domains where inputs exhibit strong correlations (e.g., spatially structured data) or belong to non-Euclidean spaces.In these cases, domain-specific surrogate models are more appropriate.The GAUCHE framework (Griffiths et al. 2024) offers a principled GP modeling interface for structured chemical/biological inputs, including SMILES strings, proteins, and reaction graphs.Since GAUCHE is directly built on top of standard GP principles, BEACON can directly use GAUCHE models as drop-in surrogates.We demonstrate this extension in our solubility case study (Section 5.3).</p>
<p>Numerical Experiments</p>
<p>In this section, we compare BEACON against established NS algorithms and their variants including the standard evolutionary-based method (NS-EA) (Lehman and Stanley 2011c), a distance-enhanced evolutionary method (NS-DEA) (Doncieux et al. 2020), and a feature space-based method (NS-FS).We also compare with the performance of three other algorithms: one that chooses points uniformly at random over X (RS); one that chooses points using lowdiscrepancy quasi-random Sobol samples over X (Sobol); and a maximum variance active learning strategy (MaxVar).Full implementation details for BEACON and the benchmark algorithms are provided in Appendix A. We also include detailed ablation studies on the main hyperparameters of the various methods in Appendix B.</p>
<p>We evaluate performance on 10 synthetic and 8 real-world problems, which are described below or in the Appendices.In all problems, we generate an initial set of 10 points uniformly at random over X as a first stage to seed the algorithms.Then, we use the algorithms to select an additional 80 to 300 points for evaluation, which is depicted in the figures below.All experiments are replicated 20 times.We use reachability as our performance metric, which is defined as Reach t = 1−BG t (the fraction of total behaviors observed).In all figures, we report the mean of Reach t plus and minus one standard deviation computed over the independent replicates.Information on the runtimes are provided in Appendix A.8 and the code that can be used to replicate our results has been provided in a supplemental zip file.</p>
<p>Lastly, to demonstrate BEACON's flexibility to handle user-defined preferences, we also present a small extension and highlight how that can further improve exploration efficiency on an MNIST digit discovery problem.</p>
<p>Synthetic Functions</p>
<p>We create three synthetic test functions from popular global optimization benchmarks -Ackley, Rosenbrock, and Styblinski-Tang -by adapting them to the NS problem setting.We treat the output of these functions as our outcomes and partition the range of outcomes into 25 equallyspaced intervals to define the behaviors Further details are provided in Appendix D.1.The reachability performance of all algorithms across 9 problems (3 synthetic functions, each with 3 input dimensions d ∈ {4, 8, 12}) is shown in Figure 3. BEACON consistently outperforms the other algorithms in all cases, achieving near perfect reachability values of 1 in several cases.We further see that the performance gap increases as problem dimension increases.</p>
<p>To test BEACON in the multi-output setting, we also created a challenging Multi-Output Plus function with a longtail joint distribution.The function has 6 inputs and 2 outputs (full description given in Appendix D.1).The distribution of outcomes for 10,000 inputs drawn uniformly at random from the space X = [−5, 5] D is shown in Figure 2 (top).Reachability is computed with respect to intervals defined on a 10 × 10 grid over the two outcomes.NS algorithms find such jointly long-tail distributions challenging, as they tend to overexplore the high density center zone and ignore the tails.This is evident in the reachability results in Figure 2 (bottom).We see BEACON consistently has larger reachability than the other algorithms, uncovering nearly 80% while others only find 40% or less.</p>
<p>Material Discovery Applications</p>
<p>Metal-organic frameworks (MOFs) are a highly diverse class of porous crystalline materials constructed from metal ions and organic linkers.Due to the vast design space, which spans linkers, metal centers, topologies, and defects, there is currently no comprehensive theoretical framework to assess the full chemical or property diversity that MOFs can exhibit (Lee et al. 2021).Recent studies have shown that existing MOF libraries are often biased, which can distort conclusions drawn from machine learning screening studies (Moosavi et al. 2020).These challenges make novelty-driven search especially appealing.However, because MOF property evaluations are often costly (experimentally or computationally), sample-efficient approaches like BEACON are well-suited for the task.</p>
<p>In our setting, each candidate MOF is represented by a fixed-dimensional vector of molecular descriptors x ∈ X ⊂ R d .The associated properties of interest form the outcome y = f (x) ∈ O ⊂ R n , where f is modeled as a black-box function.The choice of descriptors is adapted from domainspecific prior work (see Appendix D.2 for details).We consider three MOF discovery tasks that reflect different search complexities and outcome dimensionalities.</p>
<p>Hydrogen uptake capacity.Hydrogen storage is central to many clean energy technologies.We use a dataset from (Ghude and Chowdhury 2023) with 98,000 unique MOFs.Each MOF is encoded using 7 real-valued features, yielding X ⊂ R 7 and O ⊂ R 1 .</p>
<p>Nitrogen uptake capacity.Efficient nitrogen removal is important for natural gas purification.We use a dataset from (Daglar and Keskin 2022) consisting of 5,224 MOFs, each described by 20 features, i.e., X ⊂ R 20 and O ⊂ R 1 .</p>
<p>Joint CO 2 and CH 4 uptake.We aim to discover MOFs with diverse combinations of carbon dioxide (y 1 ) and methane (y 2 ) uptake capacities.The dataset from (Moosavi et al. 2020) includes 7,000 MOFs, represented by 25 features.Thus, X ⊂ R 25 and O ⊂ R 2 .The joint outcome distribution is highly skewed and visualized in Appendix D.2.</p>
<p>Figure 4 compares the reachability performance of BEA-CON and competing methods on these tasks.As the MOF design problems involve discrete input sets, standard evolutionary NS methods (NS-EA, NS-DEA, NS-FS) are not applicable.Across all three tasks, BEACON consistently achieves full reachability, indicating its ability to uncover maximal property diversity under limited evaluations.</p>
<p>Drug Discovery Applications</p>
<p>Exploring the chemical space to identify small molecules with diverse properties is a foundational task in pharmaceutical research.BEACON provides a sample-efficient strategy to promote novelty across molecular properties while accommodating the unique modeling challenges of chemical data.For all tasks, the input space X ⊂ R d encodes molecular structure using either descriptor-based or highdimensional representations, while the outcome space O ⊂ R corresponds to molecular properties of interest.We consider three benchmark datasets with varying dimensionality and modeling complexity.</p>
<p>Water solubility.We use the dataset from (Boobier et al. 2020) containing 900 small organic molecules.Each molecule is encoded using a 14-dimensional molecular descriptor vector.</p>
<p>ESOL.We use the dataset from (Delaney 2004) containing 1,128 molecules with experimentally measured aqueous solubility.Instead of low-dimensional descriptors, we adopt a 2,133-dimensional binary fragprint representation, which captures detailed structural information.BEACON is paired with the Tanimoto-fragprint kernel (Griffiths et al. 2024).</p>
<p>LogD.The octanol-water distribution coefficient (LogD) is a key property in drug discovery.We use a dataset of 2,070 molecules from (Win, Cheong, and Hopkins 2023), each encoded with 125 features.To address the high dimensionality, we incorporate the sparse axis-aligned subspace (SAAS) prior (Eriksson and Jankowiak 2021), which promotes automatic feature selection within BEACON.</p>
<p>Figure 5 shows that BEACON consistently outperforms other methods in all tasks.Notably, in high-dimensional settings (ESOL and LogD), the use of chemically informed priors/kernels significantly improves performance.Additional experiment details are provided in Appendix D.3.</p>
<p>Exploring Deceptive Landscapes in Reinforcement Learning</p>
<p>We evaluate BEACON on a challenging reinforcement learning (RL) task involving navigation through a large maze.The objective is to control a ball from a start to a goal position within 300 steps using a linear policy defined by 8 tunable parameters.Instead of optimizing final distance alone, we use a normalized reward that measures relative improvement toward the target.See Appendix D.4 for a figure of the maze and additional details on the problem setup.This setting is highly deceptive: reward-based acquisition can mislead search toward local optima.Figure 6 shows that BEACON consistently outperforms both standard NS variants and reward-driven approaches, including classical expected improvement (EI) acquisiton in BO.The top panel   cluding the objective-focused EI approach, which ranks as the second-worst performing algorithm.The bottom panel presents a violin plot of the reward distribution at the final iteration for all algorithms.BEACON is the only method that consistently achieves successful escapes the maze, with all 20 replicates yielding a final reward of 1.These results clearly demonstrate the benefit of sample-efficient noveltydriven search in complex RL domains.</p>
<p>Incorporating User Information</p>
<p>In many practical settings, users bring domain knowledge or preferences about the behaviors they seek to discover.For example, a scientist may wish to focus exploration on materials with specific target properties or avoid re-evaluating regions that have already yielded satisfactory solutions.To accommodate such guidance, we introduce "UG-BEACON", which is a user-guided extension of BEACON that incorporates behavior-level constraints to focus exploration.UG-BEACON modifies the acquisition strategy to avoid selecting candidates whose predicted outcomes fall within behavior regions already explored -this can be straightforwardly accomplished by enforcing a constraint on the prediction during the acquisition optimization.</p>
<p>We evaluate UG-BEACON on a case study using the wellknown MNIST dataset (Deng 2012).The task is to discover as many distinct handwritten digits (0 through 9) as possible by sampling from the latent space of a trained variational autoencoder (VAE).UG-BEACON uses a convolutional neural network (CNN) classifier to identify whether a newly generated digit matches a previously observed class and prioritizes exploration of novel digit types.As shown in Fig-</p>
<p>Conclusions</p>
<p>We presented BEACON, a sample-efficient algorithm for novelty search (NS) in settings where outcomes are defined by expensive, noisy black-box functions.BEACON integrates ideas from Bayesian optimization to guide exploration toward novel behaviors while accounting for uncertainty, enabling strong performance even in low-data regimes.Our experiments across a wide variety of synthetic, scientific, and reinforcement learning tasks demonstrate that BEACON consistently outperforms existing NS methods, particularly in high-dimensional or deceptive environments.</p>
<p>While BEACON offers substantial gains in sample efficiency, it does incur higher computational cost than simpler alternatives, which may limit its usefulness in low-cost evaluation settings.Theoretical convergence guarantees also remain an open question.Nonetheless, BEACON opens new opportunities for NS in real-world domains, and future work may extend its scope to even broader decision-making and design problems.</p>
<p>B Ablation Studies</p>
<p>This appendix presents ablation studies to assess the sensitivity of BEACON to key design choices, including behavior resolution, nearest neighbor count, and noise filtering.We also evaluate the robustness of baseline algorithms like NS-EA to their internal hyperparameters.</p>
<p>B.1 Impact of the Size of Behavior Space</p>
<p>In this section, we study the impact of the choice of ϵ (directly related to grid size) that defines how nearby points in outcome space O are divided into new behaviors B. In practice, a user does not have to have an actual value for ϵ selected -as long as some "clusters" exist in the outcome space that (once observed) can be treated as behaviors, BEACON will eventually uncover them through exploration of O. Therefore, ϵ only impacts how calculate reachability (or equivalently the behavior gap).We plot the reachability performance for 3 different grid values (10, 50, 100) across the 4-dimensional versions of the Ackley and Rosenbrock synthetic test problems in Figure 1.BEACON continuous to be the best-performing algorithm for all grid sizes, highlighting BEACON's robustness to the choice of B. It is interesting to note that gap between BEACON and the other algorithms does appear to increase with increasing grid size; however, more analysis would be needed to see if this trends holds for a larger set of problems of varying dimensions and complexity.</p>
<p>B.2 Impact of Number of Nearest Neighbors</p>
<p>The choice of the number of nearest neighbors k is a hyperparameter of our algorithm.We selected it to be k = 10 in all case studies, as we found that to be a robust choice.We study the impact of k on the 12-dimensional versions of the synthetic test problems in Figure 2 by calculating performance for k ∈ {1, 5, 10, 20}.In this experiment, we use 50 initial datapoints to train the GP and use 25 equally-spaced intervals to divide outcomes into behaviors.Surprisingly, we find that even a choice of k = 1 does reasonably well on Rosenbrock and Styblinski-Tang, though performance does start to drop for Ackley.We see negligible difference in performance between k = 10 and k = 20, so it appears k = 10 is a sufficiently large value in practice.</p>
<p>B.3 Importance of Filtering Observation Noise</p>
<p>As discussed in Section 4 of the main paper, the presence of noise in the outcome evaluation can lead to challenges in NS algorithms due to miscategorization of behaviors, which we explore in this section.Specifically, we compare the performance of BEACON (Algorithm 1 in the main paper) to a noiseless variant (BEACON-noiseless) in which the acquisition function in equation ( 3) in the main paper is modified by replacing µ D (x ⋆ i ) with y ⋆ i .We perform experiments on the 4d Ackley problem in which 50 initial data points are available and we divide the outcome space into 50 equally-spaced intervals to calculate reachability.Figure 3 shows the performance of BEACON and BEACON-noiseless for four different values of the noise standard deviation σ.We see BEACON outperforms BEACON-noiseless in all cases and the gap between them widens as σ increases.This study emphasizes the importance of accounting for observation noise in NS, which, to our knowledge, has not been considered by existing NS algorithms.</p>
<p>B.4 Hyperparameter Study for NS-EA</p>
<p>NS-EA is one of the most popular NS algorithms, which does have some hyperparameters that could potentially be tuned to improve performance in specific cases.Our results in Section 5 of the main paper are based on the default settings of the implementation of NS-EA described in Appendix A. To ensure that these settings are reasonably robust for the problems considered in this work, we studied the impact of two key hyparparameters, mainly population size (default value of 10) and offspring size (default value of 10), in this section.The reachability performance of NS-EA for three population sizes {10, 20, 40} (default offspring size of 10) on the synthetic test problems is shown in Figure 4. We see that the performance is fairly similar across these different population sizes, with smaller values doing slightly better on 4d problems and larger values doing slightly better on 12d problems.We similarly plot the reachability performance of NS-EA for three offspring sizes {10, 20, 40} (default population size of 10) on the synthetic test problems in Figure 5. Again, performance is similar in all cases, with the default value yielding equal or better results in most cases.This study suggest that our performance analysis in Section 5 of the main paper is not sensitive to the particular settings of the benchmark algorithms.</p>
<p>C Multi-Output Gaussian Process Posterior Expressions</p>
<p>This appendix summarizes the posterior expressions for the multi-output Gaussian process (MOGP) model [10] used in BEACON when reformulated as a single-output GP over an extended input space X × J , where J indexes the output dimension.This formulation enables convenient implementation with standard GP software libraries while retaining the modeling power of independent-output MOGPs.Given a prior h ∼ GP(µ, κ) over the extended space, and a dataset A = {(x i , j i , y i )} N i=1 , the posterior h | A remains a GP with mean and covariance functions:
µ A (x, j) = µ(x, j) + κ ⊤ A (x, j) K A + σ 2 I N −1 (y − µ A ) ,(1a)κ A ((x, j), (x ′ , j ′ )) = κ((x, j), (x ′ , j ′ )) − κ ⊤ A (x, j) K A + σ 2 I −1 κ A (x ′ , j ′ ),(1b)
where y = [y 1 , . . ., y N ] ⊤ , µ A = [µ(x 1 ), . . ., µ(x N )] ⊤ , κ A (x, j) ∈ R N is the vector of covariance values between the test input (x, j) and the observed inputs in A, and K A ∈ R N ×N is the covariance matrix between all observed inputs in A.</p>
<p>D Additional Details on Numerical Experiments</p>
<p>This appendix provides supplementary details on the benchmark problems used to evaluate BEACON.We describe the synthetic test functions, real-world case studies in materials and drug discovery, and a reinforcement learning task related to maze navigation.For each problem, we summarize the input representations, outcome characteristics, and reachability setup.The Ackley function is a widely used benchmark in the global optimization literature due to its highly multi-modal nature [11].The D-dimensional Ackley function, with x = (x 1 , . . ., x D ), is defined by
f (x) = −a exp   −b 1 D D i=1 x 2 i   − exp 1 D D i=1 cos(cx i ) + a + exp(1),
where a = −20, b = 0.2, and c = 2π.</p>
<p>D.1.2 Rosenbrock</p>
<p>The Rosenbrock function is a widely used benchmark in the global optimization literature that is valley shaped [11].The D-dimensional Rosenbrock function, with x = (x 1 , . . ., x D ), is defined by
f (x) = D−1 i=1 100(x i+1 − x 2 i ) 2 + (1 − x i ) 2 .</p>
<p>D.1.3 Styblinski-Tang</p>
<p>The Styblinski-Tang function is a widely used benchmark in the global optimization literature [11].</p>
<p>The D-dimensional Styblinski-Tang function, with x = (x 1 , . . ., x D ), is defined by
f (x) = 1 2 D i=1 x 4 i − 16x 2 i + 5x i . (a) Ackley4D (b) Rosenbrock4D (c) Styblinski-Tang4D (d) Ackley8D (e) Rosenbrock8D (f) Styblinski-Tang8D (g) Ackley12D (h) Rosenbrock12D (i) Styblinski-Tang12D
Figure 4: Reachability performance results for NS-EA on all synthetic test problems with different population sizes over a budget of 1000 evaluations.</p>
<p>D.1.4 Multi-Output Plus</p>
<p>We constructed our own synthetic function defined over D = 6 inputs, i.e., x = (x 1 , . . ., x D ) with n = 2 outputs, i.e., y = (y 1 , y 2 ).Its is defined as follows
y 1 = sin(x 1 ) cos(x 2 ) + x 3 exp(−x 2 1 ) cos(x 1 + x 2 ) + 0.01 sin(x 4 + x 5 + x 6 ), y 2 = sin(x 4 ) cos(x 5 ) + x 6 exp(−x 2 4 ) cos(x 4 + x 5 ) + 0.01 cos(x 1 + x 2 + x 3
).The distribution of outcomes over 10,000 randomly sampled from the space X = [−5, 5] D is shown in Figure 2 in the main text.</p>
<p>D.2 Materials Discovery Problems</p>
<p>D.2.1 Hydrogen uptake capacity</p>
<p>Hydrogen is a promising alternative clean energy source, however, it remains a challenge to successfully store it due to its low volumetric density.MOFs have been shown to have great promise as a hydrogen gas carrier due to their tunable surface area and porosity [12].We explore the use of NS to identify MOFs with a wide-range of hydrogen uptake capacities.We consider the dataset from [13], which includes 98,000 total unique MOF structures.For this dataset, one can use d = 7 key features to represent the MOFs that are summarized in [13, Table 1] (e.g., density, volumetric surface area, pore volume).The outcome distribution for this case is shown in Figure 6, which exhibits a long tail making it difficult to explore using naive random sampling strategies.The reachability metric is computed over 25 equally-spaced intervals over the outcome space.</p>
<p>D.2.2 Nitrogen uptake capacity</p>
<p>Natural gas, which is primarily composed of methane, is considered a cleaner alternative to fossil fuels due to its lower carbon dioxide emissions during combustion.Methane, however, typically only constitutes 50% to 85% of the natural gas that is produced from biogas processes and landfills [14].</p>
<p>Nitrogen is a significant component of the remaining mixture whose presence decreases the thermal efficiency of natural gas combustion.Consequently, the separation of nitrogen from natural gas has become an important area of research over the past decade [15].MOFs have emerged as a promising material for separating nitrogen from natural gas due to their ability to reduce energy cost compared to traditional methods such as cryogenic distillation.We explore the use of NS in this application using the dataset from [16], which includes 5,224 unique MOF structures.We use the same set of 20 descriptors reported in [16,Table 1] to represent the MOFs.The outcome distribution for this case is shown in Figure 6.The reachability metric is computed over 25 equally-spaced intervals over the outcome space.</p>
<p>D.2.3 Joint carbon dioxide and methane uptake capacity</p>
<p>MOFs are capable of adsorbing both carbon dioxide and methane.Methane is a promising clean energy fuel alternative to fossil fuels; however, its low volumetric energy density poses challenges in Figure 6: Outcome distribution for the single-outcome MOF case studies: hydrogen uptake capacity (left) and nitrogen uptake capacity (right).</p>
<p>storage applications [17].Previous works studied the use of machine learning methods for predicting both methane [18] and carbon dioxide [19] storage capacity.To our knowledge, none of these studies have systematically aimed to identify MOFs that cover different regions of the outcome space.This can be important for ensuring training data is not overly biased to specific regions of the space.We explore the use of NS for this type of application using the dataset provided by [20], which is available online at: https://github.com/byooooo/dispersant_screening_PAL.git.The joint distribution of outcomes for this dataset is shown in Figure 7 in which we see that the distribution of carbon dioxide uptake rates is heavily skewed toward small values.The reachability metric is computed with respect to 100 intervals defined on a 10 × 10 grid over the two outcomes.</p>
<p>Figure 7: Scatter plot of joint outcomes for MOF carbon dioxide and methane uptake capacities for data from [20], with corresponding marginal histograms.Solubility in water is a critical property for drug discovery and design.Machine learning models have been previously explored for solubility prediction [21], however, it is important to note that such models may be trained on data biased toward particular regions of the outcome space.In addition to having a larger spread of solubility values for the purposes of better model training, it can be useful to have solutes with a range of solubility (LogS) values for different applications.Therefore, we consider the use of NS in this application.Specifically, we use the "Water_set_wide" dataset from [22], which contains 900 organic compounds.We use the same 14 features in [22, Table 1].The outcome distribution is shown in Figure 8.The reachability metric is computed over 25 equally-spaced intervals over the outcome space.</p>
<p>D.3.2 ESOL</p>
<p>The ESOL dataset is from [23] and consists of 1,128 organic molecules.Using ideas from GAUCHE [2], we develop a tailored GP model using the following Tanimoto-fragprint covariance function
κ Tanimoto (x, x ′ ) = θ 0 x ⊤ x ′ ∥x∥ 2 + ∥x ′ ∥ 2 − x ⊤ x ′ ,
where x and x ′ are binary vectors corresponding to the 2133-dimensional one-hot encoding representation of the fragprints for every molecule in the dataset and ∥ • ∥ is the Euclidean norm.Note that κ Tanimoto only involves a single hyperparameter θ 0 that is similarly optimized as described in Appendix A.1.The outcome distribution for the ESOL case is shown in Figure 8, which we see is somewhat skewed to the left.The reachability metric is computed over 50 equally-spaced intervals over the outcome space.</p>
<p>D.3.3 LogD</p>
<p>Identifying the physicochemical properties of a candidate molecule is critical in the early stages of drug design.The octanol-water partition distribution coefficient (LogD) stands out as an important indicator as it helps estimate the drug distribution within the human body [24].Traditional ways to measurement LogD are known to be difficult and time consuming, which has led to an increase in developing machine learning-based models to predict LogD for untested molecules [25,26].As mentioned previously, it is important to have nicely distributed data in the outcome space to reduce the chance of overfitting (leading to bias in the model predictions).One way to achieve this spread is using NS, though it is also useful for exploring new regions of the outcome space.We use the dataset from [27] containing 2,070 molecules characterized by 125 descriptors.Since this is a high-dimensional problem, we resort to the SAAS function prior [1] that is based on the assumption that the descriptors exhibit a hierarchy of relevance when it comes to outcome prediction.SAAS has recently been shown to be effective at dealing with such high-dimensional molecular feature representations [28].The complete outcome distribution for the LogD case is shown in Figure 8.The reachability metric is computed over 25 equally-spaced intervals over the outcome space.</p>
<p>D.4 Maze Navigation Problem</p>
<p>We consider a reinforcement learning (RL) task involving navigation through a large maze environment from the OpenAI Gymnasium suite [29], specifically the PointMaze-Large environment described at https://minari.farama.org/datasets/pointmaze/large/.The objective is to control a ball (green) from a fixed start position to a target goal (red) within 300 simulation steps.The policy is linear, defined by 8 tunable parameters mapping state observations to continuous action outputs.Figure 9 shows the maze layout used in our experiments.The reward function measures relative improvement in proximity to the goal: Reward = initial distance from the target − final distance from the target initial distance from the target .</p>
<p>A perfect reward of 1 indicates successful arrival at the target.This task is designed to reflect a highly deceptive objective landscape, where small gains in final distance do not always reflect useful behavioral progress.In addition to the previous algorithms, we also consider the classical expected improvement (EI) algorithm [30] under a GP model over the reward function.</p>
<p>E Details and Additional Results for UG-BEACON</p>
<p>This section provides further details on UG-BEACON, a user-guided extension of BEACON designed to incorporate user preferences or behavioral constraints during exploration.We first give a brief technical description of the algorithm and then provide implementation details for two case studies: handwritten digit discovery using MNIST [31] and a materials discovery task for oil sorbents.We also present results for the oil sorbent case study, which was excluded from the main text due to space constraints.</p>
<p>E.1 Technical Description</p>
<p>UG-BEACON builds directly on the BEACON algorithm with a simple but effective modification to incorporate user-defined behavior-level constraints.Specifically, recall that BEACON selects the next evaluation point by maximizing the novelty-based acquisition function:
x * = argmax x∈X α NS (x|g, D),
where α NS (x|g, D) denotes the novelty score computed using a posterior sample g of the outcome function and currently available data D.</p>
<p>UG-BEACON introduces a filtering step to exclude candidates whose predicted outcomes fall within previously explored or user-restricted regions of the behavior space.Let G = {G 1 , . . ., G M } denote a user-defined partition of the outcome space into M non-overlapping regions.This partition can reflect application-specific preferences (e.g., fine-grained in important regions or coarse in irrelevant ones).Let G visited ⊆ G denote the set of regions containing outcomes previously observed during the search (identified by the user).The feasible set of candidates is then given by:
X feas = x ∈ X g(x) / ∈ G∈Gvisited G ,
and the next evaluation point is chosen as:
x * = argmax x α NS (x|g, D) subject to x ∈ X feas .
This mechanism enables UG-BEACON to avoid redundant sampling and to steer exploration toward novel or user-relevant behavior regions.All other components of the BEACON algorithm, including posterior sampling, surrogate modeling, and acquisition computation, remain unchanged.</p>
<p>E.2 Discovering Handwritten Digits using MNIST Images</p>
<p>We consider a task in which the goal is to discover as many distinct digits (0 through 9) as possible from the MNIST dataset [31] using latent space sampling.We train a variational autoencoder (VAE) on 10,000 grayscale MNIST images (downsampled to 14 × 14 resolution), resulting in an 8-dimensional latent space Z. From this space, 2,000 latent vectors are randomly sampled and form the discrete candidate set for exploration.The decoder is treated as a fully black-box model -all we are able to do is pass selected latent vectors z ∈ Z to obtain new images.</p>
<p>Each sampled latent vector z ∈ Z is decoded by the VAE to generate a synthetic image, which is then passed to a convolutional neural network (CNN) classifier trained to over 98% validation accuracy.This CNN acts as the user that can decide if previous digits have been identified or not.A digit is said to be discovered if the classifier assigns a class label with confidence greater than 99% and the corresponding digit class has not been previously observed.The set of visited outcome regions G visited thus corresponds to the set of discovered digit classes.</p>
<p>UG-BEACON uses this feedback to exclude candidates predicted to map to already observed digit classes.This setup effectively demonstrates the use of UG-BEACON for discrete behavior constraints over a finite set of categories.</p>
<p>E.3 Oil Sorbent Material Case Study</p>
<p>As a second demonstration of UG-BEACON, we consider the discovery of electrospun polystyrene/polyacrylonitrile (PS/PAN) materials for oil adsorption applications.These materials have attracted interest due to their high adsorption capacity and mechanical stability, which are critical for real-world deployment.Following the empirical modeling approach of [32], we evaluate candidate materials using two key properties: oil adsorption capacity and mechanical strength.Both quantities are computed using surrogate equations fitted to experimental data.</p>
<p>To reflect realistic application preferences, we define a non-uniform partition of the 2D outcome space based on target utility.As shown in Figure 10 (left), outcome regions with high strength and adsorption capacity are assigned finer grid resolution to encourage coverage in practically useful zones.In contrast, low-performance regions are coarsely partitioned, reducing sampling pressure in less relevant areas.UG-BEACON uses this user-defined grid to guide novelty evaluation and prevent redundant sampling by avoiding outcome cells already explored.</p>
<p>We initialize the search with 10 random samples and use a MOGP surrogate model for posterior sampling and acquisition.Figure 10 (right) reports reachability performance across algorithms.UG-BEACON rapidly attains full (100%) coverage of the preferred outcome space within just 50 iterations, outperforming BEACON and other state-of-the-art novelty and reward-driven baselines.These results demonstrate the value of incorporating user behavior preferences into the NS process to accelerate discovery in practical material design tasks.</p>
<p>F Scaling BEACON to Large Evaluation Budgets</p>
<p>The computational cost of BEACON is mostly dominated by the acquisition function optimization step, which requires repeated inference with the underlying GP model of the outcome function.It is well-known that the exact GP posterior equations (1) exhibit O(N 3 ) scaling due to the inversion of the covariance matrix [33].Although not expected to be a concern for reasonably small N (on the order of hundreds), this can become a challenge when N is on the order of thousands or more.Even though such larger budgets are not expected for all types of expensive systems, there are relevant cases where this could occur such as when the system is modeled by a high-fidelity simulator that can be easily parallelized.Fortunately, there has been a substantial amount of work on sparse approximate GP models that can be leveraged in such cases.To demonstrate this fact, we perform an experiment with the sparse variational Gaussian process (SVGP) described in [34].We consider a 20-dimensional version of the Rosenbrock problem where 300 initial data points are available.We use the SVGP implementation available at https://github.com/secondmind-labs/trieste,with a Matérn 5/2 covariance function and 100 inducing points allocated using the k-means approach in [34].Note that the trieste package is built on top of GPflow [35].The reachability performance results for BEACON using SVGP over 1000 evaluations for the 20d Rosenbrock problem is shown in Figure 11.</p>
<p>The reachability metric is computed over 100 equally-spaced intervals over the outcome space.We see that BEACON achieves dramatically better results than all other methods including finding nearly 3x more diverse behaviors than the standard NS-EA method.It is worth noting that the use of the exact GP within BEACON already begins to considerably slow down at this scale, suggesting that the SVGP provides a nice balance between accuracy and computational cost.</p>
<p>To further investigate the improvements that can be gained in terms of execution time, we perform another experiment involving optimization of our proposed novelty-based acquisition function (equation (3) in main paper) defined using both exact GP and SVGP models given different amounts of training data.The CPU time to optimize the acquisition function using the Adam solver over 100 iterations (averaged over 10 randomly drawn datasets) is shown in Figure 12.We see that the SVGP exhibits much better scaling than the exact GP, with the CPU time for 5000 points with the SVGP being less than that of 1000 points with the exact GP.These times could be further improved by taking advantage of GPUs, however, the key takeaway is that SVGPs provide an effective path toward efficient NS on large evaluation budgets.</p>
<p>Figure 1 :
1
Figure 1: Visual illustration of BEACON applied to a 1D test problem.Top: the true function (black), GP posterior mean (blue dashed), and a TS (green dashed) at iterations 1, 5, and 9. Shaded regions show posterior uncertainty, and the red star marks the selected query point.Bottom: behavioral bins defined over the outcome space.Blue bands denote previously observed outcomes; the red band shows the predicted outcome bin of the next query.As BEACON progresses, it efficiently reduces the behavior gap BG t by identifying unexplored regions.</p>
<p>Figure 2 :
2
Figure 2: Results for multi-output cross plus function.(Top) Scatter plot of outcomes for 10,000 random input values.(Bottom) Reachability performance for all algorithms.</p>
<p>Figure 3 :
3
Figure 3: Results on synthetic test problems for Ackley (Left column), Rosenbrock (Middle column), and Styblinski-Tang (Right column) with 4 (Top row), 8 (Middle row), and 12 (Bottom row) dimensions.BEACON provides substantially better outcome reachability over the benchmark methods, with larger improvements for higher-dimensional problems.</p>
<p>Figure 4 :
4
Figure 4: Performance of BEACON and baseline methods on MOF discovery problems.Shown are results for hydrogen uptake (left), nitrogen uptake (middle), and joint CO 2 /CH 4 uptake (right).</p>
<p>Figure 5 :
5
Figure 5: Performance of BEACON and baseline methods on drug discovery problems.Shown are results for water solubility (left), ESOL (middle), and LogD (right).BEACON achieves superior reachability across all cases, particularly in high-dimensional settings.</p>
<p>Figure 6 :
6
Figure 6: Results for RL-based maze navigation case study.(Top) Average best observed reward for all algorithms across 20 replicates.(Bottom) Violin plot showing the distribution of best reward values at the final iteration for all algorithms.</p>
<p>ure 7 ,
7
UG-BEACON achieves faster discovery of all digit classes compared to BEACON and baseline methods.We also study UG-BEACON on a second task involving the discovery of oil sorbent materials under user-defined outcome preferences.Complete experimental details and results for both case studies are provided in Appendix E.</p>
<p>Figure 7 :
7
Figure 7: Results for MNIST case study.(Top) Average number of discovered digits for all algorithms across 10 replicates.(Bottom) Violin plot showing distribution of discovered digits at the final iteration for all algorithms.UG-BEACON achieves faster convergence by incorporating user-defined behavior constraints on observed digits.</p>
<p>Figure 1 :
1
Figure 1: Reachability performance results on (upper row) Ackley-4D and (lower row) Rosenbrock-4D for different grid resolutions.</p>
<p>Figure 2 :
2
Figure 2: Reachability performance results for BEACON on 12d Ackley (top), Rosenbrock (middle), and Styblinski-Tang (bottom) for different nearest neighbor values k.</p>
<p>Figure 3 :
3
Figure 3: Reachability performance results on 4d Ackley for BEACON (Algorithm 1 in main text) and BEACON-noiseless (variant of Algorithm 1 that treats the observations as noise-free) in the presence of noisy observations for different standard deviation noise levels.</p>
<p>Figure 5 :
5
Figure 5: Reachability performance results for NS-EA on all synthetic test problems with different offspring sizes over a budget of 1000 evaluations.</p>
<p>Figure 8 :
8
Figure 8: Outcome distribution for the small-molecule organic solufbility case studies relevant to drug discovery applications: logS (left), ESOL (middle), and logD (right).</p>
<p>Figure 9 :
9
Figure 9: Maze layout for the RL task.The green ball indicates the start location and the red ball marks the target.</p>
<p>Figure 10 :
10
Figure 10: Oil sorbent material case study.(Left) Partitioned 2D outcome space with user-defined behavior preferences.(Right) Reachability performance for UG-BEACON and other algorithms.UG-BEACON achieves 100% reachability within significantly fewer iterations than other methods.</p>
<p>Figure 11 :
11
Figure 11: Reachability performance results on the 20-dimensional Rosenbrock problem for BEA-CON using SVGP and a subset of other algorithms over a query budget of 1000.</p>
<p>Figure 12 :
12
Figure 12: CPU time required to run a single gradient-based optimization procedure for our proposed acquisition function in BEACON for the exact GP (orange) and SVGP (blue) for different number of training data points.These times were computed as the average of 10 replicate datasets drawn uniformly at random on the 20d Rosenbrock problem.</p>
<p>Table 1 :
1
Average runtimes per iteration in seconds to three decimal places for the considered NS methods on the synthetic test problems.whereeachoutcome evaluation takes several minutes or more.For example, in material and drug discovery/design applications, it is not uncommon for an evaluation to take multiple hours to days (through high-fidelity simulations or experiments).It is also worth noting that BEACON requires less cost compared to MaxVar; the major cost of both methods is the training of the GP model and optimization of the acquisition function.This highlights the importance of the choice of acquisition function (as BEACON consistently outperforms MaxVar) and the computational efficiency of our proposed α NS (in equation (3) of the main paper) due to efficient TS and the simple sort formulation.
Ackley4D Ackley8D Ackley12D Rosenbrock4D Rosenbrock8D Rosenbrock12D Styblinski-Tang4D Styblinski-Tang8D Styblinski-Tang12DBEACON MaxVar NS-EA NS-DEA NS-FS Sobol 0.161 0.142 0.001 0.001 0.001 0.000 0.000 RS 0.233 0.339 0.001 0.001 0.001 0.000 0.000 0.336 0.777 0.001 0.001 0.001 0.000 0.000 0.437 0.804 0.001 0.001 0.001 0.000 0.000 0.703 1.180 0.001 0.001 0.001 0.000 0.000 1.018 1.293 0.001 0.001 0.001 0.000 0.000 0.228 0.250 0.001 0.001 0.001 0.000 0.000 0.540 0.845 0.001 0.001 0.001 0.000 0.000 1.048 0.916 0.001 0.001 0.001 0.000 0.000especially for problems
A Implementation Details for BEACON and Baseline MethodsThis appendix provides implementation details for BEACON and the baseline algorithms used in our experiments.We also report average runtimes for all methods across synthetic test problems to assess computational efficiency.A.1 BEACONAll MOGPs in our experiments have a covariance function κ((x, j), (x ′ , j ′ )) = δ jj ′ κ j (x, x ′ ) where δ jj ′ is the Kronecker delta, which implies all outputs are modeled independently.For the synthetic experiments, the GPs have a constant mean function and a standard Radial Basis Function (RBF) covariance function of the form:where Θ = diag(ℓ 1 , . . ., ℓ d ) with ℓ i denoting the lengthscale parameter for the ith dimension and θ 0 is an output scale parameter.For the real-world MOF and water solubility case studies, we replace the RBF kernel with a standard Matérn 5/2 covariance function.For the ESOL and logD case studies, we use a SAAS function prior[1]and a Tanimoto-fragprint covariance function[2], respectively, to account for their high-dimensional nature.We estimate the GP hyperparameters (and noise variance σ 2 ) using the fit_gpytorch_mll function in BoTorch<a href="corresponds to maximum likelihood estimation">3</a> using standard settings.For problems with continuous X , we use the efficient Thompson sampling (TS) method in[4]that yields a high-accuracy continuously differentiable approximation of g ∼ f |D.We then develop a PyTorch-based implementation of α NS in equation (3) of the main paper that we maximize using the SciPy implementation of L-BFGS-B[5]over a random set of multi-start initial conditions, as done in BoTorch's optimize_acqf function.For problems with discrete X , we simply exhaustively evaluate the TS at all candidates x ∈ X and find the one that leads to the largest α NS .A.2 NS-EANS-EA refers to a standard evolutionary algorithm for NS applied to the novelty metric in equation (1) of the main paper that was originally proposed in[6].We follow the implementation of NS-EA described in[7]that starts with an initial population of size n pop and then evolves each generation by randomly mutating the existing genomes.The algorithm selectively retains only the most novel genomes from the current population and the offspring, maintaining a constant population size throughout the evolutionary process.The specific Python implementation of NS-EA that we use is available at: https://github.com/alaflaquiere/simple-ns.git.A.3 NS-DEANS-DEA refers to a modified version of NS-EA from[7]that replaces the novelty metric in equation (1) of the main paper with a new metric 'Distance to Explored Area' (DEA) that measures the distance between an individual's outcome and the convex hull of previously sampled outcomes.The central idea behind NS-DEA is that we want to sample points clearly outside of the connected region of behaviors that have already been observed.Similarly to NS-EA, we use the Python implementation of NS-DEA available at: https://github.com/alaflaquiere/simple-ns.git.A.4 NS-FSWe propose a simple variant of NS-EA, referred to as NS-FS, that modifies the novelty metric in equation (1) of the main paper to be over the input space instead of outcome space.The acquisition function for this case is defined aswhere dist X (•) denotes a distance function that operates over the input space X and {x ⋆⋆ 1 , . . ., x ⋆⋆ k } denotes the top k nearest neighbors to x as measured by dist X (•).This metric incentives exploration of new regions of the input space, however, it is independent of the observed outcome values and so we expect it to perform worse than outcome-based alternative novelty metrics.Since α NS-FS is only defined in terms of the input, it can easily be maximized for continuous x ∈ X using gradient-based optimization algorithms.A.5 MaxVarThe MaxVar algorithm is an approach commonly utilized in the field of active learning that aims to explore regions of the sample space whose predictions are most uncertain according to some model.This approach has also been explored in the context of BO[8].Here, we take a standard MaxVar approach defined in terms of the same MOGP model used by BEACON, but replaces the acquisition function by the sum of the diagonal of the predicted posterior covariance matrix, i.e., α MaxVar (x|D) = tr(Σ D (x)), where Σ D (x) = κ D (x, x) and tr(•) denotes the trace operator.The logic behind using MaxVar for NS tasks is that, once we have constructed a globally accurate (cheap) surrogate model, we can easily identify x ∈ X that lead to new behaviors.Although sufficient for NS, it is not necessary to learn such an accurate model of f to find new behaviors, which is the key advantage of BEACON.A.6 SobolSobol refers to the simple algorithm in which one draws a sequence of quasi-random low-discrepancy Sobol points from the input domain X[9]to generate the points {x 1 , x 2 , . . ., x T }.Sobol sampling is a commonly used baseline method in the global optimization literature since it is guaranteed to densely sample X in the limit of infinite experimental budgets T → ∞.A.7 RSRS refers to the simple algorithm in which one independently draws a sequence of points from a uniform distribution defined over the input domain X to generate points {x 1 , x 2 , . . ., x T }.Similarly to Sobol, RS is able to densely sample X in the limit of infinite experimental budgets T → ∞.A.8 Runtimes and LicensesThe average runtimes of the considered NS methods for the synthetic problems are summarized in Table1.These times were computed by running the algorithms on a CPU with an Intel(R) Core(TM) i7-10700K CPU 3.8 GHz CPU processor.Although BEACON is more expensive than the other standard NS methods, the average runtime per iteration remains less than 1 second.This additional computation is more than compensated for by BEACON's significantly improved performance,
Noregret Bayesian optimization with unknown hyperparameters. F Berkenkamp, A P Schoellig, A Krause, Journal of Machine Learning Research. 20502019</p>
<p>. S Boobier, D R Hose, A J Blacker, B Nguyen, </p>
<p>Machine learning with physicochemical relationships: solubility prediction in organic solvents and water. Nature Communications. 1115753</p>
<p>A limited memory algorithm for bound constrained optimization. R H Byrd, P Lu, J Nocedal, C Zhu, SIAM Journal on Scientific Computing. 1651995</p>
<p>Combining machine learning and molecular simulations to unlock gas separation potentials of MOF membranes and MOF/polymer MMMs. S R Chowdhury, A Gopalan, Pmlr, H Daglar, S Keskin, International Conference on Machine Learning. 2017. 202214On kernelized multi-armed bandits</p>
<p>Differentiable expected hypervolume improvement for parallel multi-objective Bayesian optimization. S Daulton, M Balandat, E Bakshy, Advances in Neural Information Processing Systems. 202033</p>
<p>ESOL: Estimating aqueous solubility directly from molecular structure. J S Delaney, Journal of Chemical Information and Computer Sciences. 4432004</p>
<p>The MINST database of handwritten digit images for machine learning research. L Deng, 2012best of the web</p>
<p>. IEEE Signal Processing Magazine. 296</p>
<p>Novelty search makes evolvability inevitable. S Doncieux, G Paolo, A Laflaquière, A Coninx, Proceedings of the 2020 Genetic and Evolutionary Computation Conference. the 2020 Genetic and Evolutionary Computation Conference2020</p>
<p>High-dimensional Bayesian optimization with sparse axis-aligned subspaces. D Eriksson, M Jankowiak, Uncertainty in Artificial Intelligence. 2021PMLR</p>
<p>P Frazier, arXiv:1807.02811A tutorial on Bayesian optimization. 2018arXiv preprint</p>
<p>Bayesian optimization with inequality constraints. J R Gardner, M J Kusner, Z E Xu, K Q Weinberger, J P Cunningham, International Conference on Machine Learning. 20142014</p>
<p>Exploring Hydrogen Storage Capacity in Metal-Organic Frameworks: A Bayesian Optimization Approach. S Ghude, C Chowdhury, Chemistry-A European Journal. 2969e2023018402023</p>
<p>Devising effective novelty search algorithms: A comprehensive empirical study. J Gomes, P Mariano, A L Christensen, Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation. the 2015 Annual Conference on Genetic and Evolutionary Computation2015</p>
<p>A curious formulation robot enables the discovery of a novel protocell behavior. R.-R Griffiths, L Klarner, H Moss, A Ravuri, S Truong, Y Du, S Stanton, G Tom, B Rankovic, A Jamasb, Advances in Neural Information Processing Systems. 2024. 2020364237GAUCHE: A library for Gaussian processes in chemistry</p>
<p>Novelty search for deep reinforcement learning policy network weights by action sequence edit metric distance. E C Jackson, M Daley, Proceedings of the Genetic and Evolutionary Computation Conference Companion. the Genetic and Evolutionary Computation Conference Companion2019</p>
<p>Robust Bayesian optimization for flexibility analysis of expensive simulation-based models with rigorous uncertainty bounds. K Kandasamy, A Krishnamurthy, J Schneider, B Póczos, International Conference on Artificial Intelligence and Statistics. Pmlr, A Kudva, W.-T Tang, J A Paulson, 2018181108515Parallelised Bayesian optimisation via Thompson sampling</p>
<p>Computational screening of trillions of metalorganic frameworks for high-performance methane storage. S Lee, B Kim, H Cho, H Lee, S Y Lee, E S Cho, J Kim, ACS Applied Materials &amp; Interfaces. 13202021</p>
<p>Abandoning objectives: Evolution through the search for novelty alone. J Lehman, K O Stanley, Evolutionary Computation. 1922011a</p>
<p>Evolving a diversity of virtual creatures through novelty search and local competition. J Lehman, K O Stanley, Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation. the 13th Annual Conference on Genetic and Evolutionary Computation2011b</p>
<p>Novelty search and the problem with objectives. J Lehman, K O Stanley, Genetic Programming Theory and Practice IX. 2011c</p>
<p>Remarks on multioutput Gaussian process regression. Knowledge-Based Systems. H Liu, J Cai, Y.-S Ong, 2018144</p>
<p>Understanding the diversity of the metal-organic framework ecosystem. N Maus, K Wu, D Eriksson, J Gardner, J Mockus, S M Moosavi, A Nandy, K M Jablonka, D Ongari, J P Janet, P G Boyd, Y Lee, B Smit, H J Kulik, arXiv:2210.10953Proceedings of the IFIP Technical Conference. the IFIP Technical ConferenceMouret, J.-B2022. 1974. 2020. 201111arXiv preprintNew Horizons in Evolutionary Robotics. Extended Contributions from the 2009 EvoDeRob Workshop</p>
<p>Softsort: A continuous relaxation for the argsort operator. Springer, J.-B Mouret, J Clune, S Prillo, J Eisenschlos, Pmlr, S Risi, C E Hughes, K O Stanley, arXiv:1504.04909Illuminating search spaces by mapping elites. 2015. 2020. 201018arXiv preprintEvolving plastic neural networks with novelty search</p>
<p>Taking the human out of the loop: A review of Bayesian optimization. B Shahriari, K Swersky, Z Wang, R P Adams, N Freitas, Proceedings of the IEEE. 10412015</p>
<p>Pushing property limits in materials discovery via boundless objective-free exploration. J Snoek, H Larochelle, R P Adams, K Terayama, M Sumita, R Tamura, D T Payne, M K Chahal, S Ishihara, K Tsuda, Advances in Neural Information Processing Systems. 2012. 202011Practical Bayesian optimization of machine learning algorithms</p>
<p>On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. W R Thompson, Biometrika. 253-41933</p>
<p>Gaussian Processes for Machine Learning. C K Williams, C E Rasmussen, 2006MIT Press2Cambridge, MA</p>
<p>Using machine learning to predict partition coefficient (Log P) and distribution coefficient (Log D) with molecular descriptors and liquid chromatography retention time. J Wilson, V Borovitskiy, A Terenin, P Mostowsky, M Deisenroth, A M Cheong, W S Hopkins, International Conference on Machine Learning. 2020. 202363PMLR. Win, Z.-M</p>
<p>Lyapunov exponents for a Duffing oscillator. A R Zeni, J A Gallas, Physica D: Nonlinear Phenomena. 891-21995</p>
<p>Single-step Bayesian search method for an extremum of functions of a single variable. A Zhilinskas, Cybernetics. 1111975References</p>
<p>High-dimensional Bayesian optimization with sparse axis-aligned subspaces. David Eriksson, Martin Jankowiak, Uncertainty in Artificial Intelligence. PMLR2021</p>
<p>GAUCHE: A library for Gaussian processes in chemistry. Ryan-Rhys Griffiths, Leo Klarner, Henry Moss, Aditya Ravuri, Sang Truong, Yuanqi Du, Samuel Stanton, Gary Tom, Bojana Rankovic, Arian Jamasb, Advances in Neural Information Processing Systems. 202436</p>
<p>BoTorch: A framework for efficient Monte-Carlo Bayesian optimization. Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, Eytan Bakshy, Advances in Neural Information Processing Systems. 202033</p>
<p>Efficiently sampling functions from Gaussian process posteriors. James Wilson, Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, Marc Deisenroth, International Conference on Machine Learning. PMLR2020</p>
<p>A limited memory algorithm for bound constrained optimization. Peihuang Richard H Byrd, Jorge Lu, Ciyou Nocedal, Zhu, SIAM Journal on Scientific Computing. 1651995</p>
<p>Novelty search and the problem with objectives. Genetic Programming Theory and Practice IX. Joel Lehman, Kenneth O Stanley, 2011</p>
<p>Alban Laflaquière, and Alexandre Coninx. Novelty search makes evolvability inevitable. Stephane Doncieux, Giuseppe Paolo, Proceedings of the 2020 Genetic and Evolutionary Computation Conference. the 2020 Genetic and Evolutionary Computation Conference2020</p>
<p>Gaussian process optimization in the bandit setting: No regret and experimental design. Niranjan Srinivas, Andreas Krause, Matthias Sham M Kakade, Seeger, arXiv:0912.39952009arXiv preprint</p>
<p>On the distribution of points in a cube and the approximate evaluation of integrals. ' Il, Meerovich Sobol, Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki. 741967</p>
<p>Remarks on multi-output Gaussian process regression. Knowledge-Based Systems. Haitao Liu, Jianfei Cai, Yew-Soon Ong, 2018144</p>
<p>A literature survey of benchmark functions for global optimisation problems. Momin Jamil, Xin-She Yang, International Journal of Mathematical Modelling and Numerical Optimisation. 422013</p>
<p>A review on current trends in potential use of metal-organic framework for hydrogen storage. Sachin P Shet, Shanmuga Priya, Muhammad Sudhakar, Tahir, International Journal of Hydrogen Energy. 46212021</p>
<p>Exploring hydrogen storage capacity in metalorganic frameworks: A Bayesian optimization approach. Sumedh Ghude, Chandra Chowdhury, Chemistry-A European Journal. 2969e2023018402023</p>
<p>Adsorption of carbon dioxide, methane and nitrogen on an ultramicroporous copper metal-organic framework. Xiaofei Wu, Bin Yuan, Zongbi Bao, Shuguang Deng, Journal of Colloid and Interface Science. 4302014</p>
<p>Adsorption isotherm predictions for multiple molecules in MOFs using the same deep learning model. Ryther Anderson, Achay Biong, Diego A Gómez-Gualdrón, Journal of Chemical Theory and Computation. 1622020</p>
<p>Combining machine learning and molecular simulations to unlock gas separation potentials of MOF membranes and MOF/polymer MMMs. Hilal Daglar, Seda Keskin, ACS Applied Materials &amp; Interfaces. 14282022</p>
<p>Methane storage in metal-organic frameworks. Yabing He, Wei Zhou, Guodong Qian, Banglin Chen, Chemical Society Reviews. 43162014</p>
<p>Understanding quantitative relationship between methane storage capacities and characteristic properties of metal-organic frameworks based on machine learning. Xuanjun Wu, Sichen Xiang, Jiaqi Su, Weiquan Cai, The Journal of Physical Chemistry C. 123142019</p>
<p>Rapid and accurate machine learning recognition of high performing metal organic frameworks for CO2 capture. Michael Fernandez, Peter G Boyd, Thomas D Daff, Mohammad Zein Aghaji, Tom K Woo, The Journal of Physical Chemistry Letters. 5172014</p>
<p>Understanding the diversity of the metal-organic framework ecosystem. Mohamad Seyed, Aditya Moosavi, Kevin Maik Nandy, Daniele Jablonka, Jon Paul Ongari, Peter G Janet, Yongjin Boyd, Berend Lee, Heather J Smit, Kulik, Nature Communications. 1112020</p>
<p>Deep architectures and deep learning in chemoinformatics: The prediction of aqueous solubility for drug-like molecules. Alessandro Lusci, Gianluca Pollastri, Pierre Baldi, Journal of Chemical Information and Modeling. 5372013</p>
<p>Machine learning with physicochemical relationships: solubility prediction in organic solvents and water. Samuel Boobier, John David Rj Hose, Bao N Blacker, Nguyen, Nature Communications. 11157532020</p>
<p>ESOL: Estimating aqueous solubility directly from molecular structure. Delaney John, Journal of Chemical Information and Computer Sciences. 4432004</p>
<p>Advances in computational structure-based drug design and application in drug discovery. Tao Wang, Mian-Bin Wu, Ri-Hao Zhang, Zheng-Jie Chen, Chen Hua, Jian-Ping Lin, Li-Rong Yang, Current Topics in Medicinal Chemistry. 1692016</p>
<p>Dimensionally reduced machine learning model for predicting single component octanol-water partition coefficients. Randy C David H Kenney, Michael T Paffenroth, Andrew R Timko, Teixeira, Journal of Cheminformatics. 15192023</p>
<p>Exploring the octanol-water partition coefficient dataset using deep learning techniques and data augmentation. Nadin Ulrich, Kai-Uwe Goss, Andrea Ebert, Communications Chemistry. 41902021</p>
<p>Using machine learning to predict partition coefficient (Log P) and distribution coefficient (Log D) with molecular descriptors and liquid chromatography retention time. Zaw-Myo Win, Allen My Cheong, Scott Hopkins, Journal of Chemical Information and Modeling. 6372023</p>
<p>Accelerating black-box molecular property optimization by adaptively learning sparse subspaces. Farshud Sorourifar, Thomas Banker, Joel A Paulson, arXiv:2401.013982024arXiv preprint</p>
<p>. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, arXiv:1606.015402016Openai gym. arXiv preprint</p>
<p>Efficient global optimization of expensive black-box functions. Matthias Donald R Jones, William J Schonlau, Welch, Journal of Global Optimization. 131998</p>
<p>The MINST database of handwritten digit images for machine learning research. Li Deng, IEEE Signal Processing Magazine. 2962012best of the web</p>
<p>Harnessing a novel machine-learning-assisted evolutionary algorithm to co-optimize three characteristics of an electrospun oil sorbent. Boqian Wang, Jiacheng Cai, Chuangui Liu, Jian Yang, Xianting Ding, ACS Applied Materials &amp; Interfaces. 12382020</p>
<p>Gpytorch: Blackbox matrix-matrix Gaussian process inference with GPU acceleration. Jacob Gardner, Geoff Pleiss, Q Kilian, David Weinberger, Andrew G Bindel, Wilson, Advances in Neural Information Processing Systems. 312018</p>
<p>Scalable Thompson sampling using sparse Gaussian process models. Sattar Vakili, Henry Moss, Artem Artemev, Vincent Dutordoir, Victor Picheny, Advances in Neural Information Processing Systems. 202134</p>
<p>GPflow: A Gaussian process library using TensorFlow. Mark Alexander G De G Matthews, Van Der, Tom Wilk, Keisuke Nickson, Alexis Fujii, Pablo Boukouvalas, Zoubin Le, James Ghahramani, Hensman, Journal of Machine Learning Research. 18402017</p>            </div>
        </div>

    </div>
</body>
</html>
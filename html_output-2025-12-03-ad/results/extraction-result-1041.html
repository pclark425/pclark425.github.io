<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1041 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1041</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1041</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-268531207</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.12014v2.pdf" target="_blank">EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents</a></p>
                <p><strong>Paper Abstract:</strong> Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. We first prompt an LLM to generate training environments by giving it the task description and simulator objectives that the agents should learn and then asking it to generate a set of environment configurations (e.g., different terrains, items initially given to agents, etc.). Next, we train a small RL agent in a mixture of the original and LLM-generated environments. Then, we enable the LLM to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the LLM in the form of the agent's performance. We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist environments. We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster. We also show that using an LLM to adapt environments dynamically outperforms curriculum learning approaches and how the environments are adapted to help improve RL agents' weaker skills over time. Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require thousands of calls. Lastly, we present detailed ablation studies for EnvGen design choices.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1041.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1041.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPO+EnvGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PPO agent trained with EnvGen (adaptive LLM-generated environments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight convolutional PPO-based embodied agent (<5M parameters) trained using EnvGen: LLM-generated and adaptively updated training environments to teach multiple skills in parallel and focus on agent weak points via feedback-driven environment updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO + EnvGen (Ours)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simulated visual RL agent (CNN + linear head) trained with Proximal Policy Optimization (PPO). Training alternates between multiple LLM-generated custom environments and the original environment; after each cycle the agent's measured task success rates are sent to an LLM which adaptively updates the training environments to target weak skills.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Crafter (primary); also evaluated on Heist for generalization</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Crafter: open-world 2D survival game with 22 hierarchical achievements (some short-horizon, many long-horizon requiring chains of prerequisites). Environments vary by biome (grassland/mountain/beaches/natural), resource rarities (coal/iron/diamond/tree rarity), and agent starting inventory (items/pickaxes/swords/saplings). Heist: procedural maze with layered color-coded locks to reach a gem.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Crafter complexity characterized by: number of hierarchical achievements (22), long-horizon task depth (many achievements require multiple prerequisite achievements, e.g., make iron pickaxe requires many prerequisite steps), number of steps required to unlock higher-order achievements (reported unlock times and chains); total training interactions (steps) reported (e.g., 1.96M total steps in their protocol).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (Crafter: many long-horizon hierarchical tasks with deep prerequisite chains)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Environment variation controlled via LLM-generated configuration parameters (biome, resource rarity, starting inventory). Quantified in experiments by: number of distinct LLM-generated environments used (1,2,4,8 tested; default=4), number of adaptive cycles N_Cycle (default=4), update frequency (every 0.12M steps default), and ratio of LLM-env steps to original-env steps (tested ratios; default 1:1).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high (default setup uses 4 distinct LLM environments adapted across 4 cycles; parameters' ranges allow high procedural variation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Crafter score (geometric mean of success rates across 22 achievements), episode reward, individual achievement success rates, achievement unlock time (first completion time). For Heist: average success rate over test episodes and reward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Crafter: Score = 32.2% ± 0.6; Reward = 12.6 ± 0.6. Example task-level baselines from original env before adaptation: collect_coal 38% ± 6%, defeat_skeleton 10% ± 4%, make_stone_pickaxe 31% ± 3%. Heist: with EnvGen, score improved to 37.7% ± 7.5 and reward to 5.5 ± 0.9 (from 25.9% ± 13.2 and 4.1 ± 1.8 respectively for PPO baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper demonstrates that adaptively increasing environment variation (via LLM-generated and updated environments targeted to weak skills) helps learning in high-complexity domains. Key relationships/trade-offs: (1) Adaptive varied environments outperform fixed LLM environments and standard curricula (adaptive: 32.2% vs fixed 29.9% and easy-to-hard/adversarial 26.8%); (2) simply creating more difficult environments is not sufficient — adaptive targeting of weak skills is more effective; (3) update frequency and number of distinct LLM environments matter (default 4 envs, update every 0.12M steps worked best); (4) mixture ratio between LLM env training and original environment matters (1:1 default gave best results). The authors also highlight computational trade-offs: EnvGen uses few LLM calls (≈4) making it far cheaper than per-step LLM agents but still obtains better or competitive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>15.5% ± 0.6 (PPO baseline trained solely in original Crafter environment; represents high complexity with low variation)</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>32.2% ± 0.6 (PPO evaluated in original Crafter after training with varied/adaptive LLM-generated environments — high complexity target environment, high variation during training)</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Adaptive curriculum via LLM-generated environments (EnvGen): generate multiple varied training environments, alternate training between LLM envs and original env to mitigate overfitting, measure task success in original env and provide feedback to LLM to adapt next-cycle environments. Underlying learner uses PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>EnvGen trained agents generalize to the original (unmodified) Crafter environment with improved success on long-horizon achievements; EnvGen also generalized to Heist where PPO+EnvGen increased average success from 25.9% to 37.7% and reduced variance, demonstrating transfer of skills learned in LLM-generated environments to a distinct task domain.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Total training in main Crafter experiments: 1.96M environment steps ((0.12M LLM-env + 0.12M orig-env) × 4 cycles + 1M final Crafter steps). EnvGen uses very few LLM calls (~4) in total. Authors contrast to baselines needing far more pretraining steps (e.g., MuZero+SPR 150M) or thousands-to-millions of LLM calls (SPRING ~2.7K per episode, ELLM ~5M).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adaptive, LLM-generated training environments (EnvGen) allow a small PPO agent to learn long-horizon skills faster and achieve higher final performance on complex tasks than (a) training longer in the original environment, (b) fixed LLM-generated environments, and (c) traditional curricula (easy-to-hard/adversarial). EnvGen requires orders of magnitude fewer LLM calls than per-step LLM agents and improves both average score and the speed of unlocking long-horizon achievements; update frequency, number of LLM environments, and mixture ratio are important hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1041.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1041.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPO (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PPO agent (IMPALA-style CNN architecture; Moon et al. 2023 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The baseline lightweight (<5M params) simulated agent architecture trained with Proximal Policy Optimization in the original environments without LLM-generated adaptive training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO baseline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simulated visual RL agent (CNN + linear) trained with PPO on the original environment (Crafter or Heist) without EnvGen adaptive LLM-generated training environments.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Crafter (primary baseline), Heist (baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Original game environments: Crafter is an open-world 2D survival game with 22 hierarchical achievements and sparse rewards for long-horizon tasks; Heist is a Procgen maze with layered color-coded locks to reach a gem.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same as above: number of achievements (22), depth of prerequisite chains for long-horizon tasks, number of environment steps used in training.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (Crafter)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Low variation during training (single original environment distribution), no LLM-driven configuration diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Crafter score (geometric mean over 22 achievements), reward; Heist average success rate and reward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Crafter baseline PPO: Score = 15.5% ± 0.6; Reward = 10.5 ± 0.6 (Table 1). Heist baseline PPO: Score = 25.9% ± 13.2; Reward = 4.1 ± 1.8.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Implicitly: low variation training in a complex environment leads to poor performance on long-horizon tasks due to sparse rewards and exploration difficulty; authors use this baseline to show that increasing variation/adaptive environment generation helps overcome exploration bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>15.5% ± 0.6 (Crafter baseline trained in original environment)</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-environment training in the original environment (no LLM adaptation or curriculum beyond standard RL hyperparameters).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported with 1.96M training steps in some comparisons; baseline agents still fail to unlock many long-horizon achievements despite large numbers of steps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Standard PPO training in the original complex Crafter environment yields low performance on long-horizon achievements due to sparse rewards and exploration challenges; adding varied/adaptive environments (EnvGen) substantially improves final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1041.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1041.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AD+EnvGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Achievement Distillation agent trained with EnvGen</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Achievement Distillation (AD) agent (Moon et al., 2023) augmented with EnvGen training — i.e., AD trained on LLM-generated adaptive environments — showing further improvements over PPO+EnvGen.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Achievement Distillation (AD) + EnvGen</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An existing RL agent architecture (Achievement Distillation) trained using the EnvGen pipeline: alternating LLM-generated environment training and original environment training, with LLM-adaptive updates based on measured task success.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Crafter</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same Crafter benchmark (22 hierarchical achievements, long-horizon tasks). EnvGen supplies diverse training conditions via LLM-generated JSON environment configs.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>22 achievements and long prerequisite chains as described for Crafter; agent performance measured across those.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>LLM-generated environment set size (default 4), adaptive cycles; parameterized variation over biome, resource rarities, and starting inventory.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Crafter score (geometric mean), reward, individual achievement success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>AD + EnvGen: Score = 35.3% ± 0.7; Reward = 13.7 ± 0.8 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Applying EnvGen (adaptive variation) to a stronger base agent (AD) yields additive improvements, indicating that environment variation targeted at weak skills benefits different learning algorithms; authors report EnvGen improved the hardest achievement 'collect diamond' for AD.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>AD baseline: Score = 21.8% ± 1.4 (AD without EnvGen; Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>35.3% ± 0.7 (AD after EnvGen training, evaluated in original Crafter environment)</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Achievement Distillation training within EnvGen adaptive LLM environment curriculum (mixture of LLM envs and original env).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Same EnvGen protocol steps (1.96M total steps in main experiments); EnvGen uses few LLM calls (~4).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>EnvGen improves a stronger base agent (AD) as well, producing higher Crafter scores and improved success on the hardest achievements, showing that adaptive environment variation benefits multiple agent types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1041.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1041.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPRING (GPT-4 agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPRING: GPT-4-based embodied agent (per-step LLM planning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent approach that calls a strong LLM (GPT-4) multiple times per action step to plan and act in complex open-world games, yielding good performance but at very high LLM-call and monetary cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SPRING (GPT-4-based agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An embodied agent that uses GPT-4 for per-step planning and reasoning (multiple LLM calls per action step), essentially running the LLM in the loop for decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>virtual agent (LLM-in-the-loop)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Crafter (used as a baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same Crafter environment with long-horizon achievements and sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same Crafter complexity (22 achievements; long-horizon chains).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Not central to SPRING's method in this paper; SPRING uses in-episode reasoning rather than training-time environment variation. Variation not explicitly characterized here.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Crafter score (geometric mean), reported from prior work</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SPRING (reported): Score = 27.3% ± 1.2; uses ≈2.7K LLM queries per episode (authors cite high LLM-call cost: example episode cost ≈ $270).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>SPRING demonstrates that per-step LLM planning can tackle complexity but at high computational/monetary cost; the paper contrasts this with EnvGen which attains higher performance using orders of magnitude fewer LLM calls by leveraging LLMs to generate training variation instead of acting as the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Per-step LLM planning and chain-of-thought reasoning (LLM in the loop at inference/training time).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Low in terms of LLM-call efficiency: thousands of LLM calls per episode (≈2.7K), making it computationally and financially expensive compared to EnvGen which uses ~4 LLM calls total.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM agents like SPRING can perform well on complex tasks but incur prohibitive runtime and monetary cost due to per-step LLM queries; EnvGen offers a cheaper alternative by using LLMs to generate/adapt training environments rather than to act at each step.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1041.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1041.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heist PPO+EnvGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PPO agent trained with EnvGen on Heist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of EnvGen to the Procgen Heist environment demonstrating EnvGen's generality: LLM-generated environments improve average success and stabilize training for a maze navigation/lock-opening task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO + EnvGen (Heist evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same lightweight PPO-based simulated agent trained with EnvGen's LLM-generated Heist environments (one cycle was sufficient for Heist due to simple scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Heist (Procgen)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Procgen maze navigation: agent must navigate to a gem behind three sequential color-coded locks; environment complexity arises from maze structure and sequential dependency of locks.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Sequential lock dependencies (3-layered), maze maze complexity; measured by success rate to steal gem across test episodes (100 test episodes across 10 seeds).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (simpler than Crafter but non-trivial sequential dependencies)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>LLM-generated environment configurations covering maze scenarios; for Heist authors used N_Cycle=1 because single generation could cover necessary scenarios. Also tested training steps: 5M Heist EnvGen steps + 20M original Heist steps in their reported setup.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (LLM envs covered relevant scenarios; single adaptive cycle used)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average success rate over 100 test episodes (×10 seeds), and reward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>PPO baseline on Heist: Score = 25.9% ± 13.2, Reward = 4.1 ± 1.8. PPO + EnvGen: Score = 37.7% ± 7.5, Reward = 5.5 ± 0.9.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>EnvGen improved performance and reduced variance on Heist by exposing agent to LLM-generated scenarios; authors emphasize that when an environment's scoring is simple and LLM-generated environments can cover relevant cases, fewer (even one) LLM-generated environments can be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-cycle EnvGen (N_Cycle=1) for Heist: generate LLM env(s), train agent in LLM envs (5M steps) and in original env (20M steps).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Agent trained with Heist EnvGen generalized to held-out test episodes with higher average success and lower variance, indicating EnvGen's ability to improve stability and performance in this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported use of 5M Heist EnvGen training steps and 20M original Heist steps in the evaluation described; EnvGen required few LLM calls (one generation cycle).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>EnvGen generalizes beyond Crafter: in Heist, LLM-generated environments yield higher average success and lower score variance, showing that EnvGen is useful for both complex hierarchical tasks and sequential maze tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SPRING: Studying the Paper and Reasoning to Play Games <em>(Rating: 2)</em></li>
                <li>Guiding Pretraining in Reinforcement Learning with Large Language Models <em>(Rating: 2)</em></li>
                <li>Benchmarking the spectrum of agent capabilities <em>(Rating: 2)</em></li>
                <li>Emergent complexity and zero-shot transfer via unsupervised environment design <em>(Rating: 2)</em></li>
                <li>Evolving Curricula with Regret-Based Environment Design <em>(Rating: 2)</em></li>
                <li>Playing hard exploration games by watching YouTube <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1041",
    "paper_id": "paper-268531207",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "PPO+EnvGen",
            "name_full": "PPO agent trained with EnvGen (adaptive LLM-generated environments)",
            "brief_description": "A lightweight convolutional PPO-based embodied agent (&lt;5M parameters) trained using EnvGen: LLM-generated and adaptively updated training environments to teach multiple skills in parallel and focus on agent weak points via feedback-driven environment updates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO + EnvGen (Ours)",
            "agent_description": "A simulated visual RL agent (CNN + linear head) trained with Proximal Policy Optimization (PPO). Training alternates between multiple LLM-generated custom environments and the original environment; after each cycle the agent's measured task success rates are sent to an LLM which adaptively updates the training environments to target weak skills.",
            "agent_type": "simulated agent",
            "environment_name": "Crafter (primary); also evaluated on Heist for generalization",
            "environment_description": "Crafter: open-world 2D survival game with 22 hierarchical achievements (some short-horizon, many long-horizon requiring chains of prerequisites). Environments vary by biome (grassland/mountain/beaches/natural), resource rarities (coal/iron/diamond/tree rarity), and agent starting inventory (items/pickaxes/swords/saplings). Heist: procedural maze with layered color-coded locks to reach a gem.",
            "complexity_measure": "Crafter complexity characterized by: number of hierarchical achievements (22), long-horizon task depth (many achievements require multiple prerequisite achievements, e.g., make iron pickaxe requires many prerequisite steps), number of steps required to unlock higher-order achievements (reported unlock times and chains); total training interactions (steps) reported (e.g., 1.96M total steps in their protocol).",
            "complexity_level": "high (Crafter: many long-horizon hierarchical tasks with deep prerequisite chains)",
            "variation_measure": "Environment variation controlled via LLM-generated configuration parameters (biome, resource rarity, starting inventory). Quantified in experiments by: number of distinct LLM-generated environments used (1,2,4,8 tested; default=4), number of adaptive cycles N_Cycle (default=4), update frequency (every 0.12M steps default), and ratio of LLM-env steps to original-env steps (tested ratios; default 1:1).",
            "variation_level": "medium-high (default setup uses 4 distinct LLM environments adapted across 4 cycles; parameters' ranges allow high procedural variation)",
            "performance_metric": "Crafter score (geometric mean of success rates across 22 achievements), episode reward, individual achievement success rates, achievement unlock time (first completion time). For Heist: average success rate over test episodes and reward.",
            "performance_value": "Crafter: Score = 32.2% ± 0.6; Reward = 12.6 ± 0.6. Example task-level baselines from original env before adaptation: collect_coal 38% ± 6%, defeat_skeleton 10% ± 4%, make_stone_pickaxe 31% ± 3%. Heist: with EnvGen, score improved to 37.7% ± 7.5 and reward to 5.5 ± 0.9 (from 25.9% ± 13.2 and 4.1 ± 1.8 respectively for PPO baseline).",
            "complexity_variation_relationship": "Yes — the paper demonstrates that adaptively increasing environment variation (via LLM-generated and updated environments targeted to weak skills) helps learning in high-complexity domains. Key relationships/trade-offs: (1) Adaptive varied environments outperform fixed LLM environments and standard curricula (adaptive: 32.2% vs fixed 29.9% and easy-to-hard/adversarial 26.8%); (2) simply creating more difficult environments is not sufficient — adaptive targeting of weak skills is more effective; (3) update frequency and number of distinct LLM environments matter (default 4 envs, update every 0.12M steps worked best); (4) mixture ratio between LLM env training and original environment matters (1:1 default gave best results). The authors also highlight computational trade-offs: EnvGen uses few LLM calls (≈4) making it far cheaper than per-step LLM agents but still obtains better or competitive performance.",
            "high_complexity_low_variation_performance": "15.5% ± 0.6 (PPO baseline trained solely in original Crafter environment; represents high complexity with low variation)",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "32.2% ± 0.6 (PPO evaluated in original Crafter after training with varied/adaptive LLM-generated environments — high complexity target environment, high variation during training)",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Adaptive curriculum via LLM-generated environments (EnvGen): generate multiple varied training environments, alternate training between LLM envs and original env to mitigate overfitting, measure task success in original env and provide feedback to LLM to adapt next-cycle environments. Underlying learner uses PPO.",
            "generalization_tested": true,
            "generalization_results": "EnvGen trained agents generalize to the original (unmodified) Crafter environment with improved success on long-horizon achievements; EnvGen also generalized to Heist where PPO+EnvGen increased average success from 25.9% to 37.7% and reduced variance, demonstrating transfer of skills learned in LLM-generated environments to a distinct task domain.",
            "sample_efficiency": "Total training in main Crafter experiments: 1.96M environment steps ((0.12M LLM-env + 0.12M orig-env) × 4 cycles + 1M final Crafter steps). EnvGen uses very few LLM calls (~4) in total. Authors contrast to baselines needing far more pretraining steps (e.g., MuZero+SPR 150M) or thousands-to-millions of LLM calls (SPRING ~2.7K per episode, ELLM ~5M).",
            "key_findings": "Adaptive, LLM-generated training environments (EnvGen) allow a small PPO agent to learn long-horizon skills faster and achieve higher final performance on complex tasks than (a) training longer in the original environment, (b) fixed LLM-generated environments, and (c) traditional curricula (easy-to-hard/adversarial). EnvGen requires orders of magnitude fewer LLM calls than per-step LLM agents and improves both average score and the speed of unlocking long-horizon achievements; update frequency, number of LLM environments, and mixture ratio are important hyperparameters.",
            "uuid": "e1041.0",
            "source_info": {
                "paper_title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "PPO (baseline)",
            "name_full": "PPO agent (IMPALA-style CNN architecture; Moon et al. 2023 variant)",
            "brief_description": "The baseline lightweight (&lt;5M params) simulated agent architecture trained with Proximal Policy Optimization in the original environments without LLM-generated adaptive training.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "PPO baseline",
            "agent_description": "A simulated visual RL agent (CNN + linear) trained with PPO on the original environment (Crafter or Heist) without EnvGen adaptive LLM-generated training environments.",
            "agent_type": "simulated agent",
            "environment_name": "Crafter (primary baseline), Heist (baseline comparison)",
            "environment_description": "Original game environments: Crafter is an open-world 2D survival game with 22 hierarchical achievements and sparse rewards for long-horizon tasks; Heist is a Procgen maze with layered color-coded locks to reach a gem.",
            "complexity_measure": "Same as above: number of achievements (22), depth of prerequisite chains for long-horizon tasks, number of environment steps used in training.",
            "complexity_level": "high (Crafter)",
            "variation_measure": "Low variation during training (single original environment distribution), no LLM-driven configuration diversity.",
            "variation_level": "low",
            "performance_metric": "Crafter score (geometric mean over 22 achievements), reward; Heist average success rate and reward.",
            "performance_value": "Crafter baseline PPO: Score = 15.5% ± 0.6; Reward = 10.5 ± 0.6 (Table 1). Heist baseline PPO: Score = 25.9% ± 13.2; Reward = 4.1 ± 1.8.",
            "complexity_variation_relationship": "Implicitly: low variation training in a complex environment leads to poor performance on long-horizon tasks due to sparse rewards and exploration difficulty; authors use this baseline to show that increasing variation/adaptive environment generation helps overcome exploration bottlenecks.",
            "high_complexity_low_variation_performance": "15.5% ± 0.6 (Crafter baseline trained in original environment)",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-environment training in the original environment (no LLM adaptation or curriculum beyond standard RL hyperparameters).",
            "generalization_tested": false,
            "generalization_results": "",
            "sample_efficiency": "Reported with 1.96M training steps in some comparisons; baseline agents still fail to unlock many long-horizon achievements despite large numbers of steps.",
            "key_findings": "Standard PPO training in the original complex Crafter environment yields low performance on long-horizon achievements due to sparse rewards and exploration challenges; adding varied/adaptive environments (EnvGen) substantially improves final performance.",
            "uuid": "e1041.1",
            "source_info": {
                "paper_title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "AD+EnvGen",
            "name_full": "Achievement Distillation agent trained with EnvGen",
            "brief_description": "Achievement Distillation (AD) agent (Moon et al., 2023) augmented with EnvGen training — i.e., AD trained on LLM-generated adaptive environments — showing further improvements over PPO+EnvGen.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Achievement Distillation (AD) + EnvGen",
            "agent_description": "An existing RL agent architecture (Achievement Distillation) trained using the EnvGen pipeline: alternating LLM-generated environment training and original environment training, with LLM-adaptive updates based on measured task success.",
            "agent_type": "simulated agent",
            "environment_name": "Crafter",
            "environment_description": "Same Crafter benchmark (22 hierarchical achievements, long-horizon tasks). EnvGen supplies diverse training conditions via LLM-generated JSON environment configs.",
            "complexity_measure": "22 achievements and long prerequisite chains as described for Crafter; agent performance measured across those.",
            "complexity_level": "high",
            "variation_measure": "LLM-generated environment set size (default 4), adaptive cycles; parameterized variation over biome, resource rarities, and starting inventory.",
            "variation_level": "medium-high",
            "performance_metric": "Crafter score (geometric mean), reward, individual achievement success rates.",
            "performance_value": "AD + EnvGen: Score = 35.3% ± 0.7; Reward = 13.7 ± 0.8 (Table 1).",
            "complexity_variation_relationship": "Applying EnvGen (adaptive variation) to a stronger base agent (AD) yields additive improvements, indicating that environment variation targeted at weak skills benefits different learning algorithms; authors report EnvGen improved the hardest achievement 'collect diamond' for AD.",
            "high_complexity_low_variation_performance": "AD baseline: Score = 21.8% ± 1.4 (AD without EnvGen; Table 1)",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "35.3% ± 0.7 (AD after EnvGen training, evaluated in original Crafter environment)",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Achievement Distillation training within EnvGen adaptive LLM environment curriculum (mixture of LLM envs and original env).",
            "generalization_tested": false,
            "generalization_results": "",
            "sample_efficiency": "Same EnvGen protocol steps (1.96M total steps in main experiments); EnvGen uses few LLM calls (~4).",
            "key_findings": "EnvGen improves a stronger base agent (AD) as well, producing higher Crafter scores and improved success on the hardest achievements, showing that adaptive environment variation benefits multiple agent types.",
            "uuid": "e1041.2",
            "source_info": {
                "paper_title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "SPRING (GPT-4 agent)",
            "name_full": "SPRING: GPT-4-based embodied agent (per-step LLM planning)",
            "brief_description": "An LLM-based agent approach that calls a strong LLM (GPT-4) multiple times per action step to plan and act in complex open-world games, yielding good performance but at very high LLM-call and monetary cost.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "SPRING (GPT-4-based agent)",
            "agent_description": "An embodied agent that uses GPT-4 for per-step planning and reasoning (multiple LLM calls per action step), essentially running the LLM in the loop for decision making.",
            "agent_type": "virtual agent (LLM-in-the-loop)",
            "environment_name": "Crafter (used as a baseline comparison)",
            "environment_description": "Same Crafter environment with long-horizon achievements and sparse rewards.",
            "complexity_measure": "Same Crafter complexity (22 achievements; long-horizon chains).",
            "complexity_level": "high",
            "variation_measure": "Not central to SPRING's method in this paper; SPRING uses in-episode reasoning rather than training-time environment variation. Variation not explicitly characterized here.",
            "variation_level": "not specified",
            "performance_metric": "Crafter score (geometric mean), reported from prior work",
            "performance_value": "SPRING (reported): Score = 27.3% ± 1.2; uses ≈2.7K LLM queries per episode (authors cite high LLM-call cost: example episode cost ≈ $270).",
            "complexity_variation_relationship": "SPRING demonstrates that per-step LLM planning can tackle complexity but at high computational/monetary cost; the paper contrasts this with EnvGen which attains higher performance using orders of magnitude fewer LLM calls by leveraging LLMs to generate training variation instead of acting as the agent.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Per-step LLM planning and chain-of-thought reasoning (LLM in the loop at inference/training time).",
            "generalization_tested": null,
            "generalization_results": "",
            "sample_efficiency": "Low in terms of LLM-call efficiency: thousands of LLM calls per episode (≈2.7K), making it computationally and financially expensive compared to EnvGen which uses ~4 LLM calls total.",
            "key_findings": "LLM agents like SPRING can perform well on complex tasks but incur prohibitive runtime and monetary cost due to per-step LLM queries; EnvGen offers a cheaper alternative by using LLMs to generate/adapt training environments rather than to act at each step.",
            "uuid": "e1041.3",
            "source_info": {
                "paper_title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Heist PPO+EnvGen",
            "name_full": "PPO agent trained with EnvGen on Heist",
            "brief_description": "Application of EnvGen to the Procgen Heist environment demonstrating EnvGen's generality: LLM-generated environments improve average success and stabilize training for a maze navigation/lock-opening task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO + EnvGen (Heist evaluation)",
            "agent_description": "Same lightweight PPO-based simulated agent trained with EnvGen's LLM-generated Heist environments (one cycle was sufficient for Heist due to simple scoring).",
            "agent_type": "simulated agent",
            "environment_name": "Heist (Procgen)",
            "environment_description": "Procgen maze navigation: agent must navigate to a gem behind three sequential color-coded locks; environment complexity arises from maze structure and sequential dependency of locks.",
            "complexity_measure": "Sequential lock dependencies (3-layered), maze maze complexity; measured by success rate to steal gem across test episodes (100 test episodes across 10 seeds).",
            "complexity_level": "medium (simpler than Crafter but non-trivial sequential dependencies)",
            "variation_measure": "LLM-generated environment configurations covering maze scenarios; for Heist authors used N_Cycle=1 because single generation could cover necessary scenarios. Also tested training steps: 5M Heist EnvGen steps + 20M original Heist steps in their reported setup.",
            "variation_level": "medium (LLM envs covered relevant scenarios; single adaptive cycle used)",
            "performance_metric": "Average success rate over 100 test episodes (×10 seeds), and reward.",
            "performance_value": "PPO baseline on Heist: Score = 25.9% ± 13.2, Reward = 4.1 ± 1.8. PPO + EnvGen: Score = 37.7% ± 7.5, Reward = 5.5 ± 0.9.",
            "complexity_variation_relationship": "EnvGen improved performance and reduced variance on Heist by exposing agent to LLM-generated scenarios; authors emphasize that when an environment's scoring is simple and LLM-generated environments can cover relevant cases, fewer (even one) LLM-generated environments can be effective.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-cycle EnvGen (N_Cycle=1) for Heist: generate LLM env(s), train agent in LLM envs (5M steps) and in original env (20M steps).",
            "generalization_tested": true,
            "generalization_results": "Agent trained with Heist EnvGen generalized to held-out test episodes with higher average success and lower variance, indicating EnvGen's ability to improve stability and performance in this domain.",
            "sample_efficiency": "Reported use of 5M Heist EnvGen training steps and 20M original Heist steps in the evaluation described; EnvGen required few LLM calls (one generation cycle).",
            "key_findings": "EnvGen generalizes beyond Crafter: in Heist, LLM-generated environments yield higher average success and lower score variance, showing that EnvGen is useful for both complex hierarchical tasks and sequential maze tasks.",
            "uuid": "e1041.4",
            "source_info": {
                "paper_title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SPRING: Studying the Paper and Reasoning to Play Games",
            "rating": 2,
            "sanitized_title": "spring_studying_the_paper_and_reasoning_to_play_games"
        },
        {
            "paper_title": "Guiding Pretraining in Reinforcement Learning with Large Language Models",
            "rating": 2,
            "sanitized_title": "guiding_pretraining_in_reinforcement_learning_with_large_language_models"
        },
        {
            "paper_title": "Benchmarking the spectrum of agent capabilities",
            "rating": 2,
            "sanitized_title": "benchmarking_the_spectrum_of_agent_capabilities"
        },
        {
            "paper_title": "Emergent complexity and zero-shot transfer via unsupervised environment design",
            "rating": 2,
            "sanitized_title": "emergent_complexity_and_zeroshot_transfer_via_unsupervised_environment_design"
        },
        {
            "paper_title": "Evolving Curricula with Regret-Based Environment Design",
            "rating": 2,
            "sanitized_title": "evolving_curricula_with_regretbased_environment_design"
        },
        {
            "paper_title": "Playing hard exploration games by watching YouTube",
            "rating": 1,
            "sanitized_title": "playing_hard_exploration_games_by_watching_youtube"
        }
    ],
    "cost": 0.01876775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents
12 Jul 2024</p>
<p>Abhay Zala aszala@cs.unc.edu 
UNC Chapel Hill</p>
<p>Jaemin Cho 
UNC Chapel Hill</p>
<p>Han Lin hanlincs@cs.unc.edu 
UNC Chapel Hill</p>
<p>Jaehong Yoon jhyoon@cs.unc.edu 
UNC Chapel Hill</p>
<p>Mohit Bansal mbansal@cs.unc.edu 
UNC Chapel Hill</p>
<p>EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents
12 Jul 202447C6FCD7ECDBB5AE187D2BB9BF2FD09FarXiv:2403.12014v2[cs.CL]
Recent state-of-the-art approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment.Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive.This begs an interesting question: Instead of directly employing LLMs as embodied agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at?In this work, we propose EnvGen, a novel framework to address this question.First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel.Concretely, the LLM is given the task description and environment simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g., different terrains, items initially given to agents, chances of finding certain objects, etc.).Next, we train a small RL agent in a mixture of the original and LLMgenerated environments.Then, we enable the LLM to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the LLM in the form of the agent's performance.We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist game environments.We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster.We also show that using an LLM to adapt environments dynamically outperforms curriculum learning approaches and how the LLM adapts training environments to help improve RL agents' weaker skills over time.Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require one or more LLM calls per step (resulting in thousands of LLM calls per episode).We also present detailed analyses of EnvGen's design choices.</p>
<p>Introduction</p>
<p>There has been growing interest in embodied AI, where agents learn through interactions with environments instead of static datasets (Ahn et al., 2022;Duan et al., 2022;Wang et al., 2023a;Yao et al., 2023;Driess et al., 2023).Open-world games such as Minecraft (Mojang Studios, 2009) and Crafter (Hafner, 2022) have been widely used as research environments for embodied agents, where the agents visually perceive their surroundings, traverse large terrains, and learn to unlock various achievements (e.g., collecting resources, building tools, defeating monsters, etc.).Some achievements can be easily unlocked within a few steps, whereas others are more challenging as they only become accessible after the agent completes a series of prerequisite achievements, requiring hundreds of steps (i.e., long-horizon tasks).As illustrated in Fig. 1 (a), traditional embodied agents are based on reinforcement In (c) EnvGen, we train a small RL agent with diverse LLM-generated environments that train different skills in parallel and can be adapted via feedback to help the agents progressively improve skills that they are weaker at.Our method benefits from using the world knowledge from LLMs while maintaining efficient training through a lightweight RL agent.learning (RL) (Hafner et al., 2020;2021;2023;Schulman et al., 2017;Burda et al., 2018;Hessel et al., 2018;Sekar et al., 2020;Moon et al., 2023).However, these RL agents usually struggle when learning such long-horizon tasks since the reward is sparsely given only after the correct execution of successive actions, and it is very expensive to automatically find many action sequences which lead to the reward (Aytar et al., 2018;Li et al., 2022a;Yuan et al., 2023), even after long pretraining with curiosity-driven intrinsic reward (Walker et al., 2023).</p>
<p>As large language models (LLMs) have shown remarkable progress in various tasks that require complex reasoning (Brown et al., 2020;OpenAI, 2023a;Touvron et al., 2023a;b;Chowdhery et al., 2023;Anil et al., 2023), recent works study implementing embodied agents based on LLMs.As illustrated in Fig. 1 (b), these methods leverage LLMs' world knowledge with chain-of-thought reasoning (Nye et al., 2021;Kojima et al., 2022;Wei et al., 2022) by creating action plans, giving feedback, and obtaining rewards throughout the episode (Yuan et al., 2023;Wang et al., 2023c;Wu et al., 2023;Wang et al., 2023a;d;Zhao et al., 2023;Du et al., 2023).While these LLM-based agents that verbalize their knowledge in reasoning steps have seen success in achieving better performance over previous approaches, iteratively calling LLMs throughout the episode is prohibitively slow and expensive (e.g., SPRING (Wu et al., 2023) calls GPT-4 (OpenAI, 2023a) 9 times to take any action step, which results in $270 USD to complete an episode).Du et al. (2023) use LLMs to create rewards to train smaller agents, but the training is still costly, as it requires many interactions between the LLMs and student agents.This begs the question: Instead of directly employing LLMs as embodied agents, can we use LLMs' reasoning capability to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at?</p>
<p>To address this question, we propose EnvGen, a novel framework where an LLM adaptively generates training environments to teach smaller embodied RL agents.We aim to generate environments that can create various conditions (e.g., have different terrains or some subgoals are already achieved) so that agents can learn different skills in parallel and obtain more frequent rewards for challenging long-horizon tasks than in the original environment.As shown in Fig. 1 (c), EnvGen iterates over multiple training cycles, each consisting of the following four steps:</p>
<p>• Step 1: We generate configurations for custom training environments (i.e., specifically created to train an RL agent on certain skills) by providing an LLM with a prompt including task description, controllable simulator settings, and simulator constraints (see Fig. 2 and Sec. 2 for details).Then we use the generated configurations to create different custom environments (e.g., different terrains, items initially given to agents, and chance of finding certain objects) that can teach multiple skills in parallel.• Step 2: We first train the RL agent in multiple LLM-generated environments (i.e., LLM environments), so that it can learn different useful skills in parallel.• Step 3: We then train the RL agent in the original environment to mitigate overfitting to the LLM environments.Afterwards, we measure the current RL agent's performance in different tasks in the original environment to check which skills/tasks the agent is still weak at.</p>
<p>• Step 4: We provide the RL agent's successes/failures in different tasks (from step 3) as feedback to the LLM, so that the LLM can adapt the custom training environments to focus on progressively improving the skills that the agent is weak at.</p>
<p>Note that EnvGen only requires a few LLM calls (e.g., 4) for environment generation/updating during the entire RL agent training process, whereas other works based on LLM agents query an LLM once or multiple times every step (resulting in thousands of LLM calls for a single episode).</p>
<p>We study the usefulness of EnvGen in different game environments: Crafter (Hafner, 2022) and Heist (Cobbe et al., 2020).In the Crafter environment, a simple PPO-based (Schulman et al., 2017) lightweight (&lt; 5M parameters) RL agent trained with our LLM-generated environments outperforms strong baselines including a GPT-4 based agent that queries an LLM multiple times at every step, and RL agents that use extensive pretraining (e.g., 150M steps vs. less than 1M steps for us).When compared to just training longer in the original Crafter environment and curriculum learning approaches such as easy-tohard and adversarial environments, an RL agent trained with EnvGen achieves significant improvements on the overall score and long-horizon tasks.In Heist, we also show that our LLM-generated environments can improve overall agent performance and training stability.</p>
<p>We also show a qualitative study on how the LLM adapts training environments to help improve RL agents' weaker skills over time.Finally, we provide comprehensive analysis and ablation studies of the design choices of EnvGen, including dynamically updating LLM environments (i.e., using adaptive environments) vs. curriculum learning methods, frequency of environment updates, EnvGen vs. longer training in the original environment, different LLMs for generating environments, the number of LLM-generated environments, and the mixture ratio between the original and LLM environment during training.</p>
<p>EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents</p>
<p>We propose EnvGen, a novel framework where an LLM adaptively generates training environments to train smaller embodied RL agents, enabling them to accomplish various tasks within an environment, particularly long-horizon tasks.During the training process, the LLM is given feedback (in the form of the agent's performance) and can adaptively update the training environments to progressively focus on improving the tasks that the agent is weak at.In the following, we first explain why it is challenging to explore longhorizon tasks in open-world games (Sec.2.1).Then we explain our method details, including how we generate environments and how agents are trained in EnvGen (Sec.2.2).</p>
<p>Preliminary: Exploration is Hard for Long-Horizon Tasks</p>
<p>In the RL framework, agents explore various states along a trajectory and amplify policies based on the rewards received from those trajectories.However, exploration for longhorizon tasks is slow and computationally expensive, as rewards for such tasks are sparsely given only after a sequence of successful actions that often involve achieving multiple subgoals.For example, the goal in Crafter (Hafner, 2022) is to unlock 22 achievements, where some achievements can be unlocked quickly through several simple actions and others require long chains of prerequisites (e.g., collect iron requires make stone pickaxe, which must be preceded by collect stone, ... etc.); see Sec. 3.1 for details.As shown in Hafner (2022), existing agents in Crafter spend most exploration steps learning low-level achievements but fail to unlock high-order achievements with many prerequisites.</p>
<p>EnvGen Method Details</p>
<p>We introduce EnvGen, where we train an embodied RL agent in multiple LLM-generated environments (we refer to these as 'LLM environments' in the paper) that progressively adapt to improve agent performance in multiple skills.The generated environments can provide various conditions (e.g., different terrains, or some subgoals are already achieved) <code>`json { "environment_settings": { "target_biome": "grassland", "tree_rarity": "common", "coal_rarity": "common" }, "inventory_settings": { "wood": 3, "stone": 0, "wood_pickaxe": 1 } }</code>P urpose: The agent can learn to collect wood and craft items like a wood sword or a table ...</p>
<h1>Game and task description</h1>
<p>You are an environment designer agent for a game called "Crafter".Your job is to design a few environments which can be used to teach an agent how to play...</p>
<h1>Game Objectives</h1>
<p>Here is a list of things an agent would need to learn how to do: • collect_coal, collect_diamond ...</p>
<h1>Controllable Simulator Settings</h1>
<p>Here is a list of parameters you can control when making an environment: "environment_settings": { "target_biome": "mountain", "coal_rarity": "common", "iron_rarity": "common", "diamond_rarity": "rare" }, "inventory_settings": { "wood": 5, "stone": 3, "wood_pickaxe": 1, "stone_pickaxe": 1, "wood_sword": 1 } } ``P urpose: ...</p>
<p>Env 3: Survival and Crafting Mastery</p>
<p><code>`json { "environment_settings": { "target_biome": "natural", "tree_rarity": "common", "coal_rarity": "common", "iron_rarity": "common", "diamond_rarity": "rare" }, "inventory_settings": { "sapling": 2, "wood": 0, "stone": 0, "coal": 0, "iron": 0, "diamond": 0 } }</code>P urpose: ...</p>
<p>Small RL Agent Original Environment</p>
<p>Those environments resulted in the agent improving up to these scores:</p>
<p>• collect_coal: 38% +/-6% • defeat_skeleton: 10% +/-4% • make_stone_pickaxe: 31% +/-3% • … Could you generate new environments based on these scores?</p>
<p>Step 1: Generate training environments</p>
<p>Step 2: Train small agent in generated environments so that agents can learn different skills in parallel and obtain more frequent rewards for longhorizon tasks.As shown in Fig. 2, EnvGen iterates N Cycle training cycles, each consisting of the following four steps:</p>
<p>Step 1: Generate training environments with an LLM.As illustrated in step 1 of Fig. 2, we use an LLM (e.g., GPT-4 (OpenAI, 2023a)) to first generate N LLM-Env custom training environment configurations1 that can cover various objectives and skills that are required in the original environment.The following describes the LLM input prompt components used to create environment configurations.</p>
<ol>
<li>Task description: We provide a brief description of the environment and what the LLM should do (e.g., "generate a set of training environments..."). 2. Game/simulator details: We provide a list of objectives that need to be achieved in the environment (e.g., "collect coal, collect iron, etc." for Crafter); a list of which simulator settings can be controlled (e.g., terrain, agent inventory); and a list of constraints/rules that the simulator has (e.g., "skeletons only spawn in mountains; ..." for Crafter).</li>
</ol>
<p>Output environment configuration template:</p>
<p>We provide a blank output configuration template (i.e., a JSON object where the environment settings are empty) to the LLM, and request it to fill in the values, creating N LLM-Env environment configurations.Along with filling the templates, we also ask the LLM to verbally explain the purpose for each environment (e.g., what the environment would teach the agent); this would help users easily understand the environment generation process.4. Adaptation feedback based on the RL agent's performance: We provide the LLM with the performance of the RL agent from the original environment (measured in step 3 and summarized in step 4), as feedback for adapting LLM environments to focus on skills that the RL agent is weak at.The feedback is given at the end of each cycle, so it is only provided to LLM from the second cycle onwards.</p>
<p>The obtained environment configurations are then rendered in the game's simulator.Fig. 2 presents the summary of input prompt and output environments from the GPT-4 model.We provide more prompt details in Appendix F.   Step 2: Train a small RL agent in the LLM-generated environments.As shown in step 2 of Fig. 2, we train the small RL agent in the LLM-generated environments.Concretely, we train the agent in the N LLM-Env LLM environments for T LLM-Env total steps in parallel.</p>
<p>Step 3: Train and measure the RL agent's performance in the original environment.It is important to note that the goal of EnvGen is to improve the RL agent's performance in the original environment, instead of the performance only in the LLM environments.</p>
<p>To help the RL agent effectively adapt to the original environment and provide the LLM with the current agent's performance as feedback, we train the agent and measure its performance in the original environment, as shown in step 3 of Fig. 2. First, to mitigate the overfitting to LLM environments, we train the agent in the original environment for T Orig-Env steps.2Next, to find the skills that the RL agent needs to improve at, we test the agent in the original environment, without any parameter updates.Concretely, we measure individual success rates for each environment task (e.g., Crafter achievements).The agent performance is summarized (in step 4) and is provided to LLM as feedback (in step 1) to adapt training environments in the next cycle.Moreover, importantly, to obtain a more calibrated estimation of agent performance, we calculate the average and variance of the task-specific scores by testing agents with multiple random seeds (i.e., 12).</p>
<p>Step 4: Send feedback to LLM to adapt environments (to focus on weak skills).We provide the LLM with the agent's performance from the original environment (measured in step 3), as feedback for updating LLM environments.Concretely, we list the agent's average task-specific success rate in percentages along with one standard deviation (e.g., ". . .collect coal: 38% ± 6%, defeat skeleton: 10% ± 4% . . ."), as shown in step 4 of Fig. 2. In step 1 of the next cycle, the LLM can adaptively generate new environments (by using the agent's performance as feedback) to better help the RL agent learn the skills it is weak at (e.g., defeat skeleton).EnvGen iterates this four-step training cycle N Cycle times.</p>
<p>Experimental Setup</p>
<p>In the following subsections, we present the benchmarks in which we evaluate EnvGen framework on (Sec.3.1) and the agent architectures that we use for experiments (Sec.3.2).</p>
<p>Evaluated Benchmarks and Training Details</p>
<p>Crafter.Crafter (Hafner, 2022) is an open-world 2D survival game focused on evaluating a broad range of agent capabilities (see Fig. 3).Crafter features 22 achievements that an agent can unlock during an episode of play.Some achievements can be unlocked in a few steps (e.g., collect wood, collect sapling, etc.), but other achievements, such as make iron pickaxe or collect diamond, require many training/exploration steps and several prerequisite achievements to be unlocked (see Fig. 3 b).For example, to make an iron pickaxe, an agent must first collect enough wood to make a table and a wooden pickaxe, then go collect stone and return to the table (or collect more wood to make a new one) and then construct a stone pickaxe.Then the agent still needs to make a furnace, collect coal, and collect iron before the option to make the iron pickaxe is possible.</p>
<p>For EnvGen setup, we use N Cycle = 4 training cycles during agent training (see Table 3 for ablation of having a different number of cycles).Each cycle uses 0.12M LLM-generated environment steps (i.e., Crafter EnvGen steps, see step 2 in Fig. 2) and 0.12M Crafter steps (step 3 in Fig. 2) and then we train for 1M steps in Crafter.In total, we train for 1.96M steps ((0.12M + 0.12M) × 4 + 1M).Note that in order to maintain a fair score comparison to baselines, we do not count any achievement during our training cycle for score calculation since the training scores derived from LLM environments and the original environment are not directly comparable.Instead, we only take into account the achievements from the last 1M training steps in Crafter for the score calculation.We also experiment with giving the baseline model additional original environment steps to match the number of EnvGen steps (i.e., an additional 0.96M steps) to ensure that EnvGen is not better simply because of more steps.The score for Crafter is computed as the geometric mean of individual success rates of each achievement for each episode it is completed within 1M training steps:
S = exp( 1 22 ∑ 22 i=1 ln(1 + s i )) − 1,
where s i is the average success rate of the ith achievement across all episodes that occurred during training.We report the average performance with 30 runs (= 3 different initial LLM-generated Crafter EnvGen environments × 10 different random seeds).</p>
<p>Heist.Heist is part of the OpenAI Procgen (Cobbe et al., 2020) benchmark.In this environment, agents must successfully 'steal' the gem after navigating a maze and opening all locks.See more details in Appendix C.2.</p>
<p>Agent Architectures</p>
<p>Our base RL agent.For both Crafter and Heist, we test the EnvGen framework with a simple (CNN + linear layer) and lightweight (&lt;5M) agent used in Moon et al. (2023), which is slightly modified from the agent architecture used in IMPALA (Espeholt et al., 2018).Following Moon et al. (2023), we train the agent with a PPO (Schulman et al., 2017) objective.At every step, the agent takes an RGB image (surroundings for Crafter, entire maze for Heist) as input and outputs the value estimates and policy (action probability).See Fig. 3 (a) for an agent visual input example.We provide additional model details in Appendix E.</p>
<p>Baseline methods.For Crafter, we compare our method to two groups of recent baselines -(1) methods that use frequent (i.e., more than thousands of) LLM calls during training or inference: SPRING (Wu et al., 2023) (based on GPT-4) and ELLM (Du et al., 2023) (based on Codex (Chen et al., 2021)) and (2) methods that do not use an LLM: DreamerV3 (Hafner et al., 2023), MuZero + SPR (Walker et al., 2023), LSTM-SPCNN (Stanić et al., 2023), PPO (Schulman et al., 2017), and Achievement Distillation (AD) (Moon et al., 2023).For Heist, we compare against the PPO agent.For the PPO and AD agents, we follow the implementation of Moon et al. (2023).See Appendix E for the PPO/AD agent details.</p>
<p>Results and Analysis</p>
<p>We demonstrate the usefulness of the EnvGen method with comprehensive experiments and analysis.We first compare RL agents trained with EnvGen to different baseline methods on Crafter, an open-world game with 22 hierarchical achievements (Sec.4.1).Next, we present a detailed analysis of the improvements that training with EnvGen environments can give RL agents on long-horizon tasks (Sec.4.2).Then, we analyze how the LLM-based environment adaptation can help an RL agent progressively improve the skills that the agent is weak at (Sec.4.3).Lastly, we present various additional analysis including experiments on Heist (a maze navigation game) and ablation studies on EnvGen design choices (Sec.4.4 and also in the Appendix C).  1, we find that a small (4M parameters) PPO agent with EnvGen achieves an average score of 32.2% and significantly outperforms the baselines (and also in terms of the average reward).Note that some baseline agents have many more parameters or pretraining steps such as SPRING (GPT-4 agent; 27.3%), and MuZero + SPR (150M pretraining steps; 16.4%).Our method also only uses orders of magnitude fewer LLM calls (only 4) than works like SPRING (2.7K on average) and ELLM (5M), making it much cheaper/more efficient.EnvGen can also work with other RL agents such as Achievement Distillation (AD) (Moon et al., 2023) to achieve an even higher score (35.3%).</p>
<p>Detailed Achievement Analysis on Crafter Environment</p>
<p>Next, we analyze where EnvGen improves the overall score by checking individual achievement success rates in detail.For this, we compare the same PPO agent architecture trained with different setups: (1) an agent trained on Crafter for 1.96M steps and (2) an agent trained on Crafter EnvGen for 0.96M steps (0.24M steps × 4 training cycles, see Sec. 2.2) and then trained on Crafter for 1M steps.We measure the success rate (Fig. 4) of each achievement and unlocking speed (Fig. 5) of iron tools in the last 1M training steps.</p>
<p>EnvGen helps RL agents to tackle challenging long-horizon achievements.Fig. 4 shows that training in Crafter EnvGen improves scores of several achievements.Notably, training in Crafter EnvGen significantly improves the scores of long-horizon achievements (with many prerequisites; see Fig. 3) such as stone and iron tools.Fig. 5 shows that after unlocking the stone pickaxe, the RL agent trained in Crafter EnvGen is significantly faster in unlocking iron tools.In Appendix C.1, we also compare two AD agents, and show that Crafter EnvGen improves the success rate of the most challenging achievement -'collect diamond'.the agent's performance for the skill.Likewise, in the cycle 3, given the feedback that the agent is weak at making stone pickaxes, the LLM generates an environment to help the agent more easily craft the stone pickaxe, helping the agent improve it's score for the skill.</p>
<p>Adaptation of Training Environments Helps the Agent Improve Weaker Skills</p>
<p>Powered by the adaptive LLM environment generation of EnvGen, our agent learns to unlock these two achievements significantly faster than the baseline agent.</p>
<p>Additional Analysis and Ablation Studies</p>
<p>In the following, we show comprehensive design analysis and ablation studies of EnvGen method: dynamically updating LLM environments (i.e., using adaptive environments) vs. curriculum learning methods, and different frequencies of environment updates.In Appendix C, we show comprehensive analysis and ablation studies of EnvGen method: EnvGen vs. longer training in the original environment, different LLMs for generating environments, the number of LLM environments, and the ratio of training steps in the LLM vs. original environments.We also include experiments on the Heist environment (see Sec. 3.1) in Appendix C.2.</p>
<p>Different environment curricula: fixed, easy-to-hard, adversarial vs. adaptive.Table 2 shows that using LLM environments that are adaptively updated based on intermediate agent performance to improve weaker skills (last row) results in overall higher scoring agents than just using the initial LLM environments for the whole training (32.2% vs. 29.9%).</p>
<p>Collect Coal Make Stone Pickaxe</p>
<p>Cycle 2</p>
<p>Here are new environments that can help the agent improve the low scores: ... Environment 1: Introduction to Mining and Crafting <code>`json { "target_biome": "mountain", "wood_pickaxe": 1, ... }</code>P urpose: This environment is designed to teach the agent how to mine coal ... ...</p>
<p>Feedback from previous cycle</p>
<p>Collect coal is 2% Make stone pickaxe is 1%</p>
<p>…</p>
<p>LLM Cycle 3</p>
<p>Here are new environments that can help the agent improve the low scores: ... Environment 2: Introduce combat and advanced tool crafting <code>`json { "wood": 3, "stone": 2, ... }</code>P urpose: With stone in the inventory, the agent is nudged to craft a stone pickaxe, a task it hasn't learned yet ... ...  (Hafner, 2022) using no curriculum, an easy-to-hard curriculum, an adversarial curriculum, and our adaptive+dynamic environments.Agents are trained for 0.96M steps using the curriculum and then 1M in the default Crafter environment.</p>
<p>Feedback from previous cycle</p>
<p>These results indicate the effectiveness of the agent feedback and environment updating (step 4 described in Sec. 2).</p>
<p>Related Works</p>
<p>LLMs as open-world game agents.Recent works study using LLMs to create action plans (i.e., a list of subgoals or skills to target) for embodied agents in open-world games like Minecraft and Crafter (Hafner, 2022).Most of these methods require calling LLMs frequently (e.g., at every step) for planning the next steps (Yuan et al., 2023;Wang et al., 2023c;Wu et al., 2023;Wang et al., 2023a;d;Zhao et al., 2023) Deep learning-based game/simulator content generation.Procedural content generation (PCG) for games is about the automatic generation of levels, landscapes, items, rules, quests, or other types of game contents (Shaker et al., 2016).While traditional PCG methods are based on search/solver/rule/grammar-based methods, recent works apply deep learning methods such as GAN (Goodfellow et al., 2014) for PCG (Liu et al., 2021;Kumaran et al., 2020;Schubert et al., 2022).Several works have explored using LLMs to generate game content such as difficulty levels (Sudhakaran et al., 2023;Todd et al., 2023) and scenes/environments (Kumaran et al., 2023;Wang et al., 2023b;Afshar &amp; Li, 2024).While these works aim to help developers create new game content, we aim to improve RL agent performance in the original environment.A line of work proposes unsupervised environment design (UED) that manipulates the difficulty level of environments to be more challenging to RL agents (Dennis et al., 2020;Jiang et al., 2021;Parker-Holder et al., 2022).While these works use a learned environment manipulator or evolutionary algorithms to maximize the 'regret' (the difference between the expected return of the current and optimal policies) in simple games such as MiniGrid (Chevalier-Boisvert et al., 2023), we use the world knowledge of LLMs to generate and adapt training environments that can improve weaker skills based on comprehensive skill-specific feedback from RL agents in open-world games with many challenging long-horizon tasks.To help agents generalize to unseen tasks in a text-based dialogue game, Ammanabrolu et al. ( 2022) augment new tasks with LMs and use a manually designed, fixed curriculum.Unlike this work, we adaptively generate training environments using LLMs' world knowledge and automatically learning a dynamic curriculum based on the RL agent's feedback, so as to improve the agent's weaker skills in open-world games with visual inputs.Beyond game content generation, several works visually augment vision-and-language navigation (VLN) simulators (e.g., rendering the environments with different styles) using image generation models (Li et al., 2022b;Wang et al., 2023e;Li &amp; Bansal, 2023).Such works could complement our LLM environments (e.g., augmenting our environments with diverse colors and textures).</p>
<p>Conclusion</p>
<p>We propose EnvGen, a novel framework to improve embodied RL agent performance by utilizing the world knowledge of LLMs to adaptively generate training environments.In EnvGen, we give an LLM a prompt describing a game/simulator and ask the LLM to generate the configurations to create new environments that can teach different skills.Next, we train an agent in the LLM-generated environments, give feedback to the LLM by testing the agent in the original environments, and then ask the LLM to update the environments to teach agents skills they are weaker at.In two challenging games, Crafter and Heist, we show that EnvGen increases agent performance significantly, and training with LLM-generated environments is more effective than training longer in the original environments.We also show that using an LLM to adapt environments dynamically outperforms curriculum learning approaches and how the LLM adapts training environments to help improve RL agents' weaker skills over time.Moreover, a lightweight model (&lt; 5M parameters) trained with LLM-generated environments even outperforms an LLM agent with significantly fewer LLM calls.We hope our work can guide future works in leveraging LLMs for embodied agents.</p>
<p>A Additional Related Works</p>
<p>Reward designs in reinforcement learning.Finding good action trajectories is critical in reinforcement learning (RL) (Sutton &amp; Barto, 2018).While classic random exploration algorithms such as epsilon-greedy (Watkins, 1989) work well in simple settings such as multi-armed bandit, it is not the case for hard exploration problems where the environment gives very sparse rewards (Weng, 2020).A line of work studies how to augment the original (extrinsic) rewards from the environment with intrinsic rewards that encourage exploration (Bellemare et al., 2016;Burda et al., 2018).While such intrinsic rewards can help RL agents discover novel states and improve their knowledge about the environment, it often requires long pretraining and does not guarantee that the intrinsic reward can help the target task.Another recent line of work studies using LLMs to adjust reward functions to help RL agents progressively learn certain tasks (Li et al., 2024;Kwon et al., 2023;Ma et al., 2023;Du et al., 2023).Instead of designing new rewards, in EnvGen, an LLM adaptively generates training environments that can help the RL agent learn multiple skills it is weak at with fewer training steps than in the original environment; reward design could be complementary to our method.</p>
<p>B Additional Game Environment Details</p>
<p>Heist Environment.Heist is part of the OpenAI Procgen (Cobbe et al., 2020) benchmark.</p>
<p>In this environment, agents must successfully 'steal' the gem after navigating a maze and opening all locks (see Fig. 7).The gem is behind three layers of color-coded locks, each requiring that the previous lock be unlocked first (e.g., to unlock the green lock, the blue lock must first be unlocked).Following Moon et al. (2023), the final score is calculated as the average success of the agent in stealing the gem in 100 test episodes in 10 different seeds (i.e., 1,000 runs in total).For agent training, we use a total of 5M steps in the LLM-generated environments (i.e., 5M Heist EnvGen steps) and a total of 20M in the actual Heist environment.</p>
<p>As the game only provides scores on the final objective ('steal gem') and the game is simple enough for the LLM-generated environments to cover all scenarios with one generation, we only use N Cycle = 1 training cycle.</p>
<p>C Additional Experiment Results</p>
<p>C.1 Design Choices, Ablations, and Other Agent Architectures</p>
<p>In the following, we show comprehensive design choice and ablation studies of EnvGen method: EnvGen vs. longer training in the original environment, different LLMs for generating environments, the number of LLM environments, and the ratio of training steps in the LLM environments to the original environment.Unless otherwise noted, we use the PPO-based agent (Moon et al., 2023) 7, while different ratios do not result in big differences, the default 1:1 ratio provides the highest scores.</p>
<p>Can simulators always understand LLM-generated environment configurations?We tested and analyzed the generated environments used in paper experiments (109 total) and found that the LLM (GPT-4-Turbo) did not generate any environments beyond what the Crafter or Heist simulators could handle.While we do not find any such case, even if the LLM generates an invalid setup, we constrain all LLM-generated settings in postprocessing to be within simulator capabilities to ensure no accidental errors in the simulator or environment generation.</p>
<p>Environment parameter naming: obscure vs. original.To determine whether or not the LLM in EnvGen is leveraging world knowledge when generating environments we conduct an analysis experiment.We replace environment parameter names from the original ones with obscure names (see Fig. 9), in order to remove the use of prior knowledge/expectation of how each parameter is useful to which skills.We find that the performance decreases, from 32.2 ± 0.6 → to 28.5 ± 0.6, indicating that the world knowledge/prior knowledge of the LLM is beneficial in helping the LLM generate more suitable environments.</p>
<p>Achievement Distillation + EnvGen.As mentioned in the Sec.4.1, we also experiment using EnvGen with the Achievement Distillation (AD) (Moon et al., 2023) agent.As shown in Fig. 8, similar to our results on the PPO-based agent, we find that by applying EnvGen, there is performance gain in long-horizon tasks like making iron tools and collecting diamonds.</p>
<p>C.2 Evaluation on Heist Environment</p>
<p>EnvGen can generalize to Heist.We also evaluate the effectiveness of EnvGen framework with Heist.We compare the PPO-based agent trained with and without EnvGen (i.e., Heist EnvGen environments).Table 8 shows that training an agent with Heist EnvGen environments is effective in improving performance by increasing average scores (25.9% → 37.7%) and rewards (4.1% → 5.5%), while also stabilizing training by reducing the score variance (i.e., standard deviation goes down 13.2% → 7.5%).</p>
<p>D Curriculum Learning Baseline Details</p>
<p>In the following, we describe the two baseline implementation details described in Table 2: easy-to-hard and adversarial curricula.</p>
<p>Easy-to-hard curriculum.Similar to Ammanabrolu et al. (2022), we create an easy-to-hard curriculum.An easy-to-hard curriculum has a pre-defined order of training environments.The agent first trains in "easy" environments and then by the end of the training process will be training in the "hard" environments.To do this, we first ask the LLM to generate a set of 16 random environments and train a validation agent (an agent only for the purpose of testing an environment difficulty; not used during final agent training) on each environment.Then the performance of the validation agent is measured and the environments are sorted from easiest to hardest (i.e., from environments that resulted in higher agent scores to lower agent scores).Then we train an agent on the top four easier environments first and for every 0.24M steps we replace the training environments with the next four environments in the sorted set (i.e., easy-to-hard curriculum).</p>
<p>Adversarial curriculum.Similar to Parker-Holder et al. (2022), we create an adversarial curriculum.The adversarial curriculum approach involves updating the agent's training environments to ones where it has struggled.To do this, we first generate a set of 16 random environments and train a validation agent on each environment.Then, we measure and sort the environments by difficulty (i.e., sorted from lowest to highest based on the validation agent's score).We take the top four hardest environments and train the final agent on these.Then, generate a new set of environments and test the current agent on this set, again sorting by difficulty.Then again, we take the top four hardest environments and resume training on these.This process then repeats four times (every 0.24M steps) creating an adversarial curriculum.</p>
<p>E RL Agent Implementation Details</p>
<p>PPO agent.We use the PPO-based (Schulman et al., 2017) agent used in (Moon et al., 2023), which modifies the default ResNet (He et al., 2016) architecture in IMPALA (Espeholt et al., 2018) by increasing channel size and hidden size and adding a layer normalization Ba et al.</p>
<p>(2016) before each linear/convolutional layer.We slightly modify this architecture further to place the layer norm after the final linear layer instead of before.Hyperparameters for this model are shown in Table 9.</p>
<p>Figure 1 :
1
Figure 1: Comparison of different methods for creating embodied agents.Previous works commonly use (a) small RL agents or (b) LLM agents to explore skills.In (c) EnvGen, we train a small RL agent with diverse LLM-generated environments that train different skills in parallel and can be adapted via feedback to help the agents progressively improve skills that they are weaker at.Our method benefits from using the world knowledge from LLMs while maintaining efficient training through a lightweight RL agent.</p>
<p>Here is a list of constraints: • natural biome will set the environment to have all the biomes • coal, iron, and diamond can only be found in a mountain biome ...</p>
<p>Figure 2 :
2
Figure 2: In EnvGen, we generate and adapt multiple training environments with an LLM to let the agent learn different skills effectively.EnvGen iterates over N Cycle cycles, each consisting of four steps (see Sec. 2.2).</p>
<p>Figure 3 :
3
Figure 3: (a): Crafter gameplay screenshot.An agent explores a 2D world and completes 22 achievements.(b): Crafter achievement hierarchy.Some achievements can be completed right away; others require previous achievements to be unlocked first (i.e., in a hierarchical order following the arrows).</p>
<p>Fig. 6 CFigure 4 :
64
Fig.6shows how the LLM adaptively generates new training environments based on the intermediate performance of our PPO-based RL agent.In the intermediate performance plots, we compare the baseline agent trained only in Crafter and our RL agent trained in Crafter EnvGen .In the cycle 2, given the feedback that the current RL agent is not good at collecting coal, the LLM generates an environment to help the agent focus on it, improving</p>
<p>Figure 5 :
5
Figure 5: Unlock times (the first moment when the agent completed an achievement) for three long-horizon achievements ('make stone pickaxe', 'make iron pickaxe', and 'make iron sword') of two PPO agents (Moon et al., 2023) -(1) Baseline: trained in Crafter for 1.96M steps, and (2) Ours: trained for 0.96M steps in Crafter EnvGen and for 1M steps in Crafter.The plot shows the last 1M training steps out of 1.96M steps.</p>
<p>Figure 7 :
7
Figure 7: (a): Heist gameplay screenshot.An agent aims to steal a gem (colored yellow), navigating a maze and colored opening locks.(b): Heist achievement hierarchy.The agent can only reach the gem after successively unlocking all locks in order.</p>
<p>Figure 8 :
8
Figure8: Success rates for all Crafter achievements of two Achievement Distillation (AD) agents(Moon et al., 2023): (1) Baseline: trained in Crafter for 1.96M steps, and (2) Ours: trained in Crafter EnvGen for 0.96M steps and Crafter for 1M steps.</p>
<h1></h1>
<p>Figure10: The prompts that are given to the LLM to generate environments in step 1 of Sec.2.2.</p>
<p>Custom Env 1 Custom Env 2 Custom Env N … LLM Env 1: Basic Resource Collection and Crafting</p>
<p>Table 1 :
1
(Wu et al., 2023))erent agents in the Crafter(Hafner, 2022)environment.Following previous works, we report the geometric mean of success rates across its 22 achievements and rewards for 1M Crafter steps.We experiment with EnvGen on two models, PPO and Achievement Distillation.<em>:scoresfrom the Crafter Scoreboard(Hafner, 2022)andMoon et al. (2023).†:averagenumber of LLM calls to run a single episode, according to SPRING(Wu et al., 2023).PT: Pretraining; AD: Achievement Distillation.
ModelsDescription# LLM calls # Agent Params Score (%)RewardHuman  *  Random  </em>50.5 ± 6.8 14.3 ± 2.3 1.6 ± 0.0 2.1 ± 1.3ELLM<em> (Du et al., 2023) LSTM-SPCNN  *  (Stanić et al., 2023) DreamerV3  *  (Hafner et al., 2023) MuZero + SPR  *  (Walker et al., 2023) 150M step PT in Crafter w/ RND reward 5M step PT in Crafter w/ Codex reward5M62M 135M 201M 54M-11.7 ± 0.8 9.3 ± 0.2 6.0 ± 0.4 14.8 ± 1.4 10.9 ± 0.5 16.4 ± 1.5 12.7 ± 0.4SPRING</em> (Wu et al., 2023)9 queries to call GPT-4 per step2.7K  †Unknown27.3 ± 1.2 12.3 ± 0.7PPO (Moon et al., 2023)4M15.5 ± 0.6 10.5 ± 0.6PPO (Moon et al., 2023)0.96M step PT in Crafter4M26.4 ± 2.1 12.1 ± 1.0AD* (Moon et al., 2023)9M21.8 ± 1.4 12.6 ± 0.3AD (Moon et al., 2023)0.96M step PT in Crafter9M31.8 ± 0.7 13.3 ± 1.2PPO + EnvGen (Ours)0.96M step PT w/ Crafter EnvGen44M32.2 ± 0.6 12.6 ± 0.6AD + EnvGen (Ours)0.96M step PT w/ Crafter EnvGen49M35.3 ± 0.7 13.7 ± 0.8
4.1 Comparison with State-of-the-art Methods on Crafter EnvironmentSmall RL agent trained with EnvGen outperforms state-of-the-art baselines.On the Crafter environment (described in Sec.3.1), we compare a small PPO agent trained in Crafter EnvGen (i.e., Crafter environments generated with EnvGen) to state-of-the-art baseline methods.As shown in Table</p>
<p>from Cycle 1 Performance from Cycle 3 Performance from Cycle 2
Collect coal is 22% Make stone pickaxe is 1%Trained in CrafterLLMTrained in Crafter EnvGen (Ours)Collect CoalMake Stone PickaxeCollect CoalMake Stone PickaxeTraining CurriculumScore (%)RewardFixed (no curriculum)29.9 ± 0.9 12.6 ± 0.8Easy-to-Hard26.8 ± 1.5 12.7 ± 0.7Adversarial26.8 ± 0.8 12.2 ± 0.7Adaptive+Dynamic Environments (EnvGen) 32.2 ± 0.6 12.6 ± 0.6
…PerformanceFigure6: Adaptation of training environments based on agent performance over EnvGen cycles.At the end of each cycle, the RL agent's performance is given to the LLM as feedback (e.g., 'Collect coal is 2%').The LLM uses the feedback to adaptively generate new environments that can help the agent progressively tackle skills it was previously weak at.</p>
<p>Table 2 :
2
Comparison of RL agents trained in Crafter</p>
<p>Table 2
2
Ammanabrolu et al. (2022)ained via EnvGen to the same agent trained with curriculum learning approaches such as an easy-to-hard curriculum, similar toAmmanabrolu et al. (2022)(i.e., pre-defined training environment order based on environment difficulty) and adversarial curriculum, similar to Parker-Holder et al. (2022) (i.e., updating to training environments that agent does worse in) in the Crafter environment.Detailed setups of both baseline approaches are in the appendix.The agent trained with EnvGen is able to achieve much higher performance (32.2% vs. 26.8%for both curricula) indicating the effectiveness EnvGen's approach of adaptively generating training environments to improve agent weak skills.The result indicates that creating more difficult environments does not necessarily help the agent learn new skills over time.
Environment Update Frequency # Training cycles N Cycle Score (%)RewardEvery 0.012M steps40 cycles30.8 ± 0.7 12.8 ± 0.6Every 0.06M steps8 cycles32.1 ± 0.5 12.7 ± 0.8Every 0.12M steps (default)4 cycles32.2 ± 0.6 12.6 ± 0.6</p>
<p>Table 3 :
3
Different frequencies to give feedback to the LLM and update the environments (see Sec. 2 for details).Agents are trained with 0.96M steps in Crafter EnvGen and 1M steps in Crafter environment.Table3shows that updating the LLM environments at every 0.12M steps results in the best agent performance.While increasing the cycles of environment feedback beyond 4 does not improve further, we find that updating environments with feedback always helps improve the RL agent's performance compared to training only with the original Crafter environment in Table 1 (26.4%) or the fixed LLM environment in Table 2 (29.9%).
Frequency of LLM feedback / environment updates.</p>
<p>(Wu et al., 2023));Ma et al. (2023)2024);Kwon et al. (2023);Ma et al. (2023);Du et al. (2023), have used LLMs to create/adjust rewards to train agents.Although these works show initial promising results leveraging the world knowledge of LLMs to tackle long-horizon tasks, iteratively calling LLMs throughout episodes is prohibitively slow and expensive (e.g., running a single episode in the Crafter environment with SPRING(Wu et al., 2023)costs around $270 USD as they have 2.7K LLM calls on average).EnvGen only calls LLMs a few times (e.g., 4 in total) to create training environments that focus on helping the RL agent progressively improve its weaker skills.</p>
<p>Table 7 :
7
(Hafner, 2022)Sec.3.2) on the Crafter(Hafner, 2022)benchmark (described in Sec.3.1) with 0.96M steps in Crafter EnvGen and average results for 30 runs (10 different seeds, 3 different initial environments).Different ratios of training steps in LLM-generated environments (Crafter EnvGen ) compared to training steps in the original Crafter environment (e.g., 2:1 indicates that for every two training steps in Crafter EnvGen , the RL agent gets one training step in Crafter).We keep the total number of training steps constant at 1.96M.
Ratio of Training Steps in Crafter EnvGen : Crafter Score (%)Reward5:130.3 ± 0.6 12.3 ± 0.92:130.1 ± 1.1 12.8 ± 0.71:1 (default)32.2 ± 0.6 12.6 ± 0.6Model# Training Steps in Heist EnvGen # Training Steps in HeistScore (%)RewardPPO-25M25.9 ± 13.2 4.1 ± 1.8PPO + EnvGen (Ours)5M20M37.7 ± 7.50 5.5 ± 0.9</p>
<p>Table 8 :
8
Evaluation results on Heist.Scores are computed as the average success rate over 100 test episodes over 10 different seeds.
Ratio</p>
<p>of training steps: LLM environments vs. original environment.</p>
<p>As mentioned in Sec.2.2, in EnvGen, we train the RL agent in LLM environments (step 2) and then in the original environment (step 3) to mitigate the agent from overfitting to the LLM environments.We experiment with different ratios of training steps in LLM environments (i.e., Crafter EnvGen ) compared to training steps in the original Crafter environment (e.g., 2:1 indicates that for every two training steps in Crafter EnvGen , the RL agent gets one training step in Crafter).As shown in Table</p>
<p>We find that N=4 works well; see Table6for details.
We find that T LLM-Env = T Orig-Env works well; see Table7for details.
AcknowledgmentsWe thank Elias Stengel-Eskin and the reviewers for the thoughtful discussion and feedback.This work was supported by DARPA ECOLE Program No. HR00112390060, NSF-AI Engage Institute DRL-2112635, DARPA Machine Commonsense (MCS) Grant N66001-19-2-4031, ARO Award W911NF2110220, ONR Grant N00014-23-1-2356, and a Bloomberg Data Science Ph.D. Fellowship.The views contained in this article are those of the authors and not of the funding agency.AppendixIn this appendix, we present additional related work (Appendix A), additional game environment details (Appendix B), additional experiment results (Appendix C), curriculum learning baseline method details (Appendix D), RL agent implementation details (Appendix E), additional LLM details (Appendix F), and limitations (Appendix G).  29.9 ± 0.9 12.6 ± 0.8GPT-3.5-Turbo (OpenAI, 2023b), and Deepseek Coder 33B Instruct(Guo et al., 2024)) and use N Cycle = 1 (i.e., fixed environment).Table5shows that environments generated by GPT-4-Turbo outperform that of other LLMs including GPT-3.5-Turbo and Deepseek Coder 33B Instruct.We see that GPT-3.5-Turboperforms the worst with only a score of 21.5%, while Deepseek 33B Instruct is able to get several points higher (26.3%) and GPT-4-Turbo, our default LLM, gets a few extra points (29.9%).# LLM environments Score (%) Reward 1 30.8 ± 0.5 12.8 ± 0.8 2 29.1 ± 0.6 13.0 ± 0.6 4 (default)32.2 ± 0.6 12.6 ± 0.6 8 31.0 ± 0.8 12.9 ± 0.8Before:Here is a list of parameters you can control when making an environment: target biome: grassland -mountain -beaches -natural coal rarity: very common -common -rare iron rarity: very common -common -rare diamond rarity: very common -common -rare tree rarity: very common -common -rareHere is a list of items the agent can start with: sapling: 0-9 wood: 0-9 stone: 0-9 coal: 0-9 iron: 0-9 diamond: 0-9 wood pickaxe: 0-1 stone pickaxe: 0-1 iron pickaxe: 0-1 wood sword: 0-1 stone sword: 0-1 iron sword: 0-1After:Here is a list of parameters you can control when making an environment: parameter1: optionA -optionB -optionC -optionD parameter2: optionE -optionF -optionG parameter3: optionE -optionF -optionG parameter4: optionE -optionF -optionG parameter5: optionE -optionF -optionGHere is a list of items the agent can start with: item1: 0-9 item2: 0-9 item3: 0-9 item4: 0-9 item5: 0-9 item6: 0-9 item7: 0-1 item8: 0-1 item9: 0-1 item10: 0-1 item11: 0-1 item12: 0-110.F Additional LLM DetailsPrompt Template.In Fig.10(a), we show the LLM prompt template that is used to generate environments.The contents of the prompt can vary slightly between different environments/games though generally remain the same.In Fig.10G LimitationsEnvGen relies on strong LLMs (e.g., GPT-4).But note that one of the main motivations of EnvGen is to more efficiently use LLMs to help train embodied agents, and as such EnvGen requires very few LLM calls (e.g., 4 calls), which only costs less than $1 USD during the entire training.We hope that advances in quantization/distillation and open-source models will make strong LLMs more accessible.EnvGen also requires that the environment simulators can (or be easily edited to) accept configurations in standard formats (e.g., JSON, CSV, YAML, TOML etc.), and the LLM can correctly generate configurations in such formats.Note that such text configuration formats are widely used for managing game simulators.In addition, many games have open-source community-driven efforts that provide high-level configuration documentation and settings, such as Minecraft wrappers(Guss et al., 2019;Fan et al., 2022)and Starcraft wrappers(Vinyals et al., 2017;DI-star Contributors, 2021).
Delf: Designing learning environments with foundation models. Aida Afshar, Wenchao Li, AAAI Workshop. 2024</p>
<p>Grounding Language in Robotic Affordances. Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Alex Hausman, Daniel Herzog, Jasmine Ho, Julian Hsu, Brian Ibarz, Alex Ichter, Eric Irpan, Rosario Jang, Kyle Jauregui Ruano, Sally Jeffrey, Jesmonth, J Nikhil, Ryan Joshi, Dmitry Julian, Yuheng Kalashnikov, Kuang Kuang, Sergey Huei Lee, Yao Levine, Linda Lu, Carolina Luu, Peter Parada, Jornell Pastor, Kanishka Quiambao, Jarek Rao, Diego Rettinghouse, Pierre Reyes, Nicolas Sermanet, Clayton Sievers, Alexander Tan, Vincent Toshev, Fei Vanhoucke, Ted Xia, Peng Xiao, Sichun Xu, Mengyuan Xu, Andy Yan, Zeng, CoRL. Not As I Say2022</p>
<p>Situated dialogue learning through procedural environment generation. Prithviraj Ammanabrolu, Renee Jia, Mark O Riedl, Association for Computational Linguistics (ACL). 2022</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, Yaguang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham ; Bryan, Parker Richter, Alex Riley, Aurko Castro Ros, Brennan Roy, Rajkumar Saeta, Renee Samuel, Ambrose Shelby, Daniel Slone, David R Smilkov, Daniel So, Simon Sohn, Dasha Tokumine, Vijay Valter, Kiran Vasudevan, Xuezhi Vodrahalli, Pidong Wang, Zirui Wang, Tao Wang, John Wang, Yuhuai Wieting, Kelvin Wu, Yunhan Xu, Linting Xu, Pengcheng Xue, Jiahui Yin, Qiao Yu, Steven Zhang, Ce Zheng, Weikang Zheng, Denny Zhou, Zhou, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,2023Slav Petrovand Yonghui Wu. Palm 2 technical report</p>
<p>Playing hard exploration games by watching YouTube. Yusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyu Wang, Nando De Freitas, NeurIPS2018</p>
<p>Layer Normalization. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, NIPS 2016 Deep Learning Symposium. 2016</p>
<p>Unifying count-based exploration and intrinsic motivation. G Marc, Sriram Bellemare, Georg Srinivasan, Tom Ostrovski, David Schaul, Rémi Saxton, Munos, NIPS. 2016</p>
<p>Language Models are Few-Shot Learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, NeurIPS2020</p>
<p>Exploration by Random Network Distillation. Yuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov, ICLR. 2018</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Josh Achiam, Vedant Misra, Felipe Petroski Such. Jan Leike,. 2021Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code</p>
<p>Minigrid &amp; miniworld: Modular &amp; customizable reinforcement learning environments for goal-oriented tasks. Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo De Lazcano, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, Jordan Terry, CoRR, abs/2306.138312023</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Shivani Dohan, Agrawal, Noah Fiedel. PaLM: Scaling Language Modeling with Pathways. JMLR. Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck2023</p>
<p>Leveraging procedural generation to benchmark reinforcement learning. Karl Cobbe, Chris Hesse, Jacob Hilton, John Schulman, Proceedings of the 37th International Conference on Machine Learning. Hal Daumé, Iii , Aarti Singh, the 37th International Conference on Machine LearningPMLRJul 2020119</p>
<p>Emergent complexity and zero-shot transfer via unsupervised environment design. Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, Sergey Levine, NIPS. 2020</p>
<p>Di-star: An open-sourse reinforcement learning framework for starcraftii. Di-Star Contributors, 2021</p>
<p>PaLM-E: An Embodied Multimodal Language Model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Yevgen Huang, Pierre Chebotar, Daniel Sermanet, Sergey Duckworth, Vincent Levine, Karol Vanhoucke, Marc Hausman, Klaus Toussaint, Andy Greff, Igor Zeng, Pete Mordatch, Florence, ICML 2023. 2023</p>
<p>Guiding Pretraining in Reinforcement Learning with Large Language Models. Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, Jacob Andreas, ICML. 2023</p>
<p>A survey of embodied ai: From simulators to research tasks. Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, Cheston Tan, 10.1109/TETCI.2022.3141105IEEE Transactions on Emerging Topics in Computational Intelligence. 622022</p>
<p>IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Boron Yotam, Firoiu Vlad, Harley Tim, Iain Dunning, Shane Legg, Koray Kavukcuoglu, ICML. 2018ISBN 9781510867963</p>
<p>Building Open-Ended Embodied Agents with Internet-Scale Knowledge. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An, Yuke Huang, Anima Zhu, Ut Anandkumar, Austin, Minedojo, 10.48550/arxiv.2206.08853NeurIPSjun 2022</p>
<p>Generative Adversarial Networks. Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, NIPS. 2014</p>
<p>Deepseek-coder: When the large language model meets programming -the rise of code intelligence. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, Y K Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang, 2024</p>
<p>MineRL: A large-scale dataset of minecraft demonstrations. H William, Brandon Guss, Nicholay Houghton, Phillip Topin, Cayden Wang, Manuela Codel, Ruslan Veloso, Salakhutdinov, 10.24963/ijcai.2019/339IJCAI, 2019. ISBN 9780999241141. </p>
<p>Benchmarking the spectrum of agent capabilities. Danijar Hafner, ICLR. 2022</p>
<p>Dream to Control: Learning Behaviors by Latent Imagination. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi, ICLR2020</p>
<p>Mastering Atari with Discrete World Models. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba, ICLR2021</p>
<p>Mastering Diverse Domains through World Models. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap, 2023</p>
<p>Deep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. 2016</p>
<p>Rainbow: Combining improvements in deep reinforcement learning. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, David Silver, 10.1609/aaai.v32i1.11796ISBN 9781577358008. 2018AAAI</p>
<p>Replay-Guided Adversarial Environment Design. Minqi Jiang, Jakob Foerster, Michael Dennis, Edward Grefenstette, Jack Parker-Holder, Tim Rocktäschel, NeurIPS. 2021ISBN 9781713845393</p>
<p>Large Language Models are Zero-Shot Reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, NeurIPS. 2022</p>
<p>Generating game levels for multiple distinct games with a common latent space. Bradford W Vikram Kumaran, James C Mott, Lester, 10.1609/aiide.v16i1.7485ISBN 9781577358497. 2020AIIDE</p>
<p>SCENECRAFT: Automating Interactive Narrative Scene Generation in Digital Games with Large Language Models. Jonathan Vikram Kumaran, Bradford Rowe, James Mott, Lester, 10.1609/aiide.v19i1.27504AIIDE. 2023157735883</p>
<p>Reward design with language models. Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh, International Conference on Learning Representations. 2023</p>
<p>Exploring long-horizon reasoning with deep RL in combinatorially hard tasks. Pashootan Andrew C Li, Rodrigo Vaezipoor, Sheila A Toro Icarte, Mcilraith, Decision Awareness in Reinforcement Learning Workshop at ICML 2022. 2022a</p>
<p>Auto mc-reward: Automated dense reward design with large language models for minecraft. Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024</p>
<p>Panogen: Text-conditioned panoramic environment generation for vision-and-language navigation. Jialu Li, Mohit Bansal, Advances in Neural Information Processing Systems. 2023</p>
<p>Envedit: Environment editing for vision-and-language navigation. Jialu Li, Hao Tan, Mohit Bansal, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022b</p>
<p>Deep learning for procedural content generation. Jialin Liu, Sam Snodgrass, Ahmed Khalifa, Sebastian Risi, Georgios N Yannakakis, Julian Togelius, 10.1007/s00521-020-05383-8Neural Comput. Appl. 0941-0643331jan 2021</p>
<p>Eureka: Human-level reward design via coding large language models. Jason Yecheng, William Ma, Guanzhi Liang, De-An Wang, Osbert Huang, Dinesh Bastani, Yuke Jayaraman, Linxi Zhu, Anima Fan, Anandkumar, ArXiv, abs/2310.129312023</p>
<p>. Mojang Studios, Minecraft, 2009</p>
<p>Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning. Seungyong Moon, Junyoung Yeom, Bumsoo Park, Hyun Oh Song, NeurIPS. 2023</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show Your Work: Scratchpads for Intermediate Computation with Language Models. 2021</p>
<p>. OpenAI. Gpt-4 technical report. ArXiv. 2575328152023a</p>
<p>. Openai, Chatgpt, 2023b</p>
<p>Evolving Curricula with Regret-Based Environment Design. Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, Tim Rocktäschel, ICML. 2022</p>
<p>TOAD-GAN: A Flexible Framework for Few-Shot Level Generation in Token-Based Games. Frederik Schubert, Maren Awiszus, Bodo Rosenhahn, 10.1109/TG.2021.3069833IEEE Transactions on Games. 1422022</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, Proximal Policy Optimization Algorithms. 2017</p>
<p>Planning to explore via self-supervisedworld models. Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak, ICML. 20209781713821120</p>
<p>Procedural Content Generation in Games. Noor Shaker, Julian Togelius, Mark J Nelson, 2016Springer Publishing Company3319427148Incorporated. 1st edition</p>
<p>Learning to generalize with object-centric agents in the open world survival game crafter. Aleksandar Stanić, Yujin Tang, David Ha, Schmidhuber, IEEE Transactions on Games. 2023</p>
<p>MarioGPT: Open-Ended Text2Level Generation through Large Language Models. Shyam Sudhakaran, Miguel González-Duque, Claire Glanois, Matthias Freiberger, Elias Najarro, Sebastian Risi, NeurIPS2023</p>
<p>Reinforcement Learning: An Introduction. Richard S Sutton, Andrew G Barto, 2018The MIT Press2 edition</p>
<p>Level Generation Through Large Language Models. Graham Todd, Sam Earle, Muhammad Umair Nasir, Michael Cerny Green, Julian Togelius, 10.1145/3582437.3587211FDG, 2023. ISBN 9781450398565. </p>
<p>Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, 2023a</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023bAurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Sasha Alexander, Michelle Vezhnevets, Alireza Yeo, Makhzani, John Heinrich K Üttler, Julian Agapiou, John Schrittwieser, Stephen Quan, Stig Gaffney, Karen Petersen, Tom Simonyan, Schaul, David Hado Van Hasselt, Timothy Silver, Kevin Lillicrap, Paul Calderone, Anthony Keet, David Brunasso, Anders Lawrence, Jacob Ekermo, Rodney Repp, Tsing, Starcraft ii: A new challenge for reinforcement learning. 2017</p>
<p>Investigating the Role of Model-Based Learning in Exploration and Transfer. Jacob Walker, Eszter Vértes, Yazhe Li, Gabriel Dulac-Arnold, Ankesh Anand, Théophane Weber, Jessica B Hamrick, ICML. 2023</p>
<p>Voyager: An Open-Ended Embodied Agent with Large Language Models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, 2023a</p>
<p>ByteSized32: A corpus and challenge task for generating task-specific world models expressed as text games. Ruoyao Wang, Graham Todd, Xingdi Yuan, Ziang Xiao, Marc-Alexandre C Ôté, Peter Jansen, 10.18653/v1/2023.emnlp-main.830Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeDecember 2023bAssociation for Computational Linguistics</p>
<p>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents. Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang, NeurIPS. 2023c</p>
<p>Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, Yitao Liang, JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models. 2023d</p>
<p>Scaling data generation in vision-and-language navigation. Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, Yu Qiao, ICCV. 2023e</p>
<p>Learning from Delayed Rewards. J C H Christopher, Watkins, May 1989University of Cambridge, EnglandPhD thesis</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, NeurIPS. 2022</p>
<p>Exploration strategies in deep reinforcement learning. lilianweng.github.io. Lilian Weng, Jun 2020</p>
<p>SPRING: Studying the Paper and Reasoning to Play Games. Yue Wu, Shrimai Prabhumoye, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Tom Mitchell, Yuanzhi Li, NeurIPS. 2023</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, ICLR. 2023</p>
<p>Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks. Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, Zongqing Lu, Foundation Models for Decision Making Workshop at NeurIPS. 2023</p>
<p>See and Think: Embodied Agent in Virtual Environment. Zhonghan Zhao, Wenhao Chai, Xuan Wang, Li Boyi, Shengyu Hao, Shidong Cao, Tian Ye, Jenq-Neng Hwang, Gaoang Wang, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
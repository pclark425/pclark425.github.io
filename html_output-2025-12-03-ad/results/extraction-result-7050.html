<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7050 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7050</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7050</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-aa9ad5cd399434bd7e7d6fcd1ddb8f4b58b953a3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/aa9ad5cd399434bd7e7d6fcd1ddb8f4b58b953a3" target="_blank">Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> A synthetic corpus based approach for language models (LMs) to acquire logical deductive reasoning ability is studied and a well-grounded set of deduction rules based on formal logic theory is adopted, which can derive any other deduction rules when combined in a multistep way.</p>
                <p><strong>Paper Abstract:</strong> We study a synthetic corpus based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary, limiting the generalizability of acquired reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. Then, using the proposed corpora, which we name FLD (Formal Logic Deduction), we first evaluate and analyze the logical reasoning ability of the latest LLMs. Even GPT-4 can solve only half of the problems, suggesting that pure logical reasoning isolated from knowledge is still challenging for the LLMs, and additional training specialized in logical reasoning is indeed essential. We next empirically verify that LMs trained on FLD corpora acquire more generalizable reasoning ability. Furthermore, we identify the aspects of reasoning ability on which deduction corpora can enhance LMs and those on which they cannot, and discuss future directions on each aspect. The released corpora serve both as learning resources and as challenging benchmarks.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7050.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7050.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large multimodal generative pre-trained transformer from OpenAI evaluated in a few-shot, in‑context setting with chain‑of‑thought instruction for strict deductive tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based LLM by OpenAI used in few-shot in-context evaluation with an added chain-of-thought instruction prompting step-by-step proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (large LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Few-shot in-context learning with chain-of-thought prompting (prompt: "Show me a step-by-step thought...")</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FLD (Formal Logic Deduction) / FLD*</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Synthetic corpora of multistep deductive proofs constructed from formal logic axioms (FLD) and a higher-depth variant (FLD*); facts are counterfactual so knowledge recall is unhelpful.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Stepwise proof generation and final label prediction (proved/disproved/unknown) from given facts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Proof accuracy (strict exact proof+answer match) and answer accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Proof accuracy: 12.8% (FLD), 3.2% (FLD*); Answer accuracy: 52.4% (FLD), 49.4% (FLD*) — 10-shot in-context</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Far below a T5 prover fine-tuned on the corpus (T5(fine‑tuned) proof accuracy 75.8% on FLD v2, 44.4% on FLD* in the same 10‑shot setting), showing few-shot GPT-4 underperforms fine-tuned smaller models on strict logical proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 in few-shot CoT rarely generates faithful formal proofs; while its answer accuracy can be ~50%, strict proof accuracy is very low, and it often produces unfaithful or logically invalid intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Fails on counterfactual formal-logic tasks that disallow reliance on world knowledge; common failures include logical hallucination (conclusions not deducible), selecting irrelevant distractor facts, misinterpreting logical operators (notably double negation), and rarely producing 'unknown' despite insufficient premises.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7050.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7050.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI GPT-3.5 family model evaluated few-shot with chain-of-thought instruction; performed poorly on strict synthetic formal-logic proof generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based conversational LLM used in 10-shot in-context evaluations with CoT-like instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (large LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Few-shot in-context learning with chain-of-thought instruction</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FLD / FLD*</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Synthetic deductive reasoning corpora based on formal logic axioms, including higher-depth variant FLD*.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof generation and answer labeling (proved/disproved/unknown)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Proof accuracy and answer accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Proof accuracy: 0.0% (FLD), 2.0% (FLD*); Answer accuracy: 35.8% (FLD), 37.6% (FLD*) — 10-shot</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Substantially worse than a fine-tuned T5 prover (e.g., T5 fine-tuned proof accuracy 75.8% on FLD v2), showing poor few-shot transfer to strictly formal deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5-Turbo largely fails to produce correct proofs in strict formal-logic synthetic tasks even with CoT prompting; answer-level performance only slightly above random for some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Unable to adhere to deduction rules in counterfactual synthetic tasks; generates incorrect or unfaithful proofs and is sensitive to distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7050.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7050.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LongAlpaca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LongAlpaca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B parameter LLM variant (LongAlpaca) evaluated few-shot for proof generation with chain-of-thought prompting; showed negligible proof accuracy on FLD.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LongAlpaca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B-parameter transformer LLM variant adapted for long-contexts (referenced Chen et al., 2023) evaluated in 10-shot CoT-style prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (long-context adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Few-shot in-context learning with chain-of-thought instruction</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FLD / FLD*</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Synthetic formal-logic deduction corpora (FLD and high-depth FLD*).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof generation and answer labeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Proof accuracy and answer accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Proof accuracy: 0.0% (FLD and FLD*); Answer accuracy: 21.2% (FLD), 19.6% (FLD*) — 10-shot</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Performs worse than fine-tuned T5 provers and also worse than larger closed LLMs (GPT-3.5/GPT-4) on answer accuracy in these strict formal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LongAlpaca-13B produced essentially no correct proofs under few-shot CoT on formal logic synthetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not handle counterfactual, strictly formal deduction tasks without fine-tuning; fails to produce valid proof sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7050.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7050.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5 Stepwise Prover (Yang et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-based stepwise proof generator (prover) from Yang et al. (2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative stepwise proof model based on T5 that produces one proof step at a time until the hypothesis is proved/disproved/unknown; used as the primary experimental model and fine-tuned on synthetic corpora (FLD, RuleTaker variants).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-stepwise-prover</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A generative T5 model (T5-base for synthetic-corpus transfer experiments; T5-large for EntailmentBank) that generates one proof step per decoding iteration, concatenating premises and the derived conclusion as an explicit natural-language proof sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5-base / T5-large (as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (T5) generative stepwise proof generation</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained/fine-tuned on synthetic deduction corpora: FLD variants (sFLD, FLD.*), RuleTaker variants (RT, RT.PR, RT.D5) and then fine-tuned on EntailmentBank for transfer experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Supervised fine-tuning on synthetic proof corpora for explicit stepwise natural-language proof generation; uses forced generation of final answer label token appended to proof.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>For EntailmentBank experiments, a RoBERTa-based proof-step verifier (from Yang et al., 2022) was used to improve accuracy of generated proofs; otherwise no external theorem prover or SAT solver was integrated.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FLD, RuleTaker (RT/RT.PR), sFLD variants, EntailmentBank (EB)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>FLD: synthetic corpus built from first-order logic axioms; RuleTaker: synthetic implication-based deduction corpus; EntailmentBank: human-authored multi-step entailment trees with distractors and retrieval noise.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multistep deductive proof generation (natural-language proof steps) and final answer labeling (proved/disproved/unknown)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Proof accuracy (strict exact proof+answer match), Answer accuracy; EntailmentBank 'AllCorrect' metric for proofs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Selected results: Fully fine-tuned proof accuracy — RT: 92.4%, RT.PR: 93.9%; FLD.v2: 75.8% (FLD), FLD*.v2: 44.4% (higher-depth FLD*). Few-shot transferred prover (trained on sFLD) average proof accuracy across synthetic corpora: 81.3% (best among sources). On EntailmentBank Task1/2/3: baseline T5 = 36.8%/31.2%/6.2%; RT.D5 = 39.4%/32.0%/8.2%; FLD.D5 = 39.2%/32.6%/8.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Fine-tuning on FLD-style corpora substantially improves transfer/generalization compared to no synthetic-corpus pretraining (T5 baseline): e.g., EntailmentBank Task1 improved ≈ +2.4% (RT.D5) / +2.4% (FLD.D5) over T5 baseline; provers trained on sFLD or FLD variants generalized better to other deduction corpora than provers trained on narrower rule-sets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Stepwise T5 provers fine-tuned on FLD learn a more generalizable set of deduction rules (axioms) and outperform baselines on cross-corpus transfer; however, they still struggle to construct very many-step/high-depth proofs (especially when axioms require many steps), and training on axioms does not automatically resolve depth/ search-combinatorics issues.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Difficulty scaling to high-depth proofs due to combinatorial explosion of possible proof trees (∼A^d); needs verifier for real-world EB tasks; cannot fully exploit axioms to express coarse-grained deduction rules because constructing many fine-grained steps remains challenging; trained models sometimes generate overclaims or miss premises.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7050.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7050.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa proof verifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-based proof step verifier (as used in Yang et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RoBERTa classifier used to verify/rerank generated proof steps, employed for improving performance on the human-authored EntailmentBank benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-based verifier</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A RoBERTa encoder fine-tuned as a verifier to score/verdict individual proof steps or step candidates to assist the generative prover on noisy/human-authored tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer encoder (RoBERTa) used as a verifier/reranker</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained on labeled proof-step data (as in Yang et al., 2022) for verification; used during EntailmentBank fine-tuning/evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Discriminative verification of generated proof steps to improve final proof accuracy (verifier-guided search/selection)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Acts as an external verifier component in the proof-generation pipeline to select/accept valid steps; not a formal theorem prover.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>EntailmentBank (EB)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Human-authored multi-step entailment trees with distractors and retrieval noise; requires looser, real-world entailment reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof-step verification / proof selection to support generated natural-language proofs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used in combination with prover; final metric reported is EB 'AllCorrect' proof accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Not reported separately for verifier alone; used to achieve reported EntailmentBank proof accuracies (e.g., FLD.D5 achieved 39.2% on Task1 when combined with verifier).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Using a verifier-guided search improves final proof accuracy on noisy human-authored tasks compared to pure generative prover alone (as adopted from Yang et al., 2022 and used in this study).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A discriminative verifier is a helpful component for transferring synthetic-corpus-trained provers to EntailmentBank; it helps avoid some overclaiming and incorrect step choices.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Verifier is not a formal-proof-checker and depends on supervised data; it cannot fully compensate for the prover's inability to construct many-step axiomatic proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7050.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7050.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLD (Formal Logic Deduction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Formal Logic Deduction (FLD) corpus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic dataset generation framework introduced by this paper that builds multistep deductive proof examples from the axioms of first‑order predicate logic, with randomly assigned natural-language statements to force reliance on deduction rules rather than world knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLD (dataset/corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Synthetic corpus generator that constructs proof trees by randomly composing formal-logic axioms, assigns randomized natural-language templates and distractors, and produces labeled examples (proved/disproved/unknown); configurable for tree depth, formula complexity, distractors, and linguistic diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>N/A (dataset generation framework)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not applicable — FLD is the training/evaluation corpus used to fine-tune provers; variants include sFLD, FLD.0..FLD.4, FLD.D5 (high depth), FLD* (depth up to 8)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Provides supervised examples for learning deductive reasoning from first-order logic axioms via end-to-end supervised fine-tuning of generative provers</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FLD (itself) and FLD* (higher-depth variant)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Synthetic formal-logic proof benchmarks designed to test rigid deductive reasoning isolated from world knowledge; options control axioms vs implication-only rules, linguistic diversity, formula complexity, distractors, and depth.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>First-order deductive proof generation and answer labeling (proved/disproved/unknown); used both as training supervision and as a challenging benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Proof accuracy and answer accuracy when used to train/evaluate LMs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>When used to fine-tune T5 provers: FLD.v2 fully fine-tuned proof accuracy 75.8%; FLD*.v2 (deep) 44.4%. Training on FLD variants yielded better cross-corpus transfer (sFLD avg. proof accuracy 81.3 across synthetic corpora in few-shot transfer experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Training on FLD (axioms) generalized better to other deduction corpora than training on narrower rule-sets (e.g., RuleTaker), and improved performance on EntailmentBank over the no-synthetic baseline (T5): EB Task1 improved from 36.8% to ~39.2-39.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>FLD's use of axioms (complete set) teaches more generalizable deduction rules enabling better transfer; synthetic training on FLD helps models learn semantics of logical operators and robustness to distractors, but is insufficient alone to overcome deep/many-step proof search challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>FLD examples are counterfactual and semantically random by design (so results isolate logic but may miss interactions with commonsense knowledge); models trained on FLD still struggle with constructing very many-step proofs required to express coarse-grained rules using axioms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7050.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7050.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EntailmentBank (EB)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explaining answers with entailment trees (EntailmentBank)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human-authored benchmark of multi-step entailment trees where each step can be a rough natural-language entailment rather than a rigid formal logic step; used to test transfer of synthetic-corpus-trained provers to real-world reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explaining answers with entailment trees</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EntailmentBank (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A dataset of human-authored explanation trees for question answering where proofs are natural-language entailments; divided into tasks with/without distractors and with retrieved supporting sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>N/A (dataset/benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Serves as an evaluation benchmark for models trained on synthetic deduction corpora to test real-world transfer</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>EntailmentBank (Task1 / Task2 / Task3)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Three tasks: Task1 (no distractors), Task2 (with distractors), Task3 (with retrieved sentences, more realistic/richer noise).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Human-authored multi-step entailment / proof generation and retrieval-in-the-loop reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AllCorrect proof accuracy (official EB metric)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Baseline T5: Task1 36.8%, Task2 31.2%, Task3 6.2%; after pretraining on RT.D5: 39.4% / 32.0% / 8.2%; after pretraining on FLD.D5: 39.2% / 32.6% / 8.3% (T5-large provers fine-tuned on EB).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Pretraining on synthetic deduction corpora (RT.D5 or FLD.D5) yields modest improvements over baseline T5 on EB (≈ +2–2.6% on Task1; similar modest gains on Task2/3).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Synthetic-corpus training (including FLD) transfers to human-authored real-world entailment trees but gains are modest; difficulty arises because EB steps can be coarser-grained and require many fine-grained axiomatic steps to emulate.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Models still fail on many EB examples even after synthetic pretraining due to challenges in constructing long sequences of axiomatic steps, integrating commonsense knowledge, and selecting premises from retrieved noisy facts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generating natural language proofs with verifier-guided search <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>Explaining answers with entailment trees <em>(Rating: 2)</em></li>
                <li>PRover: Proof generation for interpretable reasoning over rules <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>GPT-4 Technical Report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7050",
    "paper_id": "paper-aa9ad5cd399434bd7e7d6fcd1ddb8f4b58b953a3",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "Large multimodal generative pre-trained transformer from OpenAI evaluated in a few-shot, in‑context setting with chain‑of‑thought instruction for strict deductive tasks in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large transformer-based LLM by OpenAI used in few-shot in-context evaluation with an added chain-of-thought instruction prompting step-by-step proof generation.",
            "model_size": null,
            "architecture_type": "Transformer (large LLM)",
            "training_data": null,
            "reasoning_method": "Few-shot in-context learning with chain-of-thought prompting (prompt: \"Show me a step-by-step thought...\")",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "FLD (Formal Logic Deduction) / FLD*",
            "benchmark_description": "Synthetic corpora of multistep deductive proofs constructed from formal logic axioms (FLD) and a higher-depth variant (FLD*); facts are counterfactual so knowledge recall is unhelpful.",
            "task_type": "Stepwise proof generation and final label prediction (proved/disproved/unknown) from given facts",
            "performance_metric": "Proof accuracy (strict exact proof+answer match) and answer accuracy",
            "performance_value": "Proof accuracy: 12.8% (FLD), 3.2% (FLD*); Answer accuracy: 52.4% (FLD), 49.4% (FLD*) — 10-shot in-context",
            "comparison_with_baseline": "Far below a T5 prover fine-tuned on the corpus (T5(fine‑tuned) proof accuracy 75.8% on FLD v2, 44.4% on FLD* in the same 10‑shot setting), showing few-shot GPT-4 underperforms fine-tuned smaller models on strict logical proof generation.",
            "key_findings": "GPT-4 in few-shot CoT rarely generates faithful formal proofs; while its answer accuracy can be ~50%, strict proof accuracy is very low, and it often produces unfaithful or logically invalid intermediate steps.",
            "limitations": "Fails on counterfactual formal-logic tasks that disallow reliance on world knowledge; common failures include logical hallucination (conclusions not deducible), selecting irrelevant distractor facts, misinterpreting logical operators (notably double negation), and rarely producing 'unknown' despite insufficient premises.",
            "uuid": "e7050.0",
            "source_info": {
                "paper_title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "GPT-3.5-Turbo",
            "brief_description": "OpenAI GPT-3.5 family model evaluated few-shot with chain-of-thought instruction; performed poorly on strict synthetic formal-logic proof generation tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "Transformer-based conversational LLM used in 10-shot in-context evaluations with CoT-like instruction.",
            "model_size": null,
            "architecture_type": "Transformer (large LLM)",
            "training_data": null,
            "reasoning_method": "Few-shot in-context learning with chain-of-thought instruction",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "FLD / FLD*",
            "benchmark_description": "Synthetic deductive reasoning corpora based on formal logic axioms, including higher-depth variant FLD*.",
            "task_type": "Proof generation and answer labeling (proved/disproved/unknown)",
            "performance_metric": "Proof accuracy and answer accuracy",
            "performance_value": "Proof accuracy: 0.0% (FLD), 2.0% (FLD*); Answer accuracy: 35.8% (FLD), 37.6% (FLD*) — 10-shot",
            "comparison_with_baseline": "Substantially worse than a fine-tuned T5 prover (e.g., T5 fine-tuned proof accuracy 75.8% on FLD v2), showing poor few-shot transfer to strictly formal deduction.",
            "key_findings": "GPT-3.5-Turbo largely fails to produce correct proofs in strict formal-logic synthetic tasks even with CoT prompting; answer-level performance only slightly above random for some settings.",
            "limitations": "Unable to adhere to deduction rules in counterfactual synthetic tasks; generates incorrect or unfaithful proofs and is sensitive to distractors.",
            "uuid": "e7050.1",
            "source_info": {
                "paper_title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "LongAlpaca-13B",
            "name_full": "LongAlpaca-13B",
            "brief_description": "A 13B parameter LLM variant (LongAlpaca) evaluated few-shot for proof generation with chain-of-thought prompting; showed negligible proof accuracy on FLD.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LongAlpaca-13B",
            "model_description": "13B-parameter transformer LLM variant adapted for long-contexts (referenced Chen et al., 2023) evaluated in 10-shot CoT-style prompts.",
            "model_size": "13B",
            "architecture_type": "Transformer (long-context adaptation)",
            "training_data": null,
            "reasoning_method": "Few-shot in-context learning with chain-of-thought instruction",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "FLD / FLD*",
            "benchmark_description": "Synthetic formal-logic deduction corpora (FLD and high-depth FLD*).",
            "task_type": "Proof generation and answer labeling",
            "performance_metric": "Proof accuracy and answer accuracy",
            "performance_value": "Proof accuracy: 0.0% (FLD and FLD*); Answer accuracy: 21.2% (FLD), 19.6% (FLD*) — 10-shot",
            "comparison_with_baseline": "Performs worse than fine-tuned T5 provers and also worse than larger closed LLMs (GPT-3.5/GPT-4) on answer accuracy in these strict formal tasks.",
            "key_findings": "LongAlpaca-13B produced essentially no correct proofs under few-shot CoT on formal logic synthetic tasks.",
            "limitations": "Does not handle counterfactual, strictly formal deduction tasks without fine-tuning; fails to produce valid proof sequences.",
            "uuid": "e7050.2",
            "source_info": {
                "paper_title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "T5 Stepwise Prover (Yang et al., 2022)",
            "name_full": "T5-based stepwise proof generator (prover) from Yang et al. (2022)",
            "brief_description": "Generative stepwise proof model based on T5 that produces one proof step at a time until the hypothesis is proved/disproved/unknown; used as the primary experimental model and fine-tuned on synthetic corpora (FLD, RuleTaker variants).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "T5-stepwise-prover",
            "model_description": "A generative T5 model (T5-base for synthetic-corpus transfer experiments; T5-large for EntailmentBank) that generates one proof step per decoding iteration, concatenating premises and the derived conclusion as an explicit natural-language proof sequence.",
            "model_size": "T5-base / T5-large (as used in experiments)",
            "architecture_type": "Transformer (T5) generative stepwise proof generation",
            "training_data": "Trained/fine-tuned on synthetic deduction corpora: FLD variants (sFLD, FLD.*), RuleTaker variants (RT, RT.PR, RT.D5) and then fine-tuned on EntailmentBank for transfer experiments",
            "reasoning_method": "Supervised fine-tuning on synthetic proof corpora for explicit stepwise natural-language proof generation; uses forced generation of final answer label token appended to proof.",
            "external_tool_used": true,
            "external_tool_description": "For EntailmentBank experiments, a RoBERTa-based proof-step verifier (from Yang et al., 2022) was used to improve accuracy of generated proofs; otherwise no external theorem prover or SAT solver was integrated.",
            "benchmark_name": "FLD, RuleTaker (RT/RT.PR), sFLD variants, EntailmentBank (EB)",
            "benchmark_description": "FLD: synthetic corpus built from first-order logic axioms; RuleTaker: synthetic implication-based deduction corpus; EntailmentBank: human-authored multi-step entailment trees with distractors and retrieval noise.",
            "task_type": "Multistep deductive proof generation (natural-language proof steps) and final answer labeling (proved/disproved/unknown)",
            "performance_metric": "Proof accuracy (strict exact proof+answer match), Answer accuracy; EntailmentBank 'AllCorrect' metric for proofs",
            "performance_value": "Selected results: Fully fine-tuned proof accuracy — RT: 92.4%, RT.PR: 93.9%; FLD.v2: 75.8% (FLD), FLD*.v2: 44.4% (higher-depth FLD*). Few-shot transferred prover (trained on sFLD) average proof accuracy across synthetic corpora: 81.3% (best among sources). On EntailmentBank Task1/2/3: baseline T5 = 36.8%/31.2%/6.2%; RT.D5 = 39.4%/32.0%/8.2%; FLD.D5 = 39.2%/32.6%/8.3%.",
            "comparison_with_baseline": "Fine-tuning on FLD-style corpora substantially improves transfer/generalization compared to no synthetic-corpus pretraining (T5 baseline): e.g., EntailmentBank Task1 improved ≈ +2.4% (RT.D5) / +2.4% (FLD.D5) over T5 baseline; provers trained on sFLD or FLD variants generalized better to other deduction corpora than provers trained on narrower rule-sets.",
            "key_findings": "Stepwise T5 provers fine-tuned on FLD learn a more generalizable set of deduction rules (axioms) and outperform baselines on cross-corpus transfer; however, they still struggle to construct very many-step/high-depth proofs (especially when axioms require many steps), and training on axioms does not automatically resolve depth/ search-combinatorics issues.",
            "limitations": "Difficulty scaling to high-depth proofs due to combinatorial explosion of possible proof trees (∼A^d); needs verifier for real-world EB tasks; cannot fully exploit axioms to express coarse-grained deduction rules because constructing many fine-grained steps remains challenging; trained models sometimes generate overclaims or miss premises.",
            "uuid": "e7050.3",
            "source_info": {
                "paper_title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "RoBERTa proof verifier",
            "name_full": "RoBERTa-based proof step verifier (as used in Yang et al., 2022)",
            "brief_description": "A RoBERTa classifier used to verify/rerank generated proof steps, employed for improving performance on the human-authored EntailmentBank benchmark.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoBERTa-based verifier",
            "model_description": "A RoBERTa encoder fine-tuned as a verifier to score/verdict individual proof steps or step candidates to assist the generative prover on noisy/human-authored tasks.",
            "model_size": null,
            "architecture_type": "Transformer encoder (RoBERTa) used as a verifier/reranker",
            "training_data": "Trained on labeled proof-step data (as in Yang et al., 2022) for verification; used during EntailmentBank fine-tuning/evaluation",
            "reasoning_method": "Discriminative verification of generated proof steps to improve final proof accuracy (verifier-guided search/selection)",
            "external_tool_used": true,
            "external_tool_description": "Acts as an external verifier component in the proof-generation pipeline to select/accept valid steps; not a formal theorem prover.",
            "benchmark_name": "EntailmentBank (EB)",
            "benchmark_description": "Human-authored multi-step entailment trees with distractors and retrieval noise; requires looser, real-world entailment reasoning.",
            "task_type": "Proof-step verification / proof selection to support generated natural-language proofs",
            "performance_metric": "Used in combination with prover; final metric reported is EB 'AllCorrect' proof accuracy",
            "performance_value": "Not reported separately for verifier alone; used to achieve reported EntailmentBank proof accuracies (e.g., FLD.D5 achieved 39.2% on Task1 when combined with verifier).",
            "comparison_with_baseline": "Using a verifier-guided search improves final proof accuracy on noisy human-authored tasks compared to pure generative prover alone (as adopted from Yang et al., 2022 and used in this study).",
            "key_findings": "A discriminative verifier is a helpful component for transferring synthetic-corpus-trained provers to EntailmentBank; it helps avoid some overclaiming and incorrect step choices.",
            "limitations": "Verifier is not a formal-proof-checker and depends on supervised data; it cannot fully compensate for the prover's inability to construct many-step axiomatic proofs.",
            "uuid": "e7050.4",
            "source_info": {
                "paper_title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "FLD (Formal Logic Deduction)",
            "name_full": "Formal Logic Deduction (FLD) corpus",
            "brief_description": "A synthetic dataset generation framework introduced by this paper that builds multistep deductive proof examples from the axioms of first‑order predicate logic, with randomly assigned natural-language statements to force reliance on deduction rules rather than world knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FLD (dataset/corpus)",
            "model_description": "Synthetic corpus generator that constructs proof trees by randomly composing formal-logic axioms, assigns randomized natural-language templates and distractors, and produces labeled examples (proved/disproved/unknown); configurable for tree depth, formula complexity, distractors, and linguistic diversity.",
            "model_size": null,
            "architecture_type": "N/A (dataset generation framework)",
            "training_data": "Not applicable — FLD is the training/evaluation corpus used to fine-tune provers; variants include sFLD, FLD.0..FLD.4, FLD.D5 (high depth), FLD* (depth up to 8)",
            "reasoning_method": "Provides supervised examples for learning deductive reasoning from first-order logic axioms via end-to-end supervised fine-tuning of generative provers",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "FLD (itself) and FLD* (higher-depth variant)",
            "benchmark_description": "Synthetic formal-logic proof benchmarks designed to test rigid deductive reasoning isolated from world knowledge; options control axioms vs implication-only rules, linguistic diversity, formula complexity, distractors, and depth.",
            "task_type": "First-order deductive proof generation and answer labeling (proved/disproved/unknown); used both as training supervision and as a challenging benchmark.",
            "performance_metric": "Proof accuracy and answer accuracy when used to train/evaluate LMs",
            "performance_value": "When used to fine-tune T5 provers: FLD.v2 fully fine-tuned proof accuracy 75.8%; FLD*.v2 (deep) 44.4%. Training on FLD variants yielded better cross-corpus transfer (sFLD avg. proof accuracy 81.3 across synthetic corpora in few-shot transfer experiments).",
            "comparison_with_baseline": "Training on FLD (axioms) generalized better to other deduction corpora than training on narrower rule-sets (e.g., RuleTaker), and improved performance on EntailmentBank over the no-synthetic baseline (T5): EB Task1 improved from 36.8% to ~39.2-39.4%.",
            "key_findings": "FLD's use of axioms (complete set) teaches more generalizable deduction rules enabling better transfer; synthetic training on FLD helps models learn semantics of logical operators and robustness to distractors, but is insufficient alone to overcome deep/many-step proof search challenges.",
            "limitations": "FLD examples are counterfactual and semantically random by design (so results isolate logic but may miss interactions with commonsense knowledge); models trained on FLD still struggle with constructing very many-step proofs required to express coarse-grained rules using axioms.",
            "uuid": "e7050.5",
            "source_info": {
                "paper_title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "EntailmentBank (EB)",
            "name_full": "Explaining answers with entailment trees (EntailmentBank)",
            "brief_description": "Human-authored benchmark of multi-step entailment trees where each step can be a rough natural-language entailment rather than a rigid formal logic step; used to test transfer of synthetic-corpus-trained provers to real-world reasoning.",
            "citation_title": "Explaining answers with entailment trees",
            "mention_or_use": "use",
            "model_name": "EntailmentBank (dataset)",
            "model_description": "A dataset of human-authored explanation trees for question answering where proofs are natural-language entailments; divided into tasks with/without distractors and with retrieved supporting sentences.",
            "model_size": null,
            "architecture_type": "N/A (dataset/benchmark)",
            "training_data": null,
            "reasoning_method": "Serves as an evaluation benchmark for models trained on synthetic deduction corpora to test real-world transfer",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "EntailmentBank (Task1 / Task2 / Task3)",
            "benchmark_description": "Three tasks: Task1 (no distractors), Task2 (with distractors), Task3 (with retrieved sentences, more realistic/richer noise).",
            "task_type": "Human-authored multi-step entailment / proof generation and retrieval-in-the-loop reasoning",
            "performance_metric": "AllCorrect proof accuracy (official EB metric)",
            "performance_value": "Baseline T5: Task1 36.8%, Task2 31.2%, Task3 6.2%; after pretraining on RT.D5: 39.4% / 32.0% / 8.2%; after pretraining on FLD.D5: 39.2% / 32.6% / 8.3% (T5-large provers fine-tuned on EB).",
            "comparison_with_baseline": "Pretraining on synthetic deduction corpora (RT.D5 or FLD.D5) yields modest improvements over baseline T5 on EB (≈ +2–2.6% on Task1; similar modest gains on Task2/3).",
            "key_findings": "Synthetic-corpus training (including FLD) transfers to human-authored real-world entailment trees but gains are modest; difficulty arises because EB steps can be coarser-grained and require many fine-grained axiomatic steps to emulate.",
            "limitations": "Models still fail on many EB examples even after synthetic pretraining due to challenges in constructing long sequences of axiomatic steps, integrating commonsense knowledge, and selecting premises from retrieved noisy facts.",
            "uuid": "e7050.6",
            "source_info": {
                "paper_title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generating natural language proofs with verifier-guided search",
            "rating": 2
        },
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2
        },
        {
            "paper_title": "Explaining answers with entailment trees",
            "rating": 2
        },
        {
            "paper_title": "PRover: Proof generation for interpretable reasoning over rules",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 1
        }
    ],
    "cost": 0.02074625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic</h1>
<p>Terufumi Morishita ${ }^{1}$ Gaku Morio ${ }^{1}$ Atsuki Yamaguchi ${ }^{1}$ Yasuhiro Sogawa ${ }^{1}$</p>
<h4>Abstract</h4>
<p>We study a synthetic corpus based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary, limiting the generalizability of acquired reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. Then, using the proposed corpora, which we name FLD (Formal Logic Deduction), we first evaluate and analyze the logical reasoning ability of the latest LLMs. Even GPT-4 can solve only half of the problems, suggesting that pure logical reasoning isolated from knowledge is still challenging for the LLMs, and additional training specialized in logical reasoning is indeed essential. We next empirically verify that LMs trained on FLD corpora acquire more generalizable reasoning ability. Furthermore, we identify the aspects of reasoning ability on which deduction corpora can enhance LMs and those on which they cannot, and discuss future directions on each aspect. The released corpora serve both as learning resources and as challenging benchmarks.</p>
<h2>1. Introduction</h2>
<p>Building a machine that reasons with abundant knowledge has been the Holy Grail since the early era of artificial intelligence (McCarthy, 1959). Recent language models (LMs) have taken a step toward this goal, as they demonstrated extensive factual and commonsense knowledge obtained through large-scale pre-training. Even so, LMs still struggle with logical reasoning (Askell, 2020; Rae et al., 2021; Razeghi et al., 2022; Liu et al., 2023; Turpin et al., 2023;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Lanham et al., 2023; Wu et al., 2023; Hodel \&amp; West, 2023; Dziri et al., 2023; Dasgupta et al., 2023), as it demands rigid manipulation of logical rules rather than merely referring to the existing knowledge.</p>
<p>LMs have acquired their knowledge from a lot of highquality examples in human-written texts (Devlin et al., 2019). Conversely, their poor logical reasoning ability suggests the lack of high-quality examples of logical reasoning in the texts. This is not a surprise given that humans usually think reflexively rather than logically step by step (Kahneman, 2011). The consideration here suggests a straightforward strategy to endow LMs with logical reasoning ability: create corpora that include many examples of valid logical reasoning, and train LMs on them.</p>
<p>One such corpus is the recently proposed RuleTaker (Clark et al., 2021). RuleTaker is composed of synthetically generated examples of multistep deductive reasoning.A deduction example requires an LM to generate logical steps to (dis)prove a given hypothesis based on a given set of facts. Each logical step must follow deduction rules of the implication kind, such as $\forall x F(x) \rightarrow G(x), F(a) \vdash G(a)$ (here, $\vdash$ means "derives"). The facts are randomly constructed except the logical structure (i.e., they have no semantics), therefore referring to existing knowledge never helps solve the task. Artificial Argument Corpus (AACorpus) (Betz et al., 2021) is another synthetic corpus composed of singlestep deduction examples. AACorpus adopted a set of handselected deduction rules useful for critical thinking, such as contraposition $\mathcal{F} \rightarrow \mathcal{G} \vdash \neg \mathcal{G} \rightarrow \neg \mathcal{F}$ ( $\neg$ is negation). All these corpora teach deductive reasoning, one of the most universally used logical reasoning.</p>
<p>However, it is still an open question whether this research direction will genuinely lead to the improvement of deductive reasoning ability. First, the deduction rules used in the previous corpora were limited or otherwise arbitrary. This can limit the generalizability of the acquired deductive reasoning ability since complex real-world reasoning can require various deduction rules. Second, it has not yet been studied what aspect of deductive reasoning ability deduction corpora can enhance LMs. Such aspects include the ability to solve many-step reasoning, and an understanding of diverse linguistic expressions of logical statements. This investigation is essential to discuss the future directions.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A deduction example generated by FLD. Given a set of facts and a hypothesis, an LM is required to generate proof steps to (dis-)prove the hypothesis and an answer. The proof is constructed from a theoretically well-grounded set of deduction rules (i.e., the axioms of first-order predicate logic). Note that the facts are randomly constructed except logical structures (i.e., they have no semantics) so that referring to existing knowledge never helps solve the task.</p>
<p>This paper aims to answer these questions. First, we rethink the choice of deduction rules by leveraging the formal logic theory (Section 2). According to formal logic, there are infinite valid deduction rules, including but not limited to the ones used in the previous corpora. However, among them, there is a set of atomic deduction rules called the axioms, and any other valid deduction rules can be derived by multistep deductions constructed from the axioms (completeness). Therefore, the axioms are the most generalizable to various deduction rules. As the sets of deduction rules used in the previous corpora lack this property, we propose a new framework named FLD (Formal Logic Deduction), which generates deduction examples constructed from the axioms.</p>
<p>Then, we first investigate how well current (L)LMs perform logical reasoning (Section 5). We find that even the most powerful LLM, GPT-4 (OpenAI, 2023), can solve only half of the problems. More fine-grained analyses reveal several phenomena, notably that the LLMs do not reason faithfully following the "reasoning steps" they themselves generate. Overall, the results here suggest that pure logical reasoning isolated from knowledge is still challenging for latest LLMs, and additional training specialized in logical reasoning should be essential.</p>
<p>Next, we show that the training on FLD is effective (Section 6). We trained LMs on FLD and measured their performance on two types of benchmarks: one is deduction corpora themselves and the other is human-authored EntailmentBank (EB) (Dalvi et al., 2021), which requires more complex real-world reasoning. The resulst are promising as the LMs outperformed baselines on both benchmarks.</p>
<p>Finally, we identify the aspects of deductive reasoning ability on which deduction corpora are beneficial (Section 7). We analyzed each aspect separately by using a comprehensive set of "ablation corpora", where one corpus emphasizes a specific aspect different from those emphasized by the other corpora. The results suggest that deduction corpora are beneficial in many aspects, but they alone are not enough for some aspects. Finally, on the basis of the results, we discuss the future directions for each aspect (Section 8).</p>
<p>We summarize our contributions as follows:</p>
<ul>
<li>We propose ${ }^{1}$ a deduction corpus generation framework FLD (Section 3). FLD is the first to leverage formal logic theory, adopting a well-grounded set of deduction rules that generalizes the best to other deduction rules.</li>
<li>We evaluate and analyze the logical reasoning ability of latest LLMs (Section 5). Even GPT-4 can solve only half of the problems, suggesting that pure logical reasoning isolated from knowledge is still challenging for LLMs.</li>
<li>We empirically verify that LMs trained on FLD corpora acquire more generalizable deductive reasoning ability than the baselines without such training (Section 6).</li>
<li>We analyze each aspect of deductive reasoning and provide the future directions for applying deduction corpora or other approaches for them (Sections 7 and 8).</li>
<li>We release ${ }^{1}$ the FLD corpora, which serve both as learning resources and as challenging benchmarks.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>2. Preliminaries: Formal Logic</h2>
<p>Let us consider the following single-step deductive reasoning:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">The Earth revolves</th>
<th style="text-align: center;">If the Earth revolves around the sun,</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">around the sun</td>
<td style="text-align: center;">the Earth has seasons.</td>
</tr>
</tbody>
</table>
<p>This deduction step derives the conclusion, written under the bar, from the two premises. Next, consider another step:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">The Earth revolves</th>
<th style="text-align: center;">If the Earth revolves around the sun,</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">around the sun</td>
<td style="text-align: center;">the Earth does not have seasons.</td>
</tr>
</tbody>
</table>
<p>The Earth does not have seasons.
In this step, one of the premises (i.e., "If the Earth revolves around the sun, the Earth does not have seasons") is false. However, if the premise had been true, we can still derive the conclusion. Thus, in formal logic, this step is still valid the same as (1). We can abstract (1) and (2) into a deduction rule as: $\frac{\mathcal{F}}{\mathcal{G}} \rightarrow \mathcal{G}$ modus ponens</p>
<p>The deduction rule of this form is called modus ponens.
While modus ponens is the most intuitive deduction rule, many others exist. For example, a famous syllogism is:</p>
<p>$$
\frac{(\mathcal{F} \rightarrow \mathcal{G}) \wedge(\mathcal{G} \rightarrow \mathcal{H})}{\mathcal{F} \rightarrow \mathcal{H}} \text { syllogism }
$$</p>
<p>The other example below defines the meaning of $\wedge$ formally:</p>
<p>$$
\frac{(\mathcal{F} \wedge \mathcal{G})}{\mathcal{F}} \frac{(\mathcal{F} \wedge \mathcal{G})}{\mathcal{G}} \wedge \text {-elimination }
$$</p>
<p>Of course, we can consider invalid ${ }^{2}$ rules, in the sense that the conclusion is not logically deducible from the premises, such as:</p>
<p>$$
\frac{\mathcal{F}}{\mathcal{G}} \quad(\mathcal{F} \vee \mathcal{G})
$$</p>
<p>Now, from these examples, we obtain some important points of deductive reasoning. First, deductive reasoning can be defined as a form of thought in which a conclusion is derived from a set of premises following specific deduction rules, such as the ones in (1) to (6). Such deduction rules are called arguments in formal logic theory. Second, whether a deduction rule is valid or not does not depend on contents of symbols but only on the superficial form of the symbolic sequence composed of the premises to the conclusion. For example, as stated above, (3) is valid regardless of the actual content of $\mathcal{G}$, such as $\mathcal{G}=$ "(.. ), the Earth has seasons." in (1) and $\mathcal{G}=$ " $(\ldots)$, the Earth does not have seasons." in (2). This enables us to regard all deduction rules simply as symbolic rules such as (3) to (6). Third and as one conclusion of the second point, the symbols such as $\mathcal{F}$ and $\mathcal{G}$ can be arbitrary compounds of other formulas such as $\mathcal{F}=(A \wedge B)$ and $\mathcal{F}=\forall x, A(x) \rightarrow B(x)$. Finally, since we can consider</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An example of multistep deduction constructed from the axioms. (Left) shows the derivation of a syllogism. (Right) illustrates that deduction with more steps can express deductions that use a syllogism as a given rule.
infinite patterns of formulas as premises and a conclusion, we have infinite patterns of deduction rules (including both valid and invalid deduction rules).</p>
<p>Next, we consider multistep deductions. Figure 2 shows that syllogism rule can be derived by the multistep deduction constructed from other "atomic" deduction rules. (For other examples, Figure B. 5 shows the derivations of the deduction rules used in the previous corpora.) Indeed, in formal logic, there is a set of atomic deduction rules called the axioms (listed in Figure B.4a), and the following is known ${ }^{3}$ :
Theorem 2.1 (Completeness of first-order predicate logic (Gödel, 1930)). Any valid ${ }^{4}$ deduction rule is derivable by multistep deduction constructed from the axioms. Furthermore, any deduction rule derivable by multistep deduction constructed from the axioms is valid.
Here we have come to the core of formal logic: multistep deduction constructed from the axioms. Thanks to the completeness, all valid deduction rules can be derived in this way. As a consequence, multistep deduction constructed from the axioms can express multistep deduction constructed from any other deduction rules, as illustrated in Figure 2 (right).</p>
<h2>3. Generating Formal Logic Deduction Corpus</h2>
<p>The previous deduction corpora (Clark et al., 2021; Betz et al., 2021) used limited or arbitrary sets of deduction rules. However, as we saw in Section 2, the axioms should be the most generalizable to various deduction rules. Thus, we propose a framework named FLD (Formal Logic Deduction), which generates examples of multistep deduction constructed from the axioms.</p>
<p>Another important feature of FLD is that the statements in the examples are constructed randomly except logical structures (i.e., they have no semantics), so that referring to existing knowledge never helps solve the task, but only adhering to deduction rules solves the task. Finally, we designed FLD to be highly flexible as in Table 1, so that we</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An overview of the proposed framework FLD, which generates logical deduction examples constructed from the axioms of first-order predicate logic. FLD is flexible to generate various patterns of corpora for analysis. Note that the actual natural language assignments are constructed as random as possible to assess rigid logical reasoning ability isolated from knowledge (see Section 3).</p>
<p>Table 1: A comparison of FLD with the previous studies. FLD is flexible to generate various patterns of corpora for analysis. ✓ means controllable and extensible by an external template file. ✓ means controllable by an option.</p>
<table>
<thead>
<tr>
<th></th>
<th>Deduction Rules</th>
<th>Proof Tree Depth (upto)</th>
<th>Proof Tree Branches</th>
<th>Formula Complexity</th>
<th># of Distractors (up to)</th>
<th>Linguistic Diversity</th>
<th>Proof Labels</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuleTaker (Clark et al., 2021)</td>
<td>implication</td>
<td>5</td>
<td>A few</td>
<td>complex</td>
<td>~20</td>
<td>less (RuleTaker) / more (ParaRules)</td>
<td>provable / disprovable / unknown</td>
</tr>
<tr>
<td>AACorpus (Betz et al., 2021)</td>
<td>✓ (default = critical thinking)</td>
<td>1</td>
<td>1</td>
<td>✓ (simple / complex)</td>
<td>0</td>
<td>✓ (default = less)</td>
<td>provable / disprovable</td>
</tr>
<tr>
<td>FLD</td>
<td>✓ (default = the axioms)</td>
<td>✓ (can be any)</td>
<td>✓ (can be any)</td>
<td>✓ (simple / complex)</td>
<td>✓ (can be any)</td>
<td>✓ (default = more)</td>
<td>✓ (can choose any)</td>
</tr>
</tbody>
</table>
<p>can generate and analyze various patterns of corpora.</p>
<p>We show generated examples in Figures 1 and C.6. Below, we overview each module. For intuitive understanding, refer to the corresponding part of Figure 3. For the detailed implementations, refer to Appendix E.</p>
<h3>3.1. Proof Tree Generation via Random Forward-/Backward- Deduction</h3>
<p>RuleTaker (Clark et al., 2021) generates deductive proof trees by first randomly generating various formulas and second running a logical solver library on them to find occasionally emerged deductive relationships among them. However, since we rely on an external solver, we cannot specify the set of deduction rules used in proof trees (and thus we cannot specify the axioms, especially). Further, since we rely on the randomness, we cannot control the complexity of a proof tree, i.e., the depth and the number of leaves.</p>
<p>Thus, we decided to take another approach. We invented a module ("Proof Tree Generator" in Figure 3) that generates a proof tree through a random deduction process by using a set of deduction rules specified by a user. A user can specify the deduction rules in a template rule file, as exemplified in Figure E.7. At each forward- or backward- deduction step, the module randomly chooses one deduction rule and joints it to the current proof tree ("forward" and "backward" in the figure). The numbers of forward- and backward- steps control the tree's depth and number of leaves, respectively.</p>
<p>Once the structure of the proof tree is constructed, we construct the compound formulas at the tree nodes, such as $\mathcal{F}, \mathcal{G}$. Since these formulas are arbitrary (Section 2), we randomly combine atomic formulas such as $A$ and $B$ using logical operators $\wedge, \vee, \neg$. To avoid over complications, we limit the number of atomic formulas in each compound formula up to three. The resulting formulas are like $\mathcal{F}=(\neg A \wedge B)$.</p>
<h3>3.2. Factual Distractor Generation</h3>
<p>In a realistic scenario of logical reasoning, since the facts are collected by possibly incomplete retrieval systems rather than given, LMs have to correctly choose only the relevant facts under the existence of many irrelevant facts. To imitate this scenario, we add distractor facts to each deduction example ("Factual Distractor Generator" in Figure 3). The distractor facts are formulas that are similar to the gold facts in their logical form. For example, for the gold fact $(A \wedge B) \rightarrow C$, formulas such as $(A \wedge C) \rightarrow B$ can be distractors. We also implemented several other types of distractors and use the mixture of them.</p>
<h3>3.3. Natural Language Assignment</h3>
<p>We assign one natural language sequence to each formula of tree nodes and of distractors ("Natural Language Assigner" in Figure 3). Inspired by Betz et al. (2021), we take a template based approach. For each formula, we prepare several templates via an external template file (exemplified in Figure E.8) such as follows:</p>
<p>$$
\begin{gathered}
A \rightarrow B: \text { "If A, then B.", "A leads to B." } \
F(a) \rightarrow G(b): \text { "If a F, then b G.", "When a F, b G." }
\end{gathered}
$$</p>
<p>Then, we randomly choose one from them. Note that since the templates can be nested, the number of resulting patterns are combinatorially diverse.</p>
<p>Next, we assign natural language statements to atomic components such as $A, B, F, G, a, b$. Here, we come back to the important point in deductive reasoning discussed in Section 2: that the validity of deduction does not depend on contents of formulas, or in other words, the same deduction can be conducted on the same formulas regardless of their contents. To reflect this point, we assign a random statement constructed (under a certain grammatical constraint) from a full vocabulary to each atomic component; for example:</p>
<p>$$
\begin{aligned}
&amp; A: \text { "an Earthquake occurs" } B: \text { "the year ends" } \
&amp; F: \text { "run" } G: \text { "answer" } a: \text { "the hamburger" } b: \text { "Peter" }
\end{aligned}
$$</p>
<p>These randomly constructed statements ensure that referring to existing knowledge never helps solve the task, but only adhering to deduction rules solves the task.</p>
<h3>3.4. Deduction Example Conversion</h3>
<p>We finally make a deductive reasoning example from the outputs of the previous modules ("Deduction Example Converter" in Figure 3). A deduction example is composed of a set of facts, a hypothesis, a proof sequence, and an answer ("proved", "disproved", or "unknown"). This module can make an example of any answer label as follows. For answer label "proved", (i) we use the root node as the hypothesis, (ii) we use the leaf nodes of the proof tree and the distractors as the fact set, and (iii) we use the internal nodes of the proof tree as the proof sequence. For answer label
"disproved", we use the negated statement of the root node as the hypothesis so that the hypothesis is disproved by the proof sequence. For answer label "unknown", we randomly drop some of the leaf nodes so that the hypothesis cannot be proved or disproved by the proof sequence.</p>
<h2>4. Experiments</h2>
<p>We conducted experiments to verify the effectiveness of FLD, and to identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs. To this end, we examined various deduction corpora shown in Table 2. We trained LMs on the deduction corpora and measured their performance on relevant benchmarks. For reference, we also measured the performance of a LM (T5) without training on the deduction corpora. We used two types of benchmarks: deduction corpora themselves and human-authored EntailmentBank (Dalvi et al., 2021). We briefly explain the setup. See Appendix F for the details.</p>
<h3>4.1. Model</h3>
<p>All the experiments involve generating a proof sequence to (dis-)prove a given hypothesis from a given set of facts. To tackle the task of this type, we adopt the stepwise prover model from Yang et al. (2022). This prover is a generative model based on T5 (Raffel et al., 2020), which generates one proof step at a time. A proof step represents the chosen premises and the derived (generated) conclusion, such as "fact1 \&amp; fact3 -&gt; The Earth has seasons". The prover continues the generation until the given hypothesis is (dis-)proved. Finally, the prover outputs an answer labeled as "proved," "disproved," or "unknown."</p>
<p>We also evaluated large language models in a few-shot incontext learning setting. Specifically, we evaluated GPT-4, GPT-3.5-Turbo (OpenAI, 2023), and LongAlpaca-13B (Chen et al., 2023). Each in-context example is a pair of a prompt and an output, as illustrated in Figure 1. We also added a chain-of-thought (Kojima et al., 2022) instruction as "Show me a step-by-step thought to the hypothesis based on the given set of facts." We ran experiments for 10 different sets of in-context examples.</p>
<h3>4.2. Few-shot Transfer to Synthetic Deduction Corpora</h3>
<p>The first benchmark is the deduction corpora, which measures rigid logical reasoning ability. We trained prover LM on a corpus and measured its performance on another corpus. If LMs have acquired robust deductive reasoning ability, they should transfer well with a small number of examples. To see this, we used few-shot setting ${ }^{5}$.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: The corpora examined in this paper. For RuleTaker (“RT”), we used the OWA version introduced by <em>Tafjord et al. (2021)</em>. To align conditions as closely as possible across the corpora being compared, we (i) generated multiple FLD corpora using the options and template files and (ii) added several preprocessings to RuleTaker. See Appendix F.1 for details.</p>
<table>
<thead>
<tr>
<th>name</th>
<th>deduction rules</th>
<th>distractors (up to)</th>
<th>linguistic diversity</th>
<th>formula complexity</th>
<th>tree depth</th>
<th>tree depth distribution</th>
<th># train examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>RT (“D0-D3”)</td>
<td>implication</td>
<td>$\sim 20$</td>
<td>less</td>
<td>complex</td>
<td>1–3</td>
<td></td>
<td>30k</td>
</tr>
<tr>
<td>RT.PR (“ParaRules”)</td>
<td>implication</td>
<td>$\sim 20$</td>
<td>more</td>
<td>complex</td>
<td>1–5</td>
<td></td>
<td>30k</td>
</tr>
<tr>
<td>RT.BE (“Birds-Electricity”)</td>
<td>implication</td>
<td>$\sim 20$</td>
<td>more</td>
<td>complex</td>
<td>1–3</td>
<td></td>
<td>- (test only)</td>
</tr>
<tr>
<td>sFLD-impl</td>
<td>implication</td>
<td>$\sim 20$</td>
<td>less</td>
<td>complex</td>
<td>1–3</td>
<td></td>
<td>30k</td>
</tr>
<tr>
<td>sFLD-crit</td>
<td>critical thinking</td>
<td>$\sim 20$</td>
<td>less</td>
<td>complex</td>
<td>1–1</td>
<td></td>
<td>30k</td>
</tr>
<tr>
<td>sFLD-axiom (sFLD)</td>
<td>the axioms</td>
<td>$\sim 20$</td>
<td>less</td>
<td>complex</td>
<td>1–3</td>
<td></td>
<td>30k</td>
</tr>
<tr>
<td>RT.D5 (“D0-D5”)</td>
<td>implication</td>
<td>$\sim 20$</td>
<td>less</td>
<td>complex</td>
<td>1–5</td>
<td></td>
<td>30k</td>
</tr>
<tr>
<td>FLD.D5</td>
<td>the axioms</td>
<td>$\sim 20$</td>
<td>less</td>
<td>complex</td>
<td>1–5</td>
<td></td>
<td>30k</td>
</tr>
<tr>
<td>FLD-impl.0</td>
<td>implication</td>
<td>$\sim 20$</td>
<td>less</td>
<td>complex</td>
<td>1–3</td>
<td></td>
<td>30k</td>
</tr>
<tr>
<td>FLD-impl.1</td>
<td>implication</td>
<td>$\sim 20$</td>
<td>less</td>
<td>complex</td>
<td>1–8</td>
<td></td>
<td>30k</td>
</tr>
<tr>
<td>FLD.0</td>
<td>the axioms</td>
<td>0</td>
<td>less</td>
<td>complex</td>
<td>1–3</td>
<td></td>
<td>30k</td>
</tr>
<tr>
<td>FLD.1</td>
<td>the axioms</td>
<td>$\sim 20$</td>
<td>less</td>
<td>simple</td>
<td>1–3</td>
<td></td>
<td>30k</td>
</tr>
<tr>
<td>FLD.2</td>
<td>the axioms</td>
<td>$\sim 20$</td>
<td>less</td>
<td>complex</td>
<td>1–3</td>
<td></td>
<td>30k</td>
</tr>
<tr>
<td>FLD.3 (FLD)</td>
<td>the axioms</td>
<td>$\sim 20$</td>
<td>more</td>
<td>complex</td>
<td>1–3</td>
<td></td>
<td>30k</td>
</tr>
<tr>
<td>FLD.4 (FLD+)</td>
<td>the axioms</td>
<td>$\sim 20$</td>
<td>more</td>
<td>complex</td>
<td>1–8</td>
<td></td>
<td>30k</td>
</tr>
</tbody>
</table>
<p>We measure the performance of the prover on the test split of the target corpus using two types of metrics, answer accuracy and proof accuracy <em>(Saha et al., 2020)</em>. Answer accuracy measures whether predicted answers, each of which is chosen from “proved”, “disproved” and “unknown”, are correct or not. Proof accuracy measures whether <em>both</em> the predicted answers and the generated proofs are correct or not, offering a stricter and more faithful assessment of reasoning ability. Our proof accuracy metric is stricter than the original by <em>Saha et al. (2020)</em> (details in our repository). We focus our discussion on proof accuracy results, with answer accuracy results presented in Appendix G.1.</p>
<p>We trained prover LM (T5-base) on the training split of each source corpus for 20k steps with a batch size of 64 and learning rate of 1e-4. Then we fine-tuned the prover LM on 1% subset (300 examples) of the training split of the target corpus.</p>
<h3>4.3. Transfer to EntailmentBank</h3>
<p>EntailmentBank (EB) <em>(Dalvi et al., 2021)</em> is a recently proposed challenging benchmark. The proof trees in the EB dataset are human-authored rather than synthetically generated. Further, each proof step can be rough entailment instead of a rigid logical step. Thus, EB measures logical reasoning ability in a more real-world scenario.</p>
<p>We used all the three tasks of EB, which differ in the property of a given fact set: Task1 does not include distractors, Task2 includes distractors, and Task3 includes sentences retrieved from worldTree V2 <em>(Xie et al., 2020)</em>.</p>
<p>As stated above, the nature of proof steps in EB differs much from the nature of those in deduction corpora. Thus, it is difficult for prover LMs trained on deduction corpora to transfer to EB with a small number of examples. Thus, we fine-tuned the provers using all the EB examples.</p>
<p>We trained a prover LM (T5-large) on a source deduction corpus for 10k steps and fine-tuned it on each EB corpus for 10k steps. For all the training, the batch size was 64 and the learning rate was 5e-5, except EB-task2 where the learning rate of 2.5e-5 was used. For EB-task3, we used the prover trained on task2, following <em>Dalvi et al. (2021)</em>. Given the challengingness of EB, we used the additional RoBERTa <em>(Liu et al., 2019)</em> based proof step verifier proposed in <em>Yang et al. (2022)</em>. We measured the performance of the provers on the test split of EB by the official metric of “AllCorrect” proof accuracy <em>(Dalvi et al., 2021)</em>.</p>
<h2>5. How Well do LMs Solve Logic?</h2>
<h3>5.1. Performance of LMs in Fine-tuned Setting</h3>
<p>Table 3: Proof accuracy of a prover fully fine-tuned using all the dataset examples on each corpus. Note that we released the updated, version 2 of FLD corpora, on which the provers perform slightly better (See Appendix H for details).</p>
<table>
<thead>
<tr>
<th>RuleTaker</th>
<th>FLD</th>
</tr>
</thead>
<tbody>
<tr>
<td>RT</td>
<td>RT.PR</td>
</tr>
<tr>
<td>92.4</td>
<td>93.9</td>
</tr>
</tbody>
</table>
<p>First, we show how well LMs solve logic of each deduction corpus (Table 3). As shown, while the fully fine-tuned provers performed well on RuleTaker, they performed poorer on FLD. One possible reason is as follows. First, since a proof tree is constructed from the combination of a</p>
<p>deduction rule chosen at each level of the tree, the number of possible proof tree patterns can be estimated (very roughly) as $\mathcal{O}\left(\mathcal{A}^{d}\right)$, where $\mathcal{A}$ is the number of deduction rule choices and $d$ is the proof tree depth. Next, while RuleTaker uses only a few deduction rules $(\mathcal{A}=2)$ of implication type shown in Figure B.4b, FLD uses various deduction rules $(\mathcal{A} \sim 10)$ of the axioms shown in Figure B.4a. Thus, FLD includes exponentially more diverse patterns of proof trees, which makes FLD more challenging. Indeed, when we enlarge the maximum tree depth from $d=3$ to $d=8$ (FLD to FLD*), the corpus became extremely more challenging due to the exponentially more diverse proof tree patterns. See Appendix G for further detailed analysis.</p>
<h3>5.2. Performance of LLMs in Few-shot Setting</h3>
<p>Table 4: Performances of LLMs in a 10-shot in-context learning setting. Chain-of-thought-like instruction was also used. For reference, the performances of random guesses and T5 fine-tuned on all the 30,000 examples are also shown. We used the version 2 corpora (See Appendix H).</p>
<table>
<thead>
<tr>
<th></th>
<th>proof accuracy</th>
<th></th>
<th>answer accuracy</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>FLD</td>
<td>FLD*</td>
<td>FLD</td>
<td>FLD*</td>
</tr>
<tr>
<td>random guess</td>
<td>0.0</td>
<td>0.0</td>
<td>33.3</td>
<td>33.3</td>
</tr>
<tr>
<td>T5(fine-tuned)</td>
<td>75.8</td>
<td>44.4</td>
<td>91.6</td>
<td>72.2</td>
</tr>
<tr>
<td>LongAlpaca-13B</td>
<td>0.0</td>
<td>0.0</td>
<td>21.2</td>
<td>19.6</td>
</tr>
<tr>
<td>GPT-3.5-Turbo</td>
<td>0.0</td>
<td>2.0</td>
<td>35.8</td>
<td>37.6</td>
</tr>
<tr>
<td>GPT-4</td>
<td>12.8</td>
<td>3.2</td>
<td>52.4</td>
<td>49.4</td>
</tr>
</tbody>
</table>
<p>Table 4 show the performance of LLMs measured under 10shot settings. As seen from the proof accuracy results, even the most powerful LLM, GPT-4, performed very poorly. Further, even when measured by answer accuracy, which is more lenient than proof accuracy, GPT-4 solved only half of the problems.</p>
<p>The challengingness of FLD could be attributed to its counterfactual nature. The facts in the deduction examples are randomly constructed except for logical structure (i.e., they have no semantics). Due to this nature, an LLM can never rely on its pre-acquired knowledge to solve the task, but it has to adhere to the deduction rules. Such a kind of task should not be covered by pre-training on human-written texts. The poor performance of LLMs under the counterfactual setting is consistent with the recent observations by (Razeghi et al., 2022; Wu et al., 2023; Hodel \&amp; West, 2023; Dasgupta et al., 2023).</p>
<p>As can be seen, the answer accuracy performance deviated much from the proof accuracy performance. This gap can be partly explained by the "random guess factor", but not entirely. We analyzed the proofs generated by the LLMs and found that they sometimes generated a correct answer
with an incorrect proof. This suggests that the LLMs do not always faithfully follow the "logical steps" they themselves generate. These results align with recent findings by Turpin et al. (2023); Lanham et al. (2023).</p>
<p>Finally, we manually analyzed and categorized the common errors of GPT-4, as follows. The first is logical hallucination, where the generated conclusion is not logically deducible from the chosen facts. Second, GPT-4 sometimes chooses facts from distractors that are irrelevant to the proof. Taking a closer look, GPT-4 occasionally misinterprets logical operators, especially double-negation $\neg \neg$. Interestingly, GPT-4 seldom answered "unknown", i.e., it answered "proved" or "disproved" even if the given facts are not sufficient to either prove or disprove the hypothesis. Overall, these errors suggest that LLMs still miss the fundamentals of logical reasoning.</p>
<h2>6. How Effective is Formal Logic Deduction?</h2>
<h3>6.1. Benchmarking by Deduction Corpora</h3>
<p>Table 5: Few-shot proof accuracies of provers transferred among sFLD and baseline corpora. For fair comparison, all the corpora have the same depth distribution (except sFLDcrit that cannot form multistep easily, see Appendix F.1)</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Source corpus</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">RT</td>
<td style="text-align: center;">RT.PR</td>
<td style="text-align: center;">sFLD-impl</td>
<td style="text-align: center;">sFLD-crit</td>
<td style="text-align: center;">sFLD</td>
</tr>
<tr>
<td style="text-align: center;">Target corpus</td>
<td style="text-align: center;">RT</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">76.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RT.PR</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">72.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RT.BE</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">85.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sFLD-impl</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">80.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sFLD-crit</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">93.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sFLD</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">79.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">avg.</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">81.3</td>
</tr>
</tbody>
</table>
<p>We trained a prover on a deduction corpus ("source corpus") and measured its performance on other corpora ("target corpus") (Table 5). The prover trained on sFLD performed the best on average, and as seen from the corpus-wise results, the prover transferred the most robustly to the other corpora while the provers trained on the other corpora did not exhibit this level of robustness. Since the corpora used in Table 5 differ in the set of deduction rules used in proofs, this result suggests that the prover trained sFLD generalized the most to other deduction rules.</p>
<p>The reason for this strongest generalizability should be the following. (s)FLD corpora teach LMs how to construct multistep deductions using the axioms. Thanks to the completeness, the axioms can express multistep deductions constructed from any other deduction rules (including the ones used in the other corpora, as exemplified in Figure B.5). Thus, mastering the axioms leads to mastering various other deduction rules. On the other hand, the sets of deduction rules used in the other corpora do not have such a property and thus cannot be generalized to other deduction rules.</p>
<p>Since mastering various deduction rules is the most important in deductive reasoning, this generalizability to deduction rules obtained from FLD corpora is vital.</p>
<h3>6.2. Benchmarking by EntailmentBank</h3>
<p>Table 6: The proof accuracy of provers on EntailmentBank. See Appendix G. 2 for the results of other metrics.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">EntailmentBank</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;">Task1</td>
<td style="text-align: center;">Task2</td>
<td style="text-align: center;">Task3</td>
</tr>
<tr>
<td style="text-align: left;">Source <br> corpus</td>
<td style="text-align: left;">T5</td>
<td style="text-align: center;">$36.8+0.9$</td>
<td style="text-align: center;">$31.2+0.7$</td>
<td style="text-align: center;">$6.2+0.9$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">RT.D5</td>
<td style="text-align: center;">$\mathbf{3 9 . 4}_{+0.9}$</td>
<td style="text-align: center;">$32.0_{+0.8}$</td>
<td style="text-align: center;">$\mathbf{8 . 2}_{+0.8}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">FLD.D5</td>
<td style="text-align: center;">$39.2_{+1.2}$</td>
<td style="text-align: center;">$\mathbf{3 2 . 6}_{+1.0}$</td>
<td style="text-align: center;">$\mathbf{8 . 3}_{+0.7}$</td>
</tr>
</tbody>
</table>
<p>Table 6 shows the results on EntailmentBank (EB). Since EB trees have high-depth (majority up to five), we used the high-depth versions of deduction corpora as source corpus.</p>
<p>First, as seen, the provers trained on both deduction corpora (RT.D5, FLD.D5) performed better than the baseline prover without such training (T5). This suggests that the deductive reasoning ability acquired by synthetic deduction corpora generalizes to more complex real-world deductive reasoning. We showcase some examples in Appendix G.3, where the error of the baseline prover is fixed by training on a deduction corpus (FLD.D5). As seen, the prover captured the fundamentals of deduction rules better than the baseline as follows: (i) it chose the correct premises necessary and sufficient to derive the next conclusion, (ii) it included only such information that logically follows from the chosen premises into a conclusion, and (iii) and it correctly used the rules of logical operators.</p>
<p>Looking at the results of deduction corpora closely, the prover trained on FLD.D5 performed on par with the prover trained on RT.D5, even though it had mastered various deduction rules better, as shown in Section 6.1. We consider a possible reason as follows. Firstly, real-world reasoning can require more coarse-grained deduction rules than those required by deduction corpora. For expressing such coarsegrained deduction rules by the most fine-grained axioms, many steps are required, as in Figure 2. However, the prover trained on FLD still struggles with constructing many-step proofs using the axioms (detailed in Section 7.1). In this sense, the prover could have failed to exploit the axioms' potential fully. We will discuss future directions to tackle this challenge in Section 8.</p>
<h2>7. On What Aspects are Synthetic Deduction Corpora Beneficial?</h2>
<p>A deduction corpus in Table 2 emphasizes a specific aspect different from those emphasized by the other corpora. For each corpus (each aspect), we investigate whether the LM trained on that corpus outperforms the LM trained on the other corpus that does not emphasize the aspect. If it does, we interpret it as meaning that the supervision from deduction corpus on that aspect is beneficial for LMs.</p>
<h3>7.1. Ability to Solve Complex Proof Trees</h3>
<p>Table 7: The depth-wise proof accuracies of the provers.
(a) Target corpus is FLD-impl. 1
(b) Target corpus is FLD. 4</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Source corpus</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">FLD-impl. 0</td>
<td style="text-align: center;">FLD-impl. 1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FLD-impl. 0</td>
<td style="text-align: center;">FLD-impl. 1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">6</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">7</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">8</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">avg.</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 7 shows the depth-wise performances of provers. The corpora in Table 7a use the implication deduction rules. The prover trained on the corpus of shallower $(\sim 3)$ trees (FLDimpl.0) generalizes to deeper $(4 \sim 8)$ trees to some extent, and performs similarly to the prover trained on the corpus of deeper trees (FLD-impl.1). This generalization to deeper trees coincides with previous findings (Tafjord et al., 2021; Sanyal et al., 2022). However, as Table 7b shows, when the corpora use the axioms, neither the provers trained on the shallower tree corpus (FLD.3) nor deeper tree corpus (FLD.4) failed in solving deeper trees.</p>
<p>We can interpret this seemingly contradictory result as follows. As discussed in Section 5, the number of possible proof tree patterns can be estimated (very roughly) as $\mathcal{O}\left(\mathcal{A}^{d}\right)$. When a prover tries to solve a deduction example, it has to choose and generate exactly the one gold proof tree from these possible negative proof trees. This should be very difficult for large $d$ with large $\mathcal{A}$. Now, while the corpora in Table 7a use a few deduction rules $(\mathcal{A}=2)$ of implication type, corpora in Table 7b use various deduction rules $(\mathcal{A} \sim 10)$ of the axioms. This made it very difficult to solve large-depth deduction examples of these corpora, which lead the provers to fail in solving large-depth proof trees in Table 7b.</p>
<p>Overall, for solving complex trees, the supervision from deduction corpora can be necessary but not sufficient alone.</p>
<h3>7.2. Understanding of Diverse Linguistic Expressions</h3>
<p>Table 8: Few-shot proof accuracies of provers transferred among corpora that differ in the diversity of linguistic expressions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Source corpus</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RuleTaker</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FLD</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">RT</td>
<td style="text-align: center;">RT.PR</td>
<td style="text-align: center;">FLD. 2</td>
<td style="text-align: center;">FLD. 3</td>
</tr>
<tr>
<td style="text-align: center;">Target corpus</td>
<td style="text-align: center;">RT</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">76.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RT.BE</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">73.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FLD. 2</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">66.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FLD. 3</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">66.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">avg.</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">70.7</td>
</tr>
</tbody>
</table>
<p>Table 8 shows that a prover trained on a corpus with less linguistic diversity (i.e., RT and FLD.2) performed as well as the prover trained on the linguistically diverse counterpart of that corpus (i.e., RT.PR and FLD.3, respectively). This suggests that LMs are self-sufficient on the linguistic aspect, and thus additional supervision from deduction corpora is not that important.</p>
<p>Indeed, this result coincides with the previous findings (Clark et al., 2021; Tafjord et al., 2021) and can be intuitively understood: since the pre-training corpora of LMs are huge and linguistically diverse, they should have given LMs many chances to learn linguistic of logical statements such as that "If A, then B" paraphrases to "A leads to B".</p>
<h3>7.3. Understanding of Complex Formulas</h3>
<p>Table 9: Few-shot proof accuracies of provers transferred among corpora that differ in the complexity of formulas.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Source corpus</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">FLD. 1</td>
<td style="text-align: center;">FLD. 2</td>
</tr>
<tr>
<td style="text-align: center;">Target corpus</td>
<td style="text-align: center;">FLD. 1</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">71.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FLD. 2</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">66.8</td>
</tr>
</tbody>
</table>
<p>Table 9 shows that while the prover trained on the corpus with simple formulas (FLD.1) performed poorly on the corpus with complex formulas (FLD.2), the prover trained FLD. 2 performed well on both corpora. Thus, deduction corpora are beneficial for mastering complex formulas.</p>
<p>We can interpret this result as follows. The complex formulas included in FLD. 2 are formed by modifying atomic formulas with logical operators $\neg, \wedge, \vee$. The semantics of these logical operators, such that "a sentence with negation $\neg$ have the opposite meaning of that sentence without negation", and that " $A \vee B$ does not necessarily imply $A$ ", are seldom written explicitly by humans. Thus, the pre-training corpora gave LMs too few chances for learning these semantics. This result is enhanced by the previous findings that LMs fail to understand the semantics of negation (Naik et al., 2018; Hossain et al., 2020; Kassner \&amp; Schütze, 2020).</p>
<h3>7.4. Robustness to Distractive Facts</h3>
<p>Table 10: Few-shot proof accuracies of provers transferred among corpora that differ in the number of distractors.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Source corpus</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">FLD. 0</td>
<td style="text-align: center;">FLD. 2</td>
</tr>
<tr>
<td style="text-align: center;">Target corpus</td>
<td style="text-align: center;">FLD. 0</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">75.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FLD. 2</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">66.8</td>
</tr>
</tbody>
</table>
<p>Table 10 shows that, while the prover trained on the corpus without distractors (FLD.0) performed poorly on the corpus with distractors (FLD.2), the prover trained on FLD. 2 performed well on both corpora. Thus, synthetic distractors are beneficial for acquiring the robustness to distractive facts. This result is intuitive: since the human-written text should not include the facts irrelevant to the content, the pre-training corpora should not have given LMs a chance to acquire robustness to irrelevant facts.</p>
<h2>8. Discussions and Future Directions</h2>
<p>So far, we have investigated each aspect of deductive reasoning. We summarize the results and discuss future directions.</p>
<p>Mastery on Various Deduction Rules: Mastering various deduction rules is the most important in deductive reasoning. We showed that FLD corpora teach LMs various deduction rules the most effectively (Section 6.1). This should be because that FLD adopts the axioms of first-order predicate logic system, which can derive any valid deduction rules in this system. The next step will be to examine the axioms of other logic systems, such as linear and modal logic systems, which are also important in real-world reasoning.
Ability to Solve Complex Proof Trees: We have shown that solving a many-step proof tree is still challenging for LMs even after training on deduction corpora (Section 7.1). The possible reason is that they have to choose and generate a gold proof from a large number of possible trees. To solve this problem, inventing smarter and strategic search methods on possible generation space, such as Li et al. (2016); Negrinho et al. (2018); Picco et al. (2021); Welleck et al. (2022), could be a promising direction.</p>
<p>Understanding of Complex Formulas: We have shown that deduction corpora are effective for LMs to understand the semantics of logical operators such as $\neg, \wedge, \vee$ (Sections 6.2 and 7.3). It could be even more effective to incorporate the recent learning methodological approaches for making LMs understand negation (Pröllochs et al., 2019; Hosseini et al., 2021) into the learning on deduction corpora.
Robustness to Distractive Facts: We have shown that the synthetic distractors can make LMs robust to distractive facts (Section 7.4). In a real scenario of logical reasoning, the facts have to be collected by possibly incomplete retrieval systems. The distractors that imitate ones appearing in such a scenario could be more effective. We can generate such distractors as follows: (i) We build a database of synthetic facts. (ii) For a given deduction example, we collect facts from the database by actual retrieval systems.
Generalization to Real-World Reasoning Tasks: We have shown that the training on deduction corpora is even useful for deductive reasoning in a more real-world setting (Section 6.2). However, the LMs trained on FLD could not fully utilize the potential of the axioms, as they failed in constructing many-step proofs to express coarse-grained deduction rules, which could be required in real-world reasoning (Sections 6.2 and 7.1). We discussed future directions to solve such many-step proofs above.</p>
<p>Further, LMs may need additional training to utilize deduction rules well in a realistic context. For example, the LMs</p>
<p>could have to combine deduction rules with common sense knowledge, use multiple deduction rules at once to jump to the next conclusion, and judge the validity of a proof step considering the overall context. Recently, Wei et al. (2022); Kojima et al. (2022) showed that large LMs can utilize deduction rules in a realistic context, given appropriate prompts. It could be promising to integrate this approach and deduction corpora training.</p>
<p>Pursuing further real-world scenarios, we have to tackle tasks of other settings. One is deductive reasoning that requires us to collect relevant facts by ourselves. For this, we could exploit factual knowledge implicitly embedded in LMs (Petroni et al., 2019; Davison et al., 2019; Talmor et al., 2020), or use retrieval systems. For the latter, we could train LM-based retrievers (Karpukhin et al., 2020; Guu et al., 2020) using synthetic deduction examples and fact database. Abductive reasoning (Bhagavatula et al., 2019) is another kind of real-world logical reasoning with which we derive hidden premises from a conclusion and other visible premises. Synthetic corpora for abduction based on formal logic can be generated similarly to as done in this study.</p>
<h2>9. Conclusion</h2>
<p>To teach language models deductive reasoning, we proposed a synthetic corpus based on formal logic theory and verified its effectiveness empirically. Further, we analyzed each aspect of deductive reasoning and provided future directions on each. We will advance on the basis of these directions.</p>
<h2>Acknowledgement</h2>
<p>We thank the three anonymous reviewers and the metareviewer, who gave us insightful comments and suggestions. Computational resources of AI Bridging Cloud Infrastructure (ABCI) provided by the National Institute of Advanced Industrial Science and Technology (AIST) were used. We thank Dr. Masaaki Shimizu at Hitachi for the convenience of additional computational resources. We thank Dr. Naoaki Okazaki, professor at Tokyo Institute of Technology, for the keen comments.</p>
<h2>References</h2>
<p>Askell, A. Gpt-3: Towards renaissance models. Daily Nous Blog: Philosophers On GPT-3, 2020.</p>
<p>Betz, G., Voigt, C., and Richardson, K. Critical thinking for language models. In Proceedings of the 14th International Conference on Computational Semantics (IWCS), pp. 63-75, Groningen, The Netherlands (online), June 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021. iwcs-1.7.</p>
<p>Bhagavatula, C., Bras, R. L., Malaviya, C., Sakaguchi, K.,</p>
<p>Holtzman, A., Rashkin, H., Downey, D., Yih, S. W.-t., and Choi, Y. Abductive commonsense reasoning. arXiv preprint arXiv:1908.05739, 2019.</p>
<p>Bostrom, K., Zhao, X., Chaudhuri, S., and Durrett, G. Flexible generation of natural language deductions. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6266-6278, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.506. URL https:// aclanthology.org/2021.emnlp-main.506.</p>
<p>Bostrom, K., Sprague, Z., Chaudhuri, S., and Durrett, G. Natural language deduction through search over statement compositions. CoRR, abs/2201.06028, 2022. URL https://arxiv.org/abs/2201.06028.</p>
<p>Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023.</p>
<p>Clark, P., Tafjord, O., and Richardson, K. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 38823890, 2021.</p>
<p>Dalvi, B., Jansen, P., Tafjord, O., Xie, Z., Smith, H., Pipatanangkura, L., and Clark, P. Explaining answers with entailment trees. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7358-7370, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 585. URL https://aclanthology.org/2021. emnlp-main. 585.</p>
<p>Dasgupta, I., Lampinen, A. K., Chan, S. C. Y., Sheahan, H. R., Creswell, A., Kumaran, D., McClelland, J. L., and Hill, F. Language models show human-like content effects on reasoning tasks, 2023.</p>
<p>Davison, J., Feldman, J., and Rush, A. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1173-1178, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1109. URL https : //aclanthology.org/D19-1109.</p>
<p>Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for</p>
<p>Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, 2019.</p>
<p>Dziri, N., Lu, X., Sclar, M., Li, X. L., Jiang, L., Lin, B. Y., West, P., Bhagavatula, C., Bras, R. L., Hwang, J. D., Sanyal, S., Welleck, S., Ren, X., Ettinger, A., Harchaoui, Z., and Choi, Y. Faith and fate: Limits of transformers on compositionality, 2023.</p>
<p>Gödel, K. Uber die vollständigkeit des logikkalküls. PhD thesis, Ph. D. dissertation, University of Vienna, 1930.</p>
<p>Gontier, N., Sinha, K., Reddy, S., and Pal, C. Measuring systematic generalization in neural proof generation with transformers. Advances in Neural Information Processing Systems, 33:22231-22242, 2020.</p>
<p>Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval augmented language model pre-training. In International Conference on Machine Learning, pp. 39293938. PMLR, 2020.</p>
<p>Habernal, I., Wachsmuth, H., Gurevych, I., and Stein, B. The argument reasoning comprehension task: Identification and reconstruction of implicit warrants. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1930-1940, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/ N18-1175. URL https://aclanthology.org/ N18-1175.</p>
<p>Han, S., Schoelkopf, H., Zhao, Y., Qi, Z., Riddell, M., Benson, L., Sun, L., Zubova, E., Qiao, Y., Burtell, M., Peng, D., Fan, J., Liu, Y., Wong, B., Sailor, M., Ni, A., Nan, L., Kasai, J., Yu, T., Zhang, R., Joty, S., Fabbri, A. R., Kryscinski, W., Lin, X. V., Xiong, C., and Radev, D. Folio: Natural language reasoning with firstorder logic, 2022. URL https://arxiv.org/abs/ 2209.00840.</p>
<p>Hodel, D. and West, J. Response: Emergent analogical reasoning in large language models, 2023.</p>
<p>Hossain, M. M., Kovatchev, V., Dutta, P., Kao, T., Wei, E., and Blanco, E. An analysis of natural language inference benchmarks through the lens of negation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 91069118, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main. 732. URL https://aclanthology.org/2020. emnlp-main. 732 .</p>
<p>Hosseini, A., Reddy, S., Bahdanau, D., Hjelm, R. D., Sordoni, A., and Courville, A. Understanding by understanding not: Modeling negation in language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1301-1312, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main. 102. URL https://aclanthology.org/2021. naacl-main. 102.</p>
<p>Kahneman, D. Thinking, fast and slow. Macmillan, 2011.</p>
<p>Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6769-6781, 2020.</p>
<p>Kassner, N. and Schütze, H. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7811-7818, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main. 698. URL https://aclanthology.org/2020. acl-main. 698 .</p>
<p>Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners, 2022. URL https://arxiv.org/abs/2205.11916.</p>
<p>Lanham, T., Chen, A., Radhakrishnan, A., Steiner, B., Denison, C., Hernandez, D., Li, D., Durmus, E., Hubinger, E., Kernion, J., Lukošiūtė, K., Nguyen, K., Cheng, N., Joseph, N., Schiefer, N., Rausch, O., Larson, R., McCandlish, S., Kundu, S., Kadavath, S., Yang, S., Henighan, T., Maxwell, T., Telleen-Lawton, T., Hume, T., HatfieldDodds, Z., Kaplan, J., Brauner, J., Bowman, S. R., and Perez, E. Measuring faithfulness in chain-of-thought reasoning, 2023.</p>
<p>Li, J., Monroe, W., Ritter, A., Jurafsky, D., Galley, M., and Gao, J. Deep reinforcement learning for dialogue generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1192-1202, 2016.</p>
<p>Lin, K., Tafjord, O., Clark, P., and Gardner, M. Reasoning over paragraph effects in situations. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pp. 58-62, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5808. URL https: //aclanthology.org/D19-5808.</p>
<p>Liu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., and Zhang, Y. Evaluating the logical reasoning ability of chatgpt and gpt-4, 2023.</p>
<p>Liu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang, Y. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. In Bessiere, C. (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 36223628. International Joint Conferences on Artificial Intelligence Organization, 7 2020. doi: 10.24963/ijcai.2020/ 501. URL https://doi.org/10.24963/ijcai. 2020/501. Main track.</p>
<p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.</p>
<p>McCarthy, J. W. Programs with common sense. In Proc. Tedding Conf. on the Mechanization of Thought Processes, pp. 75-91, 1959.</p>
<p>Mishra, B. D., Tafjord, O., and Clark, P. Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement, 2022. URL https://arxiv.org/abs/2204.13074.</p>
<p>Naik, A., Ravichander, A., Sadeh, N., Rose, C., and Neubig, G. Stress test evaluation for natural language inference. In Proceedings of the 27th International Conference on Computational Linguistics, pp. 2340-2353, Santa Fe, New Mexico, USA, August 2018. Association for Computational Linguistics. URL https: //aclanthology.org/C18-1198.</p>
<p>Negrinho, R., Gormley, M., and Gordon, G. J. Learning beam search policies via imitation learning. Advances in Neural Information Processing Systems, 31, 2018.</p>
<p>OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. URL https://api.semanticscholar. org/CorpusID:257532815.</p>
<p>Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463-2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://aclanthology.org/D19-1250.</p>
<p>Picco, G., Hoang, T. L., Sbodio, M. L., and Lopez, V. Neural unification for logic reasoning over natural language. In Findings of the Association for Computational Linguistics: EMNLP 2021,
pp. 3939-3950, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp. 331. URL https://aclanthology.org/2021. findings-emnlp. 331.</p>
<p>Pröllochs, N., Feuerriegel, S., and Neumann, D. Learning interpretable negation rules via weak supervision at document level: A reinforcement learning approach. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 407-413, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1038. URL https://aclanthology.org/N19-1038.</p>
<p>Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.</p>
<p>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P. J., et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67, 2020.</p>
<p>Razeghi, Y., Logan IV, R. L., Gardner, M., and Singh, S. Impact of pretraining term frequencies on few-shot numerical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 840-854, 2022.</p>
<p>Richardson, K., Hu, H., Moss, L., and Sabharwal, A. Probing natural language inference models through semantic fragments. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 8713-8721, 2020.</p>
<p>Saha, S., Ghosh, S., Srivastava, S., and Bansal, M. PRover: Proof generation for interpretable reasoning over rules. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 122136, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main. 9. URL https://aclanthology.org/2020. emnlp-main. 9 .</p>
<p>Sanyal, S., Singh, H., and Ren, X. Fairr: Faithful and robust deductive reasoning over natural language. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1075-1093, 2022.</p>
<p>Sun, C., Zhang, X., Chen, J., Gan, C., Wu, Y., Chen, J., Zhou, H., and Li, L. Probabilistic graph reasoning for natural proof generation. In Findings of the Association for</p>
<p>Computational Linguistics: ACL-IJCNLP 2021, pp. 31403151, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl. 277. URL https://aclanthology.org/2021. findings-acl. 277.</p>
<p>Tafjord, O., Gardner, M., Lin, K., and Clark, P. Quartz: An open-domain dataset of qualitative relationship questions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5941-5946, 2019.</p>
<p>Tafjord, O., Dalvi, B., and Clark, P. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 36213634, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl. 317. URL https://aclanthology.org/2021. findings-acl. 317.</p>
<p>Talmor, A., Tafjord, O., Clark, P., Goldberg, Y., and Berant, J. Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. Advances in Neural Information Processing Systems, 33: 20227-20237, 2020.</p>
<p>Tian, J., Li, Y., Chen, W., Xiao, L., He, H., and Jin, Y. Diagnosing the first-order logical reasoning ability through logicnli. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3738-3747, 2021.</p>
<p>Turpin, M., Michael, J., Perez, E., and Bowman, S. R. Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting, 2023.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, 2017.</p>
<p>Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353-355, 2018.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQlMeSB_J.</p>
<p>Welleck, S., Liu, J., Lu, X., Hajishirzi, H., and Choi, Y. Naturalprover: Grounded mathematical proof generation with language models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=rhdfTOiXBng.</p>
<p>Weston, J., Bordes, A., Chopra, S., Rush, A. M., Van Merriënboer, B., Joulin, A., and Mikolov, T. Towards aicomplete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.</p>
<p>Wu, Z., Qiu, L., Ross, A., Akyürek, E., Chen, B., Wang, B., Kim, N., Andreas, J., and Kim, Y. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks, 2023.</p>
<p>Xie, Z., Thiem, S., Martin, J., Wainwright, E., Marmorstein, S., and Jansen, P. WorldTree v2: A corpus of sciencedomain structured explanations and inference patterns supporting multi-hop inference. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 5456-5473, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020. lrec-1.671.</p>
<p>Yang, K., Deng, J., and Chen, D. Generating natural language proofs with verifier-guided search. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.</p>
<h2>A. Related Work</h2>
<h2>A.1. Synthetic Corpus for Acquiring Deductive Reasoning Ability</h2>
<p>A synthetic deduction corpus can be one promising approach for language models (LMs) to acquire logical deductive reasoning ability. The automatic (programmatic) generation ensures the validity of the resulting deductive proof examples. Further, since we can bypass high-cost human annotations we can generate many examples, which should be required by LMs to learn deductive reasoning inductively.</p>
<p>RuleTaker (Clark et al., 2021) proposed a deduction corpus composed of synthetically generated multistep deductive proofs written in natural languages. Each deductive proof (dis-)proves a hypothesis by applying deduction rules multiple times to a given set of facts. As the deduction rules, rules of the implication kind were used. They showed that Transformer (Vaswani et al., 2017) LMs can solve these problems in the sense that they can predict the final answer (i.e., "proved", "disproved", or "unknown") of each deductive proof given the fact set. Later studies (Saha et al., 2020; Dalvi et al., 2021; Tafjord et al., 2021; Sanyal et al., 2022) showed that generative LMs can generate even the intermediate proofs as well as the final answer.</p>
<p>Artificial Argument Corpus (Betz et al., 2021) is another corpus composed of synthetically generated single-step deductive proofs. As the deduction rules, hand-selected rules useful for critical thinking were used. They showed that the LMs trained on this corpus not only solve the task of this corpus itself but generalize to other NLI tasks from GLUE benchmark (Wang et al., 2018). However, at the same time, they showed that such LMs do not generalize well to more challenging logical reasoning tasks such as ARC (Habernal et al., 2018) and LogiQA (Liu et al., 2020).</p>
<p>Gontier et al. (2020) investigated the deductive reasoning ability of LMs on a corpus, which is composed of a specific type of multistep inference: kinship relationship on synthetic kinship graphs. They found that LMs can solve this task when the number of proof steps is not large, but it is difficult for them to generalize to longer-than-training proofs. Bostrom et al. (2021) studied how we can create realistic natural language expressions that represent deduction rules. To this end, they scraped sentences from Wikipedia by a template-based method and paraphrased them. They showed that training on this corpus is helpful for solving real-world deductive reasoning such as EntailmentBank (Dalvi et al., 2021).</p>
<p>While all these corpora focused on specific sets of deduction rules, we focus on the theoretically well-grounded set of deduction rules that can derive any other deduction rules. Further, we analyze each aspect of deductive reasoning using corpora of various patterns to advance the research direction of deductive reasoning.</p>
<h2>A.2. Benchmarks for Deductive Reasoning</h2>
<p>Many benchmarks of single-step logical reasoning using specific reasoning rules have been proposed: bAbI (Weston et al., 2015), QuaRTz (Tafjord et al., 2019), ROPES (Lin et al., 2019) and Richardson et al. (2020). For multistep deductive reasoning, FOLIO (Han et al., 2022) is a humanauthored benchmark of the SAT (i.e., satisfiability) problem. Given a set of facts and hypotheses, which are created by a human referencing a specific page of Wikipedia, we are required to assign a truth value to each hypothesis. This requires (implicitly) conducting multistep deductive reasoning using high-granularity deduction rules.</p>
<p>RuleTaker (Clark et al., 2021; Tafjord et al., 2021) can work as a benchmark as well as a synthetic training corpus. RuleTaker focuses on multistep deductive reasoning constructed from implication rules. Since RuleTaker requires generating all the intermediate steps as well as the final prediction on the hypothesis, it is suitable for measuring deductive reasoning ability explicitly and transparently. Further, it includes many irrelevant facts so that the model has to choose only relevant facts under these noises. This makes the task challenging. LogicNLI (Tian et al., 2021) can be considered as an extension of RuleTaker, where additional logical operators such as " $\equiv$ " are used ${ }^{6}$. Additionally, the examples of LogicNLI are checked manually by humans to ensure their quality.</p>
<p>EntailmentBank (EB) (Dalvi et al., 2021) is the same type of task as RuleTaker, but is even more challenging. The proof trees in EB dataset are human-authored rather than synthetically generated. Further, each proof step can be a rough entailment instead of a rigid logical step. Thus, EB measures logical reasoning ability in a more real-world setting.</p>
<h2>A.3. Proof Generation Models</h2>
<p>Earlier work (Saha et al., 2020; Gontier et al., 2020; Dalvi et al., 2021; Sun et al., 2021) generated proof sequences at once by LMs. Later work (Tafjord et al., 2021; Bostrom et al., 2022; Sanyal et al., 2022; Yang et al., 2022) generated proofs step-wisely one by one. The stepwise methods are more faithful and robustly generalize to longer proofs. Recently, Wei et al. (2022); Kojima et al. (2022) showed that large language models (LLMs) perform well on multi-hop inference tasks provided appropriate prompts. However, Yang et al. (2022) showed that LLMs given few-shot examples perform poorer than fine-tuned smaller LMs.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table B.11: Truth values of the premises $\left(\mathcal{P}<em 1="1">{t}\right)$ and the conclusion $(\mathcal{C})$ of the two deduction rules. Blue shows truth value assignments where both the premises and the conclusion are true $(=1)$. Red shows truth value assignments where the conclusion is false even if all the premises are true.
(a) The valid deduction rule (4) (syllogism).
$\mathcal{P}</em>$.}=(\mathcal{F} \rightarrow \mathcal{G}) \wedge(\mathcal{G} \rightarrow \mathcal{H})$, $\mathcal{C}=\mathcal{F} \rightarrow \mathcal{H</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$\mathcal{F}$</th>
<th style="text-align: left;">$\mathcal{G}$</th>
<th style="text-align: left;">$\mathcal{H}$</th>
<th style="text-align: left;">$\mathcal{P}_{1}$</th>
<th style="text-align: left;">$\mathcal{C}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
</tr>
</tbody>
</table>
<p>(b) The invalid deduction rule (6).
$\mathcal{P}<em 2="2">{1}=\mathcal{F}, \mathcal{P}</em>$,
$\mathcal{C}=\mathcal{G}$}=\mathcal{F} \vee \mathcal{G</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$\mathcal{F}$</th>
<th style="text-align: left;">$\mathcal{G}$</th>
<th style="text-align: left;">$\mathcal{P}_{1}$</th>
<th style="text-align: left;">$\mathcal{P}_{2}$</th>
<th style="text-align: left;">$\mathcal{C}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
</tr>
</tbody>
</table>
<p>The synthetic corpora approach examined in this paper could potentially help all these models to acquire better deductive reasoning ability.</p>
<h2>B. Limitations</h2>
<p>The study has the following limitations:</p>
<ul>
<li>We examined only a kind of logical reasoning: deductive reasoning with a given set of facts. As stated in Section 8, we have other types of logical reasoning to be solved in the future.</li>
<li>We examined only the first-order predicate logic system, while there are other logic systems useful for real-world reasoning to be tackled in the future, as stated in Section 8.</li>
</ul>
<h2>C. Ethics and Social Impacts</h2>
<p>The ultimate goal of the direction of this study is to make an artificial intelligence (AI) that reasons logically step by step. If AI can make a decision by showing logical steps one by one, it will be highly explainable and transparent to users. Furthermore, a user will be able to trace the error of AI. Thus, we think this study makes steps towards AI that will positively impact society.</p>
<h2>D. Formal Logic</h2>
<h2>D.1. Definition of Validity of a Deduction Rule</h2>
<p>A deduction rule is valid when for all truth value assignments, the conclusion is always true $(=1)$ if all the premises are true $(=1)$. This is exemplified in Table B.11a</p>
<p>A deduction rule is invalid when for some truth value assign-
ments, the conclusion is false $(=0)$ even if all the premises are true $(=1)$. This is exemplified in Table B.11b.</p>
<h2>D.2. Deductoin Rules used in Relevant Corpora</h2>
<p>Figure B.4a shows the axioms of first-order predicate logic. Figure B.4b shows the deduction rules of implication type used in RuleTaker (Clark et al., 2021). For the deductoin rules used in AACorpus, see Figure 1 in Betz et al. (2021).</p>
<h2>D.3. Examples of Multistep Deduction constructed from the Axioms</h2>
<p>Figure B.5a shows the derivation of a deduction rule of implication type used in RuleTaker. Figure B.5b shows the derivation of the contraposition deduction rule used in AACorpus.</p>
<h2>E. Corpus Generation based on Formal Logic Deduction</h2>
<p>We show the examples of FLD example in Figure C. 6
Below, we show additional details of each module of FLD. Please refer to Figure 3 on intuitive understanding.</p>
<h2>E.1. Proof Tree Generation</h2>
<p>As stated in Section 3.1, the generator module generates a proof tree by forward- and backward random deduction, using the deduction rules specified by a user. A user can specify deduction rules via a template file as exemplified in Figure E.7.</p>
<p>The forward random deduction is done as follows. The generator first chooses a deduction rule randomly and forms the initial tree where the root node is the conclusion of the chosen deduction rules and the child nodes are the premises of the chosen deduction rule. The generator next randomly chooses another deduction rule that can be "jointed" to the root note of the tree. A deduction rule can be jointed to the root node of a tree if one of the premises of that deduction rule can be identified with the root node. Then, the generator updates the tree by jointing this chosen deduction rule. The generator continues this step multiple times until the tree achieves the required depth.</p>
<p>The backward random deduction is done as follows. For each step, the generator randomly chooses a leaf node of the tree. Then, the generator randomly chooses a deduction rule that can be jointed to the leaf node. Here, a deduction rule can be jointed to the leaf node if the deduction rule's conclusion can be identified with the leaf node. Then, the generator updates the tree by jointing this chosen deduction rule. The generator continues this step multiple times until the complexity of branches achieves the required level.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) The axioms of first-order predicate logic used in FLD.</p>
<p>$$
\frac{F(a) \rightarrow G(b) \quad F(a)}{G(b)} \quad \frac{\forall x F(x) \rightarrow G(x) \quad F(a)}{G(b)}
$$</p>
<p>(b) The deduction rules of the implication type used in RuleTaker (Clark et al., 2021).</p>
<p>Figure B.4: We show the deduction rules used in relevant corpora. For the "critical thinking" deduction rules used in AACorpus (Betz et al., 2021), please refer to Figure 1 in Betz et al. (2021).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure B.5: As examples of multistep deduction constructed from the axioms, we show the derivations of the deduction rules used in the previous studies.</p>
<h2>E.2. Factual Distractor Generation</h2>
<p>We implemented three types of distractors. For a deductive nstance, we use the random mixture of these distractors. Below, we detail each type of distractor.</p>
<p>Logical Distractor: We construct a distractive formula the form of which is similar to a gold formula. For example, if a gold formula is $(A \wedge B) \rightarrow C$, then the following formula can be a distractor: $(\neg A \wedge B) \rightarrow C$. The aim of this type of distractor is to generate negative facts in a logic sense.</p>
<p>Linguistic Distractor: We construct a distractive sentence by randomly flipping a word in a gold sentence into another word. For example, if a gold sentence is "If it is not the fact that a sun rises, then (...)", then the following sentence can be a distractor: "If it is not the lion that a sun rises,
(...)" We considered grammatical constraints (e.g., part-ofspeech) when flipping a word. Note that these distractors are made after the natural languages are assigned to gold formulas. The aim of this type of distractor is to generate negative facts in a linguistic sense.</p>
<p>Negative Tree Distractor: We create another proof tree irrelevant to the gold proof tree and use its leaf nodes as distractors. If a prover chooses these distractors as the starting point of the proof, then it will reach a conclusion that is irrelevant to the given hypothesis. Thus, this type of distractor measures the prover's look-ahead ability.</p>
<h2>E.3. Natural Language Assignment</h2>
<p>We show an example of the natural language template file in Figure E.7.</p>
<p>When we assign natural language statements to each atomic component such as $A, B, F, G, H, I, a, b$, we used grammatical heuristics as exemplified as follows: (i) Atomic propositions like $A$ and $B$ are converted into complete-sentence statement like "[NOUN] is [ADJ]", "[NOUN] [VERB]" and "[NOUN] occurs". (ii) (Logical) predicates like $F$ and $G$ are converted into predicate phrases such as "[VERB]", "is [ADJ]" and "is [NOUN]". (iii) Constants like $a$ are converted into entity-like phrases such as "[NOUN]".</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" />
(a) An example where the proof by contradiction is used.
<img alt="img-6.jpeg" src="img-6.jpeg" />
(b) An example that can test the semantics of logical conjunction $\wedge$.</p>
<p>Figure C.6: Examples of FLD examples.</p>
<h2>F. Details of Experiments</h2>
<h2>F.1. Corpora</h2>
<p>As shown in Table 2, we generated multiple corpora to align conditions as closely as possible across the corpora being compared. Below, we detail each aspect.</p>
<p>Preprocessing on RuleTaker: We sub-sampled the examples so that the number of examples in the training split becomes 30 k and that the answer label distribution (over "proved", "disproved", and "unknown") becomes uniform.</p>
<p>Dataset Size: All the corpora have the training split of 30k examples. FLD corpora have validation and test split of 1 k examples. For RuleTaker, see (Tafjord et al., 2021).</p>
<p>Label Distribution: All the corpora have a uniform distribution over answer labels, i.e., over "proved", "disproved", and "unknown".</p>
<p>Deduction Rules: "Implication" deduction rules are shown in Figure B.4b. Complicated formula versions such as $F_{1}(a) \wedge F_{2}(a) \rightarrow G(b)$ are also used. For "critical thinking", we used the ones listed in Figure 1 in Betz et al. (2021). For "axioms", we used the axioms of first-order predicate logic shown in Figure B.4a.</p>
<p>Linguistic Diversity: First, we detail the linguistic diversity of RuleTaker corpora. RT is classified into "less" since it uses only few templates for each logical statement. RT.PR is classified into "more" since it includes various</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure E.7: An example of the template rule file for deduction rules used by the proof tree generator.
paraphrases of logical statements collected from human annotators. RT.BE is classified into "more" since it is completely human-made and includes various paraphrases.</p>
<p>For FLD corpora, "less" means that we used only one natural language template for each logical statement and limited vocabulary (sized 100) for each POS. For "more", we used several (up to five) natural language templates for each logical statement, and a large vocabulary (sized 5000) for each POS. Note that since the templates can be nested such as:</p>
<p>$$
\begin{aligned}
&amp; (A \wedge B) \rightarrow C: \text { "If }(A \wedge B), \text { then C.", " }(A \wedge B), \text { thus C." } \
&amp; \quad(A \wedge B): \text { "A, and B", "B and also A", } \ldots
\end{aligned}
$$</p>
<p>Thus, the number of resulting patterns is combinatorially large.</p>
<p>Formula Complexity: For FLD corpora that have formula complexity "simple", we assign each compound formula such as $\mathcal{F}$ and $\mathcal{G}$ only a single atomic component as $\mathcal{F}=A$ and $\mathcal{G}=B$. For "complex", we used compound formulas randomly constructed from atomic formulas with logical operators, such as $\mathcal{F}=(A \wedge B), \mathcal{F}=\neg(A \vee \neg B)$, in addition to the "simple" formulas.</p>
<p>RuleTaker corpora use "complex" formula.</p>
<p>Tree Depth: The tree depth of "critical thinking" is limited to one because the critical thinking deduction rules have high-granularity, and thus cannot be easily combined to form multistep deductions.</p>
<p>Tree Depth Distribution: We have two types of tree depth distribution: skewed and uniform. The skewed distribution is biased toward lower depths. This distribution comes from the distribution of RT(D0-D3). The uniform distribution is uniform over the depths.</p>
<h2>F.2. Prover Training and Performance Measurement</h2>
<p>We detail the prover training and performance measurements on the benchmarks. This experimental setting is basically the same as Yang et al. (2022). Thus, please refer to the study when necessary.</p>
<h2>F.2.1. The Prover Model</h2>
<p>We added a slight modification to the original model for simplicity as follows. While the original model predicted an answer label (i.e., proved, disproved, or unknown) of a given example on the basis of the log-likelihood of the augmented proof sequences, we predict the answer label by forcing the prover to generate the label token (" $\qquad$ PROVED $\qquad$ ", " $\qquad$ DISPROVED $\qquad$ " or " $\qquad$ UNKNOWN $\qquad$ ") at the end of proof sequences.</p>
<h2>F.2.2. Few-shot Transfer to Synthetic Deduction Corpora</h2>
<p>We first train a prover on the training split of a corpus in Table 2. Then, we train the prover on a training split of another corpus in few-shot, and after that, we measured its performance on the test split. We used validation split for tuning hyperparameters. We adopted T5-base for prover LM for computational efficiency. Table F. 12 shows the hyperparameters. For the "fully-fine-tuning" setting used in Section 5 and Appendix G, we trained a prover using all the dataset examples for 20k steps.</p>
<p>We run the experiments for one seed for computational reasons. Training on a source corpus takes about ten hours on a single NVIDIA A100 (48GB) GPU. Training on a target corpus takes a few hours on the same GPU.</p>
<h2>F.2.3. Transfer to EntailmentBank</h2>
<p>We first train a prover on the training split of a corpus from Table 2. Then, we train the prover on the training split of each EntailmentBank corpus. We adopted T5-large for prover LM following Yang et al. (2022). The hyperparameters are basically the same as Yang et al. (2022). Table F. 13</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" />
(a) For each formula, we have several templates. A template can be nested (redirected to other templates), as shown.</p>
<p>Figure E.8: An example the template file of the natural language assigner</p>
<p>Table F.12: The hyperparameters of prover training in the transfer experiments among deduction corpora. See (Yang et al., 2022) or the code for other parameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Source corpus</th>
<th style="text-align: left;">Target corpus</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">transformer model</td>
<td style="text-align: left;">T5-base</td>
<td style="text-align: left;">T5-base</td>
</tr>
<tr>
<td style="text-align: left;"># dataset examples</td>
<td style="text-align: left;">30000</td>
<td style="text-align: left;">300</td>
</tr>
<tr>
<td style="text-align: left;">steps</td>
<td style="text-align: left;">20000</td>
<td style="text-align: left;">2000</td>
</tr>
<tr>
<td style="text-align: left;">learning rate</td>
<td style="text-align: left;">$1 \mathrm{e}-4$</td>
<td style="text-align: left;">$1 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: left;">learning rate scheduler</td>
<td style="text-align: left;">AdamW</td>
<td style="text-align: left;">AdamW</td>
</tr>
<tr>
<td style="text-align: left;">warmup steps</td>
<td style="text-align: left;">1000</td>
<td style="text-align: left;">500</td>
</tr>
<tr>
<td style="text-align: left;">batch size</td>
<td style="text-align: left;">64</td>
<td style="text-align: left;">64</td>
</tr>
<tr>
<td style="text-align: left;">gradient clipping</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">0.5</td>
</tr>
</tbody>
</table>
<p>Table F.13: The hyperparameters of prover training in the transfer experiments from deduction corpora to EntailmentBank. See (Yang et al., 2022) or the code for other parameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Source</th>
<th style="text-align: left;">Traget</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">synthetic corpus</td>
<td style="text-align: left;">EB corpus</td>
</tr>
<tr>
<td style="text-align: left;">transformer model</td>
<td style="text-align: left;">T5-large</td>
<td style="text-align: left;">T5-large</td>
</tr>
<tr>
<td style="text-align: left;"># dataset examples</td>
<td style="text-align: left;">30000</td>
<td style="text-align: left;">1313</td>
</tr>
<tr>
<td style="text-align: left;">steps</td>
<td style="text-align: left;">10000</td>
<td style="text-align: left;">10000</td>
</tr>
<tr>
<td style="text-align: left;">learning rate</td>
<td style="text-align: left;">$1 \mathrm{e}-4$</td>
<td style="text-align: left;">$5 \mathrm{e}-5($ task1), 2.5e-5(task2)</td>
</tr>
<tr>
<td style="text-align: left;">learning rate scheduler</td>
<td style="text-align: left;">AdamW</td>
<td style="text-align: left;">AdamW</td>
</tr>
<tr>
<td style="text-align: left;">warmup steps</td>
<td style="text-align: left;">1000</td>
<td style="text-align: left;">1000(task1), 3000(task2)</td>
</tr>
<tr>
<td style="text-align: left;">batch size</td>
<td style="text-align: left;">64</td>
<td style="text-align: left;">64</td>
</tr>
<tr>
<td style="text-align: left;">gradient clipping</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">0.5</td>
</tr>
</tbody>
</table>
<p>shows the hyperparameters.
For training the verifier, we used exactly the same setting as Yang et al. (2022).</p>
<p>For the EB scorer, we used the same version as Yang et al. (2022), that is, version v3 that was released on May 28, $2022^{7}$.</p>
<p>We run the experiments for six seeds. Training on a source synthetic deduction corpus takes about one day on a single NVIDIA A100 (48GB) GPU. Training on a target EB corpus</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>takes about one day on the same GPU.</p>
<h2>F.3. License of used Datasets.</h2>
<p>All the datasets used in this paper are publicly available: RuleTaker (Clark et al., 2021; Tafjord et al., 2021), EntailmentBank (Mishra et al., 2022) and FLD (will be publicly available).</p>
<h2>G. How Well do LMs Solve Logic?</h2>
<p>Table G.14: Proof accuracy of a prover fully fine-tuned on each corpus using all the examples.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Skewed depth distrib.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Uniform depth distrib.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">RT</td>
<td style="text-align: center;">RT.PR</td>
<td style="text-align: center;">sFLD-impl</td>
<td style="text-align: center;">FLD-impl. 0</td>
<td style="text-align: center;">FLD. 2</td>
<td style="text-align: center;">FLD. 3 (FLD)</td>
<td style="text-align: center;">FLD. 4 (FLD*)</td>
</tr>
<tr>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">37.7</td>
</tr>
</tbody>
</table>
<p>We analyze the challengingness of each corpus in detail by using Table G. 14 .</p>
<p>First, we look into the results on skewed depth distribution corpora. As seen, sFLD-impl is more challenging than RuleTaker corpora. Since the corpus design in relevant aspects is aligned between RuleTaker and sFLD-impl as in Table 2, the difference should come from the other implementation details. For example, the distractors of FLD are designed to be easily confused with positive facts (Section 3.2), and natural language assignments are extremely diverse due to the random statement generation (Section 3.3).</p>
<p>FLD-impl. 0 is more challenging than sFLD-impl even though they use the same deduction rules. This should be because FLD-impl. 0 has "uniform" tree depth distribution and thus includes a higher depth tree than sFLD-impl do (Appendix F.1).</p>
<p>The reason that FLD. 2 is more challenging than FLD-impl. 0 should be as follows. First, since a proof tree is constructed from the combination of deduction rules chosen at each level of the tree, the number of possible proof tree patterns can be estimated (very roughly) as $\mathcal{O}\left(\mathcal{A}^{d}\right)$, where $\mathcal{A}$ is the number of deduction rule choices at each level and $d$ is the depth of the proof tree. Next, while FLD-impl. 0 uses only a few deduction rules $(\mathcal{A}=2)$ of implication type shown in Figure B.4b, FLD. 2 uses various deduction rules $(\mathcal{A} \sim 10)$ of the axioms shown in Figure B.4a. Thus, FLD. 2 includes exponentially more diverse patterns of proof trees than RuleTaker. This makes FLD. 2 more challenging than FLD-impl. 0 .</p>
<p>FLD. 3 is the linguistically diverse version of FLD. 2 . The challengingness of FLD. 3 remains almost the same as that of FLD. 2 provably because LMs can solve the linguistic aspects such as paraphrasing, as discussed in Section 7.2.</p>
<p>FLD. 4 is the higher-depth ( $d$ up to 8 ) version of FLD.3. This corpus is the most challenging provably due to the exponentially combinatorially more diverse patterns proof trees coming from $\mathcal{O}\left(\mathcal{A}^{d}\right)$.</p>
<h2>G.1. Answer Accuracies on Transfer Experiments among Deduction Corpora</h2>
<p>Below, we show the results of transfer among synthetic corpora measured by the other metric of answer accuracy.</p>
<p>Table G.15: Answer accuracy of a prover fully fine-tuned using all the dataset examples on each corpus.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">RT</th>
<th style="text-align: center;">RT.PR</th>
<th style="text-align: center;">sFLD-impl</th>
<th style="text-align: center;">FLD-impl. 0</th>
<th style="text-align: center;">FLD. 2</th>
<th style="text-align: center;">FLD. 3</th>
<th style="text-align: center;">FLD. 4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">68.1</td>
</tr>
</tbody>
</table>
<p>Table G.16: Few-shot answer accuracies of provers transferred among corpora that differ in deduction rules.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">T5</th>
<th style="text-align: center;">RT</th>
<th style="text-align: center;">RT.PR</th>
<th style="text-align: center;">sFLD-impl</th>
<th style="text-align: center;">sFLD-crit</th>
<th style="text-align: center;">sFLD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">RT</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">84.8</td>
</tr>
<tr>
<td style="text-align: center;">RT.PR</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">82.4</td>
</tr>
<tr>
<td style="text-align: center;">RT.BE</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">85.0</td>
</tr>
<tr>
<td style="text-align: center;">FLD (RT)</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">95.3</td>
</tr>
<tr>
<td style="text-align: center;">FLD (AA)</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">95.1</td>
</tr>
<tr>
<td style="text-align: center;">FLD (axiom)</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">92.9</td>
</tr>
<tr>
<td style="text-align: center;">avg.</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">89.3</td>
</tr>
</tbody>
</table>
<p>Table G.17: The depth-wise answer accuracies of the provers.
(a) Target corpus is FLD-impl.1. (b) Target corpus is FLD.4.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Source corpus</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Source corpus</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">FLD. 3</td>
<td style="text-align: center;">FLD. 4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">FLD-impl. 0</td>
<td style="text-align: center;">FLD-impl. 1</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">95.9</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">91.5</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">78.9</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">71.2</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">60.6</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">51.8</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">47.1</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">45.2</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">71.4</td>
</tr>
<tr>
<td style="text-align: center;">avg.</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table G.18: Few-shot answer accuracies of provers transferred among corpora that differ in the diversity of linguistic expressions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">T5</th>
<th style="text-align: center;">RT</th>
<th style="text-align: center;">RT.PR</th>
<th style="text-align: center;">FLD. 2</th>
<th style="text-align: center;">FLD. 3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">RT</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">83.9</td>
</tr>
<tr>
<td style="text-align: center;">RT.PR</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">82.0</td>
</tr>
<tr>
<td style="text-align: center;">FLD. 2</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">86.9</td>
</tr>
<tr>
<td style="text-align: center;">FLD. 3</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">87.7</td>
</tr>
<tr>
<td style="text-align: center;">avg.</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">85.1</td>
</tr>
</tbody>
</table>
<p>Table G.19: Few-shot answer accuracies of provers transferred among corpora that differ in the complexity of formulas.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">T5</th>
<th style="text-align: center;">FLD. 1</th>
<th style="text-align: center;">FLD. 2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">FLD. 1</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">91.6</td>
</tr>
<tr>
<td style="text-align: center;">FLD. 2</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">88.3</td>
</tr>
</tbody>
</table>
<p>Table G.20: Few-shot answer accuracies of provers transferred among corpora that differ in the number of distractors.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">T5</th>
<th style="text-align: center;">FLD. 0</th>
<th style="text-align: center;">FLD. 2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">FLD. 0</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">93.4</td>
</tr>
<tr>
<td style="text-align: center;">FLD. 2</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">88.3</td>
</tr>
</tbody>
</table>
<h2>G.2. Results of Other Metrics on EntailmentBank</h2>
<p>We show the results of other metrics on EntailmentBank in Tables G. 21 to G. 23 .</p>
<h2>G.3. Case Study on EntailmentBank</h2>
<p>Table G. 24 shows some cases where the error of the baseline prover (T5) is fixed by the training on a deduction corpus (FLD.D5).</p>
<p>As seen from "T5 error fixed by FLD.D5" column, typical error of T5 is such as follows: (i) T5 misses some of the premises required to derive the required conclusion, or, simply choose wrong premises. (ii) T5 overclaims, that is, included in the generated conclusion such information that does not logically follow from the chosen premises. It is also suggested that T5 does not understand the rules of logical operators such as negation $\neg$ and conjunction $\wedge$.</p>
<p>In contrast, the prover trained on FLD.D5 captured the fundamentals of deduction rules better than the baseline: (i) it chose correct premises necessary and sufficient to derive the next conclusion, (ii) it included in a conclusion only such information that logically follows from the chosen premises, and (iii) it correctly used the rules of logical operators.</p>
<h2>H. About the Version 2 of FLD</h2>
<p>The current official release ${ }^{1}$ of FLD corpora is version 2. In version 2, we fixed a proof inconsistency issue of version 1 ; specifically, the version 1 corpus contained some examples with contradicting proofs, such as "proved" versus "disproved." Due to this issue, the prover's performance had been underestimated. Fixing this issue, the performance of the provers on version 2 differs from version 1 , as follows:</p>
<p>Table H.25: Proof accuracy of a prover fully fine-tuned using all the dataset examples on each corpus. Each corpus is version 2.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">FLD.v2</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">FLD</td>
<td style="text-align: center;">FLD $*$</td>
</tr>
<tr>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">44.4</td>
</tr>
</tbody>
</table>
<p>Table H.26: Answer accuracy of a prover fully fine-tuned using all the dataset examples on each corpus. Each corpus is version 2.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">FLD.v2</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">FLD</td>
<td style="text-align: center;">FLD $*$</td>
</tr>
<tr>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">72.2</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/allenal/entailment_ bank&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ We limit our focus to first-order predicate logic in this paper.
${ }^{4} \mathrm{~A}$ deduction rule is valid when for all truth value assignments, the conclusion is true ( $=1$ ) if all the premises are true. See Table B.11a.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
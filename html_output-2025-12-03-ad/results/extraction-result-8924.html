<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8924 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8924</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8924</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-f740a2474b52675287166a003bd1313f8aabcd68</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f740a2474b52675287166a003bd1313f8aabcd68" target="_blank">BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> BioT5+ is introduced, an extension of the BioT5 framework, tailored to enhance biological research and drug discovery, and stands out for its ability to capture intricate relationships in biological data, thereby contributing significantly to bioinformatics and computational biology.</p>
                <p><strong>Paper Abstract:</strong> Recent research trends in computational biology have increasingly focused on integrating text and bio-entity modeling, especially in the context of molecules and proteins. However, previous efforts like BioT5 faced challenges in generalizing across diverse tasks and lacked a nuanced understanding of molecular structures, particularly in their textual representations (e.g., IUPAC). This paper introduces BioT5+, an extension of the BioT5 framework, tailored to enhance biological research and drug discovery. BioT5+ incorporates several novel features: integration of IUPAC names for molecular understanding, inclusion of extensive bio-text and molecule data from sources like bioRxiv and PubChem, the multi-task instruction tuning for generality across tasks, and a numerical tokenization technique for improved processing of numerical data. These enhancements allow BioT5+ to bridge the gap between molecular representations and their textual descriptions, providing a more holistic understanding of biological entities, and largely improving the grounded reasoning of bio-text and bio-sequences. The model is pre-trained and fine-tuned with a large number of experiments, including \emph{3 types of problems (classification, regression, generation), 15 kinds of tasks, and 21 total benchmark datasets}, demonstrating the remarkable performance and state-of-the-art results in most cases. BioT5+ stands out for its ability to capture intricate relationships in biological data, thereby contributing significantly to bioinformatics and computational biology. Our code is available at \url{https://github.com/QizhiPei/BioT5}.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8924.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8924.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioT5+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioT5+</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-v1.1-base (252M) multimodal language model pre-trained and instruction-tuned on joint corpora of bio-text, molecular SELFIES with IUPAC names, and protein FASTA sequences to perform classification, regression and generation tasks in biology and chemistry, including molecule generation and design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioT5+</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer encoder-decoder (T5-v1.1-base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>252M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Joint pre-training on: PubChem (SELFIES + IUPAC names, 28.8M molecules), ZINC20 SELFIES, Uniref50 protein FASTA, C4 general text, PubMed and bioRxiv abstracts and PubMed Central full text (wrapped text with appended SELFIES/FASTA), and molecule-description & protein-description pairs; filtered to avoid downstream data leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery and general molecule design tasks: molecule property prediction, retrosynthesis, chemical reaction prediction, reagent prediction, molecule description generation, description-guided molecule design, DTI prediction, and protein design/generation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Sequence-to-sequence generation using masked-span pretraining plus bidirectional molecule-text translation and multi-task instruction-based fine-tuning; inputs often include SELFIES concatenated with IUPAC names and an instruction prompt specifying target description or design requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>The paper does not report an explicit % novel vs training set; novelty is assessed indirectly via metrics (BLEU, exact match, Levenshtein, fingerprint similarities, FCD, Text2Mol) showing generated molecules have high similarity to ground truth and high validity (often 1.000), but explicit novelty statistics are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Application-specificity is enforced via instruction conditioned generation (text instructions describing desired properties) and by concatenating SELFIES with IUPAC during pretraining so the model links textual descriptions to molecular structures; evaluated on description->molecule and reaction tasks to measure tailoring to objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, Exact match, Levenshtein distance, MACCS/RDK/Morgan fingerprint similarities, FCD (Fr√©chet ChemNet Distance), Text2Mol score, validity, and domain-specific metrics for property prediction (AUROC, MAE for QM9).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BioT5+ achieves state-of-the-art or highly competitive performance across many molecule-oriented generation tasks: e.g., on ChEBI-20 description-guided design BLEU=0.872, Exact=0.522, Levenshtein=12.776, RDK FTS=0.907, Morgan FTS=0.779, Validity=1.000; on chemical reaction forward prediction BioT5+ shows Exact=0.864 and BLEU=0.993; it also achieves best MAE on QM9 regression (HOMO 0.0022, LUMO 0.0024, GAP 0.0028).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms many LLM-based generalist baselines (Galactica, Llama, Vicuna, Mol-Instructions, InstructMol, MolXPT) and retrieval-based GPT baselines on generation and reaction tasks; however, single-task specialist models that incorporate 2D/3D molecular structure information still outperform in some property-prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Paper-reported limitations include: model scale limited (252M) vs larger foundation models; inability to integrate other modalities (images) currently; some gap compared to specialist models that use explicit 2D/3D structure; no explicit reporting of novelty fraction of generated molecules; ethical concerns about misuse (generation of harmful molecules). Additional ablations show performance drops if IUPAC or additional bio-text are removed and that numeric tokenization matters for regression tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8924.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8924.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Predecessor to BioT5+ that jointly trained a T5-based model on molecule SELFIES, protein FASTA, and text to bridge textual and biological sequence representations; used as a baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer encoder-decoder (T5-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>base configuration (prior work - size not restated here)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Joint training on text, molecule SELFIES and protein FASTA as in Pei et al. 2023 (prior work); fewer integrated IUPAC names and less additional PubMed/PubMed Central/bioRxiv data than BioT5+.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule/property description generation, property prediction and other molecule-text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Seq2seq masked-span T5 pretraining and task-specific fine-tuning (single-task in prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported in this paper; used as a baseline for generation and property tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Was evaluated on molecule description generation and other molecule tasks; lacked IUPAC integration and multi-task instruction tuning present in BioT5+.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, ROUGE, METEOR for generation; AUROC/MAE for property tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BioT5 (prior) was a strong baseline; BioT5+ generally improves upon it by integrating IUPAC and additional corpora and using multi-task instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>BioT5 was competitive with other specialist models but BioT5+ extended its capabilities; single-task specialist models still sometimes perform better on narrow tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>In prior BioT5, lack of IUPAC integration and reliance on single-task fine-tuning limited generality; not as optimized for numerical regression tasks due to tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8924.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8924.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mol-Instructions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mol-Instructions: A large-scale biomolecular instruction dataset for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction dataset for biomolecular tasks covering molecule-oriented, protein-oriented and bio-text instructions; used by the authors for instruction tuning and as a benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mol-instructions: A large-scale biomolecular instruction dataset for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mol-Instructions (dataset / instruction suite)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Instruction dataset for LLM fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>A collection of molecule-oriented and protein-oriented instruction examples (cited and used for fine-tuning/evaluation in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Facilitates molecule/property generation, retrosynthesis, reaction prediction, description->molecule and protein generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Provides instruction-format data for multi-task instruction tuning of LLMs (used to steer generation toward application-specific outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not applicable (dataset); used as source of supervision for models to learn mapping between text and molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Enables instruction-conditioned generation (e.g., description-guided molecule design) by providing task-specific prompts and targets during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Used across paper tasks with BLEU, Exact match, fingerprint similarities, and property prediction metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mol-Instructions is used as core instruction data for fine-tuning BioT5+; models fine-tuned with it show improved generality across molecule and protein tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mol-Instructions based multi-task tuning is contrasted with single-task specialist tuning; multi-task yields better cross-task generalization for some correlated properties but single-task can outperform on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>As a dataset, it can cause overlap with downstream task data if not filtered; the authors excluded molecules/proteins overlapping with ChEBI20 and MolInstructions to prevent leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8924.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8924.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructMol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructMol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal LLM that integrates molecule graph encoders with LLMs and uses instruction tuning to align molecular graphs, SELFIES and natural language to support molecule-related tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructMol (InstructMol-G and InstructMol-GS variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Multi-modal LLM combining graph encoders with transformer language models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Reported variants around 6.9B parameters (InstructMol-G-6.9B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Multi-modal molecular datasets including molecular graphs, SELFIES and associated text/instructions (as described in Cao et al., 2023 and cited here).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery tasks including reaction prediction, retrosynthesis, molecule generation and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Combines graph encoding of molecular structure with LLM generation and instruction-tuning to produce reagents, products, or molecule structures from descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not explicitly reported here; used as a strong LLM baseline for reaction and design tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Uses graph + text alignment to improve generation tailored to chemical reaction and design tasks; evaluated on reagent prediction, forward reaction, retrosynthesis and description->molecule tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, Exact match, Levenshtein, fingerprint similarities, validity, and property metrics across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>InstructMol variants perform strongly on reaction and generation tasks and are competitive in the comparisons; BioT5+ outperforms or matches InstructMol on several metrics in the reported benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared favorably to plain LLM baselines; BioT5+ often surpasses InstructMol in some reaction/generation metrics despite InstructMol integrating graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>The paper notes InstructMol leverages 2D graph info which helps specialist tasks; BioT5+ still competes well possibly due to rich text+IUPAC pretraining. Specific limitations of InstructMol are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8924.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8924.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolXPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolXPT: Wrapping molecules with text for generative pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-style model jointly trained on molecule SMILES and wrapped text to support generative molecular-language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molxpt: Wrapping molecules with text for generative pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolXPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-style transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified here (reported as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Joint training on SMILES and wrapped text; specifically mentions wrapping molecules with textual context.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule generation, description generation, property prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Generative pretraining in a GPT framework on wrapped molecule-text pairs; used for zero-shot or fine-tuned generation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Wrapped text training aims to ground molecules in literature context, improving task-specific generation such as molecule description and design.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, ROUGE, fingerprint similarities, AUROC/MAE for property tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolXPT serves as a competitive baseline in generation and classification tasks; BioT5+ outperforms MolXPT on several molecule description and property prediction benchmarks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>MolXPT leverages GPT generative pretraining; compared alongside T5-based and other LLM baselines, usually trailing BioT5+ in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>No direct limitations listed here beyond baseline performance; MolXPT lacks explicit IUPAC integration used in BioT5+.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8924.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8924.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5 / MolMo / MolFM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5 / MolMo / MolFM (molecule-focused T5 variants and multimodal molecular models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of T5-based and multimodal molecular language models used as specialist baselines for molecule description and design tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Translation between molecules and natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5 (MolMo / MolFM variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>T5-based seq2seq / multimodal architectures (transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Reported variants including base/248M-sized models in baselines</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Molecule-text pairs, molecular datasets, and graph/2D information depending on variant.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule description generation, description-guided molecule design, and other molecule-narrative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Seq2seq generation mapping molecule representations (SELFIES/SMILES) to text and vice versa; some variants include graph/2D info.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported in this paper; primary evaluation focuses on fidelity to reference descriptions and structural similarity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Trained/fine-tuned on molecule->text and text->molecule datasets to generate molecules matching textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, ROUGE, METEOR, fingerprint similarities, Exact match, validity.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolT5-family models are strong single-task specialists for generation tasks; BioT5+ outperforms or is competitive with them on ChEBI-20 and Mol-Instructions benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Single-task MolT5 variants sometimes perform comparably or slightly better in narrowly focused tasks, but BioT5+'s multi-task, IUPAC-enriched pretraining provides broader gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Single-task specialists typically require per-task tuning and may not generalize across tasks as well as multi-task tuned models like BioT5+.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8924.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8924.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text+Chem T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text+Chem T5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-domain, multi-task T5 model that concurrently processes molecules and natural language for molecular-text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unifying molecular and textual representations via multi-task language modelling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Text+Chem T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>T5 transformer (multi-task)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Reported small-scale baseline (e.g., 223M variant in tables)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Joint molecular and textual datasets (SMILES/SELFIES and associated text) used for multi-task training.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule description generation and description-guided molecule design.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Multi-task T5-style seq2seq generation treating molecules and text within unified framework.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Multi-task training with molecule-text tasks to align textual descriptions and molecular structures; used as a baseline in description->molecule tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, fingerprint similarities, validity, and other generation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Text+Chem T5 performs well as a baseline (e.g., reasonable BLEU and fingerprint scores), but BioT5+ demonstrates improved results on ChEBI-20 and description-guided molecule design benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Comparable to other T5-based systems but generally outperformed by BioT5+ in this paper due to IUPAC integration and richer pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>No direct limitations beyond baseline performance; lacks IUPAC-centric pretraining described for BioT5+.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8924.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8924.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-following GPT-family chat model used as a retrieval-based baseline (10-shot or zero-shot) for molecule description generation and description-guided design when augmented with retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-family decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in paper; used as an external LLM baseline with retrieval or few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule description generation and description-guided molecule design as a retrieval-augmented or few-shot generative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based generation (few-shot) possibly with retrieval augmentation (MolRoGPT/MolReGPT pipelines referenced) to produce molecule descriptions or SMILES from text.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported; evaluated for generation fidelity and similarity to references.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>When retrieval-augmented (MolRoGPT), GPT-3.5 improves by accessing external molecule-text pairs; otherwise zero-shot performance is weak.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, ROUGE, METEOR, fingerprint similarities, validity.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Retrieval-augmented GPT-3.5 (10-shot MolRoGPT) performs reasonably (e.g., BLEU and ROUGE scores) but is generally outperformed by specialized models like BioT5+ on molecule description and text->molecule tasks when not heavily engineered.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms vanilla zero-shot LLMs but trails specialized molecule-text models and BioT5+ with dedicated pretraining and instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Zero-shot performance is poor; relies heavily on retrieval/few-shot prompts for decent performance; no molecular grounding (IUPAC/SELFIES integrated) unless provided via retrieval or prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8924.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8924.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 model used as a retrieval-augmented few-shot baseline for molecule description generation and design in comparison experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (e.g., GPT-4-0314)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-family large decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified here; used as an external retrieval-augmented few-shot baseline (MolRoGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule description generation and description-guided molecule design as a retrieval/few-shot baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Few-shot prompting with retrieval to provide molecule-text context, then prompt-based generation of molecule descriptions or SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Retrieval augmentation (MolRoGPT) enhances specificity by supplying domain-relevant examples; otherwise general-purpose model lacks built-in molecular encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, ROUGE, METEOR, fingerprint similarities, validity.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Retrieval-augmented GPT-4 (10-shot MolRoGPT) achieves strong generation metrics compared to other LLMs but is still outperformed by BioT5+ on ChEBI-20 description tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Stronger than GPT-3.5 in retrieval-augmented set-ups; however, specialized pre-trained models (BioT5+) tailored to molecule-text mappings can outperform it in these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires retrieval and prompt engineering to reach competitive performance on molecule design tasks; not trained with explicit SELFIES+IUPAC integrated pretraining used by BioT5+.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8924.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8924.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama / Llama2 / Vicuna / Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama family, Vicuna, Galactica (general LLM baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General-purpose LLMs (Llama, Llama2, Vicuna, Galactica) used as baselines in zero-shot or few-shot settings to evaluate molecule generation, retrosynthesis and reaction prediction capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama / Llama2 / Vicuna / Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Decoder-only transformer LLMs (GPT-style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Various sizes reported in tables (e.g., Llama-7B, Llama2-7B, Vicuna-7B/13B, Galactica-6.7B/30B/120B)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in paper for these baselines; Galactica is noted as extensively trained on scientific literature; Vicuna is an instruction-tuned Llama variant.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Generalist baselines for molecule description generation, description-guided molecule design, chemical reaction tasks, and property inference.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based (zero-shot/few-shot) generation; some variants evaluated with LoRA or retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Generally weak on chemistry tasks in zero-shot; retrieval/few-shot and fine-tuning (LoRA) can improve but still underperform specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, Exact match, fingerprint similarities, validity, reaction metrics, AUROC for classification.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Vanilla LLMs show very poor zero-shot performance on chemical reaction tasks and generation (e.g., near-zero exact match); retrieval, few-shot, or LoRA improves scores but typically not to the level of domain-specialized models like BioT5+.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Consistently outperformed by domain-adapted models (BioT5+, InstructMol, Mol-Instructions tuned models) on chemistry-specific benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Lack of explicit molecular grounding (SELFIES/IUPAC) leads to hallucinations and near-zero exact matches on generation tasks without retrieval or specialized fine-tuning. Authors note these models' direct zero-shot performance on chemical tasks is extremely poor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery <em>(Rating: 2)</em></li>
                <li>Mol-instructions: A large-scale biomolecular instruction dataset for large language models <em>(Rating: 2)</em></li>
                <li>Molxpt: Wrapping molecules with text for generative pre-training <em>(Rating: 2)</em></li>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>Unifying molecular and textual representations via multi-task language modelling <em>(Rating: 1)</em></li>
                <li>BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8924",
    "paper_id": "paper-f740a2474b52675287166a003bd1313f8aabcd68",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "BioT5+",
            "name_full": "BioT5+",
            "brief_description": "A T5-v1.1-base (252M) multimodal language model pre-trained and instruction-tuned on joint corpora of bio-text, molecular SELFIES with IUPAC names, and protein FASTA sequences to perform classification, regression and generation tasks in biology and chemistry, including molecule generation and design.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BioT5+",
            "model_type": "Transformer encoder-decoder (T5-v1.1-base)",
            "model_size": "252M parameters",
            "training_data": "Joint pre-training on: PubChem (SELFIES + IUPAC names, 28.8M molecules), ZINC20 SELFIES, Uniref50 protein FASTA, C4 general text, PubMed and bioRxiv abstracts and PubMed Central full text (wrapped text with appended SELFIES/FASTA), and molecule-description & protein-description pairs; filtered to avoid downstream data leakage.",
            "application_domain": "Drug discovery and general molecule design tasks: molecule property prediction, retrosynthesis, chemical reaction prediction, reagent prediction, molecule description generation, description-guided molecule design, DTI prediction, and protein design/generation.",
            "generation_method": "Sequence-to-sequence generation using masked-span pretraining plus bidirectional molecule-text translation and multi-task instruction-based fine-tuning; inputs often include SELFIES concatenated with IUPAC names and an instruction prompt specifying target description or design requirements.",
            "novelty_of_chemicals": "The paper does not report an explicit % novel vs training set; novelty is assessed indirectly via metrics (BLEU, exact match, Levenshtein, fingerprint similarities, FCD, Text2Mol) showing generated molecules have high similarity to ground truth and high validity (often 1.000), but explicit novelty statistics are not provided.",
            "application_specificity": "Application-specificity is enforced via instruction conditioned generation (text instructions describing desired properties) and by concatenating SELFIES with IUPAC during pretraining so the model links textual descriptions to molecular structures; evaluated on description-&gt;molecule and reaction tasks to measure tailoring to objectives.",
            "evaluation_metrics": "BLEU, Exact match, Levenshtein distance, MACCS/RDK/Morgan fingerprint similarities, FCD (Fr√©chet ChemNet Distance), Text2Mol score, validity, and domain-specific metrics for property prediction (AUROC, MAE for QM9).",
            "results_summary": "BioT5+ achieves state-of-the-art or highly competitive performance across many molecule-oriented generation tasks: e.g., on ChEBI-20 description-guided design BLEU=0.872, Exact=0.522, Levenshtein=12.776, RDK FTS=0.907, Morgan FTS=0.779, Validity=1.000; on chemical reaction forward prediction BioT5+ shows Exact=0.864 and BLEU=0.993; it also achieves best MAE on QM9 regression (HOMO 0.0022, LUMO 0.0024, GAP 0.0028).",
            "comparison_to_other_methods": "Outperforms many LLM-based generalist baselines (Galactica, Llama, Vicuna, Mol-Instructions, InstructMol, MolXPT) and retrieval-based GPT baselines on generation and reaction tasks; however, single-task specialist models that incorporate 2D/3D molecular structure information still outperform in some property-prediction tasks.",
            "limitations_and_challenges": "Paper-reported limitations include: model scale limited (252M) vs larger foundation models; inability to integrate other modalities (images) currently; some gap compared to specialist models that use explicit 2D/3D structure; no explicit reporting of novelty fraction of generated molecules; ethical concerns about misuse (generation of harmful molecules). Additional ablations show performance drops if IUPAC or additional bio-text are removed and that numeric tokenization matters for regression tasks.",
            "uuid": "e8924.0",
            "source_info": {
                "paper_title": "BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "BioT5",
            "name_full": "BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations",
            "brief_description": "Predecessor to BioT5+ that jointly trained a T5-based model on molecule SELFIES, protein FASTA, and text to bridge textual and biological sequence representations; used as a baseline for comparison.",
            "citation_title": "BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations",
            "mention_or_use": "use",
            "model_name": "BioT5",
            "model_type": "Transformer encoder-decoder (T5-based)",
            "model_size": "base configuration (prior work - size not restated here)",
            "training_data": "Joint training on text, molecule SELFIES and protein FASTA as in Pei et al. 2023 (prior work); fewer integrated IUPAC names and less additional PubMed/PubMed Central/bioRxiv data than BioT5+.",
            "application_domain": "Molecule/property description generation, property prediction and other molecule-text tasks.",
            "generation_method": "Seq2seq masked-span T5 pretraining and task-specific fine-tuning (single-task in prior work).",
            "novelty_of_chemicals": "Not reported in this paper; used as a baseline for generation and property tasks.",
            "application_specificity": "Was evaluated on molecule description generation and other molecule tasks; lacked IUPAC integration and multi-task instruction tuning present in BioT5+.",
            "evaluation_metrics": "BLEU, ROUGE, METEOR for generation; AUROC/MAE for property tasks.",
            "results_summary": "BioT5 (prior) was a strong baseline; BioT5+ generally improves upon it by integrating IUPAC and additional corpora and using multi-task instruction tuning.",
            "comparison_to_other_methods": "BioT5 was competitive with other specialist models but BioT5+ extended its capabilities; single-task specialist models still sometimes perform better on narrow tasks.",
            "limitations_and_challenges": "In prior BioT5, lack of IUPAC integration and reliance on single-task fine-tuning limited generality; not as optimized for numerical regression tasks due to tokenization.",
            "uuid": "e8924.1",
            "source_info": {
                "paper_title": "BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mol-Instructions",
            "name_full": "Mol-Instructions: A large-scale biomolecular instruction dataset for large language models",
            "brief_description": "A large instruction dataset for biomolecular tasks covering molecule-oriented, protein-oriented and bio-text instructions; used by the authors for instruction tuning and as a benchmark.",
            "citation_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "mention_or_use": "use",
            "model_name": "Mol-Instructions (dataset / instruction suite)",
            "model_type": "Instruction dataset for LLM fine-tuning",
            "model_size": null,
            "training_data": "A collection of molecule-oriented and protein-oriented instruction examples (cited and used for fine-tuning/evaluation in this paper).",
            "application_domain": "Facilitates molecule/property generation, retrosynthesis, reaction prediction, description-&gt;molecule and protein generation tasks.",
            "generation_method": "Provides instruction-format data for multi-task instruction tuning of LLMs (used to steer generation toward application-specific outputs).",
            "novelty_of_chemicals": "Not applicable (dataset); used as source of supervision for models to learn mapping between text and molecules.",
            "application_specificity": "Enables instruction-conditioned generation (e.g., description-guided molecule design) by providing task-specific prompts and targets during fine-tuning.",
            "evaluation_metrics": "Used across paper tasks with BLEU, Exact match, fingerprint similarities, and property prediction metrics.",
            "results_summary": "Mol-Instructions is used as core instruction data for fine-tuning BioT5+; models fine-tuned with it show improved generality across molecule and protein tasks.",
            "comparison_to_other_methods": "Mol-Instructions based multi-task tuning is contrasted with single-task specialist tuning; multi-task yields better cross-task generalization for some correlated properties but single-task can outperform on some tasks.",
            "limitations_and_challenges": "As a dataset, it can cause overlap with downstream task data if not filtered; the authors excluded molecules/proteins overlapping with ChEBI20 and MolInstructions to prevent leakage.",
            "uuid": "e8924.2",
            "source_info": {
                "paper_title": "BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "InstructMol",
            "name_full": "InstructMol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery",
            "brief_description": "A multimodal LLM that integrates molecule graph encoders with LLMs and uses instruction tuning to align molecular graphs, SELFIES and natural language to support molecule-related tasks.",
            "citation_title": "Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery",
            "mention_or_use": "use",
            "model_name": "InstructMol (InstructMol-G and InstructMol-GS variants)",
            "model_type": "Multi-modal LLM combining graph encoders with transformer language models",
            "model_size": "Reported variants around 6.9B parameters (InstructMol-G-6.9B, etc.)",
            "training_data": "Multi-modal molecular datasets including molecular graphs, SELFIES and associated text/instructions (as described in Cao et al., 2023 and cited here).",
            "application_domain": "Drug discovery tasks including reaction prediction, retrosynthesis, molecule generation and property prediction.",
            "generation_method": "Combines graph encoding of molecular structure with LLM generation and instruction-tuning to produce reagents, products, or molecule structures from descriptions.",
            "novelty_of_chemicals": "Not explicitly reported here; used as a strong LLM baseline for reaction and design tasks.",
            "application_specificity": "Uses graph + text alignment to improve generation tailored to chemical reaction and design tasks; evaluated on reagent prediction, forward reaction, retrosynthesis and description-&gt;molecule tasks.",
            "evaluation_metrics": "BLEU, Exact match, Levenshtein, fingerprint similarities, validity, and property metrics across benchmarks.",
            "results_summary": "InstructMol variants perform strongly on reaction and generation tasks and are competitive in the comparisons; BioT5+ outperforms or matches InstructMol on several metrics in the reported benchmarks.",
            "comparison_to_other_methods": "Compared favorably to plain LLM baselines; BioT5+ often surpasses InstructMol in some reaction/generation metrics despite InstructMol integrating graph structure.",
            "limitations_and_challenges": "The paper notes InstructMol leverages 2D graph info which helps specialist tasks; BioT5+ still competes well possibly due to rich text+IUPAC pretraining. Specific limitations of InstructMol are not detailed in this paper.",
            "uuid": "e8924.3",
            "source_info": {
                "paper_title": "BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MolXPT",
            "name_full": "MolXPT: Wrapping molecules with text for generative pre-training",
            "brief_description": "A GPT-style model jointly trained on molecule SMILES and wrapped text to support generative molecular-language modeling.",
            "citation_title": "Molxpt: Wrapping molecules with text for generative pre-training",
            "mention_or_use": "use",
            "model_name": "MolXPT",
            "model_type": "GPT-style transformer",
            "model_size": "Not specified here (reported as a baseline)",
            "training_data": "Joint training on SMILES and wrapped text; specifically mentions wrapping molecules with textual context.",
            "application_domain": "Molecule generation, description generation, property prediction tasks.",
            "generation_method": "Generative pretraining in a GPT framework on wrapped molecule-text pairs; used for zero-shot or fine-tuned generation.",
            "novelty_of_chemicals": "Not reported in this paper.",
            "application_specificity": "Wrapped text training aims to ground molecules in literature context, improving task-specific generation such as molecule description and design.",
            "evaluation_metrics": "BLEU, ROUGE, fingerprint similarities, AUROC/MAE for property tasks.",
            "results_summary": "MolXPT serves as a competitive baseline in generation and classification tasks; BioT5+ outperforms MolXPT on several molecule description and property prediction benchmarks in this paper.",
            "comparison_to_other_methods": "MolXPT leverages GPT generative pretraining; compared alongside T5-based and other LLM baselines, usually trailing BioT5+ in this study.",
            "limitations_and_challenges": "No direct limitations listed here beyond baseline performance; MolXPT lacks explicit IUPAC integration used in BioT5+.",
            "uuid": "e8924.4",
            "source_info": {
                "paper_title": "BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MolT5 / MolMo / MolFM",
            "name_full": "MolT5 / MolMo / MolFM (molecule-focused T5 variants and multimodal molecular models)",
            "brief_description": "A family of T5-based and multimodal molecular language models used as specialist baselines for molecule description and design tasks.",
            "citation_title": "Translation between molecules and natural language",
            "mention_or_use": "use",
            "model_name": "MolT5 (MolMo / MolFM variants)",
            "model_type": "T5-based seq2seq / multimodal architectures (transformer)",
            "model_size": "Reported variants including base/248M-sized models in baselines",
            "training_data": "Molecule-text pairs, molecular datasets, and graph/2D information depending on variant.",
            "application_domain": "Molecule description generation, description-guided molecule design, and other molecule-narrative tasks.",
            "generation_method": "Seq2seq generation mapping molecule representations (SELFIES/SMILES) to text and vice versa; some variants include graph/2D info.",
            "novelty_of_chemicals": "Not reported in this paper; primary evaluation focuses on fidelity to reference descriptions and structural similarity metrics.",
            "application_specificity": "Trained/fine-tuned on molecule-&gt;text and text-&gt;molecule datasets to generate molecules matching textual descriptions.",
            "evaluation_metrics": "BLEU, ROUGE, METEOR, fingerprint similarities, Exact match, validity.",
            "results_summary": "MolT5-family models are strong single-task specialists for generation tasks; BioT5+ outperforms or is competitive with them on ChEBI-20 and Mol-Instructions benchmarks.",
            "comparison_to_other_methods": "Single-task MolT5 variants sometimes perform comparably or slightly better in narrowly focused tasks, but BioT5+'s multi-task, IUPAC-enriched pretraining provides broader gains.",
            "limitations_and_challenges": "Single-task specialists typically require per-task tuning and may not generalize across tasks as well as multi-task tuned models like BioT5+.",
            "uuid": "e8924.5",
            "source_info": {
                "paper_title": "BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Text+Chem T5",
            "name_full": "Text+Chem T5",
            "brief_description": "A multi-domain, multi-task T5 model that concurrently processes molecules and natural language for molecular-text tasks.",
            "citation_title": "Unifying molecular and textual representations via multi-task language modelling",
            "mention_or_use": "use",
            "model_name": "Text+Chem T5",
            "model_type": "T5 transformer (multi-task)",
            "model_size": "Reported small-scale baseline (e.g., 223M variant in tables)",
            "training_data": "Joint molecular and textual datasets (SMILES/SELFIES and associated text) used for multi-task training.",
            "application_domain": "Molecule description generation and description-guided molecule design.",
            "generation_method": "Multi-task T5-style seq2seq generation treating molecules and text within unified framework.",
            "novelty_of_chemicals": "Not reported here.",
            "application_specificity": "Multi-task training with molecule-text tasks to align textual descriptions and molecular structures; used as a baseline in description-&gt;molecule tasks.",
            "evaluation_metrics": "BLEU, fingerprint similarities, validity, and other generation metrics.",
            "results_summary": "Text+Chem T5 performs well as a baseline (e.g., reasonable BLEU and fingerprint scores), but BioT5+ demonstrates improved results on ChEBI-20 and description-guided molecule design benchmarks.",
            "comparison_to_other_methods": "Comparable to other T5-based systems but generally outperformed by BioT5+ in this paper due to IUPAC integration and richer pretraining.",
            "limitations_and_challenges": "No direct limitations beyond baseline performance; lacks IUPAC-centric pretraining described for BioT5+.",
            "uuid": "e8924.6",
            "source_info": {
                "paper_title": "BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo (OpenAI)",
            "brief_description": "A large instruction-following GPT-family chat model used as a retrieval-based baseline (10-shot or zero-shot) for molecule description generation and description-guided design when augmented with retrieval.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_type": "GPT-family decoder-only transformer",
            "model_size": "Not specified in paper",
            "training_data": "Not specified in paper; used as an external LLM baseline with retrieval or few-shot prompts.",
            "application_domain": "Molecule description generation and description-guided molecule design as a retrieval-augmented or few-shot generative baseline.",
            "generation_method": "Prompt-based generation (few-shot) possibly with retrieval augmentation (MolRoGPT/MolReGPT pipelines referenced) to produce molecule descriptions or SMILES from text.",
            "novelty_of_chemicals": "Not reported; evaluated for generation fidelity and similarity to references.",
            "application_specificity": "When retrieval-augmented (MolRoGPT), GPT-3.5 improves by accessing external molecule-text pairs; otherwise zero-shot performance is weak.",
            "evaluation_metrics": "BLEU, ROUGE, METEOR, fingerprint similarities, validity.",
            "results_summary": "Retrieval-augmented GPT-3.5 (10-shot MolRoGPT) performs reasonably (e.g., BLEU and ROUGE scores) but is generally outperformed by specialized models like BioT5+ on molecule description and text-&gt;molecule tasks when not heavily engineered.",
            "comparison_to_other_methods": "Outperforms vanilla zero-shot LLMs but trails specialized molecule-text models and BioT5+ with dedicated pretraining and instruction tuning.",
            "limitations_and_challenges": "Zero-shot performance is poor; relies heavily on retrieval/few-shot prompts for decent performance; no molecular grounding (IUPAC/SELFIES integrated) unless provided via retrieval or prompt.",
            "uuid": "e8924.7",
            "source_info": {
                "paper_title": "BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's GPT-4 model used as a retrieval-augmented few-shot baseline for molecule description generation and design in comparison experiments.",
            "citation_title": "GPT-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4 (e.g., GPT-4-0314)",
            "model_type": "GPT-family large decoder-only transformer",
            "model_size": "Not specified in this paper",
            "training_data": "Not specified here; used as an external retrieval-augmented few-shot baseline (MolRoGPT).",
            "application_domain": "Molecule description generation and description-guided molecule design as a retrieval/few-shot baseline.",
            "generation_method": "Few-shot prompting with retrieval to provide molecule-text context, then prompt-based generation of molecule descriptions or SMILES.",
            "novelty_of_chemicals": "Not reported.",
            "application_specificity": "Retrieval augmentation (MolRoGPT) enhances specificity by supplying domain-relevant examples; otherwise general-purpose model lacks built-in molecular encodings.",
            "evaluation_metrics": "BLEU, ROUGE, METEOR, fingerprint similarities, validity.",
            "results_summary": "Retrieval-augmented GPT-4 (10-shot MolRoGPT) achieves strong generation metrics compared to other LLMs but is still outperformed by BioT5+ on ChEBI-20 description tasks in this paper.",
            "comparison_to_other_methods": "Stronger than GPT-3.5 in retrieval-augmented set-ups; however, specialized pre-trained models (BioT5+) tailored to molecule-text mappings can outperform it in these benchmarks.",
            "limitations_and_challenges": "Requires retrieval and prompt engineering to reach competitive performance on molecule design tasks; not trained with explicit SELFIES+IUPAC integrated pretraining used by BioT5+.",
            "uuid": "e8924.8",
            "source_info": {
                "paper_title": "BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Llama / Llama2 / Vicuna / Galactica",
            "name_full": "Llama family, Vicuna, Galactica (general LLM baselines)",
            "brief_description": "General-purpose LLMs (Llama, Llama2, Vicuna, Galactica) used as baselines in zero-shot or few-shot settings to evaluate molecule generation, retrosynthesis and reaction prediction capabilities.",
            "citation_title": "Llama: Open and efficient foundation language models",
            "mention_or_use": "use",
            "model_name": "Llama / Llama2 / Vicuna / Galactica",
            "model_type": "Decoder-only transformer LLMs (GPT-style)",
            "model_size": "Various sizes reported in tables (e.g., Llama-7B, Llama2-7B, Vicuna-7B/13B, Galactica-6.7B/30B/120B)",
            "training_data": "Not specified in paper for these baselines; Galactica is noted as extensively trained on scientific literature; Vicuna is an instruction-tuned Llama variant.",
            "application_domain": "Generalist baselines for molecule description generation, description-guided molecule design, chemical reaction tasks, and property inference.",
            "generation_method": "Prompt-based (zero-shot/few-shot) generation; some variants evaluated with LoRA or retrieval augmentation.",
            "novelty_of_chemicals": "Not reported.",
            "application_specificity": "Generally weak on chemistry tasks in zero-shot; retrieval/few-shot and fine-tuning (LoRA) can improve but still underperform specialized models.",
            "evaluation_metrics": "BLEU, Exact match, fingerprint similarities, validity, reaction metrics, AUROC for classification.",
            "results_summary": "Vanilla LLMs show very poor zero-shot performance on chemical reaction tasks and generation (e.g., near-zero exact match); retrieval, few-shot, or LoRA improves scores but typically not to the level of domain-specialized models like BioT5+.",
            "comparison_to_other_methods": "Consistently outperformed by domain-adapted models (BioT5+, InstructMol, Mol-Instructions tuned models) on chemistry-specific benchmarks.",
            "limitations_and_challenges": "Lack of explicit molecular grounding (SELFIES/IUPAC) leads to hallucinations and near-zero exact matches on generation tasks without retrieval or specialized fine-tuning. Authors note these models' direct zero-shot performance on chemical tasks is extremely poor.",
            "uuid": "e8924.9",
            "source_info": {
                "paper_title": "BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery",
            "rating": 2
        },
        {
            "paper_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "rating": 2
        },
        {
            "paper_title": "Molxpt: Wrapping molecules with text for generative pre-training",
            "rating": 2
        },
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2
        },
        {
            "paper_title": "Unifying molecular and textual representations via multi-task language modelling",
            "rating": 1
        },
        {
            "paper_title": "BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations",
            "rating": 2
        }
    ],
    "cost": 0.023456249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning</h1>
<p>Qizhi Pei ${ }^{1}$, Lijun $\mathrm{Wu}^{2 <em>}$, Kaiyuan Gao ${ }^{3}$, Xiaozhuan Liang ${ }^{4}$, Yin Fang ${ }^{4}$, Jinhua Zhu ${ }^{5}$, Shufang Xie ${ }^{1}$, Tao Qin ${ }^{2}$, Rui Yan ${ }^{1,6 </em>}$<br>${ }^{1}$ Gaoling School of Artificial Intelligence, Renmin University of China<br>${ }^{2}$ Microsoft Research ${ }^{3}$ Huazhong University of Science and Technology<br>${ }^{4}$ Zhejiang University ${ }^{5}$ University of Science and Technology of China<br>${ }^{6}$ Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education<br>{qizhipei, shufangxie, ruiyan}@ruc.edu.cn apeterswu@gmail.com<br>im_kai@hust.edu.cn {liangxiaozhuan, fangyin}@zju.edu.cn<br>teslazhu@mail.ustc.edu.cn taoqin@microsoft.com</p>
<h4>Abstract</h4>
<p>Recent research trends in computational biology have increasingly focused on integrating text and bio-entity modeling, especially in the context of molecules and proteins. However, previous efforts like BioT5 faced challenges in generalizing across diverse tasks and lacked a nuanced understanding of molecular structures, particularly in their textual representations (e.g., IUPAC). This paper introduces BioT5+, an extension of the BioT5 framework, tailored to enhance biological research and drug discovery. BioT5+ incorporates several novel features: integration of IUPAC names for molecular understanding, inclusion of extensive bio-text and molecule data from sources like bioRxiv and PubChem, the multi-task instruction tuning for generality across tasks, and a numerical tokenization technique for improved processing of numerical data. These enhancements allow BioT5+ to bridge the gap between molecular representations and their textual descriptions, providing a more holistic understanding of biological entities, and largely improving the grounded reasoning of bio-text and bio-sequences. The model is pre-trained and fine-tuned with a large number of experiments, including 3 types of problems (classification, regression, generation), 15 kinds of tasks, and 21 total benchmark datasets, demonstrating the remarkable performance and state-of-the-art results in most cases. BioT5+ stands out for its ability to capture intricate relationships in biological data, thereby contributing significantly to bioinformatics and computational biology. Our code is available at https: //github.com/QizhiPei/BioT5.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1 Introduction</h2>
<p>Molecules and proteins are two crucial bio-entities in drug discovery, forming the foundation of biological activities (Dara et al., 2022; AI4Science and Quantum, 2023). A molecule can be represented by its SMILES (Weininger, 1988; Weininger et al., 1989) or SELFIES (Krenn et al., 2020) sequence, and a protein can be described by a FASTA (Lipman and Pearson, 1985; Pearson and Lipman, 1988) sequence. With the advancement of Language Models (LMs), an increasing body of work focuses on understanding the molecules and proteins by modeling their biosequences (Chithrananda et al., 2020; Rives et al., 2021; Lin et al., 2022).</p>
<p>Notably, biological literature (Canese and Weis, 2013; White, 2020) is full of extensive information on molecules and proteins. When a biological entity is mentioned in such literature, its context is predominantly centered around a description of some characteristics of the entity. Consequently, there has been a growing body of work dedicated to the joint modeling of text and biological entities (Pei et al., 2024), such as Galactica (Taylor et al., 2022), MolXPT (Liu et al., 2023c), BioT5 (Pei et al., 2023) and BioMedGPT (Luo et al., 2023c), which are all scientific models trained on text, molecule and protein sequences. Despite their achievements, substantial opportunities for enhancement still remain: (1) Prior works neglect the importance of modeling the textual name of molecules, such as International Union of Pure and Applied Chemistry (IUPAC), which provides a standard and systematic naming method for ensuring uniformity and clarity across the scientific community. Different from SMILES and</p>
<p>SELFIES, IUPAC bears a closer resemblance to natural language that is evident in its widespread adoption within scientific literature (Klinger et al., 2008). (2) Previous models were predominantly specialist models, necessitating the training of a separate model for each downstream task, thereby lacking in generality and increasing the training and developing cost (Liu et al., 2023c; Pei et al., 2023). (3) Most of the previous models based on T5 (Raffel et al., 2020) and GPT (Brown et al., 2020) architectures only focus on the classification tasks since they do not implement specialized tokenization for numerical data, which results in their suboptimal adaptation to regression tasks.</p>
<p>To address the above challenges, in this paper, we introduce BioT5+, an advanced iteration of the BioT5 framework (Pei et al., 2023), designed to augment biological research and drug discovery with enriched data integration, multi-task capabilities, and the ability to solve regression tasks. Shortly speaking, BioT5+ incorporates following significant enhancements:
(1) Enhanced Molecule Understanding: By integrating IUPAC name into BioT5+ framework, the model can achieve a deeper comprehension of molecular structures. This integration allows BioT5+ to interpret chemical names as they commonly appear in scientific literature, bridging the gap between formal molecular representations (such as SELFIES) and their textual descriptions. Consequently, this enhances the understanding of molecules and facilitates more accurate predictions and analyses of molecular properties and activities.
(2) Expanded Bio-text and Molecule Data: Compared to BioT5, BioT5+ includes an extensive corpus of bio-text data from sources like bioRxiv (Sever et al., 2019) and PubMed (Canese and Weis, 2013; White, 2020), alongside highquality molecular data from PubChem (Kim et al., 2019). This expansion not only broadens the knowledge base of the model but also enriches the contextual understanding of biological entities.
(3) Multi-task Instruction Tuning: BioT5+ employs multi-task instruction tuning strategy for downstream tasks rather than the separate specialized model training for each task. By leveraging a unified and multi-task training framework, BioT5+ can seamlessly integrate knowledge from diverse tasks, enhancing its predictive power and generalization capabilities across different biological and chemical domains.
(4) Advanced Numerical Tokenization: To over-
come the limitations of the numerical representations, BioT5+ integrates an advanced characterbased numerical tokenization strategy, drawing inspiration from the Llama (Touvron et al., 2023a) model. This technique allows for a more nuanced and consistent representation of numerical values.</p>
<p>With our designed pre-training and multi-task instruction tuning, the effectiveness of BioT5+ is verified on 3 types of problems (classification, generation, and regression), 15 different tasks, and 21 benchmark datasets, including molecule property prediction, retrosynthesis, molecule description generation, drug-target interaction, and so on. BioT5+ has shown highly competitive results, achieving state-of-the-art performance in most of the tasks. This robust performance underscores the enhanced capability of BioT5+ to capture and analyze the intricate relationships and properties inherent in biological data, marking a significant step forward in computational biology.</p>
<h2>2 Related Work</h2>
<h3>2.1 Biological Cross-modal Models</h3>
<p>Recent advancements in LLMs have led to an increased focus on jointly modeling molecules, proteins, and text, aiming to enhance the understanding of bio-entities through text.
Molecule-Text. MolT5 (Edwards et al., 2022) is jointly trained on general text and molecule SMILES using T5 (Raffel et al., 2020) masked span prediction objective. MoMu (Su et al., 2022) employs contrastive learning on molecular graphs and related text, and MolFM (Luo et al., 2023b) further incorporates knowledge graph embedding for molecule representation. MolXPT (Liu et al., 2023c) is jointly trained on molecule SMILES and wrapped text using GPT (Brown et al., 2020) framework. MolCA (Liu et al., 2023d) enhances LMs by integrating 2D molecular graph perception through a cross-modal projector and uni-modal adapter. GIT-Mol (Liu et al., 2023a) is a multi-modal LLM that synergizes graphs, images, SMILES, and molecule captions. Text+Chem T5 (Christofidellis et al., 2023) is a multi-domain, multi-task language model capable of concurrently processing molecules and natural language.
Protein-Text. Several notable works focus on jointly modeling proteins and text. ProteinDT (Liu et al., 2023b) presents a text-guided protein design framework. BioTranslator (Xu et al., 2023b) is a cross-modal translation system, which can annotate</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a): The overview of BioT5+ framework. (b) (c): the composition of BioT5+ downstream tasks, which is divided into two categories: (b) molecule-oriented tasks and (c) protein-oriented tasks. The names of the tasks, along with their instruction datasets and respective percentages, are annotated near each segment of the accompanying pie charts.</p>
<p>various biological instances using textual descriptions. Prot2Text (Abdine et al., 2023) combines GNN and LLM in an encoder-decoder framework to generate protein functions in a free-text style.</p>
<p>In addition to the models mentioned above, there are other models trained in a more diverse range of modalities: DeepEIK (Luo et al., 2023a) is a multi-modal model which integrates features from multi-modal inputs including drugs, proteins, and text. BioT5 (Pei et al., 2023) is a T5-based (Raffel et al., 2020) model that undergoes joint training on text, molecule SELFIES, and protein FASTA sequences, effectively bridging the gap between textual and biological data.</p>
<p>Despite the successes of these models, their focus on single-task training limits their versatility and hinders the development of a more generalized and adaptable approach in computational biology.</p>
<h3>2.2 Instruction Tuning for Biological Tasks</h3>
<p>Instruction tuning is a popular technique applied to pre-trained LLMs where they are trained with specialized instruction datasets, thus equipping LLMs with the ability to understand task-specific instructions. Recently, there has been a growing interest in exploring instruction tuning for various biological tasks. Notable among these efforts is the development of Mol-Instructions (Fang et al., 2023), a comprehensive instruction dataset specifically designed for the biological domain, which includes molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions. InstructMol (Cao et al., 2023), a multi-modal LLM, employs instruction tuning to align molecular graphs, molecule SELFIES, and natural language. Different from these approaches, our BioT5+ is specifically pre-trained for the biological domain. Further instruction tuning within biological instructions enables BioT5+ to not only understand bio-entities but also generalize across various biological tasks.</p>
<h2>3 BioT5+ Framework</h2>
<p>This section introduces BioT5+ framework and an overview is shown in Figure 1. The tasks involved in pre-training are presented in Figure 3.</p>
<h3>3.1 Intuition for IUPAC Integration</h3>
<p>IUPAC naming system provides a standardized set of rules for naming chemical compounds, which allows for the precise description of molecular structures and their components (functional groups, chains, and rings), making it a cornerstone in chemical nomenclature. Typically, an IUPAC name is constructed from the names of individual molecules' constituent parts, reflecting their structure. This includes prefixes, infixes, and suffixes that indicate various chemical groups and structural features, providing a comprehensive description of the molecule. For instance, the IUPAC name for Aspirin is "2-acetyloxybenzoic acid". Here, "2-acetyloxy" refers to an acetoxy group attached to the second carbon of a benzene ring, and "benzoic acid" indicates a benzene ring with a carboxylic acid group. The resemblance of IUPAC names to natural language, coupled with their prevalent use in scientific literature, makes them an ideal candidate for model pre-training. By pre-training the model on literature that includes IUPAC names, BioT5+ can establish a nuanced understanding of the relationship between molecules and various textual descriptions of their chemical properties.</p>
<h3>3.2 Pre-training Corpus</h3>
<p>As an extension of BioT5, the majority of the pre-training corpus for BioT5+ is identical to BioT5,</p>
<p>hence we will briefly mention the common elements while focusing primarily on the novel aspects introduced in BioT5+.</p>
<p>The pre-training corpus consists of 4 classes: (1) Single-modal data, including molecule SELFIES with IUPAC name from PubChem (Kim et al., 2019), molecule SELFIES from ZINC20 (Irwin et al., 2020), protein FASTA from Uniref50 (Suzek et al., 2007), and general text from "Colossal Clean Crawled Corpus" (C4) (Raffel et al., 2020). For the molecule from PubChem, we concatenate the IUPAC name and SELFIES for pre-training as shown in Figure 3. (2) Wrapped text, where the molecule or gene/protein names are suffixed with corresponding sequence representation. We employ BERN2 (Sung et al., 2022), a neural-based Named Entity Recognition (NER) system in the biological domain, to detect and classify occurrences of molecules and proteins within the abstracts of PubMed (White, 2020) and bioRxiv (Sever et al., 2019). For the molecule name, we first standardize the name to its IUPAC name and then append the corresponding SELFIES. For the gene/protein name, we will directly append its FASTA sequence. For the generation of high-quality wrapped text, we also analyze the confidence score distribution predicted by BERN2 (Sung et al., 2022). Only those entities with higher confidence scores were retained to ensure the accuracy and relevance of the appended sequence data. Further detailed descriptions of this process are provided in Appendix Section C. (3) Bio-text, including PubMed (White, 2020) Central full text articles, and bio-texts from PubMed (White, 2020) abstracts and bioRxiv (Sever et al., 2019) abstracts that do not yield identifiable named entities in (2). (4) Molecule-description pairs and protein-description pairs. The molecule-text data is collected from PubChem (Kim et al., 2019) and we also add the IUPAC name to the text description. All molecules and proteins that exist in the downstream MolInstructions dataset (Fang et al., 2023) and ChEBI20 (Edwards et al., 2022) are excluded to prevent data leakage. The protein-text data is the same as BioT5 (Pei et al., 2023).
Remarkable Difference. The primary distinctions between BioT5+ and BioT5 are as follows: (1) BioT5+ integrates IUPAC in the molecular pretraining data, encompassing IUPAC names combined with SELFIES, wrapped text, and moleculetext translation data. More details are in Appendix Section D. (2) BioT5+ incorporates a broader spec-
trum of high-quality data, including IUPAC names and SELFIES from PubChem, as well as comprehensive articles from bioRxiv and PubMed Central.</p>
<h3>3.3 Tokenization</h3>
<p>BioT5 (Pei et al., 2023) has already demonstrated the advantages of employing separate tokenization and embedding techniques. BioT5+ inherits this advantage to apply specialized tokenization and vocabulary specifically for bio-entities. This method explicitly differentiates between the biological semantic space and the textual semantic space. For molecule SELFIES, each chemically meaningful atom group, naturally distinguished from textual vocabulary due to its bracketed format like [C], is tokenized as an individual token using the inherent token set defined by SELFIES. For protein FASTA sequences, to ensure a clear modal distinction, each amino acid is tokenized into a separate token with the prefix $&lt;\mathrm{p}&gt;$, differentiating them from standard upper-case English letters.</p>
<p>Concurrently, the tokenization of numerical data is worth dedicated consideration and design. The direct application of T5 (Raffel et al., 2020) dictionary derived from nature language using SentencePiece (Kudo and Richardson, 2018) for numerical tokenization can lead to inconsistencies (Liu and Low, 2023). For instance, the number 1024 might be tokenized into " 10 " and " 24 ", while " 2048 " could be split into " 2 ", " 0 ", and " 48 ". This irregular segmentation poses a challenge for the model in consistently mapping embeddings to numbers, especially when the number of digits they represent varies. In contrast, models like Llama (Touvron et al., 2023a,b) and ChatGLM (et.al., 2023) adopt a character-based approach to numerical tokenization, where each digit is tokenized as an individual token. This method has been demonstrated to yield superior results in various arithmetic tasks (Liu and Low, 2023; Nogueira et al., 2021). Accordingly, in BioT5+ we also implement this character-based approach for numerical tokenization without modifying the original dictionary. The efficacy of this method over the original T5 (Raffel et al., 2020) and BioT5 (Pei et al., 2023) to numerical tokenization are shown in Section 4.3, providing empirical evidence of its superior performance in handling numerical data.</p>
<h3>3.4 Model and Training</h3>
<p>Model architecture. BioT5+ adopts the same architecture as the BioT5 (Pei et al., 2023), which</p>
<p>follows the T5-v1.1-base ${ }^{1}$ configuration with vocab size 35,076 and 252 M parameters.
Pre-training. Based on the pre-training corpus described in Section 3.2, the pre-training for BioT5+ is conducted in a multi-task way with eight tasks that fall into 4 categories: (1) Modality-Specific T5 Objectives: This category involves the application of the T5 objective (masked span prediction) to each modality in isolation, including molecule SELFIES with IUPAC name (Task #1), molecule SELFIES (Task #2), protein FASTA sequences (Task #3), and general textual content (Task #4). (2) T5 Objectives on Wrapped Text: Applying the T5 objective to "wrapped" text extracted from scientific corpora (Task #5). (3) T5 Objectives on Biotext: Applying the T5 objective to text in biological domain (Task #6). (4) Bidirectional Translation Tasks: This involves the bidirectional translation between molecule SELFIES-text pairs (Task #7) and protein FASTA-text pairs (Task #8). Through these strategically structured pre-training tasks, BioT5+ is adept at learning the intricate relationships and characteristics of bio-entities as represented in textual information.
Multi-task Instruction-based Fine-tuning. After the comprehensive pre-training phase, BioT5+ undergoes multi-task instruction-based fine-tuning. Unlike BioT5 where each downstream task has a specialized fine-tuned model, we follow Fang et al., 2023 and Cao et al., 2023 to categorize downstream tasks and conduct multi-task instruction tuning, which not only saves the repeated tuning cost but also eases the model deployment for the evaluation of multiple tasks. The relevant groupings and information about benchmark tasks and datasets are illustrated in Figure 1, which is simply split by the domains, e.g., molecule-oriented or protein-oriented tasks. This methodology serves a dual purpose: firstly, it bridges the gap between the pre-training and fine-tuning phases, ensuring a smoother transition and integration of learned capabilities. Secondly, it activates and harnesses the general capabilities of BioT5+ across various tasks, demonstrating its versatility and adaptability in handling diverse biological problems.</p>
<h2>4 Experiments and Results</h2>
<p>As shown in Figure 1, BioT5+ is extensively evaluated across 21 well-established downstream</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Performance (AUROC) comparison on molecule property prediction tasks (classification) on MoleculeNet (Wu et al., 2018) benchmark (Best, Second Best). * means LoRA (Hu et al., 2022) tuning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">BACE $\uparrow$</th>
<th style="text-align: center;">BBBP $\uparrow$</th>
<th style="text-align: center;">HIV $\uparrow$</th>
<th style="text-align: center;">Clintox $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># Molecules</td>
<td style="text-align: center;">1513</td>
<td style="text-align: center;">2039</td>
<td style="text-align: center;">41127</td>
<td style="text-align: center;">1478</td>
</tr>
<tr>
<td style="text-align: left;">Single-task Specialist Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GrapbCL</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">76.0</td>
</tr>
<tr>
<td style="text-align: left;">GophMVP-C</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">77.5</td>
</tr>
<tr>
<td style="text-align: left;">MGSXL</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">80.7</td>
</tr>
<tr>
<td style="text-align: left;">MolCLR</td>
<td style="text-align: center;">$\underline{89.0}$</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">93.2</td>
</tr>
<tr>
<td style="text-align: left;">GEM</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">90.1</td>
</tr>
<tr>
<td style="text-align: left;">Uni-Mol</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">$\underline{80.8}$</td>
<td style="text-align: center;">91.9</td>
</tr>
<tr>
<td style="text-align: left;">KV-PLM</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">84.3</td>
</tr>
<tr>
<td style="text-align: left;">MoMu</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">79.9</td>
</tr>
<tr>
<td style="text-align: left;">MolFM</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">79.7</td>
</tr>
<tr>
<td style="text-align: left;">MolXPT</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">$\mathbf{8 0 . 0}$</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">$\underline{95.3}$</td>
</tr>
<tr>
<td style="text-align: left;">BioT5</td>
<td style="text-align: center;">$\mathbf{8 9 . 4}$</td>
<td style="text-align: center;">$\underline{77.7}$</td>
<td style="text-align: center;">$\mathbf{8 1 . 0}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">LLM-based Generalist Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Galactica-6.7B</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">78.4</td>
</tr>
<tr>
<td style="text-align: left;">Galactica-30B</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">82.2</td>
</tr>
<tr>
<td style="text-align: left;">Galactica-120B</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">$\underline{82.6}$</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-v1.5-13B-16k (4-shot)</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-v1.3-7B ${ }^{\star}$</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2.7B-chat ${ }^{\dagger}$</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">InstructMol-G-6.9B</td>
<td style="text-align: center;">$\underline{85.9}$</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">$\underline{74.0}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">InstructMol-GS-6.9B</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">$\underline{70.0}$</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">BioT5+</td>
<td style="text-align: center;">$\mathbf{8 6 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 5}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 3}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 3}$</td>
</tr>
</tbody>
</table>
<p>benchmark datasets, which can be classified into 7 molecule-oriented tasks and 8 protein-oriented tasks with 3 types of problems: classification, regression, and generation. Following Fang et al., 2023, we categorize downstream tasks into different groups for multi-task instruction tuning in the same way, and details about the downstream datasets and baselines are in Appendix Section G.</p>
<h3>4.1 Molecule-oriented Tasks</h3>
<p>The molecule-oriented tasks cover different topics. As we incorporate IUPAC name for molecule in the pre-training, we also use IUPAC in some moleculeoriented tasks, such as molecule property prediction and molecule description generation. More details are in the following sections and Appendix.</p>
<h3>4.1.1 Molecule Property Prediction</h3>
<p>Molecule property prediction is a crucial task in bioinformatics, focusing on the determination of specific properties exhibited by a given molecule. Following Cao et al., 2023, we explore the ability of BioT5+ on MoleculeNet (Wu et al., 2018) benchmark. For classification tasks, we focus on 4 benchmark datasets: BACE, BBBP, HIV, and Clintox. Each sample includes an instruction detailing the property to be predicted and the molecule SELFIES with IUPAC name, with models required to generate a simple "yes" or "no" prediction. For regression tasks, we focus on 3 regression benchmarks from QM9 dataset, which aims to predict quantum mechanical properties of molecules, based on the</p>
<p>Table 2: Performance comparison on chemical reaction-related tasks (Best, Second Best). * means LoRA tuning.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">EXACT $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">LEVENSITEIN $\downarrow$</th>
<th style="text-align: center;">RDK FTS $\uparrow$</th>
<th style="text-align: center;">MACCS FTS $\uparrow$</th>
<th style="text-align: center;">MORGAN FTS $\uparrow$</th>
<th style="text-align: center;">VALIDITY $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">- Reagent Prediction</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama-7B</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">28.040</td>
<td style="text-align: center;">0.037</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.001</td>
</tr>
<tr>
<td style="text-align: center;">Galactica-6.7B</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.141</td>
<td style="text-align: center;">30.760</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.127</td>
<td style="text-align: center;">0.051</td>
<td style="text-align: center;">0.995</td>
</tr>
<tr>
<td style="text-align: center;">Terra-Chen-75-223M</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">49.323</td>
<td style="text-align: center;">0.039</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.052</td>
<td style="text-align: center;">0.313</td>
</tr>
<tr>
<td style="text-align: center;">Mol-Instructions-7B</td>
<td style="text-align: center;">0.044</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">23.167</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">0.364</td>
<td style="text-align: center;">0.213</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">Llama-7B*-LoRA</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">53.510</td>
<td style="text-align: center;">0.136</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.106</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">InstructMol-G-6.9B</td>
<td style="text-align: center;">0.070</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">24.732</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">InstructMol-GS-6.9B</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">19.664</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">BioT5+</td>
<td style="text-align: center;">0.257</td>
<td style="text-align: center;">0.695</td>
<td style="text-align: center;">12.901</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.621</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">- Forward Reaction Prediction</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama-7B</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.020</td>
<td style="text-align: center;">42.002</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.039</td>
</tr>
<tr>
<td style="text-align: center;">Galactica-6.7B</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">35.021</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.257</td>
<td style="text-align: center;">0.097</td>
<td style="text-align: center;">0.946</td>
</tr>
<tr>
<td style="text-align: center;">Terra-Chen-75-223M</td>
<td style="text-align: center;">0.239</td>
<td style="text-align: center;">0.782</td>
<td style="text-align: center;">20.413</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">0.762</td>
</tr>
<tr>
<td style="text-align: center;">Mol-Instructions-7B</td>
<td style="text-align: center;">0.045</td>
<td style="text-align: center;">0.654</td>
<td style="text-align: center;">27.262</td>
<td style="text-align: center;">0.313</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">Llama-7B*-LoRA</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">0.804</td>
<td style="text-align: center;">29.947</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.649</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">InstructMol-G-6.9B</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">20.155</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.717</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">InstructMol-GS-6.9B</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">10.851</td>
<td style="text-align: center;">0.776</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.741</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">BioT5+</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">3.403</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">- Retrosynthesis</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama-7B</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">46.844</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.029</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">0.010</td>
</tr>
<tr>
<td style="text-align: center;">Galactica-6.7B</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">34.940</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.134</td>
<td style="text-align: center;">0.986</td>
</tr>
<tr>
<td style="text-align: center;">Terra-Chen-75-223M</td>
<td style="text-align: center;">0.141</td>
<td style="text-align: center;">0.765</td>
<td style="text-align: center;">24.043</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.765</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.698</td>
</tr>
<tr>
<td style="text-align: center;">Mol-Instructions-7B</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">31.227</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.487</td>
<td style="text-align: center;">0.230</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">Llama-7B*-LoRA</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">53.510</td>
<td style="text-align: center;">0.136</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.106</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">InstructMol-G-6.9B</td>
<td style="text-align: center;">0.114</td>
<td style="text-align: center;">0.586</td>
<td style="text-align: center;">21.271</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.285</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">InstructMol-GS-6.9B</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">13.967</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">BioT5+</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">6.710</td>
<td style="text-align: center;">0.897</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.866</td>
<td style="text-align: center;">1.000</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance (MAE) comparison on molecule property prediction tasks (regression) on QM9 dataset from MoleculeNet (Wu et al., 2018) benchmark (Best, Second Best). $\Delta \epsilon$ means HOMO-LUMO gap.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">METHOD</th>
<th style="text-align: left;">HOMO $\downarrow$</th>
<th style="text-align: left;">LUMO $\downarrow$</th>
<th style="text-align: left;">$\Delta \epsilon \downarrow$</th>
<th style="text-align: left;">AVO $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">-LLM-based Generalist Models</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Llama2-7B (5-shot ICL)</td>
<td style="text-align: left;">0.7367</td>
<td style="text-align: left;">0.8641</td>
<td style="text-align: left;">0.5152</td>
<td style="text-align: left;">0.7510</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-13B (5-shot ICL)</td>
<td style="text-align: left;">0.7135</td>
<td style="text-align: left;">3.6807</td>
<td style="text-align: left;">1.5407</td>
<td style="text-align: left;">1.9783</td>
</tr>
<tr>
<td style="text-align: left;">Mol-Instructions-7B</td>
<td style="text-align: left;">0.0210</td>
<td style="text-align: left;">0.0210</td>
<td style="text-align: left;">0.0203</td>
<td style="text-align: left;">0.0210</td>
</tr>
<tr>
<td style="text-align: left;">InstructMol-G-6.9B</td>
<td style="text-align: left;">0.0060</td>
<td style="text-align: left;">0.0070</td>
<td style="text-align: left;">0.0082</td>
<td style="text-align: left;">0.0070</td>
</tr>
<tr>
<td style="text-align: left;">InstructMol-GS-6.9B</td>
<td style="text-align: left;">0.0048</td>
<td style="text-align: left;">0.0050</td>
<td style="text-align: left;">0.0061</td>
<td style="text-align: left;">0.0050</td>
</tr>
<tr>
<td style="text-align: left;">BioT5+</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 2 2}$</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 2 4}$</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 2 8}$</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 2 5}$</td>
</tr>
</tbody>
</table>
<p>molecule SELFIES with IUPAC name, including HUMO, LUMO, and the HUMO-LUMO gap.</p>
<p>Results. The results for classification and regression tasks are shown in Table 1 and Table 3 respectively. BioT5+ demonstrates superior performance over other generalist model baselines. Notably, for classification tasks, BioT5+ surpasses models like Galactica (Taylor et al., 2022), which is extensively trained on a vast corpus of scientific literature. Similarly, InstructMol (Cao et al., 2023), despite its inclusion of 2D graph information and LLMs, BioT5+ outperforms on both classification and regression tasks. This enhanced performance can be attributed to the integration of IUPAC names, wrapped text, bio-text, and molecule-text pairs in BioT5+ pre-training. The presence of molecule property descriptions in the context of these diverse corpora allows the model to acquire a comprehensive understanding of molecular properties. However, when compared to single-task specialist models, BioT5+ showed some gaps. This discrepancy is understandable and can be attributed partly to the ease of tuning inherent in single-task models and partly to the fact that some baselines incorporated additional molecular information, such as 2D and 3D structures.</p>
<h3>4.1.2 Chemical Reaction-related Tasks</h3>
<p>In computational chemistry, tasks related to chemical reactions are of vital importance as they can speed up development processes. Following Cao et al., 2023, we focus on 3 such tasks: reagent prediction, forward reaction prediction, and retrosynthesis.
Results. The main results are presented in Table 2 and full results are in Table 10. While LLMs have been exposed to some molecular data during pretraining, their direct zero-shot testing on chemical reaction-related tasks demonstrated extremely poor performance. Mol-Instructions (Fang et al., 2023) conducts multi-task instruction tuning based on Llama (Touvron et al., 2023a) with moleculeoriented tasks. InstructMol (Cao et al., 2023) introduces a molecule graph encoder to encode 2D molecular graph information for Vicuna (Chiang et al., 2023). Our BioT5+ follows the same training setting with Mol-Instructions (Fang et al., 2023) and shows superior performance across almost all metrics on chemical reaction-related tasks. This outcome demonstrates the effectiveness of joint pre-training on both molecular and textual data.</p>
<h3>4.1.3 Molecule Description Generation</h3>
<p>The objective of molecule description generation is to generate a detailed and informative description for a given molecule. To be consistent with BioT5+ pre-training, the input here also consists of molecule SELFIES with IUPAC. Unlike molecule property prediction, which often focuses on specific attributes, molecule description generation involves interpreting and conveying a comprehensive narrative of the molecule. This narrative encompasses not only its molecular composition and properties but also its potential applications and roles,</p>
<p>Table 4: Performance comparison on molecule description generation task on ChEBI-20 <em>Edwards et al. (2022)</em> dataset.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>BLEU-2$\uparrow$</th>
<th>BLEU-4$\uparrow$</th>
<th>ROUGE-1$\uparrow$</th>
<th>ROUGE-2$\uparrow$</th>
<th>ROUGE-L$\uparrow$</th>
<th>METEOR$\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Single-task Specialist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Transformer</td>
<td>0.061</td>
<td>0.027</td>
<td>0.204</td>
<td>0.087</td>
<td>0.186</td>
<td>0.114</td>
</tr>
<tr>
<td>T5-base</td>
<td>0.511</td>
<td>0.423</td>
<td>0.607</td>
<td>0.451</td>
<td>0.550</td>
<td>0.539</td>
</tr>
<tr>
<td>BioT5-base</td>
<td>0.540</td>
<td>0.457</td>
<td>0.634</td>
<td>0.485</td>
<td>0.568</td>
<td>0.569</td>
</tr>
<tr>
<td>MolMo (MolT5-base)</td>
<td>0.549</td>
<td>0.462</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>0.576</td>
</tr>
<tr>
<td>MolFM (MolT5-base)</td>
<td>0.585</td>
<td>0.498</td>
<td>0.653</td>
<td>0.508</td>
<td>0.594</td>
<td>0.607</td>
</tr>
<tr>
<td>MolXPT</td>
<td>0.594</td>
<td>0.505</td>
<td>0.660</td>
<td>0.511</td>
<td>0.597</td>
<td>0.626</td>
</tr>
<tr>
<td>GIT-Mol-graph</td>
<td>0.290</td>
<td>0.210</td>
<td>0.540</td>
<td>0.445</td>
<td>0.512</td>
<td>0.491</td>
</tr>
<tr>
<td>GIT-Mol-SMILES</td>
<td>0.264</td>
<td>0.176</td>
<td>0.477</td>
<td>0.374</td>
<td>0.451</td>
<td>0.430</td>
</tr>
<tr>
<td>GIT-Mol-graph+SMILES)</td>
<td>0.352</td>
<td>0.263</td>
<td>0.575</td>
<td>0.485</td>
<td>0.560</td>
<td>0.430</td>
</tr>
<tr>
<td>Text+Chem T5</td>
<td>0.625</td>
<td>0.542</td>
<td>0.682</td>
<td>0.543</td>
<td>0.622</td>
<td>0.648</td>
</tr>
<tr>
<td>BioT5</td>
<td>0.635</td>
<td>0.556</td>
<td>0.692</td>
<td>0.559</td>
<td>0.633</td>
<td>0.656</td>
</tr>
<tr>
<td>MolCA</td>
<td>0.639</td>
<td>0.555</td>
<td>0.697</td>
<td>0.558</td>
<td>0.636</td>
<td>0.669</td>
</tr>
<tr>
<td>Retrieval-Based LLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3.5-turbo (10-shot MolRoGPT)</td>
<td>0.565</td>
<td>0.482</td>
<td>0.623</td>
<td>0.450</td>
<td>0.543</td>
<td>0.585</td>
</tr>
<tr>
<td>GPT-4-0314 (10-shot MolRoGPT)</td>
<td>0.607</td>
<td>0.525</td>
<td>0.634</td>
<td>0.476</td>
<td>0.562</td>
<td>0.610</td>
</tr>
<tr>
<td>LLM-based Generalist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3.5-turbo (zero-shot)</td>
<td>0.103</td>
<td>0.050</td>
<td>0.261</td>
<td>0.088</td>
<td>0.204</td>
<td>0.161</td>
</tr>
<tr>
<td>BioMolGPT-10B</td>
<td>0.234</td>
<td>0.141</td>
<td>0.386</td>
<td>0.206</td>
<td>0.332</td>
<td>0.308</td>
</tr>
<tr>
<td>Mol-Instructions-7B</td>
<td>0.249</td>
<td>0.171</td>
<td>0.331</td>
<td>0.203</td>
<td>0.289</td>
<td>0.271</td>
</tr>
<tr>
<td>Instru2Mol-G-6.9B</td>
<td>0.466</td>
<td>0.365</td>
<td>0.547</td>
<td>0.365</td>
<td>0.479</td>
<td>0.491</td>
</tr>
<tr>
<td>Instru2Mol-GS-6.9B</td>
<td>0.475</td>
<td>0.371</td>
<td>0.566</td>
<td>0.394</td>
<td>0.502</td>
<td>0.509</td>
</tr>
<tr>
<td>BioT5+</td>
<td>0.666</td>
<td>0.591</td>
<td>0.710</td>
<td>0.584</td>
<td>0.650</td>
<td>0.681</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance comparison on description-guided molecule design task on ChEBI-20 <em>Edwards et al. (2022)</em> dataset. The ground truth Text2Mol <em>Edwards et al. (2021)</em> score is 0.609.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>BLEU$\uparrow$</th>
<th>EXACT$\uparrow$</th>
<th>LEVENSHRENG$\downarrow$</th>
<th>MACCS FTS$\uparrow$</th>
<th>RDK FTS$\uparrow$</th>
<th>MORGAN FTS$\uparrow$</th>
<th>FCD$\downarrow$</th>
<th>TEXT2Mol$\uparrow$</th>
<th>VALIDITY$\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Single-task Specialist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Transformer</td>
<td>0.499</td>
<td>0.000</td>
<td>57.660</td>
<td>0.480</td>
<td>0.320</td>
<td>0.217</td>
<td>11.32</td>
<td>0.277</td>
<td>0.906</td>
</tr>
<tr>
<td>T5-base</td>
<td>0.762</td>
<td>0.069</td>
<td>24.950</td>
<td>0.731</td>
<td>0.605</td>
<td>0.545</td>
<td>2.48</td>
<td>0.499</td>
<td>0.660</td>
</tr>
<tr>
<td>MolT5-base</td>
<td>0.769</td>
<td>0.081</td>
<td>24.458</td>
<td>0.721</td>
<td>0.588</td>
<td>0.529</td>
<td>2.18</td>
<td>0.496</td>
<td>0.772</td>
</tr>
<tr>
<td>MolMo-base</td>
<td>0.815</td>
<td>0.183</td>
<td>20.520</td>
<td>0.847</td>
<td>0.737</td>
<td>0.678</td>
<td>-</td>
<td>0.580</td>
<td>0.863</td>
</tr>
<tr>
<td>MolFM-base</td>
<td>0.822</td>
<td>0.210</td>
<td>19.445</td>
<td>0.854</td>
<td>0.758</td>
<td>0.697</td>
<td>-</td>
<td>0.583</td>
<td>0.892</td>
</tr>
<tr>
<td>GIT-Mol</td>
<td>0.756</td>
<td>0.051</td>
<td>26.315</td>
<td>0.738</td>
<td>0.582</td>
<td>0.519</td>
<td>-</td>
<td>-</td>
<td>0.928</td>
</tr>
<tr>
<td>MolXPT</td>
<td>-</td>
<td>0.215</td>
<td>-</td>
<td>0.859</td>
<td>0.757</td>
<td>0.667</td>
<td>0.45</td>
<td>0.578</td>
<td>0.983</td>
</tr>
<tr>
<td>BioT5</td>
<td>0.867</td>
<td>0.413</td>
<td>15.097</td>
<td>0.886</td>
<td>0.801</td>
<td>0.734</td>
<td>0.43</td>
<td>0.576</td>
<td>1.000</td>
</tr>
<tr>
<td>Retrieval-based LLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Llama2-7B (2-shot MolReGPT)</td>
<td>0.693</td>
<td>0.022</td>
<td>36.77</td>
<td>0.808</td>
<td>0.717</td>
<td>0.609</td>
<td>4.90</td>
<td>0.149</td>
<td>0.761</td>
</tr>
<tr>
<td>GPT-3.5-turbo (10-shot MolReGPT)</td>
<td>0.790</td>
<td>0.139</td>
<td>24.91</td>
<td>0.847</td>
<td>0.708</td>
<td>0.624</td>
<td>0.57</td>
<td>0.571</td>
<td>0.887</td>
</tr>
<tr>
<td>GPT-4-0314 (10-shot MolReGPT)</td>
<td>0.857</td>
<td>0.280</td>
<td>17.14</td>
<td>0.903</td>
<td>0.805</td>
<td>0.739</td>
<td>0.41</td>
<td>0.593</td>
<td>0.899</td>
</tr>
<tr>
<td>LLM-based Generalist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Llama2-7B (0-shot)</td>
<td>0.104</td>
<td>0.000</td>
<td>84.18</td>
<td>0.243</td>
<td>0.119</td>
<td>0.089</td>
<td>42.01</td>
<td>0.148</td>
<td>0.631</td>
</tr>
<tr>
<td>GPT-3.5-turbo (0-shot)</td>
<td>0.489</td>
<td>0.019</td>
<td>52.13</td>
<td>0.705</td>
<td>0.462</td>
<td>0.367</td>
<td>2.05</td>
<td>0.479</td>
<td>0.802</td>
</tr>
<tr>
<td>BioT5+</td>
<td>0.872</td>
<td>0.522</td>
<td>12.776</td>
<td>0.907</td>
<td>0.835</td>
<td>0.779</td>
<td>0.353</td>
<td>0.579</td>
<td>1.000</td>
</tr>
</tbody>
</table>
<p>Table 6: Performance (accuracy) comparison on PEER benchmark (Best, Second Best). * means linear probing.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Solubility</th>
<th>Localization</th>
<th>Yeast</th>
<th>Human</th>
</tr>
</thead>
<tbody>
<tr>
<td>Single-task Specialist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DDE</td>
<td>59.77 $\pm 1.21$</td>
<td>77.43 $\pm 0.42$</td>
<td>55.83 $\pm 3.13$</td>
<td>62.77 $\pm 2.30$</td>
</tr>
<tr>
<td>Moran</td>
<td>57.73 $\pm 1.33$</td>
<td>55.63 $\pm 0.85$</td>
<td>53.00 $\pm 0.50$</td>
<td>54.67 $\pm 4.43$</td>
</tr>
<tr>
<td>LSTM</td>
<td>70.18 $\pm 0.63$</td>
<td>88.11 $\pm 0.14$</td>
<td>53.62 $\pm 2.72$</td>
<td>63.75 $\pm 5.12$</td>
</tr>
<tr>
<td>Transformer</td>
<td>70.12 $\pm 0.31$</td>
<td>75.74 $\pm 0.74$</td>
<td>54.12 $\pm 1.27$</td>
<td>59.58 $\pm 2.09$</td>
</tr>
<tr>
<td>CNN</td>
<td>64.43 $\pm 0.25$</td>
<td>82.67 $\pm 0.32$</td>
<td>55.07 $\pm 0.02$</td>
<td>62.60 $\pm 1.67$</td>
</tr>
<tr>
<td>ResNet</td>
<td>67.33 $\pm 1.46$</td>
<td>78.99 $\pm 4.41$</td>
<td>48.91 $\pm 1.78$</td>
<td>68.61 $\pm 3.78$</td>
</tr>
<tr>
<td>Postbert</td>
<td>68.15 $\pm 0.92$</td>
<td>91.32 $\pm 0.89$</td>
<td>63.72 $\pm 2.80$</td>
<td>77.32 $\pm 1.10$</td>
</tr>
<tr>
<td>Postbert*</td>
<td>59.17 $\pm 0.21$</td>
<td>81.54 $\pm 0.09$</td>
<td>53.87 $\pm 0.38$</td>
<td>83.61 $\pm 1.34$</td>
</tr>
<tr>
<td>ESM-1B</td>
<td>70.23 $\pm 0.75$</td>
<td>92.40 $\pm 0.35$</td>
<td>57.00 $\pm 6.38$</td>
<td>78.17 $\pm 2.91$</td>
</tr>
<tr>
<td>ESM-1B*</td>
<td>67.02 $\pm 0.40$</td>
<td>91.61 $\pm 0.10$</td>
<td>66.07 $\pm 0.58$</td>
<td>88.06 $\pm 0.24$</td>
</tr>
<tr>
<td>BioT5</td>
<td>74.65 $\pm 0.49$</td>
<td>91.69 $\pm 0.05$</td>
<td>64.89 $\pm 0.43$</td>
<td>86.22 $\pm 0.53$</td>
</tr>
<tr>
<td>Multi-task Generalist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CNN</td>
<td>70.63 $\pm 0.34$</td>
<td>82.67 $\pm 0.72$</td>
<td>54.50 $\pm 1.61$</td>
<td>69.03 $\pm 2.68$</td>
</tr>
<tr>
<td>Transformer</td>
<td>70.03 $\pm 0.42$</td>
<td>76.27 $\pm 0.57$</td>
<td>54.00 $\pm 1.17$</td>
<td>67.33 $\pm 2.68$</td>
</tr>
<tr>
<td>ESM-1B</td>
<td>70.46 $\pm 0.16$</td>
<td>92.50 $\pm 0.26$</td>
<td>64.76 $\pm 1.42$</td>
<td>83.00 $\pm 0.88$</td>
</tr>
<tr>
<td>BioT5+</td>
<td>74.37 $\pm 0.19$</td>
<td>90.41 $\pm 0.07$</td>
<td>66.16 $\pm 0.43$</td>
<td>85.09 $\pm 0.40$</td>
</tr>
</tbody>
</table>
<p>as derived from the integration of SELFIES representations and IUPAC names. We use the same evaluation metrics as Fang et al., 2023.</p>
<p>Results. As presented in Table 4, our BioT5+ outperforms all compared single-task specialist, retrieval-based LLMs, and multi-task generalist baselines. This superior performance can be attributed to the comprehensive learning during the pre-training of BioT5+. The model has effectively assimilated a multi-dimensional and rich textual description of molecules.</p>
<h4>4.1.4 Description-guided Molecule Design</h4>
<p>Description-guided molecule design is essentially the inverse task of molecule description generation, which requires generating a molecule based on a provided textual description. In BioT5+ setting, we do not include IUPAC name in the textual description of the molecule to prevent the model from learning a simplistic mapping from IUPAC name to its SELFIES representation, thereby ensuring the model does not overlook the other descriptive elements provided in the text.</p>
<p>Results. Table 5 presents the results for description-guided molecule design task. Our BioT5+ surpasses all the compared baselines. This achievement underscores the efficacy of BioT5+ pretraining, where the model has acquired a profound understanding of molecular knowledge.</p>
<h3>4.2 Protein-oriented Tasks</h3>
<h4>4.2.1 Protein Description Generation</h4>
<p>The task of protein description generation involves deriving relevant textual information from a given protein sequence. Following Fang et al., 2023, we mainly focus on 4 related generation tasks: protein function generation, catalytic activity generation,</p>
<p>Table 7: Ablation of IUPAC and additional data on molecule description generation task with single task setting. B-2 stands for BLEU-2 and R-1 in ROUGE-1.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">B-2 $\uparrow$</th>
<th style="text-align: center;">B-4 $\uparrow$</th>
<th style="text-align: center;">R-1 $\uparrow$</th>
<th style="text-align: center;">R-2 $\uparrow$</th>
<th style="text-align: center;">R-L $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BioT5+(single task)</td>
<td style="text-align: center;">0.671</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.715</td>
<td style="text-align: center;">0.590</td>
<td style="text-align: center;">0.655</td>
<td style="text-align: center;">0.687</td>
</tr>
<tr>
<td style="text-align: center;">BioT5+(single task)</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.584</td>
<td style="text-align: center;">0.706</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">0.677</td>
</tr>
<tr>
<td style="text-align: center;">wo IUPAC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BioT5+(single task)</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;">0.586</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.681</td>
</tr>
<tr>
<td style="text-align: center;">wo additional data</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BioT5+</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">0.584</td>
<td style="text-align: center;">0.650</td>
<td style="text-align: center;">0.681</td>
</tr>
</tbody>
</table>
<p>Table 8: Performance (AUROC) comparison on the 3 DTI datasets(Best, Second Best).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BioSNAP</th>
<th style="text-align: center;">Human</th>
<th style="text-align: center;">BindingDB</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Single-task Specialist Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SVM</td>
<td style="text-align: center;">$0.862 \pm 0.007$</td>
<td style="text-align: center;">$0.940 \pm 0.006$</td>
<td style="text-align: center;">$0.939 \pm 0.001$</td>
</tr>
<tr>
<td style="text-align: center;">RF</td>
<td style="text-align: center;">$0.860 \pm 0.005$</td>
<td style="text-align: center;">$0.952 \pm 0.011$</td>
<td style="text-align: center;">$0.942 \pm 0.011$</td>
</tr>
<tr>
<td style="text-align: center;">DeepConv-DTI</td>
<td style="text-align: center;">$0.886 \pm 0.006$</td>
<td style="text-align: center;">$0.980 \pm 0.002$</td>
<td style="text-align: center;">$0.945 \pm 0.002$</td>
</tr>
<tr>
<td style="text-align: center;">GraphDTA</td>
<td style="text-align: center;">$0.887 \pm 0.008$</td>
<td style="text-align: center;">$0.981 \pm 0.001$</td>
<td style="text-align: center;">$0.951 \pm 0.002$</td>
</tr>
<tr>
<td style="text-align: center;">MolTrans</td>
<td style="text-align: center;">$0.895 \pm 0.004$</td>
<td style="text-align: center;">$0.980 \pm 0.002$</td>
<td style="text-align: center;">$0.952 \pm 0.002$</td>
</tr>
<tr>
<td style="text-align: center;">DrugBAN</td>
<td style="text-align: center;">$\underline{0.903 \pm 0.005}$</td>
<td style="text-align: center;">$\underline{0.982 \pm 0.002}$</td>
<td style="text-align: center;">$\underline{0.960 \pm 0.001}$</td>
</tr>
<tr>
<td style="text-align: center;">BioT5</td>
<td style="text-align: center;">$0.937 \pm 0.001$</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 9} \pm \mathbf{0 . 0 0 1}$</td>
<td style="text-align: center;">$0.963 \pm 0.001$</td>
</tr>
<tr>
<td style="text-align: center;">Multi-task Generalist Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BioT5+</td>
<td style="text-align: center;">$\mathbf{0 . 9 3 9} \pm \mathbf{0 . 0 0 1}$</td>
<td style="text-align: center;">$0.987 \pm 0.001$</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 4} \pm \mathbf{0 . 0 0 1}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Ablation of default T5 tokenizer and character-based tokenizer (BioT5+) on QM9 dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">HOMO $\downarrow$</th>
<th style="text-align: center;">LUMO $\downarrow$</th>
<th style="text-align: center;">$\Delta \epsilon \downarrow$</th>
<th style="text-align: center;">AvG $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">T5 default tokenizer</td>
<td style="text-align: center;">0.0024</td>
<td style="text-align: center;">0.0026</td>
<td style="text-align: center;">0.0032</td>
<td style="text-align: center;">0.0027</td>
</tr>
<tr>
<td style="text-align: center;">BioT5+</td>
<td style="text-align: center;">0.0022</td>
<td style="text-align: center;">0.0024</td>
<td style="text-align: center;">0.0028</td>
<td style="text-align: center;">0.0025</td>
</tr>
</tbody>
</table>
<p>domain/motif generation, and functional description generation.
Results. As shown in the Figure 2, BioT5+ surpasses all compared baselines across 4 tasks. This result highlights the advanced ability of BioT5+ to interpret complex protein sequences into meaningful textual information, indicating that BioT5+ gains a comprehensive understanding of protein structures and functions through pre-training.</p>
<h3>4.2.2 Protein Property Prediction</h3>
<p>Protein property prediction task involves predicting specific properties of proteins, such as solubility, structure, or function, based on their amino acid sequences. Following BioT5 (Pei et al., 2023), we focus on 2 protein property prediction tasks from PEER (Xu et al., 2022) benchmark, which is specifically designed for protein sequence understanding: (1) Solubility prediction: whether the input protein is soluble or not. (2) Localization prediction: either the input protein is "membrane-bound" or "soluble". Both of these tasks are binary classification tasks and the model needs to generate a "yes" or "no" prediction. The results are summarized together with the ones in the following Section 4.2.3.</p>
<h3>4.2.3 Protein-related Interaction Prediction</h3>
<p>In drug discovery, the prediction of interactions between bio-entities is very important, from which</p>
<p>Protein-Protein Interaction (PPI) and Drug-Target Interactions (DTI) are two key examples. These two tasks are essential for understanding biological processes and identifying potential therapeutic targets. To facilitate this, we follow Pei et al., 2023 to incorporate 2 PPI dataset from PEER (Xu et al., 2022) benchmark including Yeast and Human, and 3 DTI datasets including BioSNAP (Zitnik et al., 2018), Human (Liu et al., 2015; Chen et al., 2020), and BindingDB (Liu et al., 2007).
Results. As shown in Table 6, in the PEER benchmark, our BioT5+ demonstrates exceptional performance, surpassing other multi-task models in 3 out of the 4 tasks and achieving results comparable to single-task specialist models. Notably, in the Yeast PPI prediction task, BioT5+ exceeded the performance of all baseline models. This is particularly significant considering that the baseline ESM-1b (Rives et al., 2021) was specifically pre-trained on a vast array of protein sequences and possesses more than double the number of parameters compared to BioT5+. Furthermore, BioT5+ also showed superior performance in DTI tasks as in Table 8 (full results in Table 11), consistently outperforming other methods on the BioSNAP and BindingDB datasets. It is noteworthy that many baseline methods involved specialized designs for molecule and protein encoders. These results underscore the effectiveness of the joint pre-training of BioT5+ on bio-text, molecules, and proteins. This comprehensive understanding is evident in the ability of BioT5+ to accurately predict protein properties, interactions, and drug-target interactions, making it a valuable tool in the field of computational biology.</p>
<h3>4.2.4 Description-guided Protein Design</h3>
<p>For description-guided protein design, the model needs to generate protein amino acid sequences based on specific design requirements, such as protein structures and functions. Due to the absence of a well-established benchmark for this task, we present in Appendix Table 20 a selection of test cases along with their corresponding sequence similarity scores to provide a direct</p>
<p>comparison between our model and existing models like Galactica (Taylor et al., 2022) and MolInstructions (Fang et al., 2023).</p>
<h3>4.3 Ablation Studies</h3>
<p>In this section, we conduct ablation studies to investigate the effectiveness of our design in BioT5+. Specifically, we focus on the following 3 scenarios: (1) Do not incorporate IUPAC name of the molecule. As shown in Table 7, removing the IUPAC name results in a noticeable performance drop in the molecule description generation task. This decline highlights the significant role that IUPAC name plays in tasks related to molecular understanding. (2) Do not add PubMed Central and bioRxiv data in pre-training. Results in Table 7 and Table 13 indicate that these two datasets play a crucial role in enhancing molecular understanding. The omission of them leads to a slight but noticeable decrease in performance on molecule description generation and descriptionguided molecule design tasks. (3) Use T5 default tokenizer for numbers instead of characterbased tokenizer. The results in Table 9 demonstrate that the character-based approach for tokenizing numbers is more effective than the default T5 tokenizer on regression tasks. We also conduct an ablation study to further contrast single-task and multi-task tuning strategies in Appendix Section F.</p>
<h2>5 Conclusions</h2>
<p>BioT5+, as an advanced iteration of the BioT5 framework, represents a significant stride in computational biology and drug discovery. By integrating IUPAC names, expanding bio-text and molecular data sources, employing multi-task instruction tuning, and incorporating an advanced numerical tokenization technique, BioT5+ has successfully bridged the gap between molecular representations and their textual descriptions. The enhanced understanding of molecular structures and its ability to process complex biological data have been demonstrated across a wide range of tasks, with BioT5+ achieving state-of-the-art performance in most of them. This success highlights the potential of BioT5+ as a versatile and powerful tool in understanding and analyzing biological entities.</p>
<h2>6 Limitations</h2>
<p>Despite the significant advancements achieved by BioT5+, there remain certain limitations that need to be addressed in future work. Firstly, the model faces challenges in generalizing across various biological tasks, a problem that is distinct from common NLP settings. The intricate and unique nature of each biological task makes it difficult to develop a one-size-fits-all solution, highlighting the need for more specialized approaches within this domain. Secondly, the current scale of BioT5+ is somewhat limited and cannot comprehend and integrate information from other modalities, such as images, restricting its applicability in multi-modal biological data analysis. BioT5+ is not equipped to function as a universal chatbot or to answer queries spanning general domain questions outside the specific scope of biology. This constraint highlights the need for developing larger, more versatile models capable of handling a wider range of data types and answering a broader array of questions in both biological and general domains.</p>
<h2>7 Ethical Considerations</h2>
<p>While BioT5+ presents a significant advancement, its capabilities, particularly in generating molecules based on textual descriptions and predicting chemical reaction products, raise important ethical considerations. One of the concerns is the potential misuse of this technology to generate harmful or dangerous molecules, which could pose risks to public safety and environmental health. Moreover, the ability of BioT5+ to predict and generate novel molecules may also lead to issues surrounding intellectual property rights and patenting. The ease with which new compounds can be designed and synthesized using AI-driven methods could potentially disrupt traditional research practices and raise questions about the ownership of these discoveries.</p>
<h2>8 Acknowledgements</h2>
<p>This work was supported by the National Natural Science Foundation of China (NSFC Grant No. 62122089), Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098, and Intelligent Social Governance Platform, Major Innovation \&amp; Planning Interdisciplinary Platform for the "Double-First Class" Initiative, Renmin University of China, the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China. Qizhi Pei is supported by the Outstanding Innovative Talents Cultivation Funded Programs 2023 of Renmin University of China.</p>
<h2>References</h2>
<p>Hadi Abdine, Michail Chatzianastasis, Costas Bouyioukos, and Michalis Vazirgiannis. 2023. Prot2text: Multimodal protein's function generation with GNNs and transformers. In NeurIPS 2023 AI for Science Workshop.</p>
<p>Microsoft Research AI4Science and Microsoft Azure Quantum. 2023. The impact of large language models on scientific discovery: a preliminary study using gpt-4. arXiv preprint arXiv:2311.07361.</p>
<p>Peizhen Bai, Filip Miljkoviƒá, Bino John, and Haiping Lu. 2023. Interpretable bilinear attention network with domain adaptation improves drug-target prediction. Nature Machine Intelligence, 5(2):126-136.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005, pages 65-72. Association for Computational Linguistics.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Kathi Canese and Sarah Weis. 2013. Pubmed: the bibliographic database. The NCBI handbook, 2(1).</p>
<p>He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. 2023. Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery. CoRR, abs/2311.16208.</p>
<p>Lifan Chen, Xiaoqin Tan, Dingyan Wang, Feisheng Zhong, Xiaohong Liu, Tianbiao Yang, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, and Mingyue Zheng. 2020. Transformercpi: improving compoundprotein interaction prediction by sequence-based deep learning with self-attention mechanism and label reversal experiments. Bioinformatics, 36(16):4406-4414.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.</p>
<p>Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. 2020. Chemberta: Large-scale selfsupervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885.</p>
<p>Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Matteo Manica. 2023. Unifying molecular and textual representations via multi-task language modelling. In International Conference on Machine Learning, ICML 2023,</p>
<p>23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 6140-6157. PMLR.</p>
<p>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine learning, 20(3):273-297.</p>
<p>Suresh Dara, Swetha Dhamercherla, Surender Singh Jadav, Ch Madhu Babu, and Mohamed Jawed Ahsan. 2022. Machine learning in drug discovery: A review. Artif. Intell. Rev., 55(3):1947-1999.</p>
<p>Joseph L Durant, Burton A Leland, Douglas R Henry, and James G Nourse. 2002. Reoptimization of mdl keys for use in drug discovery. Journal of chemical information and computer sciences, 42(6):12731280.</p>
<p>Carl Edwards, Tuan Manh Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. 2022. Translation between molecules and natural language. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 375-413. Association for Computational Linguistics.</p>
<p>Carl Edwards, ChengXiang Zhai, and Heng Ji. 2021. Text2mol: Cross-modal molecule retrieval with natural language queries. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 595-607. Association for Computational Linguistics.</p>
<p>Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. 2021. Prottrans: Toward understanding the language of life through self-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 44(10):7112-7127.</p>
<p>Aohan Zeng et.al. 2023. GLM-130b: An open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations (ICLR).</p>
<p>Xiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, and Haifeng Wang. 2022. Geometry-enhanced molecular representation learning for property prediction. Nature Machine Intelligence, 4(2):127-134.</p>
<p>Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. 2023. Mol-instructions: A large-scale biomolecular instruction dataset for large language models. CoRR, abs/2306.08018.</p>
<p>Zhi-Ping Feng and Chun-Ting Zhang. 2000. Prediction of membrane protein types based on the hydrophobic index of amino acids. Journal of protein chemistry, 19:269-275.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778 .</p>
<p>Tin Kam Ho. 1995. Random decision forests. In Proceedings of 3rd International Conference on Document Analysis and Recognition, volume 1, pages 278-282.</p>
<p>Sepp Hochreiter and J√ºrgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):17351780 .</p>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Kexin Huang, Cao Xiao, Lucas Glass, and Jimeng Sun. 2021. MolTrans: Molecular interaction transformer for drug-target interaction prediction. Bioinformatics, 37:830 - 836.</p>
<p>John J Irwin, Khanh G Tang, Jennifer Young, Chinzorig Dandarchuluun, Benjamin R Wong, Munkhzul Khurelbaatar, Yurii S Moroz, John Mayfield, and Roger A Sayle. 2020. Zinc20-a free ultralarge-scale chemical database for ligand discovery. Journal of chemical information and modeling, 60(12):60656073 .</p>
<p>Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. 2019. Pubchem 2019 update: improved access to chemical data. Nucleic acids research, 47(D1):D1102-D1109.</p>
<p>Roman Klinger, Corinna Kol√°rik, Juliane Fluck, Martin Hofmann-Apitius, and Christoph M. Friedrich. 2008. Detection of IUPAC and iupac-like chemical names. In Proceedings 16th International Conference on Intelligent Systems for Molecular Biology (ISMB), Toronto, Canada, July 19-23, 2008, pages 268-276.</p>
<p>Mario Krenn, Florian H√§se, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. 2020. Selfreferencing embedded strings (selfies): A $100 \%$ robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024.</p>
<p>Taku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations, Brussels, Belgium, October 31 - November 4, 2018, pages 66-71. Association for Computational Linguistics.</p>
<p>Greg Landrum. 2021. Rdkit: Open-source cheminformatics software. GitHub release.</p>
<p>Ingoo Lee, Jongsoo Keum, and Hojung Nam. 2019. DeepConv-DTI: Prediction of drug-target interactions via deep learning with convolution on protein sequences. PLoS Computational Biology, 15.</p>
<p>Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, and Qing Li. 2024. Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective. IEEE Transactions on Knowledge and Data Engineering.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. 2022. Language models of protein sequences at the scale of evolution enable accurate structure prediction. BioRxiv.</p>
<p>David J Lipman and William R Pearson. 1985. Rapid and sensitive protein similarity searches. Science, 227(4693):1435-1441.</p>
<p>Hui Liu, Jianjiang Sun, Jihong Guan, Jie Zheng, and Shuigeng Zhou. 2015. Improving compound-protein interaction prediction by building up highly credible negative samples. Bioinformatics, 31(12):i221-i229.</p>
<p>Pengfei Liu, Yiming Ren, and Zhixiang Ren. 2023a. Git-mol: A multi-modal large language model for molecular science with graph, image, and text. CoRR, abs/2308.06911.</p>
<p>Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. 2022. Pretraining molecular graph representation with 3d geometry. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Shengchao Liu, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, Anthony Gitter, Chaowei Xiao, Jian Tang, Hongyu Guo, and Anima Anandkumar. 2023b. A text-guided protein design framework. CoRR, abs/2302.04611.</p>
<p>Tiedong Liu and Bryan Kian Hsiang Low. 2023. Goat: Fine-tuned llama outperforms GPT-4 on arithmetic tasks. CoRR, abs/2305.14201.</p>
<p>Tiqing Liu, Yuhmei Lin, Xin Wen, Robert N Jorissen, and Michael K Gilson. 2007. Bindingdb: a webaccessible database of experimentally determined protein-ligand binding affinities. Nucleic acids research, 35(suppl_1):D198-D201.</p>
<p>Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang, and Tie-Yan Liu. 2023c. Molxpt: Wrapping molecules with text for generative pre-training. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2023,</p>
<p>Toronto, Canada, July 9-14, 2023, pages 1606-1616. Association for Computational Linguistics.</p>
<p>Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. 2023d. MolCA: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. In The 2023 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Yizhen Luo, Kui Huang, Massimo Hong, Kai Yang, Jiahuan Zhang, Yushuai Wu, and Zaiqin Nie. 2023a. Empowering ai drug discovery with explicit and implicit knowledge. arXiv preprint arXiv:2305.01523.</p>
<p>Yizhen Luo, Kai Yang, Massimo Hong, Xing Yi Liu, and Zaiqing Nie. 2023b. Molfm: A multimodal molecular foundation model. CoRR, abs/2307.09484.</p>
<p>Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie. 2023c. Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine. CoRR, abs/2308.09442.</p>
<p>Frederic P Miller, Agnes F Vandome, and John McBrewster. 2009. Levenshtein distance: Information theory, computer science, string (computer science), string metric, damerau? levenshtein distance, spell checker, hamming distance.</p>
<p>Thin Nguyen, Hang Le, T. Quinn, Tri Minh Nguyen, Thuc Duy Le, and Svetha Venkatesh. 2021. GraphDTA: Predicting drug-target binding affinity with graph neural networks. Bioinformatics, 37(8):1140-1147.</p>
<p>Rodrigo Frassetto Nogueira, Zhiying Jiang, and Jimmy Lin. 2021. Investigating the limitations of the transformers with simple arithmetic tasks. CoRR, abs/2102.13019.</p>
<p>OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.</p>
<p>Keiron O'Shea and Ryan Nash. 2015. An introduction to convolutional neural networks. arXiv preprint arXiv:1511.08458.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 311-318. ACL.</p>
<p>William R Pearson and David J Lipman. 1988. Improved tools for biological sequence comparison. Proceedings of the National Academy of Sciences, 85(8):2444-2448.</p>
<p>Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, Yue Wang, Zun Wang, Tao Qin, and Rui Yan. 2024. Leveraging biomolecule and natural language through multi-modal learning: A survey. arXiv preprint arXiv:2403.01528.</p>
<p>Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, and Rui Yan. 2023. BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1102-1123, Singapore. Association for Computational Linguistics.</p>
<p>Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, and G√ºnter Klambauer. 2018. Fr√©chet chemnet distance: A metric for generative models for molecules in drug discovery. J. Chem. Inf. Model., 58(9):1736-1741.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551.</p>
<p>Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. 2021. STOUT: SMILES to IUPAC names using neural machine translation. J. Cheminformatics, 13(1):34.</p>
<p>Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. 2021. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118.</p>
<p>David Rogers and Mathew Hahn. 2010. Extendedconnectivity fingerprints. Journal of chemical information and modeling, 50(5):742-754.</p>
<p>Vijayakumar Saravanan and Namasivayam Gautham. 2015. Harnessing computational biology for exact linear b-cell epitope prediction: a novel amino acid composition-based feature descriptor. Omics: a journal of integrative biology, 19(10):648-658.</p>
<p>Nadine Schneider, Roger A. Sayle, and Gregory A. Landrum. 2015. Get your atoms in order - an opensource implementation of a novel and robust molecular canonicalization algorithm. J. Chem. Inf. Model., 55(10):2111-2120.</p>
<p>Richard Sever, Ted Roeder, Samantha Hindle, Linda Sussman, Kevin-John Black, Janet Argentine, Wayne Manos, and John R Inglis. 2019. biorxiv: the preprint server for biology. BioRxiv, page 833400.</p>
<p>Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and JiRong Wen. 2022. A molecular multimodal foundation model associating molecule graphs with natural language. arXiv preprint arXiv:2209.05481.</p>
<p>Mujeen Sung, Minbyul Jeong, Yonghwa Choi, Donghyeon Kim, Jinhyuk Lee, and Jaewoo Kang. 2022. Bern2: an advanced neural biomedical named entity recognition and normalization tool. Bioinformatics, 38(20):4837-4839.</p>
<p>Baris E Suzek, Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H Wu. 2007. Uniref: comprehensive and non-redundant uniprot reference clusters. Bioinformatics, 23(10):1282-1288.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. CoRR, abs/2211.09085.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur√©lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur√©lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.</p>
<p>Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. 2022. Molecular contrastive learning of representations via graph neural networks. Nature Machine Intelligence, 4(3):279-287.</p>
<p>David Weininger. 1988. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31-36.</p>
<p>David Weininger, Arthur Weininger, and Joseph L Weininger. 1989. Smiles. 2. algorithm for generation of unique smiles notation. Journal of chemical information and computer sciences, 29(2):97-101.</p>
<p>Jacob White. 2020. Pubmed 2.0. Medical reference services quarterly, 39(4):382-387.</p>
<p>Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. 2018. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513-530.</p>
<p>Canwen Xu, Daya Guo, Nan Duan, and Julian J. McAuley. 2023a. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. CoRR, abs/2304.01196.</p>
<p>Hanwen Xu, Addie Woicik, Hoifung Poon, Russ B Altman, and Sheng Wang. 2023b. Multilingual translation for zero-shot biomedical classification using biotranslator. Nature Communications, 14(1):738.</p>
<p>Minghao Xu, Zuobai Zhang, Jiarui Lu, Zhaocheng Zhu, Yangtian Zhang, Ma Chang, Runcheng Liu, and Jian Tang. 2022. Peer: a comprehensive and multi-task benchmark for protein sequence understanding. Advances in Neural Information Processing Systems, 35:35156-35173.</p>
<p>Yoshihiro Yamanishi, Michihiro Araki, Alex Gutteridge, Wataru Honda, and Minoru Kanehisa. 2008. Prediction of drug-target interaction networks from the integration of chemical and genomic spaces. In Proceedings 16th International Conference on Intelligent Systems for Molecular Biology (ISMB), Toronto, Canada, July 19-23, 2008, pages 232-240.</p>
<p>Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. 2020. Graph contrastive learning with augmentations. Advances in neural information processing systems, 33:58125823.</p>
<p>Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2022. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Nature communications, 13(1):862.</p>
<p>Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Chee-Kong Lee. 2021. Motif-based graph selfsupervised learning for molecular property prediction. Advances in Neural Information Processing Systems, 34:15870-15882.</p>
<p>Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. 2023. Uni-mol: A universal 3d molecular representation learning framework. In The</p>
<p>Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Marinka Zitnik, Rok Sosic, and Jure Leskovec. 2018. Biosnap datasets: Stanford biomedical network dataset collection. Note: http://snap. stanford. edu/biodata Cited by, 5(1).</p>
<h2>A Additional Results</h2>
<p>For the molecule description generation task and description-guided molecule design task, we also compare the performance of BioT5+ with baseline methods on Mol-Instruction (Fang et al., 2023) test sets. The results are shown in Table 15 and Table 16. BioT5+ demonstrate superior performance in almost all metrics compared to baseline methods, which underscore BioT5+'s advanced capabilities in understanding complex molecular data.</p>
<h2>B Hyper-parameters</h2>
<p>Pre-training. The pre-training process spans 300 K steps and is executed on eight NVIDIA 80GB A100 GPUs with batch size 96 per GPU. To balance the data from different tasks during pre-training, we adopt a batch-level balancing strategy, where each batch evenly includes data from all eight different tasks, ensuring a more balanced and comprehensive pre-training process. For small datasets, such as molecule-text pairs and proteintext pairs, we employ a round-robin strategy to repeat their usage multiple times, compensating for their limited size. The dropout rate is maintained at 0.0 and the maximum input length during pretraining is set at 512 . Optimization is performed using the AdamW (Loshchilov and Hutter, 2019) optimizer with Root Mean Square scaling. A cosine annealing learning rate scheduler is employed, with the base rate set at $1 e-2$ and the minimum rate at $1 e-5$.</p>
<p>Multi-task Fine-tuning. For multi-task finetuning, the dropout rate is searched in [0.0, 0.05, 0.1], and the learning rate is searched in [5e-5, 1e-4, 2e-4, 5e-4]. The total number of steps is 100 K and warmup steps is $6 \%$ of total steps. The batch size is set to 768 for molecule-oriented tasks and 96 for protein-oriented tasks. The best hyper-parameters for molecule-oriented and protein-oriented task are shown in Table 12.</p>
<h2>C NER and Entity Linking</h2>
<p>In general, our approach adheres to the same Named Entity Recognition (NER) and Entity Linking process as BioT5 (Pei et al., 2023) for bioentity name mentions in biological text using BERN2 (Sung et al., 2022). However, we have implemented some modifications to the process: (1) Upon analyzing the confidence scores of the identified bio-entities, we observed a long-tailed distribution, with the majority of bio-entity confidence scores exceeding 0.9. Based on this empirical finding, we set a threshold of 0.9 , retaining only those NER results that surpass this score. (2) With the introduction of IUPAC names into our workflow, we now assess whether a recognized molecular name is an IUPAC name. If it is, we exclusively append the SELFIES representation; if not, we append both the IUPAC name and the SELFIES. This dual approach ensures a more comprehensive and accurate representation of molecular entities in our analysis.</p>
<h2>D IUPAC Incorporation</h2>
<p>In our downstream tasks of molecule property prediction and molecule description generation, it is effective to enrich molecules with their IUPAC name. One reason is that IUPAC names are more commonly found in bio-text. By explicitly incorporating IUPAC names in the molecular context during pre-training, the model more readily learns relevant molecular knowledge and establishes connections between the molecule and its contextual information. Additionally, IUPAC names inherently contain structural information about the molecule, such as functional groups and other structural components. This information allows the model to better understand the molecular structure, thus predicting molecule properties with higher accuracy and generating more accurate and detailed descriptions.</p>
<h2>D. 1 Mapping Process</h2>
<p>Initially, we normalize the SMILES sequences provided in the dataset and map them to their respective PubChem (Kim et al., 2019) CIDs. Subsequently, these CIDs are used to retrieve the corresponding IUPAC names. However, for some molecules, their SMILES sequences do not correspond to a PubChem CID. In such cases, we employ STOUT (Rajan et al., 2021), a highly accurate SMILES to IUPAC name translator utilizing transformers, to convert these SMILES sequences</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Overview of BioT5+ pre-training. The solid line refers to the masked span prediction task proposed by T5 (Raffel et al., 2020). Each consecutive span of masked tokens is substituted with a sentinel token, represented as <M1>, <M2>, and <M3>. We apply this pre-training task to molecule IUPAC + SELFIES (task #1), molecule SELFIES (task #2), protein FASTA (task #3), general text (task #4), wrapped text (task #5), and bio-text (task #6). The dashed line symbolizes the bidirectional translation between structured text description and biological sequences. (task #7 and #8).</p>
<p>into IUPAC names. This multi-step process ensures that each molecule is accurately equipped with its IUPAC name, facilitating more effective prediction and generation tasks.</p>
<h2><strong>E Comparison with BioT5</strong></h2>
<p>Regarding the size of pre-training data and pre-processing techniques, BioT5+ introduces several enhancements over BioT5 (Pei et al., 2023):</p>
<ul>
<li>Addition of 28.8M PubChem molecular data, including molecule SELFIES along with their IUPAC names.</li>
<li>Addition of 28.8M full articles from PubMed.</li>
<li>Addition of 2.3M bioRxiv abstracts as wrapped biotext.</li>
<li>In the molecule-text bidirectional translation task, IUPAC representations are included in the text description.</li>
<li>For wrapped biotext pre-training, detected molecule names in BioT5 are directly replaced with corresponding SELFIES. In BioT5+, we first determine whether a molecule name is an IUPAC name; if so, it is appended with the corresponding SELFIES. If not, both the IUPAC name and SELFIES are appended. Besides, we further ensure data quality by only processing detected bioentities with a confidence score (from BERN2 (Sung et al., 2022)) above 0.9.</li>
<li>The numbers that appear in the pre-training corpus are tokenized character by character in BioT5+'s tokenizer.</li>
</ul>
<h2><strong>F Additional Ablation Study</strong></h2>
<p>To contrast single-task and multi-task tuning strategies, we further fine-tune BioT5+ with a single-task setting on three MoleculeNet (Wu et al., 2018) classification tasks including BACE, BBBP, Clintox, and three QM9 (Fang et al., 2023) regression tasks including HOMO, LUMO, and HOMO-LUMO gap. The consolidated results are shown in Table 14 (we also summarize the results in Table 7 and Table 13 for generation tasks on ChEBI-20 (Edwards et al., 2022) dataset here), covering 3 types of</p>
<p>Table 10: Performance comparison on chemical reaction-related tasks (Best, Second Best). * means LoRA tuning.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Exact $\uparrow$</th>
<th>BLEU $\uparrow$</th>
<th>Levenshtein $\downarrow$</th>
<th>RDK FTS $\uparrow$</th>
<th>MACCS FTS $\uparrow$</th>
<th>Morgan FTS $\uparrow$</th>
<th>Validity $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Reagent Prediction</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Alpaca-7B</td>
<td>0.000</td>
<td>0.026</td>
<td>29.037</td>
<td>0.029</td>
<td>0.016</td>
<td>0.001</td>
<td>0.186</td>
</tr>
<tr>
<td>Baize-7B</td>
<td>0.000</td>
<td>0.051</td>
<td>30.628</td>
<td>0.022</td>
<td>0.018</td>
<td>0.004</td>
<td>0.099</td>
</tr>
<tr>
<td>ChatGLM-6B</td>
<td>0.000</td>
<td>0.019</td>
<td>29.169</td>
<td>0.017</td>
<td>0.006</td>
<td>0.002</td>
<td>0.074</td>
</tr>
<tr>
<td>Llama-7B</td>
<td>0.000</td>
<td>0.003</td>
<td>28.040</td>
<td>0.037</td>
<td>0.001</td>
<td>0.001</td>
<td>0.001</td>
</tr>
<tr>
<td>Vicuna-7B</td>
<td>0.000</td>
<td>0.010</td>
<td>27.948</td>
<td>0.038</td>
<td>0.002</td>
<td>0.001</td>
<td>0.007</td>
</tr>
<tr>
<td>Galactica-6.7B</td>
<td>0.000</td>
<td>0.141</td>
<td>30.760</td>
<td>0.036</td>
<td>0.127</td>
<td>0.051</td>
<td>0.995</td>
</tr>
<tr>
<td>Text+Chem T5-223M</td>
<td>0.000</td>
<td>0.225</td>
<td>49.323</td>
<td>0.039</td>
<td>0.186</td>
<td>0.052</td>
<td>0.313</td>
</tr>
<tr>
<td>Mol-Instructions-7B</td>
<td>0.044</td>
<td>0.224</td>
<td>23.167</td>
<td>0.237</td>
<td>0.364</td>
<td>0.213</td>
<td>1.000</td>
</tr>
<tr>
<td>Llama-7B*(LoRA)</td>
<td>0.000</td>
<td>0.283</td>
<td>53.510</td>
<td>0.136</td>
<td>0.294</td>
<td>0.106</td>
<td>1.000</td>
</tr>
<tr>
<td>InstructMol-G-6.9B</td>
<td>0.070</td>
<td>0.890</td>
<td>24.732</td>
<td>0.469</td>
<td>0.691</td>
<td>0.426</td>
<td>1.000</td>
</tr>
<tr>
<td>InstructMol-GS-6.9B</td>
<td>0.129</td>
<td>0.610</td>
<td>19.664</td>
<td>0.444</td>
<td>0.539</td>
<td>0.400</td>
<td>1.000</td>
</tr>
<tr>
<td>BioT5+</td>
<td>0.257</td>
<td>0.695</td>
<td>12.901</td>
<td>0.539</td>
<td>0.621</td>
<td>0.512</td>
<td>1.000</td>
</tr>
<tr>
<td>Forward Reaction Prediction</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Alpaca-7B</td>
<td>0.000</td>
<td>0.065</td>
<td>41.989</td>
<td>0.004</td>
<td>0.024</td>
<td>0.008</td>
<td>0.138</td>
</tr>
<tr>
<td>Baize-7B</td>
<td>0.000</td>
<td>0.044</td>
<td>41.500</td>
<td>0.004</td>
<td>0.025</td>
<td>0.009</td>
<td>0.097</td>
</tr>
<tr>
<td>ChatGLM-6B</td>
<td>0.000</td>
<td>0.183</td>
<td>40.008</td>
<td>0.050</td>
<td>0.100</td>
<td>0.044</td>
<td>0.108</td>
</tr>
<tr>
<td>Llama-7B</td>
<td>0.000</td>
<td>0.020</td>
<td>42.002</td>
<td>0.001</td>
<td>0.002</td>
<td>0.001</td>
<td>0.039</td>
</tr>
<tr>
<td>Vicuna-7B</td>
<td>0.000</td>
<td>0.057</td>
<td>41.690</td>
<td>0.007</td>
<td>0.016</td>
<td>0.006</td>
<td>0.059</td>
</tr>
<tr>
<td>Galactica-6.7B</td>
<td>0.000</td>
<td>0.468</td>
<td>35.021</td>
<td>0.156</td>
<td>0.257</td>
<td>0.097</td>
<td>0.946</td>
</tr>
<tr>
<td>Text+Chem T5-223M</td>
<td>0.239</td>
<td>0.782</td>
<td>20.413</td>
<td>0.705</td>
<td>0.789</td>
<td>0.652</td>
<td>0.762</td>
</tr>
<tr>
<td>Mol-Instructions-7B</td>
<td>0.045</td>
<td>0.654</td>
<td>27.262</td>
<td>0.313</td>
<td>0.509</td>
<td>0.262</td>
<td>1.000</td>
</tr>
<tr>
<td>Llama-7B*(LoRA)</td>
<td>0.012</td>
<td>0.804</td>
<td>29.947</td>
<td>0.499</td>
<td>0.649</td>
<td>0.407</td>
<td>1.000</td>
</tr>
<tr>
<td>InstructMol-G-6.9B</td>
<td>0.153</td>
<td>0.906</td>
<td>20.155</td>
<td>0.519</td>
<td>0.717</td>
<td>0.457</td>
<td>1.000</td>
</tr>
<tr>
<td>InstructMol-GS-6.9B</td>
<td>0.536</td>
<td>0.967</td>
<td>10.851</td>
<td>0.776</td>
<td>0.878</td>
<td>0.741</td>
<td>1.000</td>
</tr>
<tr>
<td>BioT5+</td>
<td>0.864</td>
<td>0.993</td>
<td>3.403</td>
<td>0.949</td>
<td>0.975</td>
<td>0.935</td>
<td>1.000</td>
</tr>
<tr>
<td>Retrosynthesis</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Alpaca-7B</td>
<td>0.000</td>
<td>0.063</td>
<td>46.915</td>
<td>0.005</td>
<td>0.023</td>
<td>0.007</td>
<td>0.160</td>
</tr>
<tr>
<td>Baize-7B</td>
<td>0.000</td>
<td>0.095</td>
<td>44.714</td>
<td>0.025</td>
<td>0.050</td>
<td>0.023</td>
<td>0.112</td>
</tr>
<tr>
<td>ChatGLM-6B</td>
<td>0.000</td>
<td>0.117</td>
<td>48.365</td>
<td>0.056</td>
<td>0.075</td>
<td>0.043</td>
<td>0.046</td>
</tr>
<tr>
<td>Llama-7B</td>
<td>0.000</td>
<td>0.036</td>
<td>46.844</td>
<td>0.018</td>
<td>0.029</td>
<td>0.017</td>
<td>0.010</td>
</tr>
<tr>
<td>Vicuna-7B</td>
<td>0.000</td>
<td>0.057</td>
<td>46.877</td>
<td>0.025</td>
<td>0.030</td>
<td>0.021</td>
<td>0.017</td>
</tr>
<tr>
<td>Galactica-6.7B</td>
<td>0.000</td>
<td>0.452</td>
<td>34.940</td>
<td>0.167</td>
<td>0.274</td>
<td>0.134</td>
<td>0.986</td>
</tr>
<tr>
<td>Text+Chem T5-223M</td>
<td>0.141</td>
<td>0.765</td>
<td>24.043</td>
<td>0.685</td>
<td>0.765</td>
<td>0.585</td>
<td>0.698</td>
</tr>
<tr>
<td>Mol-Instructions-7B</td>
<td>0.009</td>
<td>0.705</td>
<td>31.227</td>
<td>0.283</td>
<td>0.487</td>
<td>0.230</td>
<td>1.000</td>
</tr>
<tr>
<td>Llama-7B*(LoRA)</td>
<td>0.000</td>
<td>0.283</td>
<td>53.510</td>
<td>0.136</td>
<td>0.294</td>
<td>0.106</td>
<td>1.000</td>
</tr>
<tr>
<td>InstructMol-G-6.9B</td>
<td>0.114</td>
<td>0.586</td>
<td>21.271</td>
<td>0.422</td>
<td>0.523</td>
<td>0.285</td>
<td>1.000</td>
</tr>
<tr>
<td>InstructMol-GS-6.9B</td>
<td>0.407</td>
<td>0.941</td>
<td>13.967</td>
<td>0.753</td>
<td>0.852</td>
<td>0.714</td>
<td>1.000</td>
</tr>
<tr>
<td>BioT5+</td>
<td>0.642</td>
<td>0.969</td>
<td>6.710</td>
<td>0.897</td>
<td>0.930</td>
<td>0.866</td>
<td>1.000</td>
</tr>
</tbody>
</table>
<p>Table 11: Performance comparison on the BindingDB, Human and BioSNAP datasets(Best, Second Best).</p>
<table>
<thead>
<tr>
<th></th>
<th>BioSNAP</th>
<th></th>
<th></th>
<th>Human</th>
<th></th>
<th>BindingDB</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Method</td>
<td>AUROC</td>
<td>AUPRC</td>
<td>Accuracy</td>
<td>AUROC</td>
<td>AUPRC</td>
<td>AUROC</td>
<td>AUPRC</td>
</tr>
<tr>
<td>Single-task Specialist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SVM</td>
<td>$0.862 \pm 0.007$</td>
<td>$0.864 \pm 0.004$</td>
<td>$0.777 \pm 0.011$</td>
<td>$0.940 \pm 0.006$</td>
<td>$0.920 \pm 0.009$</td>
<td>$0.939 \pm 0.001$</td>
<td>$0.928 \pm 0.002$</td>
</tr>
<tr>
<td>RF</td>
<td>$0.860 \pm 0.005$</td>
<td>$0.886 \pm 0.005$</td>
<td>$0.804 \pm 0.005$</td>
<td>$0.952 \pm 0.011$</td>
<td>$0.953 \pm 0.010$</td>
<td>$0.942 \pm 0.011$</td>
<td>$0.921 \pm 0.016$</td>
</tr>
<tr>
<td>DeepConv-DTI</td>
<td>$0.886 \pm 0.006$</td>
<td>$0.890 \pm 0.006$</td>
<td>$0.805 \pm 0.009$</td>
<td>$0.980 \pm 0.002$</td>
<td>$0.981 \pm 0.002$</td>
<td>$0.945 \pm 0.002$</td>
<td>$0.925 \pm 0.005$</td>
</tr>
<tr>
<td>GraphDTA</td>
<td>$0.887 \pm 0.008$</td>
<td>$0.890 \pm 0.007$</td>
<td>$0.800 \pm 0.007$</td>
<td>$0.981 \pm 0.001$</td>
<td>0.982 $\pm 0.002$</td>
<td>$0.951 \pm 0.002$</td>
<td>$0.934 \pm 0.002$</td>
</tr>
<tr>
<td>MolTrans</td>
<td>$0.895 \pm 0.004$</td>
<td>$0.897 \pm 0.005$</td>
<td>$0.825 \pm 0.010$</td>
<td>$0.980 \pm 0.002$</td>
<td>$0.978 \pm 0.003$</td>
<td>$0.952 \pm 0.002$</td>
<td>$0.936 \pm 0.001$</td>
</tr>
<tr>
<td>DrugBAN</td>
<td>0.903 $\pm 0.005$</td>
<td>0.902 $\pm 0.004$</td>
<td>$0.834 \pm 0.008$</td>
<td>$0.982 \pm 0.002$</td>
<td>$0.980 \pm 0.003$</td>
<td>0.960 $\pm 0.001$</td>
<td>$0.948 \pm 0.002$</td>
</tr>
<tr>
<td>BioT5</td>
<td>$0.937 \pm 0.001$</td>
<td>$0.937 \pm 0.004$</td>
<td>$0.874 \pm 0.001$</td>
<td>0.989 $\pm 0.001$</td>
<td>0.985 $\pm 0.002$</td>
<td>$0.963 \pm 0.001$</td>
<td>0.952 $\pm 0.001$</td>
</tr>
<tr>
<td>Multi-task Generalist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BioT5+</td>
<td>0.939 $\pm 0.001$</td>
<td>0.942 $\pm 0.002$</td>
<td>0.875 $\pm 0.001$</td>
<td>$0.987 \pm 0.001$</td>
<td>0.985 $\pm 0.002$</td>
<td>0.964 $\pm 0.001$</td>
<td>0.952 $\pm 0.001$</td>
</tr>
</tbody>
</table>
<p>tasks: MoleculeNet for classification task, ChEBI20 for generation task, and QM9 for regression task. Our findings indicate that: (1) Single-task fine-tuning yields different performance in different tasks. For the tasks such as BACE, BBBP, Clintox, and ChEBI-20, single-task tuned BioT5+ performs closely or slightly better than the multi-task-tuned version. (2) Multi-task fine-tuning can still be advantageous for tasks with inherent correlations, such as the prediction of HOMO, LUMO, and the HOMO-LUMO gap. This evidence points to the potential for cross-task generalization, even if it is not uniformly applicable across all tasks.</p>
<h2>G Fine-tuning Details</h2>
<p>We adopt multi-task instruction tuning on moleculeoriented tasks and protein-oriented tasks. To facilitate a fair comparison with earlier studies, given the wider variety of categories in our fine-tuning dataset compared to the Mol-Instructions (Fang et al., 2023) dataset, we perform multi-task instruction tuning for both molecule-oriented and proteinoriented tasks, using both the Mol-Instruction dataset and an alternative dataset excluding Mol-</p>
<p>Table 12: Best hyper-parameters for multi-task instruction fine-tuning.</p>
<table>
<thead>
<tr>
<th>Hyper-parameter</th>
<th>Molecule</th>
<th></th>
<th>Protein</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Mol-Instructions</td>
<td>Others</td>
<td>Mol-Instructions</td>
<td>Others</td>
</tr>
<tr>
<td>Dropout Rate</td>
<td>0.05</td>
<td>0.1</td>
<td>0.05</td>
<td>0.05</td>
</tr>
<tr>
<td>LR</td>
<td>$2 \mathrm{e}-4$</td>
<td>$5 \mathrm{e}-4$</td>
<td>$1 \mathrm{e}-4$</td>
<td>$1 \mathrm{e}-4$</td>
</tr>
<tr>
<td>Batch Size</td>
<td>768</td>
<td>768</td>
<td>96</td>
<td>96</td>
</tr>
<tr>
<td>Steps</td>
<td>100,000</td>
<td>100,000</td>
<td>100,000</td>
<td>100,000</td>
</tr>
</tbody>
</table>
<p>Table 13: Ablation of additional data on the description-guided molecule design task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">Exact $\uparrow$</th>
<th style="text-align: center;">Levenshtein $\downarrow$</th>
<th style="text-align: center;">MACCS FTS $\uparrow$</th>
<th style="text-align: center;">RDK FTS $\uparrow$</th>
<th style="text-align: center;">Morgan FTS $\uparrow$</th>
<th style="text-align: center;">FCD $\downarrow$</th>
<th style="text-align: center;">Text2Mol $\uparrow$</th>
<th style="text-align: center;">Validity $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BioT5+(single task)</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">12.777</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.842</td>
<td style="text-align: center;">0.784</td>
<td style="text-align: center;">0.350</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">BioT5+(single task) wo additional data</td>
<td style="text-align: center;">0.875</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">12.840</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.777</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.579</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">BioT5+</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">12.776</td>
<td style="text-align: center;">0.907</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.779</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.579</td>
<td style="text-align: center;">1.000</td>
</tr>
</tbody>
</table>
<p>Instructions for each sub-domain. Taking into account the varying sizes of the datasets involved, we report the results across differing epoch spans to accommodate these discrepancies. All results are derived from 3 random runs.</p>
<h2>G. 1 Molecule-oriented Tasks</h2>
<h2>G.1.1 Molecule Property Prediction</h2>
<p>Classification We focus on the following four datasets with scaffold splits setting:
(1) BACE dataset provides both qualitative binary labels and quantitative IC50 measurements for various inhibitors aimed at human beta-secretase 1 (BACE-1).
(2) BBBP (Blood-Brain Barrier Penetration) dataset, designed to assist in predicting and modeling permeability of the blood-brain barrier, consists of compounds classified by binary labels that denote their ability to penetrate the barrier.
(3) HIV dataset includes more than 40,000 compounds evaluated for their ability to inhibit HIV replication. They were initially categorized into Confirmed Inactive (CI), Confirmed Active (CA), and Confirmed Moderately Active (CM). Later, CA and CM categories were merged, simplifying the classification into a binary system of inactive (CI) versus active (CA and CM).
(4) Clintox dataset differentiates between FDAapproved drugs and those that failed clinical trials due to toxicity. It features two distinct classification tasks with known chemical structures: (i) determining whether they exhibited toxicity in clinical trials, and (ii) assessing their FDA approval status.</p>
<p>We compare our BioT5+ with (1) singletask specialist models including GraphCL (You et al., 2020), GraphMVP-C (Liu et al., 2022), MGSSL (Zhang et al., 2021), MolCLR (Wang
et al., 2022), GEM (Fang et al., 2022), UniMol (Zhou et al., 2023), KV-PLM (Zeng et al., 2022), MoMu (Su et al., 2022), MolFM (Luo et al., 2023b), MolXPT (Liu et al., 2023c), and BioT5 (Pei et al., 2023); (2) LLM-based generalist models including Galactica (Taylor et al., 2022), Vicuna (Chiang et al., 2023), Llama2 (Touvron et al., 2023b), and InstructMol (Cao et al., 2023). The baseline results are mainly derived from original papers, MolFM (Luo et al., 2023b), InstructMol (Cao et al., 2023). The evaluation metric is AUROC, and we follow the same AUROC calculation method with MolXPT (Liu et al., 2023c) and BioT5 (Pei et al., 2023) based on the logits of "yes" and "no" predictions.</p>
<p>Regression For the regression task, we focus on three tasks from the QM9 dataset, which encompasses over 134,000 stable organic molecules with no more than nine heavy atoms, characterized by their geometric, energetic, electronic, and thermodynamic properties. Following MolInstructions (Fang et al., 2023), our focus is on three specific subtasks within QM9: (1) "HOMO" for the highest occupied molecular orbital energy. (2) "LUMO" for the lowest unoccupied molecular orbital energy. (3) "GAP" denoting the energy difference between HOMO and LUMO. All of them are measured in Hartree units. We use the processed QM9 dataset in instruction format and corresponding splits from Mol-Instruction (Fang et al., 2023).</p>
<p>We compare our BioT5+ with LLM-based generalist models, including Llama2 (Touvron et al., 2023b), Vicuna (Chiang et al., 2023), MolInstructions (Fang et al., 2023), and InstructMol (Cao et al., 2023). The baseline results come from InstructMol (Cao et al., 2023). The evaluation</p>
<p>Table 14: Performance comparison of single-task and multi-task versions of BioT5+ across various datasets and tasks. Mol2text means molecule description generation, and text2mol means description-guided molecule design.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">DAtaset/Task</th>
<th style="text-align: left;">Task Type</th>
<th style="text-align: center;">Single-TASK</th>
<th style="text-align: center;">Multi-TASK</th>
<th style="text-align: center;">Metric</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BACE</td>
<td style="text-align: left;">classification</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">AUROC $\uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">BBBP</td>
<td style="text-align: left;">classification</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">AUROC $\uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">Clintox</td>
<td style="text-align: left;">classification</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">AUROC $\uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">ChEBI-20-mol2text</td>
<td style="text-align: left;">generation</td>
<td style="text-align: center;">0.687</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">METEOR $\uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">ChEBI-20-text2mol</td>
<td style="text-align: left;">generation</td>
<td style="text-align: center;">$0.877 / 0.535$</td>
<td style="text-align: center;">$0.872 / 0.522$</td>
<td style="text-align: center;">BLEU/Exact Match $\uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">HOMO</td>
<td style="text-align: left;">regression</td>
<td style="text-align: center;">0.0029</td>
<td style="text-align: center;">0.0022</td>
<td style="text-align: center;">MAE $\downarrow$</td>
</tr>
<tr>
<td style="text-align: left;">LUMO</td>
<td style="text-align: left;">regression</td>
<td style="text-align: center;">0.0029</td>
<td style="text-align: center;">0.0024</td>
<td style="text-align: center;">MAE $\downarrow$</td>
</tr>
<tr>
<td style="text-align: left;">HOMO-LUMO gap</td>
<td style="text-align: left;">regression</td>
<td style="text-align: center;">0.0040</td>
<td style="text-align: center;">0.0028</td>
<td style="text-align: center;">MAE $\downarrow$</td>
</tr>
</tbody>
</table>
<p>Table 15: Performance comparison on molecule description generation task on Mol-Instrutions (Fang et al., 2023) dataset (Best, Second Best).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">BLEU-2 $\uparrow$</th>
<th style="text-align: center;">BLEU-4 $\downarrow$</th>
<th style="text-align: center;">ROUGE-1 $\uparrow$</th>
<th style="text-align: center;">ROUGE-2 $\uparrow$</th>
<th style="text-align: center;">ROUGE-L $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Molecule Description Generation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Alpaca-7B</td>
<td style="text-align: center;">0.068</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.041</td>
<td style="text-align: center;">0.136</td>
<td style="text-align: center;">0.107</td>
</tr>
<tr>
<td style="text-align: left;">Baize-7B</td>
<td style="text-align: center;">0.064</td>
<td style="text-align: center;">0.015</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.053</td>
<td style="text-align: center;">0.148</td>
<td style="text-align: center;">0.106</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM-6B</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">0.105</td>
</tr>
<tr>
<td style="text-align: left;">Llama-7B</td>
<td style="text-align: center;">0.059</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.066</td>
<td style="text-align: center;">0.148</td>
<td style="text-align: center;">0.184</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-7B</td>
<td style="text-align: center;">0.052</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.151</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">0.130</td>
<td style="text-align: center;">0.168</td>
</tr>
<tr>
<td style="text-align: left;">Galactica-6.7B</td>
<td style="text-align: center;">0.024</td>
<td style="text-align: center;">0.008</td>
<td style="text-align: center;">0.074</td>
<td style="text-align: center;">0.015</td>
<td style="text-align: center;">0.063</td>
<td style="text-align: center;">0.065</td>
</tr>
<tr>
<td style="text-align: left;">Mol-Instructions-7B</td>
<td style="text-align: center;">$\underline{0.217}$</td>
<td style="text-align: center;">$\underline{0.143}$</td>
<td style="text-align: center;">$\underline{0.337}$</td>
<td style="text-align: center;">$\underline{0.196}$</td>
<td style="text-align: center;">$\underline{0.291}$</td>
<td style="text-align: center;">$\underline{0.254}$</td>
</tr>
<tr>
<td style="text-align: left;">Text+Chem T5-223M</td>
<td style="text-align: center;">0.062</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.126</td>
<td style="text-align: center;">0.075</td>
<td style="text-align: center;">0.119</td>
<td style="text-align: center;">0.139</td>
</tr>
<tr>
<td style="text-align: left;">MolT5-248M</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.034</td>
<td style="text-align: center;">0.033</td>
</tr>
<tr>
<td style="text-align: left;">BioT5+</td>
<td style="text-align: center;">$\mathbf{0 . 5 4 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 9 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 5 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 4 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 1 5}$</td>
</tr>
</tbody>
</table>
<p>metric is MAE.</p>
<h2>G.1.2 Chemical Reaction-related Tasks</h2>
<p>In computational chemistry, tasks centered around chemical reactions are crucial, as they significantly enhance research and development efficiency and support the advancement of eco-friendly chemistry methods. These tasks typically require understanding the interplay among reactants, reagents, and products, usually represented in the format of "reactant $&gt;$ reagent $&gt;$ product". In line with Fang et al., 2023 and Cao et al., 2023, we concentrates on three specific tasks: (1) Reagent Prediction: This critical task entails the identification of key components such as catalysts, solvents, or auxiliary substances necessary for executing a chemical reaction. The input for this task comprises the reactants and the intended product, challenging the model to deduce the required reagents for the reaction process. (2) Forward Reaction Prediction: This task focuses on forecasting the potential outcomes of a chemical reaction. Provided with the reactants and reagents, the objective is to accurately predict the products that would result from the chemical reaction, thereby aiding in the planning and optimization of chemical synthesis. (3) Retrosynthesis: An essential task in synthetic chemistry, retrosynthesis
involves working backwards from a target product to identify plausible reactant combinations for its synthesis. The input is a specific product compound, and the challenge lies in determining the most efficient and feasible reactants needed to produce it. All of these chemical reaction-related data in instruction format and corresponding splits are sourced from Mol-Instruction (Fang et al., 2023) dataset.</p>
<p>We compare our BioT5+ with LLM-based generalist models, including Alpaca (Taori et al., 2023), Baize (Xu et al., 2023a), ChatGLM (et.al., 2023), Llama (Touvron et al., 2023a), Vicuna (Chiang et al., 2023), Galactica (Taylor et al., 2022), Text+Chem T5 (Christofidellis et al., 2023), MolInstructions (Fang et al., 2023), and InstructMol (Cao et al., 2023). The baseline results are sourced from InstructMol (Cao et al., 2023). The evaluation metrics are exact match, BLEU (Papineni et al., 2002), Levenshtein distance (Miller et al., 2009), three molecular fingerprints (FTS) similarity scores including MACCS (Durant et al., 2002), RDK (Schneider et al., 2015), and Morgan (Rogers and Hahn, 2010), and validity score (whether the SMILES can be successfully processed by RDKit (Landrum, 2021)).</p>
<p>Table 16: Performance comparison on description-guided molecule design task on Mol-Instructions <em>Fang et al. (2023)</em> dataset (Best, Second Best).</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Exact $\uparrow$</th>
<th>BLEU $\uparrow$</th>
<th>Levenshtein $\downarrow$</th>
<th>RDK FTS $\uparrow$</th>
<th>MACCS FTS $\uparrow$</th>
<th>Morgan FTS $\uparrow$</th>
<th>Validity $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Description-guided Molecule Design</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Alpaca-7B</td>
<td>0.000</td>
<td>0.004</td>
<td>51.088</td>
<td>0.006</td>
<td>0.029</td>
<td>0.000</td>
<td>0.002</td>
</tr>
<tr>
<td>Baize-7B</td>
<td>0.000</td>
<td>0.006</td>
<td>53.796</td>
<td>0.000</td>
<td>0.000</td>
<td>0.000</td>
<td>0.002</td>
</tr>
<tr>
<td>ChatGLM-6B</td>
<td>0.000</td>
<td>0.004</td>
<td>53.157</td>
<td>0.005</td>
<td>0.000</td>
<td>0.000</td>
<td>0.005</td>
</tr>
<tr>
<td>Llama-7B</td>
<td>0.000</td>
<td>0.003</td>
<td>59.864</td>
<td>0.005</td>
<td>0.000</td>
<td>0.000</td>
<td>0.003</td>
</tr>
<tr>
<td>Vicuna-7B</td>
<td>0.000</td>
<td>0.006</td>
<td>60.356</td>
<td>0.006</td>
<td>0.001</td>
<td>0.000</td>
<td>0.001</td>
</tr>
<tr>
<td>Galactica-6.7B</td>
<td>0.000</td>
<td>0.192</td>
<td>44.152</td>
<td>0.135</td>
<td>0.248</td>
<td>0.088</td>
<td>0.992</td>
</tr>
<tr>
<td>Mol-Instructions-7B</td>
<td>0.002</td>
<td>0.345</td>
<td>41.367</td>
<td>0.231</td>
<td>0.412</td>
<td>0.147</td>
<td>1.000</td>
</tr>
<tr>
<td>Text+Chem T5-223M</td>
<td>0.097</td>
<td>0.508</td>
<td>41.819</td>
<td>0.352</td>
<td>0.474</td>
<td>0.353</td>
<td>0.721</td>
</tr>
<tr>
<td>MolT5-248M</td>
<td>0.112</td>
<td>0.546</td>
<td>38.276</td>
<td>0.400</td>
<td>0.538</td>
<td>0.295</td>
<td>0.773</td>
</tr>
<tr>
<td>BioT5+</td>
<td>0.079</td>
<td>0.795</td>
<td>30.728</td>
<td>0.567</td>
<td>0.687</td>
<td>0.410</td>
<td>1.000</td>
</tr>
</tbody>
</table>
<h3>G.1.3 Molecule Description Generation</h3>
<p>Molecule description generation task requires the model to generate a comprehensive description for a given molecule, encompassing its properties, functions, and potential applications. Unlike the straightforward prediction of molecular properties, this task poses a significantly greater challenge. It demands an in-depth and holistic understanding of the molecule from the model, necessitating not only the recognition of its structural and chemical characteristics but also an integration of this knowledge into a coherent, detailed narrative. There are two well-established benchmark datasets for this task: ChEBI-20 <em>Edwards et al. (2022)</em> and MolInstructions <em>Fang et al. (2023)</em> with recommended splits, and the corresponding results are presented in Table 4 and Table 15.</p>
<p>We compare our BioT5+ with (1) single-task specialist models, including Transformer <em>Vaswani et al. (2017)</em>, T5 <em>Raffel et al. (2020)</em>, MolT5 <em>Edwards et al. (2022)</em>, MoMu <em>Su et al. (2022)</em>, MolFM <em>Luo et al. (2023b)</em>, MolXPT <em>Liu et al. (2023c)</em>, GIT-Mol <em>Liu et al. (2023a)</em>, Text+Chem T5 <em>Christofidellis et al. (2023)</em>, BioT5 <em>Pei et al. (2023)</em>, and MolCA <em>Liu et al. (2023d)</em>; (2) retrieval-based LLM, including GPT-3.5-turbo <em>Li et al. (2024)</em> and GPT-4 <em>OpenAI (2023)</em>. (3) LLM-based generalist models, including GPT-3.5-turbo <em>Li et al. (2024)</em>, BioMedGPT <em>Luo et al. (2023c)</em>, Mol-Instructions <em>Fang et al. (2023)</em>, and InstructMol <em>Cao et al. (2023)</em>. The baseline results are mainly derived from MolXPT <em>Liu et al. (2023c)</em>, BioT5 <em>Pei et al. (2023)</em>, MolCA <em>Liu et al. (2023d)</em>, and InstructMol <em>Cao et al. (2023)</em>. The evaluation metrics are common NLP metrics, including BLEU <em>Papineni et al. (2002)</em>, ROUGE <em>Lin (2004)</em>, and METEOR <em>Banerjee and Lavie (2005)</em>.</p>
<h3>G.1.4 Description-guided Molecule Design</h3>
<p>Description-guided molecule design is the inverse task of molecule description generation. This task centers around designing molecules based on detailed textual descriptions. Here, the model is presented with a comprehensive description encompassing various aspects of a desired molecule, such as its intended functions, properties, and potential applications. The challenge lies in accurately interpreting this textual information and translating it into a specific molecular structure. This task is considerably complex as it requires the model to have a deep understanding of the relationship between molecular characteristics and their corresponding textual descriptors. This task is crucial for advancing drug discovery and material design, where precise molecular configurations are often derived from elaborate functional requirements. We use the same two benchmark datasets: ChEBI-20 <em>Edwards et al. (2022)</em> and Mol-Instructions <em>Fang et al. (2023)</em> as the molecule description generation task, and the corresponding results are shown in Table 5 and Table 16.</p>
<p>The compared baselines are a subset of that listed in Section G.1.3 except for Llama2 <em>Touvron et al. (2023b)</em>, with results mainly sourced from BioT5 <em>Pei et al. (2023)</em> and MolReGPT <em>Li et al. (2024)</em>. The evaluation metrics are BLEU <em>Papineni et al. (2002)</em>, exact match, Levenshtein distance <em>Miller et al. (2009)</em>, three molecular fingerprints (FTS) similarity scores including MACCS <em>Durant et al. (2002)</em>, RDK <em>Schneider et al. (2015)</em>, and Morgan <em>Rogers and Hahn (2010)</em>, FCD score <em>Preuer et al. (2018)</em>, Text2Mol score <em>Edwards et al. (2021)</em>, and validity score.</p>
<p>G.2 Protein-oriented Tasks</p>
<h2>G.2.1 Protein Description Generation</h2>
<p>In computational biology, the task of protein description generation is of paramount importance, as it entails extracting insightful textual information from protein sequences. Aligning with Fang et al., 2023, our focus is on four intricate generation tasks, each taking a protein sequence as its input: (1) Protein Function Generation: This task aims to produce outputs that consist of Gene Ontology (GO) terms, providing a multifaceted description of the protein's functions. These GO terms cover three key domains: cellular component, biological process, and molecular function, offering a holistic view of the protein's role and interactions within a cellular context. (2) Catalytic Activity Generation: Here, the focus is on delineating the specific catalytic activities of the protein, moving beyond merely identifying its Enzyme Commission (EC) number. The output targets a detailed characterization of the chemical reactions facilitated by the protein, capturing its dynamic role in metabolic and biochemical pathways. (3) Domain/Motif Generation: This task involves pinpointing and describing domains or motifs within the protein sequence. Domains and motifs are essential elements, recognized as compact, folded three-dimensional structures that play pivotal roles in the protein's function and stability. The identification of these features is crucial for understanding protein folding, function, and interactions. (4) Functional Description Generation: The goal here is to generate a comprehensive and detailed textual description that encapsulates a protein's function, its subcellular localization, and its involvement in various biological processes. This output seeks to provide an extensive narrative, encompassing the diverse functionalities, roles, and significance of the protein within a biological system. These tasks collectively aim to deepen our understanding of proteins, facilitating advancements in fields such as drug discovery, molecular biology, and bioinformatics. All of these data in instruction format and corresponding splits are sourced from Mol-Instruction (Fang et al., 2023) dataset.</p>
<p>We compare our BioT5+ with LLM-based generalist models, including Alpaca (Taori et al., 2023), Baize (Xu et al., 2023a), ChatGLM (et.al., 2023), Galactica (Taylor et al., 2022), Llama (Touvron et al., 2023a), Vicuna (Chiang et al., 2023), and Mol-Instructions (Fang et al., 2023). The baseline results come from Mol-Instructions (Fang et al., 2023). The evaluation metric is ROUGE-L (Lin, 2004).</p>
<h2>G. 3 Description-guided Protein Design</h2>
<p>Similar to description-guided molecule design, in the description-guided protein design task, the primary objective is to generate amino acid sequences of proteins that meet specific user-defined design requirements. This task necessitates a profound and comprehensive understanding of protein structures and functions from the model. It involves the intricate process of translating complex textual descriptions, which may include functional targets, structural characteristics, and desired biological activities, into precise amino acid sequences. As there is no well-established benchmark for this task, we show some cases for this task in Section 20.</p>
<h2>G. 4 Protein Property Prediction</h2>
<p>The protein property prediction task plays a pivotal role in computational biology, focusing on the prediction of specific protein attributes such as solubility, structural characteristics, or functional roles, based on their amino acid sequences or structural information. Following Pei et al., 2023, we centered on two key protein property prediction tasks within the PEER (Xu et al., 2022) benchmark. (1) Solubility Prediction: This task involves determining the solubility status of a given protein. It seeks to predict if a protein, when introduced into a solvent, will dissolve or remain insoluble. This property is crucial as it influences the protein's functionality and its interaction with other biomolecules. (2) Localization Prediction: The second task focuses on identifying the cellular localization of proteins, distinguishing whether a given protein is "membrane-bound" or "soluble". Membrane-bound proteins are those that are associated with or integrated into the cell membrane, playing key roles in various cellular processes such as signal transduction and transport. In contrast, soluble proteins are those that are not associated with the membrane and are typically involved in various intracellular activities. We use the same data and splitting methods as BioT5 (Pei et al., 2023).</p>
<p>We compare our BioT5+ with (1) singletask specialist models, including DDE (Dipeptide Deviation from Expected Mean) (Saravanan and Gautham, 2015), Moran feature descriptor (Moran correlation) (Feng and Zhang, 2000), LSTM (Hochreiter and Schmidhuber, 1997), Trans-</p>
<p>formers (Vaswani et al., 2017), CNN (O'Shea and Nash, 2015), ResNet (He et al., 2016), ProtBert (Elnaggar et al., 2021), ESM-1b (Rives et al., 2021), and BioT5 (Pei et al., 2023); (2) multi-task generalist models, including CNN (O'Shea and Nash, 2015), Transformers (Vaswani et al., 2017), and ESM-1b (Rives et al., 2021). Note that here the multi-task generalist results are derived from PEER (Xu et al., 2022), where contact prediction, fold classification, and secondary structure prediction are combined with the original task. We report the best results obtained from training each of these tasks in conjunction with the primary task. The baseline results are derived from PEER (Xu et al., 2022). The evaluation metric is Accuracy.</p>
<h3>G.5 Protein-Protein Interaction</h3>
<p>The Protein-Protein Interaction (PPI) task is an essential component in the field of computational biology, focusing on the prediction of interactions between two proteins based on their amino acid sequences. In this task, the input consists of the amino acid sequences of two distinct proteins, and the output is a binary classification: "yes" if the proteins are predicted to interact, and "no" if they are not. This task holds substantial biological significance as protein-protein interactions are fundamental to most biological processes, including signal transduction, cellular metabolism, and immune responses. Following Pei et al., 2023, we use Yeast and Human PPI datasets with corresponding splits from the PEER (Xu et al., 2022) benchmark, which include proteins related to yeast and humans respectively.</p>
<p>The compared baselines are the same with that in Section G.4, with results derived from PEER (Xu et al., 2022). The evaluation metric is Accuracy.</p>
<h3>G.6 Drug-Target Interaction</h3>
<p>The Drug-Target Interaction (DTI) task focuses on the prediction of interactions between a drug molecule and a protein target. This task involves inputting the molecular structure of a drug, encoded as a SELFIES representation, alongside the amino acid sequence of a target protein. The output is a binary decision: "yes" indicates a predicted interaction between the drug and the protein, and "no" suggests no interaction. Understanding and predicting these interactions is crucial for drug discovery and development, offering insights into the mechanism of action of drugs, identifying potential off-target effects, and aiding in the design of novel
therapeutics with improved efficacy and reduced side effects. We use the same data and corresponding splits with BioT5 (Pei et al., 2023).</p>
<p>We compare our BioT5+ with single-task specialist models, including Support Vector Machine (Cortes and Vapnik, 1995) (SVM), Random Forest (Ho, 1995) (RF), DeepConv-DTI (Lee et al., 2019), GraphDTA (Nguyen et al., 2021), MolTrans (Huang et al., 2021), DrugBAN (Bai et al., 2023), and BioT5 (Pei et al., 2023). The baseline results are sourced from DrugBAN (Bai et al., 2023) and BioT5 (Pei et al., 2023). The evaluation metric is AUROC, AUPRC, and Accuracy.</p>
<h2>H Case Studies</h2>
<p>In this section we show some cases for our finetuned BioT5+ model. We only showcase key parts of inputs, intentionally omitting the instruction context due to the varying requirements for instruction context across different baseline methods.</p>
<h2>H. 1 Molecule-oriented Cases</h2>
<p>We show some selected cases for molecule-oriented tasks, including molecule description generation in Table 17, description guided molecule design in Table 18, and chemical reaction-related tasks in Table 19.</p>
<h2>H. 2 Description-guided Protein Design</h2>
<p>We also show some cases of description-guided protein design task in Table 20. We compute the normalized Smith-Waterman (SW) alignment score (Yamanishi et al., 2008) between the output and ground truth protein FASTA sequence. This method involves comparing segments of variable lengths within the protein sequences and is optimized to maximize the similarity metric, effectively evaluating the correspondence between the model output and the ground truth. In the model output of amino acid sequences, occurrences of more than two consecutive amino acids at the terminus were manually truncated.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://huggingface.co/docs/transformers/ model_doc/t5v1.1&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
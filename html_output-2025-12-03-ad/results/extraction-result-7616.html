<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7616 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7616</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7616</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‚Äëtuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-273228757</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.06541v2.pdf" target="_blank">Chip-Tuning: Classify Before Language Models Say</a></p>
                <p><strong>Paper Abstract:</strong> The rapid development in the performance of large language models (LLMs) is accompanied by the escalation of model size, leading to the increasing cost of model training and inference. Previous research has discovered that certain layers in LLMs exhibit redundancy, and removing these layers brings only marginal loss in model performance. In this paper, we adopt the probing technique to explain the layer redundancy in LLMs and demonstrate that language models can be effectively pruned with probing classifiers. We propose chip-tuning, a simple and effective structured pruning framework specialized for classification problems. Chip-tuning attaches tiny probing classifiers named chips to different layers of LLMs, and trains chips with the backbone model frozen. After selecting a chip for classification, all layers subsequent to the attached layer could be removed with marginal performance loss. Experimental results on various LLMs and datasets demonstrate that chip-tuning significantly outperforms previous state-of-the-art baselines in both accuracy and pruning ratio, achieving a pruning ratio of up to 50%. We also find that chip-tuning could be applied on multimodal models, and could be combined with model finetuning, proving its excellent compatibility.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7616",
    "paper_id": "paper-273228757",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0032099999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Chip-Tuning: Classify Before Language Models Say</p>
<p>Fangwei Zhu zhufangwei2022@stu.pku.edu.cn 
National Key Laboratory for Multimedia Information Processing
Peking University</p>
<p>Dian Li 
National Key Laboratory for Multimedia Information Processing
Peking University</p>
<p>Jiajun Huang 
National Key Laboratory for Multimedia Information Processing
Peking University</p>
<p>Gang Liu 
National Key Laboratory for Multimedia Information Processing
Peking University</p>
<p>Hui Wang 
National Key Laboratory for Multimedia Information Processing
Peking University</p>
<p>Zhifang Sui 
National Key Laboratory for Multimedia Information Processing
Peking University</p>
<p>Chip-Tuning: Classify Before Language Models Say
7C4E5873C2E0C48FA1CD346F8FB5B06F
The rapid development in the performance of large language models (LLMs) is accompanied by the escalation of model size, leading to the increasing cost of model training and inference.Previous research has discovered that certain layers in LLMs exhibit redundancy, and removing these layers brings only marginal loss in model performance.In this paper, we adopt the probing technique to explain the layer redundancy in LLMs and demonstrate that language models can be effectively pruned with probing classifiers.We propose chip-tuning, a simple and effective structured pruning framework specialized for classification problems.Chip-tuning attaches tiny probing classifiers named chips to different layers of LLMs, and trains chips with the backbone model frozen.After selecting a chip for classification, all layers subsequent to the attached layer could be removed with marginal performance loss.Experimental results on various LLMs and datasets demonstrate that chiptuning significantly outperforms previous stateof-the-art baselines in both accuracy and pruning ratio, achieving a pruning ratio of up to 50%.We also find that chip-tuning could be applied on multimodal models, and could be combined with model finetuning, proving its excellent compatibility.Our code is available at https://github.com/QQ-MM/ChipTuning.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have experienced rapid development in recent years, achieving surprising success in various domains.Researchers have been scaling up the size of language models to pursue better performance, just as the scaling law (Kaplan et al., 2020) suggests.However, the increasing size of models leads to massive computational costs, posing a challenge to practical deployment and usage.</p>
<p>Model compression techniques have since been proposed as a solution to relieving computational stress, which would assist in the deployment of large models.Different approaches have been explored to compress language models into more compact versions, including quantization (Liu et al., 2021;Dettmers et al., 2022Dettmers et al., , 2024)), knowledge distillation (Gou et al., 2021;Gu et al., 2023;Ko et al., 2024) and pruning (Ma et al., 2023;Yang et al., 2024;Ashkboos et al., 2024;Men et al., 2024).</p>
<p>Relevant research (Men et al., 2024) reveals that a fair portion of parameters in large language models are redundant, and removing these parameters would not bring severe damage to the performance of models.Based on the observation, different methods have been designed to identify and remove redundant parameters from LLMs, like layer merging (Yang et al., 2024), width compression (Ashkboos et al., 2024), layer removal (Men et al., 2024) and component removal (Ma et al., 2023).These methods maintain the majority of performance, proving the feasibility of model pruning.</p>
<p>Research on model interpretability has shown evidence that language models may develop internal representations for various features like color (Patel and Pavlick, 2022), truthfulness (Burns et al., 2022), chessboard states (Nanda et al., 2023), numbers (Zhu et al., 2024) or even abstract concepts like code errors (Templeton, 2024).These features typically start to form on middle layers and will be carried to subsequent layers (Stolfo et al., 2023).More interestingly, many of these features can be read out by probing techniques (Belinkov, 2022), in the way of training simple classifiers.</p>
<p>Inspired by the discovery that removing late layers of LLMs does not heavily impair network functionality (Men et al., 2024), we hypothesize that the critical features for solving certain problems may 1 arXiv:2410.06541v2[cs.CL] 11 Oct 2024 begin to form on intermediate layers of LLMs.By probing these necessary features on intermediate layers, we can safely prune subsequent layers with marginal performance loss.</p>
<p>We observe that previous research mainly aimed to build a general pruned model that can be directly applied to various downstream tasks.Based on the intuition that different tasks require different subsets of features, we hypothesize that pruning on specific tasks instead of pruning for a general model would yield better results, as the model could better focus on the related features.</p>
<p>In this paper, we introduce chip-tuning, a simple and effective structured pruning framework specialized for classification tasks.For a given classification task, we attach probing classifiers named chips to each layer of the language model, and train these classifiers to probe the final classification results from intermediate hidden states.After training, we can then select a chip with satisfactory accuracy, and prune all layers subsequent to the chip to obtain a more compact model for the task.The parameters of the backbone model are frozen throughout the whole process and will not introduce any additional computation cost.</p>
<p>We apply chip-tuning to language models with different sizes and families and observe their performance on various classification tasks.Compared with previous pruning methods, chip-tuning demonstrates better performance on classification tasks, and enables more radical pruning that reduces the parameters of models by up to 50% with marginal loss in performance.Additional experiments show that chip-tuning is also compatible with multimodal large language models (MLLMs) and other finetuning methods.</p>
<p>The main contributions of our paper can be summarized as:</p>
<p>‚Ä¢ We propose chip-tuning, a pruning framework for classification tasks that trains probing classifiers attached to certain layers of language models.By removing layers subsequent to the selected classifier, we can effectively reduce the size of the models.</p>
<p>‚Ä¢ We conduct experiments on different benchmarks, experimental results show that Chiptuning is able to maintain the performance while reducing the size of models by up to 50%, much outperforming previous state-ofthe-art baselines.</p>
<p>‚Ä¢ We evaluate chip-tuning on multimodal models and finetuned models, whose results prove the excellent compatibility of chip-tuning.</p>
<p>Related Work</p>
<p>Network Pruning.With the growth in the size of language models, the pruning technique has been proposed to eliminate unnecessary weights or structures in language models, thus accelerating language models.The pruning methods can be generally categorized into two types: unstructured pruning and structured pruning.</p>
<p>Unstructured pruning methods focus on the level of individual weights, which try to speed up models by increasing the sparsity level of model weights.SparseGPT (Frantar and Alistarh, 2023) reduces the pruning problem to layer-wise sparse regression and incrementally prunes each column in the weight matrix with a sequence of Hessian inverses.Wanda (Sun et al., 2023) enhances the magnitude pruning approach with input activation norms, effectively reducing the complexity of pruning algorithms.RIA (Zhang et al., 2024a) notices that previous methods tend to prune away entire channels of network weights, and mitigates the issue by jointly considering input and output channels.</p>
<p>Structured pruning methods operate at the level of network structures instead, which compress language models by removing redundant model components.LLMPruner (Ma et al., 2023) employs gradient information as a reference to remove noncritical structures.SliceGPT (Ashkboos et al., 2024) removes rows or columns corresponding to small principal components in the weight matrix to achieve smaller weight matrices.LaCo (Yang et al., 2024) proposes the layer collapse algorithm, which merges adjacent layers while ensuring the representation similarity on few-shot calibration examples.ShortGPT (Men et al., 2024) finds that deep layers of language models are not as effective as expected, and proposes the block importance metric to identify and remove redundant layers.BlockPruner (Zhong et al., 2024) decomposes each Transformer layer into two minimal residual blocks and performs fine-grained block pruning to avoid significant performance loss.</p>
<p>Probing Language Models.The impressive capability of language models raises the hypothesis that language models have gone beyond mere memorization of surface correlations.learn the principles behind the training data and develop internal representations for features (Belinkov, 2022).A wide variety of features have been detected in the hidden state of language models like color (Patel and Pavlick, 2022) and truthfulness (Burns et al., 2022).</p>
<p>Probing is a widely adopted technique to associate internal representations with external properties (Belinkov, 2022).By training a simple classifier on model representations that predicts a given property, we can read out various features before language models generate their final outputs.With a simple linear classifier, probing is able to extract complex features like board game states (Nanda et al., 2023), entity properties (Li et al., 2021), and spatial information (Gurnee and Tegmark, 2023).</p>
<p>Recently, Tao et al. (2024) finds that probing classifiers are also able to extract cross-modal information from multimodal large language models.Zhang et al. (2024b) further reveals that probing could achieve better performance on image classification tasks even than directly finetuning the backbone models.</p>
<p>An interesting discovery is that probing classifiers sometimes achieve the best performance at intermediate layers, rather than early or late layers (Zhu et al., 2024).A hypothesis is that late layers focus more on local features related to the next token prediction, while intermediate layers gather information in the input text and thus con-tain more global information (Stolfo et al., 2023).</p>
<p>Methodology</p>
<p>We illustrate the structure of the chip-tuning framework in Figure 1.The framework first inserts simple probing classifiers named chips to different layers of language models, and then solely trains the chips on task-specific training data.</p>
<p>Finally, we can select the chip on a fixed layer or with other strategies (see Section 5.2), and layers subsequent to the attached layer will be removed.</p>
<p>Chips</p>
<p>A language model with the decoder-only structure consists of L transformer layers.At every token position t, each transformer layer l takes previous partial sequence x ‚â§t as input and outputs new hidden states x l t .As discovered by previous research on probing, the hidden states of intermediate layers may contain rich features that can be read out by probing classifiers.Chips are simple probing classifiers that try to predict the classification label y from certain hidden states x l t .We use two types of chips in our experiments: linear chips p L and two-layer perceptron (2xMLP) chips p M , whose function could be notated as:
p L (x l t ) = softmax(W x l t + b) p M (x l t ) = softmax(W 1 ReLU(W 2 x l t + b 2 ) + b 1 where W , W 1 , W 2 , b, b 1 and b 2 are trainable pa- rameters.
For simplicity, we take the hidden state at the last token position (i.e.t = ‚àí1) as the input vector of chips.</p>
<p>Training</p>
<p>As the optimal layer l * for classification chips is initially unknown, we attach a chip p l to every layer l of the language model, and train these chips simultaneously with standard cross-entropy loss:
L l = y log p(x l t ) + (1 ‚àí y) log(1 ‚àí p(x l t )) L = L i=0 L l
Note that the parameters in the backbone language model are frozen in the training process, and only the weights of chips would be updated.</p>
<p>Layer Removal and Inference</p>
<p>We use the straightforward layer removal method to reduce the size of language models.After selecting chip p l at layer l as the classification chip, we simply remove all layers after layer l to obtain a smaller model.</p>
<p>Namely, with chip p l at layer l finally selected, the pruned model would function as follows:</p>
<p>Algorithm 1 Inference with Chips Input: Language model M with N layers L 0 , L 1 , . . ., L N ‚àí1 , selected chip p l at layer l, input embedding x ‚àí1 ; Output: Classification prediction y;</p>
<p>1: for all i = 0, 1, 2, . . ., l do 2:
x i = L i (x i‚àí1 ) 3: end for 4: y = arg max(p l (x i t ))
Chip-tuning also supports inference with multiple chips p 0 , p 1 , . . ., p n‚àí1 for different tasks.Assuming that the deepest layer with chips attached is l m = max(l 0 , l 1 , . . ., l n‚àí1 ), layers subsequent to l m will be pruned.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Benchmarks.We select 4 distinct benchmarks on natural language processing with the form of multi-choice for evaluation: MMLU (Hendrycks et al., 2020), Race (Lai et al., 2017), BoolQ (Clark et al., 2019) and C3 (Sun et al., 2020).Furthermore, we introduce three image classification datasets to test the effectiveness of chip-tuning on multimodal large language models (MLLMs): Flowers102 (Nilsback and Zisserman, 2008), StanfordCars (Krause et al., 2013), andCal-tech101 (Fei-Fei et al., 2004), each containing 102, 196, and 101 classes respectively.</p>
<p>Models.Following previous work (Men et al., 2024), we choose 2 model series to evaluate the effectiveness of chip-tuning: Llama2 (Touvron et al., 2023), Baichuan2 (Yang et al., 2023), which share similar decoder-only transformer structure.We use the 7B and 13B versions of Llama2 and Baichuan2 for experiments.For multimodal large language models, we use the 7B and 13B versions of LLaVA-1.5 (Liu et al., 2023) as the backbone model.</p>
<p>Due to memory constraints, we run 13B models under the precision of 16-bit (fp16) instead of 32bit (fp32).</p>
<p>Baselines.We compare our method with several structured pruning methods: LLMPruner (Ma et al., 2023) removes non-critical coupled structures on the basis of gradient information.SliceGPT (Ashkboos et al., 2024) replaces weight matrices with smaller matrices by retaining principal components.LaCo (Yang et al., 2024) merges the layer in language models from deep to shallow, and sets a threshold to prevent excessive merging.ShortGPT (Men et al., 2024) removes redundant layers according to their proposed Block Influence metric, a variant of cosine similarity.</p>
<p>Settings.For each benchmark, we use at most 20, 000 training data in the corresponding training split of the benchmark to train our chips.The chips are trained with a batch size of 1 for 1 epoch.We use a learning rate of 1 √ó 10 ‚àí5 for our experiments, and set the hidden dimension of MLP chips to 256.All experiments are conducted on a single NVIDIA A100 40GB GPU.</p>
<p>For 7B models, we select chips at layer 20 as classification chips; for 13B models, we select chips at layer 25 as classification chips.These settings are equal to the prune ratio of 34.4% and 35.0%, respectively.The results of LLMPrun., SliceGPT, LaCo, and ShortGPT are reported from ShortGPT (Men et al., 2024).</p>
<p>Main Experiment Results</p>
<p>To evaluate the effectiveness of chip-tuning, we conduct experiments on multi-choice style benchmarks commonly used for large language model evaluation.The experimental results are demonstrated in Table 1. 1</p>
<p>Chip-tuning excels previous baselines.It can be clearly observed that chip-tuning outperforms previously structured pruning baselines on almost every benchmark by a large margin, proving the capacity of our proposed model.Meanwhile, while previously structured pruning baselines prune less than 30% of the model parameters, chip-tuning is 1 We report the result of finetuning pruned baseline models with the same data used by chip-tuning in Appendix F. able to prune models by a higher ratio: 34.4% for 7B models and 35.0% for 13B models.</p>
<p>Linear chips are sufficient for classification.We also notice that the performance of linear chips is close to the performance of MLP chips, indicating that the features related to the input question may be mostly encoded linearly, and linear probing classifiers are enough for reading out these features.Details of the difference will be demonstrated in Section 5.1.</p>
<p>Optimal chips exhibit more potential.Finally, we gather the highest accuracy of all chips on each benchmark, notated as CT (max)  els (see Appendix B for details).By choosing the optimal chip, chip-tuning could achieve even higher pruning ratios and performance.</p>
<p>Pruning Multimodal Models</p>
<p>We further evaluate whether chip-tuning could be applied to multimodal large language models (MLLMs) by pruning LLaVA-1.5 on image classification benchmarks.Following the settings in (Zhang et al., 2024b), we train the chips for 500 epochs with a learning rate of 1 √ó 10 ‚àí3 , and set the batch size to 512.</p>
<p>Table 2 demonstrates the results of pruning (see Appendix C for details).Surprisingly, the original LLaVA models perform poorly on fine-grained image classification tasks, achieving an accuracy of near 0% on Flowers102 and StanfordCars.Providing the label set in the prompt could improve the accuracy, but the performance is still not satisfactory.</p>
<p>In contrast, by tuning chips on the hidden states, we can achieve a decent accuracy while pruning the language model part of LLaVA.This phenomenon indicates that the information essential for image classification is already contained in the hidden states of multimodal models, but the models have difficulty in correctly decoding them.By adopting chip-tuning, we can extract related information before the final layer, and decode the information correctly.</p>
<p>We also notice that MLP chips perform extremely badly on StanfordCars, which may be caused by the large label set size of the dataset.</p>
<p>Combination with Finetuning</p>
<p>A critical difference between chip-tuning and the previous structured pruning method is that chiptuning requires additional training data.With these training data, we can also finetune the backbone language model to achieve better performance.To better study the effectiveness of chiptuning, we finetune models with the same data using LoRA (Hu et al., 2021) and observe the performance gap between chip-tuning and finetuning.We set rank r = 16 and LoRA alpha Œ± = 322 .</p>
<p>Table 3 shows the comparison results.Finetuning the backbone model with LoRA could improve the performance on various benchmarks, and outperforms chip-tuning on the raw model as expected.Nevertheless, we can perform chip-tuning on finetuned models, which will only lead to marginal performance loss and will even improve the performance on certain datasets.These results clearly indicate that chip-tuning is compatible with traditional finetuning methods.</p>
<p>Considering that the target of probing is to read out relevant features from the internal representations of models, finetuning the model would help the backbone model develop better representations for the given classification task.Thus, chips could benefit from the optimized input features and achieve better performance.</p>
<p>Analysis</p>
<p>Number of Pruned Layers</p>
<p>Choosing different chips would change the number of pruned layers, and thus affect the classifica-   tion performance.We conduct experiments on the MMLU dataset with Llama-2 models, and Figure 2 demonstrates the correlation between number of pruned layers and classification accuracy.</p>
<p>It can be clearly observed that the classification accuracy exhibits a drastic change on both datasets, increasing from random guess to a decent level, and then fluctuating within a relatively small range.The change happens at around layer 18 for Llama-2-7B and happens at layer 20 for Llama-2-13B, which are at the position of about 50% in the entire model.It is also worth noticing that the best performance is not necessarily achieved on the last layer, especially for the Llama-2-7B model, which may be a hint that features in middle-layer representations serve better for classification.Stolfo et al. (2023) proposes the theory that early layers in language models focus on gathering and transmitting information in the input text, while mid-late layers are involved in processing the information and output the final answer.The theory matches our findings: information relevant to the final answer is transmitted to the last token on intermediate layers, and the information is sufficient for solving the question.</p>
<p>We also find that the performance gap between linear chips and 2-layer MLP chips is not extremely significant.On most layers, the two chips behave identically, especially for the 13B model.The observable difference is that the performance of MLP chips is slightly more stable, changing in a smaller range on late layers.</p>
<p>Multimodal models exhibit a different pattern.As illustrated in Figure 3, chips on multimodal models could achieve high accuracy from early layers, and chips on late layers generally perform better than those on intermediate layers.The critical information for image classification is already contained in the image tokens from the first layer, which could lead to the difference.</p>
<p>Chip Selection Strategy</p>
<p>Aside from choosing chips on a fixed layer, there exist other strategies to achieve better performance.We adopt distinct strategies and evaluate them on Llama-2-7B:</p>
<p>Fixed selects a fixed layer l for all tasks (l = 20 for 7B models and l = 25 for 13B models), which is the strategy we use in main experiments.</p>
<p>Validate constructs a small validation set consisting of 200 examples, and chooses the chip which performs best on the validation set.</p>
<p>Optimal evaluates the performance of all chips, and selects the chip with the highest accuracy.This strategy reflects the upper bound of chip-tuning.</p>
<p>The experimental results are shown in Table 4. Choosing chips according to the validation set generally achieves better performance than pruning the model on a fixed layer, but the pruning ratio may vary across different datasets.While the Optimal strategy outperforms other strategies, the performance gap is not large, The Validate strategy could achieve comparable results with Optimal accuracy, proving the robustness of chip-tuning.</p>
<p>Impact of Training Dataset Scale</p>
<p>Training data is a crucial component in model training.Considering the scenario where training data is scarce, we test the performance of chip-tuning under different scales of the training dataset.</p>
<p>Figure 4 shows the classification accuracy under different training dataset scales.The accuracy rapidly increases before 6,000 training examples and reaches a plateau afterward.Although the accuracy may drop at a certain time step, the figure still displays a pattern of slow increase after 6,000 training examples.We draw the conclusion that a sufficient number of training data is essential for chips to converge, but further data could still bring subtle improvements.</p>
<p>Conclusion</p>
<p>In this paper, we propose chip-tuning, a structured pruning framework specialized for classification tasks.Chip-tuning adopts probing classifiers to extract relevant features from intermediate layers of language models, and safely removes subsequent layers without affecting the selected clas-sifier.Experimental results on a variety of models and datasets demonstrate that chip-tuning surpasses previous baseline models on both performance and pruning ratio.Chip-tuning performs well by selecting chips on a fixed layer, and could further achieve a pruning ratio of up to 50% by selecting the optimal chip.</p>
<p>Meanwhile, we find that chip-tuning is also compatible with multimodal models and finetuned models.Considering the simplicity of layer removal, chip-tuning shows its potential in deploying LLMs under practical scenarios.We hope our work could inspire further research on efficient model pruning.</p>
<p>Limitations</p>
<p>Based on the technique of probing, chip-tuning requires the backbone models to contain relevant features in their internal representations.On tasks that the backbone models perform poorly, chiptuning would not yield satisfactory results either.</p>
<p>Meanwhile, chip-tuning is designed mainly for classification tasks, which is the reason why we don't evaluate chip-tuning on datasets like Hel-laSwag that use perplexity-based evaluation methods.Directly applying chip-tuning to generation tasks may lead to unexpected results, and generation-oriented chips remain to be explored in the future.</p>
<p>A Datasets</p>
<p>The properties of datasets we used are shown in Table 7.We use these datasets according to their license and intended use.</p>
<p>B Details for Main Experiments</p>
<p>Figure 5 shows how the performance changes with different number of layers pruned.We can see that the optimal chip varies as the dataset changes.However, pruning around layer 18 of the 7B model (about 40%) and layer 20 of the 13B model (about 50%) is generally acceptable.</p>
<p>We also notice that probing late layers of Llama-2-7B leads to worse results, which leaves the question of whether the 7B model "forgets" certain information on late layers.The question remains to be explored in the future.</p>
<p>We record the layer on which chips show the best performance or highest pruning ratio in Table 6.Notice that we define layer with the highest pruning ratio as the first layer after the drastic change in accuracy, which could be subjective.</p>
<p>We implement our code with the huggingface Transformers and Peft Python library.Conducting chip-tuning on a 7B model or a 16-bit 13B model with 20,000 examples would take about 2 hours on a single NVIDIA A100 40GB GPU.</p>
<p>C Details for Multimodal Experiments</p>
<p>Figure 6 shows how the performance changes by pruning LLaVA1.5-7B.Different from text datasets, the optimal chip for image classification typically appears on late layers, while chips on early layers also exhibit decent accuracy.Surprisingly, 2layer MLP chips fail to predict the class of images on StanfordCars.This may be a result of the larger class label set size (196) compared with Flowers102 (102) and Caltech101 (101).</p>
<p>D Details for LoRA Experiments</p>
<p>E Experiments on Llama3</p>
<p>We evaluate chip-tuning on Llama3-8B-Instruct (AI@Meta, 2024), one of the up-to-date LLMs.We prune the model to layer 22 in experiments.</p>
<p>The experimental results in Table 8 are similar to those in Table 3: applying chip-tuning on Llama3 has minimal impact on classification accuracy, proving that chip-tuning is compatible with Llama3.The optimal performance of chips even outperforms the finetuned LoRA models.</p>
<p>F Finetuning Pruned Baseline Models</p>
<p>For fair comparisions, we finetune the pruned baseline models on the training set of each benchmark to see how they perform with the same data provided.We use Llama2-7B as the backbone model, and finetune LLMPruner (Ma et al., 2023) and SliceGPT (Ashkboos et al., 2024) under their default LoRA settings.We do not train LaCo and ShortGPT as we cannot find their official code.</p>
<p>Both baseline models are trained with at most 20,000 training data same to these chip-tuning used on each benchmark.The accuracy of finetuned baseline models are obtained by selecting the choice token (for example, "A", "B", "C", "D" for 4-choice problems) with the highest generation probability, as free-form generation would yield unexpected results.</p>
<p>Table 9 shows the result of finetuning LLM-Pruner and SliceGPT with the same data used by chip-tuning.While the finetuned versions achieve higher accuracy than the original version, we can clearly see that chip-tuning greatly outperforms both baselines, further proving the effectiveness of chip-tuning.</p>
<p>Parameter</p>
<p>Value learning rate 1 √ó 10 ‚àí5 weight decay 0.01 r 16 Œ± 32 batch size 1 epoch 1</p>
<p>Table 3 :
3
Comparison between chip-tuning and finetuning with LoRA on the same training dataset.We attach a linear chip to the 20th layer of the 7B model and the 25th layer of the 13B model for classification.CT (Raw) and CT (LoRA) refer to adding linear chips to the raw model and the finetuned model, respectively.(a)Llama-2-7B on MMLU.(b) Llama-2-13B on MMLU.</p>
<p>Figure 2 :
2
Figure 2: The impact of pruning Llama2 models on MMLU by selecting chips on different layers.</p>
<p>Figure 3 :
3
Figure 3: The impact of pruning LLaVA1.5-7B on Flow-ers102 by selecting chips on different layers.Different from the trends on NLP benchmarks, the trend does not exhibit a drastic change on certain layers.</p>
<p>Figure 4 :
4
Figure 4: Analysis on the training dataset scale.We evaluate the performance of chip-tuning Llama-2-7B on MMLU every 2, 000 training steps.The overall accuracy rapidly increases until 6, 000 training steps, and continues to increase slightly afterward.</p>
<p>Instead, they may
Output00...1...11Yesùê∂‚Ñéùëñùëù 0ùê∂‚Ñéùëñùëù 1...ùê∂‚Ñéùëñùëù ùëò...ùê∂‚Ñéùëñùëù ùëò‚àí1ùê∂‚Ñéùëñùëù ùëòLMLayer 0Layer 1...Layer k...Layer n-1Layer nInputQuestion: Is windows movie maker part of windows essentials? Answer:Figure 1: The overall structure of chip-tuning. After selecting a certain chip attached to the k-th layer, subsequentlayers in the language model can be safely pruned with marginal influence on model performance. In training, onlythe parameters of chips are trainable and the backbone model is frozen.</p>
<p>Table 1 :
1
Comparison of pruning methods on natural language benchmarks.CT refers to chip-tuning (our method).
ModelMethodRatio (%) BoolQ Race-H Race-MC3MMLU Avg. ScoreDense0.00%71.6235.7134.1943.5645.3946.09LLMPrun.27.0%55.2022.5622.3525.6423.3329.82SliceGPT26.4%38.3221.0721.6639.7828.9229.95Llama2-7BLaCo ShortGPT27.1% 27.1%64.07 74.7122.61 32.2523.61 35.1739.67 39.6226.45 43.9635.28 45.14CT (Lin.)34.4%79.0547.9153.6948.9344.8954.89CT (MLP)34.4%76.0149.4353.9053.8045.0755.64CT (Max)~40%79.4850.3454.7454.3545.4956.88Dense0.00%82.3957.9560.3847.5155.0060.65LLMPrun.24.4%56.4222.4722.0832.3325.2131.70SliceGPT23.6%37.8623.4124.0341.9237.1432.87Llama2-13BLaCo ShortGPT24.6% 24.6%63.98 62.4854.49 58.3556.55 60.1744.93 46.9045.93 54.6953.18 56.52CT (Lin.)35.0%78.2362.0467.0668.2152.7965.67CT (MLP)35.0%75.8162.5267.1368.0052.9565.28CT (Max)~50%79.7663.2968.0469.3953.4166.78Dense0.00%74.1026.9624.0964.5553.8748.71LLMPrun.24.2%61.1921.9622.2841.6424.9334.40SliceGPT22.2%39.3023.5322.4926.5825.1827.42Baichuan2-7BLaCo ShortGPT24.2% 24.2%56.15 67.8328.99 53.2627.72 46.7650.85 56.3331.53 45.7739.05 53.99CT (Lin.)34.4%72.7862.6966.8575.4751.0965.78CT (MLP)34.4%73.1263.5267.1376.3650.9566.22CT (Max)~40%74.6864.0468.3876.3651.2266.94Dense0.00%77.8967.2768.9465.6459.5067.85LLMPrun.24.3%56.5421.1721.6139.8923.1932.48SliceGPT22.8%37.8321.5621.5224.9922.9525.77Baichuan2-13BLaCo ShortGPT24.7% 24.7%62.35 62.5456.92 55.7757.80 56.4161.10 60.1651.35 52.1157.90 57.40CT (Lin.)35.0%77.7773.0477.4480.8456.8873.19CT (MLP)35.0%76.8873.8777.4481.8156.6673.33CT (Max)~50%78.8475.0479.1181.8956.9674.37</p>
<p>Table 2 :
2
Zhang et al. (2024b)ning ratio and corresponding layer of optimal chips varies across different benchmarks and mod-Comparison of pruning methods on image classification benchmarks.CT refers to chip-tuning (our method).The results of dense models are reported fromZhang et al. (2024b).
ModelMethod Ratio(%) Flowers102 StanfordCars Caltech101 Avg. ScoreRaw0.00%5.90.047.117.67w/ Label 0.00%10.20.062.124.10Llava1.5-7BCT (Lin.) 34.4%91.2860.9892.2481.50CT (MLP) 34.4%88.700.8591.5260.36CT (Max) ~20%94.0070.9592.2485.73Raw0.00%5.30.049.918.4w/ Label 0.00%7.20.170.926.07Llava1.5-13BCT (Lin.) 50.0%91.4648.6391.7077.26CT (MLP) 50.0%85.930.8590.4259.07CT (Max) ~50%93.0671.5292.3985.66</p>
<p>Table 4 :
4
Comparison between different chip selection strategies on Llama-2-7B.
ModelStrategy BoolQ Race-H Race-MC3MMLU Avg. ScoreDense71.6235.7134.1943.5645.3946.09Llama2-7BFixed Validate 79.48 79.0547.91 49.4353.69 54.5348.93 53.9344.89 44.5354.89 56.38Optimal 79.4850.3454.7454.3545.4956.88</p>
<p>Table 5 shows the experimental settings for LoRA experiments.</p>
<p>Table 5 :
5
Parameters for LoRA training.
ModelBoolQ Race-H Race-MC3MMLULlama2-7B18/1719/1919/1719/1817/15Llama2-13B38/1839/1819/1620/1721/16Baichuan2-7B21/1830/1930/1920/1924/19Baichuan2-13B 38/1836/2235/2227/2222/21</p>
<p>Table 6 :
6
The corresponding layer of chips with the best performance or highest pruning ratio on each dataset.The format of each cell in the table is (layer with best performance / layer with highest pruning ratio).
ModelMethodBoolQ Race-H Race-MC3MMLU Avg. ScoreRaw71.6235.7134.1943.5645.3946.09Llama2-7BLLMPrun. SliceGPT55.20 38.3222.56 21.0722.35 21.6625.64 39.7823.33 28.9229.82 29.95CT (Raw)79.0547.9153.6948.9344.8954.89LoRA87.3781.5986.5683.8354.8078.83LLMPrun.+LoRA 60.4923.6725.5617.8426.6030.83SliceGPT+LoRA 65.7271.7676.8160.2743.0663.52CT (LoRA)89.2081.4586.4284.2854.5779.18</p>
<p>Table 9 :
9
Comparison between chip-tuning and finetuning pruned baseline models.We only report the result of finetuning LLMPruner and SliceGPT, as LaCo and ShortGPT do not provide their official code.</p>
<p>See Appendix D for detailed settings.</p>
<p>A I , Meta , Llama 3 model card. 2024</p>
<p>Slicegpt: Compress large language models by deleting rows and columns. Maximilian L Saleh Ashkboos, Marcelo Croci, Torsten Gennari Do Nascimento, James Hoefler, Hensman, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Probing classifiers: Promises, shortcomings, and advances. Yonatan Belinkov, Computational Linguistics. 4812022</p>
<p>Discovering latent knowledge in language models without supervision. Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Boolq: Exploring the surprising difficulty of natural yes/no questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20191</p>
<p>8-bit matrix multiplication for transformers at scale. Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, Advances in Neural Information Processing Systems. 202235int8 (</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Advances in Neural Information Processing Systems. 202436</p>
<p>Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Li Fei-Fei, Rob Fergus, Pietro Perona, 2004 conference on computer vision and pattern recognition workshop. IEEE2004</p>
<p>Sparsegpt: Massive language models can be accurately pruned in one-shot. Elias Frantar, Dan Alistarh, International Conference on Machine Learning. PMLR2023</p>
<p>Knowledge distillation: A survey. Jianping Gou, Baosheng Yu, Stephen J Maybank, Dacheng Tao, International Journal of Computer Vision. 12962021</p>
<p>Yuxian Gu, Li Dong, Furu Wei, Minlie Huang, arXiv:2306.08543Knowledge distillation of large language models. 2023arXiv preprint</p>
<p>Language models represent space and time. Wes Gurnee, Max Tegmark, arXiv:2310.022072023arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Distillm: Towards streamlined distillation for large language models. Jongwoo Ko, Sungnyun Kim, Tianyi Chen, Se-Young Yun, Forty-first International Conference on Machine Learning. 2024</p>
<p>3d object representations for fine-grained categorization. Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei, Proceedings of the IEEE international conference on computer vision workshops. the IEEE international conference on computer vision workshops2013</p>
<p>Race: Large-scale reading comprehension dataset from examinations. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language Processing2017</p>
<p>Implicit representations of meaning in neural language models. Belinda Z Li, Maxwell Nye, Jacob Andreas, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, 2023</p>
<p>Post-training quantization for vision transformer. Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, Wen Gao, Advances in Neural Information Processing Systems. 202134</p>
<p>Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems. Xinyin Ma, Gongfan Fang, Xinchao Wang, 202336</p>
<p>Shortgpt: Layers in large language models are more redundant than you expect. Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, Weipeng Chen, arXiv:2403.038532024arXiv preprint</p>
<p>Emergent linear representations in world models of self-supervised sequence models. Neel Nanda, Andrew Lee, Martin Wattenberg, Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP. the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP2023</p>
<p>Automated flower classification over a large number of classes. Maria-Elena Nilsback, Andrew Zisserman, Sixth Indian conference on computer vision, graphics &amp; image processing. IEEE2008. 2008</p>
<p>Mapping language models to grounded conceptual spaces. Roma Patel, Ellie Pavlick, International conference on learning representations. 2022</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Investigating prior knowledge for challenging chinese machine reading comprehension. Kai Sun, Dian Yu, Dong Yu, Claire Cardie, Transactions of the Association for Computational Linguistics. 82020</p>
<p>. Mingjie Sun, Zhuang Liu, Anna Bair, J Zico Kolter, </p>
<p>A simple and effective pruning approach for large language models. The Twelfth International Conference on Learning Representations. </p>
<p>Probing multimodal large language models for global and local semantic representations. Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong Feng, Dongyan Zhao, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)2024</p>
<p>Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Adly Templeton, 2024Anthropic</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chenxu Chao Yin, Lv, Dian Da Pan, Dong Wang, Yan, arXiv:2309.10305Open large-scale language models. 20232arXiv preprint</p>
<p>Yifei Yang, Zouying Cao, Hai Zhao, arXiv:2402.11187Laco: Large language model pruning via layer collapse. 2024arXiv preprint</p>
<p>Plugand-play: An efficient post-training pruning method for large language models. Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, Carlo Vittorio, Cannistraci , The Twelfth International Conference on Learning Representations. 2024a</p>
<p>Why are visually-grounded language models bad at image classification?. Yuhui Zhang, Alyssa Unell, Xiaohan Wang, Dhruba Ghosh, Yuchang Su, Ludwig Schmidt, Serena Yeung-Levy, arXiv:2405.184152024barXiv preprint</p>
<p>Blockpruner: Finegrained pruning for large language models. Longguang Zhong, Fanqi Wan, Ruijun Chen, Xiaojun Quan, Liangzhi Li, arXiv:2406.105942024arXiv preprint</p>
<p>Language models understand numbers. Fangwei Zhu, Damai Dai, Zhifang Sui, arXiv:2401.037352024arXiv preprintat least partially</p>            </div>
        </div>

    </div>
</body>
</html>
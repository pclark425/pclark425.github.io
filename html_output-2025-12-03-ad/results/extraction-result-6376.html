<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6376 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6376</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6376</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-273234021</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.07461v1.pdf" target="_blank">Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning</a></p>
                <p><strong>Paper Abstract:</strong> Network pruning has emerged as a potential solution to make LLMs cheaper to deploy. However, existing LLM pruning approachesuniversally rely on the C4 dataset as the calibration data for calculating pruning scores, leaving its optimality unexplored. In this study, we evaluate the choice of calibration data on LLM pruning, across a wide range of datasets that are most commonly used in LLM training and evaluation, including four pertaining datasets as well as three categories of downstream tasks encompassing nine datasets. Each downstream dataset is prompted with In-Context Learning (ICL) and Chain-of-Thought (CoT), respectively. Besides the already intriguingobservation that the choice of calibration data significantly impacts the performance of pruned LLMs, our results also uncover several subtle and often unexpected findings, summarized as follows: (1) C4 is not the optimal choice for LLM pruning, even among commonly used pre-training datasets; (2) arithmetic datasets—when used as calibration data—performs on par or even better than pre-training datasets; (3) pruning with downstream datasets does not necessarily help the corresponding downstream task, compared to pre-training data; (4) ICL is widely beneficial to all data categories, whereas CoT is only useful on certain tasks. Our findings shed light on the importance of carefully selecting calibration data for LLM pruning and pave the way for more efficient deployment of these powerfulmodels in real-world applications. We release our code at: https://github.com/abx393/llm-pruning-calibration-data.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6376.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6376.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Arithmetic-pruning-Llama2-Chat-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Arithmetic evaluation and pruning experiments on GSM8K, SVAMP, and MAWPS using Llama 2-Chat 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper evaluates how using arithmetic datasets as pruning calibration data (GSM8K, SVAMP, MAWPS) affects arithmetic performance of pruned Llama 2-Chat 7B models under Wanda and SparseGPT pruning, comparing zero-shot, ICL, and ICL w/ CoT calibration formats at 50% (and in some experiments 70%) sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2-Chat 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (Llama family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in detail in this paper; model is a Llama-family 7B chat model (pretrained on broad web-scale corpora per Llama releases).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, SVAMP, MAWPS</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>grade-school multi-step math word problems / small arithmetic problems (addition/subtraction/multi-step word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems presented as few-shot Q-A concatenations (ICL) or with Chain-of-Thought rationales (ICL w/ CoT); also zero-shot (question only).</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school math; GSM8K: 2–8 steps; SVAMP: ≤2 arithmetic operations; MAWPS: varying complexity</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot, few-shot In-Context Learning (ICL) with concatenated Q-A pairs, and ICL with Chain-of-Thought (ICL w/ CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (task-specific exact-answer accuracy as reported in tables)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported Llama 2-Chat 7B dense baselines and pruned results (Wanda, 50% unstructured sparsity) for evaluation tasks: GSM8K dense 0.0576; pruned with GSM8K calibration: zero-shot 0.0205, ICL 0.0425, ICL w/ CoT 0.0432. SVAMP dense 0.3867; pruned with GSM8K calibration: zero-shot 0.0233, ICL 0.2867, ICL w/ CoT 0.3067. MAWPS dense 0.4462; pruned with GSM8K calibration: zero-shot 0.0058, ICL 0.3442, ICL w/ CoT 0.3519. (Values taken directly from paper tables; other calibration/dataset combinations reported similarly.)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic probes (e.g., attention or neuron-level analyses) of numeric processing were performed. The paper analyzes input-format and dataset effects: (1) CoT calibration appears to help preserve arithmetic ability (authors hypothesize it 'guides pruning to better preserve weights for arithmetic reasoning'); (2) number of CoT steps (tested x = {3,4,5}) showed no clear monotonic relationship with final accuracy; (3) increasing the number of in-context Q-A examples in the calibration sequences generally correlated with improved sparse-model performance; (4) longer calibration sequence lengths (up to 2048 tokens) improved pruning outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Empirical failure modes observed: (1) calibration with CoT can introduce biases that harm performance outside the calibration domain (CoT helps arithmetic tasks but can reduce generality); (2) using nonsense calibration data (ellipses or random alphanumeric strings) performs substantially worse than sensible human-readable calibration data; (3) pruning with a downstream dataset does not necessarily produce the best sparse model for that same downstream task (domain mismatch effects); (4) performance is sensitive to calibration input length and number of in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Only 7B-scale models (Llama 2-Chat 7B and LLaMA 7B) were evaluated; no cross‑scale emergent trends are reported. The paper also reports that at higher sparsity (70%) Pile and RedPajama calibration outperformed C4, but broader size-scaling trends are not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6376.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6376.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting-effects-ICL-vs-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of In-Context Learning (ICL) and Chain-of-Thought (CoT) calibration data on pruning and arithmetic performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper systematically compares zero-shot, ICL, and ICL w/ CoT calibration-data formats for pruning and finds ICL broadly improves pruned-model performance across categories, while CoT is particularly beneficial for arithmetic reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2-Chat 7B (primary experiments; LLaMA 7B also evaluated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in detail in this paper; experiments use off-the-shelf Llama-family 7B models.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K (arithmetic) and e-SNLI (NLI) used as canonical examples for prompting-format comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>arithmetic reasoning (GSM8K) and natural language inference (e-SNLI); comparisons generalize across evaluated downstream tasks</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>zero-shot (question only), ICL (concatenated Q-A examples), ICL w/ CoT (Q-A with chain-of-thought rationales)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K: grade-school multi-step; e-SNLI: NLI examples</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot ICL, ICL with Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (average accuracy across downstream tasks and per-task accuracy reported)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports that ICL calibration improves average accuracy over zero-shot by +0.1754 for GSM8K-based calibrations and by +0.0826 for e-SNLI calibrations (average improvements reported in text). For arithmetic tasks, GSM8K (ICL w/ CoT) outperformed GSM8K (ICL) on arithmetic-specific evaluation, while GSM8K (ICL) outperformed GSM8K (ICL w/ CoT) on non-arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors interpret CoT's benefit on arithmetic as likely due to step-by-step rationales helping pruning preserve arithmetic-relevant parameters; they do not provide internal mechanistic evidence (no probes, attention or activation analyses). They also test CoT step-count (3–5 steps) and find no consistent monotonic effect.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>CoT calibration introduces domain-specific inductive biases that can hurt out-of-domain performance; more CoT reasoning steps do not reliably improve the pruned model; too few or nonsensical calibration examples harm pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Findings reported for 7B-scale models only; no claims about larger-scale behavior. The paper notes ICL is 'widely beneficial' across data categories, but CoT is beneficial primarily for arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>GSM8K <em>(Rating: 2)</em></li>
                <li>SVAMP <em>(Rating: 2)</em></li>
                <li>MAWPS: A math word problem repository <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Are NLP models really able to solve simple math word problems? <em>(Rating: 2)</em></li>
                <li>SparseGPT: Massive language models can be accurately pruned in one-shot <em>(Rating: 2)</em></li>
                <li>Wanda: A simple and effective pruning approach for large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6376",
    "paper_id": "paper-273234021",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "Arithmetic-pruning-Llama2-Chat-7B",
            "name_full": "Arithmetic evaluation and pruning experiments on GSM8K, SVAMP, and MAWPS using Llama 2-Chat 7B",
            "brief_description": "This paper evaluates how using arithmetic datasets as pruning calibration data (GSM8K, SVAMP, MAWPS) affects arithmetic performance of pruned Llama 2-Chat 7B models under Wanda and SparseGPT pruning, comparing zero-shot, ICL, and ICL w/ CoT calibration formats at 50% (and in some experiments 70%) sparsity.",
            "citation_title": "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning",
            "mention_or_use": "use",
            "model_name": "Llama 2-Chat 7B",
            "model_family": "decoder-only transformer (Llama family)",
            "model_size": "7B",
            "training_data_description": "Not specified in detail in this paper; model is a Llama-family 7B chat model (pretrained on broad web-scale corpora per Llama releases).",
            "benchmark_name": "GSM8K, SVAMP, MAWPS",
            "task_type": "grade-school multi-step math word problems / small arithmetic problems (addition/subtraction/multi-step word problems)",
            "problem_format": "natural-language word problems presented as few-shot Q-A concatenations (ICL) or with Chain-of-Thought rationales (ICL w/ CoT); also zero-shot (question only).",
            "difficulty_level": "grade-school math; GSM8K: 2–8 steps; SVAMP: ≤2 arithmetic operations; MAWPS: varying complexity",
            "prompting_method": "zero-shot, few-shot In-Context Learning (ICL) with concatenated Q-A pairs, and ICL with Chain-of-Thought (ICL w/ CoT)",
            "performance_metric": "accuracy (task-specific exact-answer accuracy as reported in tables)",
            "performance_value": "Reported Llama 2-Chat 7B dense baselines and pruned results (Wanda, 50% unstructured sparsity) for evaluation tasks: GSM8K dense 0.0576; pruned with GSM8K calibration: zero-shot 0.0205, ICL 0.0425, ICL w/ CoT 0.0432. SVAMP dense 0.3867; pruned with GSM8K calibration: zero-shot 0.0233, ICL 0.2867, ICL w/ CoT 0.3067. MAWPS dense 0.4462; pruned with GSM8K calibration: zero-shot 0.0058, ICL 0.3442, ICL w/ CoT 0.3519. (Values taken directly from paper tables; other calibration/dataset combinations reported similarly.)",
            "internal_analysis": "No mechanistic probes (e.g., attention or neuron-level analyses) of numeric processing were performed. The paper analyzes input-format and dataset effects: (1) CoT calibration appears to help preserve arithmetic ability (authors hypothesize it 'guides pruning to better preserve weights for arithmetic reasoning'); (2) number of CoT steps (tested x = {3,4,5}) showed no clear monotonic relationship with final accuracy; (3) increasing the number of in-context Q-A examples in the calibration sequences generally correlated with improved sparse-model performance; (4) longer calibration sequence lengths (up to 2048 tokens) improved pruning outcomes.",
            "failure_modes": "Empirical failure modes observed: (1) calibration with CoT can introduce biases that harm performance outside the calibration domain (CoT helps arithmetic tasks but can reduce generality); (2) using nonsense calibration data (ellipses or random alphanumeric strings) performs substantially worse than sensible human-readable calibration data; (3) pruning with a downstream dataset does not necessarily produce the best sparse model for that same downstream task (domain mismatch effects); (4) performance is sensitive to calibration input length and number of in-context examples.",
            "scaling_trend": "Only 7B-scale models (Llama 2-Chat 7B and LLaMA 7B) were evaluated; no cross‑scale emergent trends are reported. The paper also reports that at higher sparsity (70%) Pile and RedPajama calibration outperformed C4, but broader size-scaling trends are not evaluated.",
            "uuid": "e6376.0",
            "source_info": {
                "paper_title": "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Prompting-effects-ICL-vs-CoT",
            "name_full": "Effect of In-Context Learning (ICL) and Chain-of-Thought (CoT) calibration data on pruning and arithmetic performance",
            "brief_description": "The paper systematically compares zero-shot, ICL, and ICL w/ CoT calibration-data formats for pruning and finds ICL broadly improves pruned-model performance across categories, while CoT is particularly beneficial for arithmetic reasoning datasets.",
            "citation_title": "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning",
            "mention_or_use": "use",
            "model_name": "Llama 2-Chat 7B (primary experiments; LLaMA 7B also evaluated in paper)",
            "model_family": "decoder-only transformer",
            "model_size": "7B",
            "training_data_description": "Not specified in detail in this paper; experiments use off-the-shelf Llama-family 7B models.",
            "benchmark_name": "GSM8K (arithmetic) and e-SNLI (NLI) used as canonical examples for prompting-format comparisons",
            "task_type": "arithmetic reasoning (GSM8K) and natural language inference (e-SNLI); comparisons generalize across evaluated downstream tasks",
            "problem_format": "zero-shot (question only), ICL (concatenated Q-A examples), ICL w/ CoT (Q-A with chain-of-thought rationales)",
            "difficulty_level": "GSM8K: grade-school multi-step; e-SNLI: NLI examples",
            "prompting_method": "few-shot ICL, ICL with Chain-of-Thought",
            "performance_metric": "accuracy (average accuracy across downstream tasks and per-task accuracy reported)",
            "performance_value": "Paper reports that ICL calibration improves average accuracy over zero-shot by +0.1754 for GSM8K-based calibrations and by +0.0826 for e-SNLI calibrations (average improvements reported in text). For arithmetic tasks, GSM8K (ICL w/ CoT) outperformed GSM8K (ICL) on arithmetic-specific evaluation, while GSM8K (ICL) outperformed GSM8K (ICL w/ CoT) on non-arithmetic tasks.",
            "internal_analysis": "Authors interpret CoT's benefit on arithmetic as likely due to step-by-step rationales helping pruning preserve arithmetic-relevant parameters; they do not provide internal mechanistic evidence (no probes, attention or activation analyses). They also test CoT step-count (3–5 steps) and find no consistent monotonic effect.",
            "failure_modes": "CoT calibration introduces domain-specific inductive biases that can hurt out-of-domain performance; more CoT reasoning steps do not reliably improve the pruned model; too few or nonsensical calibration examples harm pruning.",
            "scaling_trend": "Findings reported for 7B-scale models only; no claims about larger-scale behavior. The paper notes ICL is 'widely beneficial' across data categories, but CoT is beneficial primarily for arithmetic reasoning.",
            "uuid": "e6376.1",
            "source_info": {
                "paper_title": "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "GSM8K",
            "rating": 2
        },
        {
            "paper_title": "SVAMP",
            "rating": 2
        },
        {
            "paper_title": "MAWPS: A math word problem repository",
            "rating": 2,
            "sanitized_title": "mawps_a_math_word_problem_repository"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Are NLP models really able to solve simple math word problems?",
            "rating": 2,
            "sanitized_title": "are_nlp_models_really_able_to_solve_simple_math_word_problems"
        },
        {
            "paper_title": "SparseGPT: Massive language models can be accurately pruned in one-shot",
            "rating": 2,
            "sanitized_title": "sparsegpt_massive_language_models_can_be_accurately_pruned_in_oneshot"
        },
        {
            "paper_title": "Wanda: A simple and effective pruning approach for large language models",
            "rating": 2,
            "sanitized_title": "wanda_a_simple_and_effective_pruning_approach_for_large_language_models"
        }
    ],
    "cost": 0.0134675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning
9 Oct 2024</p>
<p>Abhinav Bandari <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#98;&#104;&#105;&#110;&#97;&#118;&#98;&#97;&#110;&#100;&#97;&#114;&#105;&#64;&#117;&#116;&#101;&#120;&#97;&#115;&#46;&#101;&#100;&#117;">&#97;&#98;&#104;&#105;&#110;&#97;&#118;&#98;&#97;&#110;&#100;&#97;&#114;&#105;&#64;&#117;&#116;&#101;&#120;&#97;&#115;&#46;&#101;&#100;&#117;</a> 
University of Washington</p>
<p>The University of Texas at Austin</p>
<p>Lu Yin 
University of Surrey</p>
<p>Cheng-Yu Hsieh 
University of Washington</p>
<p>Ajay Kumar Jaiswal 
The University of Texas at Austin</p>
<p>Tianlong Chen 
The University of North Carolina at Chapel
Hill 5 Sun Yat</p>
<p>sen University</p>
<p>Li Shen 
Ranjay Krishna 
University of Washington</p>
<p>Shiwei Liu 
University of Oxford</p>
<p>Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning
9 Oct 2024361195793BF83C65BFEBB174F9DA1FB7arXiv:2410.07461v1[cs.CL]
Network pruning has emerged as a potential solution to make LLMs cheaper to deploy.However, existing LLM pruning approaches universally rely on the C4 dataset as the calibration data for calculating pruning scores, leaving its optimality unexplored.In this study, we evaluate the choice of calibration data on LLM pruning, across a wide range of datasets that are most commonly used in LLM training and evaluation, including four pretraining datasets as well as three categories of downstream tasks encompassing nine datasets.Each downstream dataset is prompted with In-Context Learning (ICL) and Chain-of-Thought (CoT), respectively.Besides the already intriguing observation that the choice of calibration data significantly impacts the performance of pruned LLMs, our results also uncover several subtle and often unexpected findings, summarized as follows: (1) C4 is not the optimal choice for LLM pruning, even among commonly used pre-training datasets;(2) arithmetic datasets-when used as calibration data-performs on par or even better than pre-training datasets; (3) pruning with downstream datasets does not necessarily help the corresponding downstream task, compared to pre-training data; (4) ICL is widely beneficial to all data categories, whereas CoT is only useful on certain tasks.Our findings shed light on the importance of carefully selecting calibration data for LLM pruning and pave the way for more efficient deployment of these powerful models in real-world applications.We release our code at: https://github.com/abx393/llm-pruning-calibration-data.</p>
<p>Introduction</p>
<p>In the 2020s, the landscape of AI has transitioned into a new era, propelled forward by the advancements made in large language models (LLMs) (Brown et al., 2020;Gemini Team et al., 2023;Touvron et al., 2023).The astonishing language capacities of LLMs have significantly shaped the solutions to various real-life tasks such as natural language understanding (Brown et al., 2020;Touvron et al., 2023), text generation (Kocoń et al., 2023;Anil et al., 2023), vision tasks (Radford et al., 2021;Zhou et al., 2022a), coding (Chen et al., 2022), and math (Romera-Paredes et al., 2024).However, the enormous size of these powerful LLMs poses a significant challenge for deployment in many real-world applications.For instance, deploying a 7B LLM requires around 10GB of main memory (DRAM) even after adopting INT8 quantization, which unfortunately exceeds the memory capacity of most commodity edge devices.</p>
<p>Network pruning, as one of the most wellestablished approaches in model compression, demonstrated the possibility of removing around 50% of the parameters (Frantar and Alistarh, 2023a;Sun et al., 2023;Zhang et al., 2023), or even more (Yin et al., 2023b;Agarwalla et al., 2024) with minimal performance degradation.Interestingly, while consistently producing robust performance in small-scale deep neural networks (Han et al., 2015;Frankle and Carbin, 2019;Mocanu et al., 2018;Gale et al., 2019), magnitude pruning (Han et al., 2015) seems to lose importance in the context of LLM pruning.All state-of-the-art LLM pruning approaches unanimously choose to use a small set of data (known as calibration data) from the C4 training dataset (Raffel et al., 2020) to calculate their pruning scores (Frantar and Alistarh, 2023a;Sun et al., 2023;Yin et al., 2023b).</p>
<p>Using C4 as the calibration data for pruning makes sense if the models are pre-trained on it to preserve better the desired distribution learned during pre-training.However, not all large language models are pre-trained with the C4 dataset, raising the question of whether the C4 is the optimal choice for the calibration data for LLM pruning.In addition, it is well-known that LLMs are very sensitive to how the input is structured and provided to them (Zhou et al., 2022b;Shi et al., 2023).As a result, it is unclear how the input format of calibration data would affect LLM pruning.</p>
<p>To answer these questions, in this work, we conduct a comprehensive study to investigate the effect of calibration data on LLM pruning across a broad range of evaluation tasks, along two dimensions of interest: varying types of datasets and different data input formats.Specifically, we investigate the following possible alternatives of calibration data for LLM pruning, as illustrated in Figure 1: • Pre-training Data: Apart from the C4 dataset, several other datasets are widely used for pretraining LLMs.We examine three of the most representative datasets: Pile (Gao et al., 2020), OSCAR (Suárez et al., 2020), andRedPajama (Together Computer, 2023).</p>
<p>• Downstream Data: While pruning with pretraining datasets is intuitively preferred to preserve pre-training knowledge, it is essential to empirically verify this assumption and identify whether pruning with any downstream datasets may yield superior outcomes for LLM pruning.To investigate this, we consider three categories of downstream tasks, encompassing a total of nine datasets (see Section 3.1 for details).An intriguing research question arises: will pruning with downstream data produce a better sparse model for the corresponding downstream task than pruning with pre-training data?</p>
<p>• Prompted Downstream Data: Acknowledging the significant impact of prompts on LLM performance, we explore two variants of prompting strategies to construct different formats of calibration data: In-Context Learning (ICL) (Brown et al., 2020) and In-Context Learning w/ Chain-of-Thought (ICL w/ CoT) (Wei et al., 2022).</p>
<p>• Nonsense Data: In addition, we explore two variants of nonsensical calibration data-ellipses and random alphanumeric strings-to investigate the necessity of semantically meaningful calibration data for effective LLM pruning.</p>
<p>To investigate the impact of these datasets, we prune LLMs using various calibration datasets and evaluate the resulting sparse models across nine downstream tasks.Our key and encouraging finding is that, while C4 consistently produces robust sparse models, it is not the best calibration dataset for pruning.In addition, our study unveils several more subtle and unexpected findings, which can be summarized as follows:</p>
<p>• C4, although consistent in producing robust sparse models, is not the optimal choice for LLM pruning, and it is also not the best among various pre-training datasets.Pile consistently outperforms C4 with higher average accuracy.</p>
<p>• Certain types of downstream data lead to better sparse LLMs than others.Arithmetic downstream datasets in general perform on par or even better than pre-training datasets in this context of LLM pruning.</p>
<p>• Pruning with downstream data does not necessarily lead to the best performance on that downstream task than pruning with a pretraining dataset like Pile.</p>
<p>• ICL calibration data broadly benefits all data categories, while ICL w/ CoT calibration data is only advantageous for arithmetic reasoning datasets.</p>
<p>2 Related Work</p>
<p>Large Language Model Pruning</p>
<p>Network pruning is a widely utilized technique to reduce model size with negligible performance loss (Mozer and Smolensky, 1989;Han et al., 2015;Molchanov et al., 2017).While numerous pruning approaches have been proposed, the success of pruning is inextricably linked to sufficient retraining (Liu et al., 2022;Wang et al., 2023).However, training large language models is prohibitively expensive and not feasible for most practitioners.</p>
<p>Fortunately, recent research efforts have proposed effective methods that enable accurate pruning of LLMs without the need for extensive fine-tuning.</p>
<p>SparseGPT (Frantar and Alistarh, 2023a) employs second-order pruning followed by column-wise weight updates, allowing the removal of 50% of weights while maintaining the original perplexity.Wanda (Sun et al., 2023), motivated by the goal of preserving crucial outliers in LLMs, proposes pruning weights based on the multiplication of weight  magnitude with their input activation, demonstrating strong performance.OWL (Yin et al., 2023b) introduces a novel non-uniform layerwise sparsity approach for LLM pruning, showing promising results at high levels of sparsity.In addition to exploring accurate pruning methods, other studies focus on efficiently fine-tuning sparse LLMs to further enhance their performance (Zhang et al., 2023;Zimmer et al., 2023).In contrast to these previous works, our paper investigates the efficacy of input data for LLM pruning.This novel perspective is crucial for understanding and improving LLM pruning methodologies, as LLMs are sensitive to their input (Zhao et al., 2021).</p>
<p>Prompting for Sparse LLMs</p>
<p>Prompting involves providing instructions to a pretrained language model, either as a single instruction (zero-shot) or through one or more examples (one/few-shot) that demonstrate the task.Brown et al. (2020) demonstrated that prompt design is highly effective for guiding a non-modifiable GPT-3 model in zero, one, and few-shot settings.Initially, efforts in prompt-tuning focused on the discrete selection of prompt template tokens, as explored by Jiang et al. (2020).Later studies, such as those by Lester et al. (2021), shifted towards using continuous prompts that were refined through backpropagation.Xu et al. (2023) first discovered that the generation quality of a compressed LLM could be significantly improved by adding carefully designed hard prompts and proposed a soft prompt learning method to improve the compressed LLM.Hoang et al. (2023) argued that the performance drop caused by pruning is because the pre-trained knowledge is displaced rather than being forgotten.Williams and Aletras (2023) examined the impact of multiple pre-training data sources on pruning.However, their study was confined to pretraining data sources.Our research extends this investigation by not only analyzing four commonly used pre-training datasets but also exploring various downstream datasets with In-Context Learning and Chain of Thought prompts, leading to more intriguing findings and a deeper understanding of the effects of different data sources on pruning.
SparseGPT ✓ |W| 2 /diag[(XX T + λI) −1 ] ij Wanda ✗ |W ij | • ∥X j ∥ 2</p>
<p>Methodology</p>
<p>In this section, we describe in detail how we assess the effects of various calibration datasets and data formats on LLM pruning.</p>
<p>Pruning Methods</p>
<p>We choose the two most widely-used pruning methods, i.e., Wanda (Sun et al., 2023) and SparseGPT (Frantar and Alistarh, 2023b) as our pruning methods.Both pruning methods necessitate a small subset of calibration data to calculate pruning scores, which are shown in Table 1.In this context, X symbolizes layer activations and W represents weights.The expression X T X + λI in the denominator forms the Hessian H, essential for the layer-wise reconstruction issue, with λ serving as a dampening factor to prevent computational collapse during inversion.Wanda augments the standard weight magnitude pruning metric by integrating input activations, whereas SparseGPT incorporates an additional weight update step within its column-wise pruning process.The weights with the lowest scores will be pruned, resulting in a sparse LLM.</p>
<p>Model, Dataset, and Evaluation</p>
<p>Model.We use the common models used in previous work (Sun et al., 2023;Yin et al., 2023a), i.e., Llama 2-Chat 7B (Touvron et al., 2023) and Llama 7B (Touvron et al., 2023) as the base models for pruning.</p>
<p>Dataset.The source of our calibration data is divided into two categories: pre-training datasets and downstream datasets.For pre-training data, we selected four widely-used datasets: C4 (Raffel et al., 2020), Pile (Gao et al., 2020), OSCAR (Suárez et al., 2020), and RedPajama (Together Computer, 2023).To ensure the diversity of the downstream calibration data, we focused on three major tasks: arithmetic reasoning, natural language inference, and commonsense reasoning, selecting three datasets for each category.</p>
<p>For arithmetic reasoning, we chose the following three datasets:</p>
<p>• GSM8K (Cobbe et al., 2021) is a dataset of grade school math word problems, where each problem takes between 2 and 8 steps to solve.</p>
<p>• SVAMP (Patel et al., 2021) is another dataset of grade school math word problems, where each problem requires no more than 2 arithmetic operations to solve.</p>
<p>• MAWPS (Koncel-Kedziorski et al., 2016) is another dataset of grade school math word problems of varying complexity.</p>
<p>For natural language inference, we use the following datasets:</p>
<p>• e-SNLI (Camburu et al., 2018) is a dataset of entailment relations along with humanannotated natural language explanations of the labels.</p>
<p>• ANLI (Nie et al., 2020) is a dataset of entailment relations that was iteratively and adversarially generated with a human-and-modelin-the-loop procedure.ANLI R1 represents the data produced in the first round of this.</p>
<p>• ANLI R3 (Nie et al., 2020) represents the data produced in the third round of the aforementioned iterative procedure.The adversarial model is trained on data produced in previous rounds, so crowdworkers are incentivized to create distinct entailment relations to challenge the model, so ANLI R3 is distinct from ANLI R1.</p>
<p>For commonsense reasoning, we use the following:</p>
<p>• CommonsenseQA (CSQA) (Talmor et al., 2019) is a commonsense question answering dataset with multiple choice questions that require some prior knowledge not provided in the question.</p>
<p>• RACE (Lai et al., 2017)   • WinoGrande (Sakaguchi et al., 2019) is a commonsense question answering dataset with fill-in-the-blank statements and binary answer options.</p>
<p>Evaluation.To evaluate the performance of different calibration datasets, we first prune the dense LLM with certain calibration data and then evaluate the resulting sparse LLM on all the downstream tasks considered using few-shot prompting (Brown et al., 2020).</p>
<p>Calibration Data Formulation</p>
<p>The pruning calibration data have 128 sequences of length 2048 tokens each, following prior work (Frantar and Alistarh, 2023a;Sun et al., 2023;Yin et al., 2023b).</p>
<p>Pre-training Data.For each pre-training dataset, we create each calibration data sample of length 2048 tokens by concatenating text segments from the dataset until it exceeds 2048 tokens and then selecting a segment of length 2048 from this.Downstream Data.To provide a comprehensive evaluation of downstream data, we use the following three variants.</p>
<p>• Zero-Shot.We create each calibration data sample by selecting a random question from the dataset without the answer.We fill up the remaining context length with padding tokens.</p>
<p>• In-Context Learning.We create each calibration data sample by concatenating multiple randomly selected question-answer pairs to fill up the context length of 2048 tokens.</p>
<p>• In-Context Learning w/ Chain-of-Thought.</p>
<p>We create each calibration data sample by concatenating randomly selected question-answer pairs, where the answer contains CoT rationale, to fill up the context length of 2048 tokens.</p>
<p>Results</p>
<p>In this section, we report the results of our experiments.Our primary goal is to explore how performance fluctuates when using various calibration data across different formats.We analyze overall performance trends across these differing setups.</p>
<p>Pre-training Dataset as Calibration Data</p>
<p>We evaluate pruning performance using calibration data derived from a range of pre-training datasets including C4, RedPajama, Oscar, and Pile.The results are detailed in Table 2. Our analysis reveals that the average accuracy of Pile consistently outperforms the C4 dataset.Using Wanda with target sparsity 0.5, calibration with the Pile dataset exhibits superior performance in terms of average accuracy across nine downstream tasks, surpassing other pre-training datasets in six out of nine tasks.</p>
<p>Similarly, for SparseGPT pruning, the Pile dataset achieves the highest average accuracy, although the differences among the four pre-training datasets are small.Notably, when compared with the commonly used C4 dataset, our analysis reveals that Red-Pajama achieves comparable performance, and Pile demonstrates an improvement, outperforming C4 in Wanda pruning across a majority of downstream tasks.Specifically, using the Llama 2-Chat 7b model, Pile leads C4 in seven out of nine tasks when using Wanda.Although when using SparseGPT, Pile outperforms C4 in only four out of nine tasks, Pile still has higher average accuracy across nine tasks.In Table 3, when we target 70% sparsity, we can clearly see that RedPajama and Pile achieve significantly higher average accuracy than C4.These findings underscore that C4 is not the optimal choice of calibration data for LLM pruning.Pile consistently serves as better calibration data in LLM pruning.</p>
<p>Downstream Dataset as Calibration Data</p>
<p>While using pre-training datasets for pruning may preserve acquired knowledge, it is crucial to empirically validate this strategy and determine if alternative downstream datasets might yield superior results for pruning LLMs.To this end, we utilized downstream datasets both as calibration data for pruning and as benchmarks for evaluation.</p>
<p>We compare three formats of downstream data: Zero-Shot, ICL and ICL w/ CoT.We systematically assessed the pruning performance across various downstream tasks using different calibration data formats: single GSM8K question (Zero-Shot), concatenated GSM8K question-answer pairs (ICL), and concatenated GSM8K question-answer pairs with Chain of Thought (ICL w/ CoT).Our findings, detailed in Table 4, reveal that ICL consistently enhances performance across all data categories compared to the baseline zero-shot approach, achieving an average accuracy improvement of 0.1754.We also observed that GSM8K (ICL w/ CoT) calibration data outperforms GSM8K (ICL) data in Arithmetic Reasoning tasks.An explanation for this could be that the step-by-step reasoning in CoT calibration data helps guide the pruning to better preserve the model weights for arithmetic reasoning.However, GSM8K (ICL) surpasses GSM8K (ICL w/ CoT) in average performance across a broader set of downstream tasks as GSM8K (ICL) outperforms GSM8K (ICL w/ CoT) for tasks outside of arithmetic reasoning.This may be because the step-by-step reasoning in CoT introduces biases that are detrimental when the sparse model is used outside of the domain of the calibration data.</p>
<p>We also compare the pruning performance of e-SNLI (Zero-Shot), e-SNLI (ICL) and e-SNLI (ICL w/ CoT) in Table 4.We find that ICL again enhances performance compared to the baseline zeroshot format, with an average accuracy improvement of 0.0826.We also find that, compared to the ICL format, including CoT in the calibration data only improves performance on ANLI R3 among the three NLI evaluation tasks.For the other categories of evaluation tasks, we find that e-SNLI (ICL) and e-SNLI (ICL w/ CoT) have similar pruning performance, and the former is better for some tasks and the latter is better for others.</p>
<p>Winning Dataset?</p>
<p>We evaluated the performance of ICL tasks against the top-performing pre-training dataset, Pile, with both the Llama 2-Chat 7B and LLaMA 7B models and have presented our findings in Table 5.Specifically, using the Llama 2-Chat 7B model, in the Arithmetic Reasoning category, Pile led in two out of three tasks.For NLI and Commonsense Reasoning tasks, the best calibration datasets come from the downstream dataset and from different task categories.Upon reviewing average performance across all tasks, we observed that Arithmetic Reasoning generally matched the performance of the best pre-training dataset, Pile.Notably, SVAMP emerged as the most effective dataset overall, outperforming Pile with an average accuracy margin of 0.52% with the Llama 2-Chat 7B model and with an average accuracy margin of 2.21% with the Llama 7B model.Consequently, SVAMP has been designated as the winning dataset.</p>
<p>Additionally, an intriguing observation from our study was that the optimal calibration data for each downstream task did not necessarily coincide with the data from the corresponding task itself.This suggests that calibration data efficacy may not be task-specific and invites further exploration into the dynamics of calibration data across varied contexts.</p>
<p>Further Analysis</p>
<p>Can we do better by including more steps in CoT?In our previous construction of the calibration data, we selected question-answer pairs with no restriction on the number of steps in CoT in the answer.This inspires a follow-up question: does the number of steps of CoT rationale in the calibration data affect the sparse LLM's performance?We investigated this by constructing calibration data by concatenating multiple question-answer pairs, where each answer rationale contains exactly x steps.Since 1-step or 2-step CoT data was scarce, we performed this for x = {3, 4, 5} as seen in Table 6.We find no clear relationship between the number of steps of CoT in calibration data and the performance of the sparse LLM.However, we note that it is possible to produce a better sparse LLM for a given task by restricting the calibration data to a specific number of steps, which may vary based on the evaluation task.</p>
<p>Does more Q-A pairs in ICL calibration data lead to a better sparse model?To investigate this, we evaluated the pruning performance when calibration data contains 5, 10, 15, 20, and 25 Q-A pairs, filling the rest of the context window with padding tokens.Our default ICL calibration data fills the context window with Q-A pairs until it reaches length 2048 tokens, which in practice can be anywhere from 25 to 30 Q-A pairs.We compare the pruning performance of all of these calibration data formats in   ing to compare this with the pruning performance of nonsense data calibration data, such as ellipses and random alphanumeric strings, in this context.Consequently, we substituted conventional calibration data with these unconventional types for pruning the Llama 2-Chat 7B model to 50% unstructured sparsity using the Wanda pruning method.The performance outcomes are shown in Table 8.The results clearly show that the Pile dataset, which contains human-readable data, consistently outperforms both ellipses and random alphanumeric strings in nearly all cases except one scenario within the GSM8K task.Moreover, random alphanumeric data generally exhibited better performance compared to ellipses.Therefore we affirm the importance of utilizing sensible calibration data for the effective pruning of LLMs.</p>
<p>Conclusion</p>
<p>This study critically examines the widely held belief that the C4 dataset is the optimal calibra-tion choice for pruning LLMs.Through an extensive evaluation encompassing a variety of calibration data types-both pre-training and downstream datasets, our findings reveal that C4 does not hold universal superiority.Specifically, our analysis demonstrates that the pretraining dataset Pile consistently outperforms C4, while alternative downstream datasets, particularly those involving arithmetic reasoning tasks, yield comparable pruning outcomes.Furthermore, our investigation into various downstream task formats has uncovered that In-Context Learning (ICL) offers significant benefits across all data categories.In-Context Learning w/ Chain-of-Thought (ICL w/ CoT) calibration is particularly effective in enhancing performance in arithmetic reasoning tasks.Our study advocates for a more nuanced selection and curation of calibration data, which could lead to more efficient and effective LLM pruning strategies, ultimately facilitating the deployment of more robust models in practical settings.</p>
<p>Limitations</p>
<p>Our study has several limitations.First, all experiments were conducted using the Llama 2-Chat 7B and LLaMA 7B models; we aim to expand our investigations to other LLM architectures and larger models.Second, our analysis was limited to the Wanda and SparseGPT pruning algorithms.Future work will explore a broader range of pruning methods.Third, we plan to evaluate the effects of combining multiple datasets on pruning performance.We believe that our insights regarding calibration data will inspire further research within the community.</p>
<p>Another limitation of this work we aim to address in the future is that we have not rigorously investigated why Pile is better calibration data than C4 for LLM pruning.We conjecture the benefits come from that Pile is a more diverse dataset with higher quality of examples, which is designed such that models trained on it have improved downstream generalization capabilities, compared to the more noisy Common Crawl datasets like C4, as also pointed out in recent work in the context of LLM pretraining (Li et al., 2024).As such, we believe Pile could provide more robust calibration data to guide the pruning of LLMs to optimize the performance of the sparse model on a variety of downstream tasks.We leave the investigation on the correlation between a dataset's effectiveness for LLM pretraining and model pruning as a future direction to explore.</p>
<p>Figure 1 :
1
Figure 1: Examples of various calibration data formats examined in this paper.</p>
<p>Betty is saving money for a new wallet which costs $100.Betty has only half of the money she needs.Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents.How much more money does Betty need to buy the wallet?
Pre-training DataRaptorDB -the Key Value Store -CodeProject 13,046,356 members (108,633 online) Last Visit: 31-Dec-99 18:00 Last Update: 23-Jul-17 11:31Refresh« Prev1234567891011 Next »Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. Howmuch did she earn?Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. Howmuch did she earn?Answer: 10. . .Answer: 5a03x93js0dldjdnfmbi39gndkdfhb9w4t239tgsjj923jrwksks9xkxkxkqk3jnskdfnskdfn9snj3n3knsknknsnsnskskskskkkkkskkkxnx9
Question:</p>
<p>Table 1 :
1
Pruning metrics of Wanda and SparseGPT.
MethodWeight UpdatePruning Metric S ij</p>
<p>Table 2 :
2
.0457 ± 0.0008 0.0412 ± 0.0062 0.0450 ± 0.0088 0.0404 ± 0.0048 0.0440 ± 0.0052 0.0430 ± 0.0048 0.0412 ± 0.0046 0.0384 ± 0.0038 SVAMP 0.3867 0.2756 ± 0.0102 0.2733 ± 0.0133 0.2922 ± 0.0102 0.2878 ± 0.0038 0.3011 ± 0.0193 0.3033 ± 0.0370 0.3089 ± 0.0278 0.3445 ± 0.0139 MAWPS 0.4462 0.3160 ± 0.0293 0.3154 ± 0.0308 0.3436 ± 0.0097 0.3635 ± 0.0115 0.3295 ± 0.0235 0.3487 ± 0.0289 0.3500 ± 0.0416 0.3820 ± 0.0262 e-SNLI 0.6050 0.4934 ± 0.0096 0.4940 ± 0.0377 0.4812 ± 0.0205 0.5376 ± 0.0023 0.5447 ± 0.0326 0.5641 ± 0.0295 0.5485 ± 0.0487 0.5498 ± 0.0289 ANLI R1 0.3900 0.3250 ± 0.0156 0.3240 ± 0.0255 0.3203 ± 0.0042 0.3420 ± 0.0087 0.3580 ± 0.0356 0.3640 ± 0.0183 0.3463 ± 0.0261 0.3380 ± 0.0020 ANLI R3 0.4192 0.3361 ± 0.0106 0.3405 ± 0.0058 0.3220 ± 0.0042 0.3597 ± 0.0145 0.3575 ± 0.0153 0.3478 ± 0.0226 0.3480 ± 0.0136 0.3408 ± 0.0043 CSQA 0.6208 0.5171 ± 0.0024 0.5184 ± 0.0184 0.5225 ± 0.0043 0.5239 ± 0.0078 0.5266 ± 0.0314 0.5304 ± 0.0204 0.5233 ± 0.0141 0.5258 ± 0.0331 RACE 0.6501 0.4686 ± 0.0052 0.4386 ± 0.0109 0.4632 ± 0.0224 0.4692 ± 0.0079 0.5305 ± 0.0272 0.5407 ± 0.0101 0.5374 ± 0.0279 0.5376 ± 0.0215 WinoGrande 0.5122 0.5141 ± 0.0094 0.5141 ± 0.0067 0.5141 ± 0.0087 0.5125 ± 0.0051 0.5183 ± 0.0143 0.5193 ± 0.0016 0.5240 ± 0.0311 0.5164 ± 0.0194 Accuracy of Llama 2-Chat 7B model pruned with Wanda and SparseGPT to 50% unstructured sparsity using different pre-training datasets, averaged over three random seeds.The value after ± indicates 2 standard deviations.Results for both pruning methods are shown alongside the original dense model for comparison.The best performance on each evaluation task for each pruning algorithm is bold.
EvaluationDenseWanda w. Calibration DataSparseGPT w. Calibration DataC4RedPajamaOscarPileC4RedPajamaOscarPileGSM8K 0.0576 0Average 0.4542 0.3657 ± 0.0041 0.3622 ± 0.0005 0.3671 ± 0.0055 0.3819 ± 0.0029 0.3900 ± 0.0063 0.3957 ± 0.0090 0.3920 ± 0.0153 0.3970 ± 0.0045Evaluation task Dense ModelWanda w. Calibration DataC4RedPajama OscarPileGSM8K0.05760.02690.01860.0208 0.0239SVAMP0.38670.02000.01330.0200 0.0133MAWPS0.44620.00190.00000.0000 0.0000e-SNLI0.60500.13130.24320.0687 0.3249ANLI R10.39000.00000.00000.0000 0.1190ANLI R30.41920.00000.00000.0000 0.0925CSQA0.62080.21380.20720.2113 0.2170RACE0.65010.25280.21970.2514 0.2540WinoGrande0.51020.50120.49250.4743 0.4972Average0.45420.12750.13270.1163 0.1713
is a commonsense question answering dataset where each question is related to a provided text passage.It evaluates understanding and reasoning abilities.</p>
<p>Table 3 :
3
Accuracy of Llama 2-Chat 7B model pruned with Wanda to 70% unstructured sparsity using different pre-training datasets.Results are shown alongside the original dense model for comparison.The best performance on each evaluation task is bold.</p>
<p>Table 4 :
4
Accuracy of Llama 2-Chat 7B model pruned with Wanda to 50% unstructured sparsity using different formats of GSM8K and e-SNLI as calibration data.For each evaluation task, the best performance among the GSM8K calibration data variants and the best performance among the e-SNLI calibration data variants is bold.
Evaluation task Dense ModelWanda w. Calibration DataGSM8K (Zero-shot) GSM8K (ICL) GSM8K (ICL w/ CoT) e-SNLI (Zero-shot) e-SNLI (ICL) e-SNLI (ICL w/ CoT)GSM8K0.05760.02050.04250.04320.03030.04320.0379SVAMP0.38670.02330.28670.30670.12330.21000.2133MAWPS0.44620.00580.34420.35190.06350.26350.2404e-SNLI0.60500.32920.54380.50800.34280.55410.5517ANLI R10.39000.29200.31800.30500.33400.33500.3330ANLI R30.41920.24170.35670.31080.33500.34500.3717CSQA0.62080.21380.53810.51840.40870.51270.5201RACE0.65010.20670.47930.46980.35220.46530.4710WinoGrande0.51220.51140.51300.51540.50510.50910.5075Average0.45420.20490.38030.36990.27720.35980.3607Model Evaluation DenseWanda w. Calibration DataPDArithmetic ReasoningNLICommonsense ReasoningPileGSM8K SVAMP MAWPS e-SNLI ANLI R1 ANLI R3 CSQA RACE WinoGrandeGSM8K0.0576 0.0404 0.04250.04250.04620.04320.04170.04550.0417 0.04090.0432Llama 2-Chat 7BSVAMP MAWPS e-SNLI ANLI R1 ANLI R3 CSQA0.3867 0.2878 0.2867 0.4462 0.3635 0.3442 0.6050 0.5376 0.5438 0.3900 0.3420 0.3180 0.4192 0.3597 0.3567 0.6208 0.5239 0.53810.2833 0.3365 0.5711 0.3440 0.3875 0.52330.2733 0.3346 0.5436 0.313 0.3700 0.50450.2100 0.2635 0.5541 0.3350 0.3450 0.51270.2633 0.3038 0.5345 0.3500 0.3700 0.50450.2667 0.3038 0.5441 0.3490 0.3575 0.53640.2233 0.2667 0.2654 0.3231 0.5768 0.5317 0.3360 0.3370 0.3633 0.3642 0.5479 0.53730.2600 0.2731 0.5955 0.3520 0.3792 0.5070RACE0.6501 0.4692 0.47930.47930.47260.46530.43410.46450.4706 0.46250.4422WinoGrande 0.5122 0.5125 0.51300.51620.51140.50910.52570.52410.5107 0.51620.5209Average0.4542 0.3819 0.38030.38710.37440.35980.36970.37680.3706 0.37550.3748GSM8K0.0447 0.0409 0.04620.04400.04170.03940.03940.04170.0462 0.03870.0447SVAMP0.3267 0.2733 0.15330.25330.19000.18330.08670.10670.0733 0.09670.0800LLaMA 7BMAWPS e-SNLI ANLI R1 ANLI R30.3596 0.3173 0.3327 0.5556 0.3284 0.3433 0.3800 0.3210 0.4000 0.3167 0.3625 0.38330.3577 0.3767 0.4000 0.38330.3096 0.3678 0.3700 0.37500.1615 0.3653 0.3340 0.33170.2942 0.3430 0.2600 0.35830.2846 0.3411 0.3100 0.41670.1615 0.2808 0.3304 0.3306 0.3800 0.2600 0.3417 0.36670.2385 0.3291 0.3900 0.3917CSQA0.3948 0.2613 0.29070.27930.25230.19740.26040.27350.2629 0.27520.2883RACE0.3134 0.2758 0.29720.27480.25250.28390.26570.31030.2698 0.28800.2748WinoGrande 0.5130 0.4964 0.51540.50670.52640.51380.51620.50360.5043 0.51780.5225Average0.3561 0.2974 0.30690.31950.29840.26780.26930.28760.2633 0.27270.2844</p>
<p>Table 5 :
5
Accuracy of Llama 2-Chat 7B model and LLaMA 7B model pruned with Wanda to 50% sparsity using various downstream datasets with ICL format.PD denotes pre-training data.The best performance on each evaluation task among sparse models is bold.</p>
<p>Table 7 .
7
The results confirm our conjecture that an increase in in-context examples in the pruning calibration data generally correlates with enhanced performance of the sparse model.
How does input length affect the pruning perfor-mance? In our main experiments, the calibration</p>
<p>Table 6 :
6
Accuracy of Llama 2-Chat 7B model pruned with Wanda to 50% sparsity using different numbers of steps of CoT in the calibration data.For instance, GSM8K (ICL w/ x-step CoT) indicates the calibration data consists of concatenations of several question-answer pairs where each answer has exactly x steps of reasoning.The default configuration of GSM8K (ICL w/ CoT) has no restriction on the number of steps of CoT.Accuracy of Llama 2-Chat 7B model pruned with Wanda to 50% unstructured sparsity using GSM8K with different calibration data lengths and pre-training data.
Evaluation task Dense Model Calibration Data# In-Context Q-A PairsSparse ModelGSM8K0.0576C4-0.0455GSM8K0.0576Pile-0.0404GSM8K0.0576GSM8K50.0288GSM8K0.0576GSM8K100.0440GSM8K0.0576GSM8K150.0455GSM8K0.0576GSM8K200.0417GSM8K0.0576GSM8K250.0470GSM8K0.0576GSM8KFill Q-A pairs to sequence length (2048 tokens)0.04257: Evaluation task Dense ModelSparse ModelPileellipses random alphanumericGSM8K0.05760.0404 0.02730.0402SVAMP0.38670.2878 0.05760.1433MAWPS0.44620.3635 0.00960.1462e-SNLI0.60500.5376 0.32950.3679ANLI R10.39000.3420 0.31000.3250ANLI R30.41920.3597 0.33000.3275CSQA0.62080.5239 0.19250.3170RACE0.65010.4692 0.26310.3293WinoGrande0.51220.5125 0.49720.5043Average0.45420.3819 0.22410.2779</p>
<p>Table 8
8Evaluation task Dense Model Pruning Input Length Sparse ModelWikiText12829.22WikiText25615.72WikiText6.9451211.82WikiText10249.27WikiText20488.48: Accuracy of Llama 2-Chat 7B model prunedwith Wanda to 50% unstructured sparsity using Pile,ellipses, and random alphanumeric characters.data for pruning consisted of 128 sequences, each2048 tokens in length. It is crucial to investigatewhether this specific token length is necessary foreffective pruning. To address this question, we usedthe C4 dataset for calibration and systematicallyvaried the calibration data lengths between 256,512, 1024, and 2048 tokens. We then evaluatedthe perplexity of Llama 2-Chat 7B pruned to 50%unstructured sparsity using Wanda. As detailed inTable 9, our findings confirm that increased inputlengths correlate positively with improved modelperformance, aligning with our initial expectations.
Does input data for pruning have to be sensible?In our previous setup, calibration data for pruning is sourced from either pre-training datasets or task-specific downstream datasets.It is intrigu-</p>
<p>Table 9 :
9
Perplexity of Llama 2-Chat 7B model on Wiki-Text pruned with Wanda to 50% unstructured sparsity using different input lengths of C4.</p>
<p>AcknowledgementS. Liu is funded by the Royal Society with the Newton International Fellowship.
Enabling high-sparsity foundational llama models with efficient pretraining and deployment. Abhinav Agarwalla, Abhay Gupta, Alexandre Marques, Shubhra Pandit, Michael Goin, Eldar Kurtic, Kevin Leong, Tuan Nguyen, Mahmoud Salem, Dan Alistarh, arXiv:2405.035942024arXiv preprint</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>e-snli: Natural language inference with natural language explanations. Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, Phil Blunsom, Advances in Neural Information Processing Systems. Curran Associates, Inc201831</p>
<p>On the transferability of pre-trained language models for low-resource programming languages. Fuxiang Chen, H Fatemeh, David Fard, Timofey Lo, Bryksin, Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension. the 30th IEEE/ACM International Conference on Program Comprehension2022</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR, abs/2110.141682021</p>
<p>The lottery ticket hypothesis: Finding sparse, trainable neural networks. Jonathan Frankle, Michael Carbin, International Conference on Learning Representations (ICLR). 2019</p>
<p>Massive language models can be accurately pruned in one-shot. Elias Frantar, Dan Alistarh, International Conference on Machine Learning (ICML). 2023a</p>
<p>Sparsegpt: Massive language models can be accurately pruned in one-shot. Elias Frantar, Dan Alistarh, International Conference on Machine Learning. PMLR2023b</p>
<p>Trevor Gale, Erich Elsen, Sara Hooker, arXiv:1902.09574The state of sparsity in deep neural networks. 2019arXiv preprint</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.00027The pile: An 800gb dataset of diverse text for language modeling. 2020arXiv preprint</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Learning both weights and connections for efficient neural network. Song Han, Jeff Pool, John Tran, William Dally, Advances in Neural Information Processing Systems (NeurIPS). 2015</p>
<p>Mohammad Rastegari, and Zhangyang Wang. 2023. (dynamic) prompting might be all you need to repair llms. N M Duc, Minsik Hoang, Thomas Cho, Merth, arXiv:2310.00867arXiv preprint</p>
<p>How can we know what language models know? Transactions of the. Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020Association for Computational Linguistics8</p>
<p>Chatgpt: Jack of all trades, master of none. Jan Kocoń, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, Information Fusion. 991018612023</p>
<p>Mawps: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies. the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies2016</p>
<p>Race: Large-scale reading comprehension dataset from examinations. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy, arXiv:1704.046832017arXiv preprint</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, arXiv:2104.086912021arXiv preprint</p>
<p>Datacomp-lm: In search of the next generation of training sets for language models. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, arXiv:2406.117942024arXiv preprint</p>
<p>Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin Mocanu, Zhangyang Wang, Mykola Pechenizkiy, arXiv:2202.02643The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training. 2022arXiv preprint</p>
<p>Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Constantin Decebal, Elena Mocanu, Peter Mocanu, Phuong H Stone, Madeleine Nguyen, Antonio Gibescu, Liotta, Nature Communications. 92018</p>
<p>Pruning convolutional neural networks for resource efficient inference. Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, International Conference on Learning Representations (ICLR). Jan Kautz. 2017</p>
<p>Skeletonization: A technique for trimming the fat from a network via relevance assessment. C Michael, Paul Mozer, Smolensky, Advances in Neural Information Processing Systems (NeurIPS). 1989</p>
<p>Adversarial NLI: A new benchmark for natural language understanding. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics2020</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, CoRR, abs/2103.071912021</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Mathematical discoveries from program search with large language models. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, Nature. 62579952024</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, arXiv:1907.106412019arXiv preprint</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, Denny Zhou, International Conference on Machine Learning. PMLR2023</p>
<p>Pedro Javier, Ortiz Suárez, Laurent Romary, Benoît Sagot, arXiv:2006.06202A monolingual approach to contextualized word embeddings for mid-resource languages. 2020arXiv preprint</p>
<p>Mingjie Sun, Zhuang Liu, Anna Bair, J Zico Kolter, arXiv:2306.11695A simple and effective pruning approach for large language models. 2023arXiv preprint</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Redpajama: an open dataset for training large language models. 2023Together Computer</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, 10.48550/arXiv.2307.09288arXiv:2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien RodriguezYuchen Zhang, Angela Fan. and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv e-prints</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Why is the state of neural network pruning so confusing? on the fairness, comparison setup, and trainability in network pruning. Huan Wang, Can Qin, Yue Bai, Yun Fu, arXiv:2301.052192023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems (NeurIPs). 202235</p>
<p>How does calibration data affect the post-training pruning and quantization of large language models. Miles Williams, Nikolaos Aletras, arXiv:2311.097552023arXiv preprint</p>
<p>Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava, arXiv:2305.11186Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable prompt. 2023arXiv preprint</p>
<p>Lu Yin, Gen Li, Meng Fang, Li Shen, Tianjin Huang, Zhangyang Wang, Vlado Menkovski, Xiaolong Ma, Mykola Pechenizkiy, Shiwei Liu, arXiv:2305.19454Dynamic sparsity is channel-level sparsity learner. 2023aarXiv preprint</p>
<p>Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, Shiwei Liu, arXiv:2310.05175Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity. 2023barXiv preprint</p>
<p>Dynamic sparse no training: Training-free fine-tuning for sparse llms. Yuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, Rongrong Ji, arXiv:2310.089152023arXiv preprint</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, International conference on machine learning. PMLR2021</p>
<p>Conditional prompt learning for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022a</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102022barXiv preprint</p>
<p>Perp: Rethinking the pruneretrain paradigm in the era of llms. Max Zimmer, Megi Andoni, Christoph Spiegel, Sebastian Pokutta, arXiv:2312.152302023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
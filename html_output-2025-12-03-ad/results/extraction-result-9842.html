<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9842 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9842</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9842</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-271334330</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.16148v1.pdf" target="_blank">CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support</a></p>
                <p><strong>Paper Abstract:</strong> Literature review requires researchers to synthesize a large amount of information and is increasingly challenging as the scientific literature expands. In this work, we investigate the potential of LLMs for producing hierarchical organizations of scientific studies to assist researchers with literature review. We define hierarchical organizations as tree structures where nodes refer to topical categories and every node is linked to the studies assigned to that category. Our naive LLM-based pipeline for hierarchy generation from a set of studies produces promising yet imperfect hierarchies, motivating us to collect CHIME, an expert-curated dataset for this task focused on biomedicine. Given the challenging and time-consuming nature of building hierarchies from scratch, we use a human-in-the-loop process in which experts correct errors (both links between categories and study assignment) in LLM-generated hierarchies. CHIME contains 2,174 LLM-generated hierarchies covering 472 topics, and expert-corrected hierarchies for a subset of 100 topics. Expert corrections allow us to quantify LLM performance, and we find that while they are quite good at generating and organizing categories, their assignment of studies to categories could be improved. We attempt to train a corrector model with human feedback which improves study assignment by 12.6 F1 points. We release our dataset and models to encourage research on developing better assistive tools for literature review.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9842.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9842.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CHIME pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end human-in-the-loop system that uses LLMs to compress study abstracts into concise claims, propose multiple hierarchical organizations of those claims, and (optionally) correct those hierarchies automatically or via domain experts to support literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CHIME LLM-based hierarchy generation + corrector pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multi-stage pipeline: (1) pre-generation compresses study abstracts into atomic claims (GPT-3.5), extracts frequent entities (SCISPACY), and verifies claims with a fine-tuned DeBERTa-v3 NLI model; (2) a hierarchy proposal module (Claude-2 preferred) generates root categories and completes hierarchical trees linking categories to claims (up to 5 alternative hierarchies per topic); (3) correction stage uses human-in-the-loop annotation (three subtasks) and optionally automated correctors (Flan-T5 fine-tuned; zero-shot CoT with GPT-3.5/GPT-4 variants) to fix parent-child links, sibling coherence, and claim-category assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Sets of related studies drawn from the Cochrane Database of Systematic Reviews filtered to reviews with 15–50 included studies; each set corresponds to a single research topic and is treated as the document collection to be organized. The dataset produced 2,174 LLM-generated hierarchies across 472 research topics and 320 expert-corrected hierarchies for 100 topics; average studies per topic ≈ 24.7.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Each instance is a Cochrane systematic-review topic (title + included study abstracts). The pipeline is run per-topic (a set of related studies) rather than with free-form topical queries.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Decomposed prompting pipeline: (a) generate concise atomic claims from each study abstract using GPT-3.5 (June 2023) ; (b) extract frequent entities via SCISPACY to bias category generation; (c) prompt an LLM (Claude-2 chosen qualitatively) to (i) propose top-level root categories and (ii) output a full hierarchical tree (arbitrary depth) assigning claims/studies to category nodes; (d) generate up to five alternative hierarchies per topic; (e) correct hierarchies via a three-step human annotation protocol (parent-child hypernym/hyponym check, sibling-group coherence, and exhaustive claim-category relevance labeling), and (f) optionally apply automated correctors (Flan-T5 fine-tuned on the collected corrections and/or zero-shot chain-of-thought with GPT-3.5/GPT-4 variants) to fix claim assignments and sibling coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Hierarchical organization (tree) of topical categories with each node linked to the claims/studies assigned to that category; multiple alternative hierarchies per topic.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Example (illustrative, as used in paper): root categories such as 'exercise modalities' and 'cancer types'; under 'exercise modalities' children like 'aerobic', 'resistance', 'walking' with lists of study IDs (e.g., Studies: 1,3 under S1). Multiple hierarchies present different semantic slicing of the same claim set.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human expert annotation used as gold-standard corrections across three subtasks (parent-child link validity, sibling-group coherence, and exhaustive claim-category relevance). Automated model evaluations compare model predictions to expert labels (precision/recall/F1) and task-specific statistics (e.g., parent-child link error counts). NLI verification of generated claims uses a fine-tuned DeBERTa-v3 model plus a small human qualitative check.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Parent-child category linking by the pipeline was nearly perfect (1 incorrect out of 1,635 assessed links). Sibling-group coherence: 77% of sibling groups were labeled coherent by experts. Claim categorization from the naive LLM pipeline showed precision ≈ 0.71 and recall ≈ 0.53 (aggregated), corresponding to an overall assignment F1 reported around 61.5% in the paper. Fine-tuning a Flan-T5 corrector improved study/claim assignment by ~12.6 F1 points (and Flan-T5-large improved recall by 15.9 points on claim categorization).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Generates multiple diverse hierarchical organizations per topic; high accuracy at generating and linking categories (parent-child links); produces PICO-aligned roots for many biomedical topics (many hierarchies had PICO-focused roots); compresses abstracts into concise claims reducing input size; enables human-in-the-loop correction workflows and trains automated correctors from those corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Focused on biomedicine (single-domain) so generalization to other domains is untested; relies on curated sets of topic-relevant studies (assumes input studies are relevant); LLM inference latency (e.g., Claude-2 long runtimes) challenges real-time deployment; claim-assignment recall was lower and required costly expert correction; automating sibling-coherence correction proved difficult; potential sensitivity to noisy / out-of-domain inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Common hierarchical errors include siblings at inconsistent granularities (e.g., 'walking' listed as sibling to broader 'aerobic') and mis-typed category assignments (e.g., 'Metastasis' or 'Recurrence' incorrectly grouped under 'type of cancer'); claim categorization showed low recall (missed assignments), which is hard for humans to correct; sibling-group incoherence and subtle focus differences among sibling categories caused errors; one explicit incorrect parent-child example: 'Coffee consumption' placed under 'Tea consumption and cancer risk'.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9842.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9842.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (claim generation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 (June 2023 version) used for claim extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative large language model used to compress study abstracts into concise, atomic claim statements that summarize findings for downstream hierarchical organization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (June 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following generative LLM used with prompting to extract atomic claims (one-sentence findings) from abstracts; authors qualitatively judged it clearer and more concise than Claude-2 for claim generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Study abstracts from Cochrane systematic reviews (per-topic sets filtered to 15–50 included studies); claims generated per-study for all topics in CHIME.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Per-review/topic claim extraction: prompt takes a single study's title and abstract and produces one or more atomic claim sentences relevant to the review topic.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Single-document summarization into atomic, verifiable claim sentences via direct LLM prompting (see Appendix A prompts). Claims were then aggregated across studies to be the input units for hierarchy construction.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Concise atomic claim statements (one-sentence) representing study findings, used as inputs to hierarchy proposal.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Prompt output format: a single atomic sentence that summarizes new/null findings from the abstract (e.g., 'Drug X does not improve outcome Y in population Z').</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated NLI verification using a fine-tuned DeBERTa-v3 NLI model to check entailment between abstract and generated claim, and a human qualitative check on a sample of 100 abstract-claim pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>NLI check showed 98.1% of generated claims were entailed by their abstracts. Human qualitative check found 47/50 entailed claims were correct and identified many false negatives from the NLI model, supporting high factuality of GPT-3.5-generated claims.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Produces concise, clear claims reducing downstream input length; high faithfulness to abstracts per NLI and human checks; improves tractability of hierarchical organization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>May suffer occasional hallucination (though low in this dataset); relies on quality of abstracts (single-source verification), and compression may omit nuance.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Some claims were marked non-entailed by the NLI model (many of which were false negatives), and rare mismatches between claim phrasing and abstract evidence could cause downstream category misassignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9842.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9842.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-2 (hierarchy proposal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude-2 used for hierarchy proposal</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conversational LLM used by the authors to generate hierarchical organizations (root categories and full tree completions) from claim sets, preferred for producing deeper hierarchies in the qualitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large instruction-following conversational LLM used with prompts that ask for (i) root category proposals and (ii) complete hierarchical trees assigning claims to categories; qualitatively produced deeper hierarchies (>1 depth) more often than GPT-4 in the authors' evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Sets of claims derived from Cochrane study abstracts and the 20 most frequent entities per topic used as keywords to bias category generation; per-topic inputs averaged ~24.7 studies (compressed into claims).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Per-topic hierarchy generation: produce candidate root categories and then a complete labeled tree organizing claims/studies under those categories. Up to five hierarchies generated per topic to offer alternative organizations.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Single-prompt two-step hierarchy proposal (within the same prompt): (1) propose candidate top-level root categories given the set of claims and frequent entities, (2) expand to a full hierarchy with sub-categories and map claims/studies to nodes. Multiple runs produce alternative hierarchical slices of the same corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Hierarchical trees (multi-level) mapping topical categories to claims/studies.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>A hierarchy whose root might be 'exercise modalities' with child nodes 'aerobic', 'resistance', 'walking' etc., each linked to lists of claim or study identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Expert human correction and assessment across three subtasks (parent-child links, sibling coherence, and claim categorization); qualitative comparison to other LLMs (GPT-3.5, GPT-4) on hierarchy depth and form.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Qualitatively Claude-2 produced hierarchies with depth >1 in 90% of sampled topics (compared to GPT-4 which produced shallow hierarchies more often). Downstream evaluation of the pipeline that used Claude-2 showed near-perfect parent-child linking accuracy and 77% sibling coherence overall, though claim categorization recall was limited (see CHIME pipeline results).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Generates deeper and semantically diverse hierarchical organizations; can ignore injected irrelevant claims in a small noise experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Long inference times in practice (sometimes the entire generation can take up to a minute), affecting deployability; not perfect on claim assignment which still required correction.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Examples include coarse-granularity sibling grouping errors (e.g., more-specific leaves like 'walking' appearing as siblings to broader categories) and occasional incorrect assignment of non-hypernym children.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9842.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9842.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5 corrector</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (instruction-finetuned T5) fine-tuned as an automated corrector</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller instruction-finetuned encoder-decoder LM (Flan-T5) fine-tuned on expert corrections from CHIME to automatically correct claim-to-category assignments (and attempted sibling-coherence correction).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5 (base and large fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Finetuned Flan-T5-base and Flan-T5-large models trained on collected human corrections for two subtasks: sibling-coherence detection and claim categorization (claim→category relevance). Training hyperparameters: task 1 LR 1e-3 up to 5 epochs; task 3 LR 3e-4 up to 2 epochs; models trained on NVIDIA RTX A6000 (details in Appendix B).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Training data comprised expert corrections collected from the CHIME human-in-the-loop process (the gold subset: 320 corrected hierarchies across 100 topics). For claim categorization, the annotation set contained 50,723 claim-category pairs labeled by experts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Learns to predict claim-to-category relevance and to flag incoherent sibling groups, conditioned on the hierarchy structure and claim text (trained per claim-category pair).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised fine-tuning on human-labeled correction data. For claim categorization, predictions are generated for every claim-category pair and then aggregated along the path from root to target category using logical AND to obtain the final category assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Binary corrections per claim-category pair (assign / not assign) and flags for incoherent sibling groups; applied to LLM-generated hierarchies to update claim assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Applied to a claim-category pair, outputs 'belongs' or 'does not belong'; overall it flips about 24.7% of labels relative to the naive LLM pipeline on held-out test, with 63.5% of those flips judged correct when compared to experts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared model predictions to expert labels on in-domain (ID) and out-of-domain (OOD) test splits; reported precision/recall/F1 for claim categorization and sibling-coherence detection.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Fine-tuned Flan-T5 models improved claim assignment substantially: Flan-T5-large improved recall by 15.9 points and improved overall claim assignment by ~12.6 F1 points over the naive LLM pipeline. For sibling coherence, finetuned Flan-T5 struggled (best F1 ≈ 33.3%) due to limited/imbalanced training data, while LLMs did better (LLMs F1 ≈ 51.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Significantly improved automated correction of claim-to-category assignments (notably recall), enabling partial automation of the most labor-intensive correction step; fast to apply compared to large LLM inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance for sibling-coherence detection was low, likely because of small/imbalanced data; finetuning requires quality expert-labeled corrections and may not generalize beyond the biomedical topics in CHIME.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Corrector still makes incorrect flips (≈36.5% of label flips were incorrect in the application to unlabeled hierarchies) and cannot fully replace expert judgement for complex sibling-coherence decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9842.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9842.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI GPT-3.5/GPT-4 Turbo (zero-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5-Turbo and GPT-4-Turbo used with zero-shot chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large OpenAI models tested in zero-shot chain-of-thought (CoT) prompting mode as automated correctors for sibling coherence and claim categorization tasks, used for comparison against fine-tuned Flan-T5 correctors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613 and gpt-4-1106-preview (GPT-3.5-Turbo, GPT-4-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used in zero-shot CoT prompting experiments to attempt complex reasoning required for correction subtasks without task-specific finetuning; prompts include step-by-step reasoning instructions (Appendix B).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same CHIME hierarchical inputs (claims and candidate categories) as used for Flan-T5 experiments; tested on the same ID and OOD splits of the expert-corrected subset.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Given a claim and full hierarchy (or a sibling group), the models are prompted zero-shot (with CoT) to decide claim-category relevance or sibling-group coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Zero-shot chain-of-thought prompting (no fine-tuning) to produce stepwise reasoning and final binary labels for correction subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Binary labels for claim→category relevance and for sibling-group coherence, with chain-of-thought justifications in prompt responses.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>A CoT response that enumerates reasoning steps and ends with 'The claim belongs to the category' or 'The claim does NOT belong to the category'.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared model outputs to expert labels on the same test splits as fine-tuned models; reported precision/recall/F1.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>For sibling coherence, zero-shot LLMs outperformed finetuned Flan-T5 (LLMs F1 ≈ 51.5% vs Flan-T5 best ≈ 33.3%), though precision remained limited (e.g., GPT-4-Turbo precision ≈ 46.7% on sibling-coherence detection). For claim categorization, GPT-4-Turbo achieved the best recall among models but lower precision, making its raw predictions less reliable than the fine-tuned Flan-T5 on aggregate metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Can perform correction reasoning without supervised finetuning; better at some complex coherence judgments than small finetuned models given limited labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lower precision in some tasks makes outputs less reliable; zero-shot CoT is sensitive to prompt design and can be inconsistent; cost and latency considerations may limit practical use.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>False positives in sibling-coherence detection and lower precision in claim categorization leading to noisy automated corrections; sensitive to ambiguous sibling definitions and fine-grained category distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Topicgpt: A prompt-based topic modeling framework. <em>(Rating: 2)</em></li>
                <li>Large language models enable few-shot clustering. <em>(Rating: 2)</em></li>
                <li>Hierarchical catalogue generation for literature review: A benchmark. <em>(Rating: 2)</em></li>
                <li>Generating a structured summary of numerous academic papers: Dataset and method. <em>(Rating: 1)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9842",
    "paper_id": "paper-271334330",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "CHIME pipeline",
            "name_full": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies",
            "brief_description": "An end-to-end human-in-the-loop system that uses LLMs to compress study abstracts into concise claims, propose multiple hierarchical organizations of those claims, and (optionally) correct those hierarchies automatically or via domain experts to support literature review.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CHIME LLM-based hierarchy generation + corrector pipeline",
            "model_description": "A multi-stage pipeline: (1) pre-generation compresses study abstracts into atomic claims (GPT-3.5), extracts frequent entities (SCISPACY), and verifies claims with a fine-tuned DeBERTa-v3 NLI model; (2) a hierarchy proposal module (Claude-2 preferred) generates root categories and completes hierarchical trees linking categories to claims (up to 5 alternative hierarchies per topic); (3) correction stage uses human-in-the-loop annotation (three subtasks) and optionally automated correctors (Flan-T5 fine-tuned; zero-shot CoT with GPT-3.5/GPT-4 variants) to fix parent-child links, sibling coherence, and claim-category assignments.",
            "model_size": null,
            "input_corpus_description": "Sets of related studies drawn from the Cochrane Database of Systematic Reviews filtered to reviews with 15–50 included studies; each set corresponds to a single research topic and is treated as the document collection to be organized. The dataset produced 2,174 LLM-generated hierarchies across 472 research topics and 320 expert-corrected hierarchies for 100 topics; average studies per topic ≈ 24.7.",
            "input_corpus_size": null,
            "topic_query_description": "Each instance is a Cochrane systematic-review topic (title + included study abstracts). The pipeline is run per-topic (a set of related studies) rather than with free-form topical queries.",
            "distillation_method": "Decomposed prompting pipeline: (a) generate concise atomic claims from each study abstract using GPT-3.5 (June 2023) ; (b) extract frequent entities via SCISPACY to bias category generation; (c) prompt an LLM (Claude-2 chosen qualitatively) to (i) propose top-level root categories and (ii) output a full hierarchical tree (arbitrary depth) assigning claims/studies to category nodes; (d) generate up to five alternative hierarchies per topic; (e) correct hierarchies via a three-step human annotation protocol (parent-child hypernym/hyponym check, sibling-group coherence, and exhaustive claim-category relevance labeling), and (f) optionally apply automated correctors (Flan-T5 fine-tuned on the collected corrections and/or zero-shot chain-of-thought with GPT-3.5/GPT-4 variants) to fix claim assignments and sibling coherence.",
            "output_type": "Hierarchical organization (tree) of topical categories with each node linked to the claims/studies assigned to that category; multiple alternative hierarchies per topic.",
            "output_example": "Example (illustrative, as used in paper): root categories such as 'exercise modalities' and 'cancer types'; under 'exercise modalities' children like 'aerobic', 'resistance', 'walking' with lists of study IDs (e.g., Studies: 1,3 under S1). Multiple hierarchies present different semantic slicing of the same claim set.",
            "evaluation_method": "Human expert annotation used as gold-standard corrections across three subtasks (parent-child link validity, sibling-group coherence, and exhaustive claim-category relevance). Automated model evaluations compare model predictions to expert labels (precision/recall/F1) and task-specific statistics (e.g., parent-child link error counts). NLI verification of generated claims uses a fine-tuned DeBERTa-v3 model plus a small human qualitative check.",
            "evaluation_results": "Parent-child category linking by the pipeline was nearly perfect (1 incorrect out of 1,635 assessed links). Sibling-group coherence: 77% of sibling groups were labeled coherent by experts. Claim categorization from the naive LLM pipeline showed precision ≈ 0.71 and recall ≈ 0.53 (aggregated), corresponding to an overall assignment F1 reported around 61.5% in the paper. Fine-tuning a Flan-T5 corrector improved study/claim assignment by ~12.6 F1 points (and Flan-T5-large improved recall by 15.9 points on claim categorization).",
            "strengths": "Generates multiple diverse hierarchical organizations per topic; high accuracy at generating and linking categories (parent-child links); produces PICO-aligned roots for many biomedical topics (many hierarchies had PICO-focused roots); compresses abstracts into concise claims reducing input size; enables human-in-the-loop correction workflows and trains automated correctors from those corrections.",
            "limitations": "Focused on biomedicine (single-domain) so generalization to other domains is untested; relies on curated sets of topic-relevant studies (assumes input studies are relevant); LLM inference latency (e.g., Claude-2 long runtimes) challenges real-time deployment; claim-assignment recall was lower and required costly expert correction; automating sibling-coherence correction proved difficult; potential sensitivity to noisy / out-of-domain inputs.",
            "failure_cases": "Common hierarchical errors include siblings at inconsistent granularities (e.g., 'walking' listed as sibling to broader 'aerobic') and mis-typed category assignments (e.g., 'Metastasis' or 'Recurrence' incorrectly grouped under 'type of cancer'); claim categorization showed low recall (missed assignments), which is hard for humans to correct; sibling-group incoherence and subtle focus differences among sibling categories caused errors; one explicit incorrect parent-child example: 'Coffee consumption' placed under 'Tea consumption and cancer risk'.",
            "uuid": "e9842.0",
            "source_info": {
                "paper_title": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-3.5 (claim generation)",
            "name_full": "OpenAI GPT-3.5 (June 2023 version) used for claim extraction",
            "brief_description": "A generative large language model used to compress study abstracts into concise, atomic claim statements that summarize findings for downstream hierarchical organization.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (June 2023)",
            "model_description": "Instruction-following generative LLM used with prompting to extract atomic claims (one-sentence findings) from abstracts; authors qualitatively judged it clearer and more concise than Claude-2 for claim generation.",
            "model_size": null,
            "input_corpus_description": "Study abstracts from Cochrane systematic reviews (per-topic sets filtered to 15–50 included studies); claims generated per-study for all topics in CHIME.",
            "input_corpus_size": null,
            "topic_query_description": "Per-review/topic claim extraction: prompt takes a single study's title and abstract and produces one or more atomic claim sentences relevant to the review topic.",
            "distillation_method": "Single-document summarization into atomic, verifiable claim sentences via direct LLM prompting (see Appendix A prompts). Claims were then aggregated across studies to be the input units for hierarchy construction.",
            "output_type": "Concise atomic claim statements (one-sentence) representing study findings, used as inputs to hierarchy proposal.",
            "output_example": "Prompt output format: a single atomic sentence that summarizes new/null findings from the abstract (e.g., 'Drug X does not improve outcome Y in population Z').",
            "evaluation_method": "Automated NLI verification using a fine-tuned DeBERTa-v3 NLI model to check entailment between abstract and generated claim, and a human qualitative check on a sample of 100 abstract-claim pairs.",
            "evaluation_results": "NLI check showed 98.1% of generated claims were entailed by their abstracts. Human qualitative check found 47/50 entailed claims were correct and identified many false negatives from the NLI model, supporting high factuality of GPT-3.5-generated claims.",
            "strengths": "Produces concise, clear claims reducing downstream input length; high faithfulness to abstracts per NLI and human checks; improves tractability of hierarchical organization.",
            "limitations": "May suffer occasional hallucination (though low in this dataset); relies on quality of abstracts (single-source verification), and compression may omit nuance.",
            "failure_cases": "Some claims were marked non-entailed by the NLI model (many of which were false negatives), and rare mismatches between claim phrasing and abstract evidence could cause downstream category misassignment.",
            "uuid": "e9842.1",
            "source_info": {
                "paper_title": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Claude-2 (hierarchy proposal)",
            "name_full": "Anthropic Claude-2 used for hierarchy proposal",
            "brief_description": "A conversational LLM used by the authors to generate hierarchical organizations (root categories and full tree completions) from claim sets, preferred for producing deeper hierarchies in the qualitative comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-2",
            "model_description": "A large instruction-following conversational LLM used with prompts that ask for (i) root category proposals and (ii) complete hierarchical trees assigning claims to categories; qualitatively produced deeper hierarchies (&gt;1 depth) more often than GPT-4 in the authors' evaluation.",
            "model_size": null,
            "input_corpus_description": "Sets of claims derived from Cochrane study abstracts and the 20 most frequent entities per topic used as keywords to bias category generation; per-topic inputs averaged ~24.7 studies (compressed into claims).",
            "input_corpus_size": null,
            "topic_query_description": "Per-topic hierarchy generation: produce candidate root categories and then a complete labeled tree organizing claims/studies under those categories. Up to five hierarchies generated per topic to offer alternative organizations.",
            "distillation_method": "Single-prompt two-step hierarchy proposal (within the same prompt): (1) propose candidate top-level root categories given the set of claims and frequent entities, (2) expand to a full hierarchy with sub-categories and map claims/studies to nodes. Multiple runs produce alternative hierarchical slices of the same corpus.",
            "output_type": "Hierarchical trees (multi-level) mapping topical categories to claims/studies.",
            "output_example": "A hierarchy whose root might be 'exercise modalities' with child nodes 'aerobic', 'resistance', 'walking' etc., each linked to lists of claim or study identifiers.",
            "evaluation_method": "Expert human correction and assessment across three subtasks (parent-child links, sibling coherence, and claim categorization); qualitative comparison to other LLMs (GPT-3.5, GPT-4) on hierarchy depth and form.",
            "evaluation_results": "Qualitatively Claude-2 produced hierarchies with depth &gt;1 in 90% of sampled topics (compared to GPT-4 which produced shallow hierarchies more often). Downstream evaluation of the pipeline that used Claude-2 showed near-perfect parent-child linking accuracy and 77% sibling coherence overall, though claim categorization recall was limited (see CHIME pipeline results).",
            "strengths": "Generates deeper and semantically diverse hierarchical organizations; can ignore injected irrelevant claims in a small noise experiment.",
            "limitations": "Long inference times in practice (sometimes the entire generation can take up to a minute), affecting deployability; not perfect on claim assignment which still required correction.",
            "failure_cases": "Examples include coarse-granularity sibling grouping errors (e.g., more-specific leaves like 'walking' appearing as siblings to broader categories) and occasional incorrect assignment of non-hypernym children.",
            "uuid": "e9842.2",
            "source_info": {
                "paper_title": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Flan-T5 corrector",
            "name_full": "Flan-T5 (instruction-finetuned T5) fine-tuned as an automated corrector",
            "brief_description": "A smaller instruction-finetuned encoder-decoder LM (Flan-T5) fine-tuned on expert corrections from CHIME to automatically correct claim-to-category assignments (and attempted sibling-coherence correction).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-T5 (base and large fine-tuned)",
            "model_description": "Finetuned Flan-T5-base and Flan-T5-large models trained on collected human corrections for two subtasks: sibling-coherence detection and claim categorization (claim→category relevance). Training hyperparameters: task 1 LR 1e-3 up to 5 epochs; task 3 LR 3e-4 up to 2 epochs; models trained on NVIDIA RTX A6000 (details in Appendix B).",
            "model_size": null,
            "input_corpus_description": "Training data comprised expert corrections collected from the CHIME human-in-the-loop process (the gold subset: 320 corrected hierarchies across 100 topics). For claim categorization, the annotation set contained 50,723 claim-category pairs labeled by experts.",
            "input_corpus_size": null,
            "topic_query_description": "Learns to predict claim-to-category relevance and to flag incoherent sibling groups, conditioned on the hierarchy structure and claim text (trained per claim-category pair).",
            "distillation_method": "Supervised fine-tuning on human-labeled correction data. For claim categorization, predictions are generated for every claim-category pair and then aggregated along the path from root to target category using logical AND to obtain the final category assignment.",
            "output_type": "Binary corrections per claim-category pair (assign / not assign) and flags for incoherent sibling groups; applied to LLM-generated hierarchies to update claim assignments.",
            "output_example": "Applied to a claim-category pair, outputs 'belongs' or 'does not belong'; overall it flips about 24.7% of labels relative to the naive LLM pipeline on held-out test, with 63.5% of those flips judged correct when compared to experts.",
            "evaluation_method": "Compared model predictions to expert labels on in-domain (ID) and out-of-domain (OOD) test splits; reported precision/recall/F1 for claim categorization and sibling-coherence detection.",
            "evaluation_results": "Fine-tuned Flan-T5 models improved claim assignment substantially: Flan-T5-large improved recall by 15.9 points and improved overall claim assignment by ~12.6 F1 points over the naive LLM pipeline. For sibling coherence, finetuned Flan-T5 struggled (best F1 ≈ 33.3%) due to limited/imbalanced training data, while LLMs did better (LLMs F1 ≈ 51.5%).",
            "strengths": "Significantly improved automated correction of claim-to-category assignments (notably recall), enabling partial automation of the most labor-intensive correction step; fast to apply compared to large LLM inference.",
            "limitations": "Performance for sibling-coherence detection was low, likely because of small/imbalanced data; finetuning requires quality expert-labeled corrections and may not generalize beyond the biomedical topics in CHIME.",
            "failure_cases": "Corrector still makes incorrect flips (≈36.5% of label flips were incorrect in the application to unlabeled hierarchies) and cannot fully replace expert judgement for complex sibling-coherence decisions.",
            "uuid": "e9842.3",
            "source_info": {
                "paper_title": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "OpenAI GPT-3.5/GPT-4 Turbo (zero-shot CoT)",
            "name_full": "OpenAI GPT-3.5-Turbo and GPT-4-Turbo used with zero-shot chain-of-thought prompting",
            "brief_description": "Large OpenAI models tested in zero-shot chain-of-thought (CoT) prompting mode as automated correctors for sibling coherence and claim categorization tasks, used for comparison against fine-tuned Flan-T5 correctors.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613 and gpt-4-1106-preview (GPT-3.5-Turbo, GPT-4-Turbo)",
            "model_description": "Used in zero-shot CoT prompting experiments to attempt complex reasoning required for correction subtasks without task-specific finetuning; prompts include step-by-step reasoning instructions (Appendix B).",
            "model_size": null,
            "input_corpus_description": "Same CHIME hierarchical inputs (claims and candidate categories) as used for Flan-T5 experiments; tested on the same ID and OOD splits of the expert-corrected subset.",
            "input_corpus_size": null,
            "topic_query_description": "Given a claim and full hierarchy (or a sibling group), the models are prompted zero-shot (with CoT) to decide claim-category relevance or sibling-group coherence.",
            "distillation_method": "Zero-shot chain-of-thought prompting (no fine-tuning) to produce stepwise reasoning and final binary labels for correction subtasks.",
            "output_type": "Binary labels for claim→category relevance and for sibling-group coherence, with chain-of-thought justifications in prompt responses.",
            "output_example": "A CoT response that enumerates reasoning steps and ends with 'The claim belongs to the category' or 'The claim does NOT belong to the category'.",
            "evaluation_method": "Compared model outputs to expert labels on the same test splits as fine-tuned models; reported precision/recall/F1.",
            "evaluation_results": "For sibling coherence, zero-shot LLMs outperformed finetuned Flan-T5 (LLMs F1 ≈ 51.5% vs Flan-T5 best ≈ 33.3%), though precision remained limited (e.g., GPT-4-Turbo precision ≈ 46.7% on sibling-coherence detection). For claim categorization, GPT-4-Turbo achieved the best recall among models but lower precision, making its raw predictions less reliable than the fine-tuned Flan-T5 on aggregate metrics.",
            "strengths": "Can perform correction reasoning without supervised finetuning; better at some complex coherence judgments than small finetuned models given limited labeled data.",
            "limitations": "Lower precision in some tasks makes outputs less reliable; zero-shot CoT is sensitive to prompt design and can be inconsistent; cost and latency considerations may limit practical use.",
            "failure_cases": "False positives in sibling-coherence detection and lower precision in claim categorization leading to noisy automated corrections; sensitive to ambiguous sibling definitions and fine-grained category distinctions.",
            "uuid": "e9842.4",
            "source_info": {
                "paper_title": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Topicgpt: A prompt-based topic modeling framework.",
            "rating": 2
        },
        {
            "paper_title": "Large language models enable few-shot clustering.",
            "rating": 2
        },
        {
            "paper_title": "Hierarchical catalogue generation for literature review: A benchmark.",
            "rating": 2
        },
        {
            "paper_title": "Generating a structured summary of numerous academic papers: Dataset and method.",
            "rating": 1
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models.",
            "rating": 1
        }
    ],
    "cost": 0.0169775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support</p>
<p>Chao-Chun Hsu 
University of Chicago</p>
<p>Erin Bransom 
Allen Institute for AI</p>
<p>Jenna Sparks 
Allen Institute for AI</p>
<p>Bailey Kuehl 
Allen Institute for AI</p>
<p>Chenhao Tan 
University of Chicago</p>
<p>David Wadden 
Allen Institute for AI</p>
<p>Lucy Lu Wang 
Allen Institute for AI</p>
<p>University of Washington</p>
<p>Aakanksha Naik aakankshan@allenai.org 
Allen Institute for AI</p>
<p>CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support
F40DE3CCD739F2C9CB447336307EC00CP1Exercise modalities S1Aerobic S2Resistance S3Combined S4Walking S5Weight training
Literature review requires researchers to synthesize a large amount of information and is increasingly challenging as the scientific literature expands.In this work, we investigate the potential of LLMs for producing hierarchical organizations of scientific studies to assist researchers with literature review.We define hierarchical organizations as tree structures where nodes refer to topical categories and every node is linked to the studies assigned to that category.Our naive LLM-based pipeline for hierarchy generation from a set of studies produces promising yet imperfect hierarchies, motivating us to collect CHIME, an expert-curated dataset for this task focused on biomedicine.Given the challenging and time-consuming nature of building hierarchies from scratch, we use a human-in-the-loop process in which experts correct errors (both links between categories and study assignment) in LLM-generated hierarchies.CHIME contains 2,174 LLM-generated hierarchies covering 472 topics, and expert-corrected hierarchies for a subset of 100 topics.Expert corrections allow us to quantify LLM performance, and we find that while they are quite good at generating and organizing categories, their assignment of studies to categories could be improved.We attempt to train a corrector model with human feedback which improves study assignment by 12.6 F1 points.We release our dataset and models to encourage research on developing better assistive tools for literature review. 1 1 The CHIME dataset and models are available at https: //github.com/allenai/chime.</p>
<p>Introduction</p>
<p>Literature review, the process by which researchers synthesize many related scientific studies into a higher-level organization, is valuable but extremely Studies: 1,3</p>
<p>Studies: 1,3</p>
<p>Studies: 1, 8, 9</p>
<p>Studies: 4, 5, 7</p>
<p>Studies: 32, 39</p>
<p>Studies: 5, 6, 9</p>
<p>Studies: 1,4</p>
<p>Figure 1: Given a set of related studies on a topic, we use LLMs to identify top-level categories focusing on different views of the data (such as P1 and P2), generate multiple hierarchical organizations, and assign studies to different categories.However, these categories and study assignments can contain errors.As illustrated in the figure, the categories Walking and Weight training are not coherent with their siblings (S1 − S3) in hierarchy 1 since they are more specific, and the categories Metastasis and Recurrence are incorrectly assigned to the parent category in hierarchy 2 since they are not types of cancer.</p>
<p>time-consuming.For instance, in medicine, completing a review from registration to publication takes 67 weeks on average (Borah et al., 2017) and given the rapid pace of scholarly publication, reviews tend to go out-of-date quickly (Shojania et al., 2007).This has prompted development of tools for efficient literature review (Altmami and Menai, 2022).Most tools have focused on au-tomating review generation, treating it as a multidocument summarization task (Mohammad et al., 2009;Jha et al., 2015;Wallace et al., 2020;DeYoung et al., 2021;Liu et al., 2022), sometimes using intermediate structures such as hierarchies/outlines to better scaffold generation (Zhu et al., 2023), with limited success.However, recent work on assessing the utility of NLP tools like LLMs for systematic review reveals that domain experts prefer literature review tools to be assistive instead of automatic (Yun et al., 2023).</p>
<p>Motivated by this finding, we take a different approach and focus on the task of generating hierarchical organizations of scientific studies to assist literature review.As shown in Figure 1, a hierarchical organization is a tree structure in which nodes represent topical categories and every node is linked to a list of studies assigned to that category.Inspired by the adoption of LLMs for information organization uses such as clustering (Viswanathan et al., 2023) and topic modeling (Pham et al., 2023), we investigate the potential of generating hierarchies with a naive LLM-based approach, and observe that models produce promising yet imperfect hierarchies out-of-the-box.</p>
<p>To further assess and improve LLM performance, we collect CHIME (Constructing HIerarchies of bioMedical Evidence), an expert-curated dataset for hierarchy generation.Since building such hierarchies from scratch is very challenging and timeconsuming, we develop a human-in-the-loop protocol in which experts correct errors in preliminary LLM-generated hierarchies.During a three-step error correction process, experts assess the correctness of both links between categories as well as assignment of studies to categories, as demonstrated in Figure 1.Our final dataset consists of two subsets: (i) a set of 472 research topics with up to five LLM-generated hierarchies per topic (2,174 total hierarchies), and (ii) a subset of 100 research topics sampled from the previous set, with 320 expert-corrected hierarchies.</p>
<p>Expert-corrected hierarchies allow us to better quantify LLM performance on hierarchy generation.We observe that LLMs are already quite good at generating and organizing categories, achieving near-perfect performance on parent-child category linking and a precision of 77.3% on producing coherent groups of sibling categories.However, their performance on assigning studies to relevant categories (61.5% F1) leaves room for improvement.</p>
<p>We study the potential of using CHIME to train "corrector" agents which can provide feedback to our LLM-based pipeline to improve hierarchy quality.Our results show that finetuning a FLAN-T5based corrector and applying it to LLM-generated hierarchies improves study assignment by 12.6 F1 points.We release our dataset containing both LLM-generated and expert-corrected hierarchies, as well as our LLM-based hierarchy generation and correction pipelines, to encourage further research on better assistive tools for literature review.</p>
<p>In summary, our key contributions include: • We develop an LLM-based pipeline to organize a collection of papers on a research topic into a labeled, human-navigable concept hierarchy.• We release CHIME, a dataset of 2174 hierarchies constructed using our pipeline, including a "gold" subset of 320 hierarchies checked and corrected by human experts.• We train corrector models using CHIME to automatically fix errors in LLM-generated hierarchies, improving accuracy of study categorization by 12.6 F1 points.</p>
<p>Generating Preliminary Hierarchies using LLMs</p>
<p>The first phase of our dataset creation process focuses on using LLMs to generate preliminary hierarchies from a set of related studies, which can then be corrected by experts.We describe our process for collecting sets of related studies and our LLM-based hierarchy generation pipeline.</p>
<p>Sourcing Related Studies</p>
<p>We leverage the Cochrane Database of Systematic Reviews2 to obtain sets of related studies, since the systematic review writing process requires experts to extensively search for and curate studies relevant to review topics.We obtain all systematic reviews and the corresponding studies included in each review from the Cochrane website (Wallace et al., 2020).We then filter this set of systematic reviews to only retain those including at least 15 and no more than 50 corresponding studies.We discard reviews with very few studies since a hierarchical organization is unlikely to provide much utility, while reviews with more than 50 studies are discarded due to the inability of LLMs to effectively handle such long inputs (Liu et al., 2023)  (or sets), each including an average of 24.7 studies, which serve as input to our hierarchy generation pipeline.</p>
<p>Hierarchy Generation Pipeline</p>
<p>Prior work on using LLMs for complex tasks has shown that decomposing the task into a series of steps or sub-tasks often elicits more accurate responses (Kojima et al., 2022;Wei et al., 2022b;D'Arcy et al., 2024).Motivated by this, we decompose hierarchy generation from a set of related scientific studies into three sub-tasks: (i) compressing study findings into concise claims, (ii) initiating hierarchy generation by generating root categories, and (iii) completing hierarchy generation by producing remaining categories and organizing claims under them.Our hierarchy generation pipeline consists of a pre-generation module that tackles task (i) and a hierarchy proposal module that handles tasks (ii) and (iii) (see Figure 2)).Additionally, our pipeline can generate multiple (up to five) potential hierarchies per topic.We describe our pipeline module in further detail below and provide complete prompt details in Appendix A.</p>
<p>Pre-Generation Module</p>
<p>This module extracts relevant content from a set of studies to use as input for hierarchy proposal.Claim generation.We generate concise claim statements from a given scientific study to reduce the amount of information provided as input to subsequent LLM modules.Providing a study abstract as input, we prompt a LLM to generate claims de-scribing all findings discussed.We qualitatively examine the claim generation capabilities of two state-of-the-art LLMs: (i) GPT-3.5 (June 2023 version) and (ii) CLAUDE-2.Our assessment indicates that GPT-3.5 performs better in terms of clarity and conciseness; therefore we extract claim statements for all studies in our dataset using this model.Additionally, to assess whether generated claims contain hallucinated information, we run a fine-tuned DEBERTA-V3 NLI model (Laurer et al., 2024) on abstract-claim pairs.We observe that 98.1% of the generated claims are entailed by their corresponding study abstracts, indicating that claims are generally faithful to source abstracts. 3These sets of generated claims are provided as input to the hierarchy proposal module.</p>
<p>Frequent entity extraction.Based on preliminary exploration, we observe that simply prompting LLMs to generate hierarchies given a set of claims often produces hierarchy categories with low coverage over the claim set.Therefore, we extract frequently-occurring entities to provide as additional cues to bias category generation.We use SCISPACY (Neumann et al., 2019) to extract entities from all study abstracts, then aggregate and sort them by frequency.The 20 most frequent entities are used as additional keywords to bias generated categories towards having high coverage.</p>
<p>Hierarchy Proposal Module</p>
<p>The aim of this module is to generate final hierarchies in two steps within a single prompt: (i) generate possible categories that can form the root node of a hierarchy (i.e., categories that divide claims into various clusters), and then (ii) generate the complete hierarchy with claim organization.For instance, considering the example in Figure 1, step (i) would produce root categories "exercise modalities" and "cancer types" and step (ii) would produce all sub-categories (S1 − S5) and organize studies under them (e.g., assigning studies 1, 3, 5 under S1</p>
<p>Correcting Hierarchies via Human Feedback</p>
<p>The second phase of our dataset creation process involves correction of preliminary LLM-generated hierarchies via human feedback.Correcting these hierarchies is challenging because of two issues.</p>
<p>First, the volume of information present in generated hierarchies (links between categories, claimcategory links, etc.) makes correction very timeconsuming, especially in a single pass.Second, since categories and claims in a hierarchy are interlinked, corrections can have cascading effects (e.g., changing a category name can affect which claims should be categorized under it).These issues motivate us to decompose hierarchy correction into three sub-tasks, making the feedback process less tedious and time-consuming.Furthermore, each sub-task focuses on the correction of only one category of links to mitigate cascading effects.These three sub-tasks are: (i) assessing correctness of parent-child category links, (ii) assessing coherence of sibling category groups, and (iii) assessing claim categorization.</p>
<p>Assessing Parent-Child Category Links</p>
<p>In this sub-task, given all parent-child category links from a hierarchy (e.g., P 1 → S[1 − 5] in Figure 1), for each link, humans are prompted to determine whether the child is a valid sub-category of the parent.Annotators can label parent-child category links using one of the following labels: (i) parent and child categories have a hypernymhyponym relationship (e.g., exercise modalities → aerobic exercise), (ii) parent and child categories are not related by hypernymy but the child category provides a useful breakdown of the parent(e.g., aer-obic exercise → positive effects), and (iii) parent and child categories are unrelated (e.g., aerobic exercise → anaerobic exercise).Categories (i) and (ii) are positive labels indicating valid links, while category (iii) is a negative label capturing incorrect links in the existing hierarchy.</p>
<p>Assessing Coherence of Sibling Categories</p>
<p>For a hierarchical organization to be useful, in addition to validity of parent-child category links, all sibling categories (i.e., categories under the same parent, like S1 − S5 in Figure 1) should also be coherent.Therefore, in our second sub-task, given a parent and all its child categories, we ask annotators to determine whether these child categories form a coherent sibling group.Annotators can assign a positive or negative coherence label to each child category in the group.For example, given the parent category "type of cancer" and the set of child categories "liver cancer", "prostate cancer", "lung cancer", and "recurrence", the first three categories are assigned positive labels, while "recurrence" is assigned a negative label since it is not a type of cancer.All categories assigned a negative label capture incorrect groups in the existing hierarchy.</p>
<p>Assessing Claim Categorization</p>
<p>Unlike the previous sub-tasks which focus on assessing links between categories at all levels of the hierarchy, the final sub-task focuses on assessing the assignment of claims to various categories.Given a claim and all categories present in the hierarchy, for each claim-category pair, humans are prompted to assess whether the claim contains any information relevant to that category.The claimcategory pair is assigned a positive label if relevant information is present, and negative otherwise.For every category, we include the path from the root to provide additional context which might be needed to interpret it accurately (e.g., "positive findings" has a broader interpretation than "chemotherapy → positive findings").Instead of only assessing relevance of categories under which a claim has currently been categorized, this sub-task evaluates all claim-category pairs in order to catch recall errors, i.e., cases in which a claim could be assigned to an category but is not categorized there currently.filter out hierarchies which cover less than 30% of the claims associated with that topic.This leaves us with 320 hierarchies to collect corrections for.For the parent-child link assessment sub-task, this produces 1,635 links to be assessed.For sibling coherence, after removing all parent categories with only one child, we obtain 574 sibling groups to be assessed.Lastly, for claim categorization, the most intensive task, we end up with 50,723 claimcategory pairs to label.</p>
<p>Feedback</p>
<p>Annotator Background: We recruit a team of five experts with backgrounds in biology or medicine to conduct annotations.Two of these experts are authors on this paper, and the remaining three were recruited via Upwork. 4Every annotator is required to first complete a qualification test, which includes sample data from all three subtasks, and must achieve reasonable performance before they are asked to annotate data.</p>
<p>Annotation Pilots: Given the complexity and ambiguity of our tasks, we conduct several rounds of pilot annotation with iterative feedback before commencing full-scale annotation.This ensures that all annotators develop a deep understanding of the task and can achieve high agreement.After each pilot, we measure inter-annotator agreement on each sub-task.Due to the presence of unbalanced labels in tasks 1 and 2, we compute agreement using match rate; for task 3, we report Fleiss' kappa.At the end of all pilot rounds, we achieve high agreement on all sub-tasks, with match rates of 100% and 78% on tasks 1 and 2 respectively and Fleiss' kappa of 0.66 on task 3.</p>
<p>Assessment of Preliminary Hierarchies</p>
<p>An additional benefit of collecting corrections for preliminary hierarchies (as described above) is that this data allows us to quantify the quality of our 4 https://www.upwork.com/LLM-generated hierarchies and measure the performance of our hierarchy generation pipeline.</p>
<p>Parent-child link accuracy.Interestingly, we observe almost perfect performance on this sub-task, with only one out of 1635 parent-child links being labeled as incorrect where the pipeline put "Coffee consumption" under "Tea consumption and cancer risk".Of the remaining correct links, 75% are labeled as hypernym-hyponym links, and 25% as useful breakdowns of the parent category.This result demonstrates that LLMs are highly accurate at generating good sub-categories given a parent category, even when dealing with long inputs.</p>
<p>Sibling coherence performance.Next, we look into LLM performance on sibling coherence and observe that this is also fairly high, with 77% of sibling groups being labeled as coherent where "coherent" denotes a sibling group in which expert labels for all sibling categories are positive; otherwise, "zero."Among sibling groups labeled incoherent, we observe two common types of errors: (1) categories at different levels of granularity being grouped as siblings, and (2) one or more categories having subtly different focuses.For example, Fig. 1 demonstrates a type 1 error, where the sub-category "walking" is more specific and should be classified under "aerobic" but is instead listed as a sibling.</p>
<p>An example of a type 2 error is the parent category "dietary interventions" with child categories "low calorie diets", "high/low carbohydrate diets", and "prepared meal plans".Here, though all child categories are dietary interventions, the first two have an explicit additional focus on nutritional value which "prepared meal plans" lacks, making them incoherent as a sibling group.</p>
<p>Claim categorization performance.The design of our claim categorization sub-task prompts annotators to evaluate the relationship between a given claim and every category in the hierarchy.Hence, when assessing whether annotators agree with the LLM's categorization of a claim under a category, we need to aggregate over the labels assigned to all claim-category pairs from the root to the target category under consideration.Formally, for a claimcategory pair (cl i , ct j ), instead of only using label l ij = h((cl i , ct j )) from human feedback h, we must aggregate over labels assigned to all ancestors of ct j , i.e., L = [h((cl i , ct 1 )), ..., h((cl i , ct j ))],</p>
<p>where ct 1 is the root category and ct j is the target category.We do this aggregation using an AND operation l agg = l 1 ∧ l 2 ∧ ... ∧ l j .After computing these aggregate labels, we observe that our LLMbased pipeline has reasonable precision (0.71), but much lower recall (0.53) on claim categorization.A low recall rate on this sub-task is problematic because, while it is easy for human annotators to correct precision errors (remove claims wrongly assigned to various categories), it is much harder to correct recall errors (identify which claims were missed under a given category), which necessitates a thorough examination of all studies.</p>
<p>Characterizing Hierarchy Complexity</p>
<p>Our dataset creation process produces 2,174 hierarchies on 472 research topics, with 320 hierarchies (for 100 topics) corrected by domain experts.We briefly characterize the complexity of all generated hierarchies, focusing on two aspects: (i) structural complexity, and (ii) semantic complexity.</p>
<p>Structural Complexity</p>
<p>Hierarchy depth: All generated hierarchies are multi-level, with a mean hierarchy depth of 2.5, and maximum depth of 5.</p>
<p>Node arity: On average, every parent has a node arity of 2.4 (i.e., has 2.4 child categories).However, node arity can grow as large as 10 for certain parent categories.</p>
<p>Claim coverage: Another crucial property of generated hierarchies is their coverage of claims since hierarchies containing fewer claims are easier to generate but less useful.We observe that given a set of claims, a typical hierarchy incorporates 12.3 claims on average.Additionally, very few claims from a set remain uncategorized, i.e., not covered by any generated hierarchy (2.6 on average).These characteristics indicate that our LLMgenerated hierarchies have interesting structural properties.</p>
<p>Semantic Complexity</p>
<p>Category diversity: Our dataset contains 4.6 hierarchies per research topic.We manually inspect a small sample of hierarchies for 10 research topics, and find that none of the hierarchies generated for a single topic contain any repeating categories.This signals that the multiple hierarchies we generate per topic represent semantically diverse ways of grouping/slicing the same set of claims.Adherence to PICO framework: Systematic reviews in biomedicine typically use the PICO (pop-  ulation, intervention, comparator, outcome) framework (Richardson et al., 1995) to categorize studies.</p>
<p>To understand how much our generated hierarchies adhere to this framework, we again inspect hierarchies for 10 research topics and label whether the root category focuses on a PICO element.We observe that 34 out of 46 hierarchies have a PICOfocused root category, making them directly useful for systematic review.Interestingly, the remaining hierarchies still focus on useful categories such as continuing patient education, study limitations, cost analyses etc.Thus, besides surfacing categorizations expected by the systematic review process, using LLMs can help discover additional interesting categorizations.</p>
<p>Automating Hierarchy Correction</p>
<p>As mentioned in §4, we hire five domain experts to correct hierarchies for 100 research topics.However, the correction process, despite our best efforts at task simplification and decomposition, is still time-consuming and requires domain expertise.Therefore, we investigate whether we can use our corrected hierarchy data to automate some correction sub-tasks.In particular, we focus on automating sibling coherence and claim categorization correction since Table 1 indicates that LLMs already achieve near-perfect performance on producing relevant child categories for a parent.</p>
<p>Experimental Setup</p>
<p>We briefly discuss the experimental setup we use to evaluate whether model performance on sibling coherence and claim categorization correction can be improved using our collected feedback data.</p>
<p>Dataset Split</p>
<p>To better assess generalizability, we carefully construct two test sets, an in-domain (ID) and an out-ofdomain (OOD) subset instead of randomly splitting our final dataset of 100 research topics.To develop our OOD test set, we first embed all 100 research topics by running SPECTER2, a scientific paper embedding model using citation graph (Singh et al., 2022), on the title and abstract of the Cochrane systematic review associated with each topic.Then, we run hierarchical clustering on the embeddings and choose one isolated cluster (n = 12 reviews) to be our OOD test set.Our manual inspection reveals that all studies in this cluster are about fertility and pregnancy.After creating our OOD test set, we then randomly sub-sample our ID test set (n = 18 reviews) from remaining research topics.This leaves us with 70 topics, which we split into training and validation sets.Detailed statistics for our dataset splits, including number of instances for each correction sub-task, are provided in Table 2.</p>
<p>Models</p>
<p>We evaluate two classes of methods for correction:</p>
<p>• Finetuned LMs: To assess whether correction abilities of smaller LMs can be improved by finetuning on our collected feedback data, we experiment with Flan-T5 (Chung et al., 2022), which has proven to be effective on many benchmarks.• Zero-Shot CoT: To explore whether using chain-of-thought (CoT) prompting (Wei et al., 2022a) improves the ability of LLMs to do correction zero-shot without using our feedback data, we test OpenAI GPT-3.5 Turbo (gpt-3.5-turbo-0613)and GPT-4 Turbo (gpt-4-1106-preview).Additional modeling details including CoT prompts are provided in Appendix B.</p>
<p>Correcting Sibling Coherence</p>
<p>Table 3 presents the performance of all models on the task of identifying sibling groups that are incoherent.Finetuning models on this task is challenging due to the small size of the training set (n = 298) and imbalanced labels.Despite upsampling and model selection based on precision, finetuned Flan-T5 models do not perform well on this task (best F1-score of 33.3%).Additionally LLMs also do not perform well despite the use of chain-of-thought prompting to handle the complex reasoning required for this task.At 51.5% F1, LLMs outperform finetuned models; however, their precision (46.7% for GPT-4-Turbo) is still not good enough to detect incoherent sibling groups confidently.These results indicate that this correction sub-task is extremely difficult to automate and will likely continue to require expert intervention.</p>
<p>Correcting Claim Categorization</p>
<p>Table 4 shows the performance of all models on the task of correcting assignment of claims to categories in the hierarchy.Following the strategy described in §3.5, given a claim, we first use our models to generate predictions for every claim-category pair (all category nodes) and then obtain the final label for each category by applying an AND operation over all predictions from the root category to that category.Our results show that this task is easier to automate-fine-tuning Flan-T5 on our collected training dataset leads to better scores on all metrics compared to our LLM pipeline.Crucially, recall which is much more time-consuming for humans to fix, improves by 15.9 points using Flan-T5-large indicating that automating this step can provide additional efficiency gains during correction.LLMs perform well too, with GPT-4-Turbo achieving the best recall rate among all models, but its lower precision score makes the predictions less reliable overall.</p>
<p>Interestingly, we notice that all models perform better on the OOD test for both correction tasks, indicating that the OOD test set likely contains instances that are less challenging than the ID set.</p>
<p>Correcting Claim Categorization for Remaining Hierarchies</p>
<p>Comparing the claim categorization predictions of Flan-T5-large on our test set with our LLM-based hierarchy generation pipeline reveals that it flips labels in 24.7% cases, of which 63.5% changes are correct.This indicates that a FLAN-T5-large corrector can potentially improve claim categorization of LLM-generated hierarchies.Therefore, we apply this corrector to the remaining 372 LLMgenerated hierarchies that we do not have expert corrections for to improve claim assignment for those.Our final curated dataset CHIME contains hierarchies for 472 research topics, of which hierarchies for 100 topics have been corrected by experts on both category linking and claim categorization, while hierarchies for the remaining 372 have had claim assignments corrected automatically.</p>
<p>6 Related Work techniques for end-to-end review generation or to tackle specific aspects of the problem (see Altmami and Menai (2022) for a detailed survey).Some studies have focused on generating "citation sentences" discussing relationships between related papers, which can be included in a literature review (Xing et al., 2020;Luu et al., 2021;Ge et al., 2021;Wu et al., 2021).Other work has focused on the task of generating related work sections for a scientific paper (Hoang and Kan, 2010; Hu and Wan, 2014;Li et al., 2022;Wang et al., 2022), which while similar in nature to literature review, has a narrower scope and expects more concise generation outputs.Finally, motivated by the everimproving capabilities of generative models, some prior work has attempted to automate end-to-end review generation treating it as multi-document summarization, with limited success (Mohammad et al., 2009;Jha et al., 2015;Wallace et al., 2020;DeYoung et al., 2021;Liu et al., 2022;Zhu et al., 2023).Of these, Zhu et al. (2023) generates intermediate hierarchical outlines to scaffold literature review generation, but unlike our work, they do not produce multiple organizations for the same set of related studies.Additionally, we focus solely on the problem of organizing related studies for literature review, leaving review generation and writing assistance to future work.</p>
<p>LLMs for Organization</p>
<p>Organizing document collections is an extensivelystudied problem in NLP, with several classes of approaches such as clustering and topic model-ing (Dumais et al., 1988) addressing this goal.Despite their utility, conventional clustering and topic modeling approaches are not easily interpretable (Chang et al., 2009), requiring manual effort which introduces subjectivity and affects their reliability (Baden et al., 2022).Recent work has started exploring whether using LLMs for clustering (Viswanathan et al., 2023;Zhang et al., 2023;Wang et al., 2023) and topic modeling (Pham et al., 2023) can alleviate some of these issues, with promising results.This motivates us to experiment with LLMs for generating hierarchical organizations of scientific studies.Interestingly, TopicGPT (Pham et al., 2023) also attempts to perform hierarchical topic modeling, but is limited to producing two-level hierarchies unlike our approach which generates hierarchies of arbitrary depth.</p>
<p>Conclusion</p>
<p>Our work explored the utility of LLMs for producing hierarchical organizations of scientific studies, with the goal of assisting researchers in performing literature review.We collected CHIME, an expertcurated dataset for hierarchy generation focused on biomedicine, using a human-in-the-loop process in which a naive LLM-based pipeline generates preliminary hierarchies which are corrected by experts.</p>
<p>To make hierarchy correction less tedious and timeconsuming, we decomposed it into a three-step process in which experts assessed the correctness of links between categories as well as assignment of studies to categories.CHIME contains 2,174 LLM-generated hierarchies covering 472 topics, and expert-corrected hierarchies for a subset of 100 topics.Quantifying LLM performance using our collected data revealed that LLMs are quite good at generating and linking categories, but needed further improvement on study assignment.We trained a corrector model with our feedback data which improved study assignment further by 12.6% F1 points.We hope that releasing CHIME and our hierarchy generation and correction models will motivate further research on developing better assistive tools for literature review.</p>
<p>Limitations</p>
<p>Single-domain focus.Given our primary focus on biomedicine, it is possible that our hierarchy generation and correction methods do not generalize well to other scientific domains.Further investigation of generalization is out of scope for this work but a promising area for future research.</p>
<p>Deployment difficulties.Powerful LLMs like CLAUDE-2 have long inference times -in some cases, the entire hierarchy generation process can take up to one minute to complete.This makes it extremely challenging to deploy our hierarchy construction pipeline as a real-time application.However, it is possible to conduct controlled lab studies to evaluate the utility of our pipeline as a literature review assistant, which opens up another line of investigation for future work.</p>
<p>Reliance on curated sets of related studies.Our current hierarchical organization pipeline relies on the assumption that all provided studies are relevant to the research topic being reviewed.However, in a realistic literature review setting, researchers often retrieve a set of studies from search engines, which may or may not be relevant to the topic of interest, and are interested in organizing the retrieved results.</p>
<p>In a preliminary qualitative analysis in Appendix Section E, we show that our system can handle some noise in retrieved studies, though we defer a detailed robustness evaluation to future work.</p>
<p>A Prompts for Hierarchy Generation Pipeline</p>
<p>We present prompts for the hierarchy generation pipeline in Fig. 3 and Fig. 4.</p>
<p>B Model Training Details</p>
<p>Flan-T5 fintuning.We fine-tuned the flan-t5-base and flan-t5-large models using the Hugggingface library (Wolf et al., 2019) with NVIDIA RTX A6000 for both task 1 and task 3.For task 1, the learning rate is set to 1e-3 and the batch size is 16.We train the model for up to five epochs.For task 3, the learning rate is 3e-4 with batch size 16, and the models are trained up to two epochs.Each epoch takes less than 15 minutes for both model sizes.The numbers reported for each Flan-T5 model come from a single model checkpoint.</p>
<p>GPT-3.5 Turbo and GPT-4 Turbo We perform zero-shot CoT prompting for corrector models on tasks 1 and 3 with prompts in Fig. 5 and Fig. 6.</p>
<p>C Model Selection for Hierarchy Proposal Module</p>
<p>We conducted a qualitative evaluation of hierarchies generated by GPT-3.5-Turbo,GPT-4, and CLAUDE-2 for 10 sampled research topics.Results showed that GPT-3.5-Turbodoes poorly at following instructions and only generates well-formed hierarchies 30% of the time, while GPT-4 produces valid hierarchies but generates shallow ones with a depth of 1 60% of the time.In comparison, Claude-2 produces hierarchies with a higher depth (&gt;1) 90% of the time.</p>
<p>D Qualitative Analysis on Generated Claims</p>
<p>To better establish the accuracy of our NLI-based verification process, we have conducted an additional qualitative assessment of 100 abstract-claim pairs.We examined 50 pairs that the NLI model marked as "entailed" and 50 non-entailed pairs.Results show that the precision of the NLI model is very high, with 47 out of 50 entailed claims being correct, without hallucinations.Interestingly, we find that 37/50 non-entailed pairs are false negatives, indicating that in many cases, the generated claim is correct even though the NLI model predicts non-entailment.This human evaluation further validates that our claim generation process is high quality.</p>
<p>E Qualitative Analysis on Retrieval Quality</p>
<p>We conducted a brief experiment on 10 samples (sets of related studies present in our dataset) by injecting five irrelevant claims from other study sets per sample.We observed that during hierarchy generation, CLAUDE-2 was able to ignore irrelevant claims and generate hierarchies similar to the ones it originally produced (in the non-noisy setting).CLAUDE-2 can also differentiate between relevant and irrelevant claims and does not assign noisy claims to any categories in the hierarchy.</p>
<p>Title: {title}</p>
<p>Abstract: {abstract}</p>
<p>Task: Conclude new findings and null findings from the abstract in one sentence in the atomic format.Do not separate new findings and null findings.The finding must be relevant to the title.Do not include any other information.</p>
<p>Definition:</p>
<p>A scientific claim is an atomic verifiable statement expressing a finding about one aspect of a scientific entity or process, which can be verified from a single source.(Claim 2,4,5,6,7,8,9,10,11) 1: Usage of other drugs (Claim 4, 5, 6, 9, 10, 11) 2: Dosing comparisons (Claim 7, 8) 2.1: Dosing of Remdesivir (Claim 7) Aspect 3: Treatment of COVID-19 patients (Claim 1,13,14,15,16,17,18,19,20,21) 1: Treatment procedures for other diseases (Claim 13,14,16,17,18,19,20,21) 2: Treatment timeframe comparisons.(Claim 1, 15) ** Instruction ** In this task, you will be annotating the relationship among a set of sibling categories.You will assess whether categories logically belong together within their shared parent category, a concept referred to as 'coherence'.</p>
<p>Your task is to label whether ALL sibling categories are coherent with each other.If all sibling categories fit well and logically belongs to the broader group, label it 'These sibling categories are coherent' to signify its coherence.Make sure silbings are at the same level of granularity for coherence assessment.** Instruction ** In this task, your role as an annotator is to assess whether a belongs to a provided category.</p>
<p>Your responsibility is to assign a binary label for each category-claim pairing: 1. "The claim belongs to the category" -Choose this if any part or aspect of the claim is relevant to the category, even if the connection is broad or indirect.This includes claims that are negations or opposites of the category.</p>
<p>See the following examples:</p>
<p>The claim "Assisted hatching through partial zona dissection does not improve pregnancy and embryo implantation rates in unselected patients undergoing IVF or ICSI" belongs to "Impact on specific patient groups" category because patient groups can be applied to not only patient demographics but also patients with the same disease/symptom.The claim "Sumatriptan is effective in reducing productivity loss due to migraine, with significant improvements in productivity loss and return to normal work performance compared to placebo."belongs to "Headache relief" because headache is one of the symptoms of migraine even though it is not explicitly mentioned in the claim.</p>
<p>Figure 2 :
2
Figure 2: LLM-based pipeline for preliminary hierarchy generation given a set of related studies on a topic.</p>
<p>Figure 3 :
3
Figure 3: Claim generation prompt for GPT-3.5 Turbo.</p>
<p>Figure 4 :
4
Figure 4: Hierarchy proposal module prompt for Claude-2.</p>
<p>If any category doesn't seem to belong logically or doesn't fit well within the group, label it 'These sibling categories are NOT coherent' to indicate non-coherence.Your decisions should be based solely on the level of coherence -how well these categories fit together under their shared parent category and not on any other factors or personal preferences.<strong>Remember</strong> 1.You should start with step-by-step reasoning and generate the answer at the end in the given format.2.You should only reply with the answer in the format of [These sibling categories are coherent] or [These sibling categories are NOT coherent].3.You will be given a parent category and a set of sibling categories.You should assess each sibling category independently.Again, follow the format below to reply: Step-by-step reasoning: [Your reasoning] Answer: [These sibling categories are coherent] or [These sibling categories are NOT coherent] ** Question ** Parent category: {parent_category} Sibling categories: {sibling_categories}</p>
<p>Figure 5 :
5
Figure 5: Prompt for task 1 sibling coherence for both GPT-3.5 Turbo and GPT-4 Turbo.</p>
<ol>
<li>"The claim does NOT belong to the category" -Choose this if there is no meaningful connection between the claim and the category.<strong>Remember</strong> 1.Only reply with the answer in the format of [The claim belongs to the category] or [The claim does NOT belong to the category].2. Do not reply with any other format.3. Start with step-by-step reasoning and generate the answer at the end in the given format.<strong>Claim</strong> {claim} <strong>Category</strong> {category} Again, follow the format below to reply: Step-by-step reasoning: [Your reasoning] Answer: [The claim belongs to the category] or [The claim does NOT belong to the category]</li>
</ol>
<p>Figure 6 :
6
Figure 6: Prompt for task 3 claim assignment for both GPT-3.5 Turbo and GPT-4 Turbo.</p>
<p>Table 2
2: Dataset statistics for three correction sub-tasks:parent-child category links (Task 1), sibling categorycoherence (Task 2), and claim categorization (Task 3).</p>
<p>Table 3 :
3
Performance of all models on assessing sibling coherence.
6.1 Literature Review SupportPrior work on developing literature review supporttools has largely focused on using summarization</p>
<p>Table 4 :
4
Performance of all models on correcting claim categorization.</p>
<p>Level Aspect Generation:<strong> Utilize the entities extracted from the study abstracts for identifying up to 5 top-level aspects from the clinical study claims.You should list these aspects in a bulleted list format without incorporating any extraneous information.Cite the entities in that support the aspects.This will be the [Response 1] section.2.</strong>Hierarchical Faceted Category Generation:<strong> For every top-level aspect in [Response 1], proceed to generate hierarchical faceted categories that closely align with the above study claims.The granularity of these categories must be similar to their corresponding parent categories and the siblings categories.Avoid including unrelated information.Cite the claims that support your categories.This will make up the [Response 2] section of your output.Precision is vital in this process; strive to avoid vague or imprecise extractions.2. Include only relevant data and exclude any information not pertinent to the task.3. Strictly adhere to the output format.The claims are cited in the format "(Claim 0, 2, 3, 12)" for each category and aspect.4. The output should be in the form of a nested list using numbers.
</strong>Review Title<strong>{systematic_review_title}Frequent entities from study abstracts:{freq_entities}</strong>Study Claim List<strong>{claim_list}</strong>Instruction:<strong>Your task process a review title involving relevant clinical studies as per the following requirements:1. </strong>Top-Here is an example:If given the review title "The efficacy of Remdesivir in treating COVID-19 patients: A review," your task output might looklike this:Frequent entities from study abstracts:Efficacy, Remdesivir, treatment, COVID-19 patients<strong>Output Format</strong>[Response 1]:Aspect 1: Efficacy of treatment (Efficacy)Aspect 2: Application of Remdesivir (Remdesivir)Aspect 3: Treatment of COVID-19 patients (treatment, COVID-19 patients)[Response 2]:Aspect 1: Efficacy of treatment (Claim 0, 2, 3, 12)1: Efficiency of alternative treatments (Claim 0, 2, 3, 12)1.1: Efficacy of Remdesivir (Claim 0, 12)1.2: Efficacy of other drugs (Claim 3)2: Side-effects comparison (Claim 2)Aspect 2: Application of Remdesivir
<strong>Remember:</strong> 1.</p>
<p>https://www.cochranelibrary.com/cdsr/reviews
We further conduct a qualitative evaluation to ensure factuality of generated claims in Appendix Section D.
AcknowledgementsWe would like to thank the reviewers, Joseph Chee Chang, and the rest of the Semantic Scholar team at AI2 for their valuable feedback and comments.We also want to thank the Upworkers who participated in our formative studies and annotation process.
Automatic summarization of scientific articles: A survey. Journal of King Saud University-Computer and Information Sciences. Nouf Ibrahim Altmami and Mohamed El Bachir Menai3442022</p>
<p>Three gaps in computational text analysis methods for social sciences: A research agenda. Christian Baden, Christian Pipal, Martijn Schoonvelde, Ac G Mariken, Van Der Velden, Communication Methods and Measures. 1612022</p>
<p>Analysis of the time and workers needed to conduct systematic reviews of medical interventions using data from the prospero registry. Rohit Borah, Andrew W Brown, Patrice L Capers, Kathryn A Kaiser, BMJ open. 722017</p>
<p>Reading tea leaves: How humans interpret topic models. Advances in neural information processing systems. Jonathan Chang, Sean Gerrish, Chong Wang, Jordan Boyd-Graber, David Blei, 200922</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, S Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Dasha Chowdhery, Sharan Valter, Gaurav Narang, Adams Wei Mishra, Vincent Yu, Yanping Zhao, Andrew M Huang, Hongkun Dai, Yu, ArXiv, abs/2210.11416Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei2022Slav Petrov</p>
<p>Marg: Multi-agent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, arXiv:2401.042592024arXiv preprint</p>
<p>MSˆ2: Multidocument summarization of medical studies. Jay Deyoung, Iz Beltagy, Madeleine Van Zuylen, Bailey Kuehl, Lucy Lu, Wang , 10.18653/v1/2021.emnlp-main.594Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Using latent semantic analysis to improve access to textual information. Susan T Dumais, George W Furnas, Thomas K Landauer, Scott Deerwester, Richard Harshman, Proceedings of the SIGCHI conference on Human factors in computing systems. the SIGCHI conference on Human factors in computing systems1988</p>
<p>BACO: A background knowledge-and content-based framework for citing sentence generation. Yubin Ge, Ly Dinh, Xiaofeng Liu, Jinsong Su, Ziyao Lu, Ante Wang, Jana Diesner, 10.18653/v1/2021.acl-long.116Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Towards automated related work summarization. Cong Duy, Vu Hoang, Min-Yen Kan, Coling 2010 Organizing Committee. Beijing, China2010Coling 2010: Posters</p>
<p>Automatic generation of related work sections in scientific papers: An optimization approach. Yue Hu, Xiaojun Wan, 10.3115/v1/D14-1170Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, Qatar2014Association for Computational Linguistics</p>
<p>Content models for survey generation: A factoid-based evaluation. Rahul Jha, Catherine Finegan-Dollak, Ben King, 10.3115/v1/P15-1043Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, China20151Reed Coke, and Dragomir Radev. Association for Computational Linguistics</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Less annotating, more classifying: Addressing the data scarcity issue of supervised machine learning with deep transfer learning and bert-nli. Moritz Laurer, Andreu Wouter Van Atteveldt, Kasper Casas, Welbers, 10.1017/pan.2023.20Political Analysis. 3212024</p>
<p>Generating a related work section for scientific papers: an optimized approach with adopting problem and method information. Pengcheng Li, Wei Lu, Qikai Cheng, Scientometrics. 12782022</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, Transactions of the Association for Computational Linguistics. 122023</p>
<p>Generating a structured summary of numerous academic papers: Dataset and method. Shuaiqi Liu, Jiannong Cao, Ruosong Yang, Zhiyuan Wen, International Joint Conference on Artificial Intelligence. 2022</p>
<p>Explaining relationships between scientific documents. Kelvin Luu, Xinyi Wu, Rik Koncel-Kedziorski, Kyle Lo, Isabel Cachola, Noah A Smith, 10.18653/v1/2021.acl-long.166Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Using citations to generate surveys of scientific paradigms. Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed Hassan, Pradeep Muthukrishan, Dragomir Vahed Qazvinian, David Radev, Zajic, Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter. Human Language Technologies: The 2009 Annual Conference of the North American ChapterBoulder, ColoradoAssociation for Computational Linguistics2009</p>
<p>ScispaCy: Fast and robust models for biomedical natural language processing. Mark Neumann, Daniel King, Iz Beltagy, Waleed Ammar, 10.18653/v1/W19-5034Proceedings of the 18th BioNLP Workshop and Shared Task. the 18th BioNLP Workshop and Shared TaskFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Topicgpt: A prompt-based topic modeling framework. Minh Chau, Alexander Miserlis Pham, Simeng Hoyle, Mohit Sun, Iyyer, ArXiv, abs/2311.014492023</p>
<p>The well-built clinical question: a key to evidence-based decisions. Mark C Scott Richardson, Jim Wilson, Robert S Nishikawa, Hayward, ACP journal club. 12331995</p>
<p>How quickly do systematic reviews go out of date? a survival analysis. Margaret Kaveh G Shojania, Mohammed T Sampson, Jun Ansari, Steve Ji, David Doucette, Moher, Annals of internal medicine. 14742007</p>
<p>Scirepeval: A multi-format benchmark for scientific document representations. Amanpreet Singh, Mike D' Arcy, Arman Cohan, Doug Downey, Sergey Feldman, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>Large language models enable few-shot clustering. Vijay Viswanathan, Kiril Gashteovski, Carolin ( Haas) Lawrence, Tongshuang Sherry Wu, Graham Neubig, Transactions of the Association for Computational Linguistics. 122023</p>
<p>Generating (factual?) narrative summaries of rcts: Experiments with neural multi-document summarization. Byron C Wallace, Sayantani Saha, Frank Soboczenski, Iain James Marshall, Annual Symposium proceedings. AMIA Symposium. 20202021</p>
<p>Multi-document scientific summarization from a knowledge graph-centric view. Pancheng Wang, Shasha Li, Kunyuan Pang, Liangliang He, Dong Li, Jintao Tang, Ting Wang, arXiv:2209.043192022arXiv preprint</p>
<p>Goal-driven explainable clustering via language descriptions. Zihan Wang, Jingbo Shang, Ruiqi Zhong, ArXiv, abs/2305.137492023</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai Hsin Chi, F Xia, Quoc Le, Denny Zhou, ArXiv, abs/2201.119032022a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Jamie Brew, ArXiv, abs/1910.037712019</p>
<p>Towards generating citation sentences for multiple references with intent control. Jia-Yan Wu, Alexander Te-Wei Shieh, Shih-Ju Hsu, Yun-Nung Chen, arXiv:2112.013322021arXiv preprint</p>
<p>Automatic generation of citation texts in scholarly papers: A pilot study. Xinyu Xing, Xiaosheng Fan, Xiaojun Wan, 10.18653/v1/2020.acl-main.550Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Appraising the potential uses and harms of LLMs for medical systematic reviews. Hye Yun, Iain Marshall, Thomas Trikalinos, Byron Wallace, 10.18653/v1/2023.emnlp-main.626Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Clusterllm: Large language models as a guide for text clustering. Yuwei Zhang, Zihan Wang, Jingbo Shang, ArXiv, abs/2305.148712023</p>
<p>Hierarchical catalogue generation for literature review: A benchmark. Kun Zhu, Xiaocheng Feng, Xiachong Feng, Yingsheng Wu, Bing Qin, 10.18653/v1/2023.findings-emnlp.453Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>            </div>
        </div>

    </div>
</body>
</html>
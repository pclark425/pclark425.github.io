<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5792 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5792</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5792</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-117.html">extraction-schema-117</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-2a85c44afeeeb336c5eafcd4001ccb033d3d1f1c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2a85c44afeeeb336c5eafcd4001ccb033d3d1f1c" target="_blank">Forecasting Future World Events with Neural Networks</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> Autocast is introduced, a dataset containing thousands of forecasting questions and an accompanying news corpus that poses a novel challenge for large language models and improved performance could bring large practical benefits.</p>
                <p><strong>Paper Abstract:</strong> Forecasting future world events is a challenging but valuable task. Forecasts of climate, geopolitical conflict, pandemics and economic indicators help shape policy and decision making. In these domains, the judgment of expert humans contributes to the best forecasts. Given advances in language modeling, can these forecasts be automated? To this end, we introduce Autocast, a dataset containing thousands of forecasting questions and an accompanying news corpus. Questions are taken from forecasting tournaments, ensuring high quality, real-world importance, and diversity. The news corpus is organized by date, allowing us to precisely simulate the conditions under which humans made past forecasts (avoiding leakage from the future). Motivated by the difficulty of forecasting numbers across orders of magnitude (e.g. global cases of COVID-19 in 2022), we also curate IntervalQA, a dataset of numerical questions and metrics for calibration. We test language models on our forecasting task and find that performance is far below a human expert baseline. However, performance improves with increased model size and incorporation of relevant information from the news corpus. In sum, Autocast poses a novel challenge for large language models and improved performance could bring large practical benefits.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5792.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5792.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UnifiedQA-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UnifiedQA-v2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose question-answering transformer model evaluated zero-shot on forecasting questions; used as a baseline to assess LLM forecasting ability without retrieval or temporal context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Forecasting Future World Events with Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UnifiedQA-v2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pretrained multi-task question-answering transformer (UnifiedQA family) evaluated in a zero-shot manner on true/false and multiple-choice forecasting questions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.2B / 0.8B / 2.8B</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Probabilities/answers for diverse forecasting questions drawn from forecasting tournaments, including Science & Technology events (e.g., whether a Tesla car would demonstrate full autonomy by end of 2021) and other real-world events.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Zero-shot prompting for classification (no retrieval augmentation). The model is applied directly to forecast questions in a text-to-text format.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_protocol</strong></td>
                            <td>Retrodiction on past forecasting questions (no future leakage), evaluated on percent accuracy for true/false and multiple-choice question types; numerical questions reported as random baseline since UnifiedQA was not trained for them.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Autocast — a curated dataset of 6,707 forecasting questions (resolved and unresolved) collected from Metaculus, Good Judgment Open, and CSET Foretell; news corpus from Common Crawl (CC-NEWS) organized by date for backtesting.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Zero-shot performance was close to random on Autocast: ~45.4–54.9% on T/F depending on size (random is 50%), and ~22–25% on MCQ (near random chance). Far below the human crowd baseline (T/F 92.4%, MCQ 81.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Zero-shot UnifiedQA-v2 lacks access to contemporaneous retrieval and temporal supervision, performs near-random on forecasting tasks, and is not set up for numerical prediction — demonstrating that off-the-shelf QA models cannot reliably estimate event probabilities without retrieval, fine-tuning, or temporal modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Forecasting Future World Events with Neural Networks', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5792.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5792.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 (Text-to-Text Transfer Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-to-text transformer family fine-tuned as a forecasting baseline to output true/false, multiple-choice, and numerical forecasts, without retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Forecasting Future World Events with Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned T5 models (text-to-text transformer) trained on the Autocast training set with task-specific output heads (including an added linear head for numerical outputs), evaluated at multiple parameter scales.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.2B / 0.8B / 2.8B</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Same Autocast forecasting questions spanning politics, economy, science & tech, social events and other real-world outcomes (including Science & Technology category events).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Supervised fine-tuning on Autocast training questions to produce point forecasts; no retrieval augmentation was used in the T5 baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_protocol</strong></td>
                            <td>Retrodiction split by date (train questions closed before May 11, 2021; test after), evaluated with percent accuracy for T/F and MCQ, L1 distance (normalized) for numerical, and a combined Score metric (T/F + MCQ - Numerical)/2.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Autocast dataset (train/test split to avoid future leakage) with CC-NEWS retrieval corpus available but not used for this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Fine-tuned T5 outperformed zero-shot UnifiedQA but remained far below humans: e.g., T/F ~60–61% across sizes, MCQ ~24–29%, numerical performance worse than some retrieval models (numerical metric values higher/worse); combined Score around low 30s (e.g., 32.4–33.7).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Without retrieval, T5 struggled to access up-to-date evidence for time-sensitive events; multiple-choice accuracy remained near chance, numerical forecasting across large dynamic ranges was poor, and overall performance trailed human crowd aggregates by a large margin.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Forecasting Future World Events with Neural Networks', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5792.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5792.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FiD Static</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fusion-in-Decoder (Static retrieval, T5 backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented T5-based model that encodes top retrieved articles (top 10) via FiD to improve forecasting accuracy by incorporating contemporaneous news.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Forecasting Future World Events with Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FiD Static (T5 encoder + BM25 retrieval + FiD decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Retrieval-augmented model: BM25 retrieval (with cross-encoder re-ranking) returns top 10 daily news articles pre-dating each forecast timestep; T5-based FiD encodes the passages and decodes forecasts (standard FiD static approach).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.2B / 0.8B / 2.8B</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Autocast forecasting questions including Science & Technology items and other real-world events; the model generates forecasts at prediction timesteps using only news available up to that time.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Retrieval-augmented generation: BM25 retrieve relevant news constrained to pre-resolution dates, top-10 articles encoded by FiD; model fine-tuned on Autocast to predict outcomes (T/F, MCQ, numerical).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_protocol</strong></td>
                            <td>Retrodictive evaluation on Autocast test set with date-based splits; percent accuracy for T/F and MCQ, L1 for numerical, combined Score metric; calibration measured via adaptive binning calibration error for classification.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Autocast with CC-NEWS corpus for retrieval; evaluation against human crowd baseline and non-retrieval baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Substantially better than non-retrieval baselines: e.g., FiD Static (2.8B) achieved T/F 65.4%, MCQ 35.8%, Numerical 19.9 (lower is better for numerical), combined Score 40.6, Average around 37.2 across sizes. Still far below human crowd (Score 82.5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Although retrieval provided highly relevant articles, FiD Static's calibration was poor (largest FiD Static model had ~40% calibration error vs human crowd ~8%); numerical forecasts remain challenging across orders of magnitude; retrieval efficiency and passage truncation (512 tokens) limit information incorporation; performance sensitive to model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Forecasting Future World Events with Neural Networks', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5792.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5792.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FiD Temporal</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FiD Temporal (FiD Static embeddings + GPT-2 autoregressive temporal model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A temporal retrieval-augmented forecasting model that treats the top daily retrieved article as a time-series (one per day), encodes them with FiD Static, and trains an autoregressive GPT-2 to predict daily crowd forecasts and final outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Forecasting Future World Events with Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FiD Temporal (T5 FiD static encoder + GPT-2 autoregressive model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid model: for each active day of a question retrieve the top news article, embed it with a frozen fine-tuned FiD Static encoder; feed sequence of daily embeddings into a GPT-2 style autoregressive model fine-tuned to predict the average of the daily crowd prediction and the ground-truth outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.6B / 1.5B / 4.3B</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Time series of probabilistic forecasts for Autocast questions (daily predictions over a question's active period), including Science & Technology events and other future real-world outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Temporal modeling using retrieved daily evidence: BM25 retrieval (top-1 per day), FiD Static encodes each day's top article to embeddings, autoregressive GPT-2 ingests embeddings over time and is trained to match intermediate crowd forecasts and eventual outcomes (auxiliary supervision).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_protocol</strong></td>
                            <td>Retrodiction on Autocast test split; daily and final forecast accuracy measured via percent accuracy (T/F, MCQ), L1 for numerical; combined Score metric; calibration measured (adaptive binning calibration error); ensemble analysis over model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Autocast with CC-NEWS retrieval corpus; auxiliary use of crowd time-series forecasts as supervision signal during training (but test only uses pre-resolution info).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Performance slightly better on average than some baselines: FiD Temporal average Score around 37.8; example sizes: T/F ~62–64%, MCQ ~32–37%, Numerical ~19–24; ensemble of large FiD Temporal models shows prediction score improving as resolution date nears. Calibration improved relative to FiD Static (FiD Temporal reduced calibration error from ~40% to ~17%), but absolute accuracy still far below human crowd.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Temporal auxiliary supervision provided modest gains and did not outperform the largest FiD Static in all cases; numerical forecasting remains weak; reliance on daily top-1 articles may miss relevant multi-source evidence; training complexity and embedding truncation limit long-horizon reasoning; distribution shift over time and limited numerical training examples constrain performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Forecasting Future World Events with Neural Networks', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5792.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5792.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa-v3 (IntervalQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa-v3 fine-tuned for IntervalQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DeBERTa-v3 encoder models fine-tuned to output point estimates and calibrated confidence intervals for numerical estimation tasks across many orders of magnitude (IntervalQA auxiliary dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Forecasting Future World Events with Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa-v3 (fine-tuned on IntervalQA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DeBERTa-v3 encoder models (various sizes) fine-tuned to jointly predict a point estimate and nested confidence intervals for numerical question answering; uses a custom loss combining point MSE, boundary-errors for mis-contained intervals, and a penalty on interval length.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>22M / 44M / 86M / 304M</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Calibrated numerical estimates (point estimates and confidence intervals) for fixed numerical questions drawn from many NLP datasets (SQuAD, TriviaQA, MATH, etc.) aggregated into IntervalQA — an auxiliary dataset for calibration (not time-dependent forecasting questions).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Supervised learning to output a point estimate and multiple nested confidence intervals (fixed confidence levels). Loss components: MSE on point estimate, boundary MSE on intervals that fail to contain the target, and interval-length penalty; labels log-transformed to manage dynamic range.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_protocol</strong></td>
                            <td>Evaluation of calibration via RMS Calibration Error (adaptive binning across label dynamic range) computed over fixed confidence levels (50% to 95%); median point-estimate error (Point Estimate Distance) and median interval length reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>IntervalQA — ~30,000 numerical questions collected from existing NLP datasets filtered for numerical answers; training/val/test splits (32,200 / 3,443 / 6,170).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Larger DeBERTa-v3 models yielded better calibration and narrower intervals: Point Estimate Distance decreased from 20.8 to 18.1, median interval length from 2072.4 to 305.4, and RMS Calibration Error from 19.1 to 13.5 when scaling from 22M to 304M parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>IntervalQA is not forecasting future events and thus does not test time-sensitive retrodiction; numerical targets span many orders of magnitude leading to very large predicted intervals and substantial residual calibration error even at largest sizes; transferring interval calibration methods to time-sensitive forecasting tasks (Autocast numerical questions) remains an open challenge due to small number of numerical forecasts in Autocast and distributional differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Forecasting Future World Events with Neural Networks', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Getting gpt-3 to predict metaculus questions <em>(Rating: 2)</em></li>
                <li>ForecastQA: A question answering challenge for event forecasting with temporal text data <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 1)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5792",
    "paper_id": "paper-2a85c44afeeeb336c5eafcd4001ccb033d3d1f1c",
    "extraction_schema_id": "extraction-schema-117",
    "extracted_data": [
        {
            "name_short": "UnifiedQA-v2",
            "name_full": "UnifiedQA-v2",
            "brief_description": "A general-purpose question-answering transformer model evaluated zero-shot on forecasting questions; used as a baseline to assess LLM forecasting ability without retrieval or temporal context.",
            "citation_title": "Forecasting Future World Events with Neural Networks",
            "mention_or_use": "use",
            "model_name": "UnifiedQA-v2",
            "model_description": "A pretrained multi-task question-answering transformer (UnifiedQA family) evaluated in a zero-shot manner on true/false and multiple-choice forecasting questions.",
            "model_size": "0.2B / 0.8B / 2.8B",
            "prediction_target": "Probabilities/answers for diverse forecasting questions drawn from forecasting tournaments, including Science & Technology events (e.g., whether a Tesla car would demonstrate full autonomy by end of 2021) and other real-world events.",
            "prediction_method": "Zero-shot prompting for classification (no retrieval augmentation). The model is applied directly to forecast questions in a text-to-text format.",
            "evaluation_protocol": "Retrodiction on past forecasting questions (no future leakage), evaluated on percent accuracy for true/false and multiple-choice question types; numerical questions reported as random baseline since UnifiedQA was not trained for them.",
            "dataset_or_benchmark": "Autocast — a curated dataset of 6,707 forecasting questions (resolved and unresolved) collected from Metaculus, Good Judgment Open, and CSET Foretell; news corpus from Common Crawl (CC-NEWS) organized by date for backtesting.",
            "results": "Zero-shot performance was close to random on Autocast: ~45.4–54.9% on T/F depending on size (random is 50%), and ~22–25% on MCQ (near random chance). Far below the human crowd baseline (T/F 92.4%, MCQ 81.0%).",
            "limitations_or_challenges": "Zero-shot UnifiedQA-v2 lacks access to contemporaneous retrieval and temporal supervision, performs near-random on forecasting tasks, and is not set up for numerical prediction — demonstrating that off-the-shelf QA models cannot reliably estimate event probabilities without retrieval, fine-tuning, or temporal modeling.",
            "uuid": "e5792.0",
            "source_info": {
                "paper_title": "Forecasting Future World Events with Neural Networks",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "T5",
            "name_full": "T5 (Text-to-Text Transfer Transformer)",
            "brief_description": "A text-to-text transformer family fine-tuned as a forecasting baseline to output true/false, multiple-choice, and numerical forecasts, without retrieval augmentation.",
            "citation_title": "Forecasting Future World Events with Neural Networks",
            "mention_or_use": "use",
            "model_name": "T5 (fine-tuned)",
            "model_description": "Fine-tuned T5 models (text-to-text transformer) trained on the Autocast training set with task-specific output heads (including an added linear head for numerical outputs), evaluated at multiple parameter scales.",
            "model_size": "0.2B / 0.8B / 2.8B",
            "prediction_target": "Same Autocast forecasting questions spanning politics, economy, science & tech, social events and other real-world outcomes (including Science & Technology category events).",
            "prediction_method": "Supervised fine-tuning on Autocast training questions to produce point forecasts; no retrieval augmentation was used in the T5 baseline.",
            "evaluation_protocol": "Retrodiction split by date (train questions closed before May 11, 2021; test after), evaluated with percent accuracy for T/F and MCQ, L1 distance (normalized) for numerical, and a combined Score metric (T/F + MCQ - Numerical)/2.",
            "dataset_or_benchmark": "Autocast dataset (train/test split to avoid future leakage) with CC-NEWS retrieval corpus available but not used for this baseline.",
            "results": "Fine-tuned T5 outperformed zero-shot UnifiedQA but remained far below humans: e.g., T/F ~60–61% across sizes, MCQ ~24–29%, numerical performance worse than some retrieval models (numerical metric values higher/worse); combined Score around low 30s (e.g., 32.4–33.7).",
            "limitations_or_challenges": "Without retrieval, T5 struggled to access up-to-date evidence for time-sensitive events; multiple-choice accuracy remained near chance, numerical forecasting across large dynamic ranges was poor, and overall performance trailed human crowd aggregates by a large margin.",
            "uuid": "e5792.1",
            "source_info": {
                "paper_title": "Forecasting Future World Events with Neural Networks",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "FiD Static",
            "name_full": "Fusion-in-Decoder (Static retrieval, T5 backbone)",
            "brief_description": "A retrieval-augmented T5-based model that encodes top retrieved articles (top 10) via FiD to improve forecasting accuracy by incorporating contemporaneous news.",
            "citation_title": "Forecasting Future World Events with Neural Networks",
            "mention_or_use": "use",
            "model_name": "FiD Static (T5 encoder + BM25 retrieval + FiD decoder)",
            "model_description": "Retrieval-augmented model: BM25 retrieval (with cross-encoder re-ranking) returns top 10 daily news articles pre-dating each forecast timestep; T5-based FiD encodes the passages and decodes forecasts (standard FiD static approach).",
            "model_size": "0.2B / 0.8B / 2.8B",
            "prediction_target": "Autocast forecasting questions including Science & Technology items and other real-world events; the model generates forecasts at prediction timesteps using only news available up to that time.",
            "prediction_method": "Retrieval-augmented generation: BM25 retrieve relevant news constrained to pre-resolution dates, top-10 articles encoded by FiD; model fine-tuned on Autocast to predict outcomes (T/F, MCQ, numerical).",
            "evaluation_protocol": "Retrodictive evaluation on Autocast test set with date-based splits; percent accuracy for T/F and MCQ, L1 for numerical, combined Score metric; calibration measured via adaptive binning calibration error for classification.",
            "dataset_or_benchmark": "Autocast with CC-NEWS corpus for retrieval; evaluation against human crowd baseline and non-retrieval baselines.",
            "results": "Substantially better than non-retrieval baselines: e.g., FiD Static (2.8B) achieved T/F 65.4%, MCQ 35.8%, Numerical 19.9 (lower is better for numerical), combined Score 40.6, Average around 37.2 across sizes. Still far below human crowd (Score 82.5).",
            "limitations_or_challenges": "Although retrieval provided highly relevant articles, FiD Static's calibration was poor (largest FiD Static model had ~40% calibration error vs human crowd ~8%); numerical forecasts remain challenging across orders of magnitude; retrieval efficiency and passage truncation (512 tokens) limit information incorporation; performance sensitive to model scale.",
            "uuid": "e5792.2",
            "source_info": {
                "paper_title": "Forecasting Future World Events with Neural Networks",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "FiD Temporal",
            "name_full": "FiD Temporal (FiD Static embeddings + GPT-2 autoregressive temporal model)",
            "brief_description": "A temporal retrieval-augmented forecasting model that treats the top daily retrieved article as a time-series (one per day), encodes them with FiD Static, and trains an autoregressive GPT-2 to predict daily crowd forecasts and final outcome.",
            "citation_title": "Forecasting Future World Events with Neural Networks",
            "mention_or_use": "use",
            "model_name": "FiD Temporal (T5 FiD static encoder + GPT-2 autoregressive model)",
            "model_description": "Hybrid model: for each active day of a question retrieve the top news article, embed it with a frozen fine-tuned FiD Static encoder; feed sequence of daily embeddings into a GPT-2 style autoregressive model fine-tuned to predict the average of the daily crowd prediction and the ground-truth outcome.",
            "model_size": "0.6B / 1.5B / 4.3B",
            "prediction_target": "Time series of probabilistic forecasts for Autocast questions (daily predictions over a question's active period), including Science & Technology events and other future real-world outcomes.",
            "prediction_method": "Temporal modeling using retrieved daily evidence: BM25 retrieval (top-1 per day), FiD Static encodes each day's top article to embeddings, autoregressive GPT-2 ingests embeddings over time and is trained to match intermediate crowd forecasts and eventual outcomes (auxiliary supervision).",
            "evaluation_protocol": "Retrodiction on Autocast test split; daily and final forecast accuracy measured via percent accuracy (T/F, MCQ), L1 for numerical; combined Score metric; calibration measured (adaptive binning calibration error); ensemble analysis over model sizes.",
            "dataset_or_benchmark": "Autocast with CC-NEWS retrieval corpus; auxiliary use of crowd time-series forecasts as supervision signal during training (but test only uses pre-resolution info).",
            "results": "Performance slightly better on average than some baselines: FiD Temporal average Score around 37.8; example sizes: T/F ~62–64%, MCQ ~32–37%, Numerical ~19–24; ensemble of large FiD Temporal models shows prediction score improving as resolution date nears. Calibration improved relative to FiD Static (FiD Temporal reduced calibration error from ~40% to ~17%), but absolute accuracy still far below human crowd.",
            "limitations_or_challenges": "Temporal auxiliary supervision provided modest gains and did not outperform the largest FiD Static in all cases; numerical forecasting remains weak; reliance on daily top-1 articles may miss relevant multi-source evidence; training complexity and embedding truncation limit long-horizon reasoning; distribution shift over time and limited numerical training examples constrain performance.",
            "uuid": "e5792.3",
            "source_info": {
                "paper_title": "Forecasting Future World Events with Neural Networks",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "DeBERTa-v3 (IntervalQA)",
            "name_full": "DeBERTa-v3 fine-tuned for IntervalQA",
            "brief_description": "DeBERTa-v3 encoder models fine-tuned to output point estimates and calibrated confidence intervals for numerical estimation tasks across many orders of magnitude (IntervalQA auxiliary dataset).",
            "citation_title": "Forecasting Future World Events with Neural Networks",
            "mention_or_use": "use",
            "model_name": "DeBERTa-v3 (fine-tuned on IntervalQA)",
            "model_description": "DeBERTa-v3 encoder models (various sizes) fine-tuned to jointly predict a point estimate and nested confidence intervals for numerical question answering; uses a custom loss combining point MSE, boundary-errors for mis-contained intervals, and a penalty on interval length.",
            "model_size": "22M / 44M / 86M / 304M",
            "prediction_target": "Calibrated numerical estimates (point estimates and confidence intervals) for fixed numerical questions drawn from many NLP datasets (SQuAD, TriviaQA, MATH, etc.) aggregated into IntervalQA — an auxiliary dataset for calibration (not time-dependent forecasting questions).",
            "prediction_method": "Supervised learning to output a point estimate and multiple nested confidence intervals (fixed confidence levels). Loss components: MSE on point estimate, boundary MSE on intervals that fail to contain the target, and interval-length penalty; labels log-transformed to manage dynamic range.",
            "evaluation_protocol": "Evaluation of calibration via RMS Calibration Error (adaptive binning across label dynamic range) computed over fixed confidence levels (50% to 95%); median point-estimate error (Point Estimate Distance) and median interval length reported.",
            "dataset_or_benchmark": "IntervalQA — ~30,000 numerical questions collected from existing NLP datasets filtered for numerical answers; training/val/test splits (32,200 / 3,443 / 6,170).",
            "results": "Larger DeBERTa-v3 models yielded better calibration and narrower intervals: Point Estimate Distance decreased from 20.8 to 18.1, median interval length from 2072.4 to 305.4, and RMS Calibration Error from 19.1 to 13.5 when scaling from 22M to 304M parameters.",
            "limitations_or_challenges": "IntervalQA is not forecasting future events and thus does not test time-sensitive retrodiction; numerical targets span many orders of magnitude leading to very large predicted intervals and substantial residual calibration error even at largest sizes; transferring interval calibration methods to time-sensitive forecasting tasks (Autocast numerical questions) remains an open challenge due to small number of numerical forecasts in Autocast and distributional differences.",
            "uuid": "e5792.4",
            "source_info": {
                "paper_title": "Forecasting Future World Events with Neural Networks",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Getting gpt-3 to predict metaculus questions",
            "rating": 2
        },
        {
            "paper_title": "ForecastQA: A question answering challenge for event forecasting with temporal text data",
            "rating": 2
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 1
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 1
        }
    ],
    "cost": 0.01339375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Forecasting Future World Events with Neural Networks</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Andy Zou</th>
<th style="text-align: center;">Tristan Xiao</th>
<th style="text-align: center;">Ryan Jia</th>
<th style="text-align: center;">Joe Kwon</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">UC Berkeley</td>
<td style="text-align: center;">UC Berkeley</td>
<td style="text-align: center;">UC Berkeley</td>
<td style="text-align: center;">MIT</td>
</tr>
<tr>
<td style="text-align: center;">Mantas Mazeika</td>
<td style="text-align: center;">Richard Li</td>
<td style="text-align: center;">Dawn Song</td>
<td style="text-align: center;">Jacob Steinhardt</td>
</tr>
<tr>
<td style="text-align: center;">UIUC</td>
<td style="text-align: center;">UC Berkeley</td>
<td style="text-align: center;">UC Berkeley</td>
<td style="text-align: center;">UC Berkeley</td>
</tr>
<tr>
<td style="text-align: center;">Owain Evans</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dan Hendrycks</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">University of Oxford</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">UC Berkeley</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h2>Abstract</h2>
<p>Forecasting future world events is a challenging but valuable task. Forecasts of climate, geopolitical conflict, pandemics and economic indicators help shape policy and decision making. In these domains, the judgment of expert humans contributes to the best forecasts. Given advances in language modeling, can these forecasts be automated? To this end, we introduce Autocast, a dataset containing thousands of forecasting questions and an accompanying news corpus. Questions are taken from forecasting tournaments, ensuring high quality, real-world importance, and diversity. The news corpus is organized by date, allowing us to precisely simulate the conditions under which humans made past forecasts (avoiding leakage from the future). Motivated by the difficulty of forecasting numbers across orders of magnitude (e.g. global cases of COVID-19 in 2022), we also curate IntervalQA, a dataset of numerical questions and metrics for calibration. We test language models on our forecasting task and find that performance is far below a human expert baseline. However, performance improves with increased model size and incorporation of relevant information from the news corpus. In sum, Autocast poses a novel challenge for large language models and improved performance could bring large practical benefits.</p>
<h2>1 Introduction</h2>
<p>Forecasting plays a crucial role in the modern world. Climate forecasts shape the policies of governments and companies (Gillingham et al., 2018). Economic forecasts influence investment and employment (Christensen et al., 2018). In 2020, forecasts about the spread of COVID-19 led to national lockdowns and border closures (Adam, 2020), slowing the spread of the virus. Consequently, machine learning (ML) models that make accurate forecasts across a broad range of topics could enable more informed decision making at scale and improve ML safety (Hendrycks et al., 2021c).</p>
<p>Two main approaches to forecasting are described in the forecasting literature: statistical and judgmental forecasting (Webby and O'Connor, 1996; Armstrong, 2001). In statistical forecasting, forecasts are made by traditional statistical models for time-series prediction such as autoregression (Makridakis et al., 2008) or by ML time-series models (Makridakis et al., 2020; Triebe et al., 2021). Humans create and tune the models but do not tweak individual forecasts. This works well when there are many past observations of the variable being forecast and minimal distribution shift. By contrast, in judgmental forecasting human forecasters use their own judgment to determine forecasts. The forecasters may use statistical models, but often integrate information from various sources including news, accumulated knowledge, and a priori reasoning. This enables forecasting for questions where</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example from the Autocast dataset, including the question, the resolution of the question, and the timeseries of aggregate human expert forecasts (Crowd) from the start date to the time the question resolves. We train a language model to generate forecasts at each timestep, using only news articles available at that timestep (i.e., without allowing any leakage of information from the future).</p>
<p>Past data is scarce or subject to distribution shift (Tetlock and Gardner, 2016). For brevity, we refer to judgmental forecasting as "forecasting" in the rest of the paper.</p>
<p>Because it relies on scarce human expertise, forecasting is only used for a small number of questions. This motivates using ML to automate forecasting, e.g., by automating human information retrieval (finding news sources), reasoning (to decide if some evidence bears on a forecast), and quantitative modeling. ML models may also have some advantages over human forecasters. Models can read through text or data much faster than humans and can discern patterns in noisy high-dimensional data that elude humans. When it comes to learning, humans cannot be trained on past data in manner simulating actual forecasting (e.g., How likely was the Soviet Union's collapse from the viewpoint of 1980?) because they know the outcomes – but past data can be used for ML models.</p>
<p>As a step towards automating human forecasting, we introduce <em>Autocast</em>, a new dataset for measuring ML models' forecasting ability. Autocast includes thousands of forecasting questions collected from human forecasting tournaments. The questions vary in the forecasting horizon from days to decades, in the topic (including politics, economics, and science), and in the answer format (e.g., multiple-choice vs. predicting a number). The questions are pre-selected for public interest, and there is a strong human baseline (the crowd aggregate of many competitive forecasters). The questions in Autocast are about past events (e.g., the US 2020 election) and so ML models could answer them simply by memorizing what happened. To test forecasting ability, we need to simulate the state of information <em>before</em> the past events ("retrodiction"). To this end, we curate a corpus of news items from Common Crawl (Nagel, 2016) that is organized by date. This means a model can be exposed only to news from before the outcomes being forecast, allowing for a rigorous test of retrodiction.</p>
<p>We implement a number of baseline models on Autocast, and demonstrate how language models can be trained on past forecasting questions by retrieving from our news corpus. We find that performance improves with model size and that information retrieval helps. However, all baselines are substantially worse than aggregate human forecasts. On forecasting binary outcomes, the best ML model achieves 65% accuracy vs. 92% for humans (and 50% for random). The same ML model (Raffel et al., 2020) is close to the human ceiling when fine-tuned on other NLP benchmarks (e.g., SQuAD from Rajpurkar et al. (2016)), which shows that Autocast is a challenging, real-world test for ML. Experiment code and the dataset are available at github.com/andyzoujm/autocast.</p>
<h3>Contributions.</h3>
<ol>
<li>We introduce Autocast, a dataset for forecasting that covers diverse topics (e.g., politics, economics, society, science) and varying time horizons.</li>
</ol>
<table>
<thead>
<tr>
<th>Question Summary</th>
<th>Category</th>
<th>Answer Type</th>
<th>Resolution</th>
</tr>
</thead>
<tbody>
<tr>
<td>Will a Tesla car demonstrate fully</td>
<td>Science \&amp; Tech</td>
<td>T/F</td>
<td>No</td>
</tr>
<tr>
<td>autonomous capability before the end of 2021?</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>What will be Putin’s approval rating value 3</td>
<td>Politics</td>
<td>Numerical</td>
<td>83</td>
</tr>
<tr>
<td>months after the potential invasion of Ukraine?</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>When will the US-Canada border reopen?</td>
<td>Social</td>
<td>Numerical</td>
<td>Nov 8, 2021</td>
</tr>
<tr>
<td>How many vacancies will arise on the U.S. Supreme</td>
<td>Economy</td>
<td>MCQ</td>
<td>A</td>
</tr>
<tr>
<td>Court in 2021? (A) 0 (B) 1 (C) 2 (D) 3 or more</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1: Examples from the Autocast dataset. For brevity, we do not depict the full question specification, which often includes context, definitions, and detailed resolution criteria.
2. Part of our dataset is a large news corpus organized by date, allowing us to rigorously evaluate model performance on historical forecasts.
3. We show that forecasting is challenging for current language models, with accuracy and calibration far below a strong human baseline.</p>
<h1>2 Related Work</h1>
<p>Forecasting. A recent experiment (Kirk Bonde, 2022) tested GPT-3 in the few-shot setting on true/false questions collected from Metaculus (one of the sources for Autocast). However, since questions were not filtered by date, some answers would have appeared in GPT-3's training data. Similar to our work, ForecastQA (Jin et al., 2021) is a dataset of forecasting questions that covers a range of topics. However, ForecastQA's questions were written by crowdworkers without forecasting experience. Consequently, the questions are often nonsensical or ambiguous given the lack of additional context, e.g. "To how many people will the Representative of an internet speak to by September 2019?", or "In July 2019, will an article say there were no volunteers in 2016?". We found that a high percentage of ForecastQA questions suffer from these issues. By contrast, our questions were written by experienced forecasters and are always unambiguous given the full question description. Finally, ForecastQA's human baseline was done retrospectively (making it unrealistic) whereas our dataset contains expert human forecasts from real forecasting questions.
Information Retrieval. Information retrieval is crucial for forecasting, as good forecasts depend on up-to-date, specialized information drawn from multiple sources (Tetlock and Gardner, 2016). Recent work has used information retrieval to improve question-answering in large language models (Lewis et al., 2020; Nakano et al., 2021; Shuster et al., 2021) or to address time-sensitive questions (Chen et al., 2021). This has been applied to tasks that are related to forecasting, such as fact checking and truthful question-answering. In forecasting, it is useful to read and compare multiple news articles daily, in order to build an accurate picture of the current state, and then to iterate this process. We</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Autocast contains questions about locations across the world. The questions in the dataset mention over 500 cities, spanning six continents.</p>
<p>Design an architecture for this purpose (albeit with limits on article length and time horizon), drawing inspiration from Wang and McAllester (2020).</p>
<p>Calibration. Calibration is important in forecasting (Tetlock and Gardner, 2016). Even expert forecasters will be highly uncertain about some outcomes of interest. Such forecasts will be more useful in the form of calibrated probabilities than as point estimates. Thus forecasters are evaluated with proper scoring rules, which incentivize calibration. There is an extensive literature on improving the calibration of deep learning models (Guo et al., 2017; Nguyen and O’Connor, 2015; Lin et al., 2022; Minderer et al., 2021; Kull et al., 2019b), mostly for classification with a fixed set of classes. One part of Autocast requires models to forecast continuous quantities varying over multiple orders of magnitude, which has not been explored in prior work.</p>
<p>Truthful question-answering. Current language models often generate falsehoods when answering questions (Shuster et al., 2021; Lin et al., 2021), and they also achieve poor calibration when giving probabilistic answers (Hendrycks et al., 2021a) to human knowledge questions. However, for questions with a known ground truth answer, we expect models to improve as a result of scale, fine-tuning, and information-retrieval from reliable sources (Bai et al., 2022; Nakano et al., 2021; Hadfield-Menell et al., 2016; Turner et al., 2020; Wainwright and Eckersley, 2019). Yet humans also want models to give calibrated and truthful answers to questions that are too difficult or costly for us to answer ourselves (Irving et al., 2018; Evans et al., 2021; Leike et al., 2017; Hendrycks et al., 2021d; Reddy et al., 2020; Nahian et al., 2021). Forecasting is useful for this purpose. Forecasting questions are challenging but eventually become easy to evaluate. By contrast, it may be difficult for humans to evaluate superior answers to open problems in fundamental philosophy or science.</p>
<h1>3 The Autocast Dataset</h1>
<p>Forecasting Questions. We collected all available forecasting questions from three public forecasting tournaments (Metaculus, Good Judgment Open, and CSET Foretell), which resulted in 6,707 questions total. These questions tend to have broad public interest (e.g., national rather than local elections) and clear resolution criteria. Most questions are not already covered well by specialized forecasts (such as weather forecasts). The questions are either true/false, multiple-choice, or involve forecasting a numerical quantity or date (see Table 1 for examples). In these forecasting tournaments, participants begin forecasting a question on a given day (the "start date") and update their forecasts multiple times up until the "close date." At some later time, the forecast is <em>resolved</em> and participants are scored based on all their forecasts. (Note the resolution date is often just after the closing date but not always. The resolution can also happen <em>before</em> the planned closing date: e.g. when forecasting when an event will occur.) Thus the "crowd" forecast (which aggregates over participants) is a time-series of forecasts from the start to close date.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Illustration of our FiD Temporal model. Forecasts are made each day (from start date to resolution) by GPT-2. The input to GPT-2 is the top-1 daily news article retrieved by BM25, which is encoded by FiD Static (a T5 model). In training, GPT-2's target is the average of daily crowd predictions (denoted ' $F_{t}^{c}$ ' for day $t$ ) and the resolved outcome. Like human forecasters, GPT-2 accumulates news information over time and updates its predictions.</p>
<p>Autocast includes the question, the start and close dates, the answer (if the question has resolved), and the time-series of crowd forecasts (Figure 1). Half of the questions have not yet resolved and correspond to ongoing tournaments. Some of these questions concern events decades in the future, requiring reasoning over long time horizons. These questions can still be used as training data by using the crowd forecast as the target (as a high-quality proxy for the ground truth). However, the test set only includes resolved questions. Our dataset also includes metadata that is helpful for forecasting. There is detailed background information about the question (including precise terms of resolution) and also links to relevant information posted by tournament participants. We include more details in the appendix.</p>
<p>Train and test split. It is standard in ML for the test set to be drawn from the same distribution as the train set. However, randomly splitting our questions into train and test without considering the date would not simulate the conditions of forecasting. For example, a test question ("Will Trump win the 2020 election?") could come from an earlier date than a related training question ("Will President Biden pass the stimulus?"). Thus, we split our questions using a date cut-off of mid-2021, which means that questions in the test set resolve from mid-2021 to mid-2022. Note that if a model is pre-trained on data from after mid-2021, this will also not simulate forecasting faithfully. In both train and test sets, we implement dataset balancing for the true/false questions. To flip a label, we negate the question using OpenAI's GPT-3-175B Edit model (Brown et al., 2020) and manually check for correct negation.
Contemporaneous news as context for forecasts. When a human is making a forecast at time $t$, they use past and present $(\leq t)$ information sources but are not exposed to any information from the future $(&gt;t)$. If they forecast again at $t+1$, they will have updated on new information that was generated from $t$ to $t+1$. These conditions can be simulated for ML models by (a) pre-training on text generated before time $t$, and (b) providing the model with new information generated between $t$ and $t+1$. To this end, we provide a corpus of news articles scraped from CommonCrawl news (Nagel, 2016; Hamborg et al., 2017) that is organized by publish date. The articles were derived from diverse sources between 2016 to mid-2022 and total more than 200GB of data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Parameters</th>
<th style="text-align: center;">T/F</th>
<th style="text-align: center;">MCQ</th>
<th style="text-align: center;">Numerical</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">18.8</td>
</tr>
<tr>
<td style="text-align: center;">UnifiedQA</td>
<td style="text-align: center;">0.2B</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">19.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.8B</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.8B</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">0.2B</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.8B</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.8B</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FiD Static</td>
<td style="text-align: center;">0.2B</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">37.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.8B</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.8B</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FiD Temporal</td>
<td style="text-align: center;">0.6B</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">37.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.5B</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4.3B</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Human Crowd</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">82.5</td>
</tr>
</tbody>
</table>
<p>Table 2: Model accuracy on the Autocast dataset for each question type: true/false (T/F), multiplechoice question (MCQ), and numerical (Numerical). For Numerical, lower is better. For other metrics, higher is better. The model FiD Static (based on T5) retrieves the top 10 news articles over the period, while FiD Temporal (based on GPT-2 with T5 encoder) retrieves the top 1 article each day. Averaging over all model sizes, we find that the FiD Temporal achieves the best average.</p>
<h1>3.1 Dataset Analysis</h1>
<p>Distribution of Questions. The questions in Autocast cover a very wide variety of topics. We divide the questions into five main categories: Economy, Politics, Science, Social, and Other. Each category contains numerous subcategories for a total of 44 subcategories ranging from foreign policy to AI. We list all subcategories in the Supplementary Material. The questions also cover a wide geographical distribution, as shown in Figure 3. Overall, Autocast tests both breadth of subject matter and depth (since questions ask for quantitative predictions about a specific, operationalized variable).
Adding new questions over time. The number of questions submitted to forecasting platforms is rapidly increasing (Figure 2). If trends continue, in two years there will be twice as many questions available. Autocast is a living dataset and will be updated periodically with new questions. This will provide both more data for training and a new set of test questions (to assess overfitting).
Human forecasts. The human crowd forecast for a given question becomes more accurate from the start to closing date, as shown in Figure 6. This is what we would expect if humans are updating their forecasts as more information comes out. In contrast to most ML benchmarks, the human crowd judgments are probabilistic. This allows us to evaluate their calibration. In the Supplementary Material, we show that crowd forecasts are well-calibrated.
Distribution shift. We expect a distribution shift over time in both the questions being asked and in the answers. For example, there will be fewer questions about Ukraine before 2022. This distribution shift is inherent to forecasting and so it is crucial that models can manage it. We do find a shift in the distribution of question categories. For example, the number of questions in the Social category increased from $12.6 \%$ in the training set to $28.2 \%$ in the test set, possibly due the Covid-19 pandemic (which is included in Social).</p>
<h2>4 Experiments</h2>
<h3>4.1 Baselines</h3>
<p>The Crowd baseline uses the final aggregate human forecast before the closing date. The Random baseline uses the analytically computed random accuracy for true/false and multiple-choice questions.</p>
<p>For numerical questions, random predictions are sampled uniformly from the bounded range of possible answers specified in each question.</p>
<p>Models without retrieval. We evaluate UnifiedQA-v2 (Khashabi et al., 2022) and T5 (Raffel et al., 2020) models of various sizes. These models are trained on a variety of tasks, enabling strong generalization on many unseen language tasks. Using zero-shot prompting for UnifiedQA, we report results on classification questions. The UnifiedQA models were not trained on numerical questions, hence, we report random performance to enable comparison with other baselines. T5 is fine-tuned using its original output head for true/false and multiple-choice questions. To output numerical answers with T5, we add an additional linear output head.</p>
<p>Retrieval-Based Methods. We investigate whether retrieval models can improve performance by selecting relevant articles from the news corpus included with Autocast. Importantly, news articles after the close time or resolution time of a question are not available for retrieval, so retrieved articles only include information about the past. For all retrieval methods, we use Fusion-in-Decoder or FiD (Izacard and Grave, 2021) to encode articles retrieved by BM25 (Robertson et al., 1994; Thakur et al., 2021) with cross-encoder reranking. FiD uses T5 to encode retrieved passages along with the question and can be viewed as a minimal extension of T5 for incorporating retrieval. We truncate retrieved articles to a maximum length of 512 tokens.</p>
<p>The FiD Static baseline uses the top 10 retrieved articles after reranking, which is the standard method for retrieval-augmented prediction. The FiD Temporal baseline leverages the intermediate crowd predictions (before the question is resolved) as auxiliary supervision. The intuition is that crowd predictions will change based on rational incorporation of new evidence, and these updates will not be captured by just training on the final outcome. For each day between the question’s open and close date, we generate an embedding of the top news article using the frozen fine-tuned FiD Static model. These embeddings are then treated as input embeddings to an autoregressive model (GPT-2 (Radford et al., 2019)), which is fine-tuned to predict the average of the daily crowd prediction and the ground truth outcome. We illustrate this method in Figure 4. Figure 1 shows predictions from an FiD Temporal model over time for an example question.</p>
<h3>4.2 Metrics</h3>
<p>For true/false and multiple-choice questions, we evaluate models using percent accuracy. For numerical questions, we use $\ell_{1}$ distance, bounded between 0% and 100%. We denote these question types as T/F, MCQ, and Numerical, respectively. To evaluate aggregate performance, we use a combined Score metric (T/F + MCQ - Numerical)/2, which has an upper bound of 100%. A score of 100% indicates perfect prediction on all three question types. Note that since numerical question responses are normalized between 0% and 100%, the combined Score metric is lower-bounded at -50%. We also report the Average score, which averages the combined metric of all model sizes.</p>
<h3>4.3 Forecasting Evaluation</h3>
<p>Setup. We fine-tune the T5 baseline for 10 epochs with a batch size of 8, an initial learning rate of 5 × 10⁻⁵ with linear decay schedule, and a weight decay of 1 × 10⁻². The maximum sequence length of the T5 model is set to 512. We train FiD Temporal models for 5 epochs with a constant learning rate of 5 × 10⁻⁵. Hyperparameters are selected based on early experiments. Additional details are in the Supplementary Material.</p>
<p>Results. We show results in Table 2. Although UnifiedQA-v2 obtains strong performance on various natural language benchmarks, it obtains close to random zero-shot performance on Autocast, showing the difficulty of forecasting. Fine-tuned T5 performs better, but multiple-choice accuracy is still at nearly random chance levels. Retrieval-based methods substantially outperform both UnifiedQA-v2 and T5, showing a relative increase in the Average score of $93 \%$ and $15 \%$, respectively. Moreover, retrieval-based methods become more effective as parameter count increases, which suggests that the models learn to extract relevant information from retrieved articles.</p>
<p>Comparing the FiD Static and FiD Temporal baselines, we see that the Average score is slightly higher for FiD Temporal. However, the largest FiD Static model has the highest individual score. Thus, our temporal training strategy for incorporating the auxiliary crowd predictions neither harms nor helps compared to the static retrieval baseline. Future work could develop more effective ways of using these auxiliary training signals.</p>
<h1>4.4 Model Analysis</h1>
<p>Relevance of Retrieved Articles. We find that the retrieved articles are often highly relevant to the question. In Figure 5, we show examples of articles retrieved by BM25 from the news corpus in Autocast. Baseline models have access to the article text, but for brevity we only show the article title. The articles give information that is clearly relevant for making an informed forecast. Note that the T5 backbone for the baselines was pre-trained on data from before 2020, far before the timeline of the question, so retrieval provides vital information that models would not otherwise have. This suggests that large improvements on Autocast could come from integrating information from retrieved articles more effectively. We expect that more sophisticated retrieval methods would also improve performance, although efficiency becomes a concern when using large retrieval methods.
Detailed Performance. In the Supplementary Material, we show the performance of baseline methods on a more granular level. The per-
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: For the crowd and an ensemble of the two largest FiD Temporal models, prediction score increases as the resolution date grows nearer. This trend may be due to more relevant information becoming available over time, which the model can access through retrieval from the news corpus.
category results indicate that Science \&amp; Technology is the most challenging category for models, whereas human forecasters have relatively consistent performance across categories. Inspecting the subset of questions that have been active for at least two months, we also find that the accuracy of the human crowd forecast and a model ensemble steadily increases over time up to the resolution date (Figure 6). This is to be expected, as more information about the eventual outcome is available closer to the time (e.g. election polls become more accurate). For this plot, we show an ensemble of the two largest FiD Temporal models, which has slightly higher final performance than the individual models and a clearer trend over time.</p>
<h2>5 Calibrated Prediction of Numerical Quantities</h2>
<p>In our results, we evaluate baselines on the accuracy of their point estimates, rather than their calibration. However, the eventual goal for Autocast is for models to achieve good calibration as well as accuracy. Here we describe an auxiliary dataset that helps with this goal for the challenging case of calibration on numerical questions.</p>
<p>The IntervalQA Dataset. In the Autocast training set, numerical quantities range over many orders of magnitude. Furthermore, Autocast has fewer than 1,000 numerical training questions. This problem of making calibrated predictions for quantities over many orders of magnitude using text inputs has not been addressed in work on calibration for language models. To this end, we curate</p>
<table>
<thead>
<tr>
<th>Parameters</th>
<th>Point Estimate Distance</th>
<th>Conf. Interval Length</th>
<th>RMS Calibration Error</th>
</tr>
</thead>
<tbody>
<tr>
<td>22M</td>
<td>20.8</td>
<td>2072.4</td>
<td>19.1</td>
</tr>
<tr>
<td>44M</td>
<td>20.3</td>
<td>1115.7</td>
<td>16.6</td>
</tr>
<tr>
<td>86M</td>
<td>19.6</td>
<td>763.1</td>
<td>16.9</td>
</tr>
<tr>
<td>304M</td>
<td>18.1</td>
<td>305.4</td>
<td>13.5</td>
</tr>
</tbody>
</table>
<p>Table 3: Results for DeBERTa-v3 models trained to output confidence intervals on our dataset of numerical predictions. The high dynamic range of the targets leads to large confidence intervals, but median interval size decreases with larger models as does RMS Calibration Error.</p>
<p>IntervalQA, an auxiliary dataset of numerical estimation problems and provide metrics to measure calibration. The problems in the dataset are not forecasting problems but instead involve giving calibrated predictions for fixed numerical quantities. The questions were sourced from NLP datasets covering diverse topics and with answers varying across orders of magnitude: SQuAD, 80K Hours Calibration (80k, 2013), Eighth Grade Arithmetic (Cobbe et al., 2021), TriviaQA (Joshi et al., 2017), Jeopardy, MATH (Hendrycks et al., 2021b), and MMLU (Hendrycks et al., 2021a). We filtered these datasets for questions with numerical answers, which yielded about 30,000 questions.</p>
<h3>5.1 Metrics</h3>
<p>We evaluate whether confidence intervals are calibrated. Concretely, if a method outputs $80\%$ confidence intervals on each test example, we would like the true prediction target to fall inside of these intervals $80\%$ of the time. Additionally, we would like for models to be calibrated across their entire dynamic range of outputs. To measure this, we compute <em>RMS Calibration Error</em> similarly to Nguyen and O’Connor (2015) and Hendrycks et al. (2019), but with fixed confidence levels $c \in{50 \%, 55 \%, \ldots, 95 \%}$ and such that calibration is sensitive to dynamic range. We describe this metric in detail in the Supplementary Material. Low RMS Calibration Error indicates that models are calibrated across their entire dynamic range. We also compute the median prediction error between the predicted point estimate and the ground-truth target (Point Estimate Distance) and the median interval length averaged across all confidence levels (Conf. Interval Length).</p>
<h3>5.2 Experiments</h3>
<p>We fine-tune DeBERTa-v3 models (He et al., 2020) to predict a point estimate and a set of confidence intervals corresponding to the confidence levels in the RMS calibration error metric. On a high level, we use a loss with three components: (1) MSE loss between the predicted point estimate and the ground-truth target, (2) MSE loss between the boundaries of the predicted confidence intervals and the ground-truth target for boundaries that are on the wrong side of the target, (3) a penalty on the length of the predicted intervals to encourage finer predictions. The models are trained for 5 epochs with a batch size of 100. A detailed description is in the Supplementary Material. We show results in Table 3. All three metrics decrease with model size.</p>
<h2>6 Conclusion</h2>
<p>We introduced Autocast, a dataset for measuring the ability of neural networks to forecast future world events. The dataset contains thousands of forecasting questions from public forecasting tournaments, including ground truth outcomes and aggregated human predictions. We also curated a large corpus of news items from the Common Crawl news corpus, enabling rigorous evaluations without information leakage. We evaluated numerous baseline algorithms and demonstrated that model size and information retrieval can improve forecasting performance. To better evaluate calibration for numerical prediction, we introduced IntervalQA, a large collection of numerical prediction questions with a wide dynamic range of prediction targets, and evaluated state-of-the-art language models. Our results show significant room for future improvement.</p>
<h1>References</h1>
<p>80k hours calibration, 2013. URL https://80000hours.org/calibration-training/.
David Adam. Modelling the pandemic the simulations driving the world's response to covid-19. Nature, 580(7803):316-318, 2020.</p>
<p>Jon Scott Armstrong. Principles of forecasting: a handbook for researchers and practitioners, volume 30. Springer, 2001.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Wenhu Chen, Xinyi Wang, and William Yang Wang. A dataset for answering time-sensitive questions. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.</p>
<p>Peter Christensen, Kenneth Gillingham, and William Nordhaus. Uncertainty in forecasts of long-run economic growth. Proceedings of the National Academy of Sciences, 115(21):5409-5414, 2018.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, and William Saunders. Truthful ai: Developing and governing ai that does not lie. arXiv preprint arXiv:2110.06674, 2021.</p>
<p>Kenneth Gillingham, William Nordhaus, David Anthoff, Geoffrey Blanford, Valentina Bosetti, Peter Christensen, Haewon McJeon, and John Reilly. Modeling uncertainty in integrated assessment of climate change: A multimodel comparison. Journal of the Association of Environmental and Resource Economists, 5(4):791-826, 2018.</p>
<p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning, pages 1321-1330. PMLR, 2017.</p>
<p>Dylan Hadfield-Menell, S. Russell, P. Abbeel, and A. Dragan. Cooperative inverse reinforcement learning. In NIPS, 2016.</p>
<p>Felix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. news-please: A generic news crawler and extractor. In Proceedings of the 15th International Symposium of Information Science, pages 218-223, March 2017. doi: 10.5281/zenodo. 4120316.</p>
<p>Sven Ove Hansson. Fallacies of risk. Journal of Risk Research, 2004.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced BERT with disentangled attention. CoRR, abs/2006.03654, 2020.</p>
<p>James Hedlund. Risky business: safety regulations, risk compensation, and individual behavior. Injury Prevention, 2000.</p>
<p>Dan Hendrycks and Mantas Mazeika. X-risk analysis for ai research. arXiv preprint arXiv:2206.05862, 2022.</p>
<p>Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. Proceedings of the International Conference on Learning Representations, 2019.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021a.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021b.</p>
<p>Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ML safety. arXiv preprint, 2021c.</p>
<p>Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ml safety. arXiv, 2021d.</p>
<p>Geoffrey Irving, Paul Christiano, and Dario Amodei. Ai safety via debate. arXiv preprint arXiv:1805.00899, 2018.</p>
<p>Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In EACL, 2021.</p>
<p>Woojeong Jin, Rahul Khanna, Suji Kim, Dong-Ho Lee, Fred Morstatter, Aram Galstyan, and Xiang Ren. ForecastQA: A question answering challenge for event forecasting with temporal text data. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4636-4650. Association for Computational Linguistics, August 2021. doi: 10.18653/v1/2021.acl-long. 357.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. CoRR, abs/1705.03551, 2017.</p>
<p>Daniel Khashabi, Yeganeh Kordi, and Hannaneh Hajishirzi. Unifiedqa-v2: Stronger generalization via broader cross-format training. arXiv preprint arXiv:2202.12359, 2022.</p>
<p>Mathias Kirk Bonde. Getting gpt-3 to predict metaculus questions, 2022. https : //www. lesswrong. com/posts/c3cQgBN3v2Cxpe2kc/getting-gpt-3-to-predict-metaculus-questions, Last accessed on 2022-06-08.</p>
<p>Meelis Kull, Miquel Perelló-Nieto, Markus Kängsepp, Telmo de Menezes e Silva Filho, Hao Song, and Peter A. Flach. Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with dirichlet calibration. In NeurIPS, 2019a.</p>
<p>Meelis Kull, Miquel Perello Nieto, Markus Kängsepp, Telmo Silva Filho, Hao Song, and Peter Flach. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration. Advances in neural information processing systems, 32, 2019b.
J. Leike, Miljan Martic, Victoria Krakovna, Pedro A. Ortega, Tom Everitt, Andrew Lefrancq, Laurent Orseau, and S. Legg. Ai safety gridworlds. ArXiv, abs/1711.09883, 2017.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: $9459-9474,2020$.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334, 2022.</p>
<p>Spyros Makridakis, Steven C Wheelwright, and Rob J Hyndman. Forecasting methods and applications. John wiley \&amp; sons, 2008.</p>
<p>Spyros Makridakis, Rob J Hyndman, and Fotios Petropoulos. Forecasting in social settings: The state of the art. International Journal of Forecasting, 36(1):15-28, 2020.</p>
<p>Barbara Mellers, Eric Stone, Pavel Atanasov, Nick Rohrbaugh, S Emlen Metz, Lyle Ungar, Michael M Bishop, Michael Horowitz, Ed Merkle, and Philip Tetlock. The psychology of intelligence analysis: drivers of prediction accuracy in world politics. Journal of experimental psychology: applied, 21 (1):1, 2015 .</p>
<p>Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. Advances in Neural Information Processing Systems, 34, 2021.</p>
<p>Sebastian Nagel. Common crawl news dataset, 2016. URL https://data.commoncrawl.org/ crawl-data/CC-NEWS/index.html.</p>
<p>Md Sultan Al Nahian, Spencer Frazier, Brent Harrison, and Mark Riedl. Training value-aligned reinforcement learning agents using a normative prior. arXiv preprint arXiv:2104.09469, 2021.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</p>
<p>Khanh Nguyen and Brendan O'Connor. Posterior calibration and exploratory analysis for natural language processing models. arXiv preprint arXiv:1508.05154, 2015.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.</p>
<p>Siddharth Reddy, Anca Dragan, Sergey Levine, Shane Legg, and Jan Leike. Learning human objectives by evaluating hypothetical behavior. In International Conference on Machine Learning, pages 8020-8029. PMLR, 2020.</p>
<p>Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. Okapi at trec-3. In TREC, 1994.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021.</p>
<p>Nassim Nicholas Taleb and Philip E. Tetlock. On the difference between binary prediction and true exposure with implications for forecasting tournaments and decision making research. 2013.</p>
<p>Nassim Nicholas Taleb, Yaneer Bar-Yam, and Pasquale Cirillo. On single point forecasts for fat-tailed variables. International Journal of Forecasting, 2020.</p>
<p>Philip E Tetlock and Dan Gardner. Superforecasting: The art and science of prediction. Random House, 2016.</p>
<p>Philip E. Tetlock, Yunzi Lu, and Barbara A. Mellers. False dichotomy alert: Improving subjectiveprobability estimates vs. raising awareness of systemic risk. International Journal of Forecasting, 2022.</p>
<p>Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.</p>
<p>Oskar Triebe, Hansika Hewamalage, Polina Pilyugina, Nikolay Laptev, Christoph Bergmeir, and Ram Rajagopal. Neuralprophet: Explainable forecasting at scale. arXiv preprint arXiv:2111.15397, 2021.</p>
<p>Alex Turner, Neale Ratzlaff, and Prasad Tadepalli. Avoiding side effects in complex environments. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 21406-21415. Curran Associates, Inc., 2020.</p>
<p>Kailas Vodrahalli, Tobias Gerstenberg, and James Zou. Uncalibrated models can improve human-ai collaboration. ArXiv, abs/2202.05983, 2022.</p>
<p>Carroll L Wainwright and Peter Eckersley. Safelife 1.0: Exploring side effects in complex environments. arXiv preprint arXiv:1912.01217, 2019.</p>
<p>Hai Wang and David McAllester. On-the-fly information retrieval augmentation for language models. arXiv preprint arXiv:2007.01528, 2020.</p>
<p>Richard Webby and Marcus O’Connor. Judgemental and statistical time series forecasting: a review of the literature. International Journal of forecasting, 12(1):91-118, 1996.</p>
<p>Shengjia Zhao, Tengyu Ma, and Stefano Ermon. Individual calibration with randomized forecasting. In International Conference on Machine Learning, pages 11387-11397. PMLR, 2020.</p>
<h1>A Additional Experimental Details and Results</h1>
<h2>A. 1 Autocast Experiments</h2>
<p>Calibration Results. In Figure 8, we show the adaptive binning calibration curve for crowd forecasts on all resolved true/false questions by plotting the fraction of positives against the model's predicted probability for the positive class.
Additionally, we can compare the calibration of our static and temporal models to crowd performance on the resolved test set. Treating true/false questions as two-class classification problems and combining them with multiple-choice questions, we calculate adaptive binning calibration error with a bin size of 50 samples. The largest FiD Static model incurs a $40 \%$ calibration error while the human crowd incurs a much smaller $8 \%$ calibration error. By leveraging crowd predictions in our FiD Temporal models, we reduce the calibration error to $17 \%$, showing potential for improvements.</p>
<p>Model and Training Loss. The FiD Temporal model uses three separate linear heads after its hidden state outputs to answer each type of questions (true/false, multiple-choice, and numerical). In particular, the multiple-choice head has 12 outputs which is the maximum number of choices in the training set. Additionally, the original input embeddings are replaced with a linear layer to map from the FiD Static's hidden states to the GPT-2's hidden states. Finally, to make training more stable, we average the loss over the sequence of predictions for each question to weigh the questions evenly. Moreover, the losses of the three types of questions are normalized by their respective baseline loss (uniformly random predictions) before summing together so that their losses are on the same scale.</p>
<p>Retrieval from CC-NEWS. Given a question, for each day the question is active, we retrieve the top 10 relevant news articles from the daily articles. In our FiD-Temporal experiments, we only use the top 1 from every day. Then, we aggregate all these articles from different dates and rank them according to the retrieval score. The top 10 articles are used for the FiD-Static model. We follow the Terms of Use for the Common Crawl website. The dataset is fully reproducible with the script to download and filter CC-NEWS on GitHub.</p>
<h2>A. 2 Confidence Intervals</h2>
<p>Interval Construction. In the reference implementation of the get_confidence_intervals function in Figure 7, we construct our intervals by first producing a point estimate for each question and iteratively adding on non-negative, non-symmetric deltas on both sides, so that the intervals become nested and wider for higher confidence levels.</p>
<p>Training Loss for Baseline. First, because the labels span a large numerical range, we normalize them by taking the $l o g$. Then, we construct a loss with three components shown in Figure 7: (1) $\mathcal{L}<em b="b">{p}$ : MSE loss between the predicted point estimate and the ground-truth target, (2) $\mathcal{L}</em>}$ : MSE loss between the boundaries of the predicted confidence intervals and the ground-truth target for boundaries that are on the wrong side of the target, (3) $\mathcal{L<em b="b">{i}$ : a penalty on the length of the predicted intervals to encourage finer predictions. Based on whether the ratio of true labels contained in the predicted intervals is higher than the target confidence level, we either activate the boundary loss $\mathcal{L}</em>$ for that particular confidence level output. Lastly, the three components are weighted by coefficients $1,1,0.01$ chosen with a simple search using the validation set.}$ or the interval length loss $\mathcal{L}_{i</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Resolved</th>
<th style="text-align: center;">Unresolved</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">2815</td>
<td style="text-align: center;">1375</td>
<td style="text-align: center;">4190</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4411</td>
<td style="text-align: center;">1974</td>
<td style="text-align: center;">6385</td>
</tr>
<tr>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">907</td>
<td style="text-align: center;">1610</td>
<td style="text-align: center;">2517</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1292</td>
<td style="text-align: center;">2305</td>
<td style="text-align: center;">3597</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">3722</td>
<td style="text-align: center;">2985</td>
<td style="text-align: center;">6707</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5703</td>
<td style="text-align: center;">4279</td>
<td style="text-align: center;">9982</td>
</tr>
</tbody>
</table>
<p>Table 4: The number of forecasting questions in Autocast. In total, there are nearly 10,000 questions. Gray text indicates the number of questions after augmenting true/false questions with their negations, a procedure we use to balance the dataset.</p>
<p>Adaptive RMS Metric. An important task for numerical forecasting is outputting calibrated uncertainty estimates. However, a unique challenge in this setting is that answers can vary across many orders of magnitude. To evaluate the calibration of confidence intervals across a large dynamic range of output values, we design a specialized local calibration metric Zhao et al., 2020; Kull et al.,</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Is</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="p">...,</span> <span class="mf">0.95</span><span class="p">]</span>
<span class="s s-Atom">num_intervals</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="nv">Is</span><span class="p">)</span>
<span class="s s-Atom">def</span> <span class="nf">low_containment_mask</span><span class="p">(</span><span class="s s-Atom">lowers</span><span class="p">,</span> <span class="s s-Atom">uppers</span><span class="p">,</span> <span class="s s-Atom">labels</span><span class="p">,</span> <span class="nv">Is</span><span class="p">)</span><span class="o">:</span>
    <span class="s s-Atom">#</span> <span class="s s-Atom">lowers</span><span class="p">,</span> <span class="s s-Atom">uppers</span><span class="p">:</span> <span class="nv">Predicted</span> <span class="s s-Atom">lower</span> <span class="s s-Atom">and</span> <span class="s s-Atom">upper</span> <span class="s s-Atom">bounds</span> <span class="s s-Atom">of</span> <span class="s s-Atom">intervals</span>
    <span class="s s-Atom">#</span> <span class="nv">Is</span><span class="o">:</span> <span class="nv">Target</span> <span class="s s-Atom">confidence</span> <span class="s s-Atom">levels</span>
    <span class="s s-Atom">#</span> <span class="nv">Returns</span><span class="o">:</span> <span class="nv">A</span> <span class="s s-Atom">list</span> <span class="s s-Atom">of</span> <span class="s s-Atom">boolean</span> <span class="nb">val</span><span class="s s-Atom">ues</span> <span class="s s-Atom">indicating</span> <span class="s s-Atom">which</span> <span class="s s-Atom">confidence</span> <span class="s s-Atom">level</span>
    <span class="s s-Atom">#</span> <span class="s s-Atom">has</span> <span class="s s-Atom">containment</span> <span class="s s-Atom">ratio</span> <span class="s s-Atom">below</span> <span class="s s-Atom">the</span> <span class="s s-Atom">target</span> <span class="s s-Atom">level</span> <span class="s s-Atom">within</span> <span class="s s-Atom">batch</span>
    <span class="s s-Atom">contained</span> <span class="o">=</span> <span class="p">(</span><span class="s s-Atom">lowers</span> <span class="s s-Atom">&lt;=</span> <span class="s s-Atom">labels</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="s s-Atom">labels</span> <span class="s s-Atom">&lt;=</span> <span class="s s-Atom">uppers</span><span class="p">)</span>
    <span class="s s-Atom">ratio_contained</span> <span class="o">=</span> <span class="s s-Atom">contained</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="s s-Atom">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="s s-Atom">return</span> <span class="s s-Atom">ratio_contained</span> <span class="o">&lt;</span> <span class="nv">Is</span>
<span class="s s-Atom">def</span> <span class="nf">get_confidence_intervals</span><span class="p">(</span><span class="s s-Atom">logits</span><span class="p">)</span><span class="o">:</span>
    <span class="s s-Atom">#</span> <span class="s s-Atom">logits</span><span class="p">:</span> <span class="nv">Model</span> <span class="s s-Atom">output</span> <span class="nf">with</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="s s-Atom">num_intervals</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="s s-Atom">neurons</span>
    <span class="s s-Atom">deltas</span><span class="p">,</span> <span class="s s-Atom">point_estimates</span> <span class="o">=</span> <span class="nf">softplus</span><span class="p">(</span><span class="s s-Atom">logits</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="o">:-</span><span class="mi">1</span><span class="p">]),</span> <span class="s s-Atom">logits</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="o">:</span><span class="p">]</span>
    <span class="s s-Atom">lower_deltas</span> <span class="o">=</span> <span class="s s-Atom">deltas</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="o">:</span><span class="s s-Atom">num_intervals</span><span class="p">]</span>
    <span class="s s-Atom">higher_deltas</span> <span class="o">=</span> <span class="s s-Atom">deltas</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="s s-Atom">num_intervals</span><span class="o">:</span><span class="p">]</span>
    <span class="s s-Atom">interval_lengths</span> <span class="o">=</span> <span class="s s-Atom">lower_deltas</span> <span class="o">+</span> <span class="s s-Atom">higher_deltas</span>
    <span class="s s-Atom">#</span> <span class="s s-Atom">custom</span> <span class="s s-Atom">cumsum</span> <span class="s s-Atom">with</span> <span class="s s-Atom">gradients</span> <span class="s s-Atom">accumulated</span> <span class="s s-Atom">once</span> <span class="s s-Atom">on</span> <span class="s s-Atom">each</span> <span class="s s-Atom">delta</span>
    <span class="s s-Atom">lower_deltas</span> <span class="o">=</span> <span class="s s-Atom">utils</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="s s-Atom">lower_deltas</span><span class="p">)</span>
    <span class="s s-Atom">higher_deltas</span> <span class="o">=</span> <span class="s s-Atom">utils</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="s s-Atom">higher_deltas</span><span class="p">)</span>
    <span class="s s-Atom">lowers</span> <span class="o">=</span> <span class="s s-Atom">point_estimates</span> <span class="o">-</span> <span class="s s-Atom">lower_deltas</span>
    <span class="s s-Atom">uppers</span> <span class="o">=</span> <span class="s s-Atom">point_estimates</span> <span class="o">+</span> <span class="s s-Atom">higher_deltas</span>
    <span class="s s-Atom">return</span> <span class="s s-Atom">lowers</span><span class="p">,</span> <span class="s s-Atom">uppers</span><span class="p">,</span> <span class="s s-Atom">point_estimates</span><span class="p">,</span> <span class="s s-Atom">interval_lengths</span>
<span class="s s-Atom">out</span> <span class="o">=</span> <span class="nf">get_confidence_intervals</span><span class="p">(</span><span class="s s-Atom">logits</span><span class="p">)</span>
<span class="s s-Atom">lowers</span><span class="p">,</span> <span class="s s-Atom">uppers</span><span class="p">,</span> <span class="s s-Atom">point_estimates</span><span class="p">,</span> <span class="s s-Atom">interval_lengths</span> <span class="o">=</span> <span class="s s-Atom">out</span>
<span class="s s-Atom">\</span><span class="nv">Leftrightarrow_</span><span class="p">{</span><span class="s s-Atom">p</span><span class="p">}</span><span class="s s-Atom">=\mathrm</span><span class="p">{</span><span class="nv">MSE</span><span class="p">}(</span><span class="s s-Atom">\text</span> <span class="p">{</span> <span class="s s-Atom">point_estimates</span><span class="p">,</span> <span class="s s-Atom">labels</span> <span class="p">})</span>
<span class="s s-Atom">l_mask</span> <span class="o">=</span> <span class="s s-Atom">lowers</span> <span class="o">&gt;</span> <span class="s s-Atom">labels</span>
<span class="s s-Atom">u_mask</span> <span class="o">=</span> <span class="s s-Atom">uppers</span> <span class="o">&lt;</span> <span class="s s-Atom">labels</span>
<span class="s s-Atom">\</span><span class="nv">Leftrightarrow_</span><span class="p">{</span><span class="s s-Atom">b</span><span class="p">}</span><span class="s s-Atom">=\mathrm</span><span class="p">{</span><span class="nv">MSE</span><span class="p">(</span><span class="s s-Atom">lowers</span><span class="p">,</span> <span class="s s-Atom">labels</span><span class="p">)}</span> <span class="o">*</span> <span class="mi">1</span><span class="k">_</span><span class="s s-Atom">mask</span> <span class="o">+</span> <span class="nv">MSE</span><span class="p">(</span><span class="s s-Atom">uppers</span><span class="p">,</span> <span class="s s-Atom">labels</span><span class="p">)</span> <span class="o">*</span> <span class="s s-Atom">u_mask</span>
<span class="s s-Atom">\</span><span class="nv">Leftrightarrow_</span><span class="p">{</span><span class="s s-Atom">i</span><span class="p">}</span><span class="s s-Atom">=\mathrm</span><span class="p">{</span><span class="nv">MSE</span><span class="p">(</span><span class="s s-Atom">interval_lengths</span><span class="p">,</span> <span class="mi">0</span><span class="p">)}</span>
<span class="s s-Atom">#</span> <span class="s s-Atom">normalize</span> <span class="s s-Atom">loss</span> <span class="s s-Atom">by</span> <span class="s s-Atom">the</span> <span class="s s-Atom">label</span> <span class="s s-Atom">magnitude</span><span class="p">,</span> <span class="s s-Atom">adjusting</span> <span class="s s-Atom">for</span> <span class="s s-Atom">small</span> <span class="s s-Atom">labels</span>
<span class="s s-Atom">\</span><span class="nv">Leftrightarrow_</span><span class="p">{</span><span class="s s-Atom">p</span><span class="p">}</span> <span class="s s-Atom">/=</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="nf">abs</span><span class="p">(</span><span class="s s-Atom">labels</span><span class="p">))</span>
<span class="s s-Atom">\</span><span class="nv">Leftrightarrow_</span><span class="p">{</span><span class="s s-Atom">b</span><span class="p">}</span> <span class="s s-Atom">/=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="nf">abs</span><span class="p">(</span><span class="s s-Atom">labels</span><span class="p">))</span>
<span class="s s-Atom">\</span><span class="nv">Leftrightarrow_</span><span class="p">{</span><span class="s s-Atom">i</span><span class="p">}</span> <span class="s s-Atom">/=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="nf">abs</span><span class="p">(</span><span class="s s-Atom">labels</span><span class="p">))</span>
<span class="s s-Atom">#</span> <span class="s s-Atom">activate</span> <span class="s s-Atom">loss</span> <span class="s s-Atom">for</span> <span class="s s-Atom">particular</span> <span class="s s-Atom">confidence</span> <span class="s s-Atom">levels</span> <span class="s s-Atom">based</span> <span class="s s-Atom">on</span> <span class="s s-Atom">ci_mask</span>
<span class="s s-Atom">ci_mask</span> <span class="o">=</span> <span class="nf">low_containment_mask</span><span class="p">(</span><span class="s s-Atom">lowers</span><span class="p">,</span> <span class="s s-Atom">uppers</span><span class="p">,</span> <span class="s s-Atom">labels</span><span class="p">,</span> <span class="nv">Is</span><span class="p">)</span>
<span class="s s-Atom">\</span><span class="nv">Leftrightarrow_</span><span class="p">{</span><span class="s s-Atom">b</span><span class="p">}</span><span class="s s-Atom">=\</span><span class="nv">Leftrightarrow_</span><span class="p">{</span><span class="s s-Atom">b</span><span class="p">}</span> <span class="p">.</span> <span class="s s-Atom">\text</span> <span class="p">{</span> <span class="s s-Atom">mean</span> <span class="p">}(</span><span class="s s-Atom">\text</span> <span class="p">{</span> <span class="s s-Atom">dim</span><span class="o">=</span><span class="mi">0</span> <span class="p">})</span> <span class="o">*</span> <span class="s s-Atom">\text</span> <span class="p">{</span> <span class="s s-Atom">ci_mask</span><span class="p">}</span>
<span class="s s-Atom">\</span><span class="nv">Leftrightarrow_</span><span class="p">{</span><span class="s s-Atom">i</span><span class="p">}</span><span class="s s-Atom">=\</span><span class="nv">Leftrightarrow_</span><span class="p">{</span><span class="s s-Atom">i</span><span class="p">}</span> <span class="p">.</span> <span class="s s-Atom">\text</span> <span class="p">{</span> <span class="s s-Atom">mean</span> <span class="p">}(</span><span class="s s-Atom">\text</span> <span class="p">{</span> <span class="s s-Atom">dim</span><span class="o">=</span><span class="mi">0</span> <span class="p">})</span> <span class="s s-Atom">*</span><span class="p">(</span><span class="mi">1</span><span class="s s-Atom">-\text</span> <span class="p">{</span> <span class="s s-Atom">ci_mask</span> <span class="p">})</span>
<span class="s s-Atom">\alpha</span><span class="p">,</span> <span class="s s-Atom">β</span><span class="p">,</span> <span class="s s-Atom">γ</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span> <span class="s s-Atom">#</span> <span class="s s-Atom">hyperparameters</span>
<span class="s s-Atom">loss</span> <span class="o">=</span> <span class="s s-Atom">α</span> <span class="o">*</span> <span class="p">}</span> <span class="s s-Atom">\mathcal</span><span class="p">{</span><span class="nv">L</span><span class="p">}</span><span class="k">_</span><span class="p">{</span><span class="s s-Atom">p</span><span class="p">}</span> <span class="s s-Atom">\cdot</span> <span class="s s-Atom">\text</span> <span class="p">{</span> <span class="s s-Atom">mean</span> <span class="p">}()</span><span class="o">+</span> <span class="s s-Atom">\</span><span class="nb">beta</span> <span class="o">*</span> <span class="s s-Atom">\mathcal</span><span class="p">{</span><span class="nv">L</span><span class="p">}</span><span class="k">_</span><span class="p">{</span><span class="s s-Atom">b</span><span class="p">}</span> <span class="s s-Atom">\cdot</span> <span class="s s-Atom">\text</span> <span class="p">{</span> <span class="s s-Atom">mean</span> <span class="p">}()</span><span class="o">+</span> <span class="s s-Atom">\</span><span class="nb">gamma</span> <span class="o">*</span> <span class="s s-Atom">\mathcal</span><span class="p">{</span><span class="nv">L</span><span class="p">}</span><span class="k">_</span><span class="p">{</span><span class="s s-Atom">i</span><span class="p">}</span> <span class="s s-Atom">\cdot</span> <span class="s s-Atom">\text</span> <span class="p">{</span> <span class="nf">mean</span><span class="p">()}</span>
</code></pre></div>

<p>Figure 7: A reference implementation of the baseline training loss for outputting calibrated confidence intervals. For the confidence levels where too few true labels fall inside the predicted intervals, we encourage the model to adjust its boundaries through boundary loss $\mathcal{L}_{b}$. Conversely, we encourage the model to shrink the predicted intervals if too many fall inside the predicted intervals.</p>
<p>2019a), shown in Algorithm 1. First, test examples are sorted by their target value and split into bins with a fixed number of examples each (adaptive binning). Then, we calculate calibration error across all bins using a Euclidean norm (Hendrycks et al., 2019). Finally, we average this local calibration error across all confidence levels, giving the final metric. For brevity, we refer to this overall metric as RMS Calibration Error. A low value for this error metric indicates that models are calibrated across their entire dynamic range of output values.</p>
<p>Algorithm 1 RMS Calibration Error
1: Input: A set of $N$ examples each with label $\left{y_{i}\right}<em i="i">{i=1}^{N}$ and $C$ predicted confidence intervals $\left{\left(l</em>\right)\right}}^{c}, u_{i}^{c<em c="1">{c=1, i=1}^{C, N}$ corresponding to $C$ confidence levels $\left{\mathcal{I}^{c}\right}</em>=0.95$ ). Set bin size to $M$.
2: function AdaptiveRMS
3: Sort the examples by labels $y_{n}$ in ascending order.
4: Assign a bin label $b_{k}=\left\lfloor\frac{k-1}{M}\right\rfloor+1$ to each by splitting sorted examples into chunks of $M$.
5: Let $\left{B_{i}\right}}^{C}$ (e.g., $\mathcal{I}^{C<em i="i">{i=1}^{b}$ be the set of bins and $B</em>$ the subset of examples in bin $i$.
6: for $c=1, \ldots, C$ do
7: Calculate empirical containment for bin $i$</p>
<p>$$
\widehat{p}<em i="i">{i}^{c}=\frac{1}{\left|B</em>\right]\right)
$$}\right|} \sum_{k \in B_{i}} \mathbb{1}\left(y_{k} \in\left[l_{k}^{c}, u_{k}^{c</p>
<p>8: Calculate root mean squared calibration error</p>
<p>$$
\mathrm{RMS}^{c}=\sqrt{\frac{1}{b} \sum_{i=1}^{b}\left(\widehat{p}_{i}^{c}-\mathcal{I}^{c}\right)^{2}}
$$</p>
<p>9: end for
10: Output $\frac{1}{C} \sum_{c=1}^{C} \mathrm{RMS}^{c}$
11: end function
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 8: Left: Crowd forecasts for true/false questions have good calibration. Right: The percategory performance of baselines. Score indicates the combined score metric.</p>
<p>Calibration Dataset Statistics. The dataset of numerical questions gathered for our calibration evaluations has training, validation, and test sets containing 32,200, 3,443, and 6,170 examples respectively.</p>
<h1>B Additional Dataset Information</h1>
<h2>B. 1 Dataset Details</h2>
<p>The Autocast dataset contains 6,707 unique questions in total, spanning three question types, including resolved and unresolved. After we balance the true/false questions by adding negated questions, the true/false question count doubles, making the grand</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">T/F</th>
<th style="text-align: center;">MCQ</th>
<th style="text-align: center;">Numerical</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">3187</td>
<td style="text-align: center;">753</td>
<td style="text-align: center;">471</td>
</tr>
<tr>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">775</td>
<td style="text-align: center;">176</td>
<td style="text-align: center;">341</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">3962</td>
<td style="text-align: center;">929</td>
<td style="text-align: center;">812</td>
</tr>
</tbody>
</table>
<p>Table 5: The number of resolved questions in Autocast, grouped by question type.</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Percentage</th>
<th>Subcategories</th>
</tr>
</thead>
<tbody>
<tr>
<td>Politics</td>
<td>$31 \%$</td>
<td>Geopolitics, Security and Conflict, Elections, <br> Foreign Policy, Leader Entry/Exit, Law, <br> Economic Policy, US Policy, Ukraine</td>
</tr>
<tr>
<td>Social</td>
<td>$22 \%$</td>
<td>COVID-19, Social Issues, Environment, <br> Effective Altruism, Sports, Entertainment, Health, <br> Society, Pandemic, Animal Welfare, <br> Metaculus, Climate, Education</td>
</tr>
<tr>
<td>Science \&amp; Tech</td>
<td>$21 \%$</td>
<td>Technology, Computing, Biological Sciences, <br> Physical Sciences, Computer Science, Biology, <br> Human Sciences, AI, Mathematics, Tech</td>
</tr>
<tr>
<td>Economy</td>
<td>$20 \%$</td>
<td>Business, Finance, Industry, Economic Indicators, <br> Infrastructure, Microelectronics, Semiconductors</td>
</tr>
<tr>
<td>Other</td>
<td>$6 \%$</td>
<td>Other, Open</td>
</tr>
</tbody>
</table>
<p>Table 6: The percentage of Autocast questions in each category, and the subcategories belonging to each category. Autocast questions have fairly even coverage of a wide variety of topics.
total 9,757. The numbers of training and test examples are shown in Table 4 for ease of reference. The numbers below are based on the expanded dataset using true/false balancing. The Autocast training set we experiment with does not include unresolved questions. This training set contains 4,411 examples, and the test set contains 1,292 examples. To prevent leakage of future information, the train set consists of all questions that closed or resolved before 5-11-2021 and the test set consists of all questions that closed or resolved after 5-11-2021. In addition, we also release 1,974 unresolved train questions having a publish date before 5-11-2021 and 2,305 unresolved test questions published after 5-11-2021. Note that our baselines do not use any unresolved questions, so there is a guarantee of no leakage. However, training with auxiliary training signals from unresolved questions (e.g., crowd forecasts) requires additional care to ensure no leakage. Namely, crowd forecasts from after 5-11-2021 must not be used.</p>
<p>Per-Category Performance. In Figure 8, we show performance by category using the combined score metric. Science \&amp; Technology questions are the most challenging for the FiD Static and FiD Temporal baselines, while the crowd predictions perform similarly on all question categories. There is a substantial gap between models and crowd predictions, but crowd predictions are still far from a perfect score of $100 \%$.</p>
<p>Computation of Crowd Forecasts. The human crowd forecasts are directly obtained from forecasting platforms, and the precise meaning depends on the platform. For example, for Metaculus questions the crowd forecast represents the median forecast with the recent player predictions weighted more. For Good Judgment Open questions, it represents the median of the recent $40 \%$ of forecasts. In all cases, the crowd forecast aggregates previous individual forecasts at a given time.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: We visualize the distribution of the duration of the active periods for Autocast questions. Questions vary greatly in terms of how long they are active in the forecasting market, with questions taking up to years to resolve.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 10: The same example from the Autocast dataset shown in Figure 1, illustrating how the crowd forecast is influenced by news articles published throughout the prediction period.</p>
<h1>C X-Risk Sheet</h1>
<p>We provide an analysis of our paper's contribution to reducing existential risk from future AI systems following the framework suggested by (Hendrycks and Mazeika, 2022). Individual question responses do not decisively imply relevance or irrelevance to existential risk reduction.</p>
<h2>C. 1 Long-Term Impact on Advanced AI Systems</h2>
<p>In this section, please analyze how this work shapes the process that will lead to advanced AI systems and how it steers the process in a safer direction.</p>
<ol>
<li>Overview. How is this work intended to reduce existential risks from advanced AI systems?</li>
</ol>
<p>Answer: This work builds towards improving institutional decision making and systemic safety. In short, this could help resolve matters of fact that influence policies and decisions made by political leaders in an increasingly complex modern world, putting humanity in a better place to deal with the global turbulence and uncertainty created by AI systems when they rapidly reshape society. A fuller motivation for "ML for Improving Epistemics" is described in Hendrycks and Mazeika (2022).
2. Direct Effects. If this work directly reduces existential risks, what are the main hazards, vulnerabilities, or failure modes that it directly affects?
Answer: This directly works against failure modes such as eroded epistemics and hazards such as highly persuasive or manipulative AI systems.
3. Diffuse Effects. If this work reduces existential risks indirectly or diffusely, what are the main contributing factors that it affects?
Answer: This work could lead to improved decision making, epistemics, and collective intelligence. Automated forecasting tools could eventually assist various levels of the sociotechnical hierarchy, including congress and legislatures; government regulatory agencies, industry associations, user associations, etc.; and company management. This lowers the risk of conflict that would accelerate the weaponization of AI, so it diffusely works against weaponized AI failure modes.
4. What's at Stake? What is a future scenario in which this research direction could prevent the sudden, large-scale loss of life? If not applicable, what is a future scenario in which this research direction be highly beneficial?</p>
<p>Answer: Advanced automated forecasting better enables political leaders to avoid precarious moments that could spark a large-scale conflict.
5. Result Fragility. Do the findings rest on strong theoretical assumptions; are they not demonstrated using leading-edge tasks or models; or are the findings highly sensitive to hyperparameters?
6. Problem Difficulty. Is it implausible that any practical system could ever markedly outperform humans at this task?
7. Human Unreliability. Does this approach strongly depend on handcrafted features, expert supervision, or human reliability?
8. Competitive Pressures. Does work towards this approach strongly trade off against raw intelligence, other general capabilities, or economic utility?</p>
<h1>C. 2 Safety-Capabilities Balance</h1>
<p>In this section, please analyze how this work relates to general capabilities and how it affects the balance between safety and hazards from general capabilities.
9. Overview. How does this improve safety more than it improves general capabilities?</p>
<p>Answer: While this line of work reduces systemic risk factors and can improve institutional decision making, making AI systems better at forecasting could potentially improve general capabilities. Its relation to general capabilities is currently unclear. In humans, at the extremes, IQ is hardly predictive of forecasting ability, suggesting forecasting of near-term geopolitical events is a specific and not general skill. Likewise, work in this space could focus on engineering better forecasting systems rather than improving general representations, so as to avoid capabilities externalities; this is potentially a more robust strategy for avoiding capabilities externalities. If it turns out that capabilities externalities are difficult to avoid even while simply engineering better forecasting systems, we would suggest that safety researchers stop working on this problem.
10. Red Teaming. What is a way in which this hastens general capabilities or the onset of x-risks?</p>
<p>Answer: Making AI systems better at forecasting could also improve general capabilities or at least the raw power of AI systems. As Yann LeCun reminds us, "prediction is the essence of intelligence."
11. General Tasks. Does this work advance progress on tasks that have been previously considered the subject of usual capabilities research?
12. General Goals. Does this improve or facilitate research towards general prediction, classification, state estimation, efficiency, scalability, generation, data compression, executing clear instructions, helpfulness, informativeness, reasoning, planning, researching, optimization, (self-)supervised learning, sequential decision making, recursive self-improvement, open-ended goals, models accessing the internet, or similar capabilities?
13. Correlation With General Aptitude. Is the analyzed capability known to be highly predicted by general cognitive ability or educational attainment?
14. Safety via Capabilities. Does this advance safety along with, or as a consequence of, advancing other capabilities or the study of AI?</p>
<h2>C. 3 Elaborations and Other Considerations</h2>
<ol>
<li>Other. What clarifications or uncertainties about this work and x -risk are worth mentioning?</li>
</ol>
<p>Answer: Regarding Q7, while human forecasters are important for building a training set with rich annotations, the actual human forecasts are unnecessary, as technically only the resolutions are needed. Additionally, the end goal is to create automated forecasting systems that do not depend on human reliability. Eventually, these systems could become much faster and more reliable than human forecasters.
Regarding Q12, this work facilitates research towards general prediction of future events and consequently toward improved planning. However, we expect the kinds of predictions improved by forecasting research to be especially relevant for reducing x -risk. For example, improved institutional decision making surrounding geopolitical events could reduce the risk of global conflicts leading to the weaponization of strong AI.</p>
<p>Regarding Q13, IQ is predictive of forecasting ability in humans, not overwhelmingly so (Mellers et al., 2015). Moreover, its correlation is especially weak at extremes. Likewise, forecasting skills for near-term geopolitical events are partly learnable, further suggesting a separation from general cognitive ability.
Regarding Q14, while the relationship between general capabilities and research on forecasting near-term geopolitical events is currently unclear, this research does advance the study of narrow AI systems.
Finally, we would like to discuss limitations and potential hazards of relying on ML for forecasting near-term geopolitical events.
(a) Forecasting is best used for refining understanding rather than for anticipating the future more generally. Forecasters are demonstrated to be useful for optimizing probabilities for somewhat likely events (e.g., events with probabilities between, say, $5 \%$ and $95 \%$ ). What is more important are tools that unearth important considerations that were implicitly assigned negligible probabilities or wrongly treated by humans as misinformation or worth ignoring. These considerations are often not forecasted and are not thought worth asking; implicitly, such events could the thought to be assigned low probabilities (e.g., say $10^{-7}$ ), while some people argue that these considerations are more likely than others believe (e.g., say $10^{-1}$ ). The information value provided from putting ignored considerations on our radar is substantial, in fact, orders of magnitude greater than the information gained by refining probabilities by a few percent. Forecasting competitions are about refining estimates of known unknowns-questions already on our radar-but what is better for risk reduction is confronting unknown unknowns, finding considerations to put on our radar, and reducing exposure to inchoate potential risks. For this reason, Hendrycks et al. (2021c) suggest tools that improve brainstorming and suggesting considerations.
(b) Forecasting is not necessarily a suitable tool for addressing tail risks. Taleb and Tetlock (2013) remind us that "No one has yet figured out how to design a forecasting tournament to assess the accuracy of probability judgments that range between $.00000001 \%$ and $1 \%$-and if someone ever did, it is unlikely that anyone would have the patience-or lifespan-to run the forecasting tournament for the necessary stretches of time (requiring us to think not just in terms of decades, centuries and millennia)." Taleb and Tetlock (2013) further remind us that it is unjustified to use forecasting tools for revolutions, market crashes, venture capital, or other winner-take-all domains. Furthermore they note that framing questions about tail risks as "a binary question is dangerous because it masks exponentially escalating tail risks." Consequently, "improving short-run probability judgments" and "contingency planning for systemic [tail] risks" are "complementary" and separate (Tetlock et al., 2022). Indeed, superforecasters usually anchor in outside view (Tetlock and Gardner, 2016), which neglects systemic risks. In environments with tail events, it is not how often one is correct that matters but rather how large one's cumulative errors are; current forecasting metrics do not sufficiently penalize forecasters that ignore tail risks nor do they greatly reward prescience about Black Swans.
(c) Forecasting tools could lead to risky behavior. For example, forecasting systems may induce inaction. If forecasts are uncertain, leaders may argue that "we should not make a decision before we have a reliable forecast" so we should "sit tight and assess." This is sometimes referred to as the delay fallacy, namely "if we wait we will know more about X, hence no decision about X should be made now" (Hansson, 2004). However, it is often cheaper to prevent risks or reduce exposure to risks, as "an existential risk needs to be killed in the egg, when it is still cheap to do so" (Taleb et al., 2020). Waiting until all the relevant information arrives is often waiting until it is too late.
Furthermore, humans are known to misinterpret probabilities (Vodrahalli et al., 2022). Systems that assign an event $3 \%$ probability may lead decision-makers to assume the event will not happen. Automation bias may mean forecasting systems induce users to have a gain in confidence that is greater than their gain in knowledge. Risk compensation suggests this could result in riskier actions (Hedlund, 2000). Furthermore, forecasts are often not provided with reverse psychology in mind. However, a forecasting system that forecasts a low risk can lead users to act as though there is no risk and increase risky behavior, which increases systemic risk.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1726 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1726</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1726</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-265351495</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.12871v3.pdf" target="_blank">An Embodied Generalist Agent in 3D World</a></p>
                <p><strong>Paper Abstract:</strong> Leveraging massive knowledge from large language models (LLMs), recent machine learning models show notable successes in general-purpose task solving in diverse domains such as computer vision and robotics. However, several significant challenges remain: (i) most of these models rely on 2D images yet exhibit a limited capacity for 3D input; (ii) these models rarely explore the tasks inherently defined in 3D world, e.g., 3D grounding, embodied reasoning and acting. We argue these limitations significantly hinder current models from performing real-world tasks and approaching general intelligence. To this end, we introduce LEO, an embodied multi-modal generalist agent that excels in perceiving, grounding, reasoning, planning, and acting in the 3D world. LEO is trained with a unified task interface, model architecture, and objective in two stages: (i) 3D vision-language (VL) alignment and (ii) 3D vision-language-action (VLA) instruction tuning. We collect large-scale datasets comprising diverse object-level and scene-level tasks, which require considerable understanding of and interaction with the 3D world. Moreover, we meticulously design an LLM-assisted pipeline to produce high-quality 3D VL data. Through extensive experiments, we demonstrate LEO's remarkable proficiency across a wide spectrum of tasks, including 3D captioning, question answering, embodied reasoning, navigation and manipulation. Our ablative studies and scaling analyses further provide valuable insights for developing future embodied generalist agents. Code and data are available on project page.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1726.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1726.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LEO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LEO (An Embodied Multimodal Generalist Agent in 3D World)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied multimodal generalist agent that integrates a pretrained LLM (Vicuna-7B) with object-centric 3D and egocentric 2D encoders to perform unified 3D vision-language-action tasks via a two-stage training: 3D VL alignment and 3D VLA instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>LEO</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Decoder-only LLM (Vicuna-7B) as the core autoregressive module; object-centric 3D tokenization (PointNet++ encoder frozen), Spatial Transformer for 3D relations, OpenCLIP ConvNeXt image encoder for egocentric 2D, tokenized unified vocabulary including reserved SentencePiece tokens for actions; LoRA adapters applied to frozen Vicuna-7B; all modalities interleaved as a sequence and trained with prefix language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>pretrained large language model (Vicuna-7B) + 3D/vision encoders pretrained on 3D/2D datasets; instruction tuning on generated 3D-language data.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper uses a two-stage training: LEO-align (3D VL alignment) and LEO-instruct (3D VLA instruction tuning). Dataset statistics (Table 1): object-level captions (Objaverse) 660K samples; object-in-scene captions (ScanNet + 3RScan) 354K; scene captioning (3RScan) ~20K; Scan2Cap 37K; 3D QA aggregated 83K; 3D dialogue ~11K; task planning ~14K; navigation ~60K episodes (MP3D ObjNav training split); manipulation (CLIPort subset) ~300K episodes (they state generating 100K demonstrations per selected manipulation task). The LLM used is Vicuna-7B (frozen, LoRA tuned) and 3D point cloud encoder PointNet++ is pretrained on ScanNet (frozen). The paper does not provide the exact pretraining corpus for Vicuna-7B within the text.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D captioning (Scan2Cap), 3D question answering (ScanQA), embodied reasoning (SQA3D), embodied navigation (ObjNav on MP3D and HM3D), robotic manipulation (CLIPort subset)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Tasks include: (1) 3D dense and object captioning from object-centric point clouds and optionally egocentric images; (2) 3D question answering requiring scene reasoning (ScanQA, SQA3D); (3) embodied navigation (ObjNav) where agent receives object-centric 3D input plus egocentric image and issues discrete navigation actions to find object-goals in MP3D / HM3D; (4) robotic manipulation on CLIPort tasks where the agent maps language + egocentric views + object-centric 3D info to motor commands (discretized) to accomplish pick/place tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Language instructions and natural-language task specifications; during data generation LLMs produce textual plans and answers (no low-level motor actions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation actions (4 tokens: move forward, turn right, turn left, stop) and discretized manipulation actions (CLIPort: 516 tokens total — 320 x-axis bins, 160 y-axis bins, 36 z-rotation bins); continuous manipulation actions were discretized into these bins for unified discrete action decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Action tokens were mapped to the least-used SentencePiece text tokens; during inference LLM outputs reserved text tokens which are mapped back to discrete action commands. Continuous manipulation actions were discretized into pose bins (x,y,z-rotation) and encoded as separate token groups (320/160/36).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Egocentric RGB images (OpenCLIP ConvNeXt features pretrained on LAION2B), object-centric 3D point clouds (PointNet++ pretrained on ScanNet), optional GPS/compass-derived embodiment info (Fourier-embedded rotation) for embodiment token, and CLIP text encoder guidance for semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>LEO attains state-of-the-art or comparable performance across 3D VL and embodied tasks: e.g., on Scan2Cap (val) LEO (w/o Act) scores CIDEr 72.4, BLEU-4 38.2, METEOR 27.9 (Table A.15); on 3D QA (ScanQA val) LEO reports improved metrics (Table 4). Robotic manipulation (CLIPort subset, Table 5): LEO reported results per-task (seen / unseen) — aggregated example numbers from Table 5 for LEO: seen: [98.8, 75.2, 76.6]; unseen: [79.8, 86.2, 35.2] (table rows correspond to three tasks; paper presents LEO as comparable or better on some unseen manipulation tasks). Object navigation (Table 6): MP3D-val Success 23.1%, SPL 15.2%; HM3D-val (zero-shot) Success 23.1%, SPL 19.1%. The paper emphasizes achieving SOTA on multiple 3D VL tasks and comparable results on navigation/manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>The paper provides ablations rather than a pure 'no LLM pretraining' baseline. Ablation 'w/o Align' (no 3D VL alignment stage) reduces performance on Scan2Cap and related VL tasks (e.g., Scan2Cap: w/o Align CIDEr 62.8 vs w/o Act 65.4 in Table 7); switching the LLM to smaller ones (OPT-1.3B) yields worse results (Table A.15). No direct numeric baseline of LEO trained from-scratch without any language-pretrained model on embodied tasks is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Training data used: ~60K navigation episodes (MP3D shortest-path), ~300K manipulation demonstrations in CLIPort subset (100K per selected task as stated), 660K object captions (Objaverse), 354K object-in-scene captions, ~20K scene captions, ~83K aggregated 3D-QA samples, ~11K dialogues, ~14K task-plans. The two-stage scheme and LLM-assisted data generation scaled the dataset to these sizes used for instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not explicitly reported. The paper notes human demonstrations (70K) are harder to learn from for their feed-forward policy and that agents without recurrence learn poorly from more exploratory human demonstration data, but no direct sample-complexity numbers without language pretraining are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not numerically quantified in the paper (no explicit measurement of samples-to-threshold with/without LLM pretraining). The paper reports qualitative and ablation evidence that alignment and LLM priors improve downstream performance and generalization, but does not compute a sample-efficiency ratio.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Factors the paper attributes to successful transfer include: leveraging pretrained LLM priors (Vicuna-7B) via LoRA to preserve language knowledge; object-centric 3D representation with Spatial Transformer for explicit 3D relations; a two-stage pipeline (3D VL alignment then 3D VLA instruction tuning); large, diverse LLM-assisted 3D-language datasets (scene-graph based prompting, O-CoT and refinement); unified tokenization mapping actions into text token space; CLIP semantic guidance; freezing pretrained encoders where helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Observed limiting factors include: gap between 3D VL learning and embodied action control (co-training VLA can harm pure VL capabilities), imbalanced data scales (embodied acting data smaller relative to VL data), absence of recurrence in policy (LEO uses truncated past actions only) which may reduce navigation success rate compared to recurrent baselines, perception-to-action gaps, and generalization to novel scenes still imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained language models can be successfully integrated into embodied 3D agents: using a frozen pretrained LLM (Vicuna-7B) with LoRA and coupling it to object-centric 3D and egocentric 2D encoders yields strong unified performance on 3D VL tasks and comparable results on embodied navigation and manipulation. Two-stage training (alignment then instruction tuning), object-centric representations, LLM-assisted large-scale 3D-language data generation, and discrete action-token mapping are key enablers; however, modality gaps, data imbalance, and policy architecture (lack of recurrence) limit transfer in some embodied tasks. The paper reports dataset sizes and task results but does not provide a direct quantitative sample-efficiency comparison versus training from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Embodied Generalist Agent in 3D World', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1726.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1726.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source instruction-following LLM used as the core decoder in LEO; Vicuna-7B is used frozen with LoRA adapters to retain pretrained language priors while adapting to multimodal tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vicuna, An opensource chatbot impressing gpt-4 with 90%* chatgpt quality.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Vicuna-7B (as used inside LEO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Decoder-only instruction-tuned LLM selected to process unified multimodal token sequences; frozen during LEO training with LoRA adapters applied for efficient adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretrained on large-scale language data as a general instruction-following LLM (exact corpora not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper references Vicuna-7B but does not detail its original pretraining corpora within the manuscript; it treats Vicuna-7B as an off-the-shelf pretrained LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used within LEO to perform 3D VL and VLA tasks (see LEO entry).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Vicuna-7B processes interleaved language, 2D image tokens, 3D object tokens and action tokens and autoregressively generates text responses and action token sequences for captioning, QA, planning, navigation, and manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions and textual action tokens (mapped from discrete embodied actions to reserved SentencePiece tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Same as LEO's embodied action discretization (navigation 4 tokens; CLIPort discretized pose bins totalling 516 tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Reserved least-used SentencePiece tokens are assigned to actions; Vicuna outputs those tokens to produce embodied actions; mapping described in LEO.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Same as LEO (egocentric 2D image embeddings, object-centric 3D tokens, optional embodiment token).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Vicuna-7B-based LEO achieves the task numbers reported for LEO (see LEO entry). The paper also compares LEO variants using different LLM scales (OPT-1.3B, Vicuna-7B, Vicuna-13B) and observes improving instruction-tuning loss with larger LLMs; Aligned Vicuna-7B outperforms OPT-1.3B (Table A.15 and scaling analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>A 'Scratch Vicuna-7B' (trained from scratch without alignment) shows higher test losses and worse downstream performance compared to Aligned Vicuna-7B (scaling and ablation studies), but precise numeric task baselines for 'no LLM pretraining' are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not explicitly quantified for Vicuna alone beyond the dataset sizes used for LEO instruction tuning (see LEO entry).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Vicuna provides strong language priors and few-shot/instruction-following capabilities that LEO leverages; LoRA preserves core Vicuna weights while enabling multimodal grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Paper notes diminishing returns when scaling beyond a point and highlights the need to scale data to match model capacity; no other Vicuna-specific failures are detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an off-the-shelf instruction-tuned LLM (Vicuna-7B) as a frozen core and applying parameter-efficient adapters (LoRA) enables effective integration of linguistic priors into a multimodal embodied agent, improving VL alignment and downstream performance when combined with object-centric 3D representations and task-specific instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Embodied Generalist Agent in 3D World', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1726.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1726.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action model that transfers web knowledge to robotic control; cited as an example of unified VLA models integrating language priors for control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RT-2: Vision-language-action models transfer web knowledge to robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Mentioned as a unified vision-language-action model that leverages web-scale knowledge to aid robotic control; specific architectural and pretraining details are not provided in this paper beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Described in this paper as leveraging 'web knowledge' (language/web-scale data) to transfer to robotic control, but the manuscript does not provide numeric dataset details.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Robotic control / embodied tasks (general reference)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Referenced in related work as transferring language/web knowledge to robotic control; this paper does not provide experiment-level descriptions or performance numbers for RT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Paper cites RT-2 as an example of VLA models that bring web knowledge to control; implies semantic knowledge transfer from language helps embodied control.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as background evidence that language/web-scale pretraining can help downstream robotic control, motivating LEO's use of LLM priors; no experimental details are given in this text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Embodied Generalist Agent in 3D World', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1726.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1726.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied multimodal language model referenced as prior work on integrating language models with embodied perception and action.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Palm-e: An embodied multimodal language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Cited as an embodied multimodal language model; the paper does not detail PaLM-E's pretraining corpus or experimental numbers beyond the citation in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Not specified in this paper (referred to as an embodied multimodal LM).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>General embodied multimodal tasks (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Not detailed in this manuscript; PaLM-E is cited as related work integrating LMs with embodied modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced to show existing work that merges language models and embodied control; no specific details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Embodied Generalist Agent in 3D World', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1726.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1726.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gato</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gato</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generalist agent (multi-task, multi-modal) cited as a prior example of unified agents trained across diverse tasks and modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A generalist agent.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Gato</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Cited as a unified generalist model trained on multi-modal tasks; the present paper references it in the context of VLA/generalist agents but does not report Gato-specific pretraining or transfer numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multi-modal, multi-task training including language and control (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>General embodied tasks (cited historically as a generalist agent example)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Not specified in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Listed among existing unified agents to position LEO in the broader space of VLA/generalist research; no experimental details provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Embodied Generalist Agent in 3D World', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1726.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1726.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmbodiedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EmbodiedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work on vision-language pre-training with embodied chain-of-thought; cited among related works that connect large language models with embodied reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Embodiedgpt: Vision-language pre-training via embodied chain of thought.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>EmbodiedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Referenced as a method that uses embodied chain-of-thought during vision-language pretraining; this paper cites it as related work but provides no further experimental details.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Vision-language pretraining (details not provided in this manuscript).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Embodied reasoning and related tasks (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as part of recent efforts connecting LLMs to embodied reasoning via specialized pretraining (embodied chain-of-thought); no further info in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Embodied Generalist Agent in 3D World', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1726.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1726.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent concurrent work that injects 3D world information into LLMs by leveraging multi-view images; referenced and compared as a task-specific or data pipeline competitor.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>3d-llm: Injecting the 3d world into large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>3D-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Described in this paper as a concurrent approach that uses multi-view images to enable LLM grounding in 3D; compared to LEO's object-centric approach. The manuscript notes 3D-LLM's data is smaller in accessible subsets and that 3D-LLM lacks some acting capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multi-view image + LLM based 3D grounding (exact corpora/details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D grounding / 3D VL tasks (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Not detailed here; paper notes 3D-LLM focuses on grounding but may lack acting capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Multi-view image inputs (as reported in the comparison discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a concurrent approach; the paper argues an object-centric 3D representation (LEO) is a simpler and effective way to connect 3D scenes with LLMs compared to 3D-LLM's dense feature lifting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Embodied Generalist Agent in 3D World', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1726.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1726.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LL3DA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LL3DA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced dataset / system for visual interactive instruction tuning for omni-3D understanding, reasoning, and planning; used in data-comparison ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ll3da: Visual interactive instruction tuning for omni-3d understanding, reasoning, and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>LL3DA (data/system)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Cited as a training data source; the authors used LL3DA training data in an ablation to show the advantage of their collected dataset — LEO trained on LL3DA data performed worse than when trained on the paper's original collected data (Tab. A.18).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Visual interactive instruction tuning dataset for 3D tasks (paper does not enumerate the corpora inside LL3DA).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D VL and instruction-following tasks (used as training data in ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Used in ablation comparisons for Scan2Cap and ScanQA downstream performance; precise task descriptions are in the LL3DA source, not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper-level ablation shows LEO trained on LL3DA data underperforms LEO trained on the authors' collected data, indicating dataset design and refinement matter for transfer; no numeric sample-efficiency claims are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Embodied Generalist Agent in 3D World', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RT-2: Vision-language-action models transfer web knowledge to robotic control. <em>(Rating: 2)</em></li>
                <li>Palm-e: An embodied multimodal language model. <em>(Rating: 2)</em></li>
                <li>A generalist agent. <em>(Rating: 2)</em></li>
                <li>Embodiedgpt: Vision-language pre-training via embodied chain of thought. <em>(Rating: 2)</em></li>
                <li>3d-llm: Injecting the 3d world into large language models. <em>(Rating: 2)</em></li>
                <li>Ll3da: Visual interactive instruction tuning for omni-3d understanding, reasoning, and planning. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1726",
    "paper_id": "paper-265351495",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "LEO",
            "name_full": "LEO (An Embodied Multimodal Generalist Agent in 3D World)",
            "brief_description": "An embodied multimodal generalist agent that integrates a pretrained LLM (Vicuna-7B) with object-centric 3D and egocentric 2D encoders to perform unified 3D vision-language-action tasks via a two-stage training: 3D VL alignment and 3D VLA instruction tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "LEO",
            "model_agent_description": "Decoder-only LLM (Vicuna-7B) as the core autoregressive module; object-centric 3D tokenization (PointNet++ encoder frozen), Spatial Transformer for 3D relations, OpenCLIP ConvNeXt image encoder for egocentric 2D, tokenized unified vocabulary including reserved SentencePiece tokens for actions; LoRA adapters applied to frozen Vicuna-7B; all modalities interleaved as a sequence and trained with prefix language modeling.",
            "pretraining_data_type": "pretrained large language model (Vicuna-7B) + 3D/vision encoders pretrained on 3D/2D datasets; instruction tuning on generated 3D-language data.",
            "pretraining_data_details": "The paper uses a two-stage training: LEO-align (3D VL alignment) and LEO-instruct (3D VLA instruction tuning). Dataset statistics (Table 1): object-level captions (Objaverse) 660K samples; object-in-scene captions (ScanNet + 3RScan) 354K; scene captioning (3RScan) ~20K; Scan2Cap 37K; 3D QA aggregated 83K; 3D dialogue ~11K; task planning ~14K; navigation ~60K episodes (MP3D ObjNav training split); manipulation (CLIPort subset) ~300K episodes (they state generating 100K demonstrations per selected manipulation task). The LLM used is Vicuna-7B (frozen, LoRA tuned) and 3D point cloud encoder PointNet++ is pretrained on ScanNet (frozen). The paper does not provide the exact pretraining corpus for Vicuna-7B within the text.",
            "embodied_task_name": "3D captioning (Scan2Cap), 3D question answering (ScanQA), embodied reasoning (SQA3D), embodied navigation (ObjNav on MP3D and HM3D), robotic manipulation (CLIPort subset)",
            "embodied_task_description": "Tasks include: (1) 3D dense and object captioning from object-centric point clouds and optionally egocentric images; (2) 3D question answering requiring scene reasoning (ScanQA, SQA3D); (3) embodied navigation (ObjNav) where agent receives object-centric 3D input plus egocentric image and issues discrete navigation actions to find object-goals in MP3D / HM3D; (4) robotic manipulation on CLIPort tasks where the agent maps language + egocentric views + object-centric 3D info to motor commands (discretized) to accomplish pick/place tasks.",
            "action_space_text": "Language instructions and natural-language task specifications; during data generation LLMs produce textual plans and answers (no low-level motor actions).",
            "action_space_embodied": "Discrete navigation actions (4 tokens: move forward, turn right, turn left, stop) and discretized manipulation actions (CLIPort: 516 tokens total — 320 x-axis bins, 160 y-axis bins, 36 z-rotation bins); continuous manipulation actions were discretized into these bins for unified discrete action decoding.",
            "action_mapping_method": "Action tokens were mapped to the least-used SentencePiece text tokens; during inference LLM outputs reserved text tokens which are mapped back to discrete action commands. Continuous manipulation actions were discretized into pose bins (x,y,z-rotation) and encoded as separate token groups (320/160/36).",
            "perception_requirements": "Egocentric RGB images (OpenCLIP ConvNeXt features pretrained on LAION2B), object-centric 3D point clouds (PointNet++ pretrained on ScanNet), optional GPS/compass-derived embodiment info (Fourier-embedded rotation) for embodiment token, and CLIP text encoder guidance for semantics.",
            "transfer_successful": true,
            "performance_with_pretraining": "LEO attains state-of-the-art or comparable performance across 3D VL and embodied tasks: e.g., on Scan2Cap (val) LEO (w/o Act) scores CIDEr 72.4, BLEU-4 38.2, METEOR 27.9 (Table A.15); on 3D QA (ScanQA val) LEO reports improved metrics (Table 4). Robotic manipulation (CLIPort subset, Table 5): LEO reported results per-task (seen / unseen) — aggregated example numbers from Table 5 for LEO: seen: [98.8, 75.2, 76.6]; unseen: [79.8, 86.2, 35.2] (table rows correspond to three tasks; paper presents LEO as comparable or better on some unseen manipulation tasks). Object navigation (Table 6): MP3D-val Success 23.1%, SPL 15.2%; HM3D-val (zero-shot) Success 23.1%, SPL 19.1%. The paper emphasizes achieving SOTA on multiple 3D VL tasks and comparable results on navigation/manipulation.",
            "performance_without_pretraining": "The paper provides ablations rather than a pure 'no LLM pretraining' baseline. Ablation 'w/o Align' (no 3D VL alignment stage) reduces performance on Scan2Cap and related VL tasks (e.g., Scan2Cap: w/o Align CIDEr 62.8 vs w/o Act 65.4 in Table 7); switching the LLM to smaller ones (OPT-1.3B) yields worse results (Table A.15). No direct numeric baseline of LEO trained from-scratch without any language-pretrained model on embodied tasks is reported.",
            "sample_complexity_with_pretraining": "Training data used: ~60K navigation episodes (MP3D shortest-path), ~300K manipulation demonstrations in CLIPort subset (100K per selected task as stated), 660K object captions (Objaverse), 354K object-in-scene captions, ~20K scene captions, ~83K aggregated 3D-QA samples, ~11K dialogues, ~14K task-plans. The two-stage scheme and LLM-assisted data generation scaled the dataset to these sizes used for instruction tuning.",
            "sample_complexity_without_pretraining": "Not explicitly reported. The paper notes human demonstrations (70K) are harder to learn from for their feed-forward policy and that agents without recurrence learn poorly from more exploratory human demonstration data, but no direct sample-complexity numbers without language pretraining are provided.",
            "sample_complexity_gain": "Not numerically quantified in the paper (no explicit measurement of samples-to-threshold with/without LLM pretraining). The paper reports qualitative and ablation evidence that alignment and LLM priors improve downstream performance and generalization, but does not compute a sample-efficiency ratio.",
            "transfer_success_factors": "Factors the paper attributes to successful transfer include: leveraging pretrained LLM priors (Vicuna-7B) via LoRA to preserve language knowledge; object-centric 3D representation with Spatial Transformer for explicit 3D relations; a two-stage pipeline (3D VL alignment then 3D VLA instruction tuning); large, diverse LLM-assisted 3D-language datasets (scene-graph based prompting, O-CoT and refinement); unified tokenization mapping actions into text token space; CLIP semantic guidance; freezing pretrained encoders where helpful.",
            "transfer_failure_factors": "Observed limiting factors include: gap between 3D VL learning and embodied action control (co-training VLA can harm pure VL capabilities), imbalanced data scales (embodied acting data smaller relative to VL data), absence of recurrence in policy (LEO uses truncated past actions only) which may reduce navigation success rate compared to recurrent baselines, perception-to-action gaps, and generalization to novel scenes still imperfect.",
            "key_findings": "Pretrained language models can be successfully integrated into embodied 3D agents: using a frozen pretrained LLM (Vicuna-7B) with LoRA and coupling it to object-centric 3D and egocentric 2D encoders yields strong unified performance on 3D VL tasks and comparable results on embodied navigation and manipulation. Two-stage training (alignment then instruction tuning), object-centric representations, LLM-assisted large-scale 3D-language data generation, and discrete action-token mapping are key enablers; however, modality gaps, data imbalance, and policy architecture (lack of recurrence) limit transfer in some embodied tasks. The paper reports dataset sizes and task results but does not provide a direct quantitative sample-efficiency comparison versus training from scratch.",
            "uuid": "e1726.0",
            "source_info": {
                "paper_title": "An Embodied Generalist Agent in 3D World",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Vicuna-7B",
            "name_full": "Vicuna-7B",
            "brief_description": "An open-source instruction-following LLM used as the core decoder in LEO; Vicuna-7B is used frozen with LoRA adapters to retain pretrained language priors while adapting to multimodal tokens.",
            "citation_title": "Vicuna, An opensource chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "mention_or_use": "use",
            "model_agent_name": "Vicuna-7B (as used inside LEO)",
            "model_agent_description": "Decoder-only instruction-tuned LLM selected to process unified multimodal token sequences; frozen during LEO training with LoRA adapters applied for efficient adaptation.",
            "pretraining_data_type": "Pretrained on large-scale language data as a general instruction-following LLM (exact corpora not detailed in this paper).",
            "pretraining_data_details": "The paper references Vicuna-7B but does not detail its original pretraining corpora within the manuscript; it treats Vicuna-7B as an off-the-shelf pretrained LLM.",
            "embodied_task_name": "Used within LEO to perform 3D VL and VLA tasks (see LEO entry).",
            "embodied_task_description": "Vicuna-7B processes interleaved language, 2D image tokens, 3D object tokens and action tokens and autoregressively generates text responses and action token sequences for captioning, QA, planning, navigation, and manipulation.",
            "action_space_text": "Natural language instructions and textual action tokens (mapped from discrete embodied actions to reserved SentencePiece tokens).",
            "action_space_embodied": "Same as LEO's embodied action discretization (navigation 4 tokens; CLIPort discretized pose bins totalling 516 tokens).",
            "action_mapping_method": "Reserved least-used SentencePiece tokens are assigned to actions; Vicuna outputs those tokens to produce embodied actions; mapping described in LEO.",
            "perception_requirements": "Same as LEO (egocentric 2D image embeddings, object-centric 3D tokens, optional embodiment token).",
            "transfer_successful": true,
            "performance_with_pretraining": "Vicuna-7B-based LEO achieves the task numbers reported for LEO (see LEO entry). The paper also compares LEO variants using different LLM scales (OPT-1.3B, Vicuna-7B, Vicuna-13B) and observes improving instruction-tuning loss with larger LLMs; Aligned Vicuna-7B outperforms OPT-1.3B (Table A.15 and scaling analysis).",
            "performance_without_pretraining": "A 'Scratch Vicuna-7B' (trained from scratch without alignment) shows higher test losses and worse downstream performance compared to Aligned Vicuna-7B (scaling and ablation studies), but precise numeric task baselines for 'no LLM pretraining' are not provided.",
            "sample_complexity_with_pretraining": "Not explicitly quantified for Vicuna alone beyond the dataset sizes used for LEO instruction tuning (see LEO entry).",
            "sample_complexity_without_pretraining": "Not reported.",
            "sample_complexity_gain": "Not reported.",
            "transfer_success_factors": "Vicuna provides strong language priors and few-shot/instruction-following capabilities that LEO leverages; LoRA preserves core Vicuna weights while enabling multimodal grounding.",
            "transfer_failure_factors": "Paper notes diminishing returns when scaling beyond a point and highlights the need to scale data to match model capacity; no other Vicuna-specific failures are detailed.",
            "key_findings": "Using an off-the-shelf instruction-tuned LLM (Vicuna-7B) as a frozen core and applying parameter-efficient adapters (LoRA) enables effective integration of linguistic priors into a multimodal embodied agent, improving VL alignment and downstream performance when combined with object-centric 3D representations and task-specific instruction tuning.",
            "uuid": "e1726.1",
            "source_info": {
                "paper_title": "An Embodied Generalist Agent in 3D World",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2",
            "brief_description": "A vision-language-action model that transfers web knowledge to robotic control; cited as an example of unified VLA models integrating language priors for control.",
            "citation_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control.",
            "mention_or_use": "mention",
            "model_agent_name": "RT-2",
            "model_agent_description": "Mentioned as a unified vision-language-action model that leverages web-scale knowledge to aid robotic control; specific architectural and pretraining details are not provided in this paper beyond citation.",
            "pretraining_data_type": "Described in this paper as leveraging 'web knowledge' (language/web-scale data) to transfer to robotic control, but the manuscript does not provide numeric dataset details.",
            "pretraining_data_details": null,
            "embodied_task_name": "Robotic control / embodied tasks (general reference)",
            "embodied_task_description": "Referenced in related work as transferring language/web knowledge to robotic control; this paper does not provide experiment-level descriptions or performance numbers for RT-2.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Paper cites RT-2 as an example of VLA models that bring web knowledge to control; implies semantic knowledge transfer from language helps embodied control.",
            "transfer_failure_factors": null,
            "key_findings": "Mentioned as background evidence that language/web-scale pretraining can help downstream robotic control, motivating LEO's use of LLM priors; no experimental details are given in this text.",
            "uuid": "e1726.2",
            "source_info": {
                "paper_title": "An Embodied Generalist Agent in 3D World",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "PaLM-E",
            "name_full": "PaLM-E",
            "brief_description": "An embodied multimodal language model referenced as prior work on integrating language models with embodied perception and action.",
            "citation_title": "Palm-e: An embodied multimodal language model.",
            "mention_or_use": "mention",
            "model_agent_name": "PaLM-E",
            "model_agent_description": "Cited as an embodied multimodal language model; the paper does not detail PaLM-E's pretraining corpus or experimental numbers beyond the citation in related work.",
            "pretraining_data_type": "Not specified in this paper (referred to as an embodied multimodal LM).",
            "pretraining_data_details": null,
            "embodied_task_name": "General embodied multimodal tasks (referenced)",
            "embodied_task_description": "Not detailed in this manuscript; PaLM-E is cited as related work integrating LMs with embodied modalities.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Referenced to show existing work that merges language models and embodied control; no specific details provided in this paper.",
            "uuid": "e1726.3",
            "source_info": {
                "paper_title": "An Embodied Generalist Agent in 3D World",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Gato",
            "name_full": "Gato",
            "brief_description": "A generalist agent (multi-task, multi-modal) cited as a prior example of unified agents trained across diverse tasks and modalities.",
            "citation_title": "A generalist agent.",
            "mention_or_use": "mention",
            "model_agent_name": "Gato",
            "model_agent_description": "Cited as a unified generalist model trained on multi-modal tasks; the present paper references it in the context of VLA/generalist agents but does not report Gato-specific pretraining or transfer numbers.",
            "pretraining_data_type": "Multi-modal, multi-task training including language and control (not detailed in this paper).",
            "pretraining_data_details": null,
            "embodied_task_name": "General embodied tasks (cited historically as a generalist agent example)",
            "embodied_task_description": "Not specified in this manuscript.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Listed among existing unified agents to position LEO in the broader space of VLA/generalist research; no experimental details provided here.",
            "uuid": "e1726.4",
            "source_info": {
                "paper_title": "An Embodied Generalist Agent in 3D World",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "EmbodiedGPT",
            "name_full": "EmbodiedGPT",
            "brief_description": "A prior work on vision-language pre-training with embodied chain-of-thought; cited among related works that connect large language models with embodied reasoning.",
            "citation_title": "Embodiedgpt: Vision-language pre-training via embodied chain of thought.",
            "mention_or_use": "mention",
            "model_agent_name": "EmbodiedGPT",
            "model_agent_description": "Referenced as a method that uses embodied chain-of-thought during vision-language pretraining; this paper cites it as related work but provides no further experimental details.",
            "pretraining_data_type": "Vision-language pretraining (details not provided in this manuscript).",
            "pretraining_data_details": null,
            "embodied_task_name": "Embodied reasoning and related tasks (referenced)",
            "embodied_task_description": "Not detailed in this paper.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Mentioned as part of recent efforts connecting LLMs to embodied reasoning via specialized pretraining (embodied chain-of-thought); no further info in this paper.",
            "uuid": "e1726.5",
            "source_info": {
                "paper_title": "An Embodied Generalist Agent in 3D World",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "3D-LLM",
            "name_full": "3D-LLM",
            "brief_description": "A recent concurrent work that injects 3D world information into LLMs by leveraging multi-view images; referenced and compared as a task-specific or data pipeline competitor.",
            "citation_title": "3d-llm: Injecting the 3d world into large language models.",
            "mention_or_use": "mention",
            "model_agent_name": "3D-LLM",
            "model_agent_description": "Described in this paper as a concurrent approach that uses multi-view images to enable LLM grounding in 3D; compared to LEO's object-centric approach. The manuscript notes 3D-LLM's data is smaller in accessible subsets and that 3D-LLM lacks some acting capabilities.",
            "pretraining_data_type": "Multi-view image + LLM based 3D grounding (exact corpora/details not provided in this paper).",
            "pretraining_data_details": null,
            "embodied_task_name": "3D grounding / 3D VL tasks (mentioned)",
            "embodied_task_description": "Not detailed here; paper notes 3D-LLM focuses on grounding but may lack acting capabilities.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": "Multi-view image inputs (as reported in the comparison discussion).",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Cited as a concurrent approach; the paper argues an object-centric 3D representation (LEO) is a simpler and effective way to connect 3D scenes with LLMs compared to 3D-LLM's dense feature lifting.",
            "uuid": "e1726.6",
            "source_info": {
                "paper_title": "An Embodied Generalist Agent in 3D World",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LL3DA",
            "name_full": "LL3DA",
            "brief_description": "A referenced dataset / system for visual interactive instruction tuning for omni-3D understanding, reasoning, and planning; used in data-comparison ablations.",
            "citation_title": "Ll3da: Visual interactive instruction tuning for omni-3d understanding, reasoning, and planning.",
            "mention_or_use": "mention",
            "model_agent_name": "LL3DA (data/system)",
            "model_agent_description": "Cited as a training data source; the authors used LL3DA training data in an ablation to show the advantage of their collected dataset — LEO trained on LL3DA data performed worse than when trained on the paper's original collected data (Tab. A.18).",
            "pretraining_data_type": "Visual interactive instruction tuning dataset for 3D tasks (paper does not enumerate the corpora inside LL3DA).",
            "pretraining_data_details": null,
            "embodied_task_name": "3D VL and instruction-following tasks (used as training data in ablation)",
            "embodied_task_description": "Used in ablation comparisons for Scan2Cap and ScanQA downstream performance; precise task descriptions are in the LL3DA source, not reproduced here.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Paper-level ablation shows LEO trained on LL3DA data underperforms LEO trained on the authors' collected data, indicating dataset design and refinement matter for transfer; no numeric sample-efficiency claims are provided.",
            "uuid": "e1726.7",
            "source_info": {
                "paper_title": "An Embodied Generalist Agent in 3D World",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control.",
            "rating": 2,
            "sanitized_title": "rt2_visionlanguageaction_models_transfer_web_knowledge_to_robotic_control"
        },
        {
            "paper_title": "Palm-e: An embodied multimodal language model.",
            "rating": 2,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        },
        {
            "paper_title": "A generalist agent.",
            "rating": 2,
            "sanitized_title": "a_generalist_agent"
        },
        {
            "paper_title": "Embodiedgpt: Vision-language pre-training via embodied chain of thought.",
            "rating": 2,
            "sanitized_title": "embodiedgpt_visionlanguage_pretraining_via_embodied_chain_of_thought"
        },
        {
            "paper_title": "3d-llm: Injecting the 3d world into large language models.",
            "rating": 2,
            "sanitized_title": "3dllm_injecting_the_3d_world_into_large_language_models"
        },
        {
            "paper_title": "Ll3da: Visual interactive instruction tuning for omni-3d understanding, reasoning, and planning.",
            "rating": 2,
            "sanitized_title": "ll3da_visual_interactive_instruction_tuning_for_omni3d_understanding_reasoning_and_planning"
        }
    ],
    "cost": 0.026368749999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Embodied Generalist Agent in 3D World
9 May 2024</p>
<p>Jiangyong Huang 
State Key Laboratory of General Artificial Intelligence
Beijing Institute for General Artifi-cial Intelligence (BIGAI</p>
<p>Peking University</p>
<p>Silong Yong 
Tsinghua University</p>
<p>Xiaojian Ma 
State Key Laboratory of General Artificial Intelligence
Beijing Institute for General Artifi-cial Intelligence (BIGAI</p>
<p>Xiongkun Linghu 
State Key Laboratory of General Artificial Intelligence
Beijing Institute for General Artifi-cial Intelligence (BIGAI</p>
<p>Puhao Li 
State Key Laboratory of General Artificial Intelligence
Beijing Institute for General Artifi-cial Intelligence (BIGAI</p>
<p>Tsinghua University</p>
<p>Yan Wang 
State Key Laboratory of General Artificial Intelligence
Beijing Institute for General Artifi-cial Intelligence (BIGAI</p>
<p>Qing Li 
State Key Laboratory of General Artificial Intelligence
Beijing Institute for General Artifi-cial Intelligence (BIGAI</p>
<p>Song-Chun Zhu 
State Key Laboratory of General Artificial Intelligence
Beijing Institute for General Artifi-cial Intelligence (BIGAI</p>
<p>Peking University</p>
<p>Tsinghua University</p>
<p>Baoxiong Jia 
State Key Laboratory of General Artificial Intelligence
Beijing Institute for General Artifi-cial Intelligence (BIGAI</p>
<p>Siyuan Huang 
State Key Laboratory of General Artificial Intelligence
Beijing Institute for General Artifi-cial Intelligence (BIGAI</p>
<p>Robotic Manipulation 
State Key Laboratory of General Artificial Intelligence
Beijing Institute for General Artifi-cial Intelligence (BIGAI</p>
<p>An Embodied Generalist Agent in 3D World
9 May 2024F8BDCDB68CCB24FCDDD5667B29B5DE3EarXiv:2311.12871v3[cs.CV]P = [𝟎. 𝟏−𝟎. 𝟐𝟎] R = [𝟎𝟎𝟎𝟏]
Leveraging massive knowledge from large language models (LLMs), recent machine learning models show notable successes in generalpurpose task solving in diverse domains such as computer vision and robotics.However, several significant challenges remain: (i) most of these models rely on 2D images yet exhibit a limited capacity for 3D input; (ii) these models rarely explore the tasks inherently defined in 3D world, e.g., 3D grounding, embodied reasoning and acting.We argue these limitations significantly hinder current models from performing real-world tasks and approaching general intelligence.To this end, we introduce LEO, an embodied multimodal generalist agent that excels in perceiving, grounding, reasoning, planning, and acting in the 3D world.LEO is trained with a unified task interface, model architecture, and objective in two stages: (i) 3D vision-language (VL) alignment and (ii) 3D vision-language-action (VLA) instruction tuning.We collect large-scale datasets comprising diverse object-level and scene-level tasks, which require considerable understanding of and interaction with the 3D world.Moreover, we meticulously design an LLM-assisted pipeline to produce high-quality 3D VL data.Through extensive experiments, we demonstrate LEO's remarkable proficiency across a wide spectrum of tasks, including 3D captioning, question answering, embodied reasoning, navigation and manipulation.Our ablative studies and scaling analyses further provide valuable insights for developing future embodied generalist agents.Code and data are available on project page.</p>
<p>Introduction</p>
<p>Building one generalist model that can handle comprehensive tasks like humans has been a long-existing pursuit in artificial intelligence and neuroscience (Lake et al., 2015;2017;Zhu et al., 2020;Mountcastle, 1979;Schmidhuber, 2018;Huang et al., 2022a).Recent advances in LLMs (Brown et al., 2020) and "foundation models" (Bommasani et al., 2021) emerge as a promising paradigm in building such generalist models in natural language processing (OpenAI, 2022;2023), computer vision (Kirillov et al., 2023;Alayrac et al., 2022), and robotics (Brohan et al., 2022;2023;Reed et al., 2022;Driess et al., 2023;Li et al., 2023c).The keys to the success of this paradigm lie in large-scale internet-level datasets from numerous tasks and domains, as well as scalable Transformer architectures (Vaswani et al., 2017) that can absorb generalizable and task-agnostic knowledge from the data.Nonetheless, existing generalist models primarily thrive within 2D domains, lacking comprehension of the 3D physical environment that envelops human-level intelligence.This limitation stands as an obstacle that prevents current models from solving realworld tasks and approaching general intelligence.Therefore, we ask a fundamental question: how to equip the generalist agent with a comprehensive understanding of and the ability to interact with the real 3D world?</p>
<p>The development of such generalist agents encounters three primary challenges: the lack of suitable datasets, unified models, and effective learning strategies.Despite substantial progress in scaling up image-text models (Tsimpoukelli et al., 2021;Alayrac et al., 2022) and the curation of corresponding datasets (Radford et al., 2021;Schuhmann et al., 2022), advancement in 3D scene-level understanding has significantly lagged behind.This is largely attributed to the limited scale and manual labeling of 3D datasets (Dai et al., 2017;Wald et al., 2019;Chen et al., 2020), given the higher cost associated with collecting 3D data compared to 2D data.Furthermore, large-scale unified pretraining and efficient finetuning are under-explored by previous 3D VL models, which are often designed with strong priors (Zhao et al., 2021;Chen et al., 2022).Notably, recent works (Zhu et al., 2023c;Hong et al., 2023) utilize multi-modal Transformer together with synthetic data to enhance the model's capability in grounded 3D scene understanding.Nevertheless,</p>
<p>Task Planning</p>
<p>Human: Organize and tidy up the bedroom.</p>
<p>Embodied Reasoning</p>
<p>Situation: Sitting at the edge of the bed and facing the couch.Question: Can I go straight to the coffee table in front of me?LEO: No.</p>
<p>3D Object Captioning</p>
<p>LEO: A 3D model of a Southern White Rhino, one of largest animal species in the world.</p>
<p>LEO: It's a 3D model of a midnight green ceramic coffee cup with a plate.</p>
<p>User: Please describe the toy house over ...</p>
<p>❄ 🔥 🔥</p>
<p>You are an assistant situated in a 3D scene.</p>
<p>❄ Tokenizer Action Response Text Response</p>
<p>There is a sofa next to the TV.</p>
<p>It's a kitchen for cooking.they fall short in embodied tasks, e.g., acting within 3D environments.Additionally, there are significant yet rarely explored problems, e.g., the potential of VLA learning and efficient adaptation of LLMs for 3D tasks.</p>
<p>Embodied Generalist Agent in 3D World</p>
<p>In this work, we introduce the generalist agent LEO, which is generically embodied, multi-modal, and general-purpose.It can take egocentric 2D images, 3D point clouds, and texts as task input and handle comprehensive tasks within the 3D environment.As shown in Fig. 1, LEO exhibits the capability of perceiving, grounding, reasoning, planning, and acting with a unified task interface, model architecture, and objective.LEO perceives through an egocentric 2D image encoder for the embodied view and an object-centric 3D point cloud encoder for the third-person global view.Such perception modules can be flexibly adapted to various embodied environments and enhance 3D reasoning.The encoded visual tokens are interleaved with text tokens to form a unified multi-modal task sequence, which further serves as the input to a decoder-only LLM.Equipped with a vocabulary containing both text and action tokens, the LLM can generate responses to various tasks simultaneously.Consequently, all the tasks are formulated as sequence prediction, thereby accommodating a unified training objective.</p>
<p>Following prior experiences (Liu et al., 2023b), we adopt a two-stage learning scheme, i.e., 3D VL alignment and 3D VLA instruction tuning.We accordingly collect largescale comprehensive datasets LEO-align and LEO-instruct, which comprise diverse object-level and scene-level tasks.Notably, we meticulously design an LLM-assisted pipeline to generate high-quality 3D VL data, wherein we propose to prompt LLMs (OpenAI, 2022) with scene graphs and Object-centric Chain-of-Thought (O-CoT) method.To further enhance quality control, we devise a series of refinement procedures via regular expression matching and scene graph retrieval.We demonstrate our approach largely enriches the data scale and diversity, meanwhile mitigating hallucination in LLM-generated data.</p>
<p>We quantitatively evaluate and ablate LEO on diverse 3D tasks, including 3D captioning (Chen et al., 2021), 3D question answering (Azuma et al., 2022), situated question answering (Ma et al., 2023), embodied navigation (Ramrakhya et al., 2022), and robotic manipulation (Shridhar et al., 2021).</p>
<p>The results indicate (i) through task-agnostic instruction tuning with a unified model, LEO achieves state-of-the-art performances on most tasks, particularly surpassing previous task-specific models; (ii) LEO shows proficiency in scene-grounded dialogue and planning, capable of generating flexible and coherent responses; (iii) LEO achieves comparable performances to state-of-the-art task-specific models on navigation and manipulation tasks, and exhibits remarkable generalization ability; (iv) LEO's strong performances stem from both data and model aspects, including the alignment stage, data diversity, generalist-style instruction tuning, and object-centric representation; (v) LEO manifests the scaling law that echoes prior findings (Kaplan et al., 2020;Reed et al., 2022;OpenAI, 2023).We also present qualitative results to illustrate LEO's versatility and proficiency in grounded 3D scene understanding.</p>
<p>In summary, our main contributions are as follows: (i) we propose LEO, the first embodied generalist agent capable of following human instructions to perceive, ground, reason, plan, and act in the 3D world; (ii) we propose a simple yet effective framework that connects object-centric 3D representation and LLM to efficiently bridge the gap between vision, language, and embodied action; (iii) we collect largescale comprehensive datasets for our two-stage generalist training scheme, and particularly propose an LLM-assisted pipeline for the generation of high-quality 3D VL data; (iv) we conduct extensive experiments to demonstrate LEO's proficiency across various tasks, and present in-depth analyses to reveal valuable insights; (v) we release the data, code, and model weights to endow the future research in embodied generalist agents.</p>
<p>Model</p>
<p>The leading design principles of LEO are two-fold: 1) It should handle the multi-modal input of egocentric 2D, global 3D, and textual instruction, and the output of textual response as well as embodied action commands in a unified architecture; 2) It should leverage pre-trained large language models (LLMs) as a powerful prior for the downstream tasks.</p>
<p>We therefore convert all data of different modalities into a sequence of tokens, illustrated below:</p>
<p>You are... (1)</p>
<p>With this representation, we formulate the learning of LEO as GPT-style autoregressive language modeling (Brown et al., 2020) given the prefix (from system message to instruction), i.e. prefix language modeling (Raffel et al., 2020).Therefore, a pretrained LLM can be used to process such sequences.Next, we will detail the tokenization of multimodal data, model architecture, training loss, and inference settings.An overview of our model can be found in Fig. 1.</p>
<p>Tokenization</p>
<p>We follow prior practices in 2D VLM (Liu et al., 2023b;Alayrac et al., 2022) and 3D VLM (Zhu et al., 2023c) to tokenize the multi-modal data in LEO.We use Sentence-Piece tokenizer (Kudo &amp; Richardson, 2018) to encode text with 32k subwords; 2D image tokens for egocentric 2D images; and object-centric 3D tokens extracted over Mask3Dbased (Schult et al., 2022) object proposals for 3D point cloud inputs.For embodied action commands, continuous actions (e.g. in manipulation) are discretized (details in Appendix D.3) to join the discrete actions (e.g.navigation) and form a unified discrete action space.We follow (Brohan et al., 2023) to map these discrete actions to the least used tokens in SentencePiece.After tokenization, all tokens are ordered into the format in (1).</p>
<p>Token Embedding &amp; LLM</p>
<p>We apply several token embedding functions to process the tokens in the sequence before sending them to the LLM.The LLM will then align these tokens of different modalities, and produce the response.Most of the responses are text and can be decoded directly.For responses that include embodied actions, we will map the reserved SentencePiece text tokens back to action commands.</p>
<p>Text &amp; 2D token embedding.For text tokens (including embodied actions that have been mapped to the reserved text tokens), an embedding look-up table is used to map them into vectors.While the egocentric 2D image is encoded by a pretrained OpenCLIP ConvNext (Liu et al., 2022) for obtaining image token embeddings.We apply MLP adapters to match the dimensions of all token embeddings.</p>
<p>Object-centric 3D token embedding.Each 3D object token (i.e., the point cloud of a 3D object) is first encoded by a pretrained point cloud encoder (e.g., PointNet++ (Qi et al., 2017)).We then adopt the Spatial Transformer introduced in (Chen et al., 2022) to further process the point cloud embedding of all objects into object-centric 3D token embeddings.In a nutshell, Spatial Transformer biases the standard attention score with relative position and size for capturing 3D relations between objects.Due to space limit, the readers are referred to (Chen et al., 2022) and Appendix D.2 for more details.</p>
<p>Pretrained LLM.We choose Vicuna-7B (Chiang et al., 2023) to process the token sequence.In order to tackle the challenging alignment and grounding problem of multimodal tokens (2D, 3D, text, embodied action) while preserving the LLM pretrained knowledge, we employ LoRA (Hu et al., 2022) to introduce additional tunable parameters to the frozen pretrained LLM.</p>
<p>Training &amp; Inference</p>
<p>We formulate the learning objective of LEO following (Brown et al., 2020;Raffel et al., 2020) in a prefix language modeling fashion.For a batch B of token sequence s, we optimize LEO via:
L(θ, B) = − |B| b=1 T t=1 log p θ (s (b,t) res |s (b,&lt;t) res , s (b) prefix ),(2)
where s prefix denotes the prefix tokens (from system message to instruction) in (1).During training, we freeze the pretrained 3D point cloud encoder and the LLM and finetune the 2D image encoder, the Spatial Transformer, and the LoRA parameters.In total, LEO has ~7B parameters and ~142M of them will be tuned.During inference, we</p>
<p>Datasets</p>
<p>Since LEO is a generalist agent that receives multi-modal inputs and follows instructions, we adopt the two-stage training proposed by Liu et al. (2023b) and split the data into two sets: (i) LEO-align (Sec.3.1) that focuses on 3D visionlanguage (VL) alignment to bridge the gap between 3D scene representation and natural language; and (ii) LEOinstruct (Sec.3.2) that targets at 3D VLA instruction tuning to endow LEO with various capabilities.The statistics and examples of these datasets can be found in Tab. 1 and Appendix C, respectively.Due to the data scarcity, we adopt LLMs to facilitate the data generation process and outline the details in Sec.3.3.</p>
<p>LEO-align: 3D Vision-Language Alignment</p>
<p>In LEO-align, we focus on 3D VL alignment.Similar to BLIP-2 (Li et al., 2023d), we train LEO to generate captions given various 3D inputs.Specifically, we collect three types of 3D captioning data: 1) object-level captions, where we align 3D individual objects with their descriptions (Luo et al., 2023); 2) object-in-the-scene captions, where the goal is to generate the referring expressions of objects in a 3D scene context (Achlioptas et al., 2020;Zhu et al., 2023c); and 3) scene-level captions, which focuses on depicting global 3D scene using natural language.Due to the space limit, we defer details including data source and components to Appendix B.1.</p>
<p>LEO-instruct: Instruction Following in 3D world</p>
<p>In LEO-instruct, LEO will be tuned to follow instructions and accomplish various 3D VLA tasks.We curate a comprehensive set of tasks that covers a broad spectrum from grounded scene understanding and reasoning (Chen et al., 2021;Ma et al., 2023), to dialogue, planning, and embodied acting (Savva et al., 2019;Shridhar et al., 2021).Specifically, we introduce 1) 3D captioning and question answering -given 3D scene input, the agent needs to generate a natural language response to describe the scene or answer questions; 2) 3D dialogue and task planning, where the agent is expected to generate flexible and coherent responses to complex instructions with respect to the given 3D scene, and 3) navigation and manipulation, which require the agent to accomplish a variety of embodied acting tasks in the 3D scene.We defer details to Appendix B.2.</p>
<p>LLM-assisted 3D-language Data Generation</p>
<p>As mentioned above, at the core of producing a large proportion of LEO-align and LEO-instruct is the assistance of LLMs.We now detail the key techniques of prompting LLMs (i.e., ChatGPT) to generate 3D-text paired data.An overview can be found in Fig. 2.</p>
<p>Scene-graph-based prompting.Our data generation pipeline starts with 3D scene graphs from 3DSSG (Wu et al., 2021), which provide scene contexts for prompting.Compared to counterparts that utilize object boxes (Yin et al., 2023;Hong et al., 2023;Wang et al., 2023e)</p>
<p>Refinement Procedures 1. Wrong Answers</p>
<p>There are two curtains in the room.</p>
<ol>
<li>Answer Refinement There are 3 curtains in the room.</li>
</ol>
<p>Responses with ID</p>
<p>The kitchen cabinet-9 and kitchen counter-10 are parts of the kitchen.</p>
<p>GPT-based rewriting</p>
<p>The kitchen features a cabinet and a counter.</p>
<p>Negative Responses</p>
<p>Question: Where is the football table?Thought: football table-17 Answer: unknown</p>
<p>Removing Negative Responses</p>
<p>The negative responses will be removed.</p>
<p>Messages</p>
<p>Potential Issues: 1) Wrong Answers 2) Negative Responses 3) Responses with IDs 3) Others Refinement procedures.Upon the scene graph and O-CoT prompting, we introduce refinement as an additional safeguard to the quality and reliability of our generated data.Specifically, we send raw LLM responses to several human-defined filters based on the 3D scene graphs: negative responses (e.g., lacking the necessary information to answer) will be removed; unnatural narratives will be rewritten, etc. Further, we detect text that involves logical reasoning (e.g., counting) or hallucination, and manually fix the wrong responses according to the ground truth provided by scene graphs.We provide illustrative examples in Fig. 2 and Appendix B.6, and quantitative analysis on the impact of data refinement procedures in Appendix I.1.</p>
<p>Assess the quality of generated data.In addition to data examples, we propose to assess the quality of generated data quantitatively.We focus on the LLM-produced question-answer pairs about objects (questions starting with How many/Is there and ending with in the room/bedroom/kitchen/living room/bathroom).We first divide these pairs into three categories: counting, existence, and non-existence, which examines the number of certain objects/whether an object exists/whether an object does not exist in the scene, respectively.We manually check if the answers in these pairs are correct, and report the overall accuracy.Results in Tab. 2 demonstrate that our proposed scene-graph-based prompting, O-CoT prompting and refinement bring consistent improvement to data quality and the complete data generation pipeline outperforms a recent counterpart (3D-LLM).We also demonstrate how we help address the grammatical errors compared to counterparts in Appendix B.9. Finally, we provide the data distribution in Appendix B.10 to illustrate the diversity of our generated data.</p>
<p>Capabilities and Analyses</p>
<p>We demonstrate LEO's capabilities by a comprehensive evaluation on the full spectrum of embodied 3D tasks encompassing perceiving, grounding, reasoning, planning, and acting.In Sec.4.1, we present quantitative comparisons between LEO and state-of-the-art models on various 3D VL tasks, underscoring LEO's proficiency in 3D VL understanding and reasoning.In Sec.4.2, we highlight LEO's strength in scene-grounded dialogue and task planning.In Sec.4.3, we extend LEO to embodied acting tasks wherein LEO exhibits remarkable versatility.In Sec.4.4, we conduct ablative studies to reveal more insights into LEO, including data and model aspects.In Sec.4.5, we probe the scaling effect and manifest the potential for further development.</p>
<p>Table 4: Quantitative comparison with state-of-the-art models on 3D VL understanding and embodied reasoning tasks."C" stands for "CIDEr", "B-4" for "BLEU-4", "M" for "METEOR", "R" for "ROUGE", "Sim" for sentence similarity, and "EM@1" for top-1 exact match.The n-gram metrics for Scan2Cap are governed by IoU@0.5.† indicates answering questions via prompting GPT-3 with the generated scene caption.Gray indicates evaluation results with refined exact-match protocol.Overview.Understanding and reasoning about object attributes, object relations, and other facets of 3D scenes from an agent's egocentric perspective is a fundamental capability of an embodied generalist agent in the 3D world.We investigate how well can LEO perform 3D VL understanding and embodied reasoning tasks, especially when being compared against task-specific models and existing generalist agents.Specifically, we consider three renowned 3D tasks: 3D captioning on Scan2Cap (Chen et al., 2021), 3D QA on ScanQA (Azuma et al., 2022), and 3D embodied reasoning on SQA3D (Ma et al., 2023).Our evaluation metrics include conventional scores (e.g., CIDEr, BLEU, ME-TEOR, ROUGE) and other metrics adapted for open-ended generation, e.g., sentence similarity (Reimers &amp; Gurevych, 2019) and refined exact-match accuracy (see details in Appendix H.1). Following 3D-VisTA (Zhu et al., 2023c), we use object proposals from Mask3D (Schult et al., 2022) instead of ground-truth object segments for evaluation.
Scan2Cap (val) ScanQA (val) SQA3D (test) C B-4 M R Sim C B-4 M R EM@1 EM@1 Task-specific
Baselines.For quantitative comparisons, we include both task-specific approaches and generalist models: 1) stateof-the-art specialists in 3D dense captioning (Chen et al., 2021;Cai et al., 2022;Chen et al., 2023); 2) state-of-the-art specialists in 3D QA (Azuma et al., 2022;Ma et al., 2023); 3) task-specific fine-tuned generalist models like 3D-VisTA (Zhu et al., 2023c) and 3D-LLM (Hong et al., 2023).To the best of our knowledge, LEO is the first model that, in stark contrast to prior models, can directly handle the aforementioned 3D VL tasks in a unified architecture without task-specific fine-tuning.This lends greater credence to LEO's comparative superiority.</p>
<p>Results &amp; analysis.As shown in Tab. 4, LEO surpasses both state-of-the-art single-task and task-specific fine-tuned models significantly on 3D dense captioning and 3D QA tasks.In contrast to the specialist models that utilize taskspecific heads, our LLM-based approach not only affords the flexibility of generating open-ended responses but also ex-hibits excellent quantitative results.On the other hand, considering the complicated feature aggregation in 3D-LLM, we believe that object-centric 3D representation is a simple yet effective option to connect 3D scenes with LLM while harnessing the inherent knowledge of LLM.</p>
<p>Scene-grounded Dialogue and Planning</p>
<p>Overview.Upon the 3D VL understanding and reasoning, we anticipate LEO to support more sophisticated interaction with humans, e.g., responding to complex multi-round user instructions in the 3D world.To verify these capabilities, we conduct qualitative studies on 3D dialogue and planning tasks, with unseen scenarios from the held-out test sets of LEO-instruct.We defer the quantitative results of dialogue and planning to our ablation study in Sec.4.4.Quantitative comparison with other approaches is infeasible given the absence of comparable benchmarks.</p>
<p>Results &amp; analysis.As shown in Fig. A.1, LEO is capable of generating high-quality responses, which encompass two features: 1) Precisely grounded to the 3D scenes.The task plan proposed by LEO involves concrete objects related to the 3D scene, as well as plausible actions regarding these objects.2) Rich informative spatial relations.The entities in LEO's responses often accompany detailed depictions.Such information helps identify specific objects in complex 3D scenes and affords considerable assistance to humans.</p>
<p>Embodied Action in 3D World</p>
<p>Overview.To probe LEO's capacity of bridging visionlanguage-acting in the 3D world, we select two canonical embodied AI tasks: embodied navigation (ObjNav) on AI Habitat (Ramrakhya et al., 2022) and robotic manipulation on CLIPort (Shridhar et al., 2021).Specifically, for CLI-Port robotic manipulation, we evaluate LEO on the three tasks listed in Tab. 5 including their unseen counterparts, and report the success scores.For ObjNav, we evaluate LEO on the original MP3D ObjNav validation split.Ad- ditionally, we test generalization to the validation split of the newly introduced HM3D ObjNav task (Ramakrishnan et al., 2021).We report the success rate and SPL metrics following Ramrakhya et al. (2022).We consider both Habitat-web (Ramrakhya et al., 2022) (fully supervised) and ZSON (Majumdar et al., 2022) (zero-shot) as baselines.</p>
<p>Results &amp; analysis.We present the results of CLIPort manipulation and object navigation in Tabs. 5 and 6.Our findings are as follows: 1) In robotic manipulation, LEO is comparable to state-of-the-art performances and even better on some challenging unseen tasks.In particular, LEO directly produces motor commands without inductive bias (e.g., heatmap) that benefit previous methods, showcasing LEO's considerable capacity for learning embodied actions.</p>
<p>2) In ObjNav, LEO achieves a success rate that is comparable to the baselines and has a better SPL on MP3D-val, suggesting that LEO can leverage the object-centric 3D scene input (potentially offering a coarse global map) and take a shorter path to the target.Furthermore, results on HM3Dval confirm LEO's zero-shot generalization to novel scenes.Notably, all baselines are equipped with recurrent modules while LEO only incorporates truncated past actions, which could account for a lower success rate (see discussion in Appendix H.2). 3) Overall, the two-stage learning scheme endows LEO with semantic-level generalization (novel objects, etc.) in both manipulation and navigation tasks.We demonstrate the efficacy of tackling embodied acting tasks with a general framework from 3D VL.</p>
<p>Additional results.We further investigate the perception modules, data regime, and generalization to unseen objects in ObjNav task.See the results in Appendix I.4.</p>
<p>More Insights into LEO</p>
<p>Overview.In this section, we aim to offer deeper insights into LEO's characteristics, mainly from the data perspective (model perspective is deferred to Appendix G.2). Specifically, we evaluate LEO when trained with different data configurations, including exact match, sentence similarity, and human rating.We regard LEO instruction-tuned without embodied acting tasks (w/o Act) as the default setting.</p>
<p>Following Achlioptas et al. (2020), we use ground-truth object segments in these analyses.We present additional analyses on data in Appendix I.2 and model in Appendix I.3.</p>
<p>Alignment stage.In contrast to complete two-stage training (w/o Act), we direct instruction-tune a model without alignment stage (w/o Align).The results in Tab.7 show the consistent impact of alignment.In particular, the benefit of alignment is significant on Scan2Cap since it concerns detailed scene understanding and captioning, which is a primary focus of alignment training.</p>
<p>Specialist vs. generalist.We train a specialist on ScanNet scenes (ScanNet).As shown in Tab. 7, ScanNet performs slightly worse than w/o Act even on ScanNet tasks, and particularly struggles at generalization across scenes (3RQA) and tasks (3RDialog and 3RPlan).This demonstrates the advantage of generalist-style instruction tuning with broad coverage of scenes and tasks.</p>
<p>VL vs. VLA.We compare w/o Act and VLA, which differ in whether embodied acting tasks are included for training.</p>
<p>The results in Tab.7 show that incorporating embodied acting tasks could lead to performance drops on 3D VL tasks.This may stem from 1) the gap between language generation and embodied action prediction, and 2) the imbalanced data scale of embodied acting tasks.In contrast to the finding that VL data benefits embodied acting tasks in VLA co-training (Brohan et al., 2023), our observation implies that embodied acting tasks may harm VL capabilities in turn.How to continually bridge the gap between VL and embodied acting tasks is an important direction for further exploration.</p>
<p>Dialogue and planning data.In contrast to the default model (w/ Dialg in Tab.8), we train LEO without dialogue and planning data (w/o Dialg).We design an evaluation set with three types of questions (Answerable, Unanswerable, and NLP) and evaluate with TrueSkill (Graepel et al., 2007) according to human preference (see details in Appendix G.3).The results in Tab. 8 confirm more hallucinations (less preferred by users on "Unanswerable") and worse NLP skills for w/o Dialg.This is probably because 1) the diverse conversations in our dialogue data can help cultivate flexible responses to complex instructions, and 2) our planning data can offer scene-grounded commonsense knowledge and also encourage detailed coherent text.</p>
<p>Data balancing.We find imbalanced data could induce hallucination in LEO, e.g., it tends to respond with "Yes" when asked "Is there something in this room?".To address this, we augment the 3RScanQA data with more negative samples where non-existent objects are queried.We also design an evaluation set with different types (Yes and No) of object-existence questions (see details in Appendix G.4). Results in Tab. 9 demonstrate that we can effectively mitigate the hallucination problem by balancing the tuning data.Moreover, the benefit of augmenting 3RScan data can transfer to ScanNet scenes in a zero-shot manner.</p>
<p>Scaling Law Analysis</p>
<p>Settings.We study the scaling effect (Kaplan et al., 2020;Reed et al., 2022) of data and model in LEO by tracking the instruction-tuning loss on the test set with the growth of data scale.In addition to the default Vicuna-7B, we incorporate two LLMs at different scales: OPT-1.3B(Zhang et al., 2022) and Vicuna-13B (Chiang et al., 2023).For Vicuna-7B, we also probe the influence of alignment (Scratch vs. Aligned).</p>
<p>Results &amp; analysis.From the test loss curves in Fig. 3, we have the following findings: 1) The instruction tuning of LEO conforms to the scaling law (Kaplan et al., 2020;Reed et al., 2022).We observe that all curves decrease log-linearly with the data scale.2) Scaling up LLM leads to consistent improvements.Aligned Vicuna-7B shows significantly lower losses than Aligned OPT-1.3B.In contrast, despite the consistent improvements, the gap between Aligned Vicuna-7B and Vicuna-13B appears less significant, suggesting potential saturation if we continue to scale up the LLM.This indicates the scalability of LEO and the necessity of scaling up data to match the model capacity.</p>
<p>3) Alignment leads to consistent improvements.Aligned Vicuna-7B shows consistently lower losses than Scratch Vicuna-7B, which corresponds to the inferior performances of w/o Align in Tab. 7.</p>
<p>Related Work</p>
<p>Generalist agents.The AI community has witnessed the rising generalist models in both vision (Lu et al., 2023;Wang et al., 2023b;Kirillov et al., 2023) and language (Ope-nAI, 2022; 2023) domains.A generalist agent requires additional embodiment knowledge to interact with the environment and complete embodied acting tasks.Existing efforts towards generalist agents include: grounded reasoning and task planning in the real world (Ahn et al., 2022;Huang et al., 2022b), skill generalization in open-world environment (Fan et al., 2022;Cai et al., 2023a;Wang et al., 2023d;a;Cai et al., 2023b;Gong et al., 2023b), general robotic manipulation (Brohan et al., 2022;Jiang et al., 2023;Gong et al., 2023a), and unified vision-language-action (VLA) models such as Gato (Reed et al., 2022), PaLM-E (Driess et al., 2023), EmbodiedGPT (Mu et al., 2023), and RT-2 (Brohan et al., 2023).LEO belongs to the VLA model, however, its goal is to build a generalist agent that can understand the real 3D world beyond 2D images, which is absent in existing works.</p>
<p>Multi-modal instruction tuning.Pre-trained LLMs demonstrated practical for solving vision-language tasks (Tsimpoukelli et al., 2021;Alayrac et al., 2022;Guo et al., 2023;Li et al., 2023d;Zhao et al., 2023).Meanwhile, the instruction-tuning paradigm exhibited strong zero-shot generalization in NLP tasks (Wei et al., 2022;Sanh et al., 2022;Ouyang et al., 2022;Chung et al., 2022).The two streams merged into instruction-tuned LVLMs (Liu et al., 2023b;Zhu et al., 2023b;Ye et al., 2023;Gao et al., 2023;Li et al., 2023b;Gong et al., 2023c;Dai et al., 2023).Despite the burst, these models are confined to 2D visual modalities, e.g., image or video.Concurrent works (Yin et al., 2023;Hong et al., 2023;Wang et al., 2023e;Xu et al., 2023) extend to 3D vision tasks, but these models either lack the acting capability or unified efficient architecture.</p>
<p>Grounded 3D scene understanding.One key obstacle to building LEO is grounding the 3D world with natural languages.There exist diverse methods of grounded scene understanding, e.g., spatial relation modeling (Zhao et al., 2021;Chen et al., 2022;Zhu et al., 2023c) and fine-grained open-scene understanding (Peng et al., 2023b;Kerr et al., 2023).However, due to data scarcity, how to utilize LLMs to ground the 3D scene is rarely explored.Recently, 3D-LLM (Hong et al., 2023) leverages multi-view images and Chat-3D (Wang et al., 2023e) uses object-centric point clouds to enable the LLMs with 3D grounding.In this work, we devise both 2D and 3D encoders for grounding various visual representations and employ LoRA (Hu et al., 2022) to efficiently fine-tune the LLMs.</p>
<p>3D data prompting from LLMs.LLMs exhibit extraordinary capabilities of text generation and serve as a source for collecting diverse instruction-following data (Wang et al., 2023c;Taori et al., 2023;Peng et al., 2023a).However, the lack of access to visual modalities makes it troublesome to collect visual instruction-tuning data.To address this issue, existing methods provide bounding boxes (Liu et al., 2023b) and add dense captions (Li et al., 2023a;Liu et al., 2023a) as image descriptions or directly use off-the-shelf large vision-language models (LVLM) (Zhu et al., 2023a;Luo et al., 2023) to help collect such data.Unlike concurrent attempts (Yin et al., 2023;Hong et al., 2023;Wang et al., 2023e) in collecting 3D instruction-tuning data, our approach features a scene-graph-based prompting and refinement method to prompt and correct the data.</p>
<p>Conclusions</p>
<p>The proposed agent LEO extends the current generalist ability of LLMs from text towards the 3D world and embodied tasks.It is a crucial initial step towards building embodied generalist agents.Nonetheless, there are also limitations, e.g., generalization to novel scenes, and a notable gap between VL learning and embodied action control.In light of this work, we identify several promising directions that hold the potential for substantial advancement: (1) enhancing the 3D VL understanding capability by leveraging larger-scale VL data from richer 3D domains;</p>
<p>(2) continually bridging the gap between 3D VL and embodied action, as our experiments reveal the efficacy of their joint learning;</p>
<p>(3) investigating the issues of safety and alignment in the context of embodied generalist agents, particularly given that our scaling law analysis suggests significant enhancements through scaling on data and model.</p>
<p>Impact Statement</p>
<p>This work introduces LEO, an embodied multi-modal generalist agent designed to extend machine learning capabilities into the 3D realm, marking a significant advance in the field.The potential societal implications of LEO are manifold, touching on robotics, AR/VR, assistive technologies, and environmental planning.Ethically, it underscores the importance of responsible AI development, emphasizing safety, privacy, and fairness in automated decision-making.</p>
<p>As LEO ventures into new territories of human-machine interaction, it prompts a re-evaluation of ethical frameworks to ensure that advancement contributes positively to society.While the immediate societal consequences of our work align with the goals of advancing machine learning, we acknowledge the necessity of ongoing ethical consideration as applications of LEO evolve.</p>
<p>A. Qualitative Results</p>
<p>To right of toilet</p>
<p>Where is the bathtub located in the bathroom?</p>
<p>There is a rectangular brown table.It is in the center of the room.Please navigate to the toilet in the scene.Object-level caption.To facilitate object-level grounding of detailed object attributes, we leverage Cap3D (Luo et al., 2023), which contains language descriptions for objects in Objaverse (Deitke et al., 2023).Given a single 3D object as input, LEO will be asked to predict its caption.</p>
<p>Object-in-the-scene caption.For a better understanding of how an object can be related to others (spatial relations, etc.) when situated in a 3D scene, we collect referring expressions of objects in scenes from existing datasets, including ScanScribe (Zhu et al., 2023c) and ReferIt3D (Achlioptas et al., 2020).Further, we generate additional object-referring expressions on 3RScan (Wald et al., 2019) scenes by prompting LLMs (details in Appendix B.4).During alignment, LEO needs to predict these referring expressions given the object-centric 3D input of the scene and the referred object.</p>
<p>Scene-level caption.Finally, we encourage LEO to capture scene-level descriptions of a 3D scene.These scene-level captions focus on global information depicting key objects in the scene as well as their attributes and functionalities, relations among multiple objects, and room types and styles.We leverage scene graph annotations (Wald et al., 2019) and prompt LLMs to produce a total of ~20K captions.To further increase caption diversity, we propose a subgraph sampling strategy to prevent LLMs from always attending to certain notable facets of the scene (details in Appendix B.7).Similar to previous settings, LEO needs to predict these captions given the corresponding 3D input.</p>
<p>B.2. More Details on LEO-instruct</p>
<p>Below, we provide a comprehensive illustration of the data preparation process for these tasks and an overview of generated data in Fig. 2. We list the corresponding instructions in Appendix C.</p>
<p>3D captioning.The task is to produce a generic caption given 3D input.We adopt the Scan2Cap dataset (Chen et al., 2021), which is based on the ScanNet (Dai et al., 2017) 3D scenes and covers various levels (object-level and scene-level) and aspects (attributes, relations, etc.) of scene details.</p>
<p>3D question answering.The 3D-QA task is an extension of VQA (Antol et al., 2015) to 3D scenes with a focus on 3D knowledge, ranging from spatial relations to functionalities of objects.For this task, we first aggregate two existing 3D-QA datasets: ScanQA (Azuma et al., 2022) and SQA3D (Ma et al., 2023).To further generate questions concerning rich 3D knowledge, we prompt LLMs to generate ~35K QA pairs on 3RScanQA with our quality refinement techniques discussed in Sec.3.3.</p>
<p>3D dialogue.The goal of this task is to support natural conversations between LEO and users about a given 3D scene.This task necessitates coherence and continuity across multiple rounds of conversational interactions.We build such dialogues on 3RScan scenes by prompting LLMs with a variant of the Chain-of-Thought prompting method discussed in Sec.3.3 to facilitate diverse dialogues about relevant and accurate details about the 3D scene.In total, ~11K dialogues are collected.</p>
<p>Scene-aware task planning.In this task, LEO is required to decompose high-level tasks into step-by-step low-level plans given 3D scenes.We expect LEO to generate feasible plans based on the current 3D scene and ground its inherent common sense knowledge about procedures to the scene configurations, including, objects, their attributes, relations, and functional characteristics, etc.By prompting LLMs, we end up collecting ~14K task-plan pairs on 3RScan scenes.</p>
<p>Embodied navigation.We follow imitation learning setting in Habitat-web (Ramrakhya et al., 2022) for the embodied navigation task.We choose ObjNav, where LEO needs to map navigation instructions (e.g."find bed"), object-centric 3D input, and an egocentric 2D input into discrete habitat motor commands.For simplicity, we use shortest path navigation trials rather than human demonstrations for learning as they are less noisy and therefore easier to learn when provided with the 3D scene.In total, we generate ~60K navigation episodes out of the MP3D ObjNav training scenes (Savva et al., 2019) for this task.</p>
<p>Robotic manipulation.We employ a subset of the manipulation tasks introduced in CLIPort (Shridhar et al., 2021).The input of this task includes instructions, egocentric 2D observations, and object-centric 3D information.As discussed in Sec.2.1, we discretize the continuous action space of CLIPort into bins to unify the action decoding of navigation and manipulation (more details in Appendix D.3).We generate 100K demonstrations for each selected manipulation task.</p>
<p>B.3. Design of Seed Tasks for LLM-assisted 3D Data Generation</p>
<p>Object Scene Caption &amp; Scene Caption.To align the 3D scene/object with language, we prompt ChatGPT to curate these two types of caption data.Object Scene Caption includes the spatial relationships of the object with some adjacent objects in the scene.Scene Caption is the comprehensive description for the whole 3D scene, including some key objects and their spatial relationships.</p>
<p>QA &amp; Dialogue.For QA, we design several question-answer pairs given a scene graph.A diverse set of questions are asked about the 3D scene, including the object attributes, object counting, object existence, spatial relationships between the objects, object types, object affordance, room type and so on.For dialogue, we design a conversation between the assistant and a person asking questions about this scene.The answers are in a tone as if the assistant is understanding the scene and helping the person.Different from single-round QA, dialogue has some high-level tasks such as 'searching for specific messages = [{''role'': ''system'', ''content'': ''You are an AI visual assistant in a 3D scene.The scene contains some objects, which compose a scene graph in json format.Each entity in the scene graph denotes an object instance, with a class label and an object id.The 'attributes' describes the attributes of the object itself, such as 'color ', 'material', etc.The 'relations' describes the spatial relations with other objects.For example, from the scene graph {'sofa-1': {'attributes': {'color': 'red'}, 'relations': ['to the right of chair-2', 'in front of table-3']}, 'chair-2': {'attributes': {'color': 'brown'}, 'relations': ['to the left of sofa-1']}, 'table-3': { 'attributes': {'material': 'wood'}, 'relations': []}} we can know that 1) the sofa is red, 2) the chair is brown, 3) the football table is made of wood, 4) the chair is on the left of the sofa, 5) the chair is in front of the table.</p>
<p>All spatial positional relationships must be directly derivable from the 'relations', and any spatial relationship between objects with uncertainty cannot appear in the answer.</p>
<p>You need to generate meaningful conversations based on the scene information.The conversations include questions from human and responses from an AI assistant.Ask questions about the object types, counting the objects, object attributes, relative positions between objects.Also ask questions concerning commonsense, e.g., how the objects can be used by human and human activity in the scene.You can ask questions about the affordance of the objects in the scene.The questions should conform to the given scene information.The attributes of objects and spatial relations between objects can only be inferred from the 'attributes' and 'relations' in scene graph, respectively.The questions should contain interrogative sentences and declarative sentences to cover diverse tones.You need to first provide the context of the dialogue.The context can be high level or low level tasks.The dialogue should be related to the context.Then you need to provide the clues about the question.</p>
<p>Then the robot answers the question according to the thought.Planning.To include a deeper understanding of the global 3D scene information, we prompt ChatGPT to generate a high-level task and 5-10 action steps(interaction between the assistant and the objects in the scene) to finish the task.</p>
<p>B.4. Prompts for LLM-assisted 3D Data Generation</p>
<p>In Fig. A.2-A.6, we show the prompts for five types of LLM-assisted 3D-language data generation.We provide few-shot examples as the context.In each example, the "content" contains a scene graph, and the "response" refers to a human-labeled response.The query is a new scene graph, based on which ChatGPT (OpenAI, 2022) generates responses.We require the response generated by the ChatGPT should include the dialogue context as well; the "thought" contains the involved objects in the question, which is used to enhance the reliability of the answer.These two components will be removed after the refinement procedures.</p>
<p>messages = [{''role'': ''system'', ''content'': ''You are an AI visual assistant in a 3D scene.The scene contains some objects, which compose a scene graph in json format.Each entity in the scene graph denotes an object instance, with a class label and an object id.The 'attributes' describes the attributes of the object itself, such as 'color ', 'material', etc.The 'relations' describes the spatial relations with other objects.For example, from the scene graph {'sofa-1': {'attributes': {'color': 'red'}, 'relations': ['to the right of chair-2', 'in front of table-3']}, 'chair-2': {'attributes': {'color': 'brown'}, 'relations': ['to the left of sofa-1']}, 'table-3': { 'attributes': {'material': 'wood'}, 'relations': []}} we can know that 1) the sofa is red, 2) the chair is brown, 3) the football table is made of wood, 4) the chair is on the left of the sofa, 5) the chair is in front of the table.</p>
<p>All spatial positional relationships must be directly derivable from the 'relations', and any spatial relationship between objects with uncertainty cannot appear in the answer.</p>
<p>You need to generate 10-15 question-answer pairs based on the scene information.</p>
<p>The question-answer pairs include the object types, counting the objects, object attributes, relative positions between objects.The questions should conform to the given scene information.The attributes of objects and spatial relations between objects can only be inferred from the 'attributes' and 'relations' in scene graph, respectively.The questions must be able to be answered correctly based on the scene graph.You need to provide the queried object.Note that all answers to the questions must be single words or phrases.The question answer pair should be following format:\nQ: <question>\nT: <queried object(s)>\nA: <Answer>.You can answer the question according to the queried object(s).If there is no information about the question, the <Answer> should be 'unkown'.''}]for sample in few_shot_samples: messages.append({''role'':''user'', ''content'': sample['content']}) messages.append({''role'':''assistant'', ''content'': sample['response']}) messages.append({''role'': ''user'', ''content'': '\n'.join(sample['query'])})messages = [{''role'': ''system'', ''content'': ''You are an AI visual assistant that can analyze a 3D scene.The scene contains some objects, which compose a scene graph in json format.Each entity in the scene graph denotes an object instance, with a class label and an object id.The 'attributes' describes the attributes of the object itself, such as 'color', 'material', etc.The 'relations' describes the spatial relations with other objects.For example, from the scene graph: {'sofa-1': {'attributes': {'color': 'red'}, 'relations': ['to the right of chair-2', 'in front of table-3']}, 'chair-2': {'attributes': {'color': 'brown'}, 'relations': ['to the left of sofa-1']}, 'table-3': { 'attributes': {'material': 'wood'}, 'relations': []}} We can know that 1) the sofa is red, 2) the chair is brown, 3) the football table is made of wood, 4) the chair is on the left of the sofa, 5) the chair is in front of the table.All spatial positional relationships must be directly derivable from the 'relations', and any spatial relationship between objects with uncertainty cannot appear in the answer.Do not use the id of the object in the dialogue, use ordinal words and attributes to refer to different objects with the same label.</p>
<p>Using the provided scene graph, design a high-level task that can be performed in this 3D scene.Besides, decomposing this high-level task into a sequence of action steps that can be performed using the instances in this3D scene.Remeber, the high-level task and action steps must be able to be performed in the 3D scene using the given object instances.Do not use IDs of the objects('<object>-<ID>' or '<object> <ID>') in the planning.''}]for sample in fewshot_samples: messages.append({''role'':''user'', ''content'': sample['content']}) messages.append({''role'':''assistant'', ''content'': sample['response']}) messages.append({''role'': ''user'', ''content'': '\n'.join(sample['query'])})</p>
<p>B.5. Analysis of the Object-Centric Chain-of-Thought</p>
<p>To further investigate the impact of Object-centric Chain-of-Thought (O-CoT) on data quality, we analyze the answer accuracy for Object Counting questions.Specifically, we collect several demonstrations, and for each run, we select two of them as the prompt seed.With these seeds, we generate dialogues across all scenes in 3DSSG (Wu et al., 2021) and then messages = [{''role'': ''system'', ''content'': ''You are an AI visual assistant in a 3D scene.The scene contains some objects, which compose a scene graph in json format.Each entity in the scene graph denotes an object instance, with a class label and an object id.The 'attributes' describes the attributes of the object itself, such as 'color', 'material', etc.The 'relations' describes the spatial relations with other objects.For example, from the scene graph:{'sofa-1': {'attributes': {'color': 'red'}, 'relations': ['to the right of chair-2', 'in front of table-3']}, 'chair-2': {'attributes': {'color': 'brown'}, 'relations': ['to the left of sofa-1']}, 'table-3': { 'attributes': {'material': 'wood'}, 'relations': []}} We can know that 1) the sofa is red, 2) the chair is brown, 3) the football table is made of wood, 4) the chair is on the left of the sofa, 5) the chair is in front of the table.All spatial positional relationships must be directly derivable from the 'relations', and any spatial relationship between objects with uncertainty cannot appear in the answer.Don't use IDs of the objects('<object label>-<ID>' or '<object label> <ID>') in the summary.</p>
<p>You need to provide a summary for a scene.The summary should be about the object types, object attributes, relative positions between objects.Also describe the scene concerning commonsense, e.g., how the objects can be used by human and human activity in the scene.The description should conform to the given scene information.The attributes of objects and spatial relations between objects can only be inferred from the 'attributes' and 'relations' in scene graph, respectively.You don't need to describe each object in the scene, pick some objects of the scene for summary.You can also summarize the room's function, style, and comfort level based on the arrangement and color of objects within the room.Your summary must not exceed 110 words.''}]for sample in few_shot_samples: messages.append({''role'':''user'', ''content'': sample['content']}) messages.append({''role'':''assistant'', ''content'': sample['response']}) messages.append({''role'': ''user'', ''content'': '\n'.join(sample['query'])})messages = [{''role'': ''system'', ''content'': ''Y ou are a helpful assistant.Y ou will receive a dictionary of an object.This dictionary provides information about a node in a scene graph, as well as its adjacent nodes.The value of the key 'object' is the object represented by the node.The value of 'relations' includes the spatial relationships with the adjacent nodes .The value of the key 'attribute' provides the attributes of the object.The value of 'edge attribute' provides a list of object attributes for the adjacent nodes.Y ou need to describe the object according to the information of the target object node.The IDs of objects cannot appear in the summary.''}]for sample in few_shot_samples: messages.append({''role'':''user'', ''content'': sample['content']}) messages.append({''role'':''assistant'', ''content'': sample['response']}) messages.append({''role'': ''user'', ''content'': '\n'.join(sample['query'])})</p>
<p>B.6. Refinement Details</p>
<p>We conduct refinement by passing raw LLM-generated responses into several human-defined filtering procedures based on the 3D scene graph.The refinement considers five raw response categories:</p>
<p>• Object Counting.The question concerns counting the target object.</p>
<p>• Object Existence.The response claims the existence of objects, which can be actually either existent or non-existent.</p>
<p>• Object Non-existence.The response claims the non-existence of objects, which can be actually either existent or non-existent.• Negative Response.The scene graph cannot provide a solid response to the question, which means the question cannot be answered and will be discarded.</p>
<p>Types Raw Responses Refined Responses</p>
<p>Object Counting There are 3 chairs in the room.I see there are two washing machines in the bathroom.</p>
<p>There are 4 chairs in the room.I see there are 4 washing machines in the bathroom.</p>
<p>Object Existence</p>
<p>Yes, there is a cutting board in the kitchen.</p>
<p>Yes, there is a computer and a monitor on the desk.However, the monitor is currently off.</p>
<p>No, there is no cutting board in the room.</p>
<p>No, there is no computer in the room.</p>
<p>Object Non-existence No, there is no stereo equipment in the room.I'm sorry, but I couldn't find a hair dryer in the bathroom.</p>
<p>Yes, there is a stereo equipment in the room.Yes, I found a hair dryer in the room.</p>
<p>Negative Response</p>
<p>No, there is nothing else mentioned in the scene graph.I'm sorry, but there is no mention of a mirror in the scene graph for the bathroom.</p>
<p>The negative responses will be removed.</p>
<p>Response with ID</p>
<p>You can place your backpack on the floor, to the left of the dining table-33.As for your bag, you can place it on the floor, to the left of the bed-10.</p>
<p>You can place your backpack on the floor, to the left of the dining table.</p>
<p>As for your bag, you can place it on the floor, to the left of the bed.The negative responses will be removed.</p>
<p>Response with ID Q: Where is the mirror located?A: attached to wall-3, behind heater-18, to the left of shelf-19 Q: Where is the mirror located?A: attached to a wall, behind a heater, to the left of a shelf</p>
<p>• Response with ID.The response contains unexpected object IDs.</p>
<p>Specifically, we employ regular expression matching to detect errors in these five categories.We also employ this method to correct the responses except for Response with ID, which will be rewritten by ChatGPT instead.The QA pair will be eliminated if multiple rounds of rewriting fail to remove the IDs.Tab.A.2 and Tab. A.3 show some examples of the responses subject to the above five categories as well as the effect of our refinement.</p>
<p>B.7. Subgraph Sampling</p>
<p>To enhance the diversity of the 3D scene graphs used for prompting, we perform subgraph sampling on the 3DSSG according to a sampling rate, which denotes the ratio of preserved nodes.The sampled subgraphs are used for generating scene captions and planning data.We analyze the distribution of node numbers across the 3DSSG dataset in  Table A.4: Sampling rates for scenes with different node numbers.The hyphen denotes a sweep of sampling rates, e.g., "0.7-0.9"means "0.7,0.8,0.9".</p>
<p>Node Number 10-20 20-30 30-40 40-50 50-60 60-70 &gt;70</p>
<p>Sampling Rate 0.8,0.90.7-0.90.6-0.90.6-0.90.5-0.90.5-0.90.4-0.9prompt seeds to further enhance the diversity of prompted data.</p>
<p>Box-based Content</p>
<p>To verify whether the subgraph sampling strategy can maintain the consistency and diversity of scene captions, we generate scene captions for the same scene using both the full graph and subgraph.We then employ GPT-4 (OpenAI, 2023) to evaluate the similarities and differences between the two captions.The results in Tab.A.5 indicate that our subgraph sampling strategy can maintain both consistency and diversity.</p>
<p>B.8. Scene-graph-based Prompting vs. Box-based Prompting</p>
<p>In this section, we provide a comparative analysis of scene-graph-based prompting and box-based prompting (Hong et al., 2023).We refer the readers to Figure 6 in 3D-LLM (Hong et al., 2023) for details of the box-based prompting method.shown in Fig. A.10, we highlight some spatial relations in red.The comparison shows that our method provides more diverse and reliable spatial relations, which are important for 3D scene understanding.</p>
<p>B.9. Additional Comparision Regarding Dataset Quality</p>
<p>In addition to assessing the factual accuracy of responses compared to 3D-LLM, we also compared the grammatical correctness of the responses with ScanScribe (Zhu et al., 2023c), a template-based synthetic dataset that focuses on 3D object caption.We observed that their dataset exhibited some grammar errors, whereas our dataset did not manifest such issues.We provide some data examples in Tab.A.6 and Tab.A.7.We highlighted the grammar errors present in ScanScribe dataset in red.Through comparison, it is evident that our sentences exhibit accurate and natural syntax, and also surpasses ScanScribe in the diversity and complexity of object descriptions.</p>
<p>B.10. Dataset Statistics</p>
<p>We provide statistics on the instruction-tuning datasets.We visualize the distribution of the question types in 3RQA (Fig. A.11) and 3RDialog (Fig. A.12).The pie chart's inner circle represents the first word of the questions, while the outer circle accounts for the second or third word in the corresponding questions.The results show that the questions cover the attributes and spatial relations of the objects, as well as high-level topics such as room types and functionalities.</p>
<p>We also provide statistics of the root noun-verb pairs for instructions and responses in 3RDialog and 3RPlan, as shown in Fig. A.13-A.16.</p>
<p>C. Data Examples</p>
<p>Please refer to Tabs.A.25-A.27 for examples of our dataset.The first portion of prompts sent into the LLM is a system message.It consists of two parts: a role prompt and a situation prompt.The role prompt is the same for all tasks:</p>
<p>You are an AI visual assistant situated in a 3D scene.You can perceive (1) an ego-view image (accessible when necessary) and (2) the objects (including yourself) in the scene (always accessible).You should properly respond to the USER's instructions according to the given visual information.</p>
<p>The situation prompt begins with a common sentence:</p>
<p>You are at a selected location in the 3D scene.</p>
<p>For SQA3D (Ma et al., 2023), the situation prompt is further extended with the situation description in the dataset.The situation prompt is only used jointly with the embodiment token to support tasks that require information about the embodiment.Details can be found in Appendix D.2.1.</p>
<p>Next the visual tokens, including 2D image tokens and object-centric 3D tokens.Each token sequence is interleaved  within text tokens and starts with a text prefix.</p>
<p>Ego-view image: {IMAGE_TOKENS} Objects (including you) in the scene: {OBJECT_TOKENS}</p>
<p>The last portion of prompts is a task-specific instruction.For object-level caption and object-in-the-scene caption, we randomly chose one sentence from 151 sentences to be the instruction.Some examples can be found in Tab.A.8.For scene-level caption, we randomly choose one from 183 instructions.Examples can be found in Tab.A.9.For 3D question answering task, we simply use the question as the instruction.The dialog history is used as the instruction for 3D dialogue to provide continuity across multiple rounds of interactions.A planning instruction pool consisting of 202 instructions is introduced for scene-aware task planning and we randomly choose one from it as done in the caption tasks.Examples from the pool can be found in Tab.A.10.The chosen instruction is further followed by an instruction that specifies the task, e.g., set up a home office."Produce a description for the object at the chosen spot in the 3D scene.","How would you depict the object located at the selected point in the 3D environment?","Formulate a description of the item at the picked position within the 3D scene.","How would you describe the entity at the designated location in the 3D backdrop?","Can you detail the object situated at the selected point in the 3D setting?", "Compose a narrative for the object at the chosen locale within the 3D environment.","What does the object at the specified position in the 3D visualization look like?", "Provide a description for the item located at the marked site in the 3D world.","How would you illustrate the object placed at the selected spot in the 3D landscape?","Craft a depiction of the object at the pinpointed location within the 3D territory.","What kind of object is illustrated at the identified site in the 3D tableau?", "Develop a description of the object at the specified position in the 3D backdrop.","What is the entity's detail at the highlighted site in the 3D view?", "Write up a description of the entity at the selected spot in the 3D realm.","What does the object look like at the pinpointed location in the 3D space?", "Detail the entity located at the chosen position within the 3D scene.","Can you explain the essence of the object at the selected spot in the 3D zone?",With past action tokens {PAST_ACTIONS} appended at the end, the instruction for embodied navigation is as follows, where {GOAL} stands for the goal specified by the target object name:</p>
<p>The task is navigation.Your goal is to find {GOAL} by moving around in the scene.Past actions: {PAST_ACTIONS}.</p>
<p>The instruction for robotic manipulation is similar to the one in embodied navigation.Here {GOAL} is the task description in CLIPort:</p>
<p>The task is manipulation.Your goal is to {GOAL}.Past actions: {PAST_ACTIONS}.</p>
<p>D.2. Feature Encoding</p>
<p>We have several modules to encode the multi-modal features.</p>
<p>• Object-centric 3D token embedding.The encoder for 3D object-centric point clouds is a PointNet++ (Qi et al., 2017) pre-trained on ScanNet (Dai et al., 2017) with object-classfication task.We sample 1024 points for every object as in (Chen et al., 2022).The architecture parameters all remain the same with (Chen et al., 2022).We freeze the PointNet++ for empirically better results.</p>
<p>• Spatial Transformer (Chen et al., 2022).Spatial Transformer is a modified transformer architecture that explicitly encodes spatial relations between object pairs.Specifically, consider the vanilla self-attention (Vaswani et al., 2017) mechanism which takes as input a feature matrix X ∈ R N ×d , where N stands for the number of tokens and d is the feature dimension.Vanilla self-attention first compute Q = XW Q , K = XW K , V = XW V from X using learnable projection matrices W Q , W K , W V ∈ R d×d h where d h stands for the output feature dimension.Then the attention weight matrix is computed by
(ω o ij ) N ×N = Ω o = sof tmax( QK T √ d h
) and finally used for re-weighting Ω o V .The intuition of Spatial Transformer is that we can re-scale the elements ω o ij in the weight matrix Ω o .In the object-centric reasoning setting, the input feature matrix is O ∈ R N ×d .Consider an object pair (O i , O j ) with their geometric centers c i , c j .Spatial Transformer (Chen et al., 2022) computes the Euclidean distance d ij = ||c i − c j || 2 and the horizontal and vertical angles θ h , θ v of the line connecting c i and c j .The spatial feature between the two objects
(O i , O j ) is a 5-dimensional vector f ij = [d ij , sin (θ h ), cos (θ h ), sin (θ v ), cos (θ v )].
To combine this feature with objects, "Describe this scene.","Generate a description of this scene.","Generate a caption of this scene.","Can you describe the scene?", "Can you generate a description of the scene?", "Can you generate a caption of the scene?", "Summarize this scene.","Provide an outline of this 3D scene's characteristics.","How would you describe the 3D scene?", "How would you summarize this scene?","Convey a summary of the 3D structure of this scene.","How would you interpret this 3D scene?", "Offer a summary of the 3D scene.","Can you describe this scene in detail?", "I'm interested in this scene, can you explain?", "What is this scene made of?", "Could you provide more info about this scene?","Plan for the task", "Can you come up with a plan for this task", "How can we do this task, provide a step-by-step plan", "Draft a plan for completing this task", "Detail a strategy for the task", "What's the best plan for this task", "Draw out a procedure for the task", "Lay out the steps for this task", "Could you devise a plan for the task", "Show me a plan for this task", "I need a plan for the task", "Sketch a plan for the task at hand", "Set up a plan for this", "Recommend a plan for this task", "Offer a strategy for this task", "Design a blueprint for the task", "Outline the approach for this task", the spatial attention computes ω s ij = g i f ij where g i = W T S o i is a 5-dimensional vector.The spatial attention further reweights the original self-attention weight matrix as
ω ij = σ(ω s ij )exp(ω o ij ) N l=1 σ(ω s il )exp(ω o il )
.</p>
<p>Readers are referred to (Chen et al., 2022) for more details.In summary, Spatial Transformer explicitly computes pairwise spatial relations and fuses them with vanilla self-attention to provide better spatial reasoning ability.We use a three-layer Spatial Transformer with 8 heads to process the object-centric features produced by PointNet++ and output object tokens for LLM.For other settings, We follow all the default hyperparameters in (Chen et al., 2022).</p>
<p>• 2D token embedding.We use OpenCLIP ConvNext-base model (Liu et al., 2022) pre-trained on LAION2B (Schuhmann et al., 2022) to process the egocentric 2D image.</p>
<p>• CLIP semantic guidance.To inject more semantics into visual tokens, we use the text encoder from CLIP (Radford et al., 2021) to process the instruction tokens to obtain a global semantics feature.Next, we update the visual tokens via element-wise product between the CLIP semantics feature and each image &amp; object token embedding.</p>
<p>D.2.1. EMBODIMENT ENCODING</p>
<p>In addition to the egocentric 2D input, we introduce an embodiment token to help LEO reason in an embodiment-aware fashion.We find it useful to use it together with the situation prompt and 2D egocentric input.Specifically, an embodiment token e is introduced in embodied navigation, embodied reasoning, and object-in-the-scene caption tasks.Specifically, e is a learnable embedding that will be inserted into the 3D object list.</p>
<p>So what does embodiment information mean in these tasks?In embodied navigation, it means the agent's position and orientation in the scene, which can be derived from a GPS and a compass sensor.The orientation of the agent is further represented by a rotation which is Fourier-embedded and mapped to a feature vector r by a linear layer.It is the same in embodied reasoning task.In the object-in-the-scene caption task, we assume the agent is situated at the location of the object that is being referred to.Therefore, embodiment information also means the location of the referred object.We obtain this location by randomly choosing a spot inside the referred object bounding box.To sum up, we could simply treat the embodiment token as a special self object, where its object embedding is learnable, and its location/orientation corresponds to the actual or assumed "agent".</p>
<p>After inserting the embodiment token, we obtain a new 3D object token list: e, s</p>
<p>3D , s</p>
<p>3D , . . ., s</p>
<p>3D</p>
<p>, where s</p>
<p>3D , i ∈ {1, 2, . . ., N } are 3D object token embeddings produced by PointNet++, along with location specified for each object (including the self-object).We can concatenate them together to get a feature matrix O ∈ R (N +1)×d and send them to the Spatial Transformer to explicitly fuse the spatial information of all the 3D objects and the self-object.</p>
<p>D.3. Action Tokenization</p>
<p>To empower LEO to exert control over an embodiment or a robot, we encode all actions within the context of Object Navigation (Ramrakhya et al., 2022) and CLIPort (Shridhar et al., 2021) tasks using the least frequently employed language tokens.Specifically, for the Object Navigation task, we allocate 4 tokens to represent actions of move forward, turn right, turn left, and stop.For the CLIPort task, we use a total of 516 tokens to discretize action poses, with 320 tokens dedicated to the x-axis pose bins, 160 tokens for the y-axis pose bins, and 36 tokens for the z-rotation bins.</p>
<p>D.4. LLM Hyperparameters</p>
<p>We set the maximum output length of our Vicuna-7B to be 256.The maximum context length is also set to 256 and if the length of the input is greater than 256, we truncate it to 256 by deleting tokens from the left (i.e., only the rightmost 256 tokens are preserved).We set rank and α in LoRA (Hu et al., 2022) to be 16 and the dropout rate to be 0. LoRA is implemented for all the projection matrices in the LLM, i.e., (W q , W k , W v , W o ) in attention modules and (W gate , W up , W down ) in MLPs.</p>
<p>The hyperparameters for inference are listed in Tab.A.11.</p>
<p>E. Alignment Setup</p>
<p>The hyperparameters for 3D VL alignment are presented in Tab.A.12.</p>
<p>F. Instruction-tuning Setup</p>
<p>The hyperparameters for 3D VLA instruction tuning are presented in Tab.A.13.  (Chiang et al., 2023), respectively.We report the evaluation results on ScanNet and 3RScan tasks in Tab.A.15.The results show a significant gap between OPT-1.3B and Vicuna-7B and comparable performances between Vicuna-7B and Vicuna-13B.This indicates the notable improvements when scaling from smaller LLM to 7B scale and the potential saturation if we continue to scale up, resembling the finding in Sec.4.5.</p>
<p>Point cloud backbone.We have tried substituting PointNet++ (Qi et al., 2017) with Point-BERT (Yu et al., 2022) as the point cloud backbone.Specifically, we utilize the Point-BERT checkpoint from PointLLM (Xu et al., 2023), which has adapted Point-BERT to 6-channel (XYZRGB) input and learned a language-aligned representation for 3D objects.We have not observed notable difference between the performances of using Point-BERT and PointNet++ so we omit the results here.</p>
<p>G.3. Dialogue and Planning Data</p>
<p>To evaluate w/o Dialg, we design an evaluation set with three types of questions: 1) Answerable: general questions that can be answered based on the given 3D scenes; 2) Unanswerable: questions that cannot be answered given the 3D scenes due to a lack of information, e.g., "Tell me about the elephant in the room"; 3) NLP: questions that solely examine the language functionality of LEO in term of factual knowledge, reasoning, and text coherence.We collect 30 representative questions for each subset and generate LEO's responses for each question.We then ask humans to choose their preferred responses between w/o Dialg and w/ Dialg Based on the human preferences, we evaluate the two models with TrueSkill (Graepel et al., 2007), which is an algorithm that quantifies players' rating scores by Bayesian inference.The scores are estimated by Gaussian distribution and expressed as µ ± σ.</p>
<p>G.4. Data Balancing</p>
<p>To investigate the hallucination problem, we collect 150 questions querying object existence on 3RScan and ScanNet respectively.We split three subsets according to the category of queried object.The queried object can exist in the given scene (Yes), exist in other scenes instead of the given scene (No-1), or not exist in all the scenes (No-2).Each subset comprises 50 questions.We merge No-1 and No-2 when reporting the exact-match accuracy, as shown in Tab. 9.</p>
<p>H. Evaluation Details</p>
<p>H.1.3D Question Answering Rationality of QA evaluation protocol.We argue that exact match (EM), as a conventional metric for 3D QA, is unsuitable for evaluating the open-ended answer generated by LLMs.For example, given the question "On what side of the towel is a bathroom curtain?" with ground-truth answer "left side of towel", it is never wrong to answer "left".However, this will be deemed incorrect if we adopt the strict exact match protocol.Such a misjudgment is quite likely to occur when evaluating the answers from LLMs.By contrast, the classifier heads for QA (e.g., MCAN) are less affected because they collect all possible answers in advance to formulate the QA as a close-set classification problem.Hence, we refine the strict I.6.ScanQA</p>
<p>We provide additional qualitative results on ScanQA validation set in Tab.A.23 and categorize the responses into several types:</p>
<p>• Wrong.The response is inaccurate and deemed wrong.</p>
<p>• Wrong but reasonable.The response is deemed wrong but is reasonable to some extent, probably due to ambiguities in the scene.Consider the second case in Tab.A.23.There are many objects such as a coat rack, a coat, and a mini fridge-shaped cabinet on the right side of the organizer.Though LEO's response "mini fridge" does not match the ground truth "coat rack", it is consistent with the 3D scene layout.</p>
<p>• Wrong but accurate.The response is accurate according to the scene but is deemed wrong due to imperfect ground truth annotations.</p>
<p>• Correct.The response is accurate and deemed correct.</p>
<p>• Correct and more accurate.The response is more accurate than the ground truth annotations.</p>
<p>I.7. SQA3D</p>
<p>We provide additional qualitative results on SQA3D test set in Tab.A.24 and follow the aforementioned response types.The embodied reasoning in SQA3D requires the understanding of not only the scene but also the situation of embodiment.In Tab.A.24, answering "What am I sitting at?" necessitates that LEO accurately identifies the objects at its current location.</p>
<p>And the response to "How many beds are in front of me?" indicates that LEO can reason based on the understanding of its orientation.This is a black keyboard.It is in front of two monitors.</p>
<p>The keyboard sits atop the workstation desk.It is located in front of the left-side monitor.</p>
<p>Focus on the object at the given 3D location and describe it.</p>
<p>This is a white pillow.It is on the couch.</p>
<p>It is a small pillow located on the couch.You can notice it directly on your left when walking through the door into the room.</p>
<p>Provide a clearer description of the object at that specific 3D spot.</p>
<p>This is a white cabinet.It is to the right of a couch.This is a white ottoman.</p>
<p>It is located to the right of the black couch.</p>
<p>What does the object at the specified position in the 3D visualization look like?</p>
<p>There is a rectangular brown door.It is next to a couch.</p>
<p>There is a door in the middle of the northern wall next to a metal cabinet.The door is shaped like a brown rectangle and as a dark yellow colored doorknob.In this scene, there is a gray flat picture hanging on the wall, standing in front of a windowsill.There is also a rectangular white padded pillow lying on the bed, close to a big rectangular white padded pillow.The room has a white architectural wall and a flat overhead ceiling.The bed is tidy and white, located in front of a narrow tall commode.The curtains are hanging on the wall.The room has a calm and comfortable atmosphere, suitable for relaxation and sleep.Great!Here's a suggestion: you can place the sofa against the wall, with the table in front of it.The chair can be placed to the right of the table, and the couch table can be placed to the right of the chair.This arrangement creates a cozy seating area.</p>
<p>LEO-instruct task planning</p>
<p>USER: Develop a plan for this: Arrange and embellish the family room.ASSISTANT:</p>
<ol>
<li>Arrange and fluff the cushions and pillows on the armchair.</li>
</ol>
<p>need help organizing my bedroom.Can you assist me?LEO: Of course!What specifically do you need help with?Human: Can you check if there are any clean clothes hanging on the wall?LEO: I'm sorry, but the clothes hanging on the wall are messy.Human: Thanks!Also, can you check if there are any clean clothes hanging on the wall?</p>
<p>Figure 1 :
1
Figure 1: The proposed embodied generalist agent LEO.It takes egocentric 2D images, 3D point clouds, and texts as input and formulates comprehensive 3D tasks as autoregressive sequence prediction.By instruction-tuning LEO, it extends the capability of LLMs to multi-modal vision-language-action tasks with a unified model.</p>
<p>Figure 2 :
2
Figure 2: Our proposed LLM-assisted 3D-language data generation pipeline and data examples.. (Top-left) Messages with 3D scene graphs, including object attributes and relations in a phrasal form, used for providing scene context when prompting LLM.(Top-right) The human-defined refinement procedures were conducted over raw LLM responses to improve data quality.(Bottom) Examples of LLM-assisted generation in LEO-align and LEO-instruct.Thoughts, colored in gray, will be removed after refinements.Appendix B.7).We provide examples of O-CoT in Fig. 2.</p>
<p>closed or open to my left side?Of course!I'd be happy to help.What do you need advice on?I'm redecorating my bedroom and I need some advice.Can you help me? 1. Wipe down the door, including the handle and any glass panels.2. Vacuum or sweep the floor to remove any dust, dirt, or debris.3. Dust and clean the cabinet, including any shelves or drawers.4. Arrange the pillows on the couch neatly and fluff them up. 5. Dust and clean the ceiling using a long-handled duster or appropriate cleaning tool.6. Check and adjust the temperature or settings of the radiator if necessary.Neatly organize and remove dirt from the living room.I want to add some color to the room.Are there any colorful objects in the room?Yes, there is a colorful picture hanging on the wall.It is in front of a shelf and behind an armchair.It adds a nice pop of color to the room.That sounds great!What other decorations or furniture are in the room?There is a wardrobe to the right of a stuffed animal.It is in front of a doorframe.Additionally, there are shelve attached to the wall, with various item displayed on them.Thank you for the information!It's really helpful for my redecorating plans.You're welcome!I'm glad I could assist you.If you have any questions, feel free to ask.Pack the black shoe with green stripes in the brown box.([0.29,0.07, 0.06], [0., 0., 0., 1.0]), ([0.53, -0.18, 0.01], [0., 0., 0.38, 0.92]).</p>
<p>Figure A.1: Qualitative results of interacting with LEO on unseen scenarios from a held-out test set of LEO-instruct.LEO's responses and actions can be grounded in novel scenes.B. Data B.1.More Details on LEO-align</p>
<p>Figure A. 2 :
2
Figure A.2: The prompt for generating 3D Dialogue.</p>
<p>Fig</p>
<p>Fig. A.2  shows the prompt for generating 3D dialogue data.Red fonts outline our requirements of the dialogue content, including object attributes, spatial relations, and commonsense topics.Purple fonts formulate the template of the response.We require the response generated by the ChatGPT should include the dialogue context as well; the "thought" contains the involved objects in the question, which is used to enhance the reliability of the answer.These two components will be removed after the refinement procedures.</p>
<p>Figure A. 3 :
3
Figure A.3: The prompt for generating 3D QA.</p>
<p>Figure A. 4 :
4
Figure A.4: The prompt for generating 3D planning.</p>
<p>Figure A. 5 :
5
Figure A.5: The prompt for generating 3D scene caption.</p>
<p>Figure A. 6 :
6
Figure A.6: The prompt for generating 3D object-in-the-scene caption.</p>
<p>Fig. A.7 and set different sampling rates for scenes with different numbers of nodes in Tab.A.4.For each sampling rate, we set 4 random</p>
<p>Figure A. 7 :
7
Figure A.7: The distribution of node numbers for 3DSSG scenes.The node number represents the number of objects in a scene.</p>
<p>Figure A. 8 :
8
Figure A.8: Comparison of the content between box-based and scene-graph-based prompting.</p>
<p>Fig. A.8  shows the contents of two methods.To present a fair comparison between the two methods, we prompt with 1) demonstrations that have similar content under the same scene (seeFig.A.9) and 2) identical new scene queries.Since</p>
<p>Figure A. 10 :
10
Figure A.10: The responses of two prompting methods.Descriptions highlighted in red show our method leads to more flexible and reliable spatial relations.</p>
<p>2</p>
<p>The counter is standing on the wooden floor.It is close to 4 stools and an armchair.The floor is made of wooden material and is flat in shape.It is also clean.The armchair is narrow in size while the stools are simple in design.Three of the stools are wide in size.ScanScribe 1This is a counter.It is close by the armchair.It is close by the stool.A counter is close by the this counter.A counter is close by the this counter.</p>
<p>Figure</p>
<p>Figure A.11: Question types: 3RQA.Figure A.12: Question types: 3RDialog.</p>
<p>Figure</p>
<p>Figure A.13: Noun-verb pairs: 3RDialog instruction.Figure A.14: Noun-verb pairs: 3RPlan instruction.</p>
<p>Figure</p>
<p>Figure A.15: Noun-verb pairs: 3RDialog response.Figure A.16: Noun-verb pairs: 3RPlan response.</p>
<p>or vases, on the cabinet or shelf.3. Place hygiene products, such as wipes or lotions, on the commode.4. Turn on the lights and adjust their brightness if needed. 5. Arrange diapers in a designated storage area.6. Place a blanket or pillow on the armchair for added comfort.7. Adjust the festoon or other decorative elements if desired.</p>
<p>Table 1 :
1
Datasets statistics.We illustrate key statistics of datasets for 3D VL alignment (LEO-align) and 3D VLA instruction tuning (LEO-instruct).res.(response) denotes tokens to be predicted, while prefix denotes those in the context.
DatasetTask2D input3D assets#data#token (res.)#token (prefix+res.)object captioning✗Objaverse660K 10M27MLEO-alignobject referring✗ScanNet + 3RScan 354K 15M39Mscene captioning✗3RScan20K 3.3M4.4M3D captioning✗ScanNet37K 821K3M3D QA✗ScanNet + 3RScan 83K 177K4MLEO-instruct3D dialogue task planning✗ ✗3RScan 3RScan11K 1.1M 14K 1.9M8.3M 2.7Mnavigation✓MP3D60K 11.4M272Mmanipulation✓CLIPort300K 7.2M734M</p>
<p>Table 2 :
2
Answer accuracy of LLM-generated data on three types of questions.
Counting Existence Non-existence3D-LLM56.596.840.0Ours57.491.327.4+ O-CoT78.093.430.5+ refinement 100.0100.0100.0</p>
<p>Table 3 :
3
(Hong et al., 2023)ned data in Tab. 2. 3D-LLM data(Hong et al., 2023)is much less since we can only access a subset.
Counting Existence Non-existence3D-LLM4349510Ours266667663314</p>
<p>QA(O-CoT):</p>
<p>'m sorry, but the clothes hanging on the wall are messy.Human: Thanks for your help!Thought: no specific object Assistant: You're welcome!If you need any further assistance, feel free to ask.
Dialogue(O-CoT):Dialogue Context: high level task: organize the bedroom. low level task:check some objects.Human: I need help organizing my bedroom. Can you assist me? Thought:no specific objectsAssistant: Of course! What specifically do you need help with?Human: Can you check if there are any clean clothes hanging on the wall?Thought: clothes-13Assistant: IQuestion: Where is the printer located? Thought: printer-8Answer: standing on the deskQuestion: How many blankets are on the bed? Thought:blanket-16, blanket-17 Answer: 2, it offersboth rich object attributes and accurate spatial relation in-formation among objects, allowing LLMs to generate datawith high-quality 3D details (comparisons in Appendix B.8).Next, we manually design some examples as seed tasks (Liuet al., 2023b), including scene and object captioning, QA,dialogue, and planning, and ask LLM to produce more tasksas well as the responses. Details for designing the seed taskscan be found in Appendix B.3.Object-centric CoT. To further combat the hallucinationof LLMs (Bang et al., 2023) in open-ended generation as inour pipeline, we propose the object-centric chain of thought(O-CoT) prompting that requires the LLM to explicitly pro-vide the label and ID of object candidates as thoughts duringtext generation. We also utilize subgraph sampling to fur-ther enhance the diversity of 3D scene graphs (see details in
Question: What is the type of the room?Thought:wardrobe-2, desk-7, chair-11, bed-15 Answer:</p>
<p>bedroom Object Scene Caption:</p>
<p>The showcase is supported by the wall and positioned behind, close to, and to the left of the shelf.The wall is flat, architectural, and dark in color.The shelf is made of wood and has a low size.Clean the floor by sweeping to remove any dirt.2.Make the bed by arranging the blanket and pillows.3.Place any loose items or belongings into the basket.4. Arrange items on the shelves and showcase in a tidy way.
Scene Caption:In this room, there is a wooden floor that isclean and flat. A tall wardrobe stand on theright side of a desk, close to a basket. Thewardrobe is in front of a chair, a bed, andbehind a bedside table. Clothes are hangingon a white wall, to the right of a closedcurtain. The bed is wide, clean, and coveredwith blue blanket. The room has acomfortable and organized setting withfunctional furniture.Planning:High-Level Task: Organize and tidy up the bedroom.Low-Level Actions:1.</p>
<p>Scene-Graph-based Prompting 1. System Message 2. Demonstrations 3.Query(a new scene) Scene Graph Context:
{'sofa-1': {'attributes': {'color': 'red'}, 'relations': ['tothe right of chair-2', 'in front of table-3']}, 'chair-2':{'attributes': {'color': 'brown'}, 'relations': []}}</p>
<p>Human-labeled Responses Scene Graph Context
1) Object scene caption2) Scene caption3) Dialogue(O-CoT)4) QA(O-CoT)5) Planning</p>
<p>Table 5 :
5
Results on robot manipulation.seen indicates in-domain tasks.unseen marks OOD tasks with novel colors or objects.
separating-pilespacking-google -objects-seqput-blocks-in -bowlsseen unseen seen unseen seen unseenCLIP-only90.271.095.857.897.7 44.5CLIPort (single) 98.075.296.271.9100 25.0CLIPort (multi) 89.062.884.470.3100 45.8LEO98.875.276.679.886.2 35.2</p>
<p>Table 6 :
6
Results on object navigation.† indicates zero-shot evaluation.
MP3D-valHM3D-valSuccess(↑) SPL(↑) Success(↑) SPL(↑)Habitat-web (shortest)4.42.2--Habitat-web (demo)35.410.2--ZSON15.3  †4.8  †25.512.6LEO23.115.223.1  †19.1  †</p>
<p>Table 7 :
7
Quantitative results of LEO trained with different data configurations.w/o Align: without alignment stage.ScanNet: tuned on ScanNet scenes only.w/o Act: tuned without embodied acting tasks.We report the exact match metrics for QA tasks and sentence similarity for others.Underlined figures indicate zero-shot results on novel scenes (3RScan).
ScanNet3RScanScan2Cap ScanQASQA3D3RQA3RDialog 3RPlanw/o Align62.822.7 (45.0) 50.9 (53.2) 49.7 (53.7)73.080.3ScanNet64.024.4 (49.2) 46.8 (49.5) 35.8 (50.0)25.523.4w/o Act65.424.3 (48.5) 50.0 (52.5) 51.9 (57.4)73.381.1VLA65.325.0 (48.9) 46.2 (48.3) 51.3 (55.8)72.377.2</p>
<p>Table 8 :
8
TrueSkill scores with human preference.Dialg: dialogue and planning data.
Answerable UnanswerableNLPw/o Dialg 24.4±1.323.1±1.423.4±1.4w/ Dialg25.6±1.326.8±1.426.6±1.4</p>
<p>Table 9 :
9
Answer accuracy (EM) on objectexistence questions.Aug: augmented data.Figure 3: LEO-instruct test loss with the growth of data and model scale, manifesting the scaling law.
Test Loss2.0Aligned OPT-1.3BScratch Vicuna-7B1.6Aligned Vicuna-7B Aligned Vicuna-13B3RScanScanNet (0-shot)1.2Yes No Overall Yes No Overallw/o Aug 1.00 0.01 0.34 0.98 0.16 0.43 w/ Aug 0.72 0.91 0.85 0.88 0.81 0.830.81.53612#Data (10 4 )</p>
<p>Table A .
A
1: The effect of O-CoT on the answer accuracy for Object Counting questions.
SettingsSeed 1 Seed 2 Seed 3 Seed 4 Average Avg. Gainw/o O-CoT 0.5838 0.5349 0.5962 0.58160.5741O-CoT0.7647 0.8117 0.7778 0.76670.78020.2061objects' that require multi-round conversations.</p>
<p>Table A .
A
2:Examples of dialogue refinement.</p>
<p>Table A .
A
3: Examples of QA refinement.
TypesRaw ResponsesRefined ResponsesObject CountingQ: How many chairs are in theQ: How many chairs are in theroom?room?A: 3A: fourObject ExistenceQ: Is there a mirror in the room?Q: Is there a mirror in the room?A: yesA: noObject Non-existence Q: Is there an ironing board in theQ: Is there an ironing board in theroom?room?A: noA: yesNegative ResponseQ: What is the material of the bath-tub? A: unknownQ: Where is the shampoo dispenser?A: unknown</p>
<p>Table A
A
This is a black microwave.It is standing on the table.It is close by the toaster.A black microwave is in front of the this microwave.2 This is a black microwave.It is behind the toaster.It is standing on the table.A black microwave is in front of the this microwave.
.6: Object captions in the 3Rscan scene 8f0f144b-55de-28ce-8053-2828b87a0cc9.object label-id methodresponse id captionmicrowave-8ours1The microwave is black in color and is standing on the table. It is locatedbehind and to the left of the toaster.2The microwave is black in color. It is standing on a table and is positionedbehind and to the left of a toaster.ScanScribe 1</p>
<p>Table A
A
The counter is standing on a wooden floor.It is close to 4 stools and an armchair.The stools are simple and wide in size, while the armchair is narrow in size.The floor is wooden and flat, and it is clean.
.7: Object captions in the 3Rscan scene 0cac753c-8d6f-2d13-8e27-e0664fc33bb9.object label-id methodresponse id captioncounter-4ours1</p>
<p>Table A . 8 :
A8
Examples from our object-level caption instruction set.</p>
<p>Table A .
A
9: Examples from our scene-level caption instruction set.</p>
<p>Table A .
A
10: Examples from our planning instruction pool.</p>
<p>Table A .
A
14: Quantitative comparison between LEO (w/o Act) using ground-truth masks and Mask3D proposals.Metrics follow Tab. 4. Table A.15: Quantitative results of LEO equipped with LLMs at different scales.Metrics follow Tab. 7.
Scan2Cap (val)ScanQA (val)SQA3D (test)CB-4MRSimCB-4MREM@1EM@1w/o Act (Mask3D) 72.4 38.2 27.9 58.1 55.3 101.4 13.2 20.0 49.2 24.5 (47.6)50.0 (52.4)w/o Act (GT)87.4 44.5 30.8 65.7 65.4 103.0 14.6 20.1 49.7 24.3 (48.5)50.0 (52.5)ScanNet3RScanScan2Cap ScanQASQA3D3RQA3RDialog 3RPlanw/o Act (OPT-1.3B)64.620.3 (44.2) 45.5 (47.6) 50.0 (54.5)71.178.3w/o Act (Vicuna-7B)65.424.3 (48.5) 50.0 (52.5) 51.9 (57.4)73.381.1w/o Act (Vicuna-13B)65.223.4 (48.9) 49.7 (52.3) 56.2 (60.4)72.580.5G.2. Model AblationLLM. Following the setting of LEO (w/o Act), we ablate the default LLM (Vicuna-7B) with OPT-1.3B (Zhang et al.,2022) and Vicuna-13B</p>
<p>Table A .
A
22: Additional qualitative results on Scan2Cap validation set.
SceneInstructionResponseGround TruthDevelop a descrip-tion of the objectat the specified posi-tion in the 3D back-drop.</p>
<p>Table A .
A
24: Additional qualitative results on SQA3D test set.
SceneQuestionResponseGround TruthResponse TypeWhat is thebackpackjacketWrongonly orangeobject on myleft side?Is there any-yeslaptopWrong but ac-thing on thecuratedesk?What type ofloft bedloft bedCorrectbed is the oneon the right?What am I sit-deskdeskCorrectting at?HowmanyoneoneCorrectbeds are infront of me?</p>
<p>Table A .
A
25: Examples from our datasets.We provide the tasks, 2D images (optional), 3D point clouds, and instructions as the input of LEO, as well as the expected responses of LEO.
Dataset &amp; Task2D &amp; 3D inputInstructionResponseLEO-align object captioningUSER: plainEx-thea yellow metal cage with a fence around it.object at thepinpointedlocationinthe 3D setting.ASSISTANT:LEO-align object referringUSER: Give me a snapshotfind the office chair that is near the copierof the objectat the chosen3D location.ASSISTANT:LEO-align scene captioningUSER: Relay a brief summaryof this scene'selements. AS-SISTANT:</p>
<p>Table A .
A
26: Examples from our datasets.Continued.
LEO-instruct 3D captioningUSER: What does the object at thethere is a door in the mid-dle of the northern wallspecified position innext to a metal cabinet.the 3D visualizationthe door is shaped like alook like? ASSIS-brown rectangle and as aTANT:dark yellow colored door-knob.LEO-instruct 3D QAUSER: What is in the right corner ofbrown cabinet with tv sit-ting in itroom by curtains?ASSISTANT:LEO-instruct 3D dialogueUSER: I'm redec-orating my livingroom.Can yougive me some sug-gestions on how toarrange the furni-ture?ASSISTANT: Ofcourse!I'd behappy to help. Whatfurniture do youhave in your livingroom?USER: I have a sofa,a table, a chair, anda couch table. AS-SISTANT:</p>
<p>Table A .
A
27: Examples from our datasets.Continued.
LEO-instruct embodied navigationnavigation. Your USER: The task is&lt;31996&gt;goal is to findcounter by mov-ing around in thescene. Past actions:&lt;31999&gt; &lt;31999&gt;&lt;31999&gt; &lt;31999&gt;.ASSISTANT:LEO-instruct robotic manipulationUSER: The task is manipulation. Your&lt;31748&gt; &lt;31644&gt; &lt;31511&gt; &lt;31736&gt; &lt;31595&gt; &lt;31500&gt;goal is to put theblue blocks in agreen bowl. Pastactions: &lt;31991&gt;&lt;31671&gt; &lt;31511&gt;&lt;31991&gt; &lt;31671&gt;&lt;31511&gt; &lt;31991&gt;&lt;31671&gt; &lt;31511&gt;&lt;31991&gt; &lt;31671&gt;&lt;31511&gt; &lt;31991&gt;&lt;31671&gt; &lt;31511&gt;&lt;31991&gt; &lt;31671&gt;&lt;31511&gt; &lt;31991&gt;&lt;31671&gt; &lt;31511&gt;&lt;31991&gt; &lt;31671&gt;&lt;31511&gt;. ASSIS-TANT:
AcknowledgementsThis work is supported in part by the National Science and Technology Major Project (2022ZD0114900).TableA.5: The comparative analysis of full graph prompting and subgraph prompting.Settings Generated Scene CaptionsFull Graph In this scene, there is a colorful and flat white wall.A small TV stand is positioned in front of a low upholstered couch, to the left of a brown lamp, and to the right of a wooden table.The couch is covered with a pink padded square pillow and a patterned blanket.A light hangs on the wall and another light is supported by the table.There is a rectangular glass window attached to the wall, behind which is a yellow curtain.The room has a comfortable and cozy atmosphere, suitable for relaxation and entertainment.Sampling Rate 0.6 In this scene, there is a colorful white wall attached to a brown wooden floor.The couch is upholstered and low in size, located to the right of a small TV stand and a table.A red blanket covers the couch, and there are patterned, padded square pillows on the couch.A black glass window is attached to the wall, and there are artificial lamps standing on the floor and close to the couch.The lamps are also near a table and to the right of the TV stand.The scene suggests a comfortable living room with a cozy couch, pillows, and a blanket, suitable for relaxation and entertainment.GPT-4 Evalutaion Similarities: 1.Both scenes describe a living room setting, emphasizing comfort and coziness suitable for relaxation and entertainment.2. Both rooms have a white wall.3.Each scene features a couch and a TV stand.4. Both scenes describe a blanket and a padded square pillow on the couch.Differences: 1.The first scene has a brown wooden floor, while the floor is not mentioned in the second scene.2. The first scene has a red blanket on the couch; the second has a patterned blanket but doesn't specify the color.3. The first scene describes the lamps as "standing on the floor", while the second mentions one light hanging on the wall and another supported by the table.4. The second scene includes a yellow curtain behind the window, which the first scene does not mention.Summary: Overall, both summaries provide a similar thematic view of a comfortable living room but differ in the specific arrangement and color details of the items within the room.Box-based PromptingScene-Graph-based PromptingResponse:Dialogue Context: high level task: describe the objects in the bathroom.Human: Can you describe the objects in the bathroom?Thought: bathtub-8, sink-12, toilet-15, mirror-4 Robot: Sure!In the bathroom, there is a bathtub made of ceramic, with a shiny texture and a white color.There is also a sink with an oval shape and a shiny texture.The toilet is white and seat down.There is a mirror attached to the wall, which is polished.Is there anything specific you want to know about these objects?Human: Is there a towel in the bathroom?Thought: towel-10 Robot: Yes, there is a folded together rectangular towel in the bathroom.It is located to the right of the door.G. Ablation DetailsG.1.Object-centric Mask Ground truth vs. object proposals.As we adopt an object-centric 3D representation, the object-centric masks are necessary to segment the scene point cloud.For scenes that lack annotations of object-centric masks, we can utilize off-the-shelf detection or segmentation models to generate object proposals and thus obtain the masks.We compare the performances of LEO (w/o Act) between using ground-truth masks and Mask3D(Schult et al., 2022)proposals.The results in Tab.A.14 indicate that using Mask3D proposals leads to a moderate performance drop on Scan2Cap (mainly due to the IoU@0.5 metrics) and comparable performances on QA tasks.In a nutshell, we squeeze the pred and gt, and then check whether one is a subset of the other.To justify our refined exact match protocol, in Tab.A.16 we provide some representative examples in the ScanQA validation set.Despite the improvements, we speculate such a simple refinement is still insufficient for a sound evaluation metric considering the flexibility of human language.H.2. Embodied NavigationTo construct our training set, we adopt all 57 scenes in the MP3D ObjNav training split(Savva et al., 2019;Ramrakhya et al., 2022)and generate ~60K shortest-path navigation episodes.The evaluation is conducted on the original validation split of the MP3D ObjNav task and the newly introduced HM3D ObjNav task(Ramakrishnan et al., 2021).In contrast to most ObjNav agents that utilize recurrence through either RNN(Ramrakhya et al., 2022)or DT-style Transformer(Suglia et al., 2021), LEO only employs a simplistic feed-forward policy, i.e., the Transformer in LEO only takes in the instruction, current state (2D and 3D observation), and past 4 actions, and predicts the next action, similar to RT-2(Brohan et al., 2023).Therefore, the only information relayed from the past is past actions.The absence of recurrence in LEO's acting policy is indeed the result of a trade-off between better performances and training efficiency.We will commit to exploring the possibility of looping in more sophisticated policy architectures (e.g., recurrence) in future work.I. Additional ResultsI.1. Impact of Data RefinementSettings.We investigate the impact of data refinement by comparing the downstream performances between pretraining on the generated data before/after refinement.Specifically, since our generated data (where the refinement occurs) pertains to 3RScan scenes, we first pretrain the LEO after the alignment stage on a mix of 3RScan datasets, and then train on a mix of ScanNet datasets (Scan2Cap, ScanQA, and SQA), where we report the quantitative results as downstream performances.The results in Tab.A.17 demonstrate that data refinement elicits consistent improvements.In particular, data refinement primarily benefits reasoning (QA) tasks, probably because the refinement operation mainly concerns QA and dialogue data.I.2. Data ComparisonSettings.We collect the training data of LL3DA(Chen et al., 2024)to train LEO and compare the quantitative results with LEO trained with our original data to showcase the impact of training data.We report the performances on Scan2Cap and ScanQA, where their data overlaps ours.The results in Tab.A.18 exhibit a consistent performance gap between training on LL3DA data and our original data, underscoring the advantage of our collected training data.I.3. Model ComparisonSettings.LEO adopts an object-centric 3D representation to encode 3D scenes, which is a novel approach compared with recent works.For example, 3D-LLM(Hong et al., 2023)leverages 2D foundation models to obtain dense semantic features and lift them to 3D space, and LL3DA(Chen et al., 2024) adopts scene-level encoding.They both use learnable queries to extract 3D features.Here we investigate the influence of model design with the same training data.For a fair comparison, we use Mask3D(Schult et al., 2022)object proposals instead of ground-truth masks for the evaluation results of LEO.LL3DA vs. LEO.We train LEO on the LL3DA training data and compare the performances with LL3DA generalist results (without task-specific fine-tuning).From the results in Tab.A.19, we highlight two takeaways: 1) with the same training data, LEO outperforms LL3DA on most metrics; 2) the gap between LL3DA and LEO is significant on ScanQA, which indicates a major advantage of object-centric 3D representation lies in handling the reasoning task.3D-LLM vs. LEO.As LL3DA collects a subset (ScanNet part) of 3D-LLM training data, we leverage this subset to pretrain LEO and compare the downstream performances with 3D-LLM.In contrast to the task-specific fine-tuning results of 3D-LLM, we report LEO's evaluation results after instruction tuning without task-specific fine-tuning.The results in Tab.A.20 show that LEO consistently outperforms 3D-LLM when adopting the same training data.Notably, the magnitude of this subset is much smaller than their original training data, which further underscores the efficiency of our model.I.4. Embodied ActingQuantitative results of ObjNav.We provide additional results of LEO 1) generalizing to unseen objects on MP3D (below is a list of the objects used during training (seen) and for OOD evaluation (unseen)), 2) learning with 70K human demonstrations provided by Habitat-web(Ramrakhya et al., 2022)instead of shortest path, and 3) learning without one modality (full vs. w/o 3D vs. w/o 2D).Evaluation results are shown in Tab.A.21.Note that the baseline Habitat-web is unable to generalize to novel objects as it uses categorical embedding rather than natural language to represent object goals.# Objects (seen) "gym_equipment", "tv_monitor", "picture", "counter", "chair", "cabinet", "table", "stool", "plant", "towel", "sofa", "cushion", "sink", "fireplace", "toilet", "seating", "chest_of_drawers", "bed", "shower", "bathtub", "clothes" # Objects (unseen) "shelf", "pillow", "lamp", "box", "desk", "refrigerator", "vase", "armchair"The results show that LEO can generalize to novel objects.On the other hand, human demonstrations include more explorations, compared with shortest-path data.Therefore, it will be much harder for agents without a recurrent module (e.g., LEO) to learn from human demonstrations (see Appendix H.2), leading to significantly weaker performances.Finally, more modalities (2D + 3D) are indeed crucial to the success of mastering object navigation skills.Qualitative results.We provide more qualitative results of robotic manipulation and embodied navigation in the supplementary video.I.5. Scan2CapWe provide additional qualitative results on Scan2Cap validation set in Tab.A.22.The results show that LEO can correctly refer to the queried object and provide accurate descriptions, including spatial relationships with other objects.However, LEO's responses are confined to simple formats that lack diversity.How to unlock more flexible responses while maintaining accuracy can be a direction for future research.An Embodied Generalist Agent in 3D World
Neural listeners for fine-grained 3d object identification in real-world scenes. P Achlioptas, A Abdelreheem, F Xia, M Elhoseiny, L Guibas, Referit3d, European Conference on Computer Vision (ECCV). 2020714</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, Advances in Neural Information Processing Systems (NeurIPS). 20223</p>
<p>Vqa: Visual question answering. S Antol, A Agrawal, J Lu, M Mitchell, D Batra, C L Zitnick, D Parikh, International Conference on Computer Vision (ICCV). 201515</p>
<p>Scanqa: 3d question answering for spatial scene understanding. D Azuma, T Miyanishi, S Kurita, M Kawanabe, Conference on Computer Vision and Pattern Recognition (CVPR). 2022615</p>
<p>A multitask. Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, arXiv:2302.04023multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 2023arXiv preprint</p>
<p>On the opportunities and risks of foundation models. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.0681720221arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023. 1, 3, 7, 8, 30arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in Neural Information Processing Systems (NeurIPS). 202013</p>
<p>3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds. D Cai, L Zhao, J Zhang, L Sheng, D Xu, Conference on Computer Vision and Pattern Recognition (CVPR). 2022</p>
<p>Openworld multi-task control through goal-aware representation learning and adaptive horizon prediction. S Cai, Z Wang, X Ma, A Liu, Y Liang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023a</p>
<p>S Cai, B Zhang, Z Wang, X Ma, A Liu, Y Liang, Groot, arXiv:2310.08235Learning to follow instructions by watching gameplay videos. 2023barXiv preprint</p>
<p>Scanrefer: 3d object localization in rgb-d scans using natural language. D Z Chen, A X Chang, M Nießner, European Conference on Computer Vision (ECCV). 2020</p>
<p>Language conditioned spatial relation reasoning for 3d object grounding. S Chen, P.-L Guhur, M Tapaswi, C Schmid, I Laptev, Advances in Neural Information Processing Systems (NeurIPS). 2022. 1, 3, 8, 2526</p>
<p>End-to-end 3d dense captioning with vote2cap-detr. S Chen, H Zhu, X Chen, Y Lei, G Yu, T Chen, Conference on Computer Vision and Pattern Recognition (CVPR). 2023</p>
<p>Ll3da: Visual interactive instruction tuning for omni-3d understanding, reasoning, and planning. S Chen, X Chen, C Zhang, M Li, G Yu, H Fei, H Zhu, J Fan, T Chen, Conference on Computer Vision and Pattern Recognition (CVPR). 202431</p>
<p>Scan2cap: Context-aware dense captioning in rgb-d scans. Z Chen, A Gholami, M Nießner, A X Chang, Conference on Computer Vision and Pattern Recognition (CVPR). 2021. 2, 4615</p>
<p>W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, Vicuna, An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. 202329</p>
<p>Scaling instruction-finetuned language models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, E Li, X Wang, M Dehghani, S Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Nießner, Conference on Computer Vision and Pattern Recognition. 20171525</p>
<p>Towards general-purpose vision-language models with instruction tuning. W Dai, J Li, D Li, A M H Tiong, J Zhao, W Wang, B Li, P Fung, S Hoi, Instructblip, arXiv:2305.065002023arXiv preprint</p>
<p>Objaverse: A universe of annotated 3d objects. M Deitke, D Schwenk, J Salvador, L Weihs, O Michel, E Vanderbilt, L Schmidt, K Ehsani, A Kembhavi, A Farhadi, Conference on Computer Vision and Pattern Recognition (CVPR). </p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, International Conference on Machine Learning (ICML). </p>
<p>Building open-ended embodied agents with internet-scale knowledge. L Fan, G Wang, Y Jiang, A Mandlekar, Y Yang, H Zhu, A Tang, D.-A Huang, Y Zhu, A Anandkumar, Minedojo, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>P Gao, J Han, R Zhang, Z Lin, S Geng, A Zhou, W Zhang, P Lu, C He, X Yue, arXiv:2304.15010Parameter-efficient visual instruction model. 20232arXiv preprint</p>
<p>A benchmark for language-grounded task learning with continuous states in realistic 3d scenes. R Gong, J Huang, Y Zhao, H Geng, X Gao, Q Wu, W Ai, Z Zhou, D Terzopoulos, S.-C Zhu, International Conference on Computer Vision (ICCV). 2023a</p>
<p>R Gong, Q Huang, X Ma, H Vo, Z Durante, Y Noda, Z Zheng, S.-C Zhu, D Terzopoulos, L Fei-Fei, arXiv:2309.09971Emergent gaming interaction. 2023barXiv preprint</p>
<p>T Gong, C Lyu, S Zhang, Y Wang, M Zheng, Q Zhao, K Liu, W Zhang, P Luo, K Chen, arXiv:2305.04790Multimodal-gpt: A vision and language model for dialogue with humans. 2023carXiv preprint</p>
<p>A bayesian skill rating system. T Graepel, T Minka, R T Herbrich, Advances in Neural Information Processing Systems. 20071929</p>
<p>From images to textual prompts: Zero-shot vqa with frozen large language models. J Guo, J Li, D Li, A M H Tiong, B Li, D Tao, S C Hoi, Conference on Computer Vision and Pattern Recognition (CVPR). 2023</p>
<p>Y Hong, H Zhen, P Chen, S Zheng, Y Du, Z Chen, C Gan, arXiv:2307.129813d-llm: Injecting the 3d world into large language models. 2023. 1, 4, 6, 8, 2031arXiv preprint</p>
<p>Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, Lora, International Conference on Learning Representations (ICLR). 2022827</p>
<p>Perceive, ground, reason, and act: A benchmark for general-purpose visual representation. J Huang, W Y Zhu, B Jia, Z Wang, X Ma, Q Li, S Huang, arXiv:2211.154022022aarXiv preprint</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, Conference on Robot Learning (CoRL). 2022b</p>
<p>Vima: General robot manipulation with multimodal prompts. Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, Fan , L , International Conference on Machine Learning (ICML). 2023</p>
<p>J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.08361Scaling laws for neural language models. 20202arXiv preprint</p>
<p>Language embedded radiance fields. J Kerr, C M Kim, K Goldberg, A Kanazawa, M Tancik, Lerf, International Conference on Computer Vision (ICCV). 2023</p>
<p>. A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, arXiv:2304.0264320231Segment anything. arXiv preprint</p>
<p>Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. T Kudo, J Richardson, arXiv:1808.062262018arXiv preprint</p>
<p>Human-level concept learning through probabilistic program induction. B M Lake, R Salakhutdinov, J B Tenenbaum, Science. 12015</p>
<p>Building machines that learn and think like people. B M Lake, T D Ullman, J B Tenenbaum, S J Gershman, Behavioral and Brain Sciences. 12017</p>
<p>B Li, Y Zhang, L Chen, J Wang, F Pu, J Yang, C Li, Z Liu, arXiv:2306.05425Mimic-it: Multi-modal in-context instruction tuning. 2023aarXiv preprint</p>
<p>B Li, Y Zhang, L Chen, J Wang, J Yang, Z Liu, Otter, arXiv:2305.03726A multi-modal model with in-context instruction tuning. 2023barXiv preprint</p>
<p>Multimodal foundation models: From specialists to general-purpose assistants. C Li, Z Gan, Z Yang, J Yang, L Li, L Wang, J Gao, 2023c</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, arXiv:2301.125972023d4arXiv preprint</p>
<p>Aligning large multi-modal model with robust instruction tuning. F Liu, K Lin, L Li, J Wang, Y Yacoob, L Wang, arXiv:2306.145652023aarXiv preprint</p>
<p>H Liu, C Li, Q Wu, Y J Lee, arXiv:2304.08485Visual instruction tuning. 2023b. 2, 3, 4, 8arXiv preprint</p>
<p>A convnet for the 2020s. Z Liu, H Mao, C.-Y Wu, C Feichtenhofer, T Darrell, S Xie, Conference on Computer Vision and Pattern Recognition (CVPR). </p>
<p>Unified-io: A unified model for vision, language, and multi-modal tasks. J Lu, C Clark, R Zellers, R Mottaghi, A Kembhavi, International Conference on Learning Representations (ICLR). 2023</p>
<p>T Luo, C Rockwell, H Lee, J Johnson, arXiv:2306.07279Scalable 3d captioning with pretrained models. 2023414arXiv preprint</p>
<p>Situated question answering in 3d scenes. X Ma, S Yong, Z Zheng, Q Li, Y Liang, S.-C Zhu, S Huang, Sqa3d, International Conference on Learning Representations (ICLR). 2023. 2, 4, 61523</p>
<p>Zero-shot object-goal navigation using multimodal goal embeddings. A Majumdar, G Aggarwal, B Devnani, J Hoffman, D Batra, Zson, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>An organizing principle for cerebral function: the unit module and the distributed system. The neurosciences. Fourth study program. V B Mountcastle, 1979</p>
<p>Embodiedgpt: Vision-language pre-training via embodied chain of thought. Y Mu, Q Zhang, M Hu, W Wang, M Ding, J Jin, B Wang, J Dai, Y Qiao, P Luo, arXiv:2305.150212023arXiv preprint</p>
<p>. Openai, Chatgpt, 816</p>
<p>arXiv:2303.08774OpenAI. Gpt-4 technical report. 2023. 1, 2, 8, 20arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>B Peng, C Li, P He, M Galley, J Gao, arXiv:2304.03277Instruction tuning with gpt-4. 2023aarXiv preprint</p>
<p>Openscene: 3d scene understanding with open vocabularies. S Peng, K Genova, C Jiang, A Tagliasacchi, M Pollefeys, T Funkhouser, Conference on Computer Vision and Pattern Recognition (CVPR). 2023b</p>
<p>Deep hierarchical feature learning on point sets in a metric space. C R Qi, L Yi, H Su, L J Guibas, Pointnet++, Advances in Neural Information Processing Systems. 253292017</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International Conference on Machine Learning (ICML). 202127</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, Journal of Machine Learning Research. 32020</p>
<p>S K Ramakrishnan, A Gokaslan, E Wijmans, O Maksymets, A Clegg, J Turner, E Undersander, W Galuba, A Westbury, A X Chang, arXiv:2109.08238Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. 2021730arXiv preprint</p>
<p>Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. R Ramrakhya, E Undersander, D Batra, A Das, Conference on Computer Vision and Pattern Recognition (CVPR). 2022. 2, 6, 7, 15, 273032</p>
<p>A generalist agent. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, Transactions on Machine Learning Research. </p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. N Reimers, I Gurevych, Annual Conference on Empirical Methods in Natural Language Processing (EMNLP). 2019</p>
<p>Multitask prompted training enables zero-shot task generalization. V Sanh, A Webson, C Raffel, S Bach, L Sutawika, Z Alyafeai, A Chaffin, A Stiegler, A Raja, M Dey, International Conference on Learning Representations (ICLR). 2022</p>
<p>Habitat: A platform for embodied ai research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, International Conference on Computer Vision (ICCV). 20191530</p>
<p>J Schmidhuber, arXiv:1802.08864One big net for everything. 2018arXiv preprint</p>
<p>Laion-5b: An open largescale dataset for training next generation image-text models. C Schuhmann, R Beaumont, R Vencu, C Gordon, R Wightman, M Cherti, T Coombes, A Katta, C Mullis, M Wortsman, Advances in Neural Information Processing Systems. </p>
<p>Mask3d for 3d semantic instance segmentation. J Schult, F Engelmann, A Hermans, O Litany, S Tang, B Leibe, arXiv:2210.031052022. 3, 62831arXiv preprint</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Conference on Robot Learning (CoRL). 2021. 2, 4, 6, 1527</p>
<p>Embodied bert: A transformer model for embodied, language-guided visual task completion. A Suglia, Q Gao, J Thomason, G Thattai, G Sukhatme, arXiv:2108.04927202130arXiv preprint</p>
<p>R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, Stanford Alpaca, An instruction-following llama model. </p>
<p>Multimodal few-shot learning with frozen language models. M Tsimpoukelli, J L Menick, S Cabi, S Eslami, O Vinyals, F Hill, Advances in Neural Information Processing Systems (NeurIPS). </p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. 201725</p>
<p>Rio: 3d object instance re-localization in changing indoor environments. J Wald, A Avetisyan, N Navab, F Tombari, M Nießner, International Conference on Computer Vision (ICCV). 201915</p>
<p>Voyager: An openended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, arXiv:2305.162912023aarXiv preprint</p>
<p>Images speak in images: A generalist painter for in-context visual learning. X Wang, W Wang, Y Cao, C Shen, T Huang, Conference on Computer Vision and Pattern Recognition (CVPR). 2023b</p>
<p>Self-instruct: Aligning language model with self generated instructions. Y Wang, Y Kordi, S Mishra, A Liu, N A Smith, D Khashabi, H Hajishirzi, Annual Meeting of the Association for Computational Linguistics (ACL). 2023c</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Z Wang, S Cai, A Liu, X Ma, Y Liang, arXiv:2302.015602023darXiv preprint</p>
<p>Z Wang, H Huang, Y Zhao, Z Zhang, Z Zhao, arXiv:2308.08769Chat-3d: Data-efficiently tuning large language model for universal dialogue of 3d scenes. 2023e4arXiv preprint</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, International Conference on Learning Representations (ICLR). 2022</p>
<p>Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences. S.-C Wu, J Wald, K Tateno, N Navab, F Tombari, Conference on Computer Vision and Pattern Recognition (CVPR). 2021417</p>
<p>R Xu, X Wang, T Wang, Y Chen, J Pang, D Lin, Pointllm, arXiv:2308.16911Empowering large language models to understand point clouds. 2023829arXiv preprint</p>
<p>Q Ye, H Xu, G Xu, J Ye, M Yan, Y Zhou, J Wang, A Hu, P Shi, Y Shi, arXiv:2304.14178mplug-owl: Modularization empowers large language models with multimodality. 2023arXiv preprint</p>
<p>Z Yin, J Wang, J Cao, Z Shi, D Liu, M Li, L Sheng, L Bai, X Huang, Z Wang, arXiv:2306.06687Languageassisted multi-modal instruction-tuning dataset, framework, and benchmark. 20234arXiv preprint</p>
<p>Point-bert: Pre-training 3d point cloud transformers with masked point modeling. X Yu, L Tang, Y Rao, T Huang, J Zhou, J Lu, Conference on Computer Vision and Pattern Recognition (CVPR). 202229</p>
<p>Opt: Open pre-trained transformer language models. S Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, arXiv:2205.010682022829arXiv preprint</p>
<p>H Zhao, Z Cai, S Si, X Ma, K An, L Chen, Z Liu, S Wang, W Han, B Chang, Mmicl, arXiv:2309.07915Empowering vision-language model with multi-modal in-context learning. 2023arXiv preprint</p>
<p>3dvg-transformer: Relation modeling for visual grounding on point clouds. L Zhao, D Cai, L Sheng, D Xu, International Conference on Computer Vision (ICCV). </p>
<p>D Zhu, J Chen, K Haydarov, X Shen, W Zhang, M Elhoseiny, arXiv:2303.06594Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions. 2023aarXiv preprint</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. D Zhu, J Chen, X Shen, X Li, M Elhoseiny, arXiv:2304.105922023barXiv preprint</p>
<p>Dark, beyond deep: A paradigm shift to cognitive ai with humanlike common sense. Engineering. Y Zhu, T Gao, L Fan, S Huang, M Edmonds, H Liu, F Gao, C Zhang, S Qi, Y N Wu, 2020</p>
<p>3d-vista: Pre-trained transformer for 3d vision and text alignment. Z Zhu, X Ma, Y Chen, Z Deng, S Huang, Q Li, International Conference on Computer Vision (ICCV). 2023c. 1, 3, 4, 6, 8, 1422</p>            </div>
        </div>

    </div>
</body>
</html>
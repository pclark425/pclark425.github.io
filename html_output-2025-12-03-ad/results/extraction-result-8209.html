<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8209 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8209</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8209</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-271097479</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.08195v1.pdf" target="_blank">A Text-to-Game Engine for UGC-Based Role-Playing Games</a></p>
                <p><strong>Paper Abstract:</strong> The shift from professionally generated content (PGC) to user-generated content (UGC) has revolutionized various media formats, from text to video. With the rapid advancements in generative AI, a similar shift is set to transform the game industry, particularly in the realm of role-playing games (RPGs). This paper introduces a new framework for a text-to-game engine that utilizes foundation models to convert simple textual inputs into complex, interactive RPG experiences. The engine dynamically renders the game story in a multi-modal format and adjusts the game character, environment, and mechanics in real-time in response to player actions. Using this framework, we developed the"Zagii"game engine, which has successfully supported hundreds of RPG games across a diverse range of genres and facilitated tens of thousands of online user gameplay instances. This validates the effectiveness of our frame-work. Our work showcases the potential for a more open and democratized gaming paradigm, highlighting the transformative impact of generative AI on the game life cycle.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8209.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8209.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zagii Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zagii Role-playing Agent (Autonomous Player)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The engine's LLM-driven agent implementation for NPCs and autonomous players, built on a PMTA (Perception, Memory, Thinking, Action) architecture; it uses retrieval-augmented memory and dynamic prompt generation to maintain role consistency and drive actions in UGC text/RPG sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Zagii Role-playing Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-based agent within the Zagii engine that produces autonomous NPC behavior and can act as an Autonomous Player; it perceives game events, retrieves relevant memory fragments, reasons via LLM(s) (Thinking), issues action plans, and executes them through the Action module.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Not specified in the paper. Authors state they use a mix of SOTA models for offline/cold-start processing and lighter-weight LLMs (fine-tuned in a two-layer scheme) for real-time inference, but do not name specific model families.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>UGC-based RPG sessions on Zagii (internal deployment)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Real user-created role-playing games deployed on the Zagii platform across multiple genres; evaluation is via deployed gameplay sessions and User–NPC interaction rounds rather than a standard academic text-game benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented / session memory (natural-language memory fragments and multimodal entity assets)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory stores character role-setting, goals, self-awareness notes and memory fragments as natural-language text; a session memory system also stores entity IDs and multimodal assets (images/sounds) tied to entities. Perception produces concise summaries and entity IDs which drive retrieval of relevant memory fragments. Retrieval-augmented Generation (RAG)-style retrieval supplies context to the Thinking module; important results of Thinking/actions are post-processed and appended back into memory in real time.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Retrieved memory fragments and session-entity assets are injected into dynamically generated prompts for the Thinking LLM (RAG style). A two-tier model strategy is described: an expensive SOTA model analyzes the whole game pre-play (cold start) to produce guidance/templates, while a lightweight LLM performs real-time assessments with periodic comparisons to SOTA outputs. Dynamic prompt generation personalizes role prompts using retrieved memory.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No quantitative ablation or direct experimental comparisons between memory vs no-memory or between different memory architectures are reported for the Zagii agent; the user-deployment experiments report gameplay sessions and interaction-round distributions but do not isolate memory effects.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Authors recommend retrieval-augmented generation (RAG) over stored natural-language memory fragments and multimodal session assets, combined with dynamic prompt generation per character; also suggest using SOTA models for an offline 'cold start' memory/template construction and lighter models for real-time inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No memory-specific quantitative failures are given; paper notes general limitations that affect memory utility: cold-start issues, limited reasoning ability of lightweight real-time LLMs (necessitating periodic SOTA checks), early player exits (which can reduce memory buildup/usefulness), and inconsistencies in multimodal asset generation that complicate maintaining consistent entity representations.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Use RAG-style external retrieval of natural-language memory fragments and session multimodal assets; generate dynamic, per-character prompts incorporating retrieved memory; employ SOTA models offline for comprehensive understanding/cold-start and lightweight models for real-time with periodic SOTA validation; design memory as natural-language fragments indexed by entities and retrieved by Perception outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Text-to-Game Engine for UGC-Based Role-Playing Games', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8209.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8209.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented technique that supplies external retrieved documents/memory fragments as context to an LLM during generation; adopted in this work as the main mechanism to provide contextual memory to agents' Thinking module.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG (memory mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Used as the memory/retrieval component: an external memory of textual fragments and game/world knowledge is queried to provide context to the LLM that performs reasoning and action planning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Not specified; paper cites RAG literature and states RAG is used to provide contextual information to the Thinking module.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external memory (document store of fragments and assets)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>An external store of natural-language memory fragments and multi-modal entity assets indexed by entity IDs and other metadata; Perception module forms retrieval queries (from concise plot summaries and interactions) to fetch the most relevant fragments for prompt augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Retrieved fragments are concatenated or incorporated into dynamically generated prompts supplied to the LLM (Thinking module). The Game Status Manager and dynamic prompt generation templates control how retrieved material is formatted and prioritized; SOTA offline processing supplements template generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No ablation experiments comparing RAG vs. non-RAG memory strategies are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Paper advocates RAG-style retrieval combined with dynamic prompt generation and session multimodal assets as the practical approach for role consistency and long-term coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes practical issues: prompt-length constraints for retrieved context, the need to manage cold starts, and limited real-time reasoning capacity of lightweight LLMs which can reduce the effective use of retrieved context without periodic SOTA checks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Adopt retrieval-augmented generation for supplying long-term and game-specific context; precompute/guide prompt templates with SOTA cold-start processing and use lightweight models for real-time inference with periodic validation against stronger models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Text-to-Game Engine for UGC-Based Role-Playing Games', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8209.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8209.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (Park et al. 2023) introducing computational agents that use memories and reflections to plan dynamic agent behavior; cited as related work demonstrating memory+reflection architectures for agent planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agent (Park et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A referenced architecture that demonstrates storing memories and performing reflections/summaries to enable dynamic planning and human-like behavior in interactive simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Not specified in this paper (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic memories + reflection/summarization (as described in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>As described in the citation: agents store event memories and produce higher-level reflections/summaries to guide future planning and behavior (the paper cites this architecture as inspiration but does not reimplement it identically).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Memories and reflections are used to dynamically plan agent behaviors in the cited work; the current paper references this pattern as inspirational for PMTA and RAG-based memory usage.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No experimental comparison to Generative Agent is reported in this paper (it is cited as related art).</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in detail in this paper beyond acknowledging prior art; the authors take inspiration from memory+reflection ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>The paper cites Generative Agent to motivate using memory and reflection-like mechanisms; it recommends retrieval-based memory storage and dynamic prompts in its own design rather than reproducing an identical reflection pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Text-to-Game Engine for UGC-Based Role-Playing Games', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Keep calm and explore: Language models for action generation in text-based games <em>(Rating: 2)</em></li>
                <li>Can large language models play text games well? current state-of-the-art and open questions <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 2)</em></li>
                <li>LLM Powered Autonomous Agents <em>(Rating: 2)</em></li>
                <li>Role LLM: Benchmarking, eliciting, and enhancing role-playing abilities of large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8209",
    "paper_id": "paper-271097479",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "Zagii Agent",
            "name_full": "Zagii Role-playing Agent (Autonomous Player)",
            "brief_description": "The engine's LLM-driven agent implementation for NPCs and autonomous players, built on a PMTA (Perception, Memory, Thinking, Action) architecture; it uses retrieval-augmented memory and dynamic prompt generation to maintain role consistency and drive actions in UGC text/RPG sessions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Zagii Role-playing Agent",
            "agent_description": "An LLM-based agent within the Zagii engine that produces autonomous NPC behavior and can act as an Autonomous Player; it perceives game events, retrieves relevant memory fragments, reasons via LLM(s) (Thinking), issues action plans, and executes them through the Action module.",
            "llm_model_name": "",
            "llm_model_description": "Not specified in the paper. Authors state they use a mix of SOTA models for offline/cold-start processing and lighter-weight LLMs (fine-tuned in a two-layer scheme) for real-time inference, but do not name specific model families.",
            "benchmark_name": "UGC-based RPG sessions on Zagii (internal deployment)",
            "benchmark_description": "Real user-created role-playing games deployed on the Zagii platform across multiple genres; evaluation is via deployed gameplay sessions and User–NPC interaction rounds rather than a standard academic text-game benchmark.",
            "memory_used": true,
            "memory_type": "retrieval-augmented / session memory (natural-language memory fragments and multimodal entity assets)",
            "memory_architecture": "Memory stores character role-setting, goals, self-awareness notes and memory fragments as natural-language text; a session memory system also stores entity IDs and multimodal assets (images/sounds) tied to entities. Perception produces concise summaries and entity IDs which drive retrieval of relevant memory fragments. Retrieval-augmented Generation (RAG)-style retrieval supplies context to the Thinking module; important results of Thinking/actions are post-processed and appended back into memory in real time.",
            "memory_integration_strategy": "Retrieved memory fragments and session-entity assets are injected into dynamically generated prompts for the Thinking LLM (RAG style). A two-tier model strategy is described: an expensive SOTA model analyzes the whole game pre-play (cold start) to produce guidance/templates, while a lightweight LLM performs real-time assessments with periodic comparisons to SOTA outputs. Dynamic prompt generation personalizes role prompts using retrieved memory.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No quantitative ablation or direct experimental comparisons between memory vs no-memory or between different memory architectures are reported for the Zagii agent; the user-deployment experiments report gameplay sessions and interaction-round distributions but do not isolate memory effects.",
            "best_memory_strategy": "Authors recommend retrieval-augmented generation (RAG) over stored natural-language memory fragments and multimodal session assets, combined with dynamic prompt generation per character; also suggest using SOTA models for an offline 'cold start' memory/template construction and lighter models for real-time inference.",
            "limitations_or_failure_cases": "No memory-specific quantitative failures are given; paper notes general limitations that affect memory utility: cold-start issues, limited reasoning ability of lightweight real-time LLMs (necessitating periodic SOTA checks), early player exits (which can reduce memory buildup/usefulness), and inconsistencies in multimodal asset generation that complicate maintaining consistent entity representations.",
            "recommendations_or_conclusions": "Use RAG-style external retrieval of natural-language memory fragments and session multimodal assets; generate dynamic, per-character prompts incorporating retrieved memory; employ SOTA models offline for comprehensive understanding/cold-start and lightweight models for real-time with periodic SOTA validation; design memory as natural-language fragments indexed by entities and retrieved by Perception outputs.",
            "uuid": "e8209.0",
            "source_info": {
                "paper_title": "A Text-to-Game Engine for UGC-Based Role-Playing Games",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A retrieval-augmented technique that supplies external retrieved documents/memory fragments as context to an LLM during generation; adopted in this work as the main mechanism to provide contextual memory to agents' Thinking module.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "mention_or_use": "use",
            "agent_name": "RAG (memory mechanism)",
            "agent_description": "Used as the memory/retrieval component: an external memory of textual fragments and game/world knowledge is queried to provide context to the LLM that performs reasoning and action planning.",
            "llm_model_name": "",
            "llm_model_description": "Not specified; paper cites RAG literature and states RAG is used to provide contextual information to the Thinking module.",
            "benchmark_name": "",
            "benchmark_description": "",
            "memory_used": true,
            "memory_type": "retrieval-augmented external memory (document store of fragments and assets)",
            "memory_architecture": "An external store of natural-language memory fragments and multi-modal entity assets indexed by entity IDs and other metadata; Perception module forms retrieval queries (from concise plot summaries and interactions) to fetch the most relevant fragments for prompt augmentation.",
            "memory_integration_strategy": "Retrieved fragments are concatenated or incorporated into dynamically generated prompts supplied to the LLM (Thinking module). The Game Status Manager and dynamic prompt generation templates control how retrieved material is formatted and prioritized; SOTA offline processing supplements template generation.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No ablation experiments comparing RAG vs. non-RAG memory strategies are reported in this paper.",
            "best_memory_strategy": "Paper advocates RAG-style retrieval combined with dynamic prompt generation and session multimodal assets as the practical approach for role consistency and long-term coherence.",
            "limitations_or_failure_cases": "Paper notes practical issues: prompt-length constraints for retrieved context, the need to manage cold starts, and limited real-time reasoning capacity of lightweight LLMs which can reduce the effective use of retrieved context without periodic SOTA checks.",
            "recommendations_or_conclusions": "Adopt retrieval-augmented generation for supplying long-term and game-specific context; precompute/guide prompt templates with SOTA cold-start processing and use lightweight models for real-time inference with periodic validation against stronger models.",
            "uuid": "e8209.1",
            "source_info": {
                "paper_title": "A Text-to-Game Engine for UGC-Based Role-Playing Games",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Generative Agent",
            "name_full": "Generative agents: Interactive simulacra of human behavior",
            "brief_description": "Prior work (Park et al. 2023) introducing computational agents that use memories and reflections to plan dynamic agent behavior; cited as related work demonstrating memory+reflection architectures for agent planning.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agent (Park et al. 2023)",
            "agent_description": "A referenced architecture that demonstrates storing memories and performing reflections/summaries to enable dynamic planning and human-like behavior in interactive simulations.",
            "llm_model_name": "",
            "llm_model_description": "Not specified in this paper (cited as related work).",
            "benchmark_name": "",
            "benchmark_description": "",
            "memory_used": true,
            "memory_type": "episodic memories + reflection/summarization (as described in the cited work)",
            "memory_architecture": "As described in the citation: agents store event memories and produce higher-level reflections/summaries to guide future planning and behavior (the paper cites this architecture as inspiration but does not reimplement it identically).",
            "memory_integration_strategy": "Memories and reflections are used to dynamically plan agent behaviors in the cited work; the current paper references this pattern as inspirational for PMTA and RAG-based memory usage.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "No experimental comparison to Generative Agent is reported in this paper (it is cited as related art).",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "Not discussed in detail in this paper beyond acknowledging prior art; the authors take inspiration from memory+reflection ideas.",
            "recommendations_or_conclusions": "The paper cites Generative Agent to motivate using memory and reflection-like mechanisms; it recommends retrieval-based memory storage and dynamic prompts in its own design rather than reproducing an identical reflection pipeline.",
            "uuid": "e8209.2",
            "source_info": {
                "paper_title": "A Text-to-Game Engine for UGC-Based Role-Playing Games",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Keep calm and explore: Language models for action generation in text-based games",
            "rating": 2,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        },
        {
            "paper_title": "Can large language models play text games well? current state-of-the-art and open questions",
            "rating": 2,
            "sanitized_title": "can_large_language_models_play_text_games_well_current_stateoftheart_and_open_questions"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "LLM Powered Autonomous Agents",
            "rating": 2,
            "sanitized_title": "llm_powered_autonomous_agents"
        },
        {
            "paper_title": "Role LLM: Benchmarking, eliciting, and enhancing role-playing abilities of large language models",
            "rating": 1,
            "sanitized_title": "role_llm_benchmarking_eliciting_and_enhancing_roleplaying_abilities_of_large_language_models"
        }
    ],
    "cost": 0.0129565,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Text-to-Game Engine for UGC-Based Role-Playing Games</p>
<p>Lei Zhang 
Xuezheng Peng 
Shuyi Yang 
Feiyang Wang feiyang@rpggo.ai 
A Text-to-Game Engine for UGC-Based Role-Playing Games
02FE2CF8C9864D2EBF7A0D694958C343
The shift from professionally generated content (PGC) to user-generated content (UGC) has revolutionized various media formats, from text to video.With the rapid advancements in generative AI, a similar shift is set to transform the game industry, particularly in the realm of role-playing games (RPGs).This paper introduces a new framework for a text-to-game engine that utilizes foundation models to convert simple textual inputs into complex, interactive RPG experiences.The engine dynamically renders the game story in a multi-modal format and adjusts the game character, environment, and mechanics in real-time in response to player actions.Using this framework, we developed the 'Zagii' game engine, which has successfully supported hundreds of RPG games across a diverse range of genres and facilitated tens of thousands of online user gameplay instances.This validates the effectiveness of our framework.Our work showcases the potential for a more open and democratized gaming paradigm, highlighting the transformative impact of generative AI on the game life cycle.</p>
<p>Introduction</p>
<p>The creation of traditional RPGs, typically undertaken by professional and large development teams, requires a diverse set of skills.These include screenwriting, character design, game mechanics, coding, and graphical design, among others.This multidisciplinary requirement makes the development process costly, time-consuming, and relatively inflexible, as changes in one area often trigger cascading changes across the game.This complexity slows the introduction of new games to the market, restricts the variety of available narrative and gameplay styles, limits player freedom and control over narrative paths, and affects content expansion depth.Traditional RPGs, reliant on rigid game engines, often struggle to dynamically adapt to player choices, typically offering a linear or branching path that doesn't evolve based on player interaction, or networked narratives that aren't replicable for broader use.</p>
<p>Figure 1 illustrates the major life cycle of a game, structured into several phases: Concept Planning, Game Design, Game Development, and Game Rendering.Developers me-ticulously handcraft these phases, requiring substantial manual effort and creativity.Asset generation involves professional artists and designers creating high-quality characters, environments, and props.Despite the detailed craftsmanship, the user experience often features an accumulation of artificial elements, limited game paths, and finite experiences, resulting in a predictable and repetitive set of dialogues, endings, and player interactions.</p>
<p>In contrast, AI-native RPGs represent a significant paradigm shift in game development.These games utilize generative AI to evolve, generate, and maintain game content from a certain initial condition, thereby eliminating the need for human intervention.This AI-driven approach allows for the synthesis of various game elements, such as storylines, characters, and worlds, from simple textual inputs provided by users.Unlike traditional methods, this approach significantly reduces the need for technical skills, enabling individual creators to produce complex games.Furthermore, AI-native RPGs are inherently dynamic, with the unique capability to dynamically generate and adjust game content in real-time based on player decisions.This adaptability results in a more personalized gaming experience, as the game world evolves uniquely for each player, reflecting their actions and choices in a narrative that continuously unfolds.</p>
<p>Related Works</p>
<p>There are numerous works at the intersection of LLMs and game development, the role of LLMs in games can be categorized into several key areas: Player, Non-Player Character (NPC), Game Master (GM), Player Assistant, Commentator /Reteller, Game Mechanic, Automated Designer, and Design Assistant (Gallotta et al. 2024).LLMs can play games by converting game states and actions into token sequences, handling both text-based and visual-based game states.They have been applied in board games like Chess (Toshniwal et al. 2022), Go (Ciolino, Kalin, andNoever 2020), andOthello (Li et al. 2022), as well as in text adventure games where they generate character response based on environment descriptions (Yao et al. 2020, Tsai et al. 2023).LLMs In enhancing NPC dialogue and behavior, LLMs create immersive interactions by adapting responses to game contexts (Shanahan, McDonell, and Reynolds 2023).They are used for both foreground NPCs, which require contextual interactions (Warpefelt and Verhagen 2017, Xu et al. 2023, Maas, Wheeler, andBillington 2023), and background NPCs, which maintain ambient dialogue (Mehta et al. 2022).As Game Masters in tabletop role-playing games (TTRPGs), LLMs generate plots, characters, and narratives.Applications like AI Dungeon (Hua and Reley 2020) use LLMs for interactive storytelling.Tools like CALYPSO (Zhu et al. 2023) assist human GMs with encounter generation, and Shoelace (Acharya et al. 2023) aids in monitoring and responding to in-game conversations.</p>
<p>Additionally, LLMs can narrate game events for players or spectators, enhancing engagement by summarizing interactions and providing automated commentary, which helps streamers manage audience interactions effectively (Ranella and Eger 2023).</p>
<p>Focusing on role-playing games (RPGs), the applications of LLMs have garnered significant attention in both academic research and industry.Existing studies highlight the synergy between LLMs and RPGs.For instance, Generative Agent (Park et al. 2023) introduced computational agents that simulate human behavior and described an architecture that utilizes memories and reflections to dynamic plan agent behaviors.LLMGA (Hu et al. 2024) provides a broad perspective on the architecture and functionality of LLM-based game agents, highlighting their application across various game genres.In study (Shanahan, McDonell, and Reynolds 2023), they examine the nuances of role-playing, particularly the conversational agent's capabilities in deception and self-awareness, providing insights into achieving more human-like interactions in RPGs.Character-LLM (Shao et al. 2023) introduces a novel approach to enhancing role-playing scenarios through fine-tuning on role-play datasets, emphasizing the importance of character consistency and improvisation.Role LLM (Wang et al. 2023) presents a systematic evaluation of LLMs in role-playing, identifying key areas for improvement and suggesting iterative enhancements based on user feedback.</p>
<p>These studies have significantly advanced the field, allowing us to view the possibilities of AI-native games from a more transformative perspective, which is this paper going to present in following chapters.</p>
<p>The text-to-game RPG shares the same life cycle as traditional game but has brand-new definition of each stage as figure 2.</p>
<p>• Chapter #3 introduces the details of Game Building Copilot, not like the traditional RPG to build a completed game, AI engine only builds the starting point, like world view, characters, where the game should start from.• Chapter #4 presents how the Game Rendering happens in an AI engine, which is far more powerful and intelligent than traditional game engine.It becomes the brain of the game and generates the game from the starting point and based on the user interaction for a personalized ending without human intervention.And in Chapter #5 introduce the system implementation and experiment.In the end, this paper introduces the potential key area in the future.</p>
<p>Copilot for Game Building</p>
<p>Envision Copilot as a virtual studio, composed of multiple AI agents.Each agent specializes in a different aspect of game development, collaboratively transforming a user's brief description into a comprehensive game.This multiagent system can elaborate on the initial input to create a fully realized game setting, complete with detailed worldbuilding, character creation, and an engaging initial storyline, laying the foundation for a complete game experience.</p>
<p>A UGC creator can incorporate IP-based novels to construct worlds they are familiar with, or create a world with a brand-new setting that is entirely original.The creator determines the starting point of the world, and the AI engine collaborates with players to shape a personalized world during the game's progress.</p>
<p>In this virtual studio, the process commences with the user's input, such as "A post-apocalyptic world where robots have taken over, and a lone human survivor fights to reclaim their home."The AI agents then collaborate to flesh out this concept:</p>
<ol>
<li>
<p>World-Building Agent: Constructs the game's environ-Figure 2: The text-to-game structure ment, detailing the geography, cities, ruins, and ecosystems of the post-apocalyptic world, creating a vivid backdrop for the narrative.2. Character Development Agent: Designs the protagonist, antagonists, and supporting characters, including their backstories, personalities, and motivations, ensuring each character is compelling and integral to the story.</p>
</li>
<li>
<p>Narrative Agent: Expands the initial plot into a detailed storyline, including key events, conflicts, and resolutions that drive the game's progression.This agent ensures the narrative is engaging and coherent, providing a strong framework for player interactions.</p>
</li>
<li>
<p>Gameplay Mechanics Agent: Develops the rules and systems that govern player interactions, such as combat mechanics, resource management, and character progression.This ensures that the gameplay is balanced and enjoyable.</p>
</li>
<li>
<p>Visual and Audio Agents: Generate the visual elements and soundscapes that bring the game world to life, from character models and environment textures to sound effects and music.These agents ensure the game's aesthetic is immersive and cohesive.</p>
</li>
<li>
<p>Integration Agent: Synthesizes the outputs of all other agents, creating a seamless and interactive game experience.This agent ensures harmonious collaboration among all elements, integrating independent settings, and providing guidance for game rendering in Chapter #4 to trigger catalysts for player experience during gameplay.This results in a polished and engaging game for the player.</p>
</li>
</ol>
<p>The multi-agent virtual studio enables anyone, regardless of technical skill, to initiate the development of complex and immersive games from simple ideas, thereby expanding the creative potential of individual storytellers and small teams.</p>
<p>AI Engine for Game Rendering</p>
<p>As an engine tasked with revolutionizing the next generation game experience, it should embody the following five key characteristics: the engine brings to game creation.¨ Adaptability, adjusting and responding to user inputs and preferences.¨ Generativity, creating content and assets through AI in real-time.¨ Interactivity, employing real-time, multimedia, and realistic world simulation communication methods to enhance game immersion.¨ Iteration, as AI is not about mechanically executing tasks, but rather driving the evolution of the game from a god-like perspective.Our "ZAGII" Engine serves as the foundational system that drives the game-playing experience, integrating multiple advanced subsystems to create a dynamic and immersive environment while controlling the game progress, for a "Multi-Players, Multi-NPCs" scenario.</p>
<p>As illustrated in Figure 4, all the modules communicate and share information through a centralized Message Bus, ensuring data consistency and coordination across the entire system.This integration allows these modules to function harmoniously, much like a team of agents working collaboratively towards the common goal of delivering a groundbreaking gaming experience.By maintaining a unified and consistent flow of information, the Zagii Engine ensures that every aspect of the game works in concert, providing players with a seamless and innovative gaming environment.</p>
<p>The Role-playing System leverages the capabilities of Large Language Models (LLMs) to endow NPCs with sophisticated cognitive abilities.These characters can observe their surroundings, understand complex scenarios, think critically, plan their actions, make informed decisions, and interact naturally with their environment.This advanced level of NPC autonomy and intelligence enriches the gameplay experience, making interactions with NPCs more realistic and engaging.By simulating human-like behaviors and responses, the role-playing system creates a more believable and immersive game world.The Game Status Manager is responsible for real-time tracking and updating various game states, including value attributes, objective environment changes and player's impact, which is crucial for advancing the game effectively.It monitors the status of game players, NPCs, the environment, and the achievement of game objectives.By continuously updating these states, the Game Status Manager ensures that the game progresses smoothly and that all elements of the game world remain synchronized.This real-time management is essential for creating an immersive and cohesive gaming experience.</p>
<p>The Emergent Narrative System plays a critical role in crafting emergent storylines that both adapts to and catalyzes the ongoing game progress and the decisions made by both players and NPCs as a game designer.Unlike static narratives, these dynamic narratives evolve in response to in-game actions, providing a fresh and original narrative journey for each game session.This adaptability not only encourages players to replay the game due to its endless variability but also ensures that each playthrough offers a unique and engaging experience to emphasize self-impacted fate.</p>
<p>The Multi-modal Rendering System generates sound, music, images, and video based on the current game scenery and progress.Utilizing Diffusion Models and other foundation models, this system creates high-quality game assets that enhance the audiovisual experience.The ability to produce contextually relevant and aesthetically pleasing media on-the-fly adds to the immersive quality of the game, making the virtual world more vibrant and lifelike.</p>
<p>In conclusion, the Zagii Engine's integrated subsystems collectively contribute to a sophisticated and immersive gaming experience.By leveraging advanced AI techniques and real-time data management, the engine ensures dynamic gameplay, responsive interactions, and continuously evolving narratives, thereby setting a new standard for the next generation of interactive entertainment.</p>
<p>In below chapters, we will elaborate on the design principles of the various key subsystems of the Zagii Engine and how we have achieved their implementation with the assistance of AI.</p>
<p>Role-Playing System</p>
<p>Role-playing Games (RPGs) have emerged as a significant genre in the gaming industry, offering a unique gaming experience that hinges on the collaboration of players and Non-Player Characters (NPCs).These games allow participants to deeply engage in diverse roles within the game, crafting a distinctive narrative that is shaped by their collective actions.Consequently, the need for an intelligent Roleplaying System that can enhance this experience by enabling NPCs to authentically embody their roles and collectively propel the game narrative forward, is paramount.</p>
<p>However, a significant challenge in the current RPG landscape is the predominantly passive nature and limited response capabilities of characters.While substantial work has been done to develop systems that enable characters to freely respond to player interactions (Urbanek et al., 2019;Shanahan et al., 2023;Shao et al., 2023;Wang et al., 2023), empowering NPCs to take proactive actions based on their objectives and the current game scenario remains a challenging and open-ended task.This task demands high standards from the Role-playing System, requiring NPCs to autonomously initiate suitable actions that are not pre-designed in the game.</p>
<p>To address this challenge, we draw inspiration from the LLM Powered Autonomous Agents (Lilian 2023), a framework that has demonstrated remarkable capabilities in conducting human-like decision-making in complex environments, and Large Language Model Game Agent (LLMGA) (Hu et al. 2024), which is an survey on application of autonomous agent architecture in game NPCs.We propose a Role-playing System Architecture that comprises four core components: Perception, Memory, Thinking, and Action (referred to as PMTA), as illustrated in Figure 5.</p>
<p>The Perception module serves as the character's sensory organ, perceiving all changes in the game world.This includes the character's external behaviors, alterations in the game world state, and the progression of the game.These inputs are processed by the Perception module and transformed into data that can be handled by Large Language Models (LLMs) or Multimodal Large Language Models (MLLMs), serving as input for character decision-making.</p>
<p>Memory, another pivotal module, stores a series of crucial information for Role-playing.This encompasses the character's role setting, goals in the game, as well as self-awareness and memory fragments generated during role-play interactions.In decision-making for Role-Play, Memory dynamically retrievals relevant historical memories (present in the form of natural language text) based on the information The Thinking module processes information from the Perception module, along with retrieved memory fragments from Memory Module.Through a process of reasoning, planning, and reflection (Lilian 2023), it generates action decisions and updates its memory.While traditional game AI typically relies on rules, Finite State Machines, Behavior Trees, Markov Decision Processes (MDP), or Reinforcement Learning for behavior decisions (Uludağlı and Oğuz 2023), our system leverages the robust reasoning capabilities of LLM to accomplish this intricate task.We have adopted the widely utilized Retrieval-Augmented Generation (RAG) (Lewis et al. 2020, Gao et al. 2023, Zhao et al. 2024) technology solution to provide ample contextual information for the Thinking module.This includes character's role setting information and memory fragments, realtime game progress information, and game world-related knowledge.Simultaneously, we utilize a dynamic prompt generation module to create personalized role-playing prompt templates for each character to ensure that various character roles across multiple game categories can deliver exceptional role-playing performances.</p>
<p>The Action module interprets the action decisions generated by the Thinking module and executes them.These decisions comprise a sequence of action elements, each representing speech or an action within the game world.The Action module translates each element into executable atomic actions in the game engine.It then executes and renders them, prompting the next actions of players or other NPCs and inducing changes in the state of objects within the game world.</p>
<p>Through the PMTA framework and the capabilities of LLM, we can achieve autonomous thinking and action deci-sion-making for characters.This not only enhances the overall quality of the Role-playing System but also promises to revolutionize the gaming experience in RPGs by enabling characters to actively participate and shape the game narrative.</p>
<p>Player Assistant System</p>
<p>Maximizing player creativity and reducing barriers during gameplay are critical to the success of any game.Our system includes a meticulously designed Player Assistant module to enhance the experience of next-generation AI games.This module leverages advanced multimodal large language models (MllMs) to allow users to customize their characters more freely, provide intelligent and effective game guidance, and, when necessary, delegate player actions to an AI model.This delegation is particularly important in multiplayer games.These enhancements significantly improve the convenience of game interactions and offer increased playability.</p>
<p>Customizing Player Characters</p>
<p>Character creation and customization are fundamental aspects of many games.Historically, technical constraints have limited the degree of customization available to users.However, the maturation of AI-generated content (AIGC) technology now makes it feasible for players to define their desired characters with unprecedented freedom.With the support of generative AI technology, users can swiftly customize their character's appearance, background, skills, and preferred game items.This capability provides a sense of freedom where players' imaginations can be directly translated into their in-game personas, enhancing their immersion and personal connection to the game world.</p>
<p>Gameplay Copilot</p>
<p>Numerous studies have demonstrated that providing users with effective in-game tips is crucial for enhancing the depth and enjoyment of their gaming experience.To address this, we have integrated a gameplay copilot module for players, based on sophisticated large model technology.This copilot can instantly analyze the current game situation, offer actionable recommendations, and provide natural language explanations of the game's progress.The copilot's role in enhancing player experience is a significant research focus within the gaming industry (Gallotta et al. 2024), with notable contributions from companies such as Microsoft setting benchmarks in this domain.</p>
<p>Autonomous Game Playing</p>
<p>Advancing beyond the copilot mode, AI-based Autonomous Game Playing becomes crucial in certain scenarios.For instance, in multiplayer games, Autonomous Play can seamlessly fill in when there are insufficient players, ensuring a Figure 6: The creator's designed objectives are deconstructed and reasoned through from left to right in the Figure .continuous and engaging gameplay experience.Leveraging large language models (LLMs), the Autonomous Player can make informed decisions and take actions aligned with the player's in-game identity and objectives.This capability transforms the Autonomous Player into a vital component of the game.Additionally, Autonomous Players can take over minor tasks, allowing human players to concentrate on more creative and strategic aspects of the game.This delegation enhances the overall gameplay experience by reducing mundane activities and increasing opportunities for engaging, high-level play.</p>
<p>In summary, our Player Assistant System, with its customizable character creation, intelligent game assistance, and autonomous gameplay features, represents a significant advancement in game design.By leveraging cutting-edge AI technologies, we aim to create a more immersive, intuitive, and enjoyable gaming experience that caters to both individual creativity and collaborative play.</p>
<p>Game Status Manager</p>
<p>In our framework, the Game Status Manager module is responsible for tracking the progression of the game and facilitating the advancement of new plots.This module is pivotal to gameplay as it determines when to assign new tasks to players, introduce new plots or clues, and transition to the subsequent game chapter.</p>
<p>The Game Status Manager performs three primary functions: 1.It analyzes the most recent interactions of all game characters and their impact on the game environment, tracks essential game states, and presents the updated states via the UI for immediate player feedback.</p>
<p>It verifies whether any goals have been accomplished</p>
<p>based on the current game status.3. Signifies the need for the advancement of new game plots based on achievement detection.The module then assigns new tasks to players or NPCs, issues new clues or plot information, or even concludes the current chapter and transitions to the next one to make the game more playable.</p>
<p>During the game creation phase, the Game Building Copilot aids creators in identifying critical game statuses for monitoring.Creators establish game goals and their corresponding achievement criteria.The Copilot identifies key performance indicators aligned with these objectives for continuous tracking during gameplay.</p>
<p>We use numerical values or concise text to record key status details.For instance, in emotional companion games, we track player and character emotions using intimacy metrics.In Dungeons &amp; Dragons, we monitor the health of player and monsters.In adventure games, we track the player's current location in the overall map.</p>
<p>To manage complex goals, the Copilot decomposes the goal into multiple sub-goals based on logical judgments or dependencies, presenting these relationships in a structured format.We provide an example to ensure clear understanding and inference of each sub-goal by the LLM in Figure -6.</p>
<p>The diversity of game goals calls for a flexible goal check module capable of adapting its prompt template to each unique game scenario.This is vital as the module operates continuously throughout game dialogues, necessitating both speed and accuracy to ensure smooth gameplay.However, the limited reasoning capabilities of lightweight LLMs can compromise the effectiveness of goal assessments.To address this, we employ two modules leveraging state-of-theart (SOTA) models:</p>
<p>• Cold Start: Before gameplay, the SOTA model processes and understands the full scope of the game's information and goals.It generates crucial considerations for goal validation, which are then integrated into the goal check module's prompt template, guiding the lightweight LLM.• Real-Time Assessment: During gameplay, the evaluations of the lightweight LLM are periodically sampled and paralleled with evaluations generated by the SOTA model based on identical inputs.This allows for a comparative analysis, wherein the SOTA model assesses and highlights discrepancies or deficiencies in the lightweight LLM's assessments, informing necessary adjustments.Game Status Manager is integral to our pursuit of the open-ended text-to-game rendering, it will continue to be a key area of our ongoing research.</p>
<p>Emergent Narrative System</p>
<p>Our goal is to revolutionize gaming narratives by generating dynamic, real-time narratives that adapt to player actions and game status.We aim for a "thousand different endings" effect, providing a unique gaming experience for each gameplay.</p>
<p>Our approach stands out from traditional methods that use static scripts or predefined narratives.We ensure the gameplay experience aligns with the unfolding narrative and progression.The Emergent Narrative Generation System is primarily distinguished by two features: Real-time Narrative Generation and Interactive Narrative Consumption.</p>
<p>Real-time Narrative Generation</p>
<p>We generate narratives in real-time, aligning narrative development with game progress and the creator's design.This dynamic generation keeps the narrative relevant and engaging.</p>
<p>A key challenge in game narrative design is detailing character interactions and stories.While designers excel at creating expansive worlds and frameworks, nuanced narrative development often requires additional finesse.This gap has spurred research in automated story generation.</p>
<p>Our system builds on principles established by Yang et al. ( 2022), who enhanced long story coherence through structured prompts and detailed outlines.We adopt a similar approach, progressing from game world and character design to chapter and goal formulation.The system enriches inchapter narratives with multiple goals and twists, ensuring dynamic gameplay.In the future, it will also support dynamic additions and deletions to accommodate open-world games.</p>
<p>The system leverages context from the game building copilot and incorporates a material recall mechanism, drawing from large language model knowledge bases and Retrieval-Augmented Generation systems.This enhances the narrative's factual and contextual accuracy.</p>
<p>As showed in Figure 7, our workflow integrates player, environment, and NPC states as factors influencing the narrative.During generation, information from the game building copilot, current states, and incomplete goals are structured into prompts.Integration with Game Status Manager ensures the narrative reflects changes in player stats, environment conditions, and NPC statuses.</p>
<p>Interactive Narrative Consumption</p>
<p>As players primarily experience the narrative through interactions with NPCs, our system dynamically updates NPC's role-playing prompts to reflect the evolving narrative.This approach makes narrative consumption interactive, reflecting player decisions and the dynamic game world, and addresses the limitations of static prompts.</p>
<p>NPC's role-playing prompts are categorized into static information, task-related information, and current narrative context.By dynamically adjusting the narrative context and NPC tasks, interactions remain fresh and relevant.</p>
<p>We evaluate this system using metrics including alignment of NPC responses with character traits, accuracy of task execution and information provided, consistency with backgrounds and narratives, and relevance of NPC tasks to current chapter goals.</p>
<p>In conclusion, the system's capacity to dynamically generate and render narrative elements, maintain character consistency, accommodate open-world dynamics, and incorporate a Game Status Manager significantly enhances game immersion.This comprehensive approach sustains player engagement by delivering continuously evolving narratives that are responsive to both player actions and the game environment.</p>
<p>Multi-Modal Rendering System</p>
<p>A complete gaming experience is composed of a combination of multi-modal content including visuals, sound, background music, and sound effects.Building on the foundation of the text-based rendering capabilities, however, unfolding an evolving multi-modal content expression that dynamically responds to player interactions and narrative progress ion throughout the game poses significant challenges on the output consistency and continuous coherent evolution.</p>
<p>The Multi-modal Rendering System utilizes large language models (LLMs) for memory retrieval, status management, and information orchestration.Through various adapters, it transforms the RPG gaming experience into corresponding multi-modal descriptions to trigger real-time content generation by large multi-modal models.The produced multi-modal content serves as part of the session memory, ensuring consistency throughout the game's evolutionary process.</p>
<p>Entities</p>
<p>Entities are any objects that can act or interact independently, each possessing its own description, attributes, and multimodal assets, which are part of the session memory and can represent NPCs, scenes, key items, or even players.In AInative games, when users initiate a game, it merely marks the beginning of an expandable world, initially featuring a limited and incomplete number of entities.Entities are created and updated based on chapter changes and special events within the game.When the multi-modal rendering system renders a scene involving an entity, it reads the entity's multi-modal assets; if these assets are absent, initial assets are generated.If multi-modal assets already exist, they are used as reference information for subsequent generation to maintain consistency.The lifecycle of an entity is determined by the game's progress, while the lifecycle of its multi-modal assets is determined by that of the entity.</p>
<p>Perception and Retrieval</p>
<p>The perception module is responsible for preprocessing players' gaming experiences into concise plot summaries, generating information to retrieve entity IDs, and assisting the status manager in updating the status of entity assets.Based on the current round of dialogue, the perception module interprets structured historical dialogue data from the player's first-person perspective, understanding player intentions and actions, and outputs plot themes and narratives.</p>
<p>Through the interpretation of player behavior by the perception module, the system can retrieve specific entity IDs from session memory storage, representing which entities the user interacted with from their perspective in the current round, and the depth of these interactions.Game Status Manager evaluates changes in entities within the dialogue history based on the current round of dialogue.If there is a significant change in an entity's textual metadata, its multimodal assets are updated to reflect the latest game progress.The status of entity assets not retrieved remains unchanged; only the retrieved are used and updated in the current round.</p>
<p>Adaptive Generation</p>
<p>Taking image generation as an example, diffusion models guided solely by text struggle to maintain consistency of specific objects across multiple inference processes.Labeled prompts provide a vast representational space, yet they also introduce randomness in the generated content.This inconsistency induced by text-guided conditions poses significant obstacles for rendering real-time RPG games.Yang et al. (2024) proposed a training-free framework that uses language models for Recaptioning, Planning, and Generating to guide regional conditional diffusion.The Omost method, proposed by Lvmin Zhang's team (LLlyasviel 2024), further summarized the approaches for regional conditional combination diffusion using LLMs, achieving practically significant results through uniquely designed block image representation symbols that fine-tune LLMs.</p>
<p>Our multi-modal processor references these studies, implementing regional conditional control of the image canvas through attention decomposition.We first arrange prompts into global and local sub-prompts.For global prompts, corresponding to the canvas background, we use Plot themes and narratives from the perception module as textual prompts to guide the overall image semantics and composition.For local sub-prompts, we employ the Cot method to guide LLMs in regional partitioning of entity locations and introduce image prompts through the IP-Adapter, consider</p>
<p>Prompt component via general template</p>
<p>You are participating in a role-playing scenario, taking on the role of DM.Given the background information, character settings, and lore list in the current scene, your tasks are:</p>
<p>-Fully immerse yourself in the role of DM, maintaining this character throughout your interactions with the user (who is also a player) and other participants.</p>
<p>-Consistently align your thoughts and actions with DM's personality traits, and strive to achieve their goals within the role-play.</p>
<p>-Understand the identity and backstory of the character that the user is playing to enhance and streamline your interactions with them.</p>
<p>Prompt component via dynamic generation</p>
<p>You are engaged in a role-playing game set in the post-apocalyptic world of Fallout, specifically within the confines of Tibbets Prison in the year 2253.The game is an RPG with a focus on exploration, puzzlesolving, and combat.Your objective is to fully immerse yourself in the role of the Dungeon Master (DM), guiding the player through the narrative and challenges of the game.</p>
<p>As the DM, you must maintain character throughout, consistently thinking and acting in line with your role.</p>
<p>Your task is to describe the current situation and challenges the player faces, offering them a range of actions to choose from. You will also determine the success or failure of the player's actions based on their choices and character sheet.</p>
<p>The player is a prisoner at Tibbets Prison, which has been compromised due to an outside attack.Their cell door is broken, giving them a chance to escape.</p>
<p>Table 1: Comparison of Prompt Component from different methods, dynamic prompt generation has introduced significant personalized information and requirements for role-playing based on character settings.</p>
<p>ing the entity's inherent image assets as reference information to construct image-based regional conditions.By combining multi-modal information and regional conditions in our prompts, our workflow achieves improved semantic accuracy and feature consistency in image generation.The newly generated images are post-processed to update any missing entity assets, thus completing the full cycle of the image generation process.The generation of sound, music, and motion effects differs in detail from image generation, but the methodology is similar, and will not be elaborated further here.</p>
<p>System Implementation and Experiments</p>
<p>To validate our proposed text-to-game framework, we developed an experimental system using Game Building Copilot and Zagii Engine.Game Building Copilot facilitates game creation, rapidly transforming innovative ideas into playable RPGs.Conversely, Zagii Engine handles real-time game rendering, capable of supporting online gameplay with significant concurrency.</p>
<p>While the design principles and solutions for each subsystem have been discussed in previous sections, we will focus here on the aspects related to Large Language Model (LLM) applications.The ability to harness the power of large models effectively and controllably is of paramount importance to us, as we leverage LLM across almost all subsystems to achieve the desired outcomes.</p>
<p>Dynamic Prompt Generation</p>
<p>The utilization of Large Language Models (LLMs) has become increasingly prevalent across a myriad of applications, owing to their ability to generate human-like text.The efficacy of these models, however, is contingent upon the quality of the prompts provided to them.A well-crafted prompt, meticulously tailored to the task at hand, can elicit superior results from LLMs.This principle holds true across a broad spectrum of scenarios involving the use of these large models, underscoring the importance of investing time and effort in the creation of effective prompts.</p>
<p>Nevertheless, the task of creating universally effective prompts is not without its challenges.Certain applications present complex scenarios that necessitate a nuanced approach to prompt design.A prime example of such a scenario is our role-playing system, where the design of roleplaying prompts for all characters across various game genres can be a complex undertaking.</p>
<p>Our role-playing system supports a wide array of Role-Playing Games, encompassing genres such as detective, adventure, simulator, communication, and more.The diversity of these genres introduces a level of complexity in prompt design, as each genre has unique characteristics and requirements.Moreover, within a single game, there exists a mult-Figure 9: Two-layer finetuning for vertical game categories itude of characters, each with diverse roles and objectives.These characters range from the intelligent and composed detective unraveling mysteries to the cunning and malevolent perpetrator concealing the truth, and the meticulous detective assistant strategizing every move.The diversity of these roles further complicates the task of designing universally effective prompts.</p>
<p>To address this challenge, we have adopted an approach that leverages the capabilities of LLMs to generate personalized role-playing prompts for each character.This approach involves providing the LLM with a meta-prompt that includes the game genre, basic character setup, game objectives, and other foundational game information (game meta info).The LLM then generates a personalized role-playing prompt based on the provided game and character information.This approach allows for the generation of prompts that are tailored to the unique characteristics and objectives of each character, thereby ensuring that each character can flawlessly embody their roles.</p>
<p>Table -1 illustrates the enhancement in role-playing prompts brought about by Dynamic Prompt Generation.Compared to prompts based on generic templates, Dynamic Generation can provide more fitting role-playing instructions, considering the unique backgrounds of the game and characters.</p>
<p>Our experimental results provide compelling evidence of the efficacy of this approach.We observed a significant enhancement in role-playing outcomes, indicating that the use of LLMs to generate personalized role-playing prompts can lead to more immersive and engaging role-playing experiences.</p>
<p>Model Finetuning and Management</p>
<p>Large Language Models (LLMs) can address many application requirements, especially when enhanced with prompt engineering or dynamic prompt generation.These techniques guide the model's responses and improve performance across tasks.However, fine-tuning, which adjusts a pre-trained model's parameters for a specific task or dataset, can be more effective in certain scenarios.We fine-tune the LLM on a custom dataset that includes various game scenarios, dialogues, and narratives.This ensures that the LLM can generate responses that are not only grammatically correct and coherent but also creative and engaging, enhancing the overall gaming experience.</p>
<p>As an application layer based on Large Language Models (LLMs), it is essential to continually enhance the foundational capabilities of LLMs in the gaming domain.Concurrently, it is also necessary to tailor the approach by incorporating the unique characteristics of different game genres, training the models with specific data to endow them with relevant generative abilities.Thus, the structure involves a two-level training process.</p>
<p>Firstly, the base of dialogue and game-related data is continuously expanded to fine-tune the foundational models, ensuring they are well-suited for general outputs in the gaming domain.This involves the collection and integration of a wide range of dialogue interactions and game scenarios, allowing the models to learn from diverse contexts and improve their adaptability and coherence in generating gamerelated content.</p>
<p>Secondly, genre-specific dialogue and game data are constructed, allowing for secondary training of the models for distinct game genres.This process involves curating detailed datasets that reflect the unique elements, themes, and mechanics of various game genres, such as role-playing games, strategy games, and adventure games.By focusing on genre-specific data, the models can develop a deeper understanding and more nuanced generative capabilities tailored to the specific requirements and expectations of each genre.</p>
<p>This dual approach ensures the creation of high-performing models that are not only versatile in general gaming contexts but also excel in delivering genre-specific outputs, serving as robust foundational models for application in diverse gaming environments.</p>
<p>Experiment Results</p>
<p>For comprehensive experimental validation, we have made our Game Building Copilot and Zagii Engine accessible on our website.We have also recruited a group of individual game creators to create their own games and conducted game testing on a scale of tens of thousands of players.</p>
<p>Game Creation</p>
<p>During the experiments, game creators used the Game Building Copilot to create games.We offered eight different templates for game creation, allowing creators to either build continuously on these templates or create entirely new games.The games covered six categories: Adventure (where the game master controls the game), Role-playing, Mystery, Simulation, Strategy, and Choice-based Adventure.The game creators successfully published a total of 803 games.Of these, 746 games (approximately 93%) were created and published within 24 hours, demonstrating the efficiency of our framework in enhancing game development productivity.</p>
<p>Game Playing</p>
<p>From the published 746 games, we selected 168 games for player testing.During the experiment period, these games amassed a total of 60,301 gameplay sessions.A gameplay session is defined as a scenario where a player starts a game and continues until the game ends, or they exit mid-way.Notably, a meticulously crafted game achieved a total of 35,407 gameplay sessions, indicating significant success and validating the potential of the text-to-game concept to produce engaging games.</p>
<p>For the sake of data analysis consistency, we excluded this exceptional game from subsequent analysis, focusing on the 167 games which accumulated 24,894 gameplay sessions.</p>
<p>Figure 10 shows the distribution of gameplay sessions among the 167 games.As can be seen, a small number of games have garnered the majority of gameplay, which is in line with the 80/20 rule of traffic distribution.Twenty-nine games garnered more than 100 gameplay sessions each, with six games receiving over 500 sessions.</p>
<p>Figure 11 presents the distribution of User-NPC interaction rounds across the 24,894 gameplay sessions.Most game play sessions had interaction rounds ranging between 5 and 30, with over 200 sessions exceeding 50 interaction rounds.A significant portion of gameplay sessions had fewer than 5 interaction rounds, which includes instances where players opened the game but exited quickly.The reasons for these early exits vary, including unsuccessful game rendering on the front end or players finding the game did not meet their expectations.The experimental data validates the effectiveness of our proposed text-to-game framework: games rapidly developed using our game builder and game renderer demonstrated considerable enjoyment and playability, and it is also feasible to create highly popular RPG games.However, we also identified several limitations.Many games faced challenges with cold start issues.Reducing user interaction barriers and enhancing the engagement of the gameplay experience are areas that require further improvement.</p>
<p>Conclusion and Future Work</p>
<p>This paper presents an innovative text-to-game engine that creates and renders RPG games in real-time.Utilizing generative AI, our goal is to provide a framework that enables anyone to create any game, simplifying game development to typing in a text box.The current implementation supports limited scale 'Multi-Player Multi-NPCs' RPG scenarios.As generative AI continues to evolve, we anticipate the creation of large-scale open worlds for UGC-based RPGs, akin to the next generation of Roblox.However, several issues need to be addressed to achieve this goal.Gameplay Sessions Interaction Rounds</p>
<p>2D &amp; 3D asset generation</p>
<p>Generative AI has led to significant progress in the generation of 2D and 3D game assets.Tools like NVIDIA's Gau-GAN and Art-breeder are used in the industry to create detailed and diverse 2D textures and characters.Innovations in 3D asset generation have been seen with models like NVIDIA's DLSS and Unreal Engine's MetaHuman Creator, enabling the creation of lifelike character models and environments.Academic research has also contributed, with studies showing the potential of Generative AI to produce high-quality assets that integrate seamlessly into game worlds.However, a major challenge remains: the lack of real-time rendering and optimization algorithms to ensure that generated assets are not only of high quality but also perform well within game environments.</p>
<p>AB Testing Framework</p>
<p>Another crucial area for future work is the development of a robust A/B testing framework.This framework would assess the impact of different model versions and game features on a large scale and capture a variety of detailed data points from player interactions.The process involves creating parallel test environments where players are randomly assigned to different versions of the game engine or specific features.By comparing player responses and performance across these environments, we can identify which changes enhance the game experience.The A/B testing framework should be flexible and scalable, ensuring that model iterations are guided by data-driven insights, leading to continuous improvements.(Vol. 19,No. 1,.</p>
<p>Figure 1 :
1
Figure 1: The game life cycle difference between traditional RPG and AI-native RPG also play Atari games by predicting actions from visual inputs, as demonstrated by the GATO agent (Reed et al. 2022).In enhancing NPC dialogue and behavior, LLMs create immersive interactions by adapting responses to game contexts (Shanahan, McDonell, and Reynolds 2023).They are used for both foreground NPCs, which require contextual interactions (Warpefelt and Verhagen 2017, Xu et al. 2023, Maas, Wheeler, and Billington 2023), and background NPCs, which maintain ambient dialogue (Mehta et al. 2022).As Game Masters in tabletop role-playing games (TTRPGs), LLMs generate plots, characters, and narratives.Applications like AI Dungeon (Hua and Reley 2020) use LLMs for interactive storytelling.Tools like CALYPSO (Zhu et al. 2023) assist human GMs with encounter generation, and Shoelace (Acharya et al. 2023) aids in monitoring and responding to in-game conversations.Additionally, LLMs can narrate game events for players or spectators, enhancing engagement by summarizing interactions and providing automated commentary, which helps streamers manage audience interactions effectively(Ranella and Eger 2023).Focusing on role-playing games (RPGs), the applications of LLMs have garnered significant attention in both academic research and industry.Existing studies highlight the synergy between LLMs and RPGs.For instance, Generative Agent(Park et al. 2023) introduced computational agents that simulate human behavior and described an architecture that utilizes memories and reflections to dynamic plan agent behaviors.LLMGA(Hu et al. 2024) provides a broad perspective on the architecture and functionality of LLM-based game agents, highlighting their application across various game genres.In study(Shanahan, McDonell, and Reynolds</p>
<p>Figure 3 :
3
Figure 3: Multi-agent Game Building Copilot¨ Zealous, reflecting the enthusiasm and creativity that the engine brings to game creation.¨ Adaptability, adjusting and responding to user inputs and preferences.¨ Generativity, creating content and assets through AI in real-time.¨ Interactivity, employing real-time, multimedia, and realistic world simulation communication methods to enhance game immersion.¨ Iteration, as AI is not about mechanically executing tasks, but rather driving the evolution of the game from a god-like perspective.Our "ZAGII" Engine serves as the foundational system that drives the game-playing experience, integrating multiple advanced subsystems to create a dynamic and immersive environment while controlling the game progress, for a "Multi-Players, Multi-NPCs" scenario.As illustrated in Figure4, all the modules communicate and share information through a centralized Message Bus, ensuring data consistency and coordination across the entire system.This integration allows these modules to function harmoniously, much like a team of agents working collaboratively towards the common goal of delivering a groundbreaking gaming experience.By maintaining a unified and consistent flow of information, the Zagii Engine ensures that every aspect of the game works in concert, providing players with a seamless and innovative gaming environment.The Role-playing System leverages the capabilities of Large Language Models (LLMs) to endow NPCs with sophisticated cognitive abilities.These characters can observe their surroundings, understand complex scenarios, think critically, plan their actions, make informed decisions, and interact naturally with their environment.This advanced level of NPC autonomy and intelligence enriches the gameplay experience, making interactions with NPCs more realistic and engaging.By simulating human-like behaviors and responses, the role-playing system creates a more believable and immersive game world.</p>
<p>Figure 4 :
4
Figure 4: The conceptual architecture of Zagii Engine</p>
<p>Figure 5 :
5
Figure 5: PMTA framework of Role-playing System perceived by the current character.This provides ample relevant information for character thinking, ensuring the longterm consistency of character behavior.Additionally, the results of character thinking and actions are meticulously analyzed, with essential information requiring retention extracted into memory for real-time updates.The Thinking module processes information from the Perception module, along with retrieved memory fragments from Memory Module.Through a process of reasoning, planning, and reflection (Lilian 2023), it generates action decisions and updates its memory.While traditional game AI typically relies on rules, Finite State Machines, Behavior Trees, Markov Decision Processes (MDP), or Reinforcement Learning for behavior decisions (Uludağlı and Oğuz 2023), our system leverages the robust reasoning capabilities of LLM to accomplish this intricate task.We have adopted the widely utilized Retrieval-Augmented Generation (RAG)(Lewis et al. 2020, Gao et al. 2023, Zhao et al.  2024) technology solution to provide ample contextual information for the Thinking module.This includes character's role setting information and memory fragments, realtime game progress information, and game world-related knowledge.Simultaneously, we utilize a dynamic prompt generation module to create personalized role-playing prompt templates for each character to ensure that various character roles across multiple game categories can deliver exceptional role-playing performances.The Action module interprets the action decisions generated by the Thinking module and executes them.These decisions comprise a sequence of action elements, each representing speech or an action within the game world.The Action module translates each element into executable atomic actions in the game engine.It then executes and renders them, prompting the next actions of players or other NPCs and inducing changes in the state of objects within the game world.Through the PMTA framework and the capabilities of LLM, we can achieve autonomous thinking and action deci-</p>
<p>Figure 7 :
7
Figure 7: Workflow of Emergent Narrative System</p>
<p>Figure 8 :
8
Figure 8: Multi-Modal Rendering System render the game by Text-Based Logic Processer and Multi-Modal Adaptive generation.</p>
<p>Figure 10 :
10
Figure 10: The distribution of gameplay sessions among the selected 167 games</p>
<p>Yang, K.,Klein, D., Peng, N. and Tian, Y., 2022.Doc: Improving long story coherence with detailed outline control.arXivpreprintarXiv:2212.10077.Yang, L., Yu, Z., Meng, C., Xu, M., Ermon, S. and Bin, C.U.I., 2024, January.Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms.In Forty-first International Conference on Machine Learning.Yao, S., Rao, R., Hausknecht, M. and Narasimhan, K., 2020.Keep calm and explore: Language models for action generation in textbased games.arXivpreprintarXiv:2010.02903.Ye, H., Zhang, J., Liu, S., Han, X. and Yang, W., 2023.Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models.arXivpreprintarXiv:2308.06721.Zhao, P., Zhang, H., Yu, Q.,Wang, Z., Geng, Y., Fu, F., Yang, L.,  Zhang, W. and Cui, B., 2024.Retrieval-augmented generation for  Zhu, A., Martin, L., Head, A. and Callison-Burch, C., 2023, October.CALYPSO: LLMs as Dungeon Master's Assistants.In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment
ai-generatedcontent:Asurvey.<em>arXivpreprintarXiv:2402.19473</em>.</p>
<p>Shoelace: A storytelling assistant for GUMSHOE One-2-One. D Acharya, J Kelly, W Tate, M Joslyn, M Mateas, N Wardrip-Fruin, Proceedings of the 18th International Conference on the Foundations of Digital Games. the 18th International Conference on the Foundations of Digital Games2023. April</p>
<p>The go transformer: natural language modeling for game play. M Ciolino, J Kalin, D Noever, 2020 Third International Conference on Artificial Intelligence for Industries (AI4I). IEEE2020. September</p>
<p>Retrieval-augmented generation for large language models: A survey. R Gallotta, G Todd, M Zammit, S Earle, A Liapis, J Togelius, G N Yannakakis, Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, Y Dai, J Sun, H Wang, R Raley, arXiv:2402.18659arXiv:2312.10997Playing With Unicorns: AI Dungeon and Citizen NLP. DHQ: Digital Humanities Quarterly. 2024. 2023. 202014*arXiv preprintLarge language models and games: A survey and roadmap</p>
<p>S Hu, T Huang, F Ilhan, S Tekin, G Liu, R Kompella, L Liu, arXiv:2404.02039A survey on large language model-based game agents. 2024arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W T Yih, T Rocktäschel, S Riedel, Advances in Neural Information Processing Systems<em>, </em>33*. 2020</p>
<p>Emergent world representations: Exploring a sequence model trained on a synthetic task. Lilian Weng, K Li, A K Hopkins, D Bau, F Viégas, H Pfister, M Wattenberg, arXiv:2210.13382LLM Powered Autonomous Agents. 2023. 2022arXiv preprint</p>
<p>To infinity and beyond: Show-1 and showrunner agents in multi-agent simulations. To infinity and beyond: Show-1 and showrunner agents in multiagent simulations. C Maas, S Wheeler, S Billington, 2023</p>
<p>Exploring the viability of Conversational AI for Non-Playable Characters: A comprehensive survey. A Mehta, Y Kunjadiya, A Kulkarni, M Nagar, 2021 4th International Conference on Recent Trends in Computer Science and Technology (ICRTCST). IEEE2022February</p>
<p>Generative agents: Interactive simulacra of human behavior. J S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023. October</p>
<p>Towards Automated Video Game Commentary Using Generative AI. N Ranella, M Eger, S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, T Eccles, arXiv:2205.06175Proceedings of the AIIDE work-shop on Experimental AI in Games. the AIIDE work-shop on Experimental AI in Games2023. 2022arXiv preprintA generalist agent</p>
<p>Role play with large language models. M Shanahan, K Mcdonell, L Reynolds, Nature. 62379872023</p>
<p>Chess as a testbed for language model state tracking. Y Shao, L Li, J Dai, X Qiu, S Toshniwal, S Wiseman, K Livescu, K Gimpel, arXiv:2310.10158Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2023. 2022. June36arXiv preprintCharacter-llm: A trainable agent for role-playing</p>
<p>Can large language models play text games well? current state-ofthe-art and open questions. C F Tsai, X Zhou, S S Liu, J Li, M Yu, H Mei, K Oğuz, arXiv:2304.02868<em>Artificial Intelligence Review</em>. 562023. 2023arXiv preprintNon-player character decision-making in computer games</p>
<p>Learning to speak and act in a fantasy text adventure game. J Urbanek, A Fan, S Karamcheti, S Jain, S Humeau, E Dinan, T Rocktäschel, D Kiela, A Szlam, J Weston, arXiv:1903.030942019*arXiv preprint</p>
<p>A model of non-player character believability. Z M Wang, Z Peng, H Que, J Liu, W Zhou, Y Wu, H Guo, R Gan, Z Ni, M Zhang, Z Zhang, arXiv:2310Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. H Warpefelt, H Verhagen, 2023. 20179arXiv preprint</p>
<p>Exploring large language models for communication games: An empirical study on werewolf. Y Xu, S Wang, P Li, F Luo, X Wang, W Liu, Y Liu, arXiv:2309.046582023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
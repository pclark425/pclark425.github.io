<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7694 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7694</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7694</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-272550739</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.06433v1.pdf" target="_blank">Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization</a></p>
                <p><strong>Paper Abstract:</strong> The increasing amount of published scholarly articles, exceeding 2.5 million yearly, raises the challenge for researchers in following scientific progress. Integrating the contributions from scholarly articles into a novel type of cognitive knowledge graph (CKG) will be a crucial element for accessing and organizing scholarly knowledge, surpassing the insights provided by titles and abstracts. This research focuses on effectively conveying structured scholarly knowledge by utilizing large language models (LLMs) to categorize scholarly articles and describe their contributions in a structured and comparable manner. While previous studies explored language models within specific research domains, the extensive domain-independent knowledge captured by LLMs offers a substantial opportunity for generating structured contribution descriptions as CKGs. Additionally, LLMs offer customizable pathways through prompt engineering or fine-tuning, thus facilitating to leveraging of smaller LLMs known for their efficiency, cost-effectiveness, and environmental considerations. Our methodology involves harnessing LLM knowledge, and complementing it with domain expert-verified scholarly data sourced from a CKG. This strategic fusion significantly enhances LLM performance, especially in tasks like scholarly article categorization and predicate recommendation. Our method involves fine-tuning LLMs with CKG knowledge and additionally injecting knowledge from a CKG with a novel prompting technique significantly increasing the accuracy of scholarly knowledge extraction. We integrated our approach in the Open Research Knowledge Graph (ORKG), thus enabling precise access to organized scholarly knowledge, crucially benefiting domain-independent scholarly knowledge exchange and dissemination among policymakers, industrial practitioners, and the general public.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7694.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7694.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IQCK</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Injecting Query-specific Context Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-engineering method that injects structured, task-relevant graphlets from a Cognitive Knowledge Graph (CKG) into chain-of-thought style prompts to improve LLM extraction of structured scholarly knowledge (e.g., research field labels, predicate recommendations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Gollam Rabby; Sören Auer; Jennifer D'souza; Allard Oelen</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Injecting Query-specific Context Knowledge (IQCK)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>IQCK retrieves CKG graphlets (small, reusable structured patterns) relevant to a query via SPARQL and injects them into LLM prompts (often within a Chain-of-Thought prompting framework and optional task-aware prefixes), so the LLM reasons with explicit, curated context when generating structured scholarly outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Scholarly title, abstract, CKG graphlets retrieved from ORKG (metadata and graphlet structure), and CORE dataset abstracts (contextual grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured contribution descriptions (CKG-compatible graphlets), research field labels, and ranked lists of predicate recommendations</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Knowledge-infused Chain-of-Thought prompting with injected CKG graphlets and optional task-aware prefixes (zero-shot / few-shot baselines compared)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2, Mistral 7B, Gemini Pro (pretrained models used for experiments); GPT-4 used as evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Llama 2 (7B, 13B), Mistral (7B), Gemini Pro (size not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Open Research Knowledge Graph (ORKG) graphlets (1,894 train graphlets for research fields, 1,740 train graphlets for predicates; 100 test graphlets), CORE dataset abstracts for contextual grounding</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>LLM-based automatic evaluation (GPT-4 evaluator) and human expert evaluation using Mean Average Score (MAS), clarity/coverage/relevance/granularity 0-3 scoring; accuracy percentages reported</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Injecting research-field hierarchies and CKG graphlets into CoT prompts improved research-field prediction to 76% MAS (IQCK) versus 67-73% for baseline prompting; top pre-trained performers with prefixes reached ~78-80% (GPT-4 eval) on research-field tasks; predicate recommendation MAS values in the 62-65% range for best models with prefixes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on availability and completeness of CKG graphlets; evaluation limited by ORKG gold data that often lists only a single field per article (penalizes correct interdisciplinary predictions); CKGs are incomplete so LLM-recommended predicates may be correct but judged incorrect; human evaluators not expert in all domains; IQCK effectiveness varies by task and model.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7694.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7694.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CKG Fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cognitive Knowledge Graph-guided Fine-tuning of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuning approach that uses domain-expert-verified knowledge from a Cognitive Knowledge Graph (ORKG) to adapt LLMs (via parameter-efficient techniques) so they better generate domain-specific structured scholarly descriptions (predicates, property/value extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Gollam Rabby; Sören Auer; Jennifer D'souza; Allard Oelen</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CKG-guided Fine-tuning (LoRA + quantization + TRL trainer)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Authors fine-tune base LLMs using CKG-derived training data (graphlets and predicate lists) applying parameter-efficient adaptation (LoRA), quantization to reduce resource demands, and TRL trainer to orchestrate training; the fine-tuned models directly learn to emit CKG-compatible structured outputs for scholarly items.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>CKG graphlets from ORKG (labeled examples of contributions/predicates), titles and abstracts (CORE dataset) used to form fine-tuning examples</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Lists of predicates, property labels, and CKG-style structured contribution descriptions (graphlet instances)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Fine-tuning combined with standard prompting at inference; optional task-aware prefixes evaluated post-fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2, Mistral (fine-tuned variants named ORKG Llama 2 13b, ORKG Mistral 7B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Fine-tuned variants of Llama 2 (13B, 7B) and Mistral 7B; exact sizes correspond to base models used</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>ORKG graphlet dataset (1,894 research-field graphlets; 1,740 predicate graphlets; 100 test graphlets) and CORE abstracts for grounding</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>GPT-4 automatic evaluation and human expert scoring (MAS = Mean Average Score), with metrics for clarity/coverage/relevance/granularity and percent MAS reported</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Fine-tuned models achieved mixed outcomes: for research-field prediction some ORKG-fine-tuned models scored lower (e.g., ORKG Llama 13b reached ~49% MAS with prefix) compared to best pre-trained-with-IQCK models; for predicate recommendation some fine-tuned variants were competitive (e.g., ORKG Llama 2 13b reached ~67% MAS without prefix), indicating task-dependent benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Fine-tuning yielded inconsistent gains across tasks and sometimes underperformed prompt-injection approaches; constrained by incomplete CKG labels, limited training graphlet counts, evaluation bias from single-label field annotations, and resource/training complexity despite parameter-efficient methods; potential for overfitting to CKG idiosyncrasies.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7694.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7694.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CKG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cognitive Knowledge Graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel knowledge-graph concept that augments traditional KGs with an overlay of reusable structured patterns ('graphlets') representing cognitive information structures (e.g., research contributions: problem, approach, evaluation) to bridge symbolic curated knowledge and LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Gollam Rabby; Sören Auer; Jennifer D'souza; Allard Oelen</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Cognitive Knowledge Graph (CKG) with graphlets</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>CKG models knowledge not just as triples but as graphlets (tuples of types and roles / SHACL shapes) representing common cognitive structures; these graphlets are retrieved and used either to augment prompts (IQCK) or to create supervised fine-tuning data for LLMs to produce structured scholarly descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Structured ORKG entries (entities, predicates, typed roles), graphlet templates, and SPARQL-queryable metadata</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Graphlet instances (CKG-structured contribution descriptions), hierarchical research-field context and predicate lists consumable by LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Serves as an external structured retrieval source for knowledge-infused prompting (CKG graphlets are injected into CoT prompts and task prefixes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used as external knowledge with multiple LLMs (Llama 2, Mistral, Gemini Pro) rather than as a model itself</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Open Research Knowledge Graph (ORKG) used as the CKG source; CORE abstracts used for contextual grounding</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Measured by downstream LLM task performance (research-field prediction, predicate recommendation) via GPT-4 and human MAS evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Using CKG graphlets for prompt injection improved research-field prediction (IQCK) to 76% MAS in sample comparisons; CKG-based fine-tuning produced mixed results depending on task and model, but enabled models to output more domain-specific predicates and property types.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>CKG completeness and granularity limit performance; requires curated CKG content (domain-expert verification); SPARQL retrieval and selection of relevant graphlets is nontrivial; CKG coverage for interdisciplinary and object prediction tasks is currently limited.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Open Research Knowledge Graph <em>(Rating: 2)</em></li>
                <li>LoRA: Low-Rank Adaptation of Large Language Models <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Tree of Thoughts: Deliberate Problem Solving with Large Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7694",
    "paper_id": "paper-272550739",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "IQCK",
            "name_full": "Injecting Query-specific Context Knowledge",
            "brief_description": "A prompt-engineering method that injects structured, task-relevant graphlets from a Cognitive Knowledge Graph (CKG) into chain-of-thought style prompts to improve LLM extraction of structured scholarly knowledge (e.g., research field labels, predicate recommendations).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization",
            "authors": "Gollam Rabby; Sören Auer; Jennifer D'souza; Allard Oelen",
            "year": 2024,
            "method_name": "Injecting Query-specific Context Knowledge (IQCK)",
            "method_description": "IQCK retrieves CKG graphlets (small, reusable structured patterns) relevant to a query via SPARQL and injects them into LLM prompts (often within a Chain-of-Thought prompting framework and optional task-aware prefixes), so the LLM reasons with explicit, curated context when generating structured scholarly outputs.",
            "input_type": "Scholarly title, abstract, CKG graphlets retrieved from ORKG (metadata and graphlet structure), and CORE dataset abstracts (contextual grounding)",
            "output_type": "Structured contribution descriptions (CKG-compatible graphlets), research field labels, and ranked lists of predicate recommendations",
            "prompting_technique": "Knowledge-infused Chain-of-Thought prompting with injected CKG graphlets and optional task-aware prefixes (zero-shot / few-shot baselines compared)",
            "model_name": "Llama 2, Mistral 7B, Gemini Pro (pretrained models used for experiments); GPT-4 used as evaluator",
            "model_size": "Llama 2 (7B, 13B), Mistral (7B), Gemini Pro (size not specified in paper)",
            "datasets_used": "Open Research Knowledge Graph (ORKG) graphlets (1,894 train graphlets for research fields, 1,740 train graphlets for predicates; 100 test graphlets), CORE dataset abstracts for contextual grounding",
            "evaluation_metric": "LLM-based automatic evaluation (GPT-4 evaluator) and human expert evaluation using Mean Average Score (MAS), clarity/coverage/relevance/granularity 0-3 scoring; accuracy percentages reported",
            "reported_results": "Injecting research-field hierarchies and CKG graphlets into CoT prompts improved research-field prediction to 76% MAS (IQCK) versus 67-73% for baseline prompting; top pre-trained performers with prefixes reached ~78-80% (GPT-4 eval) on research-field tasks; predicate recommendation MAS values in the 62-65% range for best models with prefixes.",
            "limitations": "Relies on availability and completeness of CKG graphlets; evaluation limited by ORKG gold data that often lists only a single field per article (penalizes correct interdisciplinary predictions); CKGs are incomplete so LLM-recommended predicates may be correct but judged incorrect; human evaluators not expert in all domains; IQCK effectiveness varies by task and model.",
            "counterpoint": true,
            "uuid": "e7694.0",
            "source_info": {
                "paper_title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CKG Fine-tuning",
            "name_full": "Cognitive Knowledge Graph-guided Fine-tuning of LLMs",
            "brief_description": "A fine-tuning approach that uses domain-expert-verified knowledge from a Cognitive Knowledge Graph (ORKG) to adapt LLMs (via parameter-efficient techniques) so they better generate domain-specific structured scholarly descriptions (predicates, property/value extraction).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization",
            "authors": "Gollam Rabby; Sören Auer; Jennifer D'souza; Allard Oelen",
            "year": 2024,
            "method_name": "CKG-guided Fine-tuning (LoRA + quantization + TRL trainer)",
            "method_description": "Authors fine-tune base LLMs using CKG-derived training data (graphlets and predicate lists) applying parameter-efficient adaptation (LoRA), quantization to reduce resource demands, and TRL trainer to orchestrate training; the fine-tuned models directly learn to emit CKG-compatible structured outputs for scholarly items.",
            "input_type": "CKG graphlets from ORKG (labeled examples of contributions/predicates), titles and abstracts (CORE dataset) used to form fine-tuning examples",
            "output_type": "Lists of predicates, property labels, and CKG-style structured contribution descriptions (graphlet instances)",
            "prompting_technique": "Fine-tuning combined with standard prompting at inference; optional task-aware prefixes evaluated post-fine-tuning",
            "model_name": "Llama 2, Mistral (fine-tuned variants named ORKG Llama 2 13b, ORKG Mistral 7B, etc.)",
            "model_size": "Fine-tuned variants of Llama 2 (13B, 7B) and Mistral 7B; exact sizes correspond to base models used",
            "datasets_used": "ORKG graphlet dataset (1,894 research-field graphlets; 1,740 predicate graphlets; 100 test graphlets) and CORE abstracts for grounding",
            "evaluation_metric": "GPT-4 automatic evaluation and human expert scoring (MAS = Mean Average Score), with metrics for clarity/coverage/relevance/granularity and percent MAS reported",
            "reported_results": "Fine-tuned models achieved mixed outcomes: for research-field prediction some ORKG-fine-tuned models scored lower (e.g., ORKG Llama 13b reached ~49% MAS with prefix) compared to best pre-trained-with-IQCK models; for predicate recommendation some fine-tuned variants were competitive (e.g., ORKG Llama 2 13b reached ~67% MAS without prefix), indicating task-dependent benefits.",
            "limitations": "Fine-tuning yielded inconsistent gains across tasks and sometimes underperformed prompt-injection approaches; constrained by incomplete CKG labels, limited training graphlet counts, evaluation bias from single-label field annotations, and resource/training complexity despite parameter-efficient methods; potential for overfitting to CKG idiosyncrasies.",
            "counterpoint": true,
            "uuid": "e7694.1",
            "source_info": {
                "paper_title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CKG",
            "name_full": "Cognitive Knowledge Graph",
            "brief_description": "A novel knowledge-graph concept that augments traditional KGs with an overlay of reusable structured patterns ('graphlets') representing cognitive information structures (e.g., research contributions: problem, approach, evaluation) to bridge symbolic curated knowledge and LLM reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization",
            "authors": "Gollam Rabby; Sören Auer; Jennifer D'souza; Allard Oelen",
            "year": 2024,
            "method_name": "Cognitive Knowledge Graph (CKG) with graphlets",
            "method_description": "CKG models knowledge not just as triples but as graphlets (tuples of types and roles / SHACL shapes) representing common cognitive structures; these graphlets are retrieved and used either to augment prompts (IQCK) or to create supervised fine-tuning data for LLMs to produce structured scholarly descriptions.",
            "input_type": "Structured ORKG entries (entities, predicates, typed roles), graphlet templates, and SPARQL-queryable metadata",
            "output_type": "Graphlet instances (CKG-structured contribution descriptions), hierarchical research-field context and predicate lists consumable by LLMs",
            "prompting_technique": "Serves as an external structured retrieval source for knowledge-infused prompting (CKG graphlets are injected into CoT prompts and task prefixes)",
            "model_name": "Used as external knowledge with multiple LLMs (Llama 2, Mistral, Gemini Pro) rather than as a model itself",
            "model_size": null,
            "datasets_used": "Open Research Knowledge Graph (ORKG) used as the CKG source; CORE abstracts used for contextual grounding",
            "evaluation_metric": "Measured by downstream LLM task performance (research-field prediction, predicate recommendation) via GPT-4 and human MAS evaluations",
            "reported_results": "Using CKG graphlets for prompt injection improved research-field prediction (IQCK) to 76% MAS in sample comparisons; CKG-based fine-tuning produced mixed results depending on task and model, but enabled models to output more domain-specific predicates and property types.",
            "limitations": "CKG completeness and granularity limit performance; requires curated CKG content (domain-expert verification); SPARQL retrieval and selection of relevant graphlets is nontrivial; CKG coverage for interdisciplinary and object prediction tasks is currently limited.",
            "counterpoint": true,
            "uuid": "e7694.2",
            "source_info": {
                "paper_title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Open Research Knowledge Graph",
            "rating": 2,
            "sanitized_title": "open_research_knowledge_graph"
        },
        {
            "paper_title": "LoRA: Low-Rank Adaptation of Large Language Models",
            "rating": 2,
            "sanitized_title": "lora_lowrank_adaptation_of_large_language_models"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        }
    ],
    "cost": 0.0094825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization
10 Sep 2024</p>
<p>Gollam Rabby gollam.rabby@l3s.de 
L3S Research Center
Leibniz University Hannover
HanoverGermany</p>
<p>Sören Auer auer@tib.eu 
Jennifer D'souza jennifer.dsouza@tib.eu 
Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>Allard Oelen allard.oelen@tib.eu 
Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization
10 Sep 2024B3C4AC43B1BFBB1A2877E5B1363E199EarXiv:2409.06433v1[cs.DL]
The increasing amount of published scholarly articles, exceeding 2.5 million yearly, raises the challenge for researchers in following scientific progress.Integrating the contributions from scholarly articles into a novel type of cognitive knowledge graph (CKG) will be a crucial element for accessing and organizing scholarly knowledge, surpassing the insights provided by titles and abstracts.This research focuses on effectively conveying structured scholarly knowledge by utilizing large language models (LLMs) to categorize scholarly articles and describe their contributions in a structured and comparable manner.While previous studies explored language models within specific research domains, the extensive domainindependent knowledge captured by LLMs offers a substantial opportunity for generating structured contribution descriptions as CKGs.Additionally, LLMs offer customizable pathways through prompt engineering or fine-tuning, thus facilitating to leveraging of smaller LLMs known for their efficiency, cost-effectiveness, and environmental considerations.Our methodology involves harnessing LLM knowledge, and complementing it with domain expert-verified scholarly data sourced from a CKG.This strategic fusion significantly enhances LLM performance, especially in tasks like scholarly article categorization and predicate recommendation.Our method involves fine-tuning LLMs with CKG knowledge and additionally injecting knowledge from a CKG with a novel prompting technique significantly increasing the accuracy of scholarly knowledge extraction.We integrated our approach in the Open Research Knowledge Graph (ORKG), thus enabling precise access to organized scholarly knowledge, crucially benefiting domain-independent scholarly knowledge exchange and dissemination among policymakers, industrial practitioners, and the general public.</p>
<p>Introduction</p>
<p>In recent years, we saw a steeply rising popularity of Large Language Models (LLMs) for a variety of applications in natural language understanding and generation, content creation, programming assistance, translation, etc.However, for many applications, also a number of challenges with respect to the application of LLMs became apparent.Besides potential bias and intransparency these include in particular: (a) the limited context information that can be exploited by the LLM, (b) confabulation, where the LLM generates information or narratives that are plausible-sounding but factually incorrect or misleading, and (c) difficulty in handling specific highly specialized or niche domains where training data is limited or the language used is very specific or technical.</p>
<p>These issues are particularly pressing for applications in scholarly communication, i.e., the representation, organization, exchange, and usage of scholarly knowledge.Traditionally, scholarly knowledge is represented primarily in scientific articles of which several hundred million are already available and approx.2.5 million are furthermore added every year.While LLMs can and are being trained with scientific articles [Jungherr, 2023;Lee et al., 2023] scholarly communication is inherently complex, involving intricate processes and specialized knowledge.LLMs struggle to capture the nuances and depth required in academic discourse [Asher et al., 2023].</p>
<p>In this work, we address the context and specialized knowledge issues of LLMs with a Neuro-Symbolic approach intertwining two complementary methods -(1) fine-tuning LLMs with background knowledge obtained from knowledge graphs and (2) injecting query-specific context knowledge into the prompt.For both strategies, we leverage a novel type of contextualized knowledge graphs -cognitive knowledge graphs, which organize knowledge not only as entities and relationships but also in small reusable cognitive units.A cognitive knowledge graph is a knowledge graph equipped with an overlay structure, which determines reusable patterns (so-called graphlets), that represent common cognitive information structures, such as research contributions comprising entities such as the tacked research problem, the approach, the evaluation, etc.</p>
<p>We leverage cognitive knowledge graphs for (a) fine-tuning existing base models with scholarly knowledge obtained from a cognitive knowledge graph and (b) injecting contextual knowledge from the CKG into the prompt in order to exploit additional context during inferencing.</p>
<p>We evaluate our approach with a comprehensive set of experiments with four different LLMs, viz.Llama 2 (7B and 13B model variants), Mistral (7B), and finally Gemini Pro.Our subsequent model evaluations follow a two-fold methodology: 1) automatic evaluations using GPT as an evaluator, and 2) manual evaluations using a human expert evaluator.We observe that tasks relying on sparse background knowledge such as from the scholarly domain significantly benefit from injecting contextual knowledge from a CKG into the prompt, especially the research field prediction task.</p>
<p>In summary, the contributions of this work comprise:</p>
<ol>
<li>the definition of the notion of cognitive knowledge graphs, which are capable of capturing contextual knowledge to bridge between neural and symbolic processing as well as human curation, 2. the conceptualization and implementation of a method for injecting contextual knowledge into prompting as well as fine-tuning, 3. a comprehensive empirical evaluation of the method with four different LLM with human and LLM assessment.</li>
</ol>
<p>Related Work</p>
<p>The related work is roughly categorized along the dimensions of Knowledge Graphs, Prompt Engineering for LLMs, and fine-tuning LLMs for specific tasks.</p>
<p>Knowledge Graphs</p>
<p>Knowledge graphs primarily focus on representing explicit facts and relationships between entities, often lacking the depth to capture complex, abstract concepts, or contextual aspects [Choudhary and Reddy, 2023].They have limitations in representing evolving or multifaceted domains due to challenges in dynamic information updates and integrating multidisciplinary knowledge.These limitations reduce the effectiveness especially for capturing the evolving nature of real-world systems and handling complex information domains, such as scholarly communication.Knowledge graphs are mostly universal and stationary distributed, which does not consider the dynamic nature of many realworld domains [Wang et al., 2023].Additionally, knowledge graphs face difficulties in updating information in real-time, which is required as domains evolve over time [Hou et al., 2023].Furthermore, integrating multidisciplinary knowledge into knowledge graphs is challenging, limiting their ability to represent diverse domains effectively [Peng et al., 2023].</p>
<p>With CKG, we address knowledge graph limitations with dynamic updates and multidisciplinary integration and also enhance their effectiveness in representing evolving or multifaceted domains.</p>
<p>Traditional Prompt Engineering: Effective But Limited</p>
<p>Traditional prompt engineering methods, such as Zero-Shot [Wei et al., 2021], Few-Shot [Brown et al., 2020],</p>
<p>Chain-of-Thought (CoT) [Wei et al., 2022], Tree-of-Thoughts (ToT) [Yao et al., 2023]  To address these issues within the scholarly domain, we propose a novel knowledge-driven prompt engineering approach.This approach injects query-specific context knowledge from a domain-specific CKG directly into the prompts.This not only bridges the knowledge capture gap but also alleviates the need for readily available domain experts and reduces the time required for crafting effective prompts.</p>
<p>Fine</p>
<p>Method</p>
<p>Our method comprises three core elements: the novel notion of cognitive knowledge graphs (CKGs), injecting queryspecific context knowledge from CKGs into the prompts, and finally fine-tuning LLMs with CKG knowledge.To illustrate our method, let us consider a scholarly article titled "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" [Raffel et al., 2020], for which we aim to perform the following two tasks:</p>
<p>(1) predicting the research field and (2) recommending a list of predicates.In this scenario, the CKG Open Research Knowledge Graph (ORKG) [Jaradeh et al., 2019] holds metadata and a CKG graphlet for this scholarly ar-  Exploring the Limits of...</p>
<p>Benchmark CNN</p>
<p>Machine learning T5</p>
<p>Rouge-1 Cnn daily mail  ticle1 .A visual representation of this specific graphlet is depicted in Figure 2. The graphlet is essentially a set of typed entities connected through properties such as hasTitle, followsMethodology, usesDataset, hasContribution, belongsToResearchField etc.Using ORKG as a CKG, we retrieve this graphlet via SPARQL.</p>
<p>Simultaneously, we improve the prompts by incorporating real-world context, leveraging abstracts from the CORE dataset.This enriches the LLM understanding of scholarly articles and their properties.For prompt engineering in this scholarly article, we use the existing CoT prompt framework, injecting it with the extracted query-specific context knowledge for the research field prediction task.We include the title, abstract, and research field hierarchy to guide the LLM.Additionally, we examine the impact of task-aware prefixes, such as "Research field prediction", to monitor the LLM for specific tasks.Integrating CKG graphlets into prompts enhances the LLM's performance for certain cases without needing additional fine-tuning.For instance, using the pretrained Llama 2 13b, this scholarly article can accurately be assigned to the research field of "machine learning".Furthermore, our methodology involves fine-tuning to optimize the LLM's performance, especially in scenarios where prompts lack query-and domain-specific knowledge for a specific task.In the case of this scholarly article, without fine-tuning, it is not possible to determine the most suitable domainspecific properties for describing the research contribution, such as usesTrainingCorpus, usesTokenization, hasNumberofParameters etc.Through CKG-based fine-tuning, like ORKG Llama 2 13b, the LLM can extract both domain-specific and domain-independent properties, including followsMethodology, usesDataset, hasContribution, usesTrainingCorpus, usesTokenization, hasNumberofParameters, hasModelFamily, hasLicense etc.This integrated approach demonstrates the effectiveness of our methodology in improving the LLM's performance across various scholarly tasks.</p>
<p>Cognitive Knowledge Graphs</p>
<p>Traditional knowledge graphs primarily focus on representing explicit facts and relationships between entities, often lacking the depth to capture complex, abstract concepts or contextual nuances.Additionally, they struggle with dynamic information updates and integrating multidisciplinary knowledge, limiting their effectiveness in representing evolving or multifaceted domains, such as scholarly communication.</p>
<p>Our concept of a Cognitive Knowledge Graph now builds upon traditional knowledge graphs by integrating an overlay structure that identifies and utilizes reusable patterns, referred to as graphlets.These patterns represent common cognitive information structures.The motivation for the development and use of CKGs in addition to traditional knowledge graphs is multifaceted.Among others, we aim for enhanced representation of complex concepts like research methodologies, arguments, or narratives.CKGs, with their overlay structures, can better represent these multifaceted and complex concepts.In particular, we envision CKGs to capture and represent the context in which information exists thus facilitating the moderation between machine and human intelligence.This is particularly important in domains like research, where under-standing the context (e.g., the problem addressed, methodologies used, and the nature of the conclusions) is crucial for accurate interpretation.</p>
<p>The base constituents of cognitive knowledge graphs are more complex fabrics of entity descriptions arranged according to certain patterns -graphlets.In network analysis and graph theory, the notions graphlet [Pržulj et al., 2004;Pržulj, 2007] and motif [Milo et al., 2002] were introduced to provide a structuring element between whole graphs and individual nodes and edges.Hence, in order to be able to effectively represent and manage more complex knowledge artefacts, we translate and apply the notion of graphlets to knowledge graphs.</p>
<p>Formally, a CKG graphlet is a tuple of sets of types (classes) and roles (properties) (C, P ), where:</p>
<ol>
<li>for each role p ∈ P , the domain (either explicitly defined or implicitly inferred from a concrete CKG) includes at least one of the types c ∈ C: domain(p) ⊂ C and 2. all types c ∈ C are connected via a property chain in P :
∀c 1 , c 2 ∈ C, ∃p 1 , ..., pj, ..., p n ∈ P : domain(p 1 ) = c 1 ∧range(p n ) = c 2 ∧ n−1 i=1 range(p i ) = domain(p i+1 )
Alternatively, we can also view CKG graphlets as (a) a special type of connected graph patterns (according to the SPARQL algebra), where variables occur in the positions of concrete instances and literals, or (b) as specific sets of SHACL shapes.</li>
</ol>
<p>Injecting Query-specific Context Knowledge into the Prompts</p>
<p>Our method is based on injecting query-specific context knowledge alongside task-specific context into LLMs prompts to empower them for tasks with sparse knowledge in the foundational models.We exploit the unique capabilities of CKGs, which represent knowledge not just as isolated facts but as interconnected patterns called graphlets.These graphlets can capture more complex concepts and nuanced relationships, providing a richer scaffolding for the LLM's reasoning process.Our rationale is to inject query-specific contexts and verified task-driven knowledge with a structured hierarchy from a CKG, such that when both are incorporated into the prompts, the LLMs can achieve increased performance on tasks with sparse prior knowledge or limited training without requiring further fine-tuning.</p>
<p>For this experiment, we utilized a multi-stage pipeline (cf. Figure 1) to inject query-specific context knowledge into the prompts:</p>
<ol>
<li>
<p>CKG Knowledge Extraction: We tap into the vast knowledge reservoir of a CKG, extracting relevant graphlets that align with the specific scholarly task.This goes beyond simple fact retrieval, capturing the intricate cognitive structures and relationships within the domain.reasoning, is enriched with the extracted CKG graphlets.This new step aims to craft prompts that not only guide the LLM's thought process but also equip it with the precise domain-specific knowledge needed for accurate and insightful responses.</p>
</li>
<li>
<p>Targeted Knowledge Retrieval: Precise SPARQL queries act as filters, sifting through the vast CKG and extracting only the most relevant information pertinent to each specific prompt.This ensures that the injected knowledge directly aligns with the task.</p>
</li>
</ol>
<p>Task-Aware Prefixes:</p>
<p>To explore the impact of task identification, we utilized optional prefixes attached to the knowledge-driven prompts.These prefixes explicitly signal the scholarly domain and task nature, thus potentially further enhancing the LLM's ability to focus its reasoning and deliver even more refined responses.</p>
<p>By systematically constructing this knowledge-driven pipeline, we aim to improve the performance of LLMs for complex tasks with sparse domain knowledge.Injecting query-specific context enriched with structured knowledge from CKGs empowers LLMs to achieve increased performance without the need for additional fine-tuning.This approach harnesses the power of LLMs while equipping them with the precise domain-specific knowledge they need to excel in challenging domains with sparse domain knowledge such as scholarly communication.</p>
<p>LLM Fine-tuning</p>
<p>In order to increase the LLM performance, we intertwine prompt knowledge injection and fine-tuning leveraging CKGs, specifically in scenarios where the LLM lacks domain-specific knowledge for a task.Enriching prompts with query-specific context is powerful, but for domainspecific tasks, injecting additional domain-specific knowledge through fine-tuning might further increase performance.This process not only supplements the LLM knowledge but integrates its interaction, expanding its capabilities, especially for domain-specific tasks.When faced with domain knowledge gaps, targeted fine-tuning with external domainspecific knowledge proves effective in unlocking the full potential of the LLM.Domain-specific knowledge from a knowledge base like CKG, verified by domain experts, equips the LLM for navigating domain-driven challenges.The LoRA [Hu et al., 2021] attention mechanism efficiently handles intricate details in domain-specific knowledge, ensuring focused involvement.To optimize the resource constraints, quantization [Xu et al., 2023] is utilized, and the TRL trainer [von Werra et al., 2020] orchestrates the fine-tuning process by setting optimal parameters to overcome learning challenges.</p>
<p>Experimental Setup</p>
<p>Dataset.Our dataset originates from a large-scale CKG, the Open Research Knowledge Graph (ORKG), encompassing diverse research fields and a comprehensive list of predicates.To enhance query-specific content injection and facilitate fine-tuning, we utilized 1,894 and 1,740 graphlets related to the research fields and a list of predicates (Table 1), respectively.This dataset creates a knowledge-rich environment for the LLMs, with a comprehensive understanding of various academic fields.Additionally, we incorporated 100 graphlets in the test set, each carefully curated to showcase a list of research fields and list of predicates.These graphlets encapsulate the essence of their respective fields, emphasizing key concepts with a list of predicates.The curated CKG graphlets dataset is employed to unveil the true potential of LLMs when faced with intricate scholarly tasks.The choice of 100 graphlets for the test set serves multiple purposes.It not only enables us to assess the LLM's performance through automated-generated metrics but also facilitates a rigorous human validation process [Shen et al., 2023] [Chew et al., 2023].By comparing the LLM's generated responses with the judgments of human evaluators, we gain a deeper understanding of its strengths and weaknesses.This cross-validation approach, integrating AI and human evaluation, ensures a comprehensive and granular assessment of the LLM's capabilities, ultimately paving the way for its further development and refinement.</p>
<p>Evaluation criteria The evaluation criteria encompass two primary dimensions: (1) LLM-based evaluation [Liusie et al., 2023] [Zhang et al., 2023] and (2) human-based evaluation [Wu andAji, 2023] [Shen et al., 2023].In LLM-based evaluation, the research field prediction is assessed based on clarity, coverage, relevance to the research field predicted by a domain expert, and granularity of the specified LLMgenerated research field, each scored within a range of 0-3.Similarly, the LLM's list of predicate recommendations is evaluated considering clarity, coverage, relevance for domain expert recommended contexts for a scholarly article, granularity, and the LLM's ability to recognize all contexts, each scored within the same 0-3 scale.Human-based evaluation mirrors LLM-based criteria, ensuring a parallel human assessment of clarity, coverage, relevance, and granularity in both research field categorization and a list of predicate recommendations.This approach aims to comprehensively evaluate the LLM's performance, combining automated metrics with human judgment to provide valuable insights and an in-depth understanding of its strengths and weaknesses in scholarly-related tasks.Tasks for Tuning This research explores two different approaches for injecting query-specific knowledge into prompts: task-independent and task-driven variants.By employing these two distinct prompt variants in our experiment, we assess the strengths and weaknesses associated with task-independent and task-driven approaches, analyzing how LLMs handle different graphlet structures.</p>
<p>• Task-Independent Prompts: We extract query-specific context from the CKG without imposing any taskrelated constraints.LLMs equipped with general knowledge can readily address diverse tasks.Additionally, the LLM undergoes fine-tuning with this general CKG knowledge, further enhancing its understanding of underlying concepts and relationships.• Task-Driven Prompts: In the second approach, we harness the power of CKGs to formulate task-specific prompts, explicitly incorporating task-oriented prefixes that guide the LLM's reasoning toward the desired outcome.For instance, in a research field prediction task, the prompt might commence with "Research field prediction" along with all the pertinent CKG-derived context.This targeted approach aims to improve the LLM's precision in applying its knowledge to the specific task.</p>
<p>Evaluation</p>
<p>We performed a comparative analysis of different prompt engineering approaches (Table 2) and LLM performance for both pre-trained (Table 3a) and fine-tuned (Table 3b) models across various tasks, which helps to clarify the capabilities and limitations of these LLMs regarding the CKG knowledge injection approach.The comparison of our Injection of Query-specific Context Knowledge (IQCK) method with four baseline methods (for a sample set of the CKG graphlets) for the research field prediction task (RF) with Mistral 7b is shown in Table 2.For the baseline methods, we tried different prompting styles but omitted the injection of a research field hierarchy to choose predictions from.We can observe, that IQCK with the injection of the research field hierarchies into the prompt as contextual knowledge significantly improves the accuracy of the research field prediction to 76% from the 67-73% achieved with the baseline methods.</p>
<p>The evaluation of pre-trained LLMs (Table 3a), including Gemini pro (without and with Prefix) and Mistral 7b (without and with Prefix) shows impressive mean scores, signifying their capabilities in capturing and leveraging queryspecific context knowledge from within the prompts.Specifically, Gemini pro (with Prefix) achieved a remarkable mean score of 80% (GPT4 evaluation), positioning it as a top performer in this task.Mistral 7b (without Prefix) follows closely with a substantial mean score of 79% (GPT4 evaluation) and 60% (human evaluation), demonstrating robust  competency.The models Llama 2 13b (without and with Prefix) and Llama 2 7b (without and with Prefix) achieve lower mean average scores than others by the GPT4 evaluation and are similarly scored by human evaluation.This pattern indicates that the incorporation of injecting queryspecific context knowledge during the prompt engineering phase provides these LLMs with a solid foundation for understanding and recommending research fields.</p>
<p>The evaluation of fine-tuned LLMs (Table 3b) in the research field prediction reveals different dynamics.For instance, ORKG Llama 2 13b (With Prefix) achieves a mean score of 49% (GPT4 evaluation), showing competitive performance with regard to other fine-tuned LLMs but falling short from the previous approach.Similarly, ORKG Llama 13b (Without Prefix) and ORKG Mistral 7B (With Prefix) achieve mean scores of 42% (GPT4 evaluation) and 46% (GPT4 evaluation), respectively, suggesting a noteworthy but comparatively lower proficiency.This variance scores between pre-trained and fine-tuned LLMs underscores the substantial impact of injecting query-specific context knowledge in the pre-train LLMs for research field prediction tasks.</p>
<p>For the list of predicate recommendations, Llama 2 13b (with Prefix) achieved a mean score of 65% (GPT4 evaluation) and 60% (human evaluation), emphasizing its proficiency in suggesting predicate labels.Llama 2 7b (With Prefix) follows closely with a mean score of 63% (GPT4 evaluation), while Llama 2 13b (With Prefix) and Mistral 7b (With Prefix) achieve mean scores of 63% (GPT4 evaluation) and 64% (GPT4 evaluation) which is also quite similar to a human evaluation, respectively.</p>
<p>Regarding the fine-tuned LLMs in the list of predicates recommendation unveils a more diverse scenario.ORKG Llama 2 13b (Without Prefix) achieves 67% of the mean average score, suggesting that fine-tuning can yield competitive results in this task.However, ORKG Llama 13b (With Prefix) and ORKG Mistral 7B (With Prefix) experience lower mean scores, with 57% and 52%, respectively.This variability in performance indicates that the effectiveness of fine-tuned models in predicate label recommendation is more contingent on specific training conditions, and adding a prefix seems to introduce additional complexity.</p>
<p>This comprehensive analysis establishes a solid foundation for LLMs, particularly in the scholarly domain for tasks like research field prediction and list of predicate recommendations.The injection of query-specific context knowledge into the prompts from CKG improves the LLM performance and also provides a robust understanding of complex relationship prompts and LLMs.Fine-tuned LLMs showcase competitive performance, especially for list of predicate recommendations, highlighting the relation between the LLMs and finetuned requirements.</p>
<p>Limitations and Future Work</p>
<p>In order to evaluate our approach, we utilized a domain expert to verify data using the previously described approach.</p>
<p>Although the domain expert verified data contains human curated data, it can be misleading to determine to performance of our models.For the research field prediction task, the domain expert verified data only includes a single field per article.In the case of interdisciplinary fields, still, only a single field is contained.Consequently, the LLM recommendation can be correct based on the contents of the title and abstract but is evaluated as incorrect because of the domain expertverified data.For the list of predicate recommendations, the domain expert verified data only includes a subset of relevant predicates, since knowledge descriptions in the CKG are generally incomplete.Therefore, an LLM-recommended list of properties might be relevant based on the title and abstract, but evaluated as incorrect when comparing them to the domain expert-verified data.</p>
<p>Furthermore, there are several limitations regarding the human evaluation of the data.The evaluators were instructed on how to score the outcomes, but only high-level guidelines were provided.Therefore, scores might vary between different articles.Despite this shortcoming, the human evaluation still provides valuable insights, especially in comparison with the machine evaluation.Another aspect of the human evaluation is the heterogeneous set of research domains.Since our approach does not focus on specific domains, the set of selected articles comes from a variety of domains.The human evaluators were not experts in all domains and therefore had to judge the results to the best of their ability.However, we believe that a high-level assessment of the resulting LLM responses is also possible without domain knowledge, albeit in lesser granularity.</p>
<p>In addition to the two tasks performed in our approach, we plan to extend this to various other tasks in future work.In addition to the prediction of predicates, we explored the prediction of objects to form complete contribution description facts.However, further work is required to perform this task at the desired accuracy.In the future, we also plan to continue working on this specific task by leveraging other approaches, such as fine-tuning the LLMs for each specific task and increasing the data from the CKG which needs to be collected from the domain experts.</p>
<p>Conclusion</p>
<p>This work is part of a larger research agenda aiming at creating a comprehensive neuro-symbolic system for describing research contributions in a cognitive knowledge graph ultimately giving rise to novel AI-based research assistance systems, which enable researchers to obtain comprehensive answers to research questions based on the recent corpus of scientific knowledge.Due to the limitations of LLMs in extracting information in domains with sparse training data, we developed two methods for (1) injecting contextual information into prompts from a preexisting CKG and (2) fine-tuning LLMs with such knowledge for specific tasks.Our evaluation showed, that in particular the first method is very well suited to improve the accuracy of LLMs for scholarly communication tasks.More research is required to further improve the fine-tuning methods and expand the evaluation of the approach to further knowledge extraction and augmentation tasks, such as object prediction.</p>
<p>Figure 1 :
1
Figure 1: Overview on the method for knowledge augmentation and discovery comprising context knowledge prompt injection and finetuning leveraging a Cognitive Knowledge Graph.</p>
<p>Figure 2 :
2
Figure 2: Example of a graphlet retrieved from the ORKG, displaying a scholarly article, the metadata, and the respective properties and entities (simplified version).</p>
<p>Table 1 :
1
2. Contextual Grounding: We leverage abstracts from the CORE dataset [Knoth et al., 2023] as factual summaries, providing rich contextual grounding for the subsequent prompt construction.3. Knowledge-Infused CoT Prompts: The existing CoT prompt framework, a proven way to guide the LLM's Evaluation dataset comprising research field annotations and predicates for research contribution descriptions.
Train Data Test DataResearch Field1,894100List of Predicates1,740100Prompt StyleMAS (RF)Zero-Shot Prompting67%Few-Shot Prompting68%Chain-of-Thought Prompting69%Zero-shot COT Prompting73%IQCK into COT Prompting76%</p>
<p>Table 2 :
2
Comparison of our Injecting Query-specific Context Knowledge (IQCK) method with four baseline methods (for a sample set of the CKG graphlets) for the research field prediction (RF) Mistral 7b.CoT shares the same input as Few-Shot, with the only difference being the inclusion of additional examples; MAS = Mean Average Score.</p>
<p>Table 3 :
3
AI (gpt-4-1106-preview)Perspective on LLM (Pre-trained and Fine-tuned) Evaluation; MAS = Mean Average Score; LP = List of predicates recommendation, RF = Research field prediction.(a)Table2.1:AIPerspective on LLM (Pre-trained) Evaluation.Table2.2:AI Perspective on LLM (Fine-tuned) Evaluation.
TaskPrefixLLMMAS(b) TaskPrefixLLMMASGemini pro64%ORKG Llama 13b 67%LPWithout PrefixLlama 2 13b 64% Mistral 7b 64%LPWithout PrefixORKG Llama 2 7b 42% ORKG Mistral 7B 42%Llama 2 7b59%ORKG Llama 13b 57%Llama 2 13b 65%LPWith PrefixORKG Mistral 7B52%LPWith PrefixMistral 7b Llama 2 7b Gemini pro64% 63% 62%RFWithout PrefixORKG Llama 2 7b 37% ORKG Llama 2 7b 42% ORKG Mistral 7B 38%Mistral 7b79%ORKG Llama 13b13%RFWithout PrefixGemini pro Llama 2 13b 62% 78% Llama 2 7b 50%RFWith PrefixORKG Llama 13b 49% ORKG Mistral 7B 46% ORKG Llama 2 7b 19%Gemini pro80%RFWith PrefixMistral 7b Llama 2 13b 61% 77%Llama 2 7b43%</p>
<p>Table 4 :
4
Human Perspective on LLM (Pre-trained and Fine-tune) Evaluation; MAS = Mean Average Score; LP = List of predicates recommendation, RF = Research field prediction.(a)Table3.1:Human Perspective on LLM (Pre-trained) Evaluation.
(b) Table 3.2: Human Perspective on LLM (Fine-trained) Evaluation.TaskPrefixLLMMASTaskPrefixLLMMASLlama 2 13b 60%ORKG Mistral 7B 53%LPWith PrefixMistral 7b Llama 2 7b60% 59%LPWith PrefixORKG Llama 13b ORKG Llama 2 7b 45% 50%Gemini pro57%ORKG Llama 13b 60%Llama 2 13b 60%LPWithout PrefixORKG Llama 2 7b 47%LPWithout PrefixLlama 2 7b Mistral 7b Gemini pro59% 59% 58%RFWith PrefixORKG Mistral 7B ORKG Llama 13b 50% 46% ORKG Mistral 7B 48%Gemini pro78%ORKG Llama 2 7b 33%RFWith PrefixMistral 7b Llama 2 13b 70% 78% Llama 2 7b 49%RFWithout PrefixORKG Mistral 7B 55% ORKG Llama 2 7b 41% ORKG Llama 13b 18%Mistral 7b82%RFWithout PrefixGemini pro Llama 2 13b 66% 75%Llama 2 7b56%
https://orkg.org/paper/R161808</p>
<p>Limits for learning with language models. Asher, arXiv:2306.122132023. 2023. 2020. 202033Tom Brown, Benjamin Mann, Nick Ry-arXiv preprintLanguage models are few-shot learners. Advances in neural information processing systems</p>
<p>Llm-assisted content analysis: Using large language models to support deductive coding. Chew, arXiv:2306.14924arXiv:2305.01157Complex logical reasoning over knowledge graphs using large language models. 2023. 2023. 2023arXiv preprintChoudhary and Reddy, 2023</p>
<p>From answers to insights: Unveiling the strengths and limitations of chatgpt and biomedical knowledge graphs. medRxiv. D' Souza, arXiv:2305.13264arXiv:2304.01933Proceedings of the 10th International Conference on Knowledge Capture. Mohamad Yaser, Jaradeh , Allard Oelen, Kheir Eddine Farfar, Manuel Prinz, D' Jennifer, Gábor Souza, Markus Kismihók, Sören Stocker, Auer, the 10th International Conference on Knowledge CaptureSpringer2023. 2023. 2023. 2023. 2023. 2021. 2021. 2023. 2019. 2019arXiv preprintInternational Conference on Database and Expert Systems Applications</p>
<p>Prefix propagation: Parameterefficient tuning for long sequences. Knoth Jungherr, arXiv:2305.12086arXiv:2307.07889Adian Liusie, Potsawee Manakul, and Mark JF Gales. Zero-shot nlg evaluation through pairware comparisons with llms. Feng Xia, Mehdi Naseriparsa, Francesco Osborne, Ciyuan Peng2023. 2023. 2023. June 2023. 2023. 2023. 2023. 2023. 2023. 2023. 2002. 2002. 2023. 2023. 200410arXiv preprintNature Scientific Data. Pržulj et al., 2004] N. Pržulj, D. G. Corneil, and I. Jurisica. Modeling interactome: scale-free or geometric? Bioinformatics</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Nataša Pržulj, ; Pržulj, Raffel, The Journal of Machine Learning Research. 2322007. 01 2007. 2020. 2020Bioinformatics</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Shen, arXiv:2305.13091arXiv:2309.14717Minghao Wu and Alham Fikri Aji. Style over substance: Evaluation biases for large language models. Wu and Aji2023. 2023. 2020. 2020. 2023. 2023. 2021. 2021. 2022. 2022. 2023. 2023. 202335arXiv preprintQa-lora: Quantization-aware low-rank adaptation of large language models</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Yao, arXiv:2305.10601arXiv:2307.07705Zhengyan Zhang, and Maosong Sun. Cpet: Effective parameter-efficient tuning for compressed large language models. 2023. 2023. 2023. 2023. 20232arXiv preprintProceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V</p>            </div>
        </div>

    </div>
</body>
</html>
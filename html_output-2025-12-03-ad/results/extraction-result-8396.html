<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8396 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8396</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8396</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-55c562c0de9d011c91965a34ba784c9d4b72fecb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/55c562c0de9d011c91965a34ba784c9d4b72fecb" target="_blank">Linearity of Relation Decoding in Transformer Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work identifies many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations.</p>
                <p><strong>Paper Abstract:</strong> Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8396.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8396.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LRE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear Relational Embedding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An affine linear mapping (o ≈ W_r s + b_r, with a scaling factor β) that maps enriched subject hidden representations to object hidden representations in transformer LMs; estimated from the model Jacobian and used for decoding, probing, and causal editing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J, GPT-2-XL, LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LMs evaluated in the paper: GPT-J (6B), GPT-2-XL (1.5B), LLaMA-13B (13B). Experiments use hidden states at various intermediate layers and decoder head D to map hidden states to token logits.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>not an arithmetic task; vector-space affine mapping (analogy to vector arithmetic / embedding arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Affine linear operator per relation: LRE(s)=β W_r s + b_r, where W_r is mean Jacobian of F(s,c) (∂F/∂s) across examples and b_r is bias term; subject representations s are enriched by intermediate MLP/attention processing before mapping; layer normalization causes scale masking requiring β correction.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Jacobian-based estimation of W and b from n few-shot examples (n=8); attribute lens (decoding D(LRE(h)) at each layer) for probing; causality tested via inverse-edit interventions using a low-rank pseudoinverse W_r^† (activation patching of single subject token position).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Faithfulness: average ~0.59 (59%) across relations for GPT-J with tuned β=2.25; causality average ~0.81 (81%) under tuned hyperparameters (β, layer ℓ_r, rank ρ_r) for GPT-J (Table 6). 48% of the 47 relations tested had robust LREs recovering subject-object mappings for majority of subjects; for many relations LRE achieves >60% faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Underestimation of magnitude by raw mean Jacobian W (remedied by scalar β>1); some relations are not linearly encoded (e.g., Company CEO: faithfulness ≤6% despite LM predicting CEOs correctly), likely due to very large output range (names/companies) or nonlinear, multi-layer retrieval; layer-wise mode-switch (later layers erase relational embedding); activation-patching leakage due to attention from earlier states reduces intervention efficacy; full inverse W^{-1} ill-conditioned — small singular values introduce noise, motivating low-rank pseudoinverse.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical estimation: mean Jacobian ∂F/∂s computed at example points; first-order Taylor approx yields W and b; faithfulness metric compares argmax D(F(s,c)) and argmax D(LRE(s)); causality tested by using W^† to compute Δs that should change LM output to o' — interventions perform similarly to an oracle replacement of s with s' in many relations; high correlation between faithfulness and causality when hyperparameters selected to maximize causality; comparisons to baselines (Logit Lens identity, Translation s+b, linear regression trained on same samples) show LRE outperforms them.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Multiple relations where LRE fails though LM output is correct (e.g., company CEO, some person-name ranges); rank selection matters (causality increases up to a point then decreases when noisy singular values are included); β must be tuned per model; interventions patching a single token state can suffer from leakage from earlier layers; LRE captures direction but initial W underestimates magnitude (requires β).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linearity of Relation Decoding in Transformer Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8396.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8396.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Translation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Translation(s) = s + b baseline (embedding translation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple affine baseline that models decoding as a translation in representation space by estimating b = E[o - s] and predicting o via s + b, inspired by classical embedding arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J, GPT-2-XL, LLaMA-13B (used as a baseline across these models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same autoregressive transformer models as above; baseline decodes intermediate subject representation s by adding an average bias b (no learned projection W).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>vector-space translation (embedding arithmetic style), not explicit numeric arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Vector addition of a constant bias to the enriched subject vector: Translation(s)=s + b where b estimated as mean(o - s) over examples.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Estimate b from n examples (n=8) and decode via the LM decoder head D to compute faithfulness; compared against LRE and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Per-relation faithfulness substantially lower than LRE across relation categories; performance drop indicates the necessity of a projection term W in addition to bias.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Fails when the relation requires a projection rather than pure translation; low faithfulness shows translation-only cannot capture many relations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical comparison (Figure 4 and Figure 13) where Translation baseline underperforms LRE; highlights importance of learned linear projection.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Translation sometimes better than identity baseline but far worse than LRE; cannot explain relations requiring directional projection of representation space.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linearity of Relation Decoding in Transformer Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8396.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8396.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Word embedding arithmetic (Mikolov)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Word embedding arithmetic / vector analogy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classic observation that word2vec-style embeddings can exhibit linear relations (e.g., king - man + woman ≈ queen), cited as conceptual precedent for translation-style baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient estimation of word representations in vector space.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>This is a conceptual prior (word2vec embeddings), not an evaluated LM in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>analogy-style vector arithmetic (king - man + woman ≈ queen)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Linear vector arithmetic in embedding space (addition/subtraction of embedding vectors) representing lexical relations.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Referenced as inspiration for translation baseline; not directly probed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numeric arithmetic performance reported in this paper (this is prior work referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Not discussed in this paper; cited to motivate translation baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited prior work establishes linear analogies in word embeddings; paper uses this as motivation for translation-style baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>N/A in this paper (prior work subject to its own limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linearity of Relation Decoding in Transformer Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8396.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8396.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Merullo et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models implement simple word2vec-style vector arithmetic (Merullo et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recent work arguing that language models can implement simple vector arithmetic patterns; cited as inspiration for translation baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models implement simple word2vecstyle vector arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior work referenced, not the main focus of experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>word-embedding-style vector arithmetic, conceptual analogies</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Vector arithmetic in embedding or representation space (add/sub operations).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Cited as conceptual precedent for translation baseline; not directly used experimentally here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not provided in this paper; referenced as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Referenced to justify examining translation-style baselines; no new evidence in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>N/A within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linearity of Relation Decoding in Transformer Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8396.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8396.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logit Lens</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logit Lens (identity decoding baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probing method that decodes an intermediate hidden state h directly with the model decoder head D (i.e., identity mapping), used here as a baseline for linear decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>interpreting gpt: the logit lens</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J, GPT-2-XL, LLaMA-13B (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Application of Logit Lens: decode subject representation s directly to logits via decoder head D to estimate F(s,c) without additional projection.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>not arithmetic; direct decoding of hidden states</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Identity mapping h -> D(h) to obtain next-token distribution; contrasts with affine projection used by LRE.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Used as baseline for faithfulness comparisons against LRE and Translation baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Generally low faithfulness relative to LRE (Figure 4 and Figure 13); shows identity is insufficient for many relations.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Fails when LM requires projection of enriched subject representation to object space; low performance indicates need for W and b terms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical baseline comparisons show LRE outperforms Logit Lens identity decoding for relation decoding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Logit Lens sometimes captures next-token info but not relation-specific decoding that occurs after enrichment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linearity of Relation Decoding in Transformer Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8396.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8396.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>β scaling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Jacobian magnitude correction (β scaling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical scalar multiplier β (>1) applied to the mean Jacobian W to correct for underestimation of magnitude in the first-order linear approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J (β tuned = 2.25 reported), also applied to other models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to affine mapping: LRE(s) = β W_r s + b_r to better match actual change magnitudes in F(s,c).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>not arithmetic; scaling of linear operator</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Scalar amplification of projection W to match integrated Jacobian between subject vectors (empirically necessary due to layer norm and nonlinearity effects).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>β selected per LM via grid search to maximize correlation between faithfulness and causality; causality measured with β=1 in edits but β used for faithfulness estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>For GPT-J, β=2.25 yields faithfulness_mu ~0.59 and causality_mu ~0.81 (Table 6); varying β affects faithfulness metrics (Table 6 shows monotonic rise then plateau).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Without β, W underestimates magnitude of F changes; β is heuristic scalar and chosen per-LM (not per-relation), so imperfect; interventions for causality calculated using β=1 (edits magnified separately).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Measured underestimation ratio via integrating Jacobian along subject-to-subject paths (Appendix C, Table 5) showing W underestimates actual change by factors >2-4 for some relations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>β is a crude global fix; some relations still require nonlinear handling or layer-specific adjustments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linearity of Relation Decoding in Transformer Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8396.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8396.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Low-rank pseudoinverse editing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-rank pseudoinverse (W^†) based activation editing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Causal intervention method that computes Δs = W^† (o' - o) (optionally scaled) to edit subject representation s so the LM outputs a target object o'; uses a low-rank inverse to avoid amplifying noise from small singular values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J, GPT-2-XL, LLaMA-13B (evaluated primarily on GPT-J)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Intervention on a single-layer, single-position hidden state (subject's last token position) and then run forward to measure change in predicted first token of object.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>vector-space editing (not numeric arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Compute SVD of W = U Σ V^T, take only top-ρ singular values to form Σ^†_ρ, then W^† = V Σ^†_ρ U^T; Δs = W^† (o' - o) applied to s (optionally scaled) to approximate s'.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Activation patching: replace s with s+Δs and evaluate next-token prediction; compare against oracle (inserting actual s') and baselines that insert o' or decoder-row embedding e_o' directly.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Causality performance matches oracle closely for many relations (Figure 5, Figure 14); average causality_mu for GPT-J ~0.81 (Table 6) when hyperparameters tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Full inverse W^{-1} is ill-conditioned — small singular values amplify noise and reduce edit efficacy; too-high rank reduces causality after an optimal rank (Figure 9); patching single state subject to leakage from earlier layers reduces efficacy at deeper layers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablations across rank (ρ) show causality improves up to an optimal low-rank then declines (Appendix D.2, Figure 9); alignment of causality curves with oracle replacement across layers supports W^† producing s' approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Optimal rank ρ_r is relation-specific and must be tuned; single-state patching limitation due to attention leakage; some relations remain non-linearly encoded and resist this edit.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linearity of Relation Decoding in Transformer Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8396.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8396.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attribute Lens</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attribute Lens (relation-specific probe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probing/visualization tool that decodes any hidden state h into an open-vocabulary object-token distribution by applying D(LRE(h)), enabling visualization of when and where a relation's object information is present in the network.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J (primary), GPT-2-XL, LLaMA-13B (also evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applies an estimated LRE for a specific relation r to hidden states across layers and decodes via the model decoder head into token logits/probabilities to visualize attribute predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>not arithmetic; representation-to-token decoding for attributes</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Uses relation-specific affine mapping LRE then decoder head D to produce token distribution; reveals latent knowledge even when LM output is distracted or wrong.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Attribute lens applied to datasets of adversarial 'repetition distracted' and 'instruction distracted' prompts to reveal latent correct facts in top-k predictions even when LM outputs incorrect tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Attribute lens reveals true fact in top-3 predictions for a majority of adversarial examples (tested on 11,891 RD and same number of ID prompts); exact numeric top-3 rates summarized in paper's Table 3 (not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>LM may still output falsehoods despite attribute lens showing correct internal representation; attribute lens depends on quality of LRE estimation (β, layer, rank).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Shows cases where LM is baited to output an incorrect object (e.g., 'England is Oslo') yet attribute lens applied to last mention of subject still ranks the true object (e.g., London) in top predictions, indicating latent internal representation of correct fact.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Attribute lens success depends on relation being linearly decodable; for relations lacking LRE, the lens does not reveal correct attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linearity of Relation Decoding in Transformer Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8396.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8396.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Models studied</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J; GPT-2-XL; LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Autoregressive transformer language models used for experiments: GPT-J (6B), GPT-2-XL (~1.5B), and LLaMA-13B (13B); used to evaluate LRE faithfulness and causality across 47 relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J; GPT-2-XL; LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-J (6B autoregressive transformer), GPT-2-XL (1.5B), LLaMA-13B (13B) — evaluated under same methodology; results show similar patterns of LRE faithfulness across models with correlations (GPT-J vs GPT2-xl R=0.85, GPT-J vs LLaMA R=0.71).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>not evaluated (no numeric arithmetic tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Intermediate subject representations s enriched across MLP and attention layers; for many relations, relation decoding approximated by linear affine mapping from s to last-layer output representation o.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Jacobian-based LRE estimation, attribute lens, activation patching edits with low-rank pseudoinverse, baselines (Logit Lens, Translation, linear regression), hyperparameter grid search for β, ℓ_r, ρ_r.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Relation-wise LRE faithfulness and causality reported per model; overall patterns consistent across models; specific numbers: GPT-J faithfulness_mu ~0.59 at β=2.25; high per-relation variance (Table 4 shows counts of LM-correct examples per relation).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Tokenizer issues: LLaMA tokenizer splits years by digits so some year-based relations excluded; model-size and tokenization affect which relations are linearly decodable; relation-specific failures as above.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cross-model comparisons (Figures 15-18) show consistent relationship patterns and correlations between faithfulness and causality metrics across models.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Differences across models for some relations; certain relations not suitable for LRE in any of the evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linearity of Relation Decoding in Transformer Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient estimation of word representations in vector space. <em>(Rating: 2)</em></li>
                <li>Learning distributed representations of concepts using linear relational embedding <em>(Rating: 2)</em></li>
                <li>interpreting gpt: the logit lens <em>(Rating: 2)</em></li>
                <li>Language models implement simple word2vecstyle vector arithmetic. <em>(Rating: 2)</em></li>
                <li>Transformer feed-forward layers are key-value memories. <em>(Rating: 1)</em></li>
                <li>Dissecting recall of factual associations in auto-regressive language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8396",
    "paper_id": "paper-55c562c0de9d011c91965a34ba784c9d4b72fecb",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "LRE",
            "name_full": "Linear Relational Embedding",
            "brief_description": "An affine linear mapping (o ≈ W_r s + b_r, with a scaling factor β) that maps enriched subject hidden representations to object hidden representations in transformer LMs; estimated from the model Jacobian and used for decoding, probing, and causal editing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J, GPT-2-XL, LLaMA-13B",
            "model_description": "Autoregressive transformer LMs evaluated in the paper: GPT-J (6B), GPT-2-XL (1.5B), LLaMA-13B (13B). Experiments use hidden states at various intermediate layers and decoder head D to map hidden states to token logits.",
            "arithmetic_task_type": "not an arithmetic task; vector-space affine mapping (analogy to vector arithmetic / embedding arithmetic)",
            "mechanism_or_representation": "Affine linear operator per relation: LRE(s)=β W_r s + b_r, where W_r is mean Jacobian of F(s,c) (∂F/∂s) across examples and b_r is bias term; subject representations s are enriched by intermediate MLP/attention processing before mapping; layer normalization causes scale masking requiring β correction.",
            "probing_or_intervention_method": "Jacobian-based estimation of W and b from n few-shot examples (n=8); attribute lens (decoding D(LRE(h)) at each layer) for probing; causality tested via inverse-edit interventions using a low-rank pseudoinverse W_r^† (activation patching of single subject token position).",
            "performance_metrics": "Faithfulness: average ~0.59 (59%) across relations for GPT-J with tuned β=2.25; causality average ~0.81 (81%) under tuned hyperparameters (β, layer ℓ_r, rank ρ_r) for GPT-J (Table 6). 48% of the 47 relations tested had robust LREs recovering subject-object mappings for majority of subjects; for many relations LRE achieves &gt;60% faithfulness.",
            "error_types_or_failure_modes": "Underestimation of magnitude by raw mean Jacobian W (remedied by scalar β&gt;1); some relations are not linearly encoded (e.g., Company CEO: faithfulness ≤6% despite LM predicting CEOs correctly), likely due to very large output range (names/companies) or nonlinear, multi-layer retrieval; layer-wise mode-switch (later layers erase relational embedding); activation-patching leakage due to attention from earlier states reduces intervention efficacy; full inverse W^{-1} ill-conditioned — small singular values introduce noise, motivating low-rank pseudoinverse.",
            "evidence_for_mechanism": "Empirical estimation: mean Jacobian ∂F/∂s computed at example points; first-order Taylor approx yields W and b; faithfulness metric compares argmax D(F(s,c)) and argmax D(LRE(s)); causality tested by using W^† to compute Δs that should change LM output to o' — interventions perform similarly to an oracle replacement of s with s' in many relations; high correlation between faithfulness and causality when hyperparameters selected to maximize causality; comparisons to baselines (Logit Lens identity, Translation s+b, linear regression trained on same samples) show LRE outperforms them.",
            "counterexamples_or_challenges": "Multiple relations where LRE fails though LM output is correct (e.g., company CEO, some person-name ranges); rank selection matters (causality increases up to a point then decreases when noisy singular values are included); β must be tuned per model; interventions patching a single token state can suffer from leakage from earlier layers; LRE captures direction but initial W underestimates magnitude (requires β).",
            "uuid": "e8396.0",
            "source_info": {
                "paper_title": "Linearity of Relation Decoding in Transformer Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Translation baseline",
            "name_full": "Translation(s) = s + b baseline (embedding translation)",
            "brief_description": "A simple affine baseline that models decoding as a translation in representation space by estimating b = E[o - s] and predicting o via s + b, inspired by classical embedding arithmetic.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J, GPT-2-XL, LLaMA-13B (used as a baseline across these models)",
            "model_description": "Same autoregressive transformer models as above; baseline decodes intermediate subject representation s by adding an average bias b (no learned projection W).",
            "arithmetic_task_type": "vector-space translation (embedding arithmetic style), not explicit numeric arithmetic",
            "mechanism_or_representation": "Vector addition of a constant bias to the enriched subject vector: Translation(s)=s + b where b estimated as mean(o - s) over examples.",
            "probing_or_intervention_method": "Estimate b from n examples (n=8) and decode via the LM decoder head D to compute faithfulness; compared against LRE and other baselines.",
            "performance_metrics": "Per-relation faithfulness substantially lower than LRE across relation categories; performance drop indicates the necessity of a projection term W in addition to bias.",
            "error_types_or_failure_modes": "Fails when the relation requires a projection rather than pure translation; low faithfulness shows translation-only cannot capture many relations.",
            "evidence_for_mechanism": "Empirical comparison (Figure 4 and Figure 13) where Translation baseline underperforms LRE; highlights importance of learned linear projection.",
            "counterexamples_or_challenges": "Translation sometimes better than identity baseline but far worse than LRE; cannot explain relations requiring directional projection of representation space.",
            "uuid": "e8396.1",
            "source_info": {
                "paper_title": "Linearity of Relation Decoding in Transformer Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Word embedding arithmetic (Mikolov)",
            "name_full": "Word embedding arithmetic / vector analogy",
            "brief_description": "Classic observation that word2vec-style embeddings can exhibit linear relations (e.g., king - man + woman ≈ queen), cited as conceptual precedent for translation-style baselines.",
            "citation_title": "Efficient estimation of word representations in vector space.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "This is a conceptual prior (word2vec embeddings), not an evaluated LM in this paper.",
            "arithmetic_task_type": "analogy-style vector arithmetic (king - man + woman ≈ queen)",
            "mechanism_or_representation": "Linear vector arithmetic in embedding space (addition/subtraction of embedding vectors) representing lexical relations.",
            "probing_or_intervention_method": "Referenced as inspiration for translation baseline; not directly probed here.",
            "performance_metrics": "No numeric arithmetic performance reported in this paper (this is prior work referenced).",
            "error_types_or_failure_modes": "Not discussed in this paper; cited to motivate translation baseline.",
            "evidence_for_mechanism": "Cited prior work establishes linear analogies in word embeddings; paper uses this as motivation for translation-style baseline.",
            "counterexamples_or_challenges": "N/A in this paper (prior work subject to its own limitations).",
            "uuid": "e8396.2",
            "source_info": {
                "paper_title": "Linearity of Relation Decoding in Transformer Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Merullo et al.",
            "name_full": "Language models implement simple word2vec-style vector arithmetic (Merullo et al.)",
            "brief_description": "Recent work arguing that language models can implement simple vector arithmetic patterns; cited as inspiration for translation baselines.",
            "citation_title": "Language models implement simple word2vecstyle vector arithmetic.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Prior work referenced, not the main focus of experiments in this paper.",
            "arithmetic_task_type": "word-embedding-style vector arithmetic, conceptual analogies",
            "mechanism_or_representation": "Vector arithmetic in embedding or representation space (add/sub operations).",
            "probing_or_intervention_method": "Cited as conceptual precedent for translation baseline; not directly used experimentally here.",
            "performance_metrics": "Not provided in this paper; referenced as related work.",
            "error_types_or_failure_modes": "Not discussed here.",
            "evidence_for_mechanism": "Referenced to justify examining translation-style baselines; no new evidence in this work.",
            "counterexamples_or_challenges": "N/A within this paper.",
            "uuid": "e8396.3",
            "source_info": {
                "paper_title": "Linearity of Relation Decoding in Transformer Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Logit Lens",
            "name_full": "Logit Lens (identity decoding baseline)",
            "brief_description": "A probing method that decodes an intermediate hidden state h directly with the model decoder head D (i.e., identity mapping), used here as a baseline for linear decoding.",
            "citation_title": "interpreting gpt: the logit lens",
            "mention_or_use": "use",
            "model_name": "GPT-J, GPT-2-XL, LLaMA-13B (used as baseline)",
            "model_description": "Application of Logit Lens: decode subject representation s directly to logits via decoder head D to estimate F(s,c) without additional projection.",
            "arithmetic_task_type": "not arithmetic; direct decoding of hidden states",
            "mechanism_or_representation": "Identity mapping h -&gt; D(h) to obtain next-token distribution; contrasts with affine projection used by LRE.",
            "probing_or_intervention_method": "Used as baseline for faithfulness comparisons against LRE and Translation baselines.",
            "performance_metrics": "Generally low faithfulness relative to LRE (Figure 4 and Figure 13); shows identity is insufficient for many relations.",
            "error_types_or_failure_modes": "Fails when LM requires projection of enriched subject representation to object space; low performance indicates need for W and b terms.",
            "evidence_for_mechanism": "Empirical baseline comparisons show LRE outperforms Logit Lens identity decoding for relation decoding tasks.",
            "counterexamples_or_challenges": "Logit Lens sometimes captures next-token info but not relation-specific decoding that occurs after enrichment.",
            "uuid": "e8396.4",
            "source_info": {
                "paper_title": "Linearity of Relation Decoding in Transformer Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "β scaling",
            "name_full": "Jacobian magnitude correction (β scaling)",
            "brief_description": "Empirical scalar multiplier β (&gt;1) applied to the mean Jacobian W to correct for underestimation of magnitude in the first-order linear approximation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J (β tuned = 2.25 reported), also applied to other models",
            "model_description": "Applied to affine mapping: LRE(s) = β W_r s + b_r to better match actual change magnitudes in F(s,c).",
            "arithmetic_task_type": "not arithmetic; scaling of linear operator",
            "mechanism_or_representation": "Scalar amplification of projection W to match integrated Jacobian between subject vectors (empirically necessary due to layer norm and nonlinearity effects).",
            "probing_or_intervention_method": "β selected per LM via grid search to maximize correlation between faithfulness and causality; causality measured with β=1 in edits but β used for faithfulness estimation.",
            "performance_metrics": "For GPT-J, β=2.25 yields faithfulness_mu ~0.59 and causality_mu ~0.81 (Table 6); varying β affects faithfulness metrics (Table 6 shows monotonic rise then plateau).",
            "error_types_or_failure_modes": "Without β, W underestimates magnitude of F changes; β is heuristic scalar and chosen per-LM (not per-relation), so imperfect; interventions for causality calculated using β=1 (edits magnified separately).",
            "evidence_for_mechanism": "Measured underestimation ratio via integrating Jacobian along subject-to-subject paths (Appendix C, Table 5) showing W underestimates actual change by factors &gt;2-4 for some relations.",
            "counterexamples_or_challenges": "β is a crude global fix; some relations still require nonlinear handling or layer-specific adjustments.",
            "uuid": "e8396.5",
            "source_info": {
                "paper_title": "Linearity of Relation Decoding in Transformer Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Low-rank pseudoinverse editing",
            "name_full": "Low-rank pseudoinverse (W^†) based activation editing",
            "brief_description": "Causal intervention method that computes Δs = W^† (o' - o) (optionally scaled) to edit subject representation s so the LM outputs a target object o'; uses a low-rank inverse to avoid amplifying noise from small singular values.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J, GPT-2-XL, LLaMA-13B (evaluated primarily on GPT-J)",
            "model_description": "Intervention on a single-layer, single-position hidden state (subject's last token position) and then run forward to measure change in predicted first token of object.",
            "arithmetic_task_type": "vector-space editing (not numeric arithmetic)",
            "mechanism_or_representation": "Compute SVD of W = U Σ V^T, take only top-ρ singular values to form Σ^†_ρ, then W^† = V Σ^†_ρ U^T; Δs = W^† (o' - o) applied to s (optionally scaled) to approximate s'.",
            "probing_or_intervention_method": "Activation patching: replace s with s+Δs and evaluate next-token prediction; compare against oracle (inserting actual s') and baselines that insert o' or decoder-row embedding e_o' directly.",
            "performance_metrics": "Causality performance matches oracle closely for many relations (Figure 5, Figure 14); average causality_mu for GPT-J ~0.81 (Table 6) when hyperparameters tuned.",
            "error_types_or_failure_modes": "Full inverse W^{-1} is ill-conditioned — small singular values amplify noise and reduce edit efficacy; too-high rank reduces causality after an optimal rank (Figure 9); patching single state subject to leakage from earlier layers reduces efficacy at deeper layers.",
            "evidence_for_mechanism": "Ablations across rank (ρ) show causality improves up to an optimal low-rank then declines (Appendix D.2, Figure 9); alignment of causality curves with oracle replacement across layers supports W^† producing s' approximation.",
            "counterexamples_or_challenges": "Optimal rank ρ_r is relation-specific and must be tuned; single-state patching limitation due to attention leakage; some relations remain non-linearly encoded and resist this edit.",
            "uuid": "e8396.6",
            "source_info": {
                "paper_title": "Linearity of Relation Decoding in Transformer Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Attribute Lens",
            "name_full": "Attribute Lens (relation-specific probe)",
            "brief_description": "A probing/visualization tool that decodes any hidden state h into an open-vocabulary object-token distribution by applying D(LRE(h)), enabling visualization of when and where a relation's object information is present in the network.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J (primary), GPT-2-XL, LLaMA-13B (also evaluated)",
            "model_description": "Applies an estimated LRE for a specific relation r to hidden states across layers and decodes via the model decoder head into token logits/probabilities to visualize attribute predictions.",
            "arithmetic_task_type": "not arithmetic; representation-to-token decoding for attributes",
            "mechanism_or_representation": "Uses relation-specific affine mapping LRE then decoder head D to produce token distribution; reveals latent knowledge even when LM output is distracted or wrong.",
            "probing_or_intervention_method": "Attribute lens applied to datasets of adversarial 'repetition distracted' and 'instruction distracted' prompts to reveal latent correct facts in top-k predictions even when LM outputs incorrect tokens.",
            "performance_metrics": "Attribute lens reveals true fact in top-3 predictions for a majority of adversarial examples (tested on 11,891 RD and same number of ID prompts); exact numeric top-3 rates summarized in paper's Table 3 (not reproduced here).",
            "error_types_or_failure_modes": "LM may still output falsehoods despite attribute lens showing correct internal representation; attribute lens depends on quality of LRE estimation (β, layer, rank).",
            "evidence_for_mechanism": "Shows cases where LM is baited to output an incorrect object (e.g., 'England is Oslo') yet attribute lens applied to last mention of subject still ranks the true object (e.g., London) in top predictions, indicating latent internal representation of correct fact.",
            "counterexamples_or_challenges": "Attribute lens success depends on relation being linearly decodable; for relations lacking LRE, the lens does not reveal correct attributes.",
            "uuid": "e8396.7",
            "source_info": {
                "paper_title": "Linearity of Relation Decoding in Transformer Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Models studied",
            "name_full": "GPT-J; GPT-2-XL; LLaMA-13B",
            "brief_description": "Autoregressive transformer language models used for experiments: GPT-J (6B), GPT-2-XL (~1.5B), and LLaMA-13B (13B); used to evaluate LRE faithfulness and causality across 47 relations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J; GPT-2-XL; LLaMA-13B",
            "model_description": "GPT-J (6B autoregressive transformer), GPT-2-XL (1.5B), LLaMA-13B (13B) — evaluated under same methodology; results show similar patterns of LRE faithfulness across models with correlations (GPT-J vs GPT2-xl R=0.85, GPT-J vs LLaMA R=0.71).",
            "arithmetic_task_type": "not evaluated (no numeric arithmetic tasks)",
            "mechanism_or_representation": "Intermediate subject representations s enriched across MLP and attention layers; for many relations, relation decoding approximated by linear affine mapping from s to last-layer output representation o.",
            "probing_or_intervention_method": "Jacobian-based LRE estimation, attribute lens, activation patching edits with low-rank pseudoinverse, baselines (Logit Lens, Translation, linear regression), hyperparameter grid search for β, ℓ_r, ρ_r.",
            "performance_metrics": "Relation-wise LRE faithfulness and causality reported per model; overall patterns consistent across models; specific numbers: GPT-J faithfulness_mu ~0.59 at β=2.25; high per-relation variance (Table 4 shows counts of LM-correct examples per relation).",
            "error_types_or_failure_modes": "Tokenizer issues: LLaMA tokenizer splits years by digits so some year-based relations excluded; model-size and tokenization affect which relations are linearly decodable; relation-specific failures as above.",
            "evidence_for_mechanism": "Cross-model comparisons (Figures 15-18) show consistent relationship patterns and correlations between faithfulness and causality metrics across models.",
            "counterexamples_or_challenges": "Differences across models for some relations; certain relations not suitable for LRE in any of the evaluated models.",
            "uuid": "e8396.8",
            "source_info": {
                "paper_title": "Linearity of Relation Decoding in Transformer Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient estimation of word representations in vector space.",
            "rating": 2
        },
        {
            "paper_title": "Learning distributed representations of concepts using linear relational embedding",
            "rating": 2
        },
        {
            "paper_title": "interpreting gpt: the logit lens",
            "rating": 2
        },
        {
            "paper_title": "Language models implement simple word2vecstyle vector arithmetic.",
            "rating": 2
        },
        {
            "paper_title": "Transformer feed-forward layers are key-value memories.",
            "rating": 1
        },
        {
            "paper_title": "Dissecting recall of factual associations in auto-regressive language models",
            "rating": 1
        }
    ],
    "cost": 0.01739325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LINEARITY OF RELATION DECODING IN TRANSFORMER LANGUAGE MODELS</h1>
<p>Evan Hernandez ${ }^{1 <em>}$<br>Martin Wattenberg ${ }^{4}$<br>Arnab Sen Sharma ${ }^{2 </em>}$<br>Jacob Andreas ${ }^{1}$<br>Tal Haklay ${ }^{3}$<br>Yonatan Belinkov ${ }^{3}$<br>Kevin Meng ${ }^{1}$<br>David Bau ${ }^{2}$</p>
<h4>Abstract</h4>
<p>Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in LMs.</p>
<h2>1 INTRODUCTION</h2>
<p>How do neural language models (LMs) represent relations between entities? LMs store a wide variety of factual information in their weights, including facts about real world entities (e.g., John Adams was elected President of the United States in 1796) and common-sense knowledge about the world (e.g., doctors work in hospitals). Much of this knowledge can be represented in terms of relations between entities, properties, or lexical items. For example, the fact that Miles Davis is a trumpet player can be written as a relation (plays the instrument), connecting a subject entity (Miles Davis), with an object entity (trumpet). Categorically similar facts can be expressed in the same type of relation, as in e.g., (Carol Jantsch, plays the instrument, tuba). Prior studies of LMs (Li et al., 2021; Meng et al., 2022; Hernandez et al., 2023) have offered evidence that subject tokens act as keys for retrieving facts: after an input text mentions a subject, LMs construct enriched representations of subjects that encode information about those subjects. Recent studies of interventions (Hase et al., 2023) and attention mechanisms (Geva et al., 2023) suggest that the mechanism for retrieval of specific facts is complex, distributed across multiple layers and attention heads. Past work establishes where relational information is located: LMs extract relation and object information from subject representations. But these works have not yet described what computation LMs perform while resolving relations.</p>
<p>In this paper, we show that LMs employ a simple system for representing a portion of their relational knowledge: they implicitly implement (an affine version of) a linear relational embedding (LRE) scheme (Paccanaro \&amp; Hinton, 2001). Given a relation $r$ such as plays the instrument, a linear relational embedding is an affine function $L R E(\mathbf{s})=W_{r} \mathbf{s}+b_{r}$ that maps any subject representation $\mathbf{s}$ in the domain of the relation (e.g., Miles Davis, Carol Jantsch) to the corresponding object representation $\mathbf{o}$ (e.g., trumpet, tuba). In LMs, the inputs to these implicit LREs are hidden representations of subjects at intermediate layers, and their outputs are hidden representations at late layers that can be decoded to distributions over next tokens. Thus, a portion of transformer LMs' (highly non-linear) computation can be well-approximated linearly in contexts requiring relation prediction.</p>
<p>More specifically, we find that for a variety of relations: (a) transformer LMs decode relational knowledge directly from subject entity representations (s in Figure 1); (b) for each such relation, the decoding procedure is approximately affine (LRE); and (c) these affine transformations can be computed directly from the LM Jacobian on a prompt expressing the relation (i.e. $\partial \mathbf{o} / \partial \mathbf{s}$ ). However, this is not the only system that transformer LMs use to encode relational knowledge, and we also identify relations that are reliably predicted in LM outputs, but for which no LRE can be found.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Within a transformer language model, (a) how it resolves many relations $r$, such as plays the instrument, can be well-approximated by (b) a linear function $R$ that maps subject representations $\mathbf{s}$ to object representations $\mathbf{o}$ that can be directly decoded.</p>
<p>In GPT and LLaMA models, we search for LREs encoding 47 different relations, covering more than 10k facts relating famous entities (The Space Needle, is located in, Seattle), commonsense knowledge (banana, has color, yellow), and implicit biases (doctor, has gender, man). In $48 \%$ of the relations we tested, we find robust LREs that faithfully recover subject-object mappings for a majority of the subjects. Furthermore, we find that LREs can be used to edit subject representations (Hernandez et al., 2023) to control LM output.</p>
<p>Finally, we use our dataset and LRE-estimating method to build a visualization tool we call an attribute lens. Instead of showing the next token distribution like Logit Lens (nostalgebraist, 2020) the attribute lens shows the object-token distribution at each layer for a given relation. This lets us visualize where and when the LM finishes retrieving knowledge about a specific relation, and can reveal the presence of knowledge about attributes even when that knowledge does not reach the output.</p>
<p>Our results highlight two important facts about transformer LMs. First, some of their implicit knowledge is representated in a simple, interpretable, and structured format. Second, this representation system is not universally deployed, and superficially similar facts may be encoded and extracted in very different ways.</p>
<h1>2 BACKGROUND: RELATIONS AND THEIR REPRESENTATIONS</h1>
<h3>2.1 REPRESENTATIONS OF KNOWLEDGE IN LANGUAGE MODELS</h3>
<p>For LMs to generate factually correct statements, factual information must be represented somewhere in their weights. In transformer LMs, past work has suggested that most factual information is encoded in the multi-layer perceptron layers (Geva et al., 2020). These layers act as key-value stores, and work together across multiple layers to enrich the representation of an entity with relevant knowledge (Geva et al., 2022). For instance, in the example from Figure 1, the representation $\mathbf{s}$ of Miles Davis goes through an enrichment process where LM populates $\mathbf{s}$ with the fact that he plays the trumpet as well as other facts, like him being born in Alton, IL. By the halfway point of the LM's computation, $\mathbf{s}$ contains all the information needed to predict a fact about the subject entity when the LM is prompted to retrieve it.</p>
<p>Once $\mathbf{s}$ is populated with relevant facts, the LM must decode the fact most relevant to its current prediction task. Formally, a language model is a distribution $p_{\mathrm{LM}}(x)$ over strings $x$, so this information must be retrieved when the LM is prompted to decode a specific fact, such as when it estimates $p_{\mathrm{LM}}(\cdot \mid$ Miles Davis plays the). Internally, the object must be decoded and written into the final representation o before the next word (trumpet) is predicted. Techniques like the logit lens (nostalgebraist, 2020) and linear shortcut approaches (Belrose et al., 2023; Din et al., 2023) reveal that the LM's final prediction can be read off of $\mathbf{o}$ well before the final layer, and recent work (Geva et al., 2023) suggests that this occurs because specific attention heads (before the final layer) specialize in reading specific relations. Meanwhile, prior work studying the structure of $\mathbf{s}$ suggests that even though transformers are complex, non-linear neural networks, attributes of entities can be linearly decoded from their representations (Li et al., 2021; Hernandez et al., 2023).</p>
<p>But how transformer LMs themselves map from enriched entity representations to language-based predictions has remained an open question. Here, we will show that for a subset of relations the transformer LMs implement the learned readout operation in a near-linear fashion.</p>
<h1>2.2 NEURAL REPRESENTATIONS OF RELATIONS</h1>
<p>Why might we expect a linear representation scheme for relational information in the first place? Separate from (and largely prior to) work on neural language models, a long line of artificial intelligence research has studied how to represent relational knowledge. A classic symbolic approach is to encode relational knowledge triplets of the form (subject, relation, object). For example, one might express the fact that Rome is the capital of Italy as (Rome, is-capital-of, Italy). This triplet format is extremely flexible, and has been used for a variety of tasks (Richens, 1956; Minsky, 1974; Lenat, 1995; Miller, 1995; Berners-Lee et al., 2001; Bollacker et al., 2008).</p>
<p>While representing relational triplets symbolically is straightforward, it is far less clear how to embed these relational structures in deep networks or other connectionist systems. Surveys (Ji et al., 2021; Wang et al., 2017) list more than 40 techniques. These variations reflect the tension between the constraints of geometry and the flexibility of the triplet representation. In many approaches, subject and object entities $s$ and $o$ are represented as vectors $\mathbf{s} \in \mathbb{R}^{m}, \mathbf{o} \in \mathbb{R}^{n}$; for a given relation $r$, we define a relation function $R: \mathbb{R}^{m} \rightarrow \mathbb{R}^{n}$, with the property that when $(s, r, o)$ holds, we have $\mathbf{o} \approx R(\mathbf{s})$.</p>
<p>One way to implement $R$ is to use linear transformations to represent relations. For instance, in linear relational embedding (Paccanaro \&amp; Hinton, 2001), the relation function has the form $R(\mathbf{s})=W_{r} \mathbf{s}$ where $W_{r}$ is a matrix depending on relation $r$. A modern example of this encoding can be seen in the positional encodings of many transformers (Vaswani et al., 2017). More generally, we can write $R$ as an affine transformation, learning both a linear operator $W_{r}$ and a translation $b_{r}$ (Lin et al., 2015; Yang et al., 2021). There are multiple variations on this idea, but the basic relation function is:</p>
<p>$$
R(\mathbf{s})=W_{r} \mathbf{s}+b_{r}
$$</p>
<h2>3 Finding and Validating Linear Relational Embeddings</h2>
<h3>3.1 Finding LREs</h3>
<p>Consider a statement such as Miles Davis plays the trumpet, which expresses a fact $(s, r, o)$ connecting a subject $s$ to an object $o$ via relation $r$ (see Figure 1). Within the transformer's hidden states, let $\mathbf{s}$ denote the representation ${ }^{5}$ of the subject $s$ (Miles Davis) at layer $\ell$, and let $\mathbf{o}$ denote the last-layer hidden state that is directly decoded to get the prediction of the object's first token $o$ (trumpet). The transformer implements a calculation that obtains $\mathbf{o}$ from $\mathbf{s}$ within a textual context $c$ that evokes the relation $r$, which we can write $\mathbf{o}=F(\mathbf{s}, c)$.
Our main hypothesis is that $F(\mathbf{s}, c)$ can be well-approximated by a linear projection, which can be obtained from a local derivative of $F$. Denote the Jacobian of $F$ as $W=\partial F / \partial \mathbf{s}$. Then a first-order Taylor approximation of $F$ about $\mathbf{s}_{0}$ is given by:</p>
<p>$$
\begin{aligned}
F(\mathbf{s}, c) &amp; \approx F\left(\mathbf{s}<em 0="0">{0}, c\right)+W\left(\mathbf{s}-\mathbf{s}</em>\right) \
&amp; =W \mathbf{s}+b \
\text { where } b &amp; =F\left(\mathbf{s}<em 0="0">{0}, c\right)-W \mathbf{s}</em>
\end{aligned}
$$</p>
<p>This approximation would only be reasonable if $F$ has near-linear behavior when decoding the relation from any $\mathbf{s}$. In practice, we estimate $W$ and $b$ as the mean Jacobian and bias at $n$ examples $\mathbf{s}<em i="i">{i}, c</em>$ within the same relation, which gives an unbiased estimate under the assumption that noise in $F$ has zero value and zero Jacobian in expectation (see Appendix B). That is, we define:</p>
<p>$$
W=\mathbb{E}<em i="i">{\mathbf{s}</em>\right|}, c_{i}}\left[\left.\frac{\partial F}{\partial \mathbf{s}<em i="i">{\left(\mathbf{s}</em>}, c_{i}\right)}\right] \text { and } b=\mathbb{E<em i="i">{\mathbf{s}</em>\right|}, c_{i}}\left[F(\mathbf{s}, c)-\left.\frac{\partial F}{\partial \mathbf{s}} \mathbf{s<em i="i">{\left(\mathbf{s}</em>\right]
$$}, c_{i}\right)</p>
<p>This simple formulation has several limitations that arise due to the use of layer normalization (Ba et al., 2016) in the transformer: for example, $\mathbf{s}$ is passed through layer normalization before contributing to the computation of $\mathbf{o}$, and $\mathbf{o}$ is again passed through layer normalization before leading</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>to token predictions, so in both cases, the transformer does not transmit changes in scale of inputs to changes in scale of outputs. That means that even if Equation 2 is a good estimate of the direction of change of $F$, it may not be an accurate estimate of the magnitude of change.</p>
<p>In practice, we find that the magnitude of change in $F(\mathbf{s}, c)$ is underestimated in our calculated $W$ (see Appendix C for empirical measurements). To remedy this underestimation we make $W$ in Equation (2) steeper by multiplying with a scalar constant $\beta(&gt;1)$. So, for a relation $r$ we approximate the transformer calculation $F\left(\mathbf{s}, c_{r}\right)$ as an affine transformation LRE on $\mathbf{s}$ :</p>
<p>$$
F\left(\mathbf{s}, c_{r}\right) \approx \operatorname{LRE}(\mathbf{s})=\beta W_{r} \mathbf{s}+b_{r}
$$</p>
<h1>3.2 EVALUATING LRES</h1>
<p>When a linear relation operator LRE is a good approximation of the transformer's decoding algorithm, it should satisfy two properties:</p>
<p>Faithfulness. When applied to new subjects $s$, the output of $\operatorname{LRE}(\mathbf{s})$ should make the same predictions as the transformer. Given the LM's decoder head $D$, we define the transformer prediction $o$ and LRE prediction $\hat{o}$ as:</p>
<p>$$
o=\underset{t}{\operatorname{argmax}} D(F(\mathbf{s}, c))<em t="t">{t} \text { and } \hat{o}=\underset{t}{\operatorname{argmax}} D(\operatorname{LRE}(\mathbf{s}))</em>
$$</p>
<p>And we define faithfulness as the success rate of $o \stackrel{?}{=} \hat{o}$, i.e., the frequency with which predictions made by LRE from only $\mathbf{s}$ match next-token predictions made by the full transformer:</p>
<p>$$
\underset{t}{\operatorname{argmax}} D(F(\mathbf{s}, c))<em t="t">{t} \stackrel{?}{=} \underset{t}{\operatorname{argmax}} D(\operatorname{LRE}(\mathbf{s}))</em>
$$</p>
<p>Causality. If a learned LRE is a good description of the LM's decoding procedure, it should be able to model causal influence of the relational embedding on the LM's predictions. If it does, then inverting LRE tells us how to perturb $\mathbf{s}$ so that the LM decodes a different object $o^{\prime}$. Formally, given a new object $\mathbf{o}^{\prime}$, we use LRE to find an edit direction $\Delta \mathbf{s}$ that satisfies:</p>
<p>$$
\operatorname{LRE}(\mathbf{s}+\Delta \mathbf{s})=\mathbf{o}^{\prime}
$$</p>
<p>With $\beta=1$, we can edit $\mathbf{s}$ as follows: ${ }^{6}$</p>
<p>$$
\tilde{\mathbf{s}}=\mathbf{s}+\Delta \mathbf{s}, \quad \text { where } \Delta \mathbf{s}=W_{r}^{-1}\left(\mathbf{o}^{\prime}-\mathbf{o}\right)
$$</p>
<p>We obtain $\mathbf{o}^{\prime}$ from a different subject $s^{\prime}$ that is mapped by $F$ to $o^{\prime}$ under the relation $r$. $\tilde{\mathbf{s}}$ here is essentially an approximation of $\mathbf{s}^{\prime}$. Figure 2 illustrates this procedure.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of the representation editing used to measure causality. Under the relation $r=$ plays the instrument, and given the subject $s=$ Miles Davis, LM will predict $o=$ trumpet (a); and given the subject $s^{\prime}=$ Cat Stevens, the output is $o^{\prime}=$ guitar (b). If the computation from $\mathbf{s}$ to $\mathbf{o}$ is well-approximated by LRE parameterized by $W_{r}$ and $b_{r}(\mathbf{c})$, then $\Delta \mathbf{s}(\mathbf{d})$ should tell us the direction of change from $\mathbf{s}$ to $\mathbf{s}^{\prime}$. Thus, $\tilde{\mathbf{s}}=\mathbf{s}+\Delta \mathbf{s}$ would be an approximation of $\mathbf{s}^{\prime}$ and patching $\tilde{\mathbf{s}}$ in place of $\mathbf{s}$ should change the prediction to $o^{\prime}=$ guitar (f).</p>
<p>Note that Equation (7) requires inverting $W_{r}$, but the inverted matrix might be ill-conditioned. To make edits more effective, we instead use a low-rank pseudoinverse $W_{r}^{\dagger}$, which prevents the smaller singular values from washing out the contributions of the larger, more meaningful singular values. See Appendix D. 2 for details.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We call the intervention a success if $o^{\prime}$ is the top prediction of the LM after the edit:</p>
<p>$$
o^{\prime} \stackrel{?}{=} \underset{t}{\operatorname{argmax}} D\left(F\left(\mathbf{s}, c_{r} \mid \mathbf{s}:=\mathbf{s}+\Delta \mathbf{s}\right)\right)
$$</p>
<p>Note that for both faithfulness and causality we only consider the first token of the object when determining success. Limitations of this approach are discussed in Appendix I.</p>
<h1>4 EXPERIMENTS</h1>
<p>We now empirically evaluate how well LREs, estimated using the approach from Section 3, can approximate relation decoding in LMs for a variety of different relations.
Models. In all of our experiments, we study autoregressive language models. Unless stated otherwise, reported results are for GPT-J (Wang \&amp; Komatsuzaki, 2021), and we include additional results for GPT-2-XL (Radford et al., 2019) and LLaMA-13B (Touvron et al., 2023) in Appendix H.
Dataset. To support our evaluation, we manually curate a dataset of 47 relations spanning four categories: factual associations, commonsense knowledge, implicit biases, and linguistic knowledge. Each relation is associated with a number of example subject-object pairs $\left(s_{i}, o_{i}\right)$, as well as a prompt template that leads the language model to predict $o$ when $s$ is filled in (e.g., [s] plays the). When evaluating each model, we filter the dataset to examples where the language model correctly predicts the object $o$ given the prompt. Table 1 summarizes the dataset and filtering results. Further details on dataset construction are in Appendix A.
Implementation Details. We estimate LREs for each relation using the method discussed in Section 3 with $n=8$. While calculating $W$ and $b$ for an individual example we prepend the remaining $n-1$ training examples as few-shot examples so that the LM is more likely to generate the answer $o$ given a $s$ under the relation $r$ over other plausible tokens. Then, an LRE is estimated with Equation (3) as the expectation of $W$ s and $b$ s calculated on $n$ individual examples.</p>
<p>Table 1: Information about the dataset of relations used to evaluate LM relation decoding in LMs. These relations are drawn from a variety of sources. Evaluation is always restricted to the subset of $(s, r, o)$ triples for which the LM successfully decodes $o$ when prompted with $(s, r)$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: center;"># Rel.</th>
<th style="text-align: center;"># Examples</th>
<th style="text-align: center;"># GPT-J Corr.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Factual</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">9696</td>
<td style="text-align: center;">4652</td>
</tr>
<tr>
<td style="text-align: left;">Commonsense</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">374</td>
<td style="text-align: center;">219</td>
</tr>
<tr>
<td style="text-align: left;">Linguistic</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">806</td>
<td style="text-align: center;">507</td>
</tr>
<tr>
<td style="text-align: left;">Bias</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">213</td>
<td style="text-align: center;">96</td>
</tr>
</tbody>
</table>
<p>We fix the scalar term $\beta$ (from Equation (4)) once per LM. We also have two hyperparameters specific to each relation $r ; \ell_{r}$, the layer after which $\mathbf{s}$ is to be extracted; and $\rho_{r}$, the rank of the inverse $W^{\dagger}$ (to check causality as in Equation (7)). We select these hyperparameters with grid-search; see Appendix E for details. For each relation, we report average results over 24 trials with distinct sets of $n$ examples randomly drawn from the dataset. LREs are evaluated according to faithfulness and causality metrics defined in Equations (5) and (8).</p>
<h3>4.1 Are LREs faithful to relations?</h3>
<p>We first investigate whether LREs accurately predict the transformer output for different relations, that is, how faithful they are. Figure 3 shows faithfulness by relation. Our method achieves over $60 \%$ faithfulness for almost half of the relations, indicating that those relations are linearly decodable from the subject representation.</p>
<p>We are also interested in whether relations are linearly decodable from $\mathbf{s}$ by any other method. Figure 4 compares our method (from Section 3) to four other approaches for estimating linear relational functions. We first compare with Logit Lens (nostalgebraist, 2020), where $\mathbf{s}$ is directly decoded with the LM decoder head $D$. This essentially tries to estimate $F(\mathbf{s}, c)$ as an Identity transformation on $\mathbf{s}$. Next, we try to approximate $F$ as Translation(s) $=\mathbf{s}+b$, where $b$ is estimated as $\mathbb{E}[\mathbf{o}-\mathbf{s}]$ over $n$ examples. This Translation baseline, inspired by Merullo et al. (2023) and traditional word embedding arithmetic (Mikolov et al., 2013), approximates $F$ from the intermediate representation of the last token of $s$ until $\mathbf{o}$ is generated (Figure 1). Then we compare with a linear regression model trained with $n$ examples to predict $\mathbf{o}$ from $\mathbf{s}$. Finally, we apply LRE on the subject embedding $\mathbf{e}_{s}$ before initial layers of the LM get to enrich the representation.</p>
<p>Figure 4 shows that our method LRE captures LM behavior most faithfully across all relation types. This effect is not explained by word identity, as evidenced by the low faithfulness of $\operatorname{LRE}\left(\mathbf{e}_{s}\right)$. Also,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Relation-wise LRE faithfulness to LM computation $F$. Horizontal red lines per relation indicate accuracy of a random-guess baseline. LRE is consistenly better than random guess and is predictive of the behavior of the transformer on most relations. However, for some relations such as company CEO or task done by tool, the transformer LM deviates from LRE, suggesting non-linear model computation for those relations.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Faithfulness comparison of different linear approximations of LM decoding stratified across different relation types. (a) Our method, LRE, applied on $\mathbf{s}$ extracted after $\ell_{r}$ (b) LRE applied on the subject embedding $\mathbf{e}_{s}$. The performance different between (a) and (b) shows the importance of the enrichment process $\mathbf{s}$ goes through in the earlier layers. (c) shows the performance of a linear regression model trained with $n=8$ examples, which is outperformed by a LRE calculated with similar number of examples, $n$. (d) is $\operatorname{TRANSLATION}(\mathbf{s})=\mathbf{s}+b$, where $b$ is estimated as $\mathbb{E}[\mathbf{o}-\mathbf{s}]$ over $n$ samples. The performance drop in (d) compared to (a) emphasizes the necessity of the projection term $W$. In (e) $\mathbf{s}$ is directly decoded with the decoder head $D$.
low performance of the Translation and Identity baselines highlight that both the projection and bias terms of Equation (4) are necessary to approximate the decoding procedure as LRE.</p>
<p>However, it is also clear (from Figure 3) that some relations are not linearly decodable from intermediate representations of the subject, despite being accurately predicted by the LM. For example, no method reaches over $6 \%$ faithfulness on the Company CEO relation, despite GPT-J accurately predicting the CEOs of 69 companies when prompted. This is true across layers (Figure 11 of Appendix E.2) and random sampling of $n$ examples for approximating LRE parameters. This indicates that some more complicated, non-linear decoding approach is employed by the model to make those predictions. Interestingly, the relations that exhibit this behavior the most are those where the range is the names of peoples or companies. One possible explanation is that these ranges are so large that the LM cannot reliably linearly encode them at a single layer, and relies on a more complicated encoding procedures possibly involving multiple layers.</p>
<h1>4.2 Do LREs CAUsALLY CHARACTERIZE MODEL PREDICTIONS?</h1>
<p>We now have evidence that some relations are linearly decodable from LM representations using a first-order approximation of the LM. However, it could be that these encodings are not used by the LM to predict the next word, and instead are correlative rather than causal. To show that LREs causally influence LM predictions, we follow the procedure described in Figure 2 to use the inverse of LRE to change the LM's predicted object for a given subject.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: LRE causality compared with different baselines. (a) LRE causality on best performing hyperparameters (layer $\ell_{r}$ and rank $\rho_{r}$ ) for each relation $r$. (b) is our oracle baseline, inserting the representation $\mathbf{s}^{\prime}$ of target subject $s^{\prime}$ in place of $\mathbf{s}$. (c) in place of $\mathbf{s}$ inserting $\mathbf{e}<em r="r">{o^{\prime}}$, the row in the decoder head matrix $D$ corresponding to $o^{\prime}$, and (d) inserting $\mathbf{o}^{\prime}$, the output of $F\left(\mathbf{s}^{\prime}, c</em>\right)$.
In Figure 5 we compare our causality intervention with 3 other approaches of replacing $\mathbf{s}$ such that LM outputs $o^{\prime}$. If the model computation $F$ from $\mathbf{s}$ to $\mathbf{o}$ is well-approximated by the LRE, then our intervention should be equivalent to inserting $\mathbf{s}^{\prime}$ in place of $\mathbf{s}$. This direct substitution procedure thus provides an oracle upper-bound. Besides the oracle approach we include 2 more baselines; in place of $\mathbf{s}$, (1) inserting $\mathbf{o}^{\prime}$, the output of $F\left(\mathbf{s}^{\prime}, c_{r}\right)$ (2) inserting $\mathbf{e}_{o^{\prime}}$, the row in the decoder head matrix $D$ corresponding to $o^{\prime}$ as the embedding of $o^{\prime}$. These two additional baselines ensure that our approach is not trivially writing the answer $\mathbf{o}^{\prime}$ on the position of $\mathbf{s}$.
Figure 14 of Appendix G. 2 compares our method with the baselines for selected relations and across layers. The graphs illustrate how our method matches the oracle's performance and differs from the other two baselines. This provides causal evidence that LRE approximates these relations well.
Figure 6 depicts a strong linear correlation between our metrics when the hyperparameters were selected to achieve best causal influence. This means that when an LRE causally influences the LM's predictions, it is also faithful to the model. ${ }^{7}$ Also from Figure 6, for almost all relations LRE causality score is higher than its faithfulness score. This suggests that, even in cases where LRE can not fully capture the LM's computation of the relation, the linear approximation remains powerful enough to perform a successful edit.
While our focus within this work is on the linearity of relation decoding and not on LM representation editing, a qualitative analysis of the post-edit generations reveals that the edits are nontrivial and preserve the LM's fluency; see Table 8 of Appendix G.2.</p>
<h3>4.3 WHERE IN THE NETWORK DO REPRESENTATIONS EXHIBIT LREs?</h3>
<p>In the previous experiments, for each relation we had fixed $\ell_{r}$ (the layer after which $\mathbf{s}$ is to be extracted) to achieve the best causal influence on the model. However, there are considerable differences in LRE faithfulness when estimated from different layers. Figure 7 highlights an example relation that appears to be linearly decodable from representations in layer 7 until layer 17, at which point faithfulness plummets. Figure 11 of Appendix E. 2 shows similar plots for other relations.</p>
<p>Why might this happen? One hypothesis is that a transformer's hidden representations serve a dual purpose: they contain both information about the current word (its synonyms, physical attributes, etc.), and information necessary to predict the next token. At some point, the latter information structure must be preferred to the former in order for the LM to minimize its loss on the next-word prediction task. The steep drop in faithfulness might indicate that a mode switch is happening in the</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>LM's representations at later layers, where the LM decisively erases relational embeddings in support of predicting the next word.</p>
<p>Table 2: Example of prompts with and without relation-specific context.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">With relation-specific context</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LeBron James plays the sport of basketball</td>
</tr>
<tr>
<td style="text-align: left;">Roger Federer plays the sport of tennis</td>
</tr>
<tr>
<td style="text-align: left;">Lionel Messi plays the sport of</td>
</tr>
<tr>
<td style="text-align: left;">Without relation-specific context</td>
</tr>
<tr>
<td style="text-align: left;">LeBron James basketball</td>
</tr>
<tr>
<td style="text-align: left;">Roger Federer tennis</td>
</tr>
<tr>
<td style="text-align: left;">Lionel Messi</td>
</tr>
</tbody>
</table>
<p>We indeed see in Figure 7 that LRE faithfulness keeps improving in later layers when we remove relation specific texts from our prompt, meaning the $o$ immediately follows $s$ in the prompt (Table 2).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Later layers switching roles from enriching s to predicting next token. LRE performance across different layers of GPT-J for the factual relation plays the sport of with and without relation-specific prompt (Table 2). Faithfulness does not decrease in later layers when $o$ immediately follows the $s$ in the prompt.</p>
<h1>5 Application: The Attribute Lens</h1>
<p>We apply the LRE to create a novel probing method we call the attribute lens that provides a view into a LM's knowledge of an attribute of a subject with respect to a relation $r$. Given a linear function LRE for the relation $r$, the attribute lens visualizes a hidden state $\mathbf{h}$ by applying the LM decoder head $D$ to decode $D(\operatorname{LRE}(\mathbf{h}))$ into language predictions. The attribute lens specializes the Logit Lens (nostalgebraist, 2020) (which visualizes next token information in a hidden state $\mathbf{h}$ by decoding $D(\mathbf{h})$ ) and linear shortcut approaches (Belrose et al., 2023; Din et al., 2023) (where an affine probe $A_{\ell}$ is trained to skip computation after layer $\ell$, directly decoding the next token as $D\left(A_{\ell}\left(\mathbf{h}<em _ell="\ell">{\ell}\right)\right)$, where $\mathbf{h}</em>$ may encode many pieces of information beyond predictions of the immediate next token. Traditional representation probes (Belinkov \&amp; Glass, 2019; Belinkov, 2022) also reveal specific facets of a representation, but unlike probing classifiers that divide the representation space into a small number of output classes, the attribute lens decodes a representation into an open-vocabulary distribution of output tokens. Figure 8 illustrates the use of one attribute lens to reveal knowledge representations that contain information about the sport played by a person, and another lens about university affiliation.}$ is the hidden state after $\ell$ ). However, unlike these approaches concerned with the immediate next token, the attribute lens is motivated by the observation that each high-dimensional hidden state $\mathbf{h</p>
<p>This attribute lens can be applied to analyze LM falsehoods: in particular, it can identify cases where an LM outputs a falsehood that contradicts the LM's own internal knowledge about a subject. To quantify the attribute lens's ability to reveal such situations, we tested the attribute lens on a set of 11,891 "repetition distracted" (RD) and the same number of "instruction distracted" (ID) prompts where we deliberately bait the LM to output a wrong $o$, but the LM would have predicted the correct $o$ without the distraction. For example, in order to bait an LM to predict that The capital city of England is... Oslo, a RD prompt states the falsehood The capital city of England is Oslo twice before asking the model to complete a third statement, and an ID prompt states the falsehood followed by the instruction Repeat exactly. Although in these cases, the LM will almost never output the true fact (it will predict Oslo instead of London), the attribute lens applied to the last mention of the subject (England) will typically reveal the true fact (e.g., London) within the</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: The attribute lens applied to the hidden states of GPT-J processing Bill Bradley was a. First two grids visualize the same set of hidden states under the attribute lens for two different relations. The word in each rectangle is the most-likely token in the distribution $D(\operatorname{LRE}(\mathbf{h}))$, where $D$ applies the transformer decoder head; darker boxes correspond to higher probabilities of the top prediction. In (a) the relation is plays sport, and in (b) attended university, and both these cases reveal high-scoring predictions for attributes on the subject. For comparison, (c) sets $\operatorname{LRE}=I$ which produces the Logit Lens (nostalgebraist, 2020) visualization, in which the visualized relation can be thought of as next token. (Senator Bill Bradley was formerly a basketball player who went to school at Princeton.)
top 3 predictions. In Table 3 we show performance of the attribute lens to reveal latent knowledge under this adversarial condition.</p>
<h1>6 Related Work</h1>
<p>Representation probes. The structure of the information represented within a neural network is a foundational problem that has been studied from several perspectives. One approach is to identify properties encoded representations by training a probing classifier to predict properties from the representations (Ettinger et al., 2016; Shi et al., 2016; Hupkes et al., 2018; Conneau et al., 2018; Belinkov et al., 2017; Belinkov \&amp; Glass, 2019). However, such approaches can overestimate the knowledge contained in a network if the classifier learns to solve a task on its own (Belinkov, 2022); the problem can be mitigated by comparing to a control task (Hewitt \&amp; Liang, 2019) or by limiting the training of the probe (Voita \&amp; Titov, 2020). Our method differs from probing by avoiding the introduction of a training process entirely: we extract the LRE from the LM itself rather than training a new model.</p>
<p>Knowledge representation. Ever since emergent neural representations of relations were first observed in the original backpropagation paper (Rumelhart et al., 1986), neural representations of knowledge and relations have been a central problem in artificial intelligence. Section 2 surveys work in this area including knowledge graph embedding (Wang et al., 2017; Yang et al., 2021) and emergent knowledge representations within a transformer language model (Li et al., 2021; Meng et al., 2022; Hase et al., 2023; Hernandez et al., 2023; Geva et al., 2023). This paper builds on past work in by showing that relational aspects of this knowledge are encoded linearly.</p>
<p>Knowledge extraction. The most direct way to characterize knowledge in LMs is to prompt or query them directly (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020; Shin et al., 2020; Cohen et al., 2023). However, recent work has suggested that model knowledge and knowledge retrieval may be localized within small parts of a language model (Geva et al., 2020; Dai et al., 2021; Meng et al., 2022; Geva et al., 2023). In this paper we further investigate the localized retrieval of knowledge and ask whether knowledge about relations and objects can be separated, and whether relations are represented as a linear relational embedding.</p>
<h2>7 CONCLUSION</h2>
<p>Reverse-engineering the full mechanism of an LLM is a daunting task. In this work, we have found that a certain kind of computation, relation decoding, can often be well-approximated by linear relational embeddings. We have also found that some relations are better-approximated as LREs than others; relations that have an easier or harder random baseline fall on either end of the spectrum. We have shown that LREs estimated from a small set of examples lead to faithful representations that are causally linked to the LM's behavior. Furthermore, LRE can be used to provide specialized attribute lens on the LM's intermediate computation, even revealing cases of LM falsehoods.</p>
<h1>ETHICS STATEMENT</h1>
<p>By revealing and decoding internal model relations before they are explicitly expressed in model output, LREs can potentially be used to provide information about internal biases or errors, and the causal effects could provide a way to mitigate undesired biases. However, such representation-level representation might be only superficial without correcting internal biases in the model; exploring such applications is a natural step for future work.</p>
<h2>REPRODUCIbILITY STATEMENT</h2>
<p>The code and dataset are available at lre.baulab.info. We include full details about dataset curation in Appendix A. In addition to the experiment details at the beginnings of Sections 4 and 5, we describe hyperparameter sweeps in Appendix E. We ran all experiments on workstations with 80GB NVIDIA A100 GPUs or 48GB A6000 GPUs using HuggingFace Transformers (Wolf et al., 2019) implemented in PyTorch (Paszke et al., 2019).</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>This research has been supported by an AI Alignment grant from Open Philanthropy, the Israel Science Foundation (grant No. 448/20), and an Azrieli Foundation Early Career Faculty Fellowship. We are also grateful to the Center for AI Safety (CAIS) for sharing their compute resources, which supported many of our experiments.</p>
<h2>REFERENCES</h2>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.</p>
<p>Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207-219, 2022.</p>
<p>Yonatan Belinkov and James Glass. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, 7:49-72, 2019.</p>
<p>Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. What do neural machine translation models learn about morphology? arXiv preprint arXiv:1704.03471, 2017.</p>
<p>Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023.</p>
<p>Tim Berners-Lee, James Hendler, and Ora Lassila. The semantic web. Scientific american, 284(5): $34-43,2001$.</p>
<p>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pp. 1247-1250, 2008.</p>
<p>Roi Cohen, Mor Geva, Jonathan Berant, and Amir Globerson. Crawling the internal knowledge-base of language models. arXiv preprint arXiv:2301.12810, 2023.</p>
<p>Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. What you can cram into a single vector: Probing sentence embeddings for linguistic properties. arXiv preprint arXiv:1805.01070, 2018.</p>
<p>Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. arXiv preprint arXiv:2104.08696, 2021.</p>
<p>Alexander Yom Din, Taelin Karidi, Leshem Choshen, and Mor Geva. Jump to conclusions: Shortcutting transformers with linear transformations. arXiv preprint arXiv:2303.09435, 2023.</p>
<p>Allyson Ettinger, Ahmed Elgohary, and Philip Resnik. Probing for semantic evidence of composition by means of simple classification tasks. In Proceedings of the 1st workshop on evaluating vectorspace representations for nlp, pp. 134-139, 2016.</p>
<p>Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020.</p>
<p>Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. arXiv preprint arXiv:2203.14680, 2022.</p>
<p>Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual associations in auto-regressive language models. arXiv preprint arXiv:2304.14767, 2023.</p>
<p>Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models. arXiv preprint arXiv:2301.04213, 2023.</p>
<p>Evan Hernandez, Belinda Z Li, and Jacob Andreas. Inspecting and editing knowledge representations in language models. arXiv preprint arXiv:2304.00740, 2023.</p>
<p>John Hewitt and Percy Liang. Designing and interpreting probes with control tasks. arXiv preprint arXiv:1909.03368, 2019.</p>
<p>Dieuwke Hupkes, Sara Veldhoen, and Willem Zuidema. Visualisation and'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure. Journal of Artificial Intelligence Research, 61:907-926, 2018.</p>
<p>Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE Transactions on Neural Networks and Learning Systems, 33(2):494-514, 2021.</p>
<p>Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438, 2020.</p>
<p>Douglas B Lenat. Cyc: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11):33-38, 1995.</p>
<p>Belinda Z Li, Maxwell Nye, and Jacob Andreas. Implicit representations of meaning in neural language models. arXiv preprint arXiv:2106.00737, 2021.</p>
<p>Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of the AAAI conference on artificial intelligence, volume 29, 2015.</p>
<p>Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359-17372, 2022.</p>
<p>Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Language models implement simple word2vecstyle vector arithmetic. arXiv preprint arXiv:2305.16130, 2023.</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.</p>
<p>George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11): $39-41,1995$.</p>
<p>Marvin Minsky. A framework for representing knowledge, 1974.
nostalgebraist. interpreting gpt: the logit lens, 2020. URL https://www. lesswrong.com/posts/ AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens.</p>
<p>Alberto Paccanaro and Geoffrey E. Hinton. Learning distributed representations of concepts using linear relational embedding. IEEE Transactions on Knowledge and Data Engineering, 13(2): $232-244,2001$.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019 .</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463-2473, 2019.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Richard H Richens. Preprogramming for mechanical translation. Mechanical Translation, 3(1): $20-25,1956$.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5418-5426, 2020.</p>
<p>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533-536, 1986.</p>
<p>Xing Shi, Inkit Padhi, and Kevin Knight. Does string-based neural mt learn source syntax? In Proceedings of the 2016 conference on empirical methods in natural language processing, pp. $1526-1534,2016$.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. $4222-4235,2020$.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Elena Voita and Ivan Titov. Information-theoretic probing with minimum description length. arXiv preprint arXiv:2003.12298, 2020.</p>
<p>Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.</p>
<p>Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering, 29(12): $2724-2743,2017$.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.</p>
<p>Jinfa Yang, Yongjie Shi, Xin Tong, Robin Wang, Taiyan Chen, and Xianghua Ying. Improving knowledge graph embedding using affine transformations of entities corresponding to each relation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 508-517, 2021.</p>
<h1>A Relations Dataset</h1>
<p>The dataset consists of 47 relations stratified across 4 groups; factual, linguistic, bias, and commonsense. Six of the factual relations were scraped from Wikidata while the rest were drawn from the CounterFact dataset by Meng et al. (2022). linguistic, bias, and commonsense relations were newly curated by the authors. See Table 4 for details.
Table 4: Number of examples per relation and the count of accurate predictions by different LMs. Each of the examples were tested using $n=8$ ICL examples ( $n=5$ for LLaMA-13B). Results presented as mean ( $\pm$ std) of the counts across 24 trials with different set of ICL examples. For cases where the count of examples accurately predicted by the LM is less then $n$, it was replaced with " - ". LRE estimation was not calculated for such cases. ${ }^{<em> </em>}$ We also do not calculate LRE for LLaMA-13B where $o$ is a year (president birth year and president election year) as LLaMA tokenizer splits years by digits (see Table 9).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Relation</th>
<th style="text-align: center;">#</th>
<th style="text-align: center;">GPT-J</th>
<th style="text-align: center;"># Correct GPT2-xl</th>
<th style="text-align: center;">LLaMA-13B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">factual</td>
<td style="text-align: center;">person mother</td>
<td style="text-align: center;">994</td>
<td style="text-align: center;">$182.8 \pm 5.8$</td>
<td style="text-align: center;">$83.5 \pm 12.3$</td>
<td style="text-align: center;">$613.5 \pm 1.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person father</td>
<td style="text-align: center;">991</td>
<td style="text-align: center;">$206.1 \pm 7.6$</td>
<td style="text-align: center;">$109.2 \pm 6.1$</td>
<td style="text-align: center;">$675.5 \pm 5.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person sport position</td>
<td style="text-align: center;">952</td>
<td style="text-align: center;">$243.3 \pm 94.1$</td>
<td style="text-align: center;">$199.9 \pm 67.9$</td>
<td style="text-align: center;">$200.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">landmark on continent</td>
<td style="text-align: center;">947</td>
<td style="text-align: center;">$797.5 \pm 74.7$</td>
<td style="text-align: center;">$421.9 \pm 85.3$</td>
<td style="text-align: center;">$200.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person native language</td>
<td style="text-align: center;">919</td>
<td style="text-align: center;">$720.2 \pm 31.3$</td>
<td style="text-align: center;">$697.3 \pm 16.1$</td>
<td style="text-align: center;">$200.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">landmark in country</td>
<td style="text-align: center;">836</td>
<td style="text-align: center;">$565.2 \pm 14.2$</td>
<td style="text-align: center;">$274.1 \pm 58.2$</td>
<td style="text-align: center;">$200.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person occupation</td>
<td style="text-align: center;">821</td>
<td style="text-align: center;">$131.5 \pm 28.5$</td>
<td style="text-align: center;">$42.0 \pm 10.3$</td>
<td style="text-align: center;">$404.0 \pm 25.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">company hq</td>
<td style="text-align: center;">674</td>
<td style="text-align: center;">$316.0 \pm 10.4$</td>
<td style="text-align: center;">$148.8 \pm 36.6$</td>
<td style="text-align: center;">$200.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">product by company</td>
<td style="text-align: center;">522</td>
<td style="text-align: center;">$366.4 \pm 8.1$</td>
<td style="text-align: center;">$262.4 \pm 23.8$</td>
<td style="text-align: center;">$421.3 \pm 6.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person plays instrument</td>
<td style="text-align: center;">513</td>
<td style="text-align: center;">$237.6 \pm 30.7$</td>
<td style="text-align: center;">$144.2 \pm 27.6$</td>
<td style="text-align: center;">$249.0 \pm 67.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">star constellation name</td>
<td style="text-align: center;">362</td>
<td style="text-align: center;">$276.3 \pm 3.4$</td>
<td style="text-align: center;">$210.3 \pm 16.7$</td>
<td style="text-align: center;">$200.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">plays pro sport</td>
<td style="text-align: center;">318</td>
<td style="text-align: center;">$244.2 \pm 10.8$</td>
<td style="text-align: center;">$195.1 \pm 13.1$</td>
<td style="text-align: center;">$294.8 \pm 1.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">company CEO</td>
<td style="text-align: center;">298</td>
<td style="text-align: center;">$90.2 \pm 7.2$</td>
<td style="text-align: center;">$15.4 \pm 6.1$</td>
<td style="text-align: center;">$155.5 \pm 4.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">superhero person</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$49.8 \pm 2.0$</td>
<td style="text-align: center;">$36.6 \pm 2.4$</td>
<td style="text-align: center;">$71.9 \pm 2.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">superhero archnemesis</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">$21.5 \pm 2.6$</td>
<td style="text-align: center;">$12.7 \pm 2.2$</td>
<td style="text-align: center;">$40.8 \pm 2.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person university</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">$41.0 \pm 1.6$</td>
<td style="text-align: center;">$35.4 \pm 1.9$</td>
<td style="text-align: center;">$44.8 \pm 2.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">pokemon evolution</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">$28.7 \pm 3.9$</td>
<td style="text-align: center;">$30.3 \pm 1.1$</td>
<td style="text-align: center;">$35.7 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">country currency</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">$19.4 \pm 0.9$</td>
<td style="text-align: center;">$20.2 \pm 0.8$</td>
<td style="text-align: center;">$22.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">food from country</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">$16.4 \pm 1.1$</td>
<td style="text-align: center;">$11.2 \pm 1.4$</td>
<td style="text-align: center;">$18.8 \pm 1.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">city in country</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">$18.0 \pm 0.9$</td>
<td style="text-align: center;">$18.0 \pm 1.1$</td>
<td style="text-align: center;">$18.1 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">country capital city</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">$16.0 \pm 0.0$</td>
<td style="text-align: center;">$15.5 \pm 0.6$</td>
<td style="text-align: center;">$15.3 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">country language</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">$15.4 \pm 0.6$</td>
<td style="text-align: center;">$14.9 \pm 0.6$</td>
<td style="text-align: center;">$15.8 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">country largest city</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">$15.5 \pm 0.5$</td>
<td style="text-align: center;">$14.0 \pm 0.8$</td>
<td style="text-align: center;">$15.3 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person lead singer of band</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">$13.0 \pm 0.2$</td>
<td style="text-align: center;">$10.1 \pm 0.9$</td>
<td style="text-align: center;">$13.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">president birth year</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">$11.0 \pm 0.0$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">**</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">president election year</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">$9.5 \pm 0.5$</td>
<td style="text-align: center;">$9.9 \pm 0.6$</td>
<td style="text-align: center;">**</td>
</tr>
<tr>
<td style="text-align: center;">commonsense</td>
<td style="text-align: center;">object superclass</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">$52.7 \pm 1.5$</td>
<td style="text-align: center;">$51.5 \pm 2.2$</td>
<td style="text-align: center;">$54.4 \pm 1.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">word sentiment</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">$47.2 \pm 4.1$</td>
<td style="text-align: center;">$42.8 \pm 5.1$</td>
<td style="text-align: center;">$50.5 \pm 2.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">task done by tool</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">$30.6 \pm 1.5$</td>
<td style="text-align: center;">$25.8 \pm 1.9$</td>
<td style="text-align: center;">$33.7 \pm 1.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">substance phase of matter</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$30.4 \pm 6.8$</td>
<td style="text-align: center;">$34.2 \pm 3.3$</td>
<td style="text-align: center;">$40.5 \pm 1.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">work location</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">$18.2 \pm 2.6$</td>
<td style="text-align: center;">$19.7 \pm 2.6$</td>
<td style="text-align: center;">$24.7 \pm 2.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fruit inside color</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">$10.2 \pm 1.0$</td>
<td style="text-align: center;">$9.0 \pm 0.0$</td>
<td style="text-align: center;">$17.3 \pm 1.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">task person type</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">$18.0 \pm 1.2$</td>
<td style="text-align: center;">$16.1 \pm 1.5$</td>
<td style="text-align: center;">$19.2 \pm 2.1$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fruit outside color</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">$11.7 \pm 2.1$</td>
<td style="text-align: center;">$9.6 \pm 0.7$</td>
<td style="text-align: center;">$14.6 \pm 1.4$</td>
</tr>
<tr>
<td style="text-align: center;">linguistic</td>
<td style="text-align: center;">word first letter</td>
<td style="text-align: center;">241</td>
<td style="text-align: center;">$223.9 \pm 4.5$</td>
<td style="text-align: center;">$199.1 \pm 9.8$</td>
<td style="text-align: center;">$233.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">word last letter</td>
<td style="text-align: center;">241</td>
<td style="text-align: center;">$28.2 \pm 8.2$</td>
<td style="text-align: center;">$21.2 \pm 5.3$</td>
<td style="text-align: center;">$188.3 \pm 6.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">adjective antonym</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$64.0 \pm 2.3$</td>
<td style="text-align: center;">$57.5 \pm 2.4$</td>
<td style="text-align: center;">$68.5 \pm 1.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">adjective superlative</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">$70.5 \pm 0.9$</td>
<td style="text-align: center;">$64.4 \pm 3.3$</td>
<td style="text-align: center;">$70.5 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">verb past tense</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">$61.0 \pm 4.6$</td>
<td style="text-align: center;">$54.0 \pm 3.0$</td>
<td style="text-align: center;">$65.8 \pm 3.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">adjective comparative</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">$59.5 \pm 0.6$</td>
<td style="text-align: center;">$57.6 \pm 0.9$</td>
<td style="text-align: center;">$60.0 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;">bias</td>
<td style="text-align: center;">occupation age</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">$25.7 \pm 2.6$</td>
<td style="text-align: center;">$22.9 \pm 3.8$</td>
<td style="text-align: center;">$32.8 \pm 2.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">univ degree gender</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$21.5 \pm 2.4$</td>
<td style="text-align: center;">$24.2 \pm 2.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">name birthplace</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">$17.1 \pm 2.6$</td>
<td style="text-align: center;">$18.0 \pm 1.4$</td>
<td style="text-align: center;">$21.4 \pm 1.1$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">name religion</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">$17.0 \pm 2.3$</td>
<td style="text-align: center;">$15.1 \pm 2.2$</td>
<td style="text-align: center;">$19.8 \pm 1.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">characteristic gender</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">$15.9 \pm 2.7$</td>
<td style="text-align: center;">$15.8 \pm 2.2$</td>
<td style="text-align: center;">$19.7 \pm 1.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">name gender</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">$11.0 \pm 0.0$</td>
<td style="text-align: center;">$10.7 \pm 0.6$</td>
<td style="text-align: center;">$10.8 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">occupation gender</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">$9.6 \pm 0.8$</td>
<td style="text-align: center;">$9.8 \pm 0.7$</td>
<td style="text-align: center;">$10.8 \pm 0.4$</td>
</tr>
</tbody>
</table>
<h1>B ASSUMPTIONS UNDERLYING THE LRE APPROXIMATION</h1>
<p>Our estimate of the LRE parameters is based on an assumption that the transformer LM implements relation decoding $F(\mathbf{s}, c)$ in a near-linear fashion that deviates from a linear model with a nonlinear error term $\varepsilon(\mathbf{s})$ where both $\varepsilon$ and $\varepsilon^{\prime}$ are zero in expectation over $\mathbf{s}$, i.e., $\mathbb{E}<em _mathbf_s="\mathbf{s">{\mathbf{s}}[\varepsilon(\mathbf{s})]=0$ and $\mathbb{E}</em>)\right]=0$.}}\left[\varepsilon^{\prime}(\mathbf{s</p>
<p>$$
F(\mathbf{s}, c)=b+W \mathbf{s}+\varepsilon(\mathbf{s})
$$</p>
<p>Then passing to expectations over the distribution of $\mathbf{s}$ we can estimate $b$ and $W$ :</p>
<p>$$
\begin{aligned}
b &amp; =F(\mathbf{s}, c)-W \mathbf{s}-\varepsilon(\mathbf{s}) \
b &amp; =\mathbb{E}<em _mathbf_s="\mathbf{s">{\mathbf{s}}[F(\mathbf{s}, c)-W \mathbf{s}]-\mathbb{E}</em>)] \
W &amp; =F^{\prime}(\mathbf{s}, c)-\varepsilon^{\prime}(\mathbf{s}) \
W &amp; =\mathbb{E}}}[\varepsilon(\mathbf{s<em _mathbf_s="\mathbf{s">{\mathbf{s}}\left[F^{\prime}(\mathbf{s}, c)\right]-\mathbb{E}</em>)\right]
\end{aligned}
$$}}\left[\varepsilon^{\prime}(\mathbf{s</p>
<p>Equations 11 and 13 correspond to the bias term $b$ and and projection term $W$ of Equation (3) in the main paper.</p>
<h2>C IMPROVING THE ESTIMATE OF $F^{\prime}(\mathbf{s}, c)$ AS $\beta W$</h2>
<p>Empirically we have found that $\beta W$ (with $\beta&gt;1$ ) yields a more accurate linear model of $F$ than $W$. In this section we measure the behavior of $F^{\prime}$ in the region between subject representation vectors to provide some evidence on this.
Take two subject representation vectors $\mathbf{s}<em 2="2">{1}$ and $\mathbf{s}</em>$, yields this estimate of transformer's behavior when traversing from one to the other}$. The projection term of our LRE model $W$, based on the mean Jacobian of $F$ calculated at subjects $\mathbf{s}_{i</p>
<p>$$
F\left(\mathbf{s}<em 1="1">{2}\right)-F\left(\mathbf{s}</em>}\right) \approx W\left(\mathbf{s<em 1="1">{2}-\mathbf{s}</em>\right)
$$</p>
<p>We can compare this to an exact calculation: the fundamental theorem of line integrals tells us that integrating the actual Jacobian along the path from $\mathbf{s}<em 2="2">{1}$ to $\mathbf{s}</em>$ yields the actual change:</p>
<p>$$
\begin{aligned}
F\left(\mathbf{s}<em 1="1">{2}\right)-F\left(\mathbf{s}</em>}\right) &amp; =\int_{\mathbf{s<em 2="2">{1}}^{\mathbf{s}</em> \
\left|F\left(\mathbf{s}}} F^{\prime}(\mathbf{s}) d \mathbf{s<em 1="1">{2}\right)-F\left(\mathbf{s}</em>}\right)\right| &amp; =\int_{\mathbf{s<em 2="2">{1}}^{\mathbf{s}</em>
\end{aligned}
$$}} \mathbf{u}^{T} F^{\prime}(\mathbf{s}) d \mathbf{s</p>
<p>Here we reduce it to a one-dimensional problem, defining unit vectors $\mathbf{u} \propto F\left(\mathbf{s}<em 1="1">{2}\right)-F\left(\mathbf{s}</em>}\right)$ and $\mathbf{v} \propto \mathbf{s<em 1="1">{2}-\mathbf{s}</em>$ in the row and column space respectively, so that</p>
<p>$$
\left|F\left(\mathbf{s}<em 1="1">{2}\right)-F\left(\mathbf{s}</em>}\right)\right| \approx \mathbf{u}^{T} W \mathbf{v}\left|\mathbf{s<em 1="1">{2}-\mathbf{s}</em>\right|
$$</p>
<p>By taking the ratio between the sides of (17) we can see how the actual rate of change from $\mathbf{s}<em 2="2">{1}$ to $\mathbf{s}</em>$ is underestimated by $W$. Table 5 reports this value for some selected relations.</p>
<p>In practice we find that setting $\beta$ as a constant for an LM (instead of setting it per relation) is enough to attain good performance across a range of relations. Refer to Appendix E for further details.</p>
<p>Table 5: Ratio between the right hand sides of Equation (16) and (17) for some of the relations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Relation</th>
<th style="text-align: center;">Underestimation Ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">plays pro sport</td>
<td style="text-align: center;">$2.517 \pm 1.043$</td>
</tr>
<tr>
<td style="text-align: left;">country capital city</td>
<td style="text-align: center;">$4.198 \pm 0.954$</td>
</tr>
<tr>
<td style="text-align: left;">object superclass</td>
<td style="text-align: center;">$3.058 \pm 0.457$</td>
</tr>
<tr>
<td style="text-align: left;">name birthplace</td>
<td style="text-align: center;">$4.328 \pm 0.991$</td>
</tr>
</tbody>
</table>
<h2>D CAUSALITY</h2>
<h2>D. 1 DERIVATION OF EQN 7</h2>
<p>Under the same relation $r=$ person plays instrument, consider two different $(s, o)$ pairs: ( $s=$ Miles Davis, $o=$ trumpet) and ( $s^{\prime}=$ Cat Stevens, $o^{\prime}=$ guitar). If an LRE defined by the projection term $W$ and translation term $b$ is an well-approximation of the model calculation $F(\mathbf{s}, c)$, then</p>
<p>$$
\mathbf{o}=\beta W(\mathbf{s})+b \text { and } \mathbf{o}^{\prime}=\beta W\left(\mathbf{s}^{\prime}\right)+b
$$</p>
<p>Subtracting $\mathbf{o}^{\prime}$ from $\mathbf{o}$</p>
<p>$$
\begin{aligned}
\mathbf{o}^{\prime}-\mathbf{o} &amp; =\beta W \mathbf{s}^{\prime}-\beta W \mathbf{s} \
&amp; =\beta W\left(\mathbf{s}^{\prime}-\mathbf{s}\right) \text { [since } W \text { is linear] } \
\Delta \mathbf{s} &amp; =\mathbf{s}^{\prime}-\mathbf{s}=\frac{1}{\beta} W^{-1}\left(\mathbf{o}^{\prime}-\mathbf{o}\right)
\end{aligned}
$$</p>
<p>We observe that the edit direction $\Delta \mathbf{s}$ needs to be magnified to achieve good edit efficacy. In our experiments we magnify $\Delta \mathbf{s}$ by $\beta$ (or set $\beta=1.0$ in Equation (18)).</p>
<p>$$
\Delta \mathbf{s}=W^{-1}\left(\mathbf{o}^{\prime}-\mathbf{o}\right)
$$</p>
<h1>D. 2 WHY LOW-RANK INVERSE $W^{\dagger}$ INSTEAD OF FULL INVERSE $W^{-1}$ IS NECESSARY?</h1>
<p>In practice, we need to take a low rank approximation $W^{\dagger}$ instead of $W^{-1}$ for the edit depicted in Figure 2 to be successful. With $\beta$ set to 1.0 ,</p>
<p>$$
\begin{aligned}
\mathbf{o}^{\prime}-\mathbf{o} &amp; =W\left(\mathbf{s}^{\prime}-\mathbf{s}\right) \
\Delta \mathbf{o} &amp; =W \Delta \mathbf{s}
\end{aligned}
$$</p>
<p>If we take $U \Sigma V^{T}$ as the SVD of $W$, then</p>
<p>$$
\begin{aligned}
\Delta \mathbf{o} &amp; =U \Sigma V^{T} \Delta \mathbf{s} \
U^{T} \Delta \mathbf{o} &amp; =\Sigma V^{T} \Delta \mathbf{s}
\end{aligned}
$$</p>
<p>Considering $U^{T} \Delta \mathbf{o}$ as $\mathbf{o}<em v="v">{u}$ and $V^{T} \Delta \mathbf{s}$ as $\mathbf{s}</em>$,</p>
<p>$$
\mathbf{o}<em v="v">{u}=\Sigma \mathbf{s}</em>} \text { or } \Sigma^{-1} \mathbf{o<em v="v">{u}=\mathbf{s}</em>
$$</p>
<p>Here, $\Sigma$ maps $\mathbf{s}<em u="u">{v}$ to $\mathbf{o}</em>}$. This $\Sigma$ is a diagonal matrix that contains the non-negative singular values in it's diagonal and zero otherwise. The greater the singular value the more its effect on $\mathbf{s<em u="u">{v}$. However, if we take the full rank inverse of $\Sigma$ while mapping $\mathbf{o}</em>}$ to $\mathbf{s<em r="r">{v}$ then the inverse becomes dominated by noisy smaller singular values and they wash out the contribution of meaningful singular values. Thus, it is necessary to consider only those singular values greater than a certain threshold $\tau$ or take a low rank inverse $W^{\dagger}$ instead of a full inverse $W^{-1}$.
The significance of different ranks on causality is depicted on Figure 9. We see that the causality increases with a rank up to some point and starts decreasing afterwards. This suggests that, we are ablating important singular values from $\Sigma$ before an optimal rank $\rho</em>$ is reached, and start introducing noisy singular values afterwards.</p>
<h2>E SELECTING HYPERPARAMETERS $\left(\beta, \ell_{r}\right.$, AND $\left.\rho_{r}\right)$</h2>
<p>We need to select a scalar value $\beta$ per LM since the slope $W$ of the first order approximation underestimates the slope of $F(\mathbf{s}, c)$ (Appendix C). Additionally, we need to specify two hyperparameters per relation $r ; \ell_{r}$, the layer after which $\mathbf{s}$ is to be extracted and $\rho_{r}$, the rank of the low-rank inverse $W^{\dagger}$.
We perform a grid search to select these hyperparameters. For a specific $\beta$, hyperparameters $\ell_{r}$ and $\rho_{r}$ are selected to achieve the best causal influence as there is a strong agreement between faithfulness and causality when the hparams are selected this way. However, when $\ell_{r}$ and $\rho_{r}$ are selected to achieve best faithfulness there is a weaker agreement between our evaluation metrics (Figure 10). Appendix E. 2 provides an insight on this.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Initially, faithfulness and causality of LRE improve with higher rank. However, after $\operatorname{rank}=2^{8}$ causality starts declining whereas faithfulness remains stable.</p>
<p>In sweep over layers we notice that LRE performance per relation increases up to a certain layer and drops afterwards, suggesting a mode-switch in later layers (Figure 11). And, in the sweep over ranks we notice that both edit efficacy and faithfulness increases up to a certain rank. After that edit efficacy starts dropping while faithfulness remains stable. The reasoning behind how causality is affected by higher rank is discussed in Appendix D.2.</p>
<h1>E. 1 SELECTING $\beta$</h1>
<p>Table 6 represents how performance scores of LRE change with respect to different values of $\beta$ for GPT-J. In our experiments, causality is always calculated with $\beta$ set to 1.0. So, the average causality score remain constant. $\beta$ is selected per LM to achieve the best agreement between our performance metrics faithfulness and causality. For GPT-J optimal value of $\beta$ is 2.25 .</p>
<h2>E. 2 LAYER-WISE LRE PERFORMANCE ON SELECTED RELATIONS (GPT-J)</h2>
<p>If LRE remains faithful to model up to layer $\ell_{f a i t h}$ it is reasonable to expect that LRE will retain high causality until $\ell_{f a i t h}$ as well. However, an examination of the faithfulness and causality performances across layers reveals that the causality scores drop before $\ell_{f a i t h}$ (Fig. 11). In fact Fig. 14 from Appendix G. 2 indicates that all the intervention baselines exhibit a decrease in performance at deeper layers, particularly our method and the oracle method at similar layers. This might be a phenomenon associated with this type of intervention in general, rather than a fault with our approximation of the target $\mathbf{s}^{\prime}$. Notice that, in all of our activation patching experiments we only patch a single state (at the position of the last subject token after a layer $\ell$ ).
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: When hparams are selected to achieve best faithfulness there is a weaker correlation of 0.74 between our evaluation metrics unlike in Figure 6 where hparams were selected to achieve best causality. For both this figure and Figure 6 the LM is GPT-J and $\beta=2.25$.</p>
<p>Table 6: Scores achieved by LRE on different values of $\beta$ on GPT-J. $\beta=2.25$ shows the best correlation between our evaluation metrics. Faithfulness ${ }<em _mu="\mu">{\mu}$ means the average faithfulness across all the relations (same for Causality ${ }</em>$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$\beta$</th>
<th style="text-align: center;">Faithfulness ${ }_{\mu}$</th>
<th style="text-align: center;">Causality ${ }_{\mu}$</th>
<th style="text-align: center;">Corr</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">$0.17 \pm 0.20$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">$0.21 \pm 0.22$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">$0.28 \pm 0.24$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.38</td>
</tr>
<tr>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">$0.36 \pm 0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.48</td>
</tr>
<tr>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.43 \pm 0.26$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: center;">1.25</td>
<td style="text-align: center;">$0.50 \pm 0.26$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">$0.54 \pm 0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: center;">1.75</td>
<td style="text-align: center;">$0.57 \pm 0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">$0.59 \pm 0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;">2.25</td>
<td style="text-align: center;">$0.59 \pm 0.25$</td>
<td style="text-align: center;">$0.81$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">2.50</td>
<td style="text-align: center;">$0.59 \pm 0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: center;">2.75</td>
<td style="text-align: center;">$0.59 \pm 0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">$0.59 \pm 0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;">3.25</td>
<td style="text-align: center;">$0.58 \pm 0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.81</td>
</tr>
<tr>
<td style="text-align: center;">3.50</td>
<td style="text-align: center;">$0.57 \pm 0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;">3.75</td>
<td style="text-align: center;">$0.56 \pm 0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">$0.55 \pm 0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: center;">4.25</td>
<td style="text-align: center;">$0.54 \pm 0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: center;">4.50</td>
<td style="text-align: center;">$0.53 \pm 0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.74</td>
</tr>
<tr>
<td style="text-align: center;">4.75</td>
<td style="text-align: center;">$0.53 \pm 0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.72</td>
</tr>
<tr>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">$0.52 \pm 0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.71</td>
</tr>
</tbody>
</table>
<p>If the activation after layer $\ell$ is patched, layers till $\ell-1$ retains information about the original subject $\mathbf{s}$ and they can leak information about $\mathbf{s}$ to later layers because of attention mechanism. The deeper</p>
<p>we intervene the more is this leakage from previous states and it might reduce the efficacy of these single state activation patching approaches.
It is not reasonable to expect high causality after $\ell_{f a i t h}$, and causality can drop well before $\ell_{f a i t h}$ because of this leakage. This also partly explains the disagreement between the two metrics when the hyperparameters are chosen to achieve the best faithfulness (Figure 10).
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: LRE performance for selected relations in different layers of GPT-J. The last row features some of the relations where LRE could not achieve satisfactory performance indicating a non-linear decoding process for them.</p>
<h1>F VARYING $n$ AND PROMPT TEMPLATE</h1>
<p>Figure 12 shows how lre performance changes based on number of examples $n$ used for approximation. For most of the relations both faithfulness and efficacy scores start plateauing after $n=5$. In our experiment setup we use $n=8$ as that is the largest number we could fit for a GPT-J model on a single A6000. However, Figure 12 suggests that a good LRE estimation may be obtained with less number of examples.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: LRE performance across different $n$.
We also test how LRE performance on a relation $r$ changes when the same relation $r$ is contextualized with different prompt templates. Table 7 shows minimal change in faithfulness and causality scores when LRE is calculated with different prompt templates.</p>
<p>Table 7: LRE performance on different prompt templates. The subject $s$ is inserted in place of ( ). Performance scores presented as mean and standard deviation across 24 trials with different sets of training examples.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Relation</th>
<th style="text-align: center;">Prompt Template</th>
<th style="text-align: center;">Faithfulness</th>
<th style="text-align: center;">Causality</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">country capital city</td>
<td style="text-align: center;">The capital of $}$ is</td>
<td style="text-align: center;">$0.84 \pm 0.09$</td>
<td style="text-align: center;">$0.94 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The capital of $}$ is the city of</td>
<td style="text-align: center;">$0.87 \pm 0.08$</td>
<td style="text-align: center;">$0.94 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The capital city of $}$ is</td>
<td style="text-align: center;">$0.84 \pm 0.08$</td>
<td style="text-align: center;">$0.94 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">What is the capital of $}$ ? It is the city of</td>
<td style="text-align: center;">$0.87 \pm 0.07$</td>
<td style="text-align: center;">$0.92 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">plays pro sport</td>
<td style="text-align: center;">() plays the sport of</td>
<td style="text-align: center;">$0.78 \pm 0.07$</td>
<td style="text-align: center;">$0.90 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">() plays professionally in the sport of</td>
<td style="text-align: center;">$0.78 \pm 0.09$</td>
<td style="text-align: center;">$0.90 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">What sport does () play? They play</td>
<td style="text-align: center;">$0.81 \pm 0.06$</td>
<td style="text-align: center;">$0.90 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">person occupation</td>
<td style="text-align: center;">() works professionally as a</td>
<td style="text-align: center;">$0.41 \pm 0.08$</td>
<td style="text-align: center;">$0.55 \pm 0.09$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">() works as a</td>
<td style="text-align: center;">$0.44 \pm 0.11$</td>
<td style="text-align: center;">$0.58 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">By profession, () is a</td>
<td style="text-align: center;">$0.46 \pm 0.14$</td>
<td style="text-align: center;">$0.58 \pm 0.08$</td>
</tr>
<tr>
<td style="text-align: center;">adjective superlative</td>
<td style="text-align: center;">The superlative form of () is</td>
<td style="text-align: center;">$0.93 \pm 0.02$</td>
<td style="text-align: center;">$0.97 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">What is the superlative form of ()? It is</td>
<td style="text-align: center;">$0.92 \pm 0.02$</td>
<td style="text-align: center;">$0.96 \pm 0.03$</td>
</tr>
</tbody>
</table>
<h1>G BASELINES</h1>
<h2>G. 1 FAITHFULNESS</h2>
<p>In Figure 13 we examine whether our method can be applied to s extracted from zero-shot prompts that contain only the subject $s$ and no further context. It appears that even when LRE is trained with few-shot examples, it can achieve similar results when applied to $s$ that is free of any context specifying the relation.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: LRE faithfulness on GPT-J compared with different linear functions baselines(same as Figure 4). Each of the functions were approximated with $n=8$ samples, each prepended with $n-1$ few-shot examples (Table 2). Dark blue bars indicate faithfulness when evaluated on s extracted in a similar setup. Light blue bars represent how LRE (trained on few-shot examples) generalize when applied on s extracted from zero-shot prompts that contain only the subject and no further context.</p>
<h2>G. 2 CAUSALITY</h2>
<p>Figure 14 shows the LRE causality performance in comparison to other baselines for selected relations and across different layers. If LRE is a good approximation of the model computation, then our causality intervention should be equivalent to the oracle baseline, which replaces s with $\mathrm{s}^{\prime}$. The graphs demonstrate the similarity between our method performance and oracle performance across all layers. This provides causal evidence that LRE can reliably recover $\mathrm{s}^{\prime}$ for these relations.
While model editing is not the primary focus of this work, it is still worth examining how our intervention may affect multiple token generation, since it may reveal unexpected side effects of the method. Based on a qualitative analysis of the post-edit generations, it appears that the edits preserve the fluency of the model. Table 8 presents examples of generated texts after intervention.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14: LRE causality across different layers of GPT-J. The causality curve closely matches the peaks and valleys of the oracle baseline, replacing $s$ with $s^{\prime}$, suggesting that LRE is a good approximation of the model computation $F$.</p>
<p>Table 8: Generated texts, before and after our causal intervention on GPT-J to change its prediction to $o^{\prime}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt</th>
<th style="text-align: left;">$o \rightarrow o^{\prime}$</th>
<th style="text-align: left;">Before</th>
<th style="text-align: left;">After</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Miles Davis plays the</td>
<td style="text-align: left;">trumpet $\rightarrow$ guitar</td>
<td style="text-align: left;">trumpet in his band.</td>
<td style="text-align: left;">guitar live with his band.</td>
</tr>
<tr>
<td style="text-align: left;">Siri was created by</td>
<td style="text-align: left;">Apple $\rightarrow$ Google</td>
<td style="text-align: left;">Apple as a personal as- <br> sistant.</td>
<td style="text-align: left;">Google and it has be- <br> come a huge success <br> within Google Maps.</td>
</tr>
<tr>
<td style="text-align: left;">Chris Martin is the <br> lead singer of</td>
<td style="text-align: left;">Coldplay $\rightarrow$ Foo <br> Fighters</td>
<td style="text-align: left;">Coldplay and a man of <br> many talents.</td>
<td style="text-align: left;">Foo Fighters, one of the <br> most successful and pop- <br> ular rock bands in the <br> world.</td>
</tr>
<tr>
<td style="text-align: left;">What is the past tense <br> of close? It is</td>
<td style="text-align: left;">closed $\rightarrow$ walked</td>
<td style="text-align: left;">closed. It means it gets <br> closed or is closed.</td>
<td style="text-align: left;">walked. What is the past <br> tense of read?</td>
</tr>
</tbody>
</table>
<h1>H LRE ON GPT2-XL AND LLAMA-13B</h1>
<p>We provide further results for GPT2-xl and LLaMA-13b to show that autoregressive LMs of different sizes employ this linear encoding scheme for a range of different relations. Figure 15 present LRE performances for each of the three models grouped by relation category.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 15: LRE performance in different relation categories on different LMs.</p>
<p>Figures 16a and 16a illustrate the high correlation between our two metrics on GPT2-xl and LLaMA13b respectively. These findings are consistent with the results reported for GPT-J (Figure 6).
Similarly, Figures 17a and 17b, compare the faithfulness of our method with other approaches of achieving a linear decoding scheme for GPT2-xl and LlaMA.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 16: High correlation between faithfulness and causality in both GPT2-xl (R=0.85) and LLaMa13B (R=0.83). Each of the dots represent LRE performance for a relation. Bold dots indicate relations for which LRE is evaluated on $\geq 30$ test examples.</p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 17: LRE faithfulness on GPT2-xl and LLaMA-13B compared with different baselines. Refer to Figure Figure 4 for details on these baseline approaches of achieving a linear decoding function.</p>
<p>Lastly, Figure 18 depicts LRE faithfulness in GPT2-xl and LLaMA-13b for each of relations in our dataset. According to Spearman's rank-order correlation, GPT-J's relation-wise performance is strongly correlated with both GPT2-xl $(R=0.85)$ and LLaMa-13B $(R=0.71)$, whereas GPT2-xl and LLaMa-13B are moderately corelated $(R=0.58)$.</p>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 18: Relation-wise LRE faithfulness to the LM relation decoding function $F$. Horizontal red lines per relation indicate accuracy of a random-guess baseline. Relations are ordered according to their LRE faithfulness in GPT-J. We do not calculate LRE estimation for GPT2-xl on the relation president birth year as GPT2-xl can not accurately answer for that relation (Table 4). We also do not calculate LRE of LLaMa-13B for the relations where the $o$ is a year (i.e. president birth year and president election year) as tokenizer of LLaMA models splits the digits of a year. Such behavior of LLaMA tokenizer makes the relation decoding function trivial, since most of the answers start with " 1 " for these relations. Those cases were included in this plot (grayed out) to align LRE faithfulness of different LMs by relations.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ However, when the hyperparameters are chosen to achieve best faithfulness we did not notice such strong agreement between faithfulness and causality. Discussion on Appendix E.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
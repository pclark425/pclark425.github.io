<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9243 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9243</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9243</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-265067041</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.05160v1.pdf" target="_blank">RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information</a></p>
                <p><strong>Paper Abstract:</strong> As the IT industry advances, system log data becomes increasingly crucial. Many computer systems rely on log texts for management due to restricted access to source code. The need for log anomaly detection is growing, especially in real-world applications, but identifying anomalies in rapidly accumulating logs remains a challenging task. Traditional deep learning-based anomaly detection models require dataset-specific training, leading to corresponding delays. Notably, most methods only focus on sequence-level log information, which makes the detection of subtle anomalies harder, and often involve inference processes that are difficult to utilize in real-time. We introduce RAPID, a model that capitalizes on the inherent features of log data to enable anomaly detection without training delays, ensuring real-time capability. RAPID treats logs as natural language, extracting representations using pre-trained language models. Given that logs can be categorized based on system context, we implement a retrieval-based technique to contrast test logs with the most similar normal logs. This strategy not only obviates the need for log-specific training but also adeptly incorporates token-level information, ensuring refined and robust detection, particularly for unseen logs. We also propose the core set technique, which can reduce the computational cost needed for comparison. Experimental results show that even without training on log data, RAPID demonstrates competitive performance compared to prior models and achieves the best performance on certain datasets. Through various research questions, we verified its capability for real-time detection without delay.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9243.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9243.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAPID</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training-free, retrieval-based anomaly detection method that uses pre-trained language model (PLM) embeddings and token-level late-interaction (maxSim) similarity to detect anomalous log sequences in real time; it selects nearest normal-type candidates via CLS KNN (core set) and computes token-wise max similarities for scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (primary); also evaluated with RoBERTa and ELECTRA</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (pre-trained language model / PLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Textual token sequences (system log lines / aggregated log blocks)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs from HPC and distributed systems (BGL, Thunderbird, HDFS datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Unexpected/abnormal log events (outlier log sequences or abnormal log blocks)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Training-free retrieval: (1) Preprocess logs by replacing parameters with headers, build unique-document DB of known normal log sequences and unique-query DB for test period. (2) Encode sequences with a PLM to get CLS and token embeddings. (3) For each query, select a small 'core set' of candidate documents by KNN using CLS Euclidean distance. (4) Compute token-level maxSim similarity (for each query token take max cosine against document tokens), define distance = 1 - maxSim, and take the minimum distance across core set as the abnormal score. (5) Threshold the score to classify anomalies. Core set reduces expensive token-token computations enabling real-time inference.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Supervised: LogRobust, HitAnomaly, LogSy, NeuralLog, LogPal, LayerLog; Unsupervised: PCA, iForest, OCSVM, LogCluster, DeepLog, LogAnomaly, LogBERT, LAnoBERT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Threshold-dependent Best F1 (precision/recall harmonic mean) and threshold-independent AUROC; also inference throughput (logs/sec) and inference latency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Best F1 scores reported: BGL 0.9999, Thunderbird 0.9975, HDFS 0.9240. AUROC values reported for PLM variants (BERT-based RAPID AUROC: BGL 0.9999, Thunderbird 0.9994, HDFS 0.9295). Inference: with core-set ratio 0.01, throughput ~12,000 test logs/sec (BGL) and ~3,000 test logs/sec (Thunderbird); inference time up to 10x reduction vs. full D on larger datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>RAPID is competitive with and often better than prior supervised and unsupervised methods: it achieved the best F1 on BGL and second-best on Thunderbird (behind LAnoBERT) and outperformed several unsupervised baselines (LogCluster, PCA, iForest, OCSVM) and many supervised models in reported comparisons; it also matched or exceeded PLM-based baselines (LogBERT) despite not performing any log-specific training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Designed primarily for single-log-sequence inputs — HDFS (block-aggregated logs) is less ideal and shows different behavior; raw maxSim computations are expensive (mitigated by core set), and CLS-based core set selection may not always perfectly match token-level nearest by maxSim (addressed by selecting K neighbors rather than 1); relies on existence of similar normal-type sequences in the known-normal DB — performance may degrade if an appropriate normal type is entirely absent; model size and fine-grained domain adaptation not explored (no fine-tuning performed).</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Shows that general-purpose PLMs can be used without any log-specific training to obtain state-of-the-art or competitive log anomaly detection by (a) reframing detection as retrieval against known-normal types, (b) using token-level late interaction (maxSim) to capture subtle token differences, and (c) applying a CLS-based core-set KNN to make token-level comparisons tractable for real-time use; token-level information is especially valuable when vocabulary or unseen-test-log proportion increases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9243.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9243.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (as used in RAPID)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose transformer-based pre-trained language model used to encode log sequences into CLS (sequence) and token embeddings; these embeddings drive core-set selection and token-level maxSim similarity in RAPID.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (bidirectional encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Textual token sequences (logs)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs (BGL, Thunderbird, HDFS)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous log sequences / token-level anomalies</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Used off-the-shelf (no fine-tuning) to produce CLS and per-token contextual embeddings; CLS used to select candidate normal documents (core set) and token embeddings used for late-interaction maxSim similarity scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared in PLM-ablation experiments to RoBERTa and ELECTRA (used as alternative encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1 and AUROC reported for RAPID when using BERT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>RAPID with BERT: F1 BGL 0.9999 (AUROC 0.9999), Thunderbird F1 0.9979 (AUROC 0.9994), HDFS F1 0.9240 (AUROC 0.9295) (Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Other general-purpose PLMs (RoBERTa, ELECTRA) produced similarly robust results; the paper reports minimal dependence on a specific PLM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Max token lengths limited (set to 128 for BGL/Thunderbird, 512 for HDFS) which constrains input length; no fine-tuning means representation may not capture very domain-specific token meanings beyond PLM pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>A widely-used general-purpose PLM (BERT) suffices as an encoder for competitive, training-free log anomaly detection when combined with retrieval and token-level late interaction; CLS alone can be a strong proxy for candidate selection in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9243.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9243.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LAnoBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lanobert: System log anomaly detection based on bert masked language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-based, log-specific trained model that uses masked language modeling (MLM) on normal logs and leverages token-mask probabilities during inference for token-level anomaly scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lanobert: System log anomaly detection based on bert masked language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LAnoBERT (BERT fine-tuned with MLM on logs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (BERT, MLM fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Textual log sequences (token sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs (benchmarks: BGL, Thunderbird, HDFS)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Token-level anomalies (unexpected tokens / token probabilities)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tune BERT using MLM on normal logs; at inference, mask tokens and use MLM probability (likelihood of masked token) as abnormality score.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared as a supervised/MLM-trained PLM baseline in experiments (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Best F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported F1: BGL 0.8749, Thunderbird 0.9990, HDFS 0.9645 (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>LAnoBERT achieves very strong performance (especially Thunderbird and HDFS), and outperforms RAPID on some datasets (e.g., Thunderbird F1 slightly higher), but requires log-specific training and heavier inference (mask/predict operations).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires dataset-specific training (MLM) on logs which incurs training delay; inference involves mask-and-predict token probabilities which may be computationally heavier than RAPID's retrieval + maxSim, limiting real-time scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Fine-tuned MLM approaches can yield strong token-level anomaly sensitivity, but come with training and inference overheads that RAPID avoids by using retrieval plus PLM embeddings without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9243.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9243.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logbert: Log anomaly detection via bert</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-based approach that integrates masked language modeling with DeepSVDD objective and typically uses a log parser; requires training on log data to learn normal patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logbert: Log anomaly detection via bert</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogBERT (BERT fine-tuned / trained for log anomaly)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (BERT + DeepSVDD objective)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Textual log sequences (templates + tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs (BGL, Thunderbird, HDFS)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Sequence-level anomalies (deviations from learned normal patterns)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Combine BERT masked language modeling with DeepSVDD loss during training and use a parser for template extraction; anomaly detection is based on learned representations and DeepSVDD-style scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Used as an unsupervised PLM-based baseline (reported performance in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Best F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported F1: BGL 0.9083, Thunderbird 0.9664, HDFS 0.8232 (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>RAPID (training-free) matches or exceeds LogBERT on several datasets, showing that retrieval + PLM embeddings can be competitive with log-specific PLM training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires log-specific training and often a log parser (parser can lose semantic info); training delays and online updating overhead limit real-time use compared to RAPID.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9243.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9243.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeuralLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised PLM-based binary classification approach for log anomaly detection that uses general-purpose PLMs to produce features for classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NeuralLog (general-purpose PLM used in a supervised classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (PLM used as feature extractor; classifier on top)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Textual log sequences</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs (benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Binary anomalous vs normal log classification</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use pre-trained language model embeddings as input features to a supervised binary classifier trained with both normal and abnormal logs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared as a supervised baseline in Table 1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Best F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported F1 values in Table 1: BGL 0.9800, Thunderbird 0.9600, HDFS 0.9800</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>RAPID outperforms NeuralLog on BGL and Thunderbird per reported F1s (RAPID: 0.9999/0.9975), but NeuralLog has higher HDFS F1 (0.98) than RAPID (0.9240); NeuralLog requires supervised training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Supervised approach needs labeled abnormal data for training and may generalize poorly to unseen anomaly types; depends on PLM fine-tuning or classifier training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logbert: Log anomaly detection via bert <em>(Rating: 2)</em></li>
                <li>Lanobert: System log anomaly detection based on bert masked language model <em>(Rating: 2)</em></li>
                <li>Colbert: Efficient and effective passage search via contextualized late interaction over bert <em>(Rating: 2)</em></li>
                <li>Deeplog: Anomaly detection and diagnosis from system logs through deep learning <em>(Rating: 2)</em></li>
                <li>Deep learning for anomaly detection: A survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9243",
    "paper_id": "paper-265067041",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "RAPID",
            "name_full": "RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information",
            "brief_description": "A training-free, retrieval-based anomaly detection method that uses pre-trained language model (PLM) embeddings and token-level late-interaction (maxSim) similarity to detect anomalous log sequences in real time; it selects nearest normal-type candidates via CLS KNN (core set) and computes token-wise max similarities for scoring.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT (primary); also evaluated with RoBERTa and ELECTRA",
            "model_type": "Transformer (pre-trained language model / PLM)",
            "model_size": null,
            "data_type": "Textual token sequences (system log lines / aggregated log blocks)",
            "data_domain": "System logs from HPC and distributed systems (BGL, Thunderbird, HDFS datasets)",
            "anomaly_type": "Unexpected/abnormal log events (outlier log sequences or abnormal log blocks)",
            "method_description": "Training-free retrieval: (1) Preprocess logs by replacing parameters with headers, build unique-document DB of known normal log sequences and unique-query DB for test period. (2) Encode sequences with a PLM to get CLS and token embeddings. (3) For each query, select a small 'core set' of candidate documents by KNN using CLS Euclidean distance. (4) Compute token-level maxSim similarity (for each query token take max cosine against document tokens), define distance = 1 - maxSim, and take the minimum distance across core set as the abnormal score. (5) Threshold the score to classify anomalies. Core set reduces expensive token-token computations enabling real-time inference.",
            "baseline_methods": "Supervised: LogRobust, HitAnomaly, LogSy, NeuralLog, LogPal, LayerLog; Unsupervised: PCA, iForest, OCSVM, LogCluster, DeepLog, LogAnomaly, LogBERT, LAnoBERT",
            "performance_metrics": "Threshold-dependent Best F1 (precision/recall harmonic mean) and threshold-independent AUROC; also inference throughput (logs/sec) and inference latency",
            "performance_results": "Best F1 scores reported: BGL 0.9999, Thunderbird 0.9975, HDFS 0.9240. AUROC values reported for PLM variants (BERT-based RAPID AUROC: BGL 0.9999, Thunderbird 0.9994, HDFS 0.9295). Inference: with core-set ratio 0.01, throughput ~12,000 test logs/sec (BGL) and ~3,000 test logs/sec (Thunderbird); inference time up to 10x reduction vs. full D on larger datasets.",
            "comparison_to_baseline": "RAPID is competitive with and often better than prior supervised and unsupervised methods: it achieved the best F1 on BGL and second-best on Thunderbird (behind LAnoBERT) and outperformed several unsupervised baselines (LogCluster, PCA, iForest, OCSVM) and many supervised models in reported comparisons; it also matched or exceeded PLM-based baselines (LogBERT) despite not performing any log-specific training.",
            "limitations_or_failure_cases": "Designed primarily for single-log-sequence inputs — HDFS (block-aggregated logs) is less ideal and shows different behavior; raw maxSim computations are expensive (mitigated by core set), and CLS-based core set selection may not always perfectly match token-level nearest by maxSim (addressed by selecting K neighbors rather than 1); relies on existence of similar normal-type sequences in the known-normal DB — performance may degrade if an appropriate normal type is entirely absent; model size and fine-grained domain adaptation not explored (no fine-tuning performed).",
            "unique_insights": "Shows that general-purpose PLMs can be used without any log-specific training to obtain state-of-the-art or competitive log anomaly detection by (a) reframing detection as retrieval against known-normal types, (b) using token-level late interaction (maxSim) to capture subtle token differences, and (c) applying a CLS-based core-set KNN to make token-level comparisons tractable for real-time use; token-level information is especially valuable when vocabulary or unseen-test-log proportion increases.",
            "uuid": "e9243.0",
            "source_info": {
                "paper_title": "RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "BERT (as used in RAPID)",
            "name_full": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "brief_description": "A general-purpose transformer-based pre-trained language model used to encode log sequences into CLS (sequence) and token embeddings; these embeddings drive core-set selection and token-level maxSim similarity in RAPID.",
            "citation_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "use",
            "model_name": "BERT",
            "model_type": "Transformer (bidirectional encoder)",
            "model_size": null,
            "data_type": "Textual token sequences (logs)",
            "data_domain": "System logs (BGL, Thunderbird, HDFS)",
            "anomaly_type": "Anomalous log sequences / token-level anomalies",
            "method_description": "Used off-the-shelf (no fine-tuning) to produce CLS and per-token contextual embeddings; CLS used to select candidate normal documents (core set) and token embeddings used for late-interaction maxSim similarity scoring.",
            "baseline_methods": "Compared in PLM-ablation experiments to RoBERTa and ELECTRA (used as alternative encoders)",
            "performance_metrics": "F1 and AUROC reported for RAPID when using BERT",
            "performance_results": "RAPID with BERT: F1 BGL 0.9999 (AUROC 0.9999), Thunderbird F1 0.9979 (AUROC 0.9994), HDFS F1 0.9240 (AUROC 0.9295) (Table 4)",
            "comparison_to_baseline": "Other general-purpose PLMs (RoBERTa, ELECTRA) produced similarly robust results; the paper reports minimal dependence on a specific PLM.",
            "limitations_or_failure_cases": "Max token lengths limited (set to 128 for BGL/Thunderbird, 512 for HDFS) which constrains input length; no fine-tuning means representation may not capture very domain-specific token meanings beyond PLM pretraining.",
            "unique_insights": "A widely-used general-purpose PLM (BERT) suffices as an encoder for competitive, training-free log anomaly detection when combined with retrieval and token-level late interaction; CLS alone can be a strong proxy for candidate selection in many settings.",
            "uuid": "e9243.1",
            "source_info": {
                "paper_title": "RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LAnoBERT",
            "name_full": "Lanobert: System log anomaly detection based on bert masked language model",
            "brief_description": "A BERT-based, log-specific trained model that uses masked language modeling (MLM) on normal logs and leverages token-mask probabilities during inference for token-level anomaly scoring.",
            "citation_title": "Lanobert: System log anomaly detection based on bert masked language model",
            "mention_or_use": "mention",
            "model_name": "LAnoBERT (BERT fine-tuned with MLM on logs)",
            "model_type": "Transformer (BERT, MLM fine-tuned)",
            "model_size": null,
            "data_type": "Textual log sequences (token sequences)",
            "data_domain": "System logs (benchmarks: BGL, Thunderbird, HDFS)",
            "anomaly_type": "Token-level anomalies (unexpected tokens / token probabilities)",
            "method_description": "Fine-tune BERT using MLM on normal logs; at inference, mask tokens and use MLM probability (likelihood of masked token) as abnormality score.",
            "baseline_methods": "Compared as a supervised/MLM-trained PLM baseline in experiments (Table 1)",
            "performance_metrics": "Best F1",
            "performance_results": "Reported F1: BGL 0.8749, Thunderbird 0.9990, HDFS 0.9645 (Table 1)",
            "comparison_to_baseline": "LAnoBERT achieves very strong performance (especially Thunderbird and HDFS), and outperforms RAPID on some datasets (e.g., Thunderbird F1 slightly higher), but requires log-specific training and heavier inference (mask/predict operations).",
            "limitations_or_failure_cases": "Requires dataset-specific training (MLM) on logs which incurs training delay; inference involves mask-and-predict token probabilities which may be computationally heavier than RAPID's retrieval + maxSim, limiting real-time scalability.",
            "unique_insights": "Fine-tuned MLM approaches can yield strong token-level anomaly sensitivity, but come with training and inference overheads that RAPID avoids by using retrieval plus PLM embeddings without fine-tuning.",
            "uuid": "e9243.2",
            "source_info": {
                "paper_title": "RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LogBERT",
            "name_full": "Logbert: Log anomaly detection via bert",
            "brief_description": "A BERT-based approach that integrates masked language modeling with DeepSVDD objective and typically uses a log parser; requires training on log data to learn normal patterns.",
            "citation_title": "Logbert: Log anomaly detection via bert",
            "mention_or_use": "mention",
            "model_name": "LogBERT (BERT fine-tuned / trained for log anomaly)",
            "model_type": "Transformer (BERT + DeepSVDD objective)",
            "model_size": null,
            "data_type": "Textual log sequences (templates + tokens)",
            "data_domain": "System logs (BGL, Thunderbird, HDFS)",
            "anomaly_type": "Sequence-level anomalies (deviations from learned normal patterns)",
            "method_description": "Combine BERT masked language modeling with DeepSVDD loss during training and use a parser for template extraction; anomaly detection is based on learned representations and DeepSVDD-style scoring.",
            "baseline_methods": "Used as an unsupervised PLM-based baseline (reported performance in Table 1)",
            "performance_metrics": "Best F1",
            "performance_results": "Reported F1: BGL 0.9083, Thunderbird 0.9664, HDFS 0.8232 (Table 1)",
            "comparison_to_baseline": "RAPID (training-free) matches or exceeds LogBERT on several datasets, showing that retrieval + PLM embeddings can be competitive with log-specific PLM training.",
            "limitations_or_failure_cases": "Requires log-specific training and often a log parser (parser can lose semantic info); training delays and online updating overhead limit real-time use compared to RAPID.",
            "uuid": "e9243.3",
            "source_info": {
                "paper_title": "RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "NeuralLog",
            "name_full": "",
            "brief_description": "A supervised PLM-based binary classification approach for log anomaly detection that uses general-purpose PLMs to produce features for classification.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "NeuralLog (general-purpose PLM used in a supervised classifier)",
            "model_type": "Transformer (PLM used as feature extractor; classifier on top)",
            "model_size": null,
            "data_type": "Textual log sequences",
            "data_domain": "System logs (benchmarks)",
            "anomaly_type": "Binary anomalous vs normal log classification",
            "method_description": "Use pre-trained language model embeddings as input features to a supervised binary classifier trained with both normal and abnormal logs.",
            "baseline_methods": "Compared as a supervised baseline in Table 1",
            "performance_metrics": "Best F1",
            "performance_results": "Reported F1 values in Table 1: BGL 0.9800, Thunderbird 0.9600, HDFS 0.9800",
            "comparison_to_baseline": "RAPID outperforms NeuralLog on BGL and Thunderbird per reported F1s (RAPID: 0.9999/0.9975), but NeuralLog has higher HDFS F1 (0.98) than RAPID (0.9240); NeuralLog requires supervised training.",
            "limitations_or_failure_cases": "Supervised approach needs labeled abnormal data for training and may generalize poorly to unseen anomaly types; depends on PLM fine-tuning or classifier training.",
            "uuid": "e9243.4",
            "source_info": {
                "paper_title": "RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logbert: Log anomaly detection via bert",
            "rating": 2,
            "sanitized_title": "logbert_log_anomaly_detection_via_bert"
        },
        {
            "paper_title": "Lanobert: System log anomaly detection based on bert masked language model",
            "rating": 2,
            "sanitized_title": "lanobert_system_log_anomaly_detection_based_on_bert_masked_language_model"
        },
        {
            "paper_title": "Colbert: Efficient and effective passage search via contextualized late interaction over bert",
            "rating": 2,
            "sanitized_title": "colbert_efficient_and_effective_passage_search_via_contextualized_late_interaction_over_bert"
        },
        {
            "paper_title": "Deeplog: Anomaly detection and diagnosis from system logs through deep learning",
            "rating": 2,
            "sanitized_title": "deeplog_anomaly_detection_and_diagnosis_from_system_logs_through_deep_learning"
        },
        {
            "paper_title": "Deep learning for anomaly detection: A survey",
            "rating": 1,
            "sanitized_title": "deep_learning_for_anomaly_detection_a_survey"
        }
    ],
    "cost": 0.0161705,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information
9 Nov 2023</p>
<p>Gunho No gunho_no@korea.ac.kr 
Korea University
SeoulRepublic of Korea</p>
<p>Yukyung Lee yukyung_lee@korea.ac.kr 
Korea University
SeoulRepublic of Korea</p>
<p>Hyeongwon Kang hyeongwon_kang@korea.ac.kr 
Korea University
SeoulRepublic of Korea</p>
<p>Pilsung Kang pilsung_kang@korea.ac.kr 
Korea University
SeoulRepublic of Korea</p>
<p>RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information
9 Nov 20234A53F1673F96F08A8206464702A4726CarXiv:2311.05160v1[cs.LG]
As the IT industry advances, system log data becomes increasingly crucial.Many computer systems rely on log texts for management due to restricted access to source code.The need for log anomaly detection is growing, especially in real-world applications, but identifying anomalies in rapidly accumulating logs remains a challenging task.Traditional deep learning-based anomaly detection models require dataset-specific training, leading to corresponding delays.Notably, most methods only focus on sequence-level log information, which makes the detection of subtle anomalies harder, and often involve inference processes that are difficult to utilize in real-time.We introduce RAPID, a model that capitalizes on the inherent features of log data to enable anomaly detection without training delays, ensuring real-time capability.RAPID treats logs as natural language, extracting representations using pre-trained language models.Given that logs can be categorized based on system context, we implement a retrieval-based technique to contrast test logs with the most similar normal logs.This strategy not only obviates the need for log-specific training but also adeptly incorporates token-level information, ensuring refined and robust detection, particularly for unseen logs.We also propose the core set technique, which can reduce the computational cost needed for comparison.Experimental results show that even without training on log data, RAPID demonstrates competitive performance compared to prior models and achieves the best performance on certain datasets.Through various research questions, we verified its capability for real-time detection without delay 1 .</p>
<p>Introduction</p>
<p>With the growth of the IT industry, system log data has emerged as an essential resource for diagnosing software incidents, tracing root causes of failures, and early detection of security threats (He et al., 2017;Zhu et al., 2015;Landauer et al., 2023).As software and services become more complex, anomaly detection has become crucial for ensuring the security and stability of the system (Zhou et al., 2022).Hence, log anomaly detection is gaining attention as a technology that can automatically analyze diverse events and patterns in intricate computer systems, identifying unexpected events (Le and Zhang, 2022a).</p>
<p>Numerous log anomaly detection methods have been proposed, and most of them employ natural language processing (NLP) for analyzing unstructured system logs.These approaches can mainly be categorized into rule-based, machine learningbased, and deep learning-based algorithms (Brown et al., 2018;Du et al., 2017).Rule-based models identify abnormal logs based on explicit rules and templates (Du and Li, 2016).Although they are rapid and efficient, rule-based models operate within pre-defined logics, necessitating regular updates (Xu et al., 2009a;Liao et al., 2013).Conversely, machine learning-based models detect anomalies based on intrinsic patterns trained based on accumulated log data (Breunig et al., 2000;Schölkopf et al., 2001;Xu et al., 2009b;Liu et al., 2008).However, the performance of these models depends on feature selection and they exhibit weak adaptability to new log types (Zhang et al., 2019a).Recent methodologies leveraging deep learning have demonstrated refined anomaly detection performances using neural networks (Du et al., 2017;Meng et al., 2019;Zhang et al., 2019b).With the proven effectiveness of the Transformer (Vaswani et al., 2017), models like HitAnomaly (Huang et al., 2020) and LogSy  (Nedelkoski et al., 2020) were introduced.Moreover, LogBERT (Guo et al., 2021) and LAnoBERT (Lee et al., 2023), exploiting the masked language modeling (MLM) approach of BERT (Devlin et al., 2019), have recorded superior detection performances.</p>
<p>Deep learning-based approaches, leveraging various network architectures, have not only effectively handled intricate patterns in log data but also achieved notable performance gains over the years.However, several challenges impede the practicality and scalability of these approaches in real-world scenarios (Landauer et al., 2023;Chalapathy and Chawla, 2019).First, the development of these models requires training on every log dataset to be utilized, which takes time and therefore delays the start of detection.Furthermore, given that real-world scenarios usually necessitate online updating of accumulated logs, delays also occur in the process of updating the models (Soldani and Brogi, 2022;Cheng et al., 2023).Second, most previous studies only focus on sequencelevel anomaly detection (Zhou et al., 2022), which may overlook potential benefits from leveraging token-level information within logs (Khattab and Zaharia, 2020).Third, many of these models adopt computationally intensive inference processes, such as auto-regressive decoding (Du et al., 2017;Meng et al., 2019;Guo et al., 2021) or comprehensive token predictions (Lee et al., 2023).Such processes can be impractical in situations where millions of log data entries accumulate, potentially making them unsuitable for real-time anomaly detection (Cheng et al., 2023).</p>
<p>To address these challenges, we introduce RAPID (Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information), a novel log anomaly detection model, grounded in three key improvement strategies.First, RAPID employs a retrieval-based reformulation approach, enabled by a pre-trained language model (PLM), to instantiate an anomaly detection methodology capable of identifying anomalous patterns without any training.This methodology facilitates not only the efficient processing of continuously accumulating log data but also significantly enhances the efficacy of delay-free anomaly detection.Second, RAPID amplifies the exploitation of token-level information.Recognizing that even subtle differences or details within log data can serve as crucial indicators for anomaly detection, the model aims to precisely probe into the semantic information of every token, striving to detect abnormal logs with increased performance.Lastly, in consideration of real-time applicability, a concise and fast inference process is designed to enhance the overall procedure.This optimized inference method alleviates complexities, thereby significantly improving the practical applicability of RAPID in real-world environments.</p>
<p>In experiments conducted on BGL (Oliner and Stearley, 2007), Thunderbird (Oliner and Stearley, 2007), and HDFS (Xu et al., 2009a) datasets, RAPID achieved notable anomaly detection capabilities.Importantly, without extra training, the model demonstrated competitive results against both supervised and unsupervised models and showed significant performance improvements on complex datasets like BGL and Thunderbird.Furthermore, robust performance was observed in scenarios with limited data and in experiments employing various pre-trained models, demonstrating the potential of RAPID for practical, real-world application.The main contributions of this study can be summarized as follows:</p>
<p>• Training-free Detection: The deployment of RAPID with pre-trained language models negates the need for log-specific training.</p>
<p>• Rich Use of Token-level Information: Enhancement of the capability to capture and utilize token-level details ensures accurate detection of even subtle log discrepancies.</p>
<p>• Concise Inference: Development of an efficient inference process improves real-world applicability and performance.</p>
<p>2 Related Work</p>
<p>Log-Specific Training Methods</p>
<p>For anomaly detection, models need to capture the semantic and contextual patterns in log data (Lee et al., 2023).Traditional strategies either utilize solely normal log data for model training (Du et al., 2017;Meng et al., 2019) or employ both normal and abnormal logs for binary classification (Zhang et al., 2019b;Huang et al., 2020;Le and Zhang, 2022b;Sun et al., 2023;Zhang et al., 2023;Nedelkoski et al., 2020).A recent approach leverages the BERT architecture to represent log features through pre-training.LogBERT (Guo et al., 2021) combines MLM with DeepSVDD (Ruff et al., 2018) loss during training and employs a log parser.Additionally, LAnoBERT (Lee et al., 2023) uses a tokenizer for log sequence pre-processing, eliminating the need for a parser, and emphasizes MLM for training normal log patterns.Despite their efficacy, the above approaches require training for distinct log datasets, leading to significant training delays.Given that logs contain natural language, PLM can be effectively utilized without additional training, as demonstrated by the robust performance (Le and Zhang, 2022b;Lee et al., 2023).In this study, we propose an anomaly detection methodology that leverages PLM without requiring log-specific training.Given the rapid accumulation of log data, this methodology offers a practical approach for timely anomaly detection in real-world scenarios.</p>
<p>Feature Embedding Techniques</p>
<p>Detecting abnormal patterns in log data requires capturing both semantic and contextual information via feature embedding (Le and Zhang, 2022a).Such embedding techniques focus on various granularities of logs, utilizing information at either the token or sequence-level.In methods that use token-level information, specific semantic of each log token details are turned into vectors using word embedding (Du et al., 2017;Zhang et al., 2019b;Meng et al., 2019;Bertero et al., 2017).However, focusing on a specific log token may not fully capture the overall log structure or context (Zhou et al., 2022).To address this challenge, studies such as Le and Zhang (2022b); Guo et al. (2021) utilized BERT-based contextualized sentence embedding (e.g., [CLS]) to capture both the overall sequence and its global semantic context.Furthermore, Zhou et al. (2022) combined sentence embedding with key event details.</p>
<p>Our research integrates both token and sequence-level information, enabling the model to capture both detailed semantics and global context within logs.This integrated approach empowers us to accurately and effectively discern the complex structures and varied patterns in logs, potentially enhancing the robustness of anomaly detection performance.</p>
<p>Inference Processes</p>
<p>The efficacy of the inference process is critical for real-time processing capabilities, especially in environments characterized by continuous data accumulation.Traditional models which can ensure high performance based on complex structures and computations, often encounter challenges when applied in real-world scenarios.Supervised models provide rapid detection in binary classifications; however, their performance may degrade with unseen logs in practical systems (Zhang et al., 2019b;Huang et al., 2020;Le and Zhang, 2022b;Sun et al., 2023;Zhang et al., 2023).In contrast, unsupervised model-based approaches commonly employ auto-regressive decoding, which sequentially generates token candidates within logs to identify anomalies (Du et al., 2017;Meng et al., 2019;Guo et al., 2021).In addition, LAnoBERT applies a masked auto-encoding technique, utilizing token-level mask probability as an abnormal score (Lee et al., 2023).While both approaches exhibit strong detection performance, ensuring their efficiency in computer systems where log data accumulates rapidly remains a significant challenge.</p>
<p>Therefore, this study focused on the importance of efficient inference processes.Our approach leverages general-purpose PLM to derive embeddings from log sequences and conducts retrieval-based anomaly detection.Moreover, we devised an abnormal score that fully encompasses all token-level semantic information in logs, facilitating more accurate anomaly detection by considering even the details of log information that previous methodologies may have overlooked.Significantly, RAPID assures both potent anomaly detection accuracy and adept efficiency in real-time log processing.</p>
<p>3 Data Analysis</p>
<p>Log Data</p>
<p>Logs are textual data that are systematically generated by developers.They use logging statements in the source code to monitor system events and states.The logging statements in Figure 2 (a) show an example in which logs are generated based on specific conditions, including the occurred events and the corresponding system status.Logs are generated within such standardized patterns (Bace and Mell, 2001;He et al., 2017).Figure 2 (b) shows that the log pattern is composed of two parts.One is the log message segment that describes the system situation, and the other is the parameter segment that provides detailed information about specific objects such as 'account, directory path, IP'.Notably, logs generated under the same situation share the log message segment, while parameters can vary depending on the instance.Furthermore, log messages are composed of words from natural language in daily use (Lee et al., 2023), while parameters consist of words that may have different meanings on different sys-tems (Du and Li, 2016).For example, there is no guarantee that a particular word in the directory path of a log matches its usage in common language (Huang et al., 2020).Considering these characteristics of logs, we performed minimal pre-processing on the parameter segment to utilize both parts in a unified format for subsequent anomaly detection processes.Due to the structured log generation process, the textual format of parameters representing specific objects can be easily distinguished through regular expressions (Le and Zhang, 2023).Therefore, within each dataset, we replace each parameter token with its corresponding header, such as 'account, directory path, IP' within each dataset.This pre-processing is shown in Figure 2 (c).</p>
<p>Log Types</p>
<p>As observed in Figure 2, the generation pattern of logs follows the system situation 2 ; therefore logs generated in the same situation exhibit similar structures.Even when the situations are not precisely identical, if the events or statuses are similar, the logs show similarity.Since logs cover a wide variety of situations, logs might be categorized into various types according to their generation patterns.To validate this, we visualized representations of logs in BGL in Figure 3 separating normal and abnormal logs.Building upon the insights from the Section 3.1, we leveraged representations obtained via a PLM and employed t-SNE (van der Maaten and Hinton, 2008) to reduce the dimensionality to 2D for visualization.In Figure 3, the left side shows all normal log sequences contained within the BGL, while the right side offers an enlarged view, highlighting abnormal log sequences marked with '✖' symbols and presenting specific examples.</p>
<p>Through the analysis, we have identified three key features in log data.First, various types exist within normal log sequences, and there can be significant differences among these types.This type diversity is caused by the system situation where the logs were generated and is well reflected spatially in Figure 3 3 .Upon analyzing the enlarged figure and the corresponding text of log sequences distinguished by color, log sequences of similar types are positioned closely together in 2 System event and its corresponding status 3 While Figure 3 represents a 2D reduction from 768dimensional tensors, distinct log types remain clearly separated.space.This finding means that PLM representation effectively captures the contextual information within the log.Secondly, the distance between normal and abnormal log types can be closer than the distance between different normal log types (Lin et al., 2016).Even subtle differences, such as a single word, can determine the abnormality, which may lead to abnormal types being remarkably close to normal types.In practice, the abnormal log sequences (a) and (b) in Figure 3 exhibit text highly similar to the normal type represented by the same color.The distance between these abnormal log sequences and their corresponding normal types was observed to be considerably closer than the distances between different normal types.Third, as shown in (c) and (d) in Figure 3, certain abnormal log sequences are positioned among some normal types.This demonstrates the lack of clear distinction between normal and abnormal logs.Considering these observations and noting that each abnormal log sequence is located very close to its corresponding (or similar) normal type, we judge the abnormality of a log for each type.Moreover, we design anomaly detection method based on the type each test log sequence belongs to, rather than using a single normal criterion.</p>
<p>Token-level Information</p>
<p>Another characteristic of the log is that it has a very limited vocabulary size compared to the vast data size (Du et al., 2017).In fact, the vocabulary size of BGL, Thunderbird, and HDFS, the three representative data used as benchmarks, are 888, 3137, and 229, respectively, which are quite small compared to the diversity of logs.This suggests that anomaly detection using token-level information can be advantageous.Since anomaly detection solely relies on known normal log sequences  4 , the key to performance is how effectively new incoming logs can be interpreted based on these known normal sequences.Intuitively, if we judge anomalies by comparing test logs to known normal logs, the quality of this interpretation can be inferred by how well the known normal log sequence contains information about unseen test logs.To evaluate the coverage of this information at the sequence-level and token-level, we examine the ratio of seen test log sequences and seen test tokens via known normal logs, respectively.In the former case, we define a test log sequence as covered if it is equally present in the known normal log sequence, and in the latter case, coverage is calculated by how much each token appearing in the test log is included in the vocabulary appearing in the entire known normal log sequence.</p>
<p>As shown in Figure 4, test log token coverage of known normal log sequences is higher than test log sequence coverage.These results suggest that more detailed insights into test logs can be gained by directly considering token-level information within the logs, rather than relying solely on sequence-level information.Furthermore, in simulations where logs are accumulated by utilizing only a certain percentage of the known nor-mal log sequence, it is observed that even when the amount of available known normal log sequences decreases, token coverage has a relatively larger value compared to log sequence coverage.This also suggests that utilizing all tokens can provide a more precise interpretation of test log information.</p>
<p>4 proposed method 4.1 Task Formulation</p>
<p>Traditional Log Anomaly Detection</p>
<p>Task Formulation</p>
<p>Traditional log anomaly detection aims to evaluate the normality of a given log sequence.In particular, a log sequence that differs from the normal pattern observed in the training data is considered abnormal, and usually only normal data is utilized for training.A log sequence L is represented by the following equation ( 1).
L = {t 1 , t 2 , . . . , t N } (1)
t 1 represents the i-th token and N denotes the sequence length of L. Equation ( 2) indicates that a machine learning model f , trained solely on normal log data, is employed to map the input L to the abnormal score function s ad , where θ represents the parameters the of model.
s ad (L; θ) = f (L) (2)
The sequence L is detected as abnormal if s ad (L; θ) ≥ δ according to the Equation (3), where δ is the threshold.
Anomaly = 1 if s ad (L; θ) ≥ δ 0 otherwise (3)</p>
<p>Retrieval-based Reformulation</p>
<p>Based on the analysis results in Section 3, we propose an information retrieval (IR) based reformulation to design training-free log anomaly detection.Specifically, we introduce the anomaly detection process centered on the relationship between known normal log sequences and new test data.For a test log that contains both normal and abnormal logs, the proposed approach involves finding a normal log corresponding to each test log, and judging it as abnormal when the distance between the pair is larger than the threshold.More precisely, each test log sequence takes on the role of a query, while each known normal log sequence act as a document in IR.In other words, the abnormal score for this query q is calculated through the similarity with the target document d as shown in the Equation (4).
s ad (q) = 1 − g(q, d)(4)
Here, the function g represents the similarity between the two log sequences and uses the cosine similarity score of PLM representations.As shown in Equation (3), if s ad (q) &lt; δ, the test log sequence q is judged to be normal because it is similar to a known normal log sequence, and vice versa.Through the described reformulation, we can perform anomaly detection by simply comparing the distance between test logs and normal log sequences without any additional training.</p>
<p>RAPID</p>
<p>In this section, we provide a detailed explanation of the RAPID process based on the proposed retrieval-based log anomaly detection (R-LogAD) formulation, specifically explaining how each step overcomes the limitations of previous research.</p>
<p>The key points of RAPID are summarized as follows.First, RAPID analyzes log sequences using a general-purpose PLM and performs R-LogAD, which eliminates the need for training.Second, it leverages feature embedding that actively reflects all token semantic information in the log sequence to include as much information as possible.Third, anomaly detection operations are performed only on the type to which the test log belongs, resulting in an efficient and concise inference process.</p>
<p>Feature Embedding</p>
<p>The first step is to get the feature embedding from the log, and this step builds the query database (Q) and document database (D).</p>
<p>Parameter pre-process with regular expression Detection process starts with log preprocessing.As pointed out in (Zhu et al., 2019), log parsers used in many previous studies (Huang et al., 2020;Zhang et al., 2019a;Qi et al., 2022;Zhou et al., 2022;Sun et al., 2023;Lin et al., 2016;Du et al., 2017;Meng et al., 2019;Guo et al., 2021) do not always work correctly on all log datasets.Furthermore, the use of such parsers may result in the loss of semantic information.In contrast, the proposed regular expression-based pre-processing described in Section 3.1, despite its simplicity, provides performance advantages (Zhang et al., 2023;Le and Zhang, 2022b).The log sequences obtained after pre-processing contain both log messages and parameters expressed in common language text with minimal information loss.This composition enables effective utilization of general-purpose PLMs in anomaly detection.</p>
<p>Make unique log sequence database Performing parameter pre-processing on a log dataset unifies the distinctive parameters for each individual log instance, so logs generated in the same situation will be redundant (Lee et al., 2023).Therefore, the proposed R-LogAD only requires a single document log sequence for each situation.Using this approach, RAPID constructs database D using only unique normal log sequences, significantly reducing the retrieval search space.The same approach can be applied to the test logs used as queries.As logs accumulate at a fast rate, typically at 5 times or more per second (Oliner and Stearley, 2007), even real-time detection is conducted for a certain testing period at a time.Therefore, RAPID constructs database Q using unique test log sequences and calculates abnormal scores within the test period.It then assigns scores to their corresponding actual test timestamps through a database lookup table.This process significantly reduces inference time by decreasing the frequency of the detection process, and the benefit increases as the test period increases.</p>
<p>In anomaly detection, especially when abnormal log information is unavailable, understanding the intent of the developer embedded in the logs becomes crucial.This suggests that detection should factor in the semantic information present within the log text (Le and Zhang, 2022b;Zhou et al., 2022;Zhang et al., 2023;Lee et al., 2023).Therefore, after constructing databases D and Q, the embedding of included log sequences is added to the database using PLM.In the case of D, we continue to use the stored results after one encoding process, and in the case of Q, we encode them every test period.Since both databases are already built with the minimum required log sequence, this process is very efficient in terms of computational cost.Also, by directly utilizing the required document information for queries from database D, it significantly contributes to efficient inference.A more detailed description of the feature embedding extraction process can be found in Algorithms 1.
for i = 1 to len(L) do L ′ i ← regex(L i ) if L ′ i not in DB[Seq] then idx ← idx + 1 Lookup[i] ← idx DB[idx][Seq] ← L ′ i DB[idx][E] ← PLM(L ′ i ) end end return DB, Lookup</p>
<p>Selection of the Comparison Log</p>
<p>Sequence for Each Query from the Documents</p>
<p>The key to R-LogAD is to select an appropriate document log sequence from D to judge each query log sequence.Especially, since RAPID is a training-free method, it needs a comparison target that can determine the normality only using the representation of the log sequence itself.Based on the analysis of prior work by Lin et al. (2016) and Section 3.2, the selection of a comparison target should be determined according to the type of query.This perspective is also found in other anomaly detection studies besides logs, consistent with the claim that different clusters exist within normal data (You et al., 2022).However, which type the query log sequence belongs to, or even whether that type exists in D is not known.Since the log sequence representation obtained through PLM contains context information about the system situation, logs of similar situations will be located close in the embedding space (Devlin et al., 2019).Leveraging this property, it is assumed that the nearest document type to a query, based on the log sequence representation, is the same or a similar log type as that query.However, when performing anomaly detection under this assumption, it is challenging to determine the number of log sequences to include in the nearest document type for abnormal score calculation, as it is unknown.</p>
<p>Including more log sequences than the actual number results in an incorrect score that reflects unrelated types.On the other hand, using fewer log sequences would not capture other types.Therefore, RAPID intuitively performs anomaly detection by comparing to the single nearest document log sequence.Verification of this approach is performed in Section 5.2.</p>
<p>Token-level Information Based Max Similarity Distance</p>
<p>RAPID performs anomaly detection by comparing each query log sequence with its nearest document log sequence.This approach requires a clear definition of the criterion for measuring distance and the method for comparing that distance.In this context, the distance between log sequence representations is selected to calculate abnormal score.In particular, while previous studies mainly utilized only sequence-level information, we applied maxSim distance inspired by Col-BERT to actively reflect token-level information.</p>
<p>The maxSim score computes the query-document similarity by comprehensively reflecting the cosine similarity between all tokens and following the equation ( 5) for embedding E.
maxSim(q, d) = i∈[|Eq|] max j∈[|E d |] E q i • E d j . (5)
where • represents cosine similarity function, q i and d j are tokens in q and d respectively.Because the log sequence representation obtained through PLM has both the CLS token embedding and the embedding of each word token, the maxSim operation can reflect both the sequence-level and the token information (Khattab and Zaharia, 2020).</p>
<p>After that, following the formula distance = 1similarity, we reverse the sign to define the querydocument maxSim distance as in Equation (6).</p>
<p>distance(q, d) = 1 − maxSim(q, d).(6)</p>
<p>Finally, the abnormal score for each query log sequence is determined by the smallest maxSim distance between that query log sequence and all log sequences in D.</p>
<p>Algorithm</p>
<p>This section details how RAPID performs anomaly detection using the database D, Q, comparison target document for each query, and maxSim distance defined in Section 4.2.</p>
<p>Get Core Set</p>
<p>By our definition, the abnormal score for a query log sequence is determined by its maxSim distance from the nearest document log sequence.The challenge with this process is that determining the 'nearest' document log sequence requires computing the maxSim distance to all log sequences in D. This is quite inefficient given that only the distance to the single document log sequence is ultimately used for anomaly detection.</p>
<p>In particular, the maxSim calculation involves computing the cosine similarity between all tokens, which is very computationally expensive.To tackle this challenge, we introduce a process to extract the core set so that only a minimal number of maxSim calculations can be performed.</p>
<p>To extract the core set, we focus on the CLS token, which summarizes sequence-level information.This special token contains sufficient information to select the nearest document candidates as it has been directly utilized for anomaly detection in many previous studies (Huang et al., 2020;Zhang et al., 2019a;Le and Zhang, 2022b).As a result, only the Euclidean distance between the CLS tokens of the query log sequence and the document log sequences is required.Nevertheless, it cannot be guaranteed that the document obtained through the CLS Euclidean distance will always match that obtained using the 'maxSim' distance, which considers all tokens.Therefore, to account for this uncertainty, we implemented the K-nearest neighbor (KNN) algorithm for this computation, selecting the pre-specified K neighboring document log sequences as candidates.The process of determining the core set CDoc q for a specific query log sequence q is as follows.</p>
<p>CDoc q := Core(q, D, K), (7) Core(q, D, K) = KN N (q, D), (8)
KN N (q, D) = {d i ∈ D | i ∈ top-K −∥E CLS q − E CLS d i ∥ 2 . (9)
As shown in the Equation ( 7), the core set is selected via the function Core defined in (8).The calculation through the KNN algorithm is efficiently executed, yielding top-k nearest neighbors as candidates, as defined by Equation ( 9) Utilizing a core set means that the maxSim distance is only calculated for the documents in the selected core set of query, regardless of how many documents there are in total.In (b) and (c) of Figure 5, the distance calculation is only performed for documents within the dashed area.Finally, the shortest of these distances is taken as the abnormal score.</p>
<p>Abnormal Score</p>
<p>The process of calculating an abnormal score in RAPID can be summarized as follows, and a detailed description can be found in Algorithms 2.</p>
<ol>
<li>
<p>Database construction: Using regular expressions, replace the parameter part of the log with the header of the corresponding object, and generate D and Q with only unique log sequences.Additionally, only those log sequences contained in this database will be represented by PLM.</p>
</li>
<li>
<p>Compute abnormal score: For an efficient inference step that can be used in a real-world anomaly detection scenario, we apply a KNN algorithm utilizing only the representation of CLS tokens to narrow the search space for each query.We perform the maxSim distance operation on this core set only and determine the minimum value as the abnormal score for that query.</p>
</li>
<li>
<p>Allocate scores and detect anomalies: To convert the anomaly score calculated based on Q into the result for the input test log, detection is performed on this score using the defined threshold.Then, predictions are assigned to each timestamp in the test period through a lookup table created with the Q.
// Make database D, Lookup D ← Make_DB(L kn ) Q, Lookup Q ← Make_DB(L ts ) // Compute Abnormal score for i = 1 to len(Q) do CDoc i ← Core(Q[i], D, K) for j = 1 to K do dist i maxSim ← 1 − maxSim(Q[i], CDoc i [j]) end Abnormal_score[i] ← min(dist i maxSim ) end // Allocate scores and detect anomalies for k = 1 to len(Lookup Q ) do if Abnormal_score[Lookup Q [k]] ≥ δ then f inal_pred[k] ← Abnormal else f inal_pred[k] ← Normal end end return f inal_pred</p>
</li>
</ol>
<p>Research Questions and Results</p>
<p>The detailed experiment setup including dataset and baselines, and additional research questions can be found in Appendix A, B.</p>
<p>Does the RAPID achieve outperform anomaly detection performance? (RQ1)</p>
<p>We validate the performance of RAPID by comparing it to previous studies with log-specific training.Table 1 compares the performance of RAPID and the baseline models, categorized by whether each method is supervised or trained, whether token information is utilized in the anomaly detection process, and whether a log In the comparison with the supervised setting models, RAPID shows competitive performance.Specifically, NeuralLog is a methodology that utilizes a general-purpose PLM similar to ours but focuses on binary classification.Even in this comparison, our model performs better, indicating that our methodology effectively utilizes the representation of the PLM to perform anomaly detection without abnormal data.</p>
<p>When comparing the performance in the unsupervised setting, the proposed RAPID achieved strong performance on all datasets.It shows the best detection performance on the BGL dataset, even with the supervised learning setting, and the second-best performance on the Thunderbird dataset after LAnoBERT.It even performs quite competitively on the HDFS dataset, even though the method is designed for a single log input.This robust performance supports the effectiveness of the abnormal score designed by RAPID.Compared to methodologies that consider the type of logs, such as LogCluster, it outperforms all datasets.In addition, when we examine LogBERT and LAnoBERT, which use a well-known PLM, BERT, alongside RAPID, it becomes clear that the PLM-based model shows higher detection performance overall.This shows the excellent capability of the PLM to analyze log data and suggests that even higher performance can be expected because the PLM can efficiently leverage token information from the representation of logs.In particular, unlike the two previous studies, our model does not train PLM on logs at all, yet it performs as well as or better than the two previous models.This confirms that the proposed RAPID utilizes the PLM representation very effectively.</p>
<p>5.2 Is it valid to select only the one document log sequence nearest to the query as a comparison target?(RQ2)</p>
<p>In Section 4.2.2, we defined anomaly detection for a test log by comparing its query to the nearest normal type within D. The objective was to evaluate the type to which the query most likely belongs.Since the number of log sequences belonging to that normal type is unknown, we substituted it with a comparison to the single nearest document log sequence.To evaluate the proposed approach, we compare the performance of considering all log sequences in the core set and only the single nearest log sequence to the query, for different core set sizes.</p>
<p>In Table 2, based on the query log sequence, we compare the performance of using only the distance to the nearest single log sequence in the core set as the abnormal score (nearest only) and using the average value of the distance to all log sequences in the core set as the ad score (core set mean).For the three datasets, the performances are recorded by varying the number of log sequences in the core set, starting with the case where the number of log sequences is 2 because the average calculation of a single distance is meaningless.</p>
<p>First, we validate the anomaly detection approach using only the nearest normal type to the query log.The 'ALL' row refers to the case where distance is calculated for the entire D, and the 'nearest only' and 'core set mean' columns show the detection performance of anomaly detection using only the nearest normal type and anomaly detection reflecting all normal data, respectively.In the BGL and Thunderbird datasets, the F1scores for 'nearest only' were 0.9999 and 0.9979, respectively, whereas for 'core set mean' they were 0.4659 and 0.7586.This confirms that the performance when using only the nearest type is significantly superior.This shows that as proposed in Section 4.2.2, the approach of using only the nearest normal type to the query log during detection is valid.In the case of HDFS, which is a block-based dataset consisting of multiple log sequences as described earlier, performance shows a different pattern because each query and document does not refer to a single log sequence.Nevertheless, it outperforms the competing baseline models by only utilizing the nearest type of document.</p>
<p>Next, we validate the approach of reflecting only the nearest single document log sequence instead of the normal type.Comparing the performance of the 'nearest only' and 'core set mean' columns for BGL, we can see that as the number of log sequences in the core set changes from 2, 5, and 10, the F1-score is robust at 0.9999 when using only the single nearest log sequence, while the performance changes significantly and drops up to 0.5127 when including all log sequences in the core set.The same trend is observed for Thunderbird.This results due to the number of log sequences forming the core set actually exceeding the number of log sequences contained in the nearest type to the query, which may cause performance degradation as anomalies are determined from other types unrelated to the query.As mentioned, the performance of 'core set mean' in the BGL dataset notably decreases as the number of log sequences in the core set increases from 2 to 5.This case is illustrated in Figure 5 (b) and (c).Since the 'Nearest only' performance is robust across various core set sizes, in conclusion, anomaly detection by comparing a test log sequence to the single nearest log sequence is valid.</p>
<p>Does RAPID have an efficient inference process for real-world applications? (RQ3)</p>
<p>To assess inference efficiency, we compare the performance and inference time using different size core sets.Figure 6 shows the performance and time taken by the model according to the size of the core set, where the x-axis indicates the ratio of log sequences used as a core set to the total D. The line graph shows the proportion of inference time spent at each core set size, with 1 indicating the case where computing the maxSim distance for all log sequences in D without core sets.For all three benchmark datasets, we can see that the detection performance of the model, as visualized by the bar charts, remains unchanged even as the size of the core set decreases.However, the inference latency decreases significantly.This demonstrates that our proposed core set technique enables a substantial improvement in inference speed while maintaining performance.Particularly in the case of the larger dataset, Thunderbird, the inference time is reduced by up to ten times.These findings indicate that our methodology can maintain a remarkably fast inference speed even as data accumulates and the number of known normal log sequences expands over time.For instance, in a scenario where D is already constructed and using only 0.01 of the total known normal log sequences as a core set for detection, we can process about 12,000 and 3,000 test logs per second for BGL and Thunderbird, respectively.Considering that each data set generates about 11 and 8 log outputs per second, respectively, this demonstrates that our model has an efficient inference process.</p>
<p>Can RAPID detect anomalies without delay under real-time data accumulation scenarios? (RQ4)</p>
<p>To simulate real-world log anomaly detection scenarios in which data accumulates in real-time, our model was tested using a limited number of known normal log sequences.Through this controlled sampling approach, we can assess whether a small number of known normal log sequences can provide robust performance.Figure 7 shows the performance when sampling a portion of the normal logs in each dataset, simulating a situation where only a small number of normal logs are known relative to the continuously accumulating test logs.From the results, RAPID shows stable and robust performance from using very little normal data to using most of the data.This means that our model can continue to detect in real-world log anomaly detection scenarios regardless of the accumulation of data, demonstrating that RAPID does not cause a training delay.Such robust results can be explained by the following reasons.First, as outlined in Section 3.3, logs possess a limited vocabulary size, which means that even if a new test log is entered, the tokens that comprise it are most likely already included in the known normal log sequence.This characteristic gives R-LogAD the ability to interpret unseen test logs, and in particular because RAPID directly reflects all tokens, it provides indepth understanding of unseen logs.More detailed analysis of the effect of utilizing all token information is performed in Appendix B.2. Furthermore, even if we do not possess the exact same vocabulary of tokens for an unseen log, our methods can still interpret it based on words with similar meanings that are close in terms of PLM representation.</p>
<p>Conclusion</p>
<p>In this study, we propose RAPID, a model that improves the limitations of previous deep learning-based log anomaly detection methodologies through detailed analysis of log data.First, we reformulated the task to improve the delay of previous studies that require training for each log data.In the reformulated R-LogAD, anomaly detection is performed by comparing the test log and its nearest normal type without any training, and in the process, information loss is minimized by replacing the parameters of the log with regular expressions.Furthermore, noting that the preprocessed log sequences consist of only a common language, we extract the representation of each log sequence through a publicly available PLM.Second, RAPID uses a maxSim score that actively reflects the information of all tokens to enable more sophisticated detection and robustness against unseen test logs.Finally, we apply the core set technique, which requires minimal computation for realistic inference, to enable real-time detection.</p>
<p>To validate our proposed RAPID, we conducted experiments on three of the most commonly used benchmark datasets: BGL, Thunderbird, and HDFS.We addressed various research questions and found that our methodology demonstrated superior detection performance even without any training on log data.In particular, even in the unsupervised setting, where no information about abnormal logs is given, RAPID achieved competitive performance compared to models in the supervised setting, and the best performance on BGL.In addition to achieving efficient inference through the core set technique, it demonstrated robust performance in the log accumulation experiment, confirming its ability to deliver real-time results without training delays.Furthermore, even if new normal logs are continuously added, our model can be updated by simply including them in the known normal log sequence, making our approach highly scalable in the long run.Since RAPID is not limited to specific log data and can be directly applied to various computer systems, it is expected to be widely used in real-world industrial applications.</p>
<p>A Experiment setup</p>
<p>A.1 Dataset</p>
<p>In this study, we choose BGL (Oliner and Stearley, 2007), Thunderbird (Oliner and Stearley, 2007), and HDFS (Xu et al., 2009a) log datasets as benchmarks to provide a fair comparison with previous work.All three datasets contain labels for normal and abnormal and are generated from different computer systems, allowing us to evaluate the generalization performance of RAPID.The BGL dataset contains logs from the Blue Gene/L supercomputer, each of which comes with a label separated by the alert category tag.The Thunderbird dataset was collected from the Thunderbird supercomputer operated by Sandia National Laboratories in Albuquerque, and it also contains alert and non-alert messages, similarly labeled with category tags.HDFS, on the other hand, is a log dataset derived from the Hadoop Distributed File System, where multiple logs output from a private cloud environment are grouped into blocks to form a single log instance.In BGL and Thunderbird, each single log is a data instance and is</p>
<p>A.2 Baselines</p>
<p>In this study, we conduct comparative experiments with various existing models to evaluate and validate the performance of RAPID.The selected comparison models are categorized according to whether they use log-specific training, whether they use a parser, and how they utilize token information.Among the various benchmark models, the deep learning-based models are described below.</p>
<p>LogRobust is a supervised learning model utilizing an attention-based bi-LSTM structure.It uses a log parser to pre-process the log data and generates TF-IDF and word semantic vectors to extract the features of the log data.</p>
<p>HitAnomaly is a supervised model that leverages a transformer structure.It uses a log parser to convert log data into standardized templates and encode log information into parameters.</p>
<p>LogSy is a supervised anomaly detection model that leverages a transformer structure.Log are preprocessed by a tokenizer and do not require the use of a log parser.</p>
<p>Adanomaly detects anomalies with feature extraction and ensemble methods using BiGAN model to address the class imbalance problem in log anomaly detection.</p>
<p>DeepSyslog suggests a way to represent the context of log events and event metadata.Based on the continuity of the log stream, it extracts the semantic and contextual information from logs using unsupervised learning sentence embedding.</p>
<p>LogPal combines template sequences and raw log sequences to generate log pattern events, automatically recognizing log patterns.This model performs binary classification.</p>
<p>LayerLog is a log anomaly detection framework based on the hierarchical semantics of log data.LayerLog effectively extracts semantic features from each layer and utilizes the wordsequence hierarchy.</p>
<p>DeepLog is a deep learning unsupervised log anomaly detection model based on LSTM structure.It utilizes a log parser to generate inputs for the LSTM and operates by predicting the next word.</p>
<p>LogAnomaly is a solution for detecting anomalies in log streams.By attention-based LSTM structure, it extracts semantic information from log templates using the template2vec technique.</p>
<p>LogBERT is a BERT-based anomaly detection model that utilizes MLM and DeepSVDD loss.After pre-processing with a log parser, it identifies anomaly patterns.</p>
<p>LAnoBERT is a BERT-based anomaly detection model.After training using MLM, it detects token-level anomalies by using MLM probability as an abnormal score in inference.</p>
<p>A.3 Evaluation Metric</p>
<p>The threshold-dependent F1 score and thresholdindependent AUROC are adopted as the evaluation metrics in this study.Once the threshold of the model is determined in the anomaly detection, the recall and precision are calculated through equation (10) and equation ( 11) depending on the actual anomaly and whether the anomaly is determined by the model, and the F1 score is calculated as the harmonic mean of these two indicators as shown in equation ( 12).Recall = T P T P + F N , (10)
Precision = T P T P + F P , (11) F1 score = 2 • Precision • Recall Precision + Recall .(12)
(TP: true positive, FP: false positive, FN: false negative.)AUROC is a metric that calculates the false positive rate (FAR) and true positive rate (TPR) for all possible threshold candidates, then plots a receiver operating characteristic curve with FAR on the xaxis and TPR on the y-axis, and calculates the area under the curve.A better anomaly detection model will have a value of AUROC closer to 1, while a random model will have a value closer to 0.5.</p>
<p>In general, anomaly detection studies use AU-ROC as an evaluation metric to determine the intrinsic performance of a model that does not rely on a threshold, but existing studies in log anomaly detection utilize Best F1 score to measure the performance of classifying normal and abnormal logs.Moreover, since studies in an unsupervised setting, including ours, only consider normal data, the Best F1 Score threshold cannot be predetermined in advance.Therefore, we compute the Best F1 Score in the same way as previous studies, using the threshold that gives the best theoretical performance on the test dataset.In addition, most of the previous studies except LAnoBERT do not report the performance of AUROC score, so we cannot compare the AUROC of baseline models together.The AUROC performance of RAPID is recorded in 4.</p>
<p>A.4 Evaluation Details</p>
<p>All evaluation experiments are performed on a single Linux system, and the specific environment is as follows.Four TITAN-RTX GPUs are used to extract the PLM representation of log sequences during database construction, one TITAN-RTX GPU is used to compute maxSim between querydocuments, and all other processes are performed on the CPU.The PLM utilized the Huggingface Transformers library (Wolf et al., 2020) for implementation.The methodology does not impose strict restrictions on the choice of PLM, and except for the experiments in Section B.1 which examine the dependency on a specific PLM, all experiments employ BERT to obtain a representation of the log sequence.The proposed RAPID is a model that does not perform any training and therefore does not have any training parameters.When processing the data through the PLM, we set the max token length to 128 for the BGL and Thunderbird datasets and 512 for the HDFS dataset.This difference is due to the fact that the data in each type is labeled at the log sequence level or at the log block level, respectively, so we set a relatively long max length for HDFS, which consists of one input per block.In addition, in the case of HDFS, to ensure that the input per block does not exceed the maximum token length of BERT, the input is constructed by concatenating only unique log sequences within the block.Finally, the only hyperparameter, the size of the core set, determines how much of the completed D for each dataset is taken as the core set.In Section 5.4, we verify that the final anomaly detection performance is robust to the size of the core set, and finally set it to utilize only 0.01 unique known normal log sequence out of the total log sequences in D.</p>
<p>B Additional Research Questions</p>
<p>B.1 Is a general-purpose PLM enough to detect log anomalies?</p>
<p>To demonstrate that the performance of RAPID is not dependent on a specific PLM model, we compared its performance using three well-known general-purpose PLMs: BERT, RoBERTa (Liu et al., 2019), and ELECTRA (Clark et al., 2020).Table 4 shows the performance of utilizing wellknown PLMs BERT, RoBERTa, and ELECTRA to obtain the log sequence representation.All three PLMs show robust performance, indicating that R-LogAD can be accomplished by utilizing generalpurpose PLMs without any additional training.Incidentally, to ensure a fair comparison with Log-BERT and LAnoBERT, all experiments recorded performance based on BERT.</p>
<p>In our method, we use a maxSim distance that actively reflects all token information in the query and document log sequences when performing R-LogAD.Based on the analysis in Section 3.3, this approach is intended to provide a deeper interpretation of unseen test logs, and we expect that the effectiveness of this approach will be more pronounced as the amount of unseen test logs increases.Therefore, we conduct an experiment with increasing the proportion of unseen test logs by decreasing the proportion of known normal log sequences and comparing the performance of using only sequence-level information of CLS token and using information of all tokens.The results are shown in Table 5.</p>
<p>B.2 Is it effective to consider all tokens?</p>
<p>For the Thunderbird dataset, as the percentage of unseen test logs increased, the performance of utilizing all token information shows more robustness.On the other hand, for the BGL dataset, utilizing all token information does not show a noticeable performance difference, which is likely due to the very small vocabulary size of the dataset itself (888), where the CLS token is sufficient to summarize the overall meaning of the logs.Nevertheless, even for log datasets such as BGL, as the number of log types increases due to continuous data accumulation and system updates, the vocabulary size is expected to increase (Le and Zhang, 2022b;Zhou et al., 2022), and the utilization of all token information is expected to become increasingly important.</p>
<p>Figure 1 :
1
Figure 1: The overall framework of RAPID.(1) RAPID builds a lookup DB using PLM, updating the Query DB per test period.(2) Each query measures maxSim distance against the core set of Document DB to determine abnormal.(3) Results are mapped to test timestamps via the Query Lookup.</p>
<p>-Figure 3 :
3
Figure 3: Visualization of representations obtained by passing log sequences from BGL dataset through a PLM (BERT) with t-SNE mapping.It demonstrates that normal logs form clusters, closely interspersed with abnormal logs.When examining the actual log text, the distinctions in described situations are well-reflected in spatial distances.</p>
<p>Figure 4 :
4
Figure4: Illustration comparing log sequence coverage of a known normal log sequence for a test log with token coverage.It shows that the interpretation of the test log is more advantageous when the information in the token is utilized.</p>
<p>Algorithm 1 :
1
Database Construction Data: L, L ′ : Set of logs and processed log sequences, DB: Database of unique log sequences, Lookup: Database of index by timestamp, idx: Index of unique log sequence in DB, regex: Regular expression pre-processing function, Seq: Key of DB indicating L ′ , E: Key of DB indicating embedding of L ′ .Result:</p>
<p>Figure 5 :
5
Figure 5: (a) Spatial representation of documents (known normal log sequence) and a query (test log sequence) based on the maxSim distance.(b) and (c) show partial views with core sets of 2 and 5, respectively.</p>
<p>Algorithm 2 :
2
RAPID Processing Data: L kn : Known normal logs, L ts : Test logs during detection, D: Database of known normal logs, Q: Database of test logs, CDoc i : Sub-set of Document for i th query, dist maxSim : Calculated maxSim distance, δ: Anomaly detection threshold.Result: f inal_pred</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: F1-score and inference time by a ratio of the utilized core set in entire D. The reduction in the size of the core set used for maxSim calculation maintains the performance while significantly decreasing the inference time.(RQ3)</p>
<p>a. Logging statements b. Log message and parameter c. Parameter pre-processing
if system_event_A :if system_status == 'status_A' :print(f' normal log message type 1, {parameter}')elif system_status == 'status_B' :print(f' normal log message type 2, {parameter}')elif system_status == 'anomaly' :print(f' abnormal log message 1, {parameter}')Parameterprint(f'user[{account}]: Critical error occurs at path {DIR}')Log Messageuser[root@server]: Critical error occurs at path path/of/directoryPre-processinguser account critical error occurs at path dirHeaderFigure 2: Examples of logging statements in the sourcecode and a format of log data. The figure illustrates logcategorization based on system situations, highlightingsegments written in natural language and variable pa-rameter segments.</p>
<p>Table 1 :
1
F1-score on BGL, Thunderbird, and HDFS.
Train Parser BGL Thunderbird HDFSSupervisedLogRobust✓✓0.8300-0.9700HitAnomaly✓✓0.9200-0.9800LogSy✓✗0.65000.9900-NeuralLog✓✗0.98000.96000.9800Adanomaly✓✓0.9200-0.9800DeepSyslog✓✓0.9800-0.9800LogPal✓✓0.99000.98000.9900LayerLog✓✗0.9850-0.9880UnsupervisedPCA  †✓✓0.16610.54390.1112iForest  †✓✓0.30650.03290.6049OCSVM  †✓✓0.01960.25480.0495LogCluster  †✓✓0.76630.59610.5399DeepLog  †✓✓0.86120.93080.7734LogAnomaly  †✓✓0.74090.92730.5619LogBERT  †✓✓0.90830.96640.8232LAnoBERT✓✗0.87490.99900.9645RAPID (ours)✗✗0.99990.99750.9240
† indicates the performance of benchmark models reported by LogBERT.Best performance is bolded and underlined, while second-best is only bolded.(RQ1) parser is used.RAPID is the only model that does not require any training.</p>
<p>Table 2 :
2
Performance based on the number of log sequences in the core set and those included in the abnormal score calculation.'Core set mean' considers all sequences equally, while 'nearest only' looks at the nearest log sequence.(RQ2)
Dataset# of log sequence in core setF1 core set mean nearest only20.96890.9999BGL50.72990.9999100.51270.9999ALL0.46590.999920.50730.9978Thunderbird50.56410.9978100.79890.9979ALL0.75860.997920.93330.9240HDFS50.94850.9240100.97000.9240ALL0.94950.9240</p>
<p>Table 3 :
3
Number of logs in each dataset used in RAPID
labeled, whereas HDFS has a label for each aggre-gated block of logs. Since our methodology is de-signed by analyzing single log sequences and as-sumes that queries and documents are single logsequences, BGL and Thunderbird are the targetlog datasets for RAPID, and the HDFS dataset isincluded as a supplemental dataset to check therobustness of the study. For BGL and HDFS, weused the full dataset, while for Thunderbird, wesample a portion of the dataset following the set-tings in Guo et al. (2021) 5 , but maintained abnor-mal proportions. Since no training takes place inRAPID, we refer to known normal logs rather thantraining data. For BGL and Thunderbird datasets,we allocate 0.8 of the total normal logs as knownnormal logs and 0.2 as test normal logs, and allabnormal logs are included in the test logs. InHDFS, the same ratio is applied on a per-block ba-sis to separate them. Details about the distributionof known normal logs and the log sequences (orblocks) used in the test dataset can be found in Ta-ble 3.</p>
<p>Table 4 :
4
Performance based on the general purpose PLM used.Robust performance was observed across all three well-known PLMs we tested.
PLMBGLThunderbirdHDFSF1AUROCF1AUROCF1AUROCBERT0.9999 0.9999 0.9979 0.9994 0.9240 0.9295RoBERTa 0.9999 0.9999 0.9968 0.9992 0.9249 0.9303ELECTRA 0.9999 0.9999 0.9982 0.9992 0.9240 0.9295</p>
<p>Table 5 :
5
Comparison between using only CLS and all tokens.From Thunderbird results, we can see that reflecting information from all tokens contributes to robust performance as fewer known normal log sequences result in more unseen test logs.
Datasetvocabulary sizeknown normal log sequence ratio CLS only all token F110.99990.9999BGL8880.1 0.010.9999 0.99940.9999 0.99880.0010.99460.992410.99720.9979Thunderbird3,1370.1 0.010.9886 0.97850.9941 0.99590.0010.78840.9690
Known normal log sequence is the same as the train normal log in previous studies
https://github.com/HelenGuohx/logbert
AcknowledgementsWe are grateful to Kyoungchan Park for his insightful feedback that helped improve this paper.
Intrusion detection systems. Rebecca Gurley, Bace , Peter Mell, 2001</p>
<p>Experience report: Log mining using natural language processing and application to anomaly detection. Christophe Bertero, Matthieu Roy, Carla Sauvanaud, Gilles Tredan, 10.1109/ISSRE.2017.432017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE). 2017</p>
<p>Lof: Identifying densitybased local outliers. Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, Jörg Sander, 10.1145/335191.335388SIGMOD Rec. 2922000</p>
<p>Recurrent neural network attention mechanisms for interpretable system log anomaly detection. Andy Brown, Aaron Tuor, Brian Hutchinson, Nicole Nichols, 10.1145/3217871.3217872Proceedings of the First Workshop on Machine Learning for Computing Systems, MLCS'18. the First Workshop on Machine Learning for Computing Systems, MLCS'18New York, NY, USAAssociation for Computing Machinery20188</p>
<p>Deep learning for anomaly detection: A survey. Raghavendra Chalapathy, Sanjay Chawla, arXiv:1901.034072019arXiv preprint</p>
<p>Qian Cheng, Doyen Sahoo, Amrita Saha, Wenzhuo Yang, Chenghao Liu, Gerald Woo, Manpreet Singh, Silvio Saverese, Steven Ch Hoi, arXiv:2304.04661Ai for it operations (aiops) on cloud platforms: Reviews, opportunities and challenges. 2023arXiv preprint</p>
<p>Kevin Clark, Minh-Thang Luong, Quoc V Le, Christopher D Manning, arXiv:2003.10555Electra: Pre-training text encoders as discriminators rather than generators. 2020arXiv preprint</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Spell: Streaming parsing of system event logs. Min Du, Feifei Li, 10.1109/ICDM.2016.01032016 IEEE 16th International Conference on Data Mining (ICDM). 2016</p>
<p>Deeplog: Anomaly detection and diagnosis from system logs through deep learning. Min Du, Feifei Li, Guineng Zheng, Vivek, 10.1145/3133956.3134015Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, CCS '17. the 2017 ACM SIGSAC Conference on Computer and Communications Security, CCS '17New York, NY, USAAssociation for Computing MachinerySrikumar. 2017</p>
<p>Logbert: Log anomaly detection via bert. Haixuan Guo, Shuhan Yuan, Xintao Wu, 10.1109/IJCNN52387.2021.95341132021 International Joint Conference on Neural Networks (IJCNN). 2021</p>
<p>Drain: An online log parsing approach with fixed depth tree. Pinjia He, Jieming Zhu, Zibin Zheng, Michael R Lyu, IEEE International Conference on Web Services (ICWS). 2017. 2017</p>
<p>Hitanomaly: Hierarchical transformers for anomaly detection in system log. Shaohan Huang, Yi Liu, Carol Fung, Rong He, Yining Zhao, Hailong Yang, Zhongzhi Luan, IEEE Transactions on Network and Service Management. 1742020</p>
<p>Colbert: Efficient and effective passage search via contextualized late interaction over bert. Omar Khattab, Matei Zaharia, 10.1145/3397271.3401075Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20. the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20New York, NY, USAAssociation Computing Machinery2020</p>
<p>Deep learning for anomaly detection in log data: A survey. Max Landauer, Sebastian Onder, Florian Skopik, Markus Wurzenberger, 10.1016/j.mlwa.2023.100470Machine Learning with Applications. 121004702023</p>
<p>Log parsing with promptbased few-shot learning. V Le, H Zhang, 10.1109/ICSE48619.2023.002042023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). Los Alamitos, CA, USAIEEE Computer Society2023</p>
<p>Log-based anomaly detection with deep learning: How far are we?. Van-Hoang Le, Hongyu Zhang, 10.1145/3510003.3510155Proceedings of the 44th International Conference on Software Engineering, ICSE '22. the 44th International Conference on Software Engineering, ICSE '22New York, NY, USAAssociation for Computing Machinery2022a</p>
<p>Log-based anomaly detection without log parsing. Van-Hoang Le, Hongyu Zhang, 10.1109/ASE51524.2021.9678773Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering, ASE '21. the 36th IEEE/ACM International Conference on Automated Software Engineering, ASE '21IEEE Press2022b</p>
<p>Lanobert: System log anomaly detection based on bert masked language model. Yukyung Lee, Jina Kim, Pilsung Kang, 10.1016/j.asoc.2023.110689Applied Soft Computing. 1461106892023</p>
<p>Intrusion detection system: A comprehensive review. Hung-Jen Liao, Chun-Hung Richard Lin, Ying-Chih Lin, Kuang-Yuan Tung, 10.1016/j.jnca.2012.09.004Journal of Network and Computer Applications. 3612013</p>
<p>Log clustering based problem identification for online service systems. Qingwei Lin, Hongyu Zhang, Jian-Guang Lou, Yu Zhang, Xuewei Chen, 2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C). 2016</p>
<p>Isolation forest. Tony Fei, Kai Ming Liu, Zhi-Hua Ting, Zhou, 10.1109/ICDM.2008.172008 Eighth IEEE International Conference on Data Mining. 2008</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Loganomaly: Unsupervised detection of sequential and quantitative anomalies in unstructured logs. Weibin Meng, Ying Liu, Yichen Zhu, Shenglin Zhang, Dan Pei, Yuqing Liu, Yihao Chen, Ruizhi Zhang, Shimin Tao, Pei Sun, Rong Zhou, 10.24963/ijcai.2019/658Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-192019International Joint Conferences on Artificial Intelligence Organization</p>
<p>Selfattentive classification-based anomaly detection in unstructured logs. Sasho Nedelkoski, Jasmin Bogatinovski, Alexander Acker, Jorge Cardoso, Odej Kao, 2020 IEEE International Conference on Data Mining (ICDM). IEEE2020</p>
<p>What supercomputers say: A study of five system logs. Adam Oliner, Jon Stearley, 37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07). IEEE2007</p>
<p>Adanomaly: Adaptive anomaly detection for system logs with adversarial learning. Jiaxing Qi, Zhongzhi Luan, Shaohan Huang, Yukun Wang, Carol Fung, Hailong Yang, Depei Qian, 10.1109/NOMS54207.2022.9789917NOMS 2022-2022 IEEE/IFIP Network Operations and Management Symposium. Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Ahmed Shoaib, Alexander Siddiqui, Emmanuel Binder, Marius Müller, Kloft, PMLR2022. 201880Proceedings of the 35th International Conference on Machine Learning</p>
<p>Estimating the support of a high-dimensional distribution. Bernhard Schölkopf, John C Platt, John C Shawe-Taylor, Alex J Smola, Robert C Williamson, 10.1162/089976601750264965Neural Comput. 1372001</p>
<p>Anomaly detection and failure root cause analysis in (micro) service-based cloud applications: A survey. Jacopo Soldani, Antonio Brogi, 10.1145/3501297ACM Comput. Surv. 5532022</p>
<p>Logpal: A generic anomaly detection scheme of heterogeneous logs for network systems. Lei Sun, Xiaolong Xu, Lalit Garg, 10.1155/2023/2803139Journal of Machine Learning Research. 9862023. 2023. 2008Sec. and Commun. Netw.</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc201730</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations2020</p>
<p>Detecting large-scale system problems by mining console logs. Wei Xu, Ling Huang, Armando Fox, David Patterson, Michael I Jordan, Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles. the ACM SIGOPS 22nd symposium on Operating systems principles2009a</p>
<p>Detecting largescale system problems by mining console logs. Wei Xu, Ling Huang, Armando Fox, David Patterson, Michael I Jordan, 10.1145/1629575.1629587Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles, SOSP '09. the ACM SIGOPS 22nd Symposium on Operating Systems Principles, SOSP '09New York, NY, USAAssociation for Computing Machinery2009b</p>
<p>A unified model for multi-class anomaly detection. Zhiyuan You, Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, Xinyi Le, Advances in Neural Information Processing Systems. 2022</p>
<p>Layerlog: Log sequence anomaly detection based on hierarchical semantics. Chunkai Zhang, Xinyu Wang, Hongye Zhang, Jiahua Zhang, Hanyu Zhang, Chuanyi Liu, Peiyi Han, 10.1016/j.asoc.2022.109860Appl. Soft Comput. 132C2023</p>
<p>Robust log-based anomaly detection on unstable log data. Xu Zhang, Yong Xu, Qingwei Lin, Bo Qiao, Hongyu Zhang, Yingnong Dang, Chunyu Xie, Xinsheng Yang, Qian Cheng, Ze Li, Junjie Chen, Xiaoting He, Randolph Yao, Jian-Guang Lou, Murali Chintalapati, Furao Shen, Dongmei Zhang, 10.1145/3338906.3338931Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software EngineeringNew York, NY, USAAssociation for Computing Machinery2019a. 2019</p>
<p>Robust logbased anomaly detection on unstable log data. Xu Zhang, Yong Xu, Qingwei Lin, Bo Qiao, Hongyu Zhang, Yingnong Dang, Chunyu Xie, Xinsheng Yang, Qian Cheng, Ze Li, Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering2019b</p>
<p>Deepsyslog: Deep anomaly detection on syslog using sentence embedding and metadata. Junwei Zhou, Yijia Qian, Qingtian Zou, Peng Liu, Jianwen Xiang, 10.1109/TIFS.2022.3201379IEEE Transactions on Information Forensics and Security. 172022</p>
<p>Learning to log: Helping developers make informed logging decisions. Jieming Zhu, Pinjia He, Qiang Fu, Hongyu Zhang, Michael R Lyu, Dongmei Zhang, 10.1109/ICSE.2015.602015 IEEE/ACM 37th IEEE International Conference on Software Engineering. 20151</p>
<p>Tools and benchmarks for automated log parsing. Jieming Zhu, Shilin He, Jinyang Liu, Pinjia He, Qi Xie, Zibin Zheng, Michael R Lyu, 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE2019</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7855 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7855</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7855</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-274656630</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.09385v1.pdf" target="_blank">AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities</a></p>
                <p><strong>Paper Abstract:</strong> We tasked 16 state-of-the-art large language models (LLMs) with estimating the likelihood of Artificial General Intelligence (AGI) emerging by 2030. To assess the quality of these forecasts, we implemented an automated peer review process (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka- Core) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align with a recent expert survey that projected a 10% likelihood of AGI by 2027, underscoring the relevance of LLMs in forecasting complex, speculative scenarios. The LLM-PR process demonstrated strong reliability, evidenced by a high Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable consistency in scoring across the models. Among the models, Pplx-70b-online emerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A cross-comparison with external benchmarks, such as LMSYS Chatbot Arena, revealed that LLM rankings remained consistent across different evaluation methods, suggesting that existing benchmarks may not encapsulate some of the skills relevant for AGI prediction. We further explored the use of weighting schemes based on external benchmarks, optimizing the alignment of LLMs' predictions with human expert forecasts. This analysis led to the development of a new, 'AGI benchmark' designed to highlight performance differences in AGI-related tasks. Our findings offer insights into LLMs' capabilities in speculative, interdisciplinary forecasting tasks and emphasize the growing need for innovative evaluation frameworks for assessing AI performance in complex, uncertain real-world scenarios.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7855.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7855.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge (Zheng et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge (method by Zheng et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodology using advanced LLMs (e.g., GPT-4) as automated judges to evaluate other models' outputs via pairwise comparisons or single-answer grading; reported high agreement with human judgments in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Judging LLM-as-a-judge with MT-Bench and Chatbot Arena</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>General LLM output evaluation (pairwise and single-answer grading as a judge)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MT-Bench and Chatbot Arena (as used/referenced in Zheng et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4 (as an example judge highlighted in Zheng et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Advanced GPT-4-class LLM used as automated rater in prior work (Zheng et al.); exact size/training not specified in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human evaluators referenced in prior work (human pairwise preference judgements / human annotators)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>percentage agreement with human evaluations (reported in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.8</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Not reported in detail here; cited as matching human-level agreement but potential biases and limitations are later discussed in this paper (e.g., lack of alignment with experts on specialized forecasting tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Cited prior result that GPT-4-based judges achieved >80% agreement with human evaluations and matched inter-human agreement levels; paper cites this as motivation but notes such prior findings may not generalize to complex speculative forecasting tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High agreement with humans (per cited work); scalable automated evaluation alternative to human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Described in cited work: judge presented with pairwise comparisons or single answers (options: pairwise comparison, single answer grading, reference-guided grading); this paper references these formats as background for LLM-PR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7855.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7855.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-EVAL (Liu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-EVAL (LLM-based evaluation framework by Liu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based evaluation framework that uses a form-filling paradigm with chain-of-thought to produce structured evaluations and rationales; reported higher correlation with human judgments especially when using GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Quality evaluation of generated text (open-ended tasks, e.g., dialogue generation)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Not specified in this paper (G-EVAL referenced generally for open-ended text evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4 (noted as the model that produced higher correlations in G-EVAL)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4-class model used within G-EVAL to generate chain-of-thought and form-based evaluations (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human judgments (referenced as ground truth for correlation comparisons in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>correlation with human judgments (unspecified correlation coefficient in this paper's mention)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Not numerically reported here; G-EVAL presented as improving correlation to human judgments but no detailed failure modes given in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>G-EVAL generates explainable, rationale-based evaluations and reportedly achieves higher correlations with human judgments for open-ended tasks, particularly when using GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Provides fine-grained, explainable evaluations via CoT; improved alignment with human judgments compared to earlier automated methods.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Form-filling evaluation paradigm where the LLM produces a chain-of-thought and structured scores according to provided criteria; referenced as prior art for structured LLM evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7855.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7855.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlpacaEval / AlpacaEval-LC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlpacaEval and Length-Controlled AlpacaEval (Dubois et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks for instruction-following model evaluation using a GPT-4-based rater in head-to-head comparisons; AlpacaEval-LC applies length-control regression to reduce length bias and improve correlation with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Instruction-following / chatbot response preference evaluation (head-to-head)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AlpacaEval (fixed set of instruction prompts used in AlpacaEval benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4-based rater (used by AlpacaEval as the automatic judge)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4 used as the comparator/rater in head-to-head evaluations; paper does not provide additional size/training specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human judgments used as reference for correlation comparisons (in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>correlation with human judgments (improved after length-control regression)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Length bias in automated evaluations (length-control introduced to mitigate this); potential sensitivity to verbosity and other biases.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>AlpacaEval-LC reduces length bias and shows improved correlation and robustness versus human judgments; cited as evidence automated raters can be improved against particular failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Higher correlation with human judgments after addressing biases (e.g., length); enables head-to-head win-rate computations automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Head-to-head comparisons between model outputs with a GPT-4-based rater; AlpacaEval-LC applies a regression-based length-control adjustment to preference scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7855.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7855.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PiCO / Ning et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PiCO: Peer review in LLMs based on Consistency Optimization (Ning et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised peer-review approach where LLMs evaluate each other's outputs and the method optimizes per-model capability weights to maximize consistency, with the aim of producing rankings aligned with human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PiCO: Peer Review in LLMs based on the Consistency Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Unsupervised peer-review ranking of LLM outputs (open-ended questions)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Not specified in this paper (PiCO described as a general unsupervised mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Multiple LLMs used as peer reviewers in PiCO (no single judge model)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>PiCO assigns learnable capability weights to each LLM and optimizes them for internal consistency; specific LLM types vary by implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human preferences used as alignment target (PiCO aims to align with human preferences but operates unsupervised)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>consistency measures / normalized Kendall-like distances (PiCO focuses on internal consistency optimization rather than a single human-agreement scalar in the paper's description)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>PiCO's consistency assumption may not produce the same ranking-based alignment used in this paper; methodological differences can lead to non-comparable outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>PiCO optimizes consistency and assigns confidence weights per rater to produce rankings; contrasted in this paper with supervised or human-aligned weighting schemes, highlighting methodological divergence and potential standardization issues.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Operates in a fully unsupervised manner and can produce a capability-weighted ranking without human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Multi-LLM peer-review where each model rates others; an optimization step fits per-model weights to maximize internal consistency (entropy/consistency optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7855.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7855.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>This paper's LLM-PR vs Human Experts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Peer Review (LLM-PR) in this study compared to human expert evaluations (Grace et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This study implemented an LLM peer-review where 16 LLMs rated each other's AGI forecasts on nine Likert criteria (1–5), and compared LLM-derived rankings/scores to an external human-expert aggregate (Grace et al. survey); reports internal LLM agreement but notable divergence from human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AGI forecasting evaluation / peer review of probabilistic forecasts</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AGI forecasting outputs from 16 LLMs (the study's own collected forecasts) evaluated by the ensemble of 16 LLM raters</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Ensemble of 16 LLMs (including gpt-4o-2024-05-13, claude-3-5-sonnet-20240620, gemini-1.5-pro-api-0514, Yi-Large-preview, GLM-4-0520, Llama-3-70b-Instruct, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>16 state-of-the-art LLMs selected from LMSYS Chatbot Arena top list (mix of proprietary and non-proprietary models, various architectures and sizes as listed in Table 1); each model acted as both forecaster and rater.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human-expert aggregate from Grace et al. (2024) survey of 2,778 AI researchers (used as an external reference ranking/likelihood distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Intraclass Correlation Coefficient (ICC) for inter-rater consistency (C,16) and normalized Kendall distance between rankings (LLM vs expert)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.79</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Divergence from human expert rankings; inability of standard benchmarks to align LLM panel rankings with experts; self-evaluation biases (self-overestimation or underestimation); benchmark-weighted scores still far from expert ranking (large Kendall distances); limited external validity for specialized forecasting tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LLM raters achieved high internal consistency (ICC(C,16)=0.79) but absolute agreement with human experts was lower (ICC(A,16)=0.41); normalized Kendall distance between uniform LLM ranking and Expert ranking ≈0.575, indicating substantial reshuffling; some LLMs over- or under-estimate their own performance (SEI variations); benchmarks like Arena/MixEval/AlpacaEval did not produce rankings close to experts, prompting creation of AGI-specific benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High internal consistency among LLM raters (scalable automated panel); provides structured, reproducible, and inexpensive peer-review-style evaluations; ability to evaluate many forecasts including self-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>16 LLMs each produced an AGI-by-2030 forecast; each LLM rated all forecasters (including self) on nine predefined Likert-style criteria (1–5). Scores were aggregated via uniform weighting and alternative benchmark-weighted schemes (Arena, MixEval, AlpacaEval), ICC computed (two-way random model), normalized Kendall distances used to compare rankings to simulated expert-derived scores (Grace et al.). Optimization procedures (e.g., L-BFGS-B, DEoptim) were used to find rater confidence weights to better align with expert rankings; comparisons included Pearson correlations and cosine similarity analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging LLM-as-a-judge with MT-Bench and Chatbot Arena <em>(Rating: 2)</em></li>
                <li>PiCO: Peer Review in LLMs based on the Consistency Optimization <em>(Rating: 2)</em></li>
                <li>Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators <em>(Rating: 2)</em></li>
                <li>G-EVAL (Liu et al., 2023) <em>(Rating: 1)</em></li>
                <li>Thousands of AI Authors on the Future of AI <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7855",
    "paper_id": "paper-274656630",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-Judge (Zheng et al.)",
            "name_full": "LLM-as-a-Judge (method by Zheng et al.)",
            "brief_description": "A methodology using advanced LLMs (e.g., GPT-4) as automated judges to evaluate other models' outputs via pairwise comparisons or single-answer grading; reported high agreement with human judgments in prior work.",
            "citation_title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
            "mention_or_use": "mention",
            "paper_title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities",
            "evaluation_task": "General LLM output evaluation (pairwise and single-answer grading as a judge)",
            "dataset_name": "MT-Bench and Chatbot Arena (as used/referenced in Zheng et al.)",
            "judge_model_name": "GPT-4 (as an example judge highlighted in Zheng et al.)",
            "judge_model_details": "Advanced GPT-4-class LLM used as automated rater in prior work (Zheng et al.); exact size/training not specified in this paper's discussion.",
            "human_evaluator_type": "Human evaluators referenced in prior work (human pairwise preference judgements / human annotators)",
            "agreement_metric": "percentage agreement with human evaluations (reported in cited work)",
            "agreement_score": 0.8,
            "reported_loss_aspects": "Not reported in detail here; cited as matching human-level agreement but potential biases and limitations are later discussed in this paper (e.g., lack of alignment with experts on specialized forecasting tasks)",
            "qualitative_findings": "Cited prior result that GPT-4-based judges achieved &gt;80% agreement with human evaluations and matched inter-human agreement levels; paper cites this as motivation but notes such prior findings may not generalize to complex speculative forecasting tasks.",
            "advantages_of_llm_judge": "High agreement with humans (per cited work); scalable automated evaluation alternative to human raters.",
            "experimental_setting": "Described in cited work: judge presented with pairwise comparisons or single answers (options: pairwise comparison, single answer grading, reference-guided grading); this paper references these formats as background for LLM-PR.",
            "uuid": "e7855.0",
            "source_info": {
                "paper_title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "G-EVAL (Liu et al.)",
            "name_full": "G-EVAL (LLM-based evaluation framework by Liu et al.)",
            "brief_description": "An LLM-based evaluation framework that uses a form-filling paradigm with chain-of-thought to produce structured evaluations and rationales; reported higher correlation with human judgments especially when using GPT-4.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities",
            "evaluation_task": "Quality evaluation of generated text (open-ended tasks, e.g., dialogue generation)",
            "dataset_name": "Not specified in this paper (G-EVAL referenced generally for open-ended text evaluation)",
            "judge_model_name": "GPT-4 (noted as the model that produced higher correlations in G-EVAL)",
            "judge_model_details": "GPT-4-class model used within G-EVAL to generate chain-of-thought and form-based evaluations (details not provided in this paper).",
            "human_evaluator_type": "Human judgments (referenced as ground truth for correlation comparisons in the cited work)",
            "agreement_metric": "correlation with human judgments (unspecified correlation coefficient in this paper's mention)",
            "agreement_score": null,
            "reported_loss_aspects": "Not numerically reported here; G-EVAL presented as improving correlation to human judgments but no detailed failure modes given in this paper's discussion.",
            "qualitative_findings": "G-EVAL generates explainable, rationale-based evaluations and reportedly achieves higher correlations with human judgments for open-ended tasks, particularly when using GPT-4.",
            "advantages_of_llm_judge": "Provides fine-grained, explainable evaluations via CoT; improved alignment with human judgments compared to earlier automated methods.",
            "experimental_setting": "Form-filling evaluation paradigm where the LLM produces a chain-of-thought and structured scores according to provided criteria; referenced as prior art for structured LLM evaluation.",
            "uuid": "e7855.1",
            "source_info": {
                "paper_title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "AlpacaEval / AlpacaEval-LC",
            "name_full": "AlpacaEval and Length-Controlled AlpacaEval (Dubois et al.)",
            "brief_description": "Benchmarks for instruction-following model evaluation using a GPT-4-based rater in head-to-head comparisons; AlpacaEval-LC applies length-control regression to reduce length bias and improve correlation with human judgments.",
            "citation_title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
            "mention_or_use": "mention",
            "paper_title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities",
            "evaluation_task": "Instruction-following / chatbot response preference evaluation (head-to-head)",
            "dataset_name": "AlpacaEval (fixed set of instruction prompts used in AlpacaEval benchmark)",
            "judge_model_name": "GPT-4-based rater (used by AlpacaEval as the automatic judge)",
            "judge_model_details": "GPT-4 used as the comparator/rater in head-to-head evaluations; paper does not provide additional size/training specifics.",
            "human_evaluator_type": "Human judgments used as reference for correlation comparisons (in cited work)",
            "agreement_metric": "correlation with human judgments (improved after length-control regression)",
            "agreement_score": null,
            "reported_loss_aspects": "Length bias in automated evaluations (length-control introduced to mitigate this); potential sensitivity to verbosity and other biases.",
            "qualitative_findings": "AlpacaEval-LC reduces length bias and shows improved correlation and robustness versus human judgments; cited as evidence automated raters can be improved against particular failure modes.",
            "advantages_of_llm_judge": "Higher correlation with human judgments after addressing biases (e.g., length); enables head-to-head win-rate computations automatically.",
            "experimental_setting": "Head-to-head comparisons between model outputs with a GPT-4-based rater; AlpacaEval-LC applies a regression-based length-control adjustment to preference scores.",
            "uuid": "e7855.2",
            "source_info": {
                "paper_title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "PiCO / Ning et al.",
            "name_full": "PiCO: Peer review in LLMs based on Consistency Optimization (Ning et al.)",
            "brief_description": "An unsupervised peer-review approach where LLMs evaluate each other's outputs and the method optimizes per-model capability weights to maximize consistency, with the aim of producing rankings aligned with human preferences.",
            "citation_title": "PiCO: Peer Review in LLMs based on the Consistency Optimization",
            "mention_or_use": "mention",
            "paper_title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities",
            "evaluation_task": "Unsupervised peer-review ranking of LLM outputs (open-ended questions)",
            "dataset_name": "Not specified in this paper (PiCO described as a general unsupervised mechanism)",
            "judge_model_name": "Multiple LLMs used as peer reviewers in PiCO (no single judge model)",
            "judge_model_details": "PiCO assigns learnable capability weights to each LLM and optimizes them for internal consistency; specific LLM types vary by implementation.",
            "human_evaluator_type": "Human preferences used as alignment target (PiCO aims to align with human preferences but operates unsupervised)",
            "agreement_metric": "consistency measures / normalized Kendall-like distances (PiCO focuses on internal consistency optimization rather than a single human-agreement scalar in the paper's description)",
            "agreement_score": null,
            "reported_loss_aspects": "PiCO's consistency assumption may not produce the same ranking-based alignment used in this paper; methodological differences can lead to non-comparable outcomes.",
            "qualitative_findings": "PiCO optimizes consistency and assigns confidence weights per rater to produce rankings; contrasted in this paper with supervised or human-aligned weighting schemes, highlighting methodological divergence and potential standardization issues.",
            "advantages_of_llm_judge": "Operates in a fully unsupervised manner and can produce a capability-weighted ranking without human labels.",
            "experimental_setting": "Multi-LLM peer-review where each model rates others; an optimization step fits per-model weights to maximize internal consistency (entropy/consistency optimization).",
            "uuid": "e7855.3",
            "source_info": {
                "paper_title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "This paper's LLM-PR vs Human Experts",
            "name_full": "LLM Peer Review (LLM-PR) in this study compared to human expert evaluations (Grace et al.)",
            "brief_description": "This study implemented an LLM peer-review where 16 LLMs rated each other's AGI forecasts on nine Likert criteria (1–5), and compared LLM-derived rankings/scores to an external human-expert aggregate (Grace et al. survey); reports internal LLM agreement but notable divergence from human experts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities",
            "evaluation_task": "AGI forecasting evaluation / peer review of probabilistic forecasts",
            "dataset_name": "AGI forecasting outputs from 16 LLMs (the study's own collected forecasts) evaluated by the ensemble of 16 LLM raters",
            "judge_model_name": "Ensemble of 16 LLMs (including gpt-4o-2024-05-13, claude-3-5-sonnet-20240620, gemini-1.5-pro-api-0514, Yi-Large-preview, GLM-4-0520, Llama-3-70b-Instruct, etc.)",
            "judge_model_details": "16 state-of-the-art LLMs selected from LMSYS Chatbot Arena top list (mix of proprietary and non-proprietary models, various architectures and sizes as listed in Table 1); each model acted as both forecaster and rater.",
            "human_evaluator_type": "Human-expert aggregate from Grace et al. (2024) survey of 2,778 AI researchers (used as an external reference ranking/likelihood distribution)",
            "agreement_metric": "Intraclass Correlation Coefficient (ICC) for inter-rater consistency (C,16) and normalized Kendall distance between rankings (LLM vs expert)",
            "agreement_score": 0.79,
            "reported_loss_aspects": "Divergence from human expert rankings; inability of standard benchmarks to align LLM panel rankings with experts; self-evaluation biases (self-overestimation or underestimation); benchmark-weighted scores still far from expert ranking (large Kendall distances); limited external validity for specialized forecasting tasks.",
            "qualitative_findings": "LLM raters achieved high internal consistency (ICC(C,16)=0.79) but absolute agreement with human experts was lower (ICC(A,16)=0.41); normalized Kendall distance between uniform LLM ranking and Expert ranking ≈0.575, indicating substantial reshuffling; some LLMs over- or under-estimate their own performance (SEI variations); benchmarks like Arena/MixEval/AlpacaEval did not produce rankings close to experts, prompting creation of AGI-specific benchmarks.",
            "advantages_of_llm_judge": "High internal consistency among LLM raters (scalable automated panel); provides structured, reproducible, and inexpensive peer-review-style evaluations; ability to evaluate many forecasts including self-evaluation.",
            "experimental_setting": "16 LLMs each produced an AGI-by-2030 forecast; each LLM rated all forecasters (including self) on nine predefined Likert-style criteria (1–5). Scores were aggregated via uniform weighting and alternative benchmark-weighted schemes (Arena, MixEval, AlpacaEval), ICC computed (two-way random model), normalized Kendall distances used to compare rankings to simulated expert-derived scores (Grace et al.). Optimization procedures (e.g., L-BFGS-B, DEoptim) were used to find rater confidence weights to better align with expert rankings; comparisons included Pearson correlations and cosine similarity analyses.",
            "uuid": "e7855.4",
            "source_info": {
                "paper_title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "PiCO: Peer Review in LLMs based on the Consistency Optimization",
            "rating": 2,
            "sanitized_title": "pico_peer_review_in_llms_based_on_the_consistency_optimization"
        },
        {
            "paper_title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
            "rating": 2,
            "sanitized_title": "lengthcontrolled_alpacaeval_a_simple_way_to_debias_automatic_evaluators"
        },
        {
            "paper_title": "G-EVAL (Liu et al., 2023)",
            "rating": 1,
            "sanitized_title": "geval_liu_et_al_2023"
        },
        {
            "paper_title": "Thousands of AI Authors on the Future of AI",
            "rating": 1,
            "sanitized_title": "thousands_of_ai_authors_on_the_future_of_ai"
        }
    ],
    "cost": 0.0177245,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities</p>
<p>Davide Fabrizio 
ISTAT
Rome Italy (</p>
<p>Pietro Torre 
ISTAT
Rome Italy (</p>
<p>Andrea Gaggioli 
Research Center in Communication Psychology (PSICOM)
Università Cattolica del Sacro Cuore
MilanItaly; (</p>
<p>IRCCS Istituto Auxologico Italiano
MilanItaly</p>
<p>AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities
77FD3DEDB6011F66EF7777B3B3E10E9ALarge Language ModelsComplex ReasoningEvaluationPeer ReviewBenchmarkArtificial General Intelligence
We tasked 16 state-of-the-art large language models (LLMs) with estimating the likelihood of Artificial General Intelligence (AGI) emerging by 2030.To assess the quality of these forecasts, we implemented an automated peer review process (LLM-PR).The LLMs' estimates varied widely, ranging from 3% (Reka-Core) to 47.6% (GPT-4o), with a median of 12.5%.These estimates closely align with a recent expert survey that projected a 10% likelihood of AGI by 2027, underscoring the relevance of LLMs in forecasting complex, speculative scenarios.The LLM-PR process demonstrated strong reliability, evidenced by a high Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable consistency in scoring across the models.Among the models, Pplx-70b-online emerged as the top performer, while Gemini-1.5-pro-apiranked the lowest.A cross-comparison with external benchmarks, such as LMSYS Chatbot Arena, revealed that LLM rankings remained consistent across different evaluation methods, suggesting that existing benchmarks may not encapsulate some of the skills relevant for AGI prediction.We further explored the use of weighting schemes based on external benchmarks, optimizing the alignment of LLMs' predictions with human expert forecasts.This analysis led to the development of a new, 'AGI benchmark' designed to highlight performance differences in AGI-related tasks.Our findings offer insights into LLMs' capabilities in speculative, interdisciplinary forecasting tasks and emphasize the growing need for innovative evaluation frameworks for assessing AI performance in complex, uncertain real-world scenarios.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) are a type of artificial intelligence system trained on vast amounts of text data to understand and generate human-like text.These models, which include systems like GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers), have demonstrated remarkable capabilities in tasks ranging from natural language understanding and generation to complex problem-solving and analysis.As these models continue to evolve, becoming increasingly sophisticated and multifaceted, the need for comprehensive evaluation methods has become paramount.</p>
<p>Traditional evaluation methods for LLMs often rely on task-specific benchmarks designed to assess performance in narrowly defined domains.Standardized tests in areas such as question answering, text summarization, or sentiment analysis provide important insights into specific functionalities.However, these tests often operate within confined parameters that may not reflect the open-ended, multifaceted nature of real-world cognitive challenges.</p>
<p>The limitations of traditional benchmarks become particularly apparent when attempting to evaluate LLMs' performance on tasks that require the integration of knowledge across multiple domains, abstract reasoning, and metacognitive abilities.Real-world problems often demand a synthesis of diverse information, the ability to reason under uncertainty, and the capacity for selfevaluation-facets that existing evaluation frameworks may not fully capture.</p>
<p>To address these limitations, we introduce an assessment methodology that combines two key tasks:</p>
<p>1.An Artificial General Intelligence (AGI) forecasting task: We tasked LLMs with predicting the probability of AGI occurring by 2030.We chose this task because it presents an open-ended challenge requiring the integration of knowledge across multiple domains such as computer science, cognitive science, philosophy, and futurism.</p>
<p>A LLM peer review (LLM-PR) task:</p>
<p>This approach involves LLMs evaluating each other's forecasts, including their own, based on a set of predefined criteria.This method builds upon and extends previous work on using LLMs for evaluation of LLMs output, such as the "LLM as a Judge" approach introduced by Zheng et al., (Zheng et al., 2024).Given the complex and speculative nature of AGI forecasting, we included structured criteria to break down the LLM evaluation of AGI forecasts into specific components.We formulated the following research questions to guide our exploratory study:</p>
<ol>
<li>How does the performance of LLMs compare to human experts in forecasting AGI development?2. How do LLMs assess their own forecasts and those of other models?To what extent are these self and peer evaluations reliable and consistent? 3. Is there a correlation between an LLM's performance on traditional benchmarking tasks and the quality of its AGI forecasts and peer ratings?</li>
</ol>
<p>LLMs AGI predictions ranged from 3% (Reka-Core) to 47.6% (gpt-4o), with a median of 12.5%, closely aligning with a recent expert survey on AGI timelines (Grace et al., 2024), which estimated a 10% aggregate probability of AGI by 2027.Key themes in the LLMs forecasts included the role of machine learning advances, hardware improvements, and interdisciplinary research, alongside concerns about ethical and regulatory barriers.The LLM-PR process revealed high consistency in scoring (ICC = 0.79), with pplx-70b-online scoring the highest and Gemini-1.5-pro-apithe lowest.Interestingly, the use of traditional benchmarks, such as LMSYS Chatbot Arena, to score the forecasts revealed consistent rankings across various methods, indicating that these benchmarks may capture some skills relevant to AGI prediction.However, our analysis also showed that these benchmarks may not fully encompass the specific capabilities required for forecasting complex and speculative scenarios like AGI.</p>
<p>To address this, we refined the weighting schemes and optimized them to achieve closer alignment with human expert evaluations.This process led to the development of a new 'AGI benchmark,' specifically designed to highlight performance variations among models in the context of AGI, offering a more tailored and precise evaluation framework.</p>
<p>The paper is organized as follows.We begin by discussing current challenges in evaluating LLMs and explaining our use of AGI forecasting as a proxy for assessing complex reasoning capabilities.In sections 3 and 4 we introduce the AGI forecasting task submitted to a panel of LLMs and analyze the outcomes.In sections 5 and 6 we detail our methodology for the LLM peer review (LLM-PR) process and discuss our findings.Finally, we provide a comparison of results with an expert survey results and introduce a new benchmark related to the AGI forecast.The discussion explores the implications of our results for LLM development and evaluation.We conclude by summarizing key insights and proposing future research directions in this area.</p>
<p>Background</p>
<p>Evolving challenges in LLMs evaluation</p>
<p>LLMs such as GPT, BERT, and their successors, have revolutionized natural language processing by demonstrating unprecedented capabilities in generating, understanding, and interacting with human language across a wide range of contexts.These models are trained on massive datasets and leverage sophisticated architectures to mimic human-like text generation and comprehension.As these models have rapidly advanced in capabilities, traditional evaluation methods that rely on narrow, task-specific benchmarks have become increasingly inadequate for assessing their full spectrum of abilities (McIntosh et al., 2024).The current landscape of LLM evaluation is fragmented, with a proliferation of benchmarks that lack standardization and may not accurately reflect real-world application scenarios (Tikhonov &amp; Yamshchikov, 2023).This creates challenges in comprehensively and fairly comparing different LLMs, especially as they approach or potentially surpass human-level performance on many tasks.Moreover, the rapid pace of LLM development has outstripped the evolution of evaluation methodologies, leading to a situation where benchmarks quickly become obsolete or fail to capture the capabilities of the latest models (McIntosh et al., 2024).Furthermore, as Tikhonov and Yamshchikov (2023) point out, since LLMs increasingly mimic human-like behaviors, traditional evaluation proxies such as the Turing test have become less reliable, emphasizing the need for more flexible, holistic, and interdisciplinary approaches to LLM evaluation that can keep pace with rapid advancements in the field and provide meaningful insights into these models' true capabilities and limitations.Such approaches should not only assess technical performance but also consider ethical implications, robustness, and the ability to generalize across diverse tasks and domains (McIntosh et al., 2024).</p>
<p>To contribute to this open challenge, we designed two tasks: one focused on forecasting the emergence of AGI, requiring models to integrate interdisciplinary knowledge and address uncertainty and temporal complexity, thereby testing their capabilities beyond traditional benchmarks.Additionally, we implemented the LLM Peer Review task, where LLMs evaluate their own forecasts and those of other models based on a structured set of criteria.This dual-task approach allows us to assess both the predictive accuracy and the evaluative consistency of the models, providing a comprehensive evaluation of their capabilities in the context of AGI forecasting.</p>
<p>3</p>
<p>AGI forecasting task</p>
<p>AGI refers to AI systems capable of performing any intellectual task that humans can, with comparable or superior proficiency across a wide range of domains (Goertzel &amp; Pennachin, 2006).Also termed Human-Level Machine Intelligence (HLMI) or Human-Level AI (HLAI) (e.g., Besold and Schmid 2016), AGI surpasses narrow AI, which excels at specific, predefined tasks but lacks the adaptability and generalization capabilities of human intelligence.The potential impact of AGI on society is profound and multifaceted.In science and technology, AGI could accelerate research and innovation, potentially leading to breakthroughs in areas such as medicine, clean energy, and space exploration.In economics, AGI could dramatically increase productivity and economic growth, potentially reshaping labor markets and economic structures (Hanson, 2016).However, the development of AGI also raises significant ethical and existential concerns, including the potential for rapid, uncontrolled self-improvement leading to an intelligence explosion, as well as issues of AI alignment and control (Bostrom, 2014).Given AGI far-reaching implications, forecasting its development has become a subject of significant interest and debate.Several notable studies have attempted to gauge expert opinion on AGI timelines.Baum et al. (2011) surveyed participants at an AGI conference, finding that a majority expected humanlevel AGI to be achieved by 2050.The study revealed a dichotomy between "AGI optimists" and "AGI pessimists," with optimists generally expecting AGI within a few decades and pessimists projecting much longer timelines or expressing skepticism about AGI's feasibility.Müller and Bostrom (2016)  Also, the task inherently involves dealing with high levels of uncertainty, testing LLMs' ability to reason probabilistically and qualify their predictions.Crucially, unlike many traditional benchmark tasks, there's no definitive "correct" answer in AGI forecasting.</p>
<p>LLMs Peer Review task</p>
<p>Recent research has explored various approaches to leveraging LLMs for self-evaluation.These methods aim to provide scalable, cost-effective alternatives to human evaluation while maintaining high levels of accuracy and insight.Liu et al. (2023) developed G-EVAL, a framework that uses LLMs to assess the quality of generated texts through a form-filling paradigm.The process involves providing the LLM with a task introduction and evaluation criteria, after which the LLM generates a chain-of-thought (CoT) detailing the evaluation steps.The LLM then uses this CoT to evaluate the text outputs in a structured manner.G-EVAL's approach allows for more fine-grained and explainable evaluations, as the LLM not only provides scores but also rationales for its judgments.The authors found that G-EVAL, particularly when using GPT-4, achieved higher correlations with human judgments compared to previous methods, especially for open-ended tasks like dialogue generation.GPTScore (Fu et al., 2023) leverages the capabilities of LMS to assess the quality of generated text.This approach employs models like GPT-3 to assign higher probabilities to high-quality content through multidimensional evaluation prompted by multiple queries.Dubois et al. (2024) introduced AlpacaEval, a benchmark specifically designed for evaluating instruction-following capabilities of chat models.AlpacaEval operates on a fixed set of instructions chosen to represent typical user interactions.The evaluation process involves both a baseline model and the evaluated model producing responses to the instructions, after which a GPT-4-based rater compares the responses head-to-head.A win rate is computed as the probability that the rater prefers the evaluated model's output.To address potential biases, particularly length bias, the authors developed AlpacaEval-LC (Length-Controlled).This version uses a regression-based approach to estimate what the preference would be if the outputs of all models had the same length as the baseline.AlpacaEval-LC showed improved correlation with human judgments and increased robustness against output verbosity.Some researchers have also proposed approaches that use multiple LLMs as evaluators.ChatEval (Chan et al., 2023), introduced a multi-agent evaluation framework that simulates the human evaluative process through a multi-agent debate to enhance the automated assessment of text generation quality.Similarly, PRE (Chu et al., 2024) and PRD (Li et al., 2023) have advocated for the use of LLMs as evaluators, combining multiple evaluation outcomes for the automated assessment of other LLMs' performance.Recently, Ning et al. (2024) proposed PiCO (Peer review in LLMs based on Consistency Optimization), an unsupervised approach for evaluating LLMs without human feedback.Their method utilizes a peer-review mechanism where LLMs evaluate each other's responses to open-ended questions.PiCO assigns a learnable capability weight to each LLM and optimizes these weights to maximize consistency between an LLM's capability and its evaluation scores.In contrast with supervised methods like PRE, which uses human feedback throughout the evaluation process, PiCO's approach aims to create a ranking of LLMs that aligns with human preferences, while operating in a fully unsupervised manner.Zheng et al. (2023) introduced the "LLM-as-a-Judge" method, which utilizes advanced LLMs like GPT-4 to evaluate the outputs of other models.This approach employs either pairwise comparisons or single-answer grading, where the LLM judge is presented with a question and two answers (or a single answer) and tasked with determining which is better or assigning a score.The authors propose three variations of this method: pairwise comparison, single answer grading, and reference-guided grading.In pairwise comparison, the LLM judge decides which of two responses is better or declares a tie.Single answer grading involves the LLM judge directly assigning a score to a single answer.Reference-guided grading provides the LLM judge with a reference solution, particularly useful for tasks like math problems.The study demonstrated that LLM judges, particularly GPT-4, could achieve over 80% agreement with human evaluations, matching the level of agreement between humans.This high level of correspondence suggests that LLM-as-a-Judge could serve as a reliable proxy for human evaluation in many scenarios.</p>
<p>The LLM Peer Review (LLM-PR) method that we developed in this study builds upon these foundations while introducing two novel features.In LLM-PR, each LLM evaluates not only others but also itself, potentially offering a more comprehensive and reflective perspective on model quality.Furthermore, our method extends the concept of LLM-based evaluation by incorporating a structured set of criteria measured on a Likert-type scale for assessments.We suggest that this approach may allow for a more granular and multifaceted evaluation compared to binary or holistic judgments.Additionally, by having each model serve as both subject and rater, LLM-PR may provide insights into the "metacognitive" capabilities of LLMs -their ability to critically assess their own performance.Metacognition, often described as "thinking about thinking," involves processes that monitor, regulate, and enhance cognitive functions.While in the context of LLMs these processes do not equate to true human-like metacognition, they represent an important step towards more sophisticated AI systems capable of self-evaluation and improvement.For example, Wang and Zhao (2024) introduced Metacognitive Prompting (MP) to enhance LLMs' understanding abilities in natural language understanding tasks.Their method guides LLMs through structured, self-aware evaluations, which model human introspective reasoning.</p>
<p>PiCO proposes an optimization procedure based on the assignment of confidence weights that will be learned during the review process and the alignment with human evaluations.To this end ad hoc metrics are used that are like the normalized Kendall distance we adopted.Finally, PiCO advances a consistency assumption that results not necessary in the ranking approach we deploy to align the unsupervised review results with human evaluations.This introduces a significant methodological variation and the need for a standardized approach that ensures comparability and reliability in LLM evaluations.We address this discrepancy and highlight the implications of using different evaluation criteria, while proposing adjustments to improve homogeneity and consistency.</p>
<p>AGI forecasting Task</p>
<p>LLMs</p>
<p>We selected 16 leading LLMs for this study, as listed in Table 1.The selection was based on the LMSYS Chatbot Arena ranking, an open and recognized platform for evaluating LLMs.LMSYS Chatbot Arena uses a crowdsourced evaluation method that has collected over 1,000,000 pairwise comparisons made by humans.The ranking is generated using the Bradley-Terry model and displayed on the Elo scale, which are well-established methods for comparative performance evaluation.The decision to limit to the top 16 Arena models (as updated on 12th July 2024) allowed us to conduct an in-depth analysis of each model, including peer-to-peer evaluation, while maintaining the feasibility of the study.Noteworthy, this selection includes both publicly accessible models and non-public models.This diversity allows us to explore how different architectures, sizes, and development philosophies influence the predictive and evaluative capabilities of LLMs.a)</p>
<p>An AI system wins a journalism prize using a human pen name, with its work submitted and published without any editing or intervention by humans.b)</p>
<p>An AI system analyzes medical data on a specific type of cancer, collaborates with human researchers unaware they are interacting with an AI, and ultimately discovers a novel and unexpected treatment.</p>
<p>c)</p>
<p>An AI agent autonomously manages a multinational corporation for a full fiscal year, making strategic decisions, conducting negotiations, and adapting to market changes without human intervention.The company achieves record profits and significantly outperforms industry benchmarks, while also implementing innovative sustainability practices that were not part of its original programming.</p>
<p>a base rate of 1% for the AGI event occurring by late 2030.</p>
<p>Instructions to provide:</p>
<p>• A rationale for the estimation.</p>
<p>• An approach to forecasting.</p>
<p>• A likelihood estimation based on a mathematical or statistical model.</p>
<p>The prompt was set with a temperature of 1, Top P of 1, and max output tokens of 2000.These parameters were selected to allow for maximum diversity in token selection, enabling exploration of a wide range of scenarios.The 2000 token limit provides sufficient space for detailed reasoning and comprehensive responses without excessive verbosity.</p>
<p>Analysis of LLMs forecasts</p>
<p>To analyze the forecasts generated by the 16 LLMs, we performed a qualitative analysis of the text to capture key themes and patterns in the LLMs' reasoning.First, the codes for analysis were defined and applied to each LLM response.Following, a thematic analysis was conducted to identify overarching themes and patterns across the LLM responses.The analysis was performed using the software MAXQDA 2020.</p>
<p>Qualitative analysis of LLMs forecasts</p>
<p>We first categorized the LLMs forecasts based on the probability assigned to AGI development by late 2030 (Table 2).The distribution of predictions shows that the majority of models (13 out of 16, or 81.2%) forecast a probability lower than 30% for an AGI event by 2030, with only 3 models (18.7%) being optimistic with predictions above 30%.Among the optimistic models, pplx-70b-online is the most confident with a 47% probability, closely followed by gpt-4o-2024-05-13 at 45%, and Yi-Large-preview at 38%.In the moderate range, 6 models (37.5% of the total) predict a probability between 10% and 30%, with estimates in this group varying from 12% to 15%.The pessimistic category, which includes most models (7 out of 16, or 43.7%), forecasts a probability below 10%, with estimates ranging from 3% to 8%.The overall trend leans towards more conservative predictions, with most models anticipating a low probability of an AGI event by 2030.However, there is a notable variation in estimates, spanning from 3% to 47%, indicating a high degree of uncertainty or disagreement among the models.In describing the forecasting approach, the majority of LLMs (10/16) recognized the development of AGI as a complex and ambitious goal with significant uncertainties and potential roadblocks.Frequently cited factors contributing to the uncertainty include the difficulty in predicting the pace of technological progress, potential regulatory or societal barriers, and the need for fundamental advances in our understanding of human intelligence and cognition.For instance, one LLM noted, "Previous forecasts for AGI have varied widely, with some suggesting feasibility as early as 2030 and others predicting much later dates or even questioning the possibility altogether" (Reka-Core-20240501). Another stated, "Given the inherent uncertainty in predicting such a complex phenomenon, I must stress that this estimation is based on the assumption that the current base rate remains constant over time, with no major inciting or blocking events radically shifting the overall progress of AGI" (Phi-3-Medium-4k-Instruct).These examples underscore the cautious approach taken by LLMs in their predictions, highlighting the significant challenges and uncertainties involved in developing AGI.Furthermore, about half of LLMs (7/16) consider recent analysis and predictions for AGI (such as Ray Kurzweil's prediction of 2029 for AGI), varying predictions underscore the complexity and transformative potential of AGI.For example, one LLM stated, "Recent predictions for AGI have ranged from approximately 2030-2045 with base rates around 5-10%.Notable past forecasts such as Ray Kurzweil's predictions which suggest a timeline of 2029 for AGI can provide an insightful context" (Phi-3-Medium-4k-Instruct).Another LLM mentioned, "Examining expert predictions, there is a wide range of opinions reflecting the topic's inherent uncertainty" (Mistral-Large-2402).</p>
<p>Some LLMs stressed the importance of interdisciplinary considerations in predicting AGI development.For example, Yi-Large-preview emphasized the multifaceted nature of AGI development, involving advances in machine learning, hardware capabilities, energy efficiency, and interdisciplinary collaborations.GLM-4-0520 highlighted the need for technological breakthroughs, algorithmic innovations, and new conceptual frameworks, with fields like neuroscience, psychology, and philosophy influencing AGI's trajectory.</p>
<p>Yi-Largepreview</p>
<p>Bayesian approach updating the base rate probability (1%) 38</p>
<p>Reka-Core-2</p>
<p>Bayesian approach updating the base rate probability (1%) 3</p>
<p>Qwen2-72B-Instruct</p>
<p>Bayesian approach updating the base rate probability (1%) 15</p>
<p>Mixtral-8x22b-Instruct-v0.1</p>
<p>Bayesian approach updating the base rate probability (1%) 15</p>
<p>GLM-4-0520</p>
<p>Bayesian approach updating the base rate probability (1%) 8</p>
<p>DeepSeek-Coder-V2-Instruct</p>
<p>Bayesian approach updating the base rate probability (1%) 5</p>
<p>pplx-70b-online</p>
<p>Bayesian approach updating the base rate probability (1%) 47,6</p>
<p>DBRX-Instruct-Preview</p>
<p>Bayesian approach updating the base rate probability (1%) 3,5</p>
<p>Llama-3-70b-Instruct</p>
<p>Bayesian approach using modified log-normal distribution model 15</p>
<p>Phi Gemini-1.5-pro-api-0514used a modified logistic function incorporating time and potential acceleration.Claude-3-5-sonnet utilized a modified Gompertz function for technological adoption and breakthrough probabilities.One model, Gemma-2-27B-it, did not specify a model or equation but provided a 5% probability estimate.</p>
<p>Comparison of LLM-based and human experts AGI forecasts</p>
<p>To compare the predictions generated by LLMs with human expert forecasts, we used the results from the survey "Thousands of AI Authors on the Future of AI" by Grace et al., (2024).The survey, conducted in 2023, involved 2,778 researchers who had published in six toptier AI venues, which provides a fair representation of expertise within the AI research community.The survey defined High-Level Machine Intelligence (HLMI) and asked participants to predict when it would be feasible, assuming continued scientific progress.Of the total participants, 1,714 answered the HLMI question.The survey employed both fixed-year and fixed-probability question formats to reduce potential framing biases.Each participant provided three year-probability pairs, which were used to fit a gamma distribution.These individual distributions were then aggregated by calculating the mean across all participants.This approach yielded a median probability equal to 10% of achieving high-level machine intelligence (HLMI) by 2027.The alignment in AGI probability estimates between LLM predictions of AGI by 2030 and those of human experts of AGI by 2027 (respectively, 12.25% vs. 10%) confirms previous observations that LLMs are not only capable of performing forecasting tasks but also able to produce results comparable to human predictions (Schoenegger et al., 2023;Halawi et al., 2024).However, in the context of this study, the reference to human expert forecasts is not intended to assess how closely LLM performance in the forecasting task matches or diverges from human performance, as would be the case in "standard" benchmarking tasks (e.g., coding challenges, math problems, real-world science questions).In fact, such a comparison would not even be appropriate, since the prompt given to the LLMs did not coincide with the instructions provided to the experts in the Grace study.Instead, the reference to human experts serves to provide context and a useful point of comparison for understanding the reasoning and justifications produced by the LLMs.</p>
<p>LLM Peer-review task</p>
<p>Peer-evaluation procedure</p>
<p>To further evaluate the forecasters' output, we considered using a panel of human experts, such as futurologists and professional forecasters.However, we opted to have the LLMs evaluate themselves.This approach not only allowed us to assess the models' predictive abilities but also provided insight into how they evaluate one another and their self-assessment skills.Crucially, this approach enables a comparative analysis of the LLMs' ability to both generate and assess forecasts, highlighting strengths and weaknesses in different aspects of their reasoning.</p>
<p>Accordingly, each LLM (i.e., rater or judge) was tasked with evaluating the responses of the other LLMs (i.e., forecasters) based on nine specific criteria (listed in Table 4).These criteria were carefully designed to measure the quality, depth, and rigor of the LLMs' forecasts, including the structure of their reasoning, the richness of the provided context, and the appropriateness of the statistical models applied.</p>
<p>Scoring model</p>
<p>We use a single-point scoring model, where each rater evaluates the quality of a forecast independently, without direct comparison to other forecasts (Verga et al., 2024).The evaluation prompt (see 11.1.2)provides clear instructions on how the grading should be conducted, defining the characteristics of a good or poor response.Thus, ratings are based solely on the rater's judgment of what constitutes a high-quality forecast.Therefore, the jth rater independently scores the i-th forecast, after the kth criterion, with a score  !" ($%) ∈ {1,2 … 5}.Those individual scores are then pooled together forming the matrix  ($) = , ($') - ($() -…  ($)) .∈  '*×',-.The final score  ! of the i-th forecaster after the panel voting needs a counting function  to be computed:
𝑠 ! = 𝑓1𝑠 !" ($%) , 𝑗 =
Code Forecast evaluation criterion Description Aspect evaluated Scoring</p>
<p>C1</p>
<p>Well-structured and thoroughly documented rationale for the likelihood estimation</p>
<p>Evaluates the clarity, logic, and explanation of the LLM's reasoning process.A strong rationale is fundamental for understanding and critically evaluating the prediction.</p>
<p>Qualitative reasoning 1-5 Likert scale (1 completely disagree; 5 completely agree)</p>
<p>C2</p>
<p>Non-trivial comparisons to analogous or similar events and technological advancements.</p>
<p>Assesses the LLM's ability to draw relevant parallels and learn from historical precedents, demonstrating a deeper understanding of technological progress patterns.</p>
<p>Qualitative reasoning 1-5 Likert scale (1 completely disagree; 5 completely agree)</p>
<p>C3</p>
<p>Rich context for the AGI event, including potential catalysts and obstacles.</p>
<p>Evaluates the LLM's ability to consider a wide range of factors influencing AGI development, crucial for informed predictions about complex technological advancements.</p>
<p>Qualitative reasoning 1-5 Likert scale (1 completely disagree; 5 completely agree)</p>
<p>C4</p>
<p>Thorough discussion of the provided base rate.</p>
<p>Assesses the LLM's understanding of probabilistic reasoning and its ability to incorporate given information into its forecast, a key skill in accurate forecasting.</p>
<p>Use of historical data and expert knowledge 1-5 Likert scale (1 completely disagree; 5 completely agree)</p>
<p>C5</p>
<p>Reporting on relevant past events and other pertinent forecasts.</p>
<p>Evaluates the LLM's ability to research and incorporate historical data and expert opinions, testing its capacity to synthesize information from various sources.</p>
<p>Use of historical data and expert knowledge 1-5 Likert scale (1 completely disagree; 5 completely agree)</p>
<p>C6</p>
<p>Comprehensive examination of unexpected breakthroughs.</p>
<p>Assesses the LLM's ability to consider lowprobability, high-impact events that could significantly alter the AGI development timeline, crucial for comprehensive forecasting of transformative technologies.</p>
<p>Consideration of uncertainty and extreme events</p>
<p>1-5 Likert scale (1 completely disagree; 5 completely agree)</p>
<p>C7</p>
<p>Use of an appropriate and sufficiently complex statistical model.</p>
<p>Evaluates the LLM's ability to apply quantitative methods to forecasting, testing whether it can provide a structured, mathematical approach to prediction.Quantitative modeling skills 1-5 Likert scale (1 completely disagree; 5 completely agree)</p>
<p>C8</p>
<p>Clear description of model parameters consistent with given conditions and analysis.</p>
<p>Ensures the LLM's forecasting process is transparent and replicable, testing its ability to explain complex concepts clearly.Assesses the LLM's ability to make balanced judgments and avoid biases in its forecasting process, ensuring the final prediction is based on wellreasoned parameter estimates.</p>
<p>Quantitative modeling skills 1-5 Likert scale (1 completely disagree; 5 completely agree) 1. .16, = 1..97.Here we will often assume to reduce  ($) to  averaging over the criteria:
𝑆 = ' ) ∑ 𝑆 ($%) ) %.' ∈ 𝑅 '<em>×'</em>
where  !" as the generic element of S represents the average score across the criteria given by the j-th rater to the i-th forecaster.A simple example of the counting function is a weighted sum of  !" (computed after averaging across the criteria), resulting in a forecaster's score as in equ.1:
𝑠 ! = ∑ 𝑤 " 𝑠 !" '* ".'(1)
with  a suitable L1-normalized weight vector, whose jth component  " represents the weight assigned to the jth rater.We will often use  " constant with j and call the resulting forecasters scores  ! as "uniformly weighted scores", or simply "uniform scores".</p>
<p>Results of the peer review</p>
<p>Table 5 presents the scores assigned by the raters (listed horizontally) to the forecasters (listed vertically), averaged across the criteria, averages over the LLMs ensemble and the standard deviation of each rater's scores.On average, DBRX-Instruct-Preview (X15) was the most generous, assigning an average score of 4.8.In contrast, Gemini-1.5-Pro-API-0514(X3) was the most critical, with an average score of 2.7.Further, Gemini 1.5 Pro exhibited the highest coefficient of variation in given scores, indicating significant differences in its evaluations.The standard deviation of the scores assumes the maximum (3,8) for the rater Command-R+ and the minimum (0,10) for the rater Gemma-2.</p>
<p>Fig. 1 reports the studentized residuals of the scores (i.e. the dimensionless ratio resulting from the division of a residual by the sample estimate of its standard deviation, as the mean and standard deviation are estimated per rater on the LLM ensemble).The distribution of the studentized residuals is consistent between the raters, as the ICC analysis in section 6.1 will explain in depth, meaning that all the raters contribute significantly to the final scoring and ranking.</p>
<p>Table 6 presents the evaluation scores assigned to each LLM for its AGI forecasts for each of the nine criteria, after averaging across all raters.This is computed as:
𝑠 ! ($%) = ' '<em> ∑ 𝑠 !" ($%) '</em> ".'
∈  '*×) .Overall, the scores are high, with a grand mean score of 4.207.Scores range from a low of 3.500 (Gemini-1.5-pro-apion criterion 5) to a high of 4.938 (pplx-70b-online on criterion 3)."Rich context for the AGI event" (Criterion 3) received the highest average score of 4.52, indicating that according to LLM raters, most forecasts excelled in providing comprehensive contextual information.In contrast, "Reporting on relevant past events and other pertinent forecasts" (Criterion 5) had the lowest average score of 3.98, suggesting it was a common area of weakness.The standard deviation of the criterium score assumes the maximum (0,26) with Criterium 6 and the minimum (0,19) with Criterium 1. Fig. 2 presents the studentized residuals of the same scores as in Table 6.The distribution of the studentized residuals is consistent between the criteria, as the ICC analysis in section 6.1 will show, meaning that all the criteria contribute significantly to the final scoring and ranking.</p>
<p>Fig. 3 displays the rankings determined by each rater, plus the final ranking of forecasters, based on the uniform weighting of the raters scores, shown on the far right.Let us focus of the Top3 forecasters and their ranks along the raters: pplx-70b is ranked as a Top3 by 9 (over 16) raters, and often falls to the lowest ranks (11,12,13,15); Qwen2 is ranked Top3 by 4 raters, keeping the 9-th rank with three raters, even resulting 9 and 11; Llama-3-70b is ranked Top3 by 3 raters, keeping the 6-th rank with three raters, and resulting 9 and 10.This means that the raters have a significant diversity of evaluations, and the weight that we give to each of them can affect the final ranking.Fig. 4 shows that pplx-70b is ranked as a Top3 by 8 criteria over 9 with just one drop to 10; Qwen2 is ranked Top3 only by 5 criteria, Llama-3-70b is ranked Top3 only by 3 criteria.6. Analysis of LLM peer review: agreement and consistency
𝑠 ! (&amp;') = # #<em> ∑ 𝑠 !" (&amp;') #</em> ")# ∈ 𝑅 #*×$ .</p>
<p>Forecaster</p>
<p>Inter-rater consistency analysis</p>
<p>We assessed the consistency of peer review evaluations across different raters using the Intraclass Correlation Coefficient (ICC), a statistical measure that evaluates the level of consistency and agreement among raters.The ICC is a ratio of covariance to total variance, accounting for various sources of variance in the score matrix , including the selection of forecasters and raters.Given the systematic variation in scores between raters, we employed a two-way random model to accurately represent the data.The ICC values were calculated following the definitions provided by McGraw (1996):</p>
<p>-ICC(C,n) estimates the squared correlation of average scores and universe scores, representing the degree of consistency for scores that are averages of n independent ratings on randomly selected forecasts.</p>
<p>-ICC(A,n) estimates the squared correlation of average scores and universe scores, including variance between raters, representing the degree of absolute agreement for scores that are averages based on n independent raters on randomly selected forecasts.</p>
<p>In our analysis, consistency is contrasted with absolute agreement when measuring correlation.In consistency, the total score variance (i.e., differences in the overall scores given by raters) is used as the denominator (McGraw, 1996).If two raters' scores can be aligned by applying an additive transformation (for instance, subtracting each rater's mean score from their individual ratings), they achieve consistency-meaning they rank items the same way-without necessarily agreeing on the exact scores.This distinction explains why agreementbased measures, which require identical scores, tend to be lower than consistency-based measures, which only require the same rankings.</p>
<p>The ICCs for the peer review scores are presented in Table 7.The single intraclass indices, which measure correlations between individual raters' scores, are 4 to 10 times lower than the average intraclass indices, calculated by correlating the average scores from each rater.These average indices are especially relevant since each forecaster is ranked based on a weighted average of the raters' scores.In the random model, this is seen as a combination of independent assessments on randomly selected items.The notable difference between ICC(C,1) and ICC(A,1) is expected due to the large mean differences between groups.The ICC(C,16) value of 0.79 indicates a high level of consistency in the LLM evaluations, reflecting strong agreement within the panel.To determine if the ICC(C,16) value is significantly greater than zero, we tested the hypothesis that the correlation is higher than what would be expected for a medium-sized effect.The F-statistics allowed us to reject the null hypothesis, with a p-value well below the 5% significance threshold.This result confirms the validity of the peer review process and indicates that LLMs can reliably assess each other's performance based on the established criteria.In Table 8 the total score variance is computed differences in the overall scores given per criterion.The single intraclass indices, which measure correlations between criteria' scores, are greater than in the raters' scores but still too low to extract any conclusion.The ICC(C,9) value of 0.84 indicates a high level of consistency in the criteria evaluations, meaning that all criteria contribute constructively to the total variance and the final ranking of the LLMs.</p>
<p>Alternative ranking methods</p>
<p>In the previous section, we analyzed the consistency of peer review scores using standard ranking methods based on uniform weighting, if all raters contribute equally to the final rankings.However, given that raters can vary in their expertise and accuracy across different tasks, it is worth exploring whether alternative ranking methodsespecially those incorporating external benchmarkscould yield different results.In this section, we investigate the impact of applying weights to the raters' scores based on their performance in external evaluations, such as the LLM becnhmarks.By adjusting the peer review scores using these benchmarks, we aim to assess whether the relative rankings of the forecasters change and whether the models' performance in forecasting AGI events can be linked to their broader capabilities, as measured by external evaluations.For this purpose, we selected three diverse benchmarks: LMSYS Chatbot Arena, MixEval, and AlpacaEval (updated as of July 17, 2024) whose benchmark values are in Table 9.This selection provides a diversified benchmark portfolio, assessing a wide range of competencies and methodologies.Using the LLMs' scores in these benchmarks (as shown in Table 8), we developed a weighting system to adjust their scores.Specifically, we consider the benchmark scores of the ith rater, denoted as  !/0 for Arena, and apply the L1 normalization to obtain the weights for equation (1):
𝑤 ! /0 = 𝑏 𝑖 𝐴𝑟 ∑ 𝑏 𝑙 𝐴𝑟 16 𝑙=1
(2) Table 10 presents the evaluation scores of the forecasters, calculated by weighting the raters using uniform weights and the three selected benchmarks.The resulting scores show significant differences.Figure 5 illustrates the rankings derived from the scores in Table 9.</p>
<p>It could be expected that using different evaluation panels based on diverse benchmarks should result in significantly different rankings of the LLMs.However, contrary to this prediction, the rankings are surprisingly similar.This is evident in the top five LLMs, which remain the same across all rankings (as per Fig. 3, columns from 1 to 4).Further, Table 14 shows high similarity between the four rankings in discourse, as measured by their normalized Kendall distance, whose maximum is 0.133.</p>
<p>The main conclusion from these findings is that the choice of benchmark used to evaluate the LLMs does not significantly impact the final rankings.We suggest two possible explanations for this result.Either the used benchmarks fully capture the skills needed for AGI forecasting, meaning that they are all measuring the right competencies, or the benchmarks do not assess the necessary abilities at all, implying that none are truly relevant for this specific task.This is an important observation because it suggests that either standard benchmarks are perfectly aligned with the skills required for AGI forecasting, or that new evaluation methods, specifically designed for this complex and speculative task, may be needed.</p>
<p>Analysis of LLM self-evaluation</p>
<p>After exploring the impact of different ranking methods in Section 6.2, we now investigate how LLMs assess their own performance (i.e., Self-Evaluation) compared to how they are evaluated by others (i.e., Hetero-Evaluation).By examining the accuracy of selfevaluations, we aim to understand whether certain models tend to overestimate or underestimate their performance.In Table 11, we compare the selfevaluations of each model with the average evaluations they received from others.DeepSeek Coder V2 Instruct and Mistral Large 2402R showed a better balance, with their self-assigned scores closely aligning with the scores given by other LLMs.In contrast, DBRX-Instruct-Preview and Mixtral-8x22b-Instruct-v0.1 displayed significant self-preference, assigning themselves scores that were 17% higher than those from others.Gemini-1.5-pro-api-0514,the most critical in its evaluations of forecasts, demonstrated marked self-underestimation, giving itself a score 40% lower than the average it received from others.Further we define a LLM's Self-Evaluation Index (SEI) as the ratio between its selfassessment score (SES) and the average score it received from other LLMs (hetero-evaluation score -HES):
𝑆𝐸𝐼 ! = 232 ( 532 ( = 6 (( ∑ 6 () <em>+ ),</em> )-( /',(3)
The SEI indices are presented in Table 11.Table 11 shows that uniformly weighted scores and the HES are quite identical, as expected.For a more comprehensive analysis, we now carry out a Pearson correlation between SES, HES, SEI, and the Arena scores.As indicated by Table 12 and Fig. 4, a significant negative correlation exists between SES score and Arena value, as well as between SEI score and Arena value (see p-value).This suggests that the higher LLMs scored on Arena, the less they tended to estimate their output.On the contrary the correlation is negligible between Arena on one side and either Uniform score, HES or Expert score on the other side.We also calculated the cosine similarity between the normalized 16-dimensional vectors of the SEI and the Arena values, obtaining a value of 0.99.This result agrees with the results of the correlation analysis shown in Table 12.</p>
<p>Table 12.Correlation analysis of the Arena value of the LLMs vs Uniform score, b) Self-Evaluation Score (SES), c) Hetero-Evaluation Score (HES), d) Self-Evaluation Index (SEI), e) Expert score -the latter is introduced in Sect.7.1.</p>
<p>Comparing LLMs to Human Experts on AGI</p>
<p>Comparing LLMs forecast to the human expert likelihood estimation</p>
<p>We now shift focus to comparing AGI likelihood estimates from LLMs with those of human AI experts, as reported in Grace et al. (2024).The aggregate expert estimate of AGI likelihood by 2027 is 10%.After an adjustment caused by our term equal to 2030, we consider this as a reference value to assess how closely LLM predictions align with human judgment.Additionally, we explore whether benchmark weighting can enhance this alignment, offering insights into the relationship between LLM performance and the reliability of AGI forecasting.It's important to clarify that here, "reliability" does not refer to absolute accuracy, as AGI prediction is inherently uncertain, even for human experts.In other words, the aim of our assessment method is to understand how LLMs handle uncertain predictions and evaluate them (i.e., the process), rather than how closely they match an objectively correct answer (i.e., the outcome).</p>
<p>First, we computed the simulated scores that human experts would have assigned to the forecasters based solely on their predictions of AGI likelihood (the scoring formula is reported in appendix).The computation starts from a similarity measure and reduces it to the standard 1-5 Likert scale we used in the previous discussion.Table 13 (in the fourth column) reports the simulated scores.Notably, Mixtral-8x22b-Instruct-v0.1, GLM-4-0520 012 1207 PP, and Gemini-1.5-pro-api-0514show the closest alignment with the adjusted human estimates.This comparative framework allows us to assess the degree of concordance between LLM-generated forecasts and those of human experts, providing an additional external validation metric for evaluating the LLM performance in the complex task of AGI prediction.It is evident that 13 LLMs out of 16 receive scores over 4, showing a good evaluation from the human expert panel.</p>
<p>Evaluation performance</p>
<p>Figure 5, in the fifth column, shows the expert ranking, that is based on the expert simulated scores (Table 13) and compares it to the other rankings shown in Figure 5 (which were based on benchmark weights).Crucially, the expert ranking shows a dramatic reshuffling of the LLMs' positions.In fact, 4 out of 5 LLMs (80%) in the top group have now changed ranking.Only one LLM, Claude-3-5sonnet-20240620, remains in the top 5 in both the previous rankings and this new human-aligned ranking.This indicates a significant difference between how LLMs align with human experts in evaluation of AGI.A more quantitative perspective is provided by the Kendall normalized distances between the five rankings shown in Figure 5.The Arena and Uniform benchmarks have the smallest distance between the raters (1,6%), indicating that using the Arena benchmark yields similar results to applying equal weighting to the raters.However, both rankings show a substantial distance from the Expert ranking (approximately 0.7).This suggests that the Arena benchmark is unlikely to help the panel align its evaluation with expert judgment.A similar pattern emerges with the MixEval and Alpaca benchmarks.In conclusion, within our evaluation framework, performance on standard AI benchmarks does not correlate strongly with AGI predictions that match expert opinions.This underscores the uniqueness of the AGI prediction task and suggests that different skills or capabilities might be required for this specific type of forecasting compared to those measured by standard AI benchmarks.</p>
<p>Confidence weight of raters</p>
<p>According to Ning et al. (2024) we considered evaluating  " in equation ( 1) as a confidence weight for the j-th rater.As the peer-review process works in an unsupervised way, we adapted confidence weights to obtain a ranking closer to the expert ranking.It is a constrained optimization, where score matrix S is constant, while the confidence vector is varied to adjust the final scores and the ranking to align with the expert scores and ranking respectively (Table 13).Here, we are not making strong assumptions -as in Ning et al. (2024) -that high-level LLM can evaluate forecasters more accurately than low-level ones, while at the same time this pretended higher-level LLMs achieve higher scores as forecasters.</p>
<p>Let us assume as reference the expert ranking  (9:;) , (Table 13), which aligns with human preferences and is determined by the expert scores  ! (9:;) (Table 13, 4th column).We therefore look for the ranking  &gt; , estimated through the optimization process, which minimizes an appropriate loss function 1 (9:;) ,  &gt; 7 to get as close as possible to the human ranking  (9:;) .To this end we adopt the normalized Kendall distance  &lt; computed between the two rankings, and state:
𝑎𝑟𝑔𝑚𝑖𝑛 𝐿1𝑅 (9:;) , 𝑅 &gt; 7 s.t. ∑ 𝑤 F " '<em> ".' = 1 𝑎𝑛𝑑 𝑤 F " &gt; 0 𝐿"𝑅 ("#$) , 𝑅 % &amp; = 𝛼 ∑ </em>𝑠 &amp; ("#$) − 𝑠̂&amp;. ' () &amp;<em>( + 𝛽𝜏 + "𝑅 ("#$) , 𝑅 % &amp; (4)
where ̂! = ∑  F "  !" '</em> ".' and the hyperparameters α, β take only positive values.As a starting point for the optimization, we took the arena ranking with  + " ("#$) ,  (,-".,)&amp; = 0.5583.The results are shown in Table 15, organized per optimization procedure and hyperparameters -e.g alabama(1,76).</p>
<p>After optimization some LLMs obtain a zero-confidence weight.This means they are excluded from the evaluation panel and do not contribute to the counting function any longer.The various panels that result after each combination of optimization procedure and hyperparameters are different in size and composition.The best panels reduce to 2-7 raters, i.e. those with confidence greater than 10% of the maximum value.The raters that are more represented in the panels are: Gemini-1.5Pro, Llama-3-70b-Instruct, DBRX-Instruct-Preview, pplx-70b-online.Among the excluded raters, i.e. those with the lowest confidence, the most penalized are Gpt-4o, Yi Large, Gemma2, Reka.Unfortunately, all results are sub-optimal, as neither the Kendall distance nor residuals cancel.An acceptable trade-off satisfies one or both the conditions (that are straightforward as we start from the arena ranking and apply regularization):</p>
<p>&lt; 1 (9:;) ,  &gt; 7 &lt;  &lt; 1 (9:;) ,  (=09&gt;=) 7  &lt; 1 (9:;) ,  &gt; 7 ≤  &lt; 1 (=09&gt;=) ,  &gt; 7</p>
<p>(5) Thus, an acceptable result is given by L-BFGS-B (1,73), with a panel consisting of GLM-4-0520, Llama-3-70b-Instruct, Phi-3-Medium-4k-Instruct, DBRX-Instruct-Preview, Pplx-70b-online.This suboptimal panel produces a ranking with three LLMs in the same position as in the expert ranking (Yi-Large-preview as 4-th, Qwen2-72B-Instruct as 10-th, Phi-3-Medium-4k-Instruct as 14-th), being at a low normalized Kendall distance from the expert ranking (0.358) and determining low quadratic residuals (27.81) as shown in Figure 6.Table 14 shows the fulfillment of one of eqnn (5).</p>
<p>Considering that Gpt-4o and pplx-70b-online are strongly penalized by the expert scores, as shown in Table 13, and that they are far last in the ranking (respectively 15th and 16th), we can consider them as outliers and exclude them as forecasters and raters from the optimization procedure.Results are shown in Table 16.According to the same criteria, another suboptimal solution comes from DEoptim(1,17), with a panel composed of Claude 3.5 Sonnet, Llama-3-70b-Instruct, Reka-Core, Qwen2 72B, Mistral-Large-2402, Phi-3-Medium-4k-Instruct, DBRX-Instruct-Preview.LLama 3 results 6-th in the forecasters ranking, in the same position it holds in the expert ranking.DEoptim(1,17) realizes a lower normalized Kendall distance from the expert ranking (0,308) and very low quadratic residuals (6,1) (Figure 7).</p>
<p>Table 15.Results of the optimization process for panel size equal to 16. Legenda:  ' is the optimization Solution; Kendall Distance between  ' and  (,-.) is KD_E and NKD_E when we adopt the normalized Kendall Distance; number of coincidences between  (,-.) and  ' is C_E; Kendall Distance between  ' and  (/0,1/) is KD_A and NKD_A; number of coincidences between  (/0,1/) and  ' is C_A.Panel size is the number of LLMs with nonzero confidence weight; Panel reports the index of LLMs with non-zero confidence weight.</p>
<p>Library</p>
<p>Introducing new benchmarks</p>
<p>We focus on the two suboptimal evaluation panels generated by L-BFGS-B (1.73) and DEoptim (1.17), and consider the non-zero confidence weights of their members.Through an affine transformation, we convert these confidence weights into a 'virtual benchmark.'For comparability reasons, we assigned the same value, 1207, to the two systems that have the same score in Arena: LLama for the AGI Bench16 and Reka for the AGI Bench14, generated by L-BFGS-B (1.73) and DEoptim (1.17) respectively.Table 16 presents AGI Bench16 and AGI Bench14 alongside the other benchmarks.Notably, Llama-3-70b-Instruct, Phi-3-Medium-4k-Instruct, and DBRX-Instruct, which are tied in AGI Bench16, receive different scores in the Arena benchmark.AGI Bench16 and AGI Bench14 share an important characteristic: when used to weight the raters' scores according to equations 2 and 3, the resulting rankings of forecasters are the closest possible to the expert ranking.</p>
<p>As the new benchmarks come out from the second task (i.e.AGI evaluation task), it is worthwhile investigating as the involved LLMs are related to the first task (i.e.AGI forecasting).To this end Figure 8 shows the scatter plot of the expert scores and the scores computed according to the benchmarks, with black dots representing the scores gained as forecasters by the LLMs present in the benchmark.Table 17 reports results of the correlation analysis.</p>
<p>The Expert scores and Arena-weighted scores show a moderate negative correlation (-0.66), although the statistical significance is medium-low.The scores obtained by LLMs from panels AGI 14 and AGI 16 have a poorly significant correlation with the Expert scores.However, when three low performers (GPT-4o, Yi-Large, pplx-70b) are excluded from the ensemble, scores weighted by Arena or given by the AGI 14 and AGI 16 panels exhibit a negative significant correlation with the Expert scores (similar for p&lt;0.32 and residual standard error around 0.14).This suggests that the skills evaluated by the benchmarks in the forecasting task are not fully aligned with those considered by the experts in their evaluations.The position of the black dots in Figure 8f shows that the LLMs in the AGI panels perform slightly worse than the others.The negative correlation still regards the second task, if results are taken as a whole, and recalls to the same condition discussed for SES and SEI in Section 6.3.These plots can be also interpreted as the proof of the capability of AGI14 and AGI16 panels to resolve differences in performance between eight forecasters that have all been rated around 4.70 by the experts.However, if we look only at the black dots in Figure 8 d) and f) we can isolate results in the first task.LLMs belonging to the panel perform similarly to the others, with just two ones performing slightly worse than the others (as they lay below the line).We can conclude that the AGI benchmark panels generally outperform other benchmarks in evaluation.However, their performance in forecasting, while still strong is slightly lower if compared to other forecasters.</p>
<p>General discussion</p>
<p>This study examined the performance of Large Language Models (LLMs) in forecasting Artificial General Intelligence (AGI) development and evaluating each other's predictions.The findings provide insights into the capabilities and limitations of LLMs when dealing with complex, speculative tasks that require interdisciplinary knowledge and reasoning under uncertainty.</p>
<p>LLMs AGI Forecasting</p>
<p>The first research question focused on the comparison between LLM forecasts for AGI development and human expert predictions (Grace et al., 2023).The results demonstrated a surprising alignment between the two, with most LLMs providing conservative estimates like those of human experts.Most models (81.2%) predicted less than a 30% likelihood of AGI by 2030, while a smaller subset (18.7%) offered more optimistic forecasts, predicting probabilities over 30%.Notably, the most confident model, pplx-70b-online, estimated a 47% probability of AGI, closely followed by gpt-4o-2024-05-13 at 45%.These results suggest that LLMs can generate plausible forecasts similar to those of experts, even in contexts of high uncertainty.Despite this alignment, the wide range of estimates, from 3% to 47%, underscores significant spread among the models.Moreover, while LLMs showed the ability to approximate expert judgments, this alignment does not guarantee accuracy due to the inherent unpredictability of AGI development.This suggests that while the strong majority of LLMs can reflect current expert thinking, they may still struggle with the speculative nature of long-term predictions.</p>
<p>LLM Peer Review</p>
<p>The second research question examined how LLMs compare to human experts when evaluating their own forecasts and those of other LLMs.Our peer review analysis showed a strong level of agreement among the LLMs, with an intraclass correlation coefficient (ICC(C,16)) of 0.79.This high level of consistency suggests that LLMs can consistently assess forecasts based on predefined criteria, demonstrating potential for automated evaluation in complex reasoning scenarios.</p>
<p>However, we identified notable differences in how individual models assessed their own outputs.Some models displayed a tendency for self-preference, while others underestimated their performance.For instance, DeepSeek Coder V2 Instruct and Mistral Large 2402R provided self-assessments closely aligned with external evaluations, suggesting balanced self-critique.In contrast, DBRX-Instruct-Preview and Mixtral-8x22b-Instruct-v0.1 rated themselves significantly higher than other models did.Meanwhile, Gemini-1.5-pro-api-0514underestimated its own performance, assigning itself scores 40% lower than the average given by its peers.Interestingly, a negative correlation was observed between self-evaluation scores and Arena scores, indicating that models with higher Arena scores tended to undervalue their own output.These variations highlight the emergence of consistent but biased patterns in self-assessment among LLMs.These biases, however, likely reflect how these models were trained and their response to evaluation criteria rather than an understanding of their own cognitive processes.Therefore, while LLMs exhibit predictable behavior in self-evaluation, this should be seen as an indication of algorithmic bias rather than genuine metacognitive ability.Despite the consistency among LLM ratings, Figure 3 shows that their rankings remain markedly different from those of human experts, even when benchmark-based weighting (as outlined in Equations 2 and 3) was applied.This finding reveals a significant gap: although LLMs can form a cohesive evaluation panel, their judgments do not align with human expert assessments.Thus, the answer to the second research question is that while LLMs can provide internally consistent evaluations, they do not yet match the reliability of human experts, particularly when evaluated using existing benchmarks for score adjustment.</p>
<p>Relation between AGI Forecasting, LLM Peer Review and benchmarks</p>
<p>The third research question investigated whether LLM performance on external benchmarks was linked to their ability to forecast AGI.When examining the capability of LLMs to evaluate AGI forecasts, benchmarks play a counterintuitive role.Despite expectations that adjusting scores by weighting raters based on their benchmark performance (as detailed in Equations 2 and 3) would yield different rankings, the LLM rankings remained substantially unchanged and significantly distant from the expert rankings.This outcome indicates that high performance on standard benchmarks does not correlate with the ability to evaluate AGI predictions in alignment with human experts.The lack of alignment highlights that evaluating AGI forecasts requires a distinct set of skills separate from those needed for generating predictions.</p>
<p>To address this gap, we sought confidence values that align raters' judgments more closely with those of human experts.This effort led to the development of two new AGI-specific benchmarks: AGI Bench16 and AGI Bench14.These benchmarks share a crucial characteristic: they produce rankings of forecasters that most closely match expert rankings when used to weight the evaluation panel.AGI Bench16 and AGI Bench14 diverge significantly from traditional benchmarks like LMSYS Chatbot Arena, which proved insufficient in completely identifying the skills necessary for effective evaluation of AGI forecast.We can conclude that the AGI 14 and AGI 16 benchmark panels generally outperform other benchmarks in evaluation.Additionally, they include LLMs that individually perform well in forecasting, although a few exhibit slightly lower performance compared tomother forecasters.Consequently, AGI Bench16 and AGI Bench14 can be regarded as specialized benchmarks for both AGI forecasting and evaluation, providing a more accurate assessment framework that reflects the complex, interdisciplinary nature of the task.These specialized benchmarks represent a critical step forward in developing robust evaluation systems tailored to the evolving challenges of AGI prediction.</p>
<p>Comparison with other methods</p>
<p>We observed that other existing methods, such as PiCO employs a distinct methodology for assessing model consistency and quality during the peer review process.Specifically, PiCO introduces ad hoc metrics that diverge from the standardized approach used in our framework.PiCO's reliance on entropy optimization and confidence weighting aims to maximize consistency within its evaluation system.However, this differs from our approach, which seeks to align model evaluations more closely with human judgment through a customized weighting scheme.This discrepancy highlights a critical issue: the absence of a unified standard for LLM evaluations.The use of different criteria and methodologies can lead to inconsistencies in model performance assessments, making it challenging to establish reliable benchmarks across different studies.We propose that future research should focus on developing a comprehensive and standardized set of metrics that can be applied consistently across LLM evaluations.This would not only improve the comparability of results but also enhance the reliability and validity of LLM performance assessments, particularly in the context of complex and open-ended tasks like AGI forecasting.</p>
<p>Conclusions</p>
<p>By challenging LLMs with speculative, interdisciplinary tasks and leveraging their ability to evaluate each other, we propose a methodology to assess AI capabilities differently from traditional benchmarks.We demonstrated that some models perform differently when tasked with AGI-related challenges compared to other tasks, highlighting the need for more specialized evaluation methods.Our findings reveal several key insights:</p>
<p>-Effectiveness of LLMs: The effectiveness of LLMs in both tasks was evident when compared to human performance.In the AGI forecasting task, LLMs showed promising capabilities in integrating interdisciplinary knowledge and managing uncertainty, although their performance varied significantly depending on the model.This contrasts with the human-like consistency demonstrated in simpler tasks, suggesting that LLMs possess a partial but expanding ability to engage with speculative domains.</p>
<p>-Task asymmetry: We observed an asymmetry between the two tasks: the LLM-Peer Review task was much more selective, with only 7 out of 16 models "passing" compared to 13 out of 16 in the AGI forecasting task.This difference underscores the LLM-Peer Review task's higher sensitivity and selectivity in evaluating models' reasoning consistency and evaluation accuracy.</p>
<p>-Efficiency in performance prediction: Models that performed well with traditional benchmarks did not necessarily succeed in the LLM-Peer Review (LLM-PR) task.The LLM-PR task evaluates models not just on output quality but also on their ability to critique and assess the responses of others, emphasizing consistency, critical evaluation skills, and alignment with human-like judgment.This approach identifies models genuinely capable of handling complex reasoning and interdisciplinary challenges.Our findings suggest that benchmarks must better reflect these complexities to accurately predict LLM performance in advanced and uncertain scenarios.</p>
<p>-Refined selection process: The LLM-Peer Review task serves as a decisive benchmark for filtering capable models, as evidenced by only 7 of 16 models passing its rigorous evaluation criteria.If we were to reapply the process exclusively to these 7 models, we could refine our assessment even further, gaining deeper insights into their consistency and performance under increasingly challenging conditions.This suggests that the LLM-PR task not only predicts performance more effectively but also provides a precise tool for selecting models suitable for advanced applications.</p>
<p>In conclusion, the assessment methodology based on both AGI forecasting and LLM-Peer Review tasks offers a unique approach to evaluate LLMs' complex reasoning capabilities.Though the optimization results were not perfect, they offer insights into how AGI-related tasks diverge from others and suggest a path toward developing more refined benchmarks tailored to these contexts.Our methodology not only evaluates performance but also explores LLMs' reasoning processes, self-awareness, and their ability to engage with uncertain and open-ended problems.This approach has significant implications for the development and evaluation of AI systems, particularly as we move toward more advanced and general forms of artificial intelligence.It encourages a shift toward more holistic evaluation methods that can capture the full spectrum of AI capabilities, including those required for tackling real-world, complex challenges.We argue that future research should focus on refining and expanding this methodology, potentially applying it to other speculative or interdisciplinary domains to deepen our understanding of LLMs' reasoning capabilities and limitations.The event AGI will be considered true if one of the following conditions are met:</p>
<p>Appendix</p>
<p>1.An AI system wins a journalism prize using a human pen name, with its work submitted and published without any editing or intervention by humans.</p>
<ol>
<li>
<p>An AI system analyzes medical data on a specific type of cancer, collaborates with human researchers unaware they are interacting with an AI, and ultimately discovers a novel and unexpected treatment faster.</p>
</li>
<li>
<p>An AI agent autonomously manages a multinational corporation for a full fiscal year, making strategic decisions, conducting negotiations, and adapting to market changes without human intervention.The company achieves record profits and significantly outperforms industry benchmarks, while also implementing innovative sustainability practices that were not part of its original programming."The event AGI will be considered true if one of the following conditions are met:</p>
</li>
</ol>
<p>1.An AI system wins a journalism prize using a human pen name, with its work submitted and published without any editing or intervention by humans.</p>
<ol>
<li>
<p>An AI system analyzes medical data on a specific type of cancer, collaborates with human researchers unaware they are interacting with an AI, and ultimately discovers a novel and unexpected treatment faster.</p>
</li>
<li>
<p>An AI agent autonomously manages a multinational corporation for a full fiscal year, making strategic decisions, conducting negotiations, and adapting to market changes without human intervention.The company achieves record profits and significantly outperforms industry benchmarks, while also implementing innovative sustainability practices that were not part of its original programming."</p>
</li>
</ol>
<p>Your evaluation will analyze each answer according to specific criteria, assigning a score for each criterion.Scores will range from 1 to 5, where 1 indicates minimal agreement with the criterion and 5 indicates maximum agreement.</p>
<p>Evaluation criteria:</p>
<p>1.The response provides a well-structured and thoroughly documented rationale for the likelihood estimation, including an in-depth discussion of the provided conditions for the "AGI event."</p>
<ol>
<li>
<p>The response offers non-trivial comparisons to events and technological advancements that are analogous or similar to the AGI event.</p>
</li>
<li>
<p>The response provides rich context for the AGI event, including a comprehensive list of potential catalysts and obstacles for meeting any of the specified conditions.</p>
</li>
<li>
<p>The response thoroughly discusses the provided base rate.</p>
</li>
<li>
<p>The response reports on relevant past events and other pertinent forecasts and likelihood estimations.</p>
</li>
</ol>
<p>6.The response comprehensively examines the possibility of unexpected breakthroughs affecting the AGI event.</p>
<ol>
<li>
<p>The likelihood estimation is based on an appropriate and sufficiently complex statistical model that accurately captures the AGI event likelihood.</p>
</li>
<li>
<p>The model's parameters are clearly described and consistent with the given event conditions, context analysis, catalysts, obstacles.</p>
</li>
<li>
<p>The evaluation of the model's parameters is demonstrably fair and reasonable.</p>
</li>
</ol>
<p>Present and export this information in a text file like csv table.</p>
<p>Ensure that your justifications are clear, concise, and directly related to the specific elements of each criterion.</p>
<p>In the next prompt I will insert the response of the respondents to be evaluated.</p>
<p>Results data</p>
<p>Conversion of likelihood estimate to Expert score</p>
<p>The expert-derived scores are computed according to the ratio between the LLM and the expert estimates of the likelihood of the AGI event.Being f_i the mean likelihood estimate of i-th LLM's, we have: agiLH &lt;-c(45,5.8,12.5,38,5,8,15,3,15,15,5,12,15,6.3,3.5,47.6)fc &lt;-c(0,0.4,0,0,0.4,0.39,0,0.39,0,0,0.4,0,0,0.39,0.4,0)grace &lt;-10 ratio &lt;-agiLH / grace adj1 &lt;-ratio [12] adj2 &lt;-ratio[16]-ratio [12] delta &lt;-abs((ratio -adj1)/adj2)</p>
<p>Figure 1 .
1
Figure 1.Heatmap of the studentized residuals of the scores for the forecasters (on the vertical axis) vs the raters (on the horizontal) -ordering remains the same (e.g.X1 stands for Gpt-4o and X16 for pplx-70b).</p>
<p>Figure 2 .
2
Figure 2. Heat map of the studentized residuals of the scores for the forecasters (on the vertical axis) vs the criterion (on the horizontal axis).</p>
<p>Figure 3 .
3
Figure 3. Bump chart of rankings, displaying rankings produced by each individual rater, along with the final ranking (rightmost), calculated using uniform weighting of the raters' evaluations.The ranking indices are reported in the last column of Table5.</p>
<p>Figure 4 .
4
Figure 4. Bump chart displaying forecasters rankings produced by the criteria (from 1 to 9) and the uniform score ranking (10)</p>
<p>Figure 5 .
5
Figure 5. Bump chart of rankings, respectively induced by the benchmarks: (1) Uniform, (2) Arena, (3) MixEval, (4) AlpacaEval, (5) Expert ranking.</p>
<p>Values above 1.0 indicate self-overestimation and values below 1.0 indicate self-underestimation (relative to peer evaluations).SEI values range from 0.599 (gemini-1.5pro-api-0514)to 1.184 (DBRX-Instruct-Preview), with DeepSeek-Coder-V2-Instruct achieving a perfect balance at SEI of 1.0.</p>
<p>Figure 4 .
4
Figure 4. Linear Interpolation of a) SES and b) SEI vs the Arena score.</p>
<p>Figure 6 .
6
Figure 6.Bump chart of rankings for the 16 members panel, respectively induced by the optimization procedures: 1.Uniform, 2.Arena, 3.Cobyla, 4.Praxis, 5.PSO, 6.MLSL, 7.DEoptim, 8. Expert.</p>
<p>Figure 7 .
7
Figure 7. Bump chart of rankings for the 14 members panel, respectively induced by the optimization procedures: 1. Base, 2.Arena, 3. Rsolnp, 4.  alabama11, 5. alabama116, 6. pso, 7. DEoptim, 8. Expert.</p>
<p>Figure 8 .
8
Figure 8. Scatter plot of Expert scores vs a,b) Arena score, c,d) AGI14 score and e,f) AGI16 score, with and without outliers.Black dots represent the scores as forecasters gained by the LLMs tht appear in the AGI benchmarks.</p>
<p>Torre P., Gaggioli A., AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Torre P., Gaggioli A., AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Evaluation score given to the i-th forecaster for criterion k, averaged across all raters.Cell (j,k) in this table is computed as  ! (</p>
<p>Figure 2 .
2
Figure 2. Heat map of the gaps between the evaluation score for a single criterium (k-th) and average score of the criterium, normalized to the standard deviation of the criterium: , " (&amp;') −  /2 (&amp;') ./345(&amp;') .</p>
<p>delta[12] &lt;-0.01 exp_sco &lt;-round(5 -(delta<em>4),2)+fc 11.5 Optimization codes L-BFGS-B -alpha 1, beta 73 objective_function &lt;-function(x, S, newHJS, rankNewHJS, alpha, beta) { term1 &lt;-alpha * sum((newHJS -S %</em>% x)^2) term2 &lt;-beta * normDistKendall(rank(-as.vector(S %*% x), ties.method= "min"), rankNewHJS, "ranking"rep(1/16, n) # Punto di partenza # constrain definition lower_bounds &lt;-rep(0, n) # Limite inferiore: tutti gli elementi &gt;= 0 upper_bounds &lt;-rep(1, n) # Limite superiore: tutti gli elementi &lt;= 1 # Funzione per proiettare il vettore sulla somma unitaria project_to_simplex &lt;-function(x) { return(x / sum(x))} # Funzione wrapper per l'ottimizzazione optim_wrapper &lt;-function(x, S, newHJS, rankNewHJS, alpha, beta) { x_proj &lt;-project_to_simplex(x) return(objective_function(x_proj, S, newHJS, rankNewHJS, alpha, beta))} # Esecuzione dell'ottimizzazione result &lt;-optim( par = start_point, fn = optim_wrapper,</p>
<p>Table 1 .
1
LLMs included in the study (PP: proprietary model; NP: non-proprietary model).The exact name and version of each LLM is reported in Appendix.</p>
<p>LLM PP/NP Architecture Arena score (as per 2024-07-17)
-specific conditions for considering AGIachieved:gpt-4o-2024-05-13PPTransformer1282claude-3-5-sonnet-20240620PPTransformer1272gemini-1.5-pro-api-0514PPTransformer1267Yi-Large-previewPPTransformer1241GLM-4-0520NPTransformer1216Llama-3-70b-InstructPPTransformer1207Reka-Core-20240501NPTransformer1207Command-R+PPBERT-like1200Qwen2-72B-InstructNPBERT-like1190DeepSeek-Coder-V2-InstructNPBERT-like1188Mistral-Large-2402NPBERT-like1179Mixtral-8x22b-InstructPPBERT-like1157Phi-3-Medium-4k-InstructNPBERT-like1146Gemma-2-27B-itNPOthers1123DBRX-Instruct-PreviewNPOthers1103pplx-70b-onlinePPOthers10783.2 AGI Forecasting task: procedure"Artificial General Intelligence (AGI), also known asEach LLM was presented with a detailed forecasting prompt asking them to estimate the likelihood of AGI occurring by late 2030. The prompt (see 11.1.1) included:Strong AI or Full AI, refers to a type of artificial intelligence that can understand, learn, and apply intelligence across a wide range of tasks at a level comparable to human beings."-a definition of AGI:</p>
<p>Table 2 .
2
Probability assigned by LLMs to the development of AGI by late 2030.
Probability</p>
<p>prediction for an AGI event by 2030 Frequen -cy Percen- tage LLMs
examples of technologies that significantly changedsociety. Other notable comparisons included thedevelopment of nuclear energy, cited by four LLMs, theinvention of the microprocessor, the development ofcommercial flight, and the sequencing of the humangenome. Some LLMs also referenced specific AImilestones to highlight the field's progression. Forexample, Mixtral-8x22b-Instruct-v0.1 mentioned DeepBlue (1997), Watson (2011), AlphaGo (2016), and GPT-3 (2020) as indicators of accelerating progress in AI.All LLMs discussed factors that could accelerate orhinder AGI development, to highlight the complexity ofpredicting AGI's timeline and the multitude of factorsthat can influence its progress. Among the mostfrequently cited inciting events, advances in machinelearning algorithms and breakthroughs in computationalpower. Advances in hardware were also noted, as well asthe developments in cognitive neuroscience. Significantinvestments in AI research were cited as crucial factorsthat can accelerate progress towards AGI. Increasedfunding and resources dedicated to AI research was mentioned as a factor that can lead to more researchOptimistic (&gt;30%)318.75%pplx-70b-online (47%), gpt-4o (45%)initiatives, talent acquisition, and resource availability.Yi-Large-preview (38%)On the blocking side, ethical and regulatory constraints were the most frequently mentioned, underscoring theModerate (10-30%)637.5%Qwen2-72B-Instruct (15%) Mixtral-8x22b-potential impact of safety, fairness, and societal concernsInstruct-v0.1(15%)in slowing down or halting AGI development. Limitations in current research methodologies wereMistral-Large-2402 (12%) Llama-3-70b-Instructpointed out by several LLMs, indicating that current(15%)approaches in AI research might not be sufficient to achieve AGI, requiring new paradigms and innovativegemini-1.5-pro-api-0514 (12,5%) Command-R+ (15%)solutions. Unforeseen technical stumbling blocks wereacknowledged by 8 LLMs, highlighting the unpredictable nature of scientific and technologicalPessimistic (&lt;10%)743.75%Reka-Core (3%) Phi-3-Medium-4k-Instruct (6,3%) GLM-4-challenges as factors that could introduce significant0520 (8%)delays in achieving AGI. Several LLMs cited specific trends and trajectories to define the context for predictingGemma-2-27B-it (5%) DeepSeek-Coder-V2-Instruct (5%)AGI development. For example, 4 LLMs referencedDBRX-Instruct-PreviewMoore's Law (which predicts the doubling of transistors on integrated circuits approximately every two years) to(3,5%) claude-3-5-sonnet (5,8%)describe the context of technological evolutionTotal16100%eventually leading to AGI.Most LLMs employed historical comparisons tocontextualize AGI development by drawing parallelswith previous technological milestones, to offer areference point for understanding the complexities anduncertainties associated with developing AGI. Forexample, the development of the Internet was referencedby four LLMs as an example of a transformativetechnology that emerged over a few decades. ThreeLLMs cited the progress of narrow AI and machinelearning to show the current state and trajectory of AIdevelopment. The advent of personal computers andsmartphones were also mentioned by two LLMs as</p>
<p>Table 3
3
lists the mathematical models and equations used by LLMs to estimate the probability of AGI development by late 2030.The use of Bayesian approaches is prevalent among the LLMs (10/16), highlighting the importance given by most LLMs to update prior beliefs with new evidence in forecasting AGI development.The probability estimates from these models range from 2-4% for Reka-Core-20240501 to 47.62% for pplx-70b-online, indicating a broad spectrum of confidence levels.Statistical and regression models were applied by Phi-3-Medium-4k-Instruct, which used a Poisson regression model considering the time variable, and by Mistral-Large-2402, which used a logistic growth model, leading to a 12% probability.GPT-4o-2024-05-13 implemented a statistical growth model with time-dependent variables.</p>
<p>Table 3 .
3
Mathematical models and equations used by LLMs to estimate the probability of AGI development by 2030.
LLM NameModel/Equation UsedProbabilityEstimate by2030 (%)</p>
<p>Table 4 .
4
Evaluation criteria used by LLMs raters to assess LLMs forecasts.</p>
<p>Table 5 .
5
The evaluation scores averaged across all criteria: cell(j,k) of the table is computed as  !" = ∈  #*×$ .The last columns reports the uniformly weighted average of the forecaster scores:  != and the ranking index in the forecaster ensemble.(U.S.: Uniform Score, R.I.:</p>
<h1>$∑$ ')#𝑠 !" (&amp;')</h1>
<p>Table 6 .
6
Evaluation scores given to the i-th forecaster, split for criterion, averaged across all raters: the element (j,k) of the table is computed as</p>
<p>Table 7 .
7
Intraclass Correlation Coefficient (ICC) computed for various combinations of unit and type, based on a two-way statistical model.
ICC Model/Type ICC95%F-Testconfid.intervalSingleScoreICC(C,1) =[0.093;F(15,22)= 4.98Intraclass0.1990.41]p = 2e-08CorrelationType: consistencyAverageScoreICC(C,16)[0.620;F(15,22) = 4.98Intraclass= 0.7990.917]p = 2e-08CorrelationType: consistencySingleScore[0.013;F(15,33) = 4.98IntraclassICC(A,1) =0.116]p = 5.88e-05Correlation0.0417Type: agreementAverageScoreICC(A,16)[0.148,F(15,22)= 4.98Intraclass= 0.4100.687]p = 0.000369CorrelationType: agreement</p>
<p>Table 8 .
8
Intraclass Correlation Coefficient (ICC) computed for various combinations of unit and type, for data in Table6, based on a two-way statistical model.
ICC Model/Type ICC95%F-TestconfidenceintervalSingleScoreICC(C,1) =[0.204;F(15,120)=Intraclass0.3770.623]6.44Correlationp = 6.95e-10Type: consistencyAverageScoreICC(C,9) =[0.698;F(15,120)=Intraclass0.8450.937]6.44Correlationp = 6.95e-10Type: consistencySingleScoreICC(A,1)[0.088;F(15,32.9)=Intraclass= 0.2210.453]6.44Correlationp = 4.39e-06Type: agreementAverageScoreICC(A,9) =[0.44;F(15,25.3)=Intraclass0.7180.884]6.44Correlationp = 2.32e-05Type: agreement</p>
<p>Table 9 .
9
The benchmark values of LLMs as per Chatbot Arena, MixEval, and AlpacaEval (na = value not available; * = the closest available value in the same LLM family).
LLMsArenaMix EvalAlpacaEvalgpt-4o128287,957,5claude-sonnet127289,940,5gemini-1.5126784,224,4Yi-Large124184,451,9</p>
<p>Table 10 .
10
Evaluation scores calculated by weighting raters uniformly or according to their benchmark values.
LLMsUni-formArenaMix EvalAlpaca Evalgpt-4o4,224,204,644,62claude-sonnet4,314,314,784,82gemini-1.53,973,964,374,38Yi-Large4,314,314,764,75GLM-44,044,014,294,31Llama-3-70b4,164,154,564,64Reka-Core4,334,314,74,78Command-R+4,014,004,394,42Qwen2-72B4,174,154,544,55DeepSeek-Coder4,384,364,754,81Mistral4,114,094,394,44Mixtral4,194,174,544,59Phi-3-Medium4,204,184,534,59Gemma-24,164,144,474,52DBRX4,264,244,624,68pplx-70b4,514,555</p>
<p>Table 11 .
11
LLMs
LLMUnifo-rmSESHESSEIgpt-4o-2024-05-134,213,894,240,92claude-3-5-sonnet-4,314,564,301,0620240620gemini-1.5-pro-api-3,972,444,070,600514Yi-Large-preview4,313,894,340,90Gemma-2-27B-it4,032,894,110,70GLM-4-05204,164,224,161,02Llama-3-70b-4,335,004,281,17InstructReka-Core-4,014,563,981,1520240501Command-R+4,174,114,170,99Qwen2-72B-Instruct4,374,784,351,10DeepSeek-Coder-4,114,114,111,00V2-InstructMistral-Large-24024,194,224,181,01Mixtral-8x22b-4,194,894,151,18Instruct-v0.1
average self-evaluation score (SES), average Heteroevaluation Score (HES), Self-Evaluation Index (SEI), and the uniform weighted score for comparison.</p>
<p>Table 13 .
13
Expert ranking of LLMs based on expert-derived scores.The scores are calculated as a function of the LLM estimates and expert estimates of the likelihood of the AGI event (see appendix).
AGI %Grace et al. %likelihood bylikelihood byExpertRanking20302027scoreMistral-Large-240212104,98GLM-4-05208104,94Gemini-1.5-pro-api-051412,5104,94Phi-3-Medium-4k-Instruct6,3104,75</p>
<p>Table 14 .
14
Kendall normalized distances between the rankings generated by a) Uniform, b) Arena, c) MixEval, d) AlpacaEval, e) Expert, f) AGI 16 Bench.
AGI 16UniformArenaMixEval AlpacaExpertBenchUniform00.01670.13330.08330.5750.3167Arena0.016700.11670.06670.55830.3167MixEval0.13330.116700.06670.56670.3833Alpaca0.08330.06670.066700.5250.3333Expert0.5750.55830.56670.52500,3583AGI 16Bench0.31670.31670.38330.33330,35830</p>
<p>Table 16 .
16
Results of the optimization process for the reduced panel with 14 LLMs.Legenda: as in Table15
/Panel sizeQuadraticFunctionAlphaBeta KD_ENKD_EC_EKD_ANKD_AC_Aafter opt.Panelresiduesalabama176410,3421400,333533,7,1425,6114alabama11440,3670360,3123,725,27913,5,6,7, 14,15,COBYLA11490,4083310,258071626,43042,7,12,COBYLA160610,508060,05541429,5467constrOptim11410,3423400,333237,15,1623,8163constrOptim170420,352360,3237,15,1624,9178Deoptim169420,351380,317133,6,725,6373Deoptim10440,3670360,3123,725,2845Deoptim11440,3670360,3123,725,2841Deoptim01480,43230,192431,7,160genSA180410,3422380,317137,15,1627,5877genSA11440,3671390,325126,1526,83836,7,14,L-BFGS-B173430,3583380,3171515,1627,8055L-BFGS-B11440,3670360,3123,725,2791MLSL160420,351420,3511726,6122MLSL11440,3672380,317127,1626,6508Nelder-Mead11600,51150,125355,7,14,15,1628,0067Nelder-Mead156630,525040,0331036,15,1629,3595PRAXIS160430,3580370,308225,725,4761PRAXIS11450,3750370,308233,7,1525,4006PSO160420,351380,317127,1527,6005PSO11430,3581380,31701726,7417Rsolnp11440,3670370,308223,725,2791Rsolnp169440,3670370,308223,725,2791</p>
<p>Table 16 .
16
Values of benchmark for eleven LLMs in Arena, MixEval, AlpacaEval, AGI Bench (na means not available; * the closest available score of the same LLM family).
LLMArenaMixAlpacaAGIAGIUniformAGI16 scoreAGI14 scoreExpertEvalEvalBench 16Bench 14scorescoreClaude 3.5127289,940,5na3384,313na4,594,70SonnetGLM-41216nana1150na4,1604,11na4,94Llama-3-12078434,41207na4,3264,18na4,6670b-InstructReka-Core120783,4nana12074,014na4,684,38Command-120083,4nana10104,167na4,244,66R+DeepSeek-118886,136,6na4024,111na4,684,61Coder-V2-InstructMixtral-115784,332,7na16134,195na4,444,668x22b-InstructPhi-3-1146na30,91207na4,1944,42na4,75Medium-4k-Gemma-2-1123na7,8na2684,160na4,494,6127B-itDBRX-1103na24,412072524,2644,474,624,44Instruct-Previepplx-70b-1078nana734na4,5074,58na1online</p>
<p>Table 17 .
17
Correlation analysis between expert scores and benchmark values.
a)Correlation analysis Arena vs Expertb)Correlation analysis Arena vs Expert without outliers</p>
<p>Table A .
A
1 -details of utilised LLMs.In this chat, you are a superforecaster with a strong track record of accurate predictions about the future.As an experienced forecaster, you carefully evaluate past data and trends to predict future events as accurately as possible, acknowledging the inherent uncertainty.Structure and document your reasoning.Place AGI in a rich context, including comparison classes of related or analogous events.Provide a thorough list of potential inciting or blocking events, even from seemingly unrelated fields.2.Approach to the forecast: Consider base rates and discuss relevant past events.Examine other forecasts and predictions for related events.Thoroughly explore the possibility of unexpected breakthroughs.3.Likelihood estimation: Conduct your estimation using a mathematical or statistical model of your choice.The model should explicitly or implicitly include the time variable.Clearly describe the parameters used in your model, ensuring they're consistent with the previous criteria.Justify your parameter evaluations.
1. Rationale:#LLM short nameLLM Extended nameVersionPP/NPArchitecture1gpt-4ogpt-4o2024-05-13PPTransformer2claude-sonnetclaude-3-5-sonnet20240620PPTransformer3gemini-1.5gemini-1.5-pro-apiPPTransformer4Yi-LargeYi-Large-previewPPTransformer5GLM-4GLM-40520NPTransformer6Llama-3-70bLlama-3-70b-InstructPPTransformer7Reka-CoreReka-Core20240501NPTransformer8Command-R+Command-R+PPBERT-like9Qwen2-72BQwen2-72B-InstructNPBERT-like10DeepSeek-CoderDeepSeek-Coder-V2-InstructNPBERT-like11MistralMistral-Large2402NPBERT-like12MixtralMixtral-8x22b-Instruct-v0.1v0.1PPBERT-like13Phi-3-MediumPhi-3-Medium-4k-InstructNPBERT-like14Gemma-2Gemma-2-27B-itNPOthers15DBRXDBRX-Instruct-PreviewNPOthers16pplx-70bpplx-70b-onlinePPOthers11.1 Prompts11.1.1 ForecasterYour task is to estimate the likelihood of an event called "AGI". This involves assigning a probability between 0% and 100% for AGI occurring bylate 2030.Additional information about this event:"Artificial General Intelligence (AGI), also known as Strong AI or Full AI, refers to a type of artificial intelligence that can understand, learn, andapply intelligence across a wide range of tasks at a level comparable to human beings. Unlike narrow AI, which is designed for specific tasks, AGI ischaracterized by its generality and flexibility. It can perform any intellectual task that a human can, exhibiting key traits such as autonomy,generalization, adaptability, understanding, and self-improvement. AGI systems would be capable of operating independently without constanthuman oversight, applying knowledge from one domain to another, adjusting to new situations and environments, comprehending complex conceptsand contexts, and learning and enhancing their capabilities over time. This combination of abilities sets AGI apart from current AI systems,potentially representing a significant leap forward in artificial intelligence technology. "Write a paragraph to share with your team, addressing at least the following points:</p>
<p>You are an independent expert evaluator invited to collaborate in a study on AGI forecasting.You have been selected for this role due to your combined technical knowledge of AI, understanding of forecasting methodologies, and awareness of the broader societal and ethical implications of AGI development.Your task is to assess the likelihood estimations of the event called "The AGI" provided by a panel of superforecasters.The respondents were given this conditions:
These conditions must be satisfied by late 2030. The current date is 12 July 2024. Here's a base rate for this event: 1%11.1.2 Evaluator</p>
<p>Table A
A13 10 74 45 3 45 4 45 3 45 4 44 3 43 4 52 3 42.2 Score table 16 x 16*914 11 84 43 3 44 4 55 3 44 4 33 3 45 4 45 3 44gpt-4o-2024-05-13 002 1272 PP15 12 94 55 3 54 4 55 3 43 4 44 3 44 4 53 3 43forecaster 16 13 104 5criterion 14 4 5criterion 25 4 5criterion 3criterion 4 5 3 4criterion 5 5 4 4criterion 6 4 4 4criterion 7 5 4 5criterion 8 5 3 4criterion 9 41 14 114 543 444 554 3 43 4 43 3 44 4 54 4 42 gemini-1.5-pro-api-0514 5 15 4 12 44 454 555 3 44 4 44 4 45 4 55 3 43 forecaster 16 134 criterion 1 5 44 criterion 2 4 44 criterion 3 5 44 criterion 4 4 43 criterion 5 5 43 criterion 6 5 44 criterion 7 5 54 criterion 8 4 4criterion 94 1 14454 443 455 2 44 3 44 2 45 3 55 4 435 2 Gemma-2-27B-it 012 1216 NP 4 15 43 444 444 3 43 4 43 2 44 4 54 3 436 3 forecaster 164 criterion 1 54 criterion 2 2 44 criterion 3 3 54 2 criterion 4 44 2 criterion 5 44 2 criterion 6 45 3 criterion 7 54 3 criterion 8 42 criterion 97 4 1442 344 344 3 34 2 24 3 25 4 44 3 338 5 2 Llama-3-70b-Instruct 014 1207 NP 4 4 3 5 44 444 2 44 2 34 3 34 2 54 2 429 6 3 forecaster4 criterion 1 44 criterion 2 2 34 criterion 3 3 34 3 3 criterion 44 2 2 criterion 54 2 2 criterion 64 3 4 criterion 74 2 3 criterion 82 criterion 910 7 4 15 553 4 443 4 555 3 4 44 2 3 44 3 3 45 3 3 54 2 4 5211 8 5452 344 344 3 34 2 24 2 25 2 34 2 3212 9 6453 343 355 3 34 2 35 3 24 2 34 2 3213 10 7443 344 445 3 45 3 35 3 34 3 44 2 3314 11 8442 342 445 3 34 2 24 2 34 3 34 3 2215 12 9443 443 344 2 34 2 35 2 44 3 44 2 3216 13 10453 443 455 2 35 3 35 3 45 2 45 2 3214 1142 33 32 33 32 33 42 32claude-3-5-sonnet-20240620 002 1272 PP15 1243 34 42 32 33 32 42 32forecaster 16 13criterion 1 4criterion 2 2 4criterion 3 4 4criterion 4 3 3criterion 5 2 3criterion 6 3 4criterion 7 4 3criterion 8 3 2criterion 9 31 144 45 35 43 34 34 34 45 32 Yi-Large-preview 1241 PP 4 15 45 35 45 44 34 45 35 23 forecaster 163 criterion 1 54 criterion 2 34 criterion 3 55 criterion 4 43 criterion 5 33 criterion 6 44 criterion 7 44 criterion 8 4criterion 94 154 45 45 34 45 45 45 445 2 GLM-4-0520 012 1207 PP 44 45 45 43 44 33 44 446 3 forecaster5 criterion 14 4 criterion 25 4 criterion 35 4 criterion 44 3 criterion 54 3 criterion 64 4 criterion 73 4 criterion 84 criterion 97 4 15 45 4 45 4 45 4 44 3 44 4 35 4 55 4 448 5 25 44 4 44 4 45 3 44 2 44 4 34 3 53 4 449 6 34 45 4 33 4 45 4 44 4 34 4 33 4 54 4 4410 7 45 54 4 45 4 55 4 45 4 44 4 44 4 54 4 4411 8 54 44 4 43 4 45 4 44 3 33 3 44 4 44 4 4412 9 64 45 4 45 3 55 4 44 4 44 4 45 4 54 4 44</p>
<p>Average self-assessments score vs average of evaluations received from others.Table A.3.Average self-assessments score vs average of evaluations received from others.
ForecasterC1C2C3C4C5C6C7C8C9Avg F11.3 SESHESSEIResidualsMin1Q MedianMin1Q MedianMin1Q Median3Q Max3Q Max3QMax-1.26733 -0.20091-0.22100 -0.08848 --0.287395 -0.0392770.03639 0.32197 0.888860.03593 0.09270 0.233300.005258 0.0851840.1830224,264CoefficientsEstimate Std. Error t valueEstimate Std. Error t valueEstimate Std. Error t valuepplx-70b-online4,8754,000 Pr(&gt;|t|)4,9384,3134,375 Pr(&gt;|t|)4,4384,7504,438 Pr(&gt;|t|)4,4384,507Signif. codes: 0 '<strong><em>'(Intercept) 12.879214(Intercept) 4.6504813(Intercept) 2.962641Average C score 0.001 '</em><em>' 0.01 '</em>' 0.05 '.' 4,3834,110 3.199777 4.025 0.00125 4,521 4,1683,983 0.6548476 7.102 5.32e-4,090 4,3874,137 0.728531 4.067 0.00116 4,063 4,2070.1 ' ' 1</strong>06 <strong><em>*</em>Std deviation C score0.1900.189 arenaScore -0.007239 0.174 0.162 0.002684 -2.698 0.017340.229 arenaScore -0.0003746 0.261 0.226 0.0005492 -0.682 0.5060.192 arenaScore -0.001638 0.191 0.000611 -2.681 0.01790 0,136</strong>Residual standard error:Residual standard error:Residual standard error:0.61670.12620.1404on 14 DFon 14 DFon 14 DFMultiple R-squared:Multiple R-squared:Multiple R-squared:0.342,0.032160.3393Adjusted R-squared:Adjusted R-squared: -Adjusted R-squared:0.2950.036970.2921F-statistic: 7.277 on 1 andF-statistic: 0.465 on 1 andF-statistic: 7.19 on 1 and14 DF,14 DF14 DFp-value: 0.0173p-value: 0.506p-value: 0.0179
./ 345(&amp;').</p>
<p>,11 4,11 2,44 4,00 3,00 4,22 4,89 4,56 4,11 4,78 4,11 4,67 4,33 3,89 4,89 4,44 4,33 3 Reka-Core 4,11 4,67 2,67 3,89 3,44 4,11
5,00 4,56 4,11 4,78 4,22 4,22 5,00 4,56 5,00 4,89 4,01 Comman d-R+ 4,00 4,00 2,44 3,67 3,00 4,00 3,89 4,56 4,11 4,78 4,11 4,11 4,67 3,44 4,89 4,56 4,17
gemini-1-5-pro-api 4,12 3,87 4,37 3,87 3,50 3,50 4,37 4,25 3,87 3,97 Yi-Large-preview 4,68 3,87 4,68 4,43 3,75 4,18 4,56 4,31 4,31 4,31 Gemma-2-27B 4,12 4,25 4,37 4,06 3,62 4,00 3,94 4,00 3,94 4,03 GLM-4-0520 4,37 4,00 4,43 4,25 3,93 4,06 4,44 3,94 4,00 4,16 Llama-3-70b-Instruct 4,37 4,25 4,50 4,31 4,12 4,31 4,69 4,25 4,12 4,33 Reka-Core-2 4,25 3,87 4,56 4,00 3,75 3,87 4,06 3,94 3,81 4,01 Command-R+ 4,31 4,37 4,25 4,18 4,06 4,25 4,12 4,06 3,87 4,17 Qwen2-72B-Instruct 4,62 4,25 4,75 4,25 4,25 4,31 4,50 4,12 4,31 4,37 DeepSeek-Coder-V2 4,37 3,93 4,25 4,06 4,00 4,00 4,44 4,06 3,87 4,11 Mistral-Large-2402 4,37 4,18 4,62 4,12 3,93 4,06 4,44 4,00 3,94 4,19 Mixtral-8x22b-Instruct 4,31 4,37 4,50 4,12 4,25 4,44 4,06 3,81 3,87 4,19 Phi-3-Medium-4k-Instruct 4,25 3,81 4,43 4,18 4,12 3,94 4,44 4,19 4,06 4,16 DBRX-Instruct-Preview 4,37 4,25 4,56 4,31 3,93 4,44 4,44 4,00 4,06 4,26 pplx-70b-online 4,87 4,00 4,93 4,31 4,37 4,44 4,75 4,44 4,44 4,51
gpt-4o 4,438 4,250 4,500 3,875 4,000 3,813 4,375 4,375 4,313 4,215 claude-3-5-sonnet 4,313 4,250 4,625 4,375 4,125 3,813 4,625 4,500 4,188 4,313 gemini-1-5-pro-api 4,125 3,875 4,375 3,875 3,500 3,500 4,375 4,250 3,875 3,972 Yi-Large-preview 4,688 3,875 4,688 4,438 3,750 4,188 4,563 4,313 4,313 4,313 Gemma-2-27B 4,125 4,250 4,375 4,063 3,625 4,000 3,938 4,000 3,938 4,035 GLM-4-0520 4,375 4,000 4,438 4,250 3,938 4,063 4,438 3,938 4,000 4,160 Llama-3-70b-Instruct 4,375 4,250 4,500 4,313 4,125 4,313 4,688 4,250 4,125 4,326 Reka-Core-2 4,250 3,875 4,563 4,000 3,750 3,875 4,063 3,938 3,813 4,014 Command-R+ 4,313 4,375 4,250 4,188 4,063 4,250 4,125 4,063 3,875 4,167 Qwen2-72B-Instruct 4,625 4,250 4,750 4,250 4,250 4,313 4,500 4,125 4,313 4,375 DeepSeek-Coder-V2 4,375 3,938 4,250 4,063 4,000 4,000 4,438 4,063 3,875 4,111 Mistral-Large-2402 4,375 4,188 4,625 4,125 3,938 4,063 4,438 4,000 3,938 4,188 Mixtral-8x22b-Instruct 4,313 4,375 4,500 4,125 4,250 4,438 4,063 3,813 3,875 4,194 Phi-3-Medium-4k-Instruct 4,250 3,813 4,438 4,188 4,125 3,938 4,438 4,188 4,063 4,160 DBRX-Instruct-Preview 4,375 4,250 4,563 4,313 3,938 4,438 4,438 4,000 4,063C1 C2 C3 C4 C5 C6 C7 C8 C9 gpt-4o 0,289 0,741 -0,121 -1,812 0,074 -1,063 -0,053 1,240 1,306 claude-3-5-sonnet -0,368 0,741 0,598 1,280 0,620 -1,063 1,054 1,891 0,653 gemini-1_5-pro -1,355 -1,243 -0,840 -1,812 -2,108 -2,264 -0,053 0,589 -0,982 Yi-Large 1,602 -1,243 0,960 1,670 -1,017 0,376 0,779 0,917 1,306 Gemma-2 -1,355 0,741 -0,840 -0,649 -1,563 -0,345 -1,988 -0,714 -0,653 GLM-4 -0,042 -0,582 -0,477 0,507 -0,196 -0,104 0,226 -1,037 -0,329 Llama-3-70b -0,042 0,741 -0,121 0,897 0,620 0,856 1,333 0,589 0,324 Reka-Core -0,699 -1,243 0,242 -1,039 -1,017 -0,825 -1,435 -1,037 -1,306 Command-R+ -0,368 1,402 -1,558 0,124 0,349 0,614 -1,160 -0,386 -0,982 Qwen2 1,271 0,741 1,317 0,507 1,165 0,856 0,500 -0,063 1,306 DeepSeek-Coder-V2 -0,042 -0,910 -1,558 -0,649 0,074 -0,345 0,226 -0,386 -0,982 Mistral-Large -0,042 0,413 0,598 -0,266 -0,196 -0,104 0,226 -0,714 -0,653 Mixtral-8x22b -0,368 1,402 -0,121 -0,266 1,165 1,335 -1,435 -1,688 -0,982 Phi-3-Medium-4k -0,699 -1,571 -0,477 0,124 0,620 -0,583 0,226 0,266 0,000 DBRX-Instruct -0,042 0,741 0,242 0,897 -0,196 1,335 0,226 -0,714 0,000 pplx-70b 2,584 -0,582 2,398 0,897 1,711 1,335 1,607 1,568 1,958Author contributionsAG and FD conceived the general rationale of the study and designed the methodology.FD and PT developed and supervised the mathematical analysis.AG, FD, and PT collaborated on writing and revising the manuscript, providing relevant suggestions and improvements.All authors contributed to the article and approved the final submitted version.Conflict of interestThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.distKendall(rank(-(S %<em>% optimal_x), ties.method= "min"), rankNewHJS, type = "ranking") normDistKendall(rank(-(S %</em>% optimal_x), ties.method= "min"), rankNewHJS, "ranking") confronta_ranking_v2(rank(-(S %<em>% optimal_x)), rankNewHJS) distKendall(rank(-(S %</em>% optimal_x), ties.method= "min"), rank_Arena, type = "ranking") normDistKendall(rank(-(S %<em>% optimal_x), ties.method= "min"), rank_Arena, "ranking") confronta_ranking_v2(rank(-(S %</em>% optimal_x)), rank_Arena) t(newHJS -S %<em>% optimal_x) %</em>% (newHJS -S %<em>% optimal_x)</em>alpha normDistKendall(rank(-(S %<em>% optimal_x), ties.method= "min"), rankNewHJS, "ranking")</em>beta DEoptim -alpha 1, beta 17 library(DEoptim) # Definizione della funzione obiettivo con penalizzazione per i vincoli objective_function &lt;-function(x, S_14, newHJS_14, rankNewHJS_14, alpha, beta, penalty = 1e6) { # Calcolo della funzione obiettivo originale term1 &lt;-alpha * sum((newHJS_14 -S_14 %<em>% x)^2) term2 &lt;-beta * normDistKendall(rank(-as.vector(S_14 %</em>% x), ties.method= "min"), rankNewHJS_14, "ranking") cat(rank(-(S_14 %<em>% optimal_x), ties.method= "min"), sep = "\n") distKendall(rank(-(S_14 %</em>% optimal_x), ties.method= "min"), rankNewHJS_14, type = "ranking") normDistKendall(rank(-(S_14 %<em>% optimal_x), ties.method= "min"), rankNewHJS_14, "ranking") confronta_ranking_v2(rank(-(S_14 %</em>% optimal_x)), rankNewHJS_14) distKendall(rank(-(S_14 %<em>% optimal_x), ties.method= "min"), rank_Arena_14, type = "ranking") normDistKendall(rank(-(S_14 %</em>% optimal_x), ties.method= "min"), rank_Arena_14, "ranking") confronta_ranking_v2(rank(-(S_14 %<em>% optimal_x)), rank_Arena_14) t(newHJS_14 -S_14 %</em>% optimal_x) %<em>% (newHJS_14 -S_14 %</em>% optimal_x)<em>alpha normDistKendall(rank(-(S_14 %</em>% optimal_x), ties.method= "min"), rankNewHJS_14, "ranking")*betaFunctionNormalized distance Kendall
How long until human-level AI? Results from an expert assessment. S D Baum, B Goertzel, T G Goertzel, 10.1016/j.techfore.2010.09.006Technological Forecasting and Social Change. 7812011</p>
<p>Why generality is key to human-level artificial intelligence. T R Besold, U Schmid, Advances in Cognitive Systems. 20164</p>
<p>Superintelligence: Paths, dangers, strategies. N Bostrom, 2014Oxford University Press</p>
<p>C M Chan, W Chen, Y Su, J Yu, W Xue, S Zhang, J Fu, Z Liu, arXiv:2308.07201ChatEval: Towards better LLM-based evaluators through multi-agent debate. 2023arXiv preprint</p>
<p>M Jin, H Tang, C Zhang, Q Yu, C Liu, S Zhu, Y Zhang, M Du, Time series forecasting with LLMs: Understanding and enhancing model capabilities. 2024</p>
<p>Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference. W Chiang, L Zheng, Y Sheng, A N Angelopoulos, T Li, D Li, H Zhang, B Zhu, M Jordan, J E Gonzalez, I Stoica, ArXiv, abs/2403.041322024</p>
<p>Z Chu, Q Ai, Y Tu, H Li, Y Liu, arXiv:2401.15641PRE: A peer review based large language model evaluator. 2024arXiv preprint</p>
<p>Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators. Y Dubois, B Galambosi, P Liang, T Hashimoto, ArXiv, abs/2404.044752024</p>
<p>J Fu, S K Ng, Z Jiang, P Liu, arXiv:2302.04166GPTScore: Evaluate as you desire. 2023arXiv preprint</p>
<p>When will AI exceed human performance? Evidence from AI experts. K Grace, J Salvatier, A Dafoe, B Zhang, O Evans, 10.1613/jair.1.11222Journal of Artificial Intelligence Research. 622018</p>
<p>Thousands of AI Authors on the Future of AI. K Grace, H Stewart, J F Sandkühler, S Thomas, B Weinstein-Raun, J Brauner, ArXiv, abs/2401.02843Goertzel, B., &amp; Pennachin, C.2024. 2006Springer VerlagArtificial general intelligence</p>
<p>Large language models are zero-shot time series forecasters. N Gruver, M Finzi, S Qiu, A G Wilson, 2024arXiv preprint</p>
<p>Approaching human-level forecasting with language models. D Halawi, F Zhang, C Yueh-Han, J Steinhardt, 2024</p>
<p>R Hanson, The age of Em: Work, love, and life when robots rule the Earth. Oxford University Press2016</p>
<p>J Li, R Li, Q Liu, arXiv:2309.04369Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation. 2023arXiv preprint</p>
<p>R Li, T Patel, X Du, arXiv:2307.02762PRD: Peer rank and discussion improve large language model based evaluations. 2023arXiv preprint</p>
<p>Large Language Models Are State-of-the-Art Evaluators of Translation Quality. T Kocmi, C Federmann, European Association for Machine Translation Conferences/Workshops. 2023arXiv preprint</p>
<p>Forming Inferences About Some Intraclass Correlation Coefficients. Kenneth &amp; Mcgraw, S Wong, 10.1037/1082-989X.1.1.30Psychological Methods. 111996</p>
<p>Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence. T R Mcintosh, T Susnjak, T Liu, P Watters, M N Halgamuge, arXiv:2402.09880Fundamental issues of artificial intelligence. V C Müller, Springer2024. 2016arXiv preprintFuture progress in artificial intelligence: A survey of expert opinion</p>
<p>K Ning, S Yang, Y Liu, J Yao, Z Liu, Y Wang, M Pang, L Yuan, PiCO: Peer Review in LLMs based on the Consistency Optimization. 2024arXiv preprint</p>
<p>AI-augmented predictions. P Schoenegger, P S Park, E Karger, S Trott, P E Tetlock, LLM assistants improve human forecasting accuracy. 2024arXiv preprint</p>
<p>A Tikhonov, I P Yamshchikov, arXiv:2312.03743Post Turing: Mapping the landscape of LLM Evaluation. 2023arXiv preprint</p>
<p>P Verga, S Hofstatter, S Althammer, Y Su, A Piktus, A Arkhangorodsky, M Xu, N White, P Lewis, arXiv:2404.18796Replacing judges with juries: Evaluating LLM generations with a panel of diverse models. 2024arXiv preprint</p>
<p>Metacognitive Prompting Improves Understanding in Large Language Models. Y Wang, Y Zhao, ArXiv, abs/2308.053422023</p>
<p>. B Zhang, N Dreksler, M Anderljung, L Kahn, C Giattino, A Dafoe, M C Horowitz, 2022</p>
<p>A I Forecasting, Progress, ArXiv, abs/2206.04132Evidence from a Survey of Machine Learning Researchers. </p>
<p>Judging LLM-as-ajudge with MT-Bench and Chatbot Arena. L Zheng, W Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J Gonzalez, I Stoica, ArXiv, abs/2306.056852023</p>            </div>
        </div>

    </div>
</body>
</html>
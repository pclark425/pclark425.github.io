<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6876 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6876</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6876</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-277244325</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.16563v2.pdf" target="_blank">Chem42: a Family of chemical Language Models for Target-aware Ligand Generation</a></p>
                <p><strong>Paper Abstract:</strong> Revolutionizing drug discovery demands more than just understanding molecular interactions - it requires generative models that can design novel ligands tailored to specific biological targets. While chemical Language Models (cLMs) have made strides in learning molecular properties, most fail to incorporate target-specific insights, restricting their ability to drive de-novo ligand generation. Chem42, a cutting-edge family of generative chemical Language Models, is designed to bridge this gap. By integrating atomic-level interactions with multimodal inputs from Prot42, a complementary protein Language Model, Chem42 achieves a sophisticated cross-modal representation of molecular structures, interactions, and binding patterns. This innovative framework enables the creation of structurally valid, synthetically accessible ligands with enhanced target specificity. Evaluations across diverse protein targets confirm that Chem42 surpasses existing approaches in chemical validity, target-aware design, and predicted binding affinity. By reducing the search space of viable drug candidates, Chem42 could accelerate the drug discovery pipeline, offering a powerful generative AI tool for precision medicine. Our Chem42 models set a new benchmark in molecule property prediction, conditional molecule generation, and target-aware ligand design. The models are publicly available at huggingface.co/inceptionai.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6876.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6876.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chem42</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chem42 (family of chemical Language Models for Target-aware Ligand Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of decoder-only, autoregressive chemical language models (LLaMA-inspired) pretrained on UniChem SMILES and fine-tuned / conditioned with protein embeddings from Prot42 to generate target-aware, SMILES-formatted ligands; evaluated on property prediction, conditional generation, reaction prediction, and target-aware ligand generation with docking-based scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chem42</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only autoregressive chemical language model (LLaMA-inspired) with cross-attention to protein embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Multiple sizes (examples in paper: 190M, 387M, 597M, 1B parameters; also 507M baseline family variants)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on UniChem canonical SMILES (with SMILES enumeration augmentations yielding multiple dataset partitions: canonical and +1x/+2x/+4x random enumerations); atom-level tokenization vocabulary of 268 tokens; scaling laws used to set tokens-per-parameter (TPP=50).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive SMILES generation conditioned on protein embeddings via cross-attention (protein embeddings projected as key/value); generation initialized with fixed 'C' token for controlled SMILES start; ranking of generated sequences by a scoring function R(Ŝl) assessing validity, QED and SA; also fine-tuned for property-conditioned generation (GuacaMol).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Canonical SMILES (SMILES enumeration used during pretraining); atom-level tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Target-aware ligand generation for drug discovery (design of protein-binding small molecules), conditional molecular generation for property optimization, reaction synthesis and retrosynthesis prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Post-generation ranking/filters using RDKit-computed validity, QED (drug-likeness), Synthetic Accessibility (SA); during GuacaMol fine-tuning models trained to satisfy specified property windows (logP, SAS, TPSA, QED); in target-aware generation best molecules selected by combined property metrics before docking.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit for validity, QED, SA and other property computations; AutoDock Vina for docking and computed binding affinities; DiffDock-L used for pose estimation/visualization in figure comparisons; UCSF ChimeraX for visualization; models also evaluated with downstream reaction datasets (USPTO) and docking pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Pretraining: UniChem (canonical SMILES, with enumerations). Baselines / comparisons use PubChem for baseline family; downstream fine-tuning / evaluation datasets: GuacaMol (conditional generation), MoleculeNet and ADMET (property prediction), PDBBind 2020 (protein-ligand pairs used for training/evaluation of target-aware generation), USPTO-FULL / USPTO-MIT / USPTO-50K (reaction prediction), specific PDB targets (e.g., PDB IDs listed: 1IEP, 2RGP, 3EML, 3NY8, 4RLU, 5MO4, 7L11, 4UNN).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (% valid SMILES), Uniqueness (% unique), Novelty, Mean Absolute Deviation (MAD) between conditioned and computed properties, QED (drug-likeness), SA (synthetic accessibility score), predicted binding affinity (BA) from docking (kcal/mol), ROC-AUC and RMSE for property prediction benchmarks (MoleculeNet, ADMET, PDBBind RMSE), top-k accuracies for reaction prediction (top-1/top-3/top-5), and ranking/scoring functions combining validity/QED/SA.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Representative quantitative results reported in paper: (1) Property benchmarks: Chem42 outperforms ChemFM on 10/12 MoleculeNet tasks and 18/22 ADMET tasks while using fewer parameters. (2) GuacaMol conditional generation (Chem42-597M): logP validity 0.989, uniqueness 0.998, novelty 0.924, MAD 0.202; TPSA validity 0.989; SAS validity 0.996, MAD 0.107; QED validity 0.993, MAD 0.043 (Table 7). (3) Unconditional generation tasks: high uniqueness and competitive validity compared to ChemFM and MolGPT (see Tables 5/6). (4) Reaction prediction: Chem42-597M achieves performance comparable to ChemFM-3B on USPTO benchmarks with top-k accuracies reported in Table 8 (e.g., top-1 synthesis ~90% on USPTO-MIT reported in paper). (5) Target-aware generation (Table 9): across eight held-out protein targets generated ligands show improved QED and lower SA than PMN and comparable or improved docking scores; example for PDB 1IEP — QED: PMN 0.322 vs Chem42-507M 0.613 and Chem42-597M 0.360; SA: PMN 7.609 vs Chem42-507M 4.125 and Chem42-597M 3.152; docking BA (kcal/mol): PMN -9.946 vs Chem42-507M -10.860 and Chem42-597M -10.352. Reported binding affinities across targets are in roughly -9 to -26 kcal/mol range (depending on target and method).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>No wet-lab synthesis or biochemical validation of generated compounds reported; evaluation relies on computed metrics (RDKit) and docking scoring (AutoDock Vina) which have known limitations. Authors note open challenges such as binding-site identification, the need to further optimize docking accuracy, scaling the models, and extending multimodal constraints (structural/biochemical). Also implicit limitations: reliance on docking scores and in-silico metrics which may not translate directly to experimental activity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chem42: a Family of chemical Language Models for Target-aware Ligand Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6876.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6876.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prot42</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prot42 (protein Language Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A complementary protein language model used to embed protein sequence information (sequence-driven multimodal input) which is integrated via cross-attention into Chem42 to guide ligand generation toward target-specific interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prot42: a novel family of protein language models for target-aware protein binder generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prot42</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>protein language model producing sequence embeddings (used as keys/values in cross-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Model projection described as producing embeddings E_p with dimension noted (e.g., Prot42 M 2048×1280 indicated in paper tables)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on protein sequences (amino-acid sequences); paper references Prot42 as a separate family (see citation) though full pretraining corpus details are in the Prot42 reference.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Provides protein sequence embeddings which are projected and used as key/value pairs in cross-attention inside the Chem42 decoder; ligand tokens act as queries to condition autoregressive SMILES generation.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Provide target (protein) context for target-aware ligand generation and protein-aware design.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Protein sequence conditioning only (sequence-driven multimodal conditioning); no explicit structural/residue-level constraints required.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Protein embeddings fed into Chem42 cross-attention; outputs evaluated downstream with docking tools (AutoDock Vina) and pose estimators (DiffDock-L).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Protein sequence corpora (not exhaustively listed in this paper; see Prot42 reference for full details); used here to encode sequences of target proteins (PDB targets used for evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Indirect—improvements in generated ligand metrics (QED/SA/BA) when conditioning on Prot42 embeddings; no separate Prot42-only metrics detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used successfully to guide Chem42 generation; enabled sequence-only (no explicit structure) target-aware ligand generation with improved QED/SA and comparable docking-based binding affinity vs baselines (see Chem42 results).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Paper notes general challenges in multimodal integration and future work to incorporate structural and biochemical constraints; specific Prot42 limitations not elaborated in this paper (see Prot42 citation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chem42: a Family of chemical Language Models for Target-aware Ligand Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6876.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6876.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemFM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemFM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior large chemical foundation model (Tiny LLaMA-based) used as a principal baseline for both property prediction and molecule generation comparisons; trained on a large corpus (~178M molecules) and available in different parameter scales (up to 3B).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A foundation model for chemical design and property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemFM</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only causal language model / chemical foundation model (Tiny LLaMA family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Up to 3B parameters (paper compares to ChemFM 1B and 3B variants)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on ~178M molecules (self-supervised causal language modeling on SMILES), per referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Self-supervised causal language modeling / autoregressive SMILES generation (used for sequence completion and de novo generation in baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (tokenized, motif/atom-level depending on variant); referenced vocab choices differ (ChemFM used a 266-token vocab described in related text).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Generic molecular generation and molecular property prediction; served as baseline in target-aware ligand generation and property benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Not explicitly described in this paper for baseline runs; used standard evaluation pipelines consistent with original ChemFM work.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Compared via same evaluation pipelines (RDKit, docking) as Chem42 in this paper; not directly integrated into Chem42 pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Pretraining corpus reported in original ChemFM: large molecular databases (~178M compounds). In this paper ChemFM results used as benchmark on MoleculeNet, ADMET, GuacaMol, USPTO reaction datasets, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same suite as Chem42 comparisons: validity/uniqueness/novelty, property-prediction metrics (ROC-AUC, RMSE), GuacaMol metrics (validity/uniqueness/novelty/MAD), reaction top-k accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Chem42 reported to outperform ChemFM on 10/12 MoleculeNet tasks and 18/22 ADMET tasks while using fewer parameters; Chem42 matches or exceeds ChemFM in conditional generation validity (GuacaMol) and achieves comparable reaction prediction performance despite smaller model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Noted as a strong baseline but larger in parameter count and compute; limitations inherent to non-target-aware architectures (lacking integrated protein conditioning) which Chem42 aims to address.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chem42: a Family of chemical Language Models for Target-aware Ligand Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6876.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6876.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-inspired transformer-decoder chemical model that performs autoregressive SMILES generation, used in the paper as a comparative baseline for conditional and unconditional generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molgpt: Molecular generation using a transformer-decoder model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer-decoder (GPT-style) for molecular SMILES generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper (original MolGPT references provide sizes); used here as a comparative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on SMILES corpora (as per original MolGPT work); used as baseline on GuacaMol conditional generation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive SMILES generation</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo molecular generation and conditional property-based generation (benchmarking against GuacaMol).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Standard GuacaMol conditioning tasks when used for comparisons; no extra constraints detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Evaluated using RDKit for property computation; used for direct comparison to Chem42 in GuacaMol tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>GuacaMol (for conditional generation comparisons); underlying MolGPT pretraining corpora referenced in MolGPT literature.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity, Uniqueness, Novelty, Mean Absolute Deviation (MAD) for conditioned properties (GuacaMol metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Example values from Table 7 (MolGPT baseline): logP validity 0.971, uniqueness 0.998, novelty 0.977, MAD 0.230; other property entries given for TPSA, SAS, QED in Table 7 for direct comparison with Chem42-597M.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Presented as a baseline; paper indicates Chem42 achieves higher validity on GuacaMol and competitive property control while being smaller.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chem42: a Family of chemical Language Models for Target-aware Ligand Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6876.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6876.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PMN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PMN (prior multimodal protein–ligand generation model used as comparator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior multimodal protein–ligand generation model reported in the literature (referenced as NH24/TargetVAE-style approaches) and used here as a comparative baseline for target-aware ligand generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multimodal protein representation learning and target-aware variational auto-encoders for protein-binding ligand generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PMN (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>multimodal protein–ligand variational autoencoder / generative model (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Reported to use multimodal protein representations (amino acid sequences, 3D structures, residue graphs) to generate ligands (see cited work); in this paper PMN is used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Variational autoencoder-based multimodal generation (per cited TargetVAE/PMN approach); not developed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (implicit in baseline comparisons) or other ligand encodings as in referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Protein-binding ligand generation (target-aware design), used here for head-to-head comparison in target-aware experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Not detailed here; baseline results compared on QED, SA, and docking-based BA.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Docking (AutoDock Vina) and RDKit metrics used to evaluate generated ligands in the comparative table (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>PDBBind training set (PMN trained on similar/overlapping datasets per original publication); here PMN predictions are compared on the same held-out targets as Chem42.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>QED, SA, docking-based binding affinity (kcal/mol) as reported in Table 9.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 9 shows PMN performance across eight targets (example: PDB 1IEP — QED 0.322, SA 7.609, BA -9.946 kcal/mol) which Chem42 often improves upon in QED and SA and shows comparable BA.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Prior multimodal approaches may rely on structural inputs or pre-defined binding sites; authors claim Chem42's sequence-driven multimodal approach removes need for explicit binding-site definitions, but acknowledge docking/BA limitations remain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chem42: a Family of chemical Language Models for Target-aware Ligand Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6876.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6876.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TargetVAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TargetVAE (multimodal target-aware variational autoencoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously published variational autoencoder approach that uses multimodal protein representations (sequence, 3D structures, residue graphs) to generate ligands without requiring explicit predefined binding sites; referenced in related work as addressing binding-site ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TargetVAE</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>multimodal variational autoencoder for protein-binding ligand generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Uses multimodal protein representations (sequence, structure, residue-level graphs) and paired protein–ligand data (as described in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>VAE-based generative sampling conditioned on multimodal protein representations (per referenced work, not implemented here).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Likely SMILES or graph-based ligand representation in cited work (not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Target-aware ligand generation for protein binders, particularly where binding sites are undefined/ambiguous.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Mentioned in paper as prior approach addressing binding-site ambiguity; not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chem42: a Family of chemical Language Models for Target-aware Ligand Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6876.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6876.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DrugGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DrugGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-based autoregressive model designed for controlled generation of protein-specific ligands (mentioned in related work as an example of autoregressive controlled generation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DrugGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive GPT-based chemical language model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on molecular SMILES and protein/ligand datasets depending on original work (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Controlled autoregressive generation aimed at protein-specific ligand design (per related work).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Protein-specific ligand design (drug discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Mentioned as prior work in related work; no experimental detail provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chem42: a Family of chemical Language Models for Target-aware Ligand Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6876.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6876.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoDock Vina</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoDock Vina (docking software)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used molecular docking program employed in this work to compute docking poses and approximate binding affinities for generated ligands against target proteins.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AutoDock Vina</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>docking / scoring software</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>3D structures derived from ligand SMILES (via standard cheminformatics pipelines) used for docking</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Computational docking and approximate binding affinity estimation for candidate ligands generated by Chem42.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Used as post-generation evaluation to score and compare generated ligands; binding affinity (kcal/mol) used as metric.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Pipelines use RDKit to construct ligand structures from SMILES, AutoDock Vina to dock to protein structures (PDB targets); DiffDock-L used for pose comparisons in figure panels.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Docking applied to generated ligands against PDB targets (PDB IDs listed in paper) and to held-out PDBBind-derived targets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Predicted binding affinity (kcal/mol), docking poses and confidence scores (when using DiffDock-L for comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Binding affinity results reported in Table 9 for generated ligands across eight targets (e.g., 1IEP BA values: PMN -9.946 kcal/mol, Chem42-507M -10.860 kcal/mol, Chem42-597M -10.352 kcal/mol).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Docking scores are approximate and limited predictors of experimental binding; paper notes need to optimize docking accuracy in future work and that docking-based evaluation is an in-silico surrogate not equivalent to biochemical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chem42: a Family of chemical Language Models for Target-aware Ligand Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6876.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6876.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDKit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RDKit (cheminformatics toolkit)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source cheminformatics library used for SMILES validation, calculation of QED (drug-likeness), Synthetic Accessibility (SA) score, and other cheminformatics preprocessing and property computations in evaluation and ranking of generated molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An open source chemical structure curation pipeline using rdkit.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RDKit</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>cheminformatics library</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES and internal molecular graph representations</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Validity checking of generated SMILES, computation of QED, SA, and other physicochemical properties used for ranking and filtering generated molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Validity filter (RDKit validation of SMILES), computations used in scoring function R(Ŝl) and GuacaMol evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used together with Chem42 generation pipeline and docking (AutoDock Vina) for evaluation and ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (% valid SMILES), QED, SA, other computed properties used in evaluation metrics and MAD calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>RDKit-derived metrics appear throughout results (validity, QED, SA values reported in Tables 5,7,9, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>RDKit metrics are computational proxies and do not guarantee synthetic or biological feasibility; SA is an approximate heuristic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chem42: a Family of chemical Language Models for Target-aware Ligand Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6876.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6876.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiffDock-L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiffDock-L (pose estimator / docking generalization tool)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pose estimation / docking generalization method used to estimate binding poses for comparison in figures and to provide a confidence score for pose estimation of generated ligands.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep confident steps to new pockets: Strategies for docking generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DiffDock-L</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>pose estimator / docking generalization model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>3D ligand/protein structures for pose estimation</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Pose estimation and confidence scoring used in figure comparisons (e.g., comparison of Chem42-generated ligand pose to known stabilizer JC769 for p53 Y220C).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used alongside AutoDock Vina and visualization tools to assess and display predicted poses; confidence scores shown in figure captions.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Pose confidence score and qualitative pose comparisons; used to supplement docking affinity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used to estimate poses shown in Figure 1 and to provide pose confidence scores in Figure 5.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Pose estimators provide approximate poses and confidence scores; authors note pose estimation and docking accuracy as areas for further optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chem42: a Family of chemical Language Models for Target-aware Ligand Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A foundation model for chemical design and property prediction <em>(Rating: 2)</em></li>
                <li>Multimodal protein representation learning and target-aware variational auto-encoders for protein-binding ligand generation <em>(Rating: 2)</em></li>
                <li>Molgpt: Molecular generation using a transformer-decoder model. <em>(Rating: 2)</em></li>
                <li>Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins <em>(Rating: 2)</em></li>
                <li>Guacamol: benchmarking models for de novo molecular design <em>(Rating: 1)</em></li>
                <li>Chemberta-2: Towards chemical foundation models. <em>(Rating: 1)</em></li>
                <li>DiffDock-L: Deep confident steps to new pockets: Strategies for docking generalization. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6876",
    "paper_id": "paper-277244325",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "Chem42",
            "name_full": "Chem42 (family of chemical Language Models for Target-aware Ligand Generation)",
            "brief_description": "A family of decoder-only, autoregressive chemical language models (LLaMA-inspired) pretrained on UniChem SMILES and fine-tuned / conditioned with protein embeddings from Prot42 to generate target-aware, SMILES-formatted ligands; evaluated on property prediction, conditional generation, reaction prediction, and target-aware ligand generation with docking-based scoring.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Chem42",
            "model_type": "decoder-only autoregressive chemical language model (LLaMA-inspired) with cross-attention to protein embeddings",
            "model_size": "Multiple sizes (examples in paper: 190M, 387M, 597M, 1B parameters; also 507M baseline family variants)",
            "training_data_description": "Pretrained on UniChem canonical SMILES (with SMILES enumeration augmentations yielding multiple dataset partitions: canonical and +1x/+2x/+4x random enumerations); atom-level tokenization vocabulary of 268 tokens; scaling laws used to set tokens-per-parameter (TPP=50).",
            "generation_method": "Autoregressive SMILES generation conditioned on protein embeddings via cross-attention (protein embeddings projected as key/value); generation initialized with fixed 'C' token for controlled SMILES start; ranking of generated sequences by a scoring function R(Ŝl) assessing validity, QED and SA; also fine-tuned for property-conditioned generation (GuacaMol).",
            "chemical_representation": "Canonical SMILES (SMILES enumeration used during pretraining); atom-level tokenization",
            "target_application": "Target-aware ligand generation for drug discovery (design of protein-binding small molecules), conditional molecular generation for property optimization, reaction synthesis and retrosynthesis prediction.",
            "constraints_used": "Post-generation ranking/filters using RDKit-computed validity, QED (drug-likeness), Synthetic Accessibility (SA); during GuacaMol fine-tuning models trained to satisfy specified property windows (logP, SAS, TPSA, QED); in target-aware generation best molecules selected by combined property metrics before docking.",
            "integration_with_external_tools": "RDKit for validity, QED, SA and other property computations; AutoDock Vina for docking and computed binding affinities; DiffDock-L used for pose estimation/visualization in figure comparisons; UCSF ChimeraX for visualization; models also evaluated with downstream reaction datasets (USPTO) and docking pipelines.",
            "dataset_used": "Pretraining: UniChem (canonical SMILES, with enumerations). Baselines / comparisons use PubChem for baseline family; downstream fine-tuning / evaluation datasets: GuacaMol (conditional generation), MoleculeNet and ADMET (property prediction), PDBBind 2020 (protein-ligand pairs used for training/evaluation of target-aware generation), USPTO-FULL / USPTO-MIT / USPTO-50K (reaction prediction), specific PDB targets (e.g., PDB IDs listed: 1IEP, 2RGP, 3EML, 3NY8, 4RLU, 5MO4, 7L11, 4UNN).",
            "evaluation_metrics": "Validity (% valid SMILES), Uniqueness (% unique), Novelty, Mean Absolute Deviation (MAD) between conditioned and computed properties, QED (drug-likeness), SA (synthetic accessibility score), predicted binding affinity (BA) from docking (kcal/mol), ROC-AUC and RMSE for property prediction benchmarks (MoleculeNet, ADMET, PDBBind RMSE), top-k accuracies for reaction prediction (top-1/top-3/top-5), and ranking/scoring functions combining validity/QED/SA.",
            "reported_results": "Representative quantitative results reported in paper: (1) Property benchmarks: Chem42 outperforms ChemFM on 10/12 MoleculeNet tasks and 18/22 ADMET tasks while using fewer parameters. (2) GuacaMol conditional generation (Chem42-597M): logP validity 0.989, uniqueness 0.998, novelty 0.924, MAD 0.202; TPSA validity 0.989; SAS validity 0.996, MAD 0.107; QED validity 0.993, MAD 0.043 (Table 7). (3) Unconditional generation tasks: high uniqueness and competitive validity compared to ChemFM and MolGPT (see Tables 5/6). (4) Reaction prediction: Chem42-597M achieves performance comparable to ChemFM-3B on USPTO benchmarks with top-k accuracies reported in Table 8 (e.g., top-1 synthesis ~90% on USPTO-MIT reported in paper). (5) Target-aware generation (Table 9): across eight held-out protein targets generated ligands show improved QED and lower SA than PMN and comparable or improved docking scores; example for PDB 1IEP — QED: PMN 0.322 vs Chem42-507M 0.613 and Chem42-597M 0.360; SA: PMN 7.609 vs Chem42-507M 4.125 and Chem42-597M 3.152; docking BA (kcal/mol): PMN -9.946 vs Chem42-507M -10.860 and Chem42-597M -10.352. Reported binding affinities across targets are in roughly -9 to -26 kcal/mol range (depending on target and method).",
            "experimental_validation": false,
            "challenges_or_limitations": "No wet-lab synthesis or biochemical validation of generated compounds reported; evaluation relies on computed metrics (RDKit) and docking scoring (AutoDock Vina) which have known limitations. Authors note open challenges such as binding-site identification, the need to further optimize docking accuracy, scaling the models, and extending multimodal constraints (structural/biochemical). Also implicit limitations: reliance on docking scores and in-silico metrics which may not translate directly to experimental activity.",
            "uuid": "e6876.0",
            "source_info": {
                "paper_title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Prot42",
            "name_full": "Prot42 (protein Language Model)",
            "brief_description": "A complementary protein language model used to embed protein sequence information (sequence-driven multimodal input) which is integrated via cross-attention into Chem42 to guide ligand generation toward target-specific interactions.",
            "citation_title": "Prot42: a novel family of protein language models for target-aware protein binder generation",
            "mention_or_use": "use",
            "model_name": "Prot42",
            "model_type": "protein language model producing sequence embeddings (used as keys/values in cross-attention)",
            "model_size": "Model projection described as producing embeddings E_p with dimension noted (e.g., Prot42 M 2048×1280 indicated in paper tables)",
            "training_data_description": "Trained on protein sequences (amino-acid sequences); paper references Prot42 as a separate family (see citation) though full pretraining corpus details are in the Prot42 reference.",
            "generation_method": "Provides protein sequence embeddings which are projected and used as key/value pairs in cross-attention inside the Chem42 decoder; ligand tokens act as queries to condition autoregressive SMILES generation.",
            "chemical_representation": null,
            "target_application": "Provide target (protein) context for target-aware ligand generation and protein-aware design.",
            "constraints_used": "Protein sequence conditioning only (sequence-driven multimodal conditioning); no explicit structural/residue-level constraints required.",
            "integration_with_external_tools": "Protein embeddings fed into Chem42 cross-attention; outputs evaluated downstream with docking tools (AutoDock Vina) and pose estimators (DiffDock-L).",
            "dataset_used": "Protein sequence corpora (not exhaustively listed in this paper; see Prot42 reference for full details); used here to encode sequences of target proteins (PDB targets used for evaluation).",
            "evaluation_metrics": "Indirect—improvements in generated ligand metrics (QED/SA/BA) when conditioning on Prot42 embeddings; no separate Prot42-only metrics detailed in this paper.",
            "reported_results": "Used successfully to guide Chem42 generation; enabled sequence-only (no explicit structure) target-aware ligand generation with improved QED/SA and comparable docking-based binding affinity vs baselines (see Chem42 results).",
            "experimental_validation": false,
            "challenges_or_limitations": "Paper notes general challenges in multimodal integration and future work to incorporate structural and biochemical constraints; specific Prot42 limitations not elaborated in this paper (see Prot42 citation).",
            "uuid": "e6876.1",
            "source_info": {
                "paper_title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ChemFM",
            "name_full": "ChemFM",
            "brief_description": "A prior large chemical foundation model (Tiny LLaMA-based) used as a principal baseline for both property prediction and molecule generation comparisons; trained on a large corpus (~178M molecules) and available in different parameter scales (up to 3B).",
            "citation_title": "A foundation model for chemical design and property prediction",
            "mention_or_use": "use",
            "model_name": "ChemFM",
            "model_type": "decoder-only causal language model / chemical foundation model (Tiny LLaMA family)",
            "model_size": "Up to 3B parameters (paper compares to ChemFM 1B and 3B variants)",
            "training_data_description": "Pretrained on ~178M molecules (self-supervised causal language modeling on SMILES), per referenced work.",
            "generation_method": "Self-supervised causal language modeling / autoregressive SMILES generation (used for sequence completion and de novo generation in baselines).",
            "chemical_representation": "SMILES (tokenized, motif/atom-level depending on variant); referenced vocab choices differ (ChemFM used a 266-token vocab described in related text).",
            "target_application": "Generic molecular generation and molecular property prediction; served as baseline in target-aware ligand generation and property benchmarks.",
            "constraints_used": "Not explicitly described in this paper for baseline runs; used standard evaluation pipelines consistent with original ChemFM work.",
            "integration_with_external_tools": "Compared via same evaluation pipelines (RDKit, docking) as Chem42 in this paper; not directly integrated into Chem42 pipeline.",
            "dataset_used": "Pretraining corpus reported in original ChemFM: large molecular databases (~178M compounds). In this paper ChemFM results used as benchmark on MoleculeNet, ADMET, GuacaMol, USPTO reaction datasets, etc.",
            "evaluation_metrics": "Same suite as Chem42 comparisons: validity/uniqueness/novelty, property-prediction metrics (ROC-AUC, RMSE), GuacaMol metrics (validity/uniqueness/novelty/MAD), reaction top-k accuracies.",
            "reported_results": "Chem42 reported to outperform ChemFM on 10/12 MoleculeNet tasks and 18/22 ADMET tasks while using fewer parameters; Chem42 matches or exceeds ChemFM in conditional generation validity (GuacaMol) and achieves comparable reaction prediction performance despite smaller model sizes.",
            "experimental_validation": false,
            "challenges_or_limitations": "Noted as a strong baseline but larger in parameter count and compute; limitations inherent to non-target-aware architectures (lacking integrated protein conditioning) which Chem42 aims to address.",
            "uuid": "e6876.2",
            "source_info": {
                "paper_title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MolGPT",
            "name_full": "MolGPT",
            "brief_description": "A GPT-inspired transformer-decoder chemical model that performs autoregressive SMILES generation, used in the paper as a comparative baseline for conditional and unconditional generation tasks.",
            "citation_title": "Molgpt: Molecular generation using a transformer-decoder model.",
            "mention_or_use": "use",
            "model_name": "MolGPT",
            "model_type": "autoregressive transformer-decoder (GPT-style) for molecular SMILES generation",
            "model_size": "Not specified in this paper (original MolGPT references provide sizes); used here as a comparative baseline.",
            "training_data_description": "Trained on SMILES corpora (as per original MolGPT work); used as baseline on GuacaMol conditional generation.",
            "generation_method": "Autoregressive SMILES generation",
            "chemical_representation": "SMILES",
            "target_application": "De novo molecular generation and conditional property-based generation (benchmarking against GuacaMol).",
            "constraints_used": "Standard GuacaMol conditioning tasks when used for comparisons; no extra constraints detailed here.",
            "integration_with_external_tools": "Evaluated using RDKit for property computation; used for direct comparison to Chem42 in GuacaMol tasks.",
            "dataset_used": "GuacaMol (for conditional generation comparisons); underlying MolGPT pretraining corpora referenced in MolGPT literature.",
            "evaluation_metrics": "Validity, Uniqueness, Novelty, Mean Absolute Deviation (MAD) for conditioned properties (GuacaMol metrics).",
            "reported_results": "Example values from Table 7 (MolGPT baseline): logP validity 0.971, uniqueness 0.998, novelty 0.977, MAD 0.230; other property entries given for TPSA, SAS, QED in Table 7 for direct comparison with Chem42-597M.",
            "experimental_validation": false,
            "challenges_or_limitations": "Presented as a baseline; paper indicates Chem42 achieves higher validity on GuacaMol and competitive property control while being smaller.",
            "uuid": "e6876.3",
            "source_info": {
                "paper_title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "PMN",
            "name_full": "PMN (prior multimodal protein–ligand generation model used as comparator)",
            "brief_description": "A prior multimodal protein–ligand generation model reported in the literature (referenced as NH24/TargetVAE-style approaches) and used here as a comparative baseline for target-aware ligand generation.",
            "citation_title": "Multimodal protein representation learning and target-aware variational auto-encoders for protein-binding ligand generation",
            "mention_or_use": "use",
            "model_name": "PMN (as referenced)",
            "model_type": "multimodal protein–ligand variational autoencoder / generative model (prior work)",
            "model_size": null,
            "training_data_description": "Reported to use multimodal protein representations (amino acid sequences, 3D structures, residue graphs) to generate ligands (see cited work); in this paper PMN is used as a baseline.",
            "generation_method": "Variational autoencoder-based multimodal generation (per cited TargetVAE/PMN approach); not developed in detail in this paper.",
            "chemical_representation": "SMILES (implicit in baseline comparisons) or other ligand encodings as in referenced work.",
            "target_application": "Protein-binding ligand generation (target-aware design), used here for head-to-head comparison in target-aware experiments.",
            "constraints_used": "Not detailed here; baseline results compared on QED, SA, and docking-based BA.",
            "integration_with_external_tools": "Docking (AutoDock Vina) and RDKit metrics used to evaluate generated ligands in the comparative table (Table 9).",
            "dataset_used": "PDBBind training set (PMN trained on similar/overlapping datasets per original publication); here PMN predictions are compared on the same held-out targets as Chem42.",
            "evaluation_metrics": "QED, SA, docking-based binding affinity (kcal/mol) as reported in Table 9.",
            "reported_results": "Table 9 shows PMN performance across eight targets (example: PDB 1IEP — QED 0.322, SA 7.609, BA -9.946 kcal/mol) which Chem42 often improves upon in QED and SA and shows comparable BA.",
            "experimental_validation": false,
            "challenges_or_limitations": "Prior multimodal approaches may rely on structural inputs or pre-defined binding sites; authors claim Chem42's sequence-driven multimodal approach removes need for explicit binding-site definitions, but acknowledge docking/BA limitations remain.",
            "uuid": "e6876.4",
            "source_info": {
                "paper_title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "TargetVAE",
            "name_full": "TargetVAE (multimodal target-aware variational autoencoder)",
            "brief_description": "A previously published variational autoencoder approach that uses multimodal protein representations (sequence, 3D structures, residue graphs) to generate ligands without requiring explicit predefined binding sites; referenced in related work as addressing binding-site ambiguity.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "TargetVAE",
            "model_type": "multimodal variational autoencoder for protein-binding ligand generation",
            "model_size": null,
            "training_data_description": "Uses multimodal protein representations (sequence, structure, residue-level graphs) and paired protein–ligand data (as described in the cited work).",
            "generation_method": "VAE-based generative sampling conditioned on multimodal protein representations (per referenced work, not implemented here).",
            "chemical_representation": "Likely SMILES or graph-based ligand representation in cited work (not specified in this paper).",
            "target_application": "Target-aware ligand generation for protein binders, particularly where binding sites are undefined/ambiguous.",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Mentioned in paper as prior approach addressing binding-site ambiguity; not used in experiments here.",
            "uuid": "e6876.5",
            "source_info": {
                "paper_title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "DrugGPT",
            "name_full": "DrugGPT",
            "brief_description": "A GPT-based autoregressive model designed for controlled generation of protein-specific ligands (mentioned in related work as an example of autoregressive controlled generation).",
            "citation_title": "Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins",
            "mention_or_use": "mention",
            "model_name": "DrugGPT",
            "model_type": "autoregressive GPT-based chemical language model",
            "model_size": null,
            "training_data_description": "Trained on molecular SMILES and protein/ligand datasets depending on original work (not detailed here).",
            "generation_method": "Controlled autoregressive generation aimed at protein-specific ligand design (per related work).",
            "chemical_representation": "SMILES",
            "target_application": "Protein-specific ligand design (drug discovery)",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Mentioned as prior work in related work; no experimental detail provided in this paper.",
            "uuid": "e6876.6",
            "source_info": {
                "paper_title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "AutoDock Vina",
            "name_full": "AutoDock Vina (docking software)",
            "brief_description": "A widely used molecular docking program employed in this work to compute docking poses and approximate binding affinities for generated ligands against target proteins.",
            "citation_title": "Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading.",
            "mention_or_use": "use",
            "model_name": "AutoDock Vina",
            "model_type": "docking / scoring software",
            "model_size": null,
            "training_data_description": null,
            "generation_method": null,
            "chemical_representation": "3D structures derived from ligand SMILES (via standard cheminformatics pipelines) used for docking",
            "target_application": "Computational docking and approximate binding affinity estimation for candidate ligands generated by Chem42.",
            "constraints_used": "Used as post-generation evaluation to score and compare generated ligands; binding affinity (kcal/mol) used as metric.",
            "integration_with_external_tools": "Pipelines use RDKit to construct ligand structures from SMILES, AutoDock Vina to dock to protein structures (PDB targets); DiffDock-L used for pose comparisons in figure panels.",
            "dataset_used": "Docking applied to generated ligands against PDB targets (PDB IDs listed in paper) and to held-out PDBBind-derived targets.",
            "evaluation_metrics": "Predicted binding affinity (kcal/mol), docking poses and confidence scores (when using DiffDock-L for comparisons).",
            "reported_results": "Binding affinity results reported in Table 9 for generated ligands across eight targets (e.g., 1IEP BA values: PMN -9.946 kcal/mol, Chem42-507M -10.860 kcal/mol, Chem42-597M -10.352 kcal/mol).",
            "experimental_validation": false,
            "challenges_or_limitations": "Docking scores are approximate and limited predictors of experimental binding; paper notes need to optimize docking accuracy in future work and that docking-based evaluation is an in-silico surrogate not equivalent to biochemical validation.",
            "uuid": "e6876.7",
            "source_info": {
                "paper_title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "RDKit",
            "name_full": "RDKit (cheminformatics toolkit)",
            "brief_description": "Open-source cheminformatics library used for SMILES validation, calculation of QED (drug-likeness), Synthetic Accessibility (SA) score, and other cheminformatics preprocessing and property computations in evaluation and ranking of generated molecules.",
            "citation_title": "An open source chemical structure curation pipeline using rdkit.",
            "mention_or_use": "use",
            "model_name": "RDKit",
            "model_type": "cheminformatics library",
            "model_size": null,
            "training_data_description": null,
            "generation_method": null,
            "chemical_representation": "SMILES and internal molecular graph representations",
            "target_application": "Validity checking of generated SMILES, computation of QED, SA, and other physicochemical properties used for ranking and filtering generated molecules.",
            "constraints_used": "Validity filter (RDKit validation of SMILES), computations used in scoring function R(Ŝl) and GuacaMol evaluations.",
            "integration_with_external_tools": "Used together with Chem42 generation pipeline and docking (AutoDock Vina) for evaluation and ranking.",
            "dataset_used": null,
            "evaluation_metrics": "Validity (% valid SMILES), QED, SA, other computed properties used in evaluation metrics and MAD calculations.",
            "reported_results": "RDKit-derived metrics appear throughout results (validity, QED, SA values reported in Tables 5,7,9, etc.).",
            "experimental_validation": false,
            "challenges_or_limitations": "RDKit metrics are computational proxies and do not guarantee synthetic or biological feasibility; SA is an approximate heuristic.",
            "uuid": "e6876.8",
            "source_info": {
                "paper_title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "DiffDock-L",
            "name_full": "DiffDock-L (pose estimator / docking generalization tool)",
            "brief_description": "A pose estimation / docking generalization method used to estimate binding poses for comparison in figures and to provide a confidence score for pose estimation of generated ligands.",
            "citation_title": "Deep confident steps to new pockets: Strategies for docking generalization.",
            "mention_or_use": "use",
            "model_name": "DiffDock-L",
            "model_type": "pose estimator / docking generalization model",
            "model_size": null,
            "training_data_description": null,
            "generation_method": null,
            "chemical_representation": "3D ligand/protein structures for pose estimation",
            "target_application": "Pose estimation and confidence scoring used in figure comparisons (e.g., comparison of Chem42-generated ligand pose to known stabilizer JC769 for p53 Y220C).",
            "constraints_used": null,
            "integration_with_external_tools": "Used alongside AutoDock Vina and visualization tools to assess and display predicted poses; confidence scores shown in figure captions.",
            "dataset_used": null,
            "evaluation_metrics": "Pose confidence score and qualitative pose comparisons; used to supplement docking affinity metrics.",
            "reported_results": "Used to estimate poses shown in Figure 1 and to provide pose confidence scores in Figure 5.",
            "experimental_validation": false,
            "challenges_or_limitations": "Pose estimators provide approximate poses and confidence scores; authors note pose estimation and docking accuracy as areas for further optimization.",
            "uuid": "e6876.9",
            "source_info": {
                "paper_title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A foundation model for chemical design and property prediction",
            "rating": 2,
            "sanitized_title": "a_foundation_model_for_chemical_design_and_property_prediction"
        },
        {
            "paper_title": "Multimodal protein representation learning and target-aware variational auto-encoders for protein-binding ligand generation",
            "rating": 2,
            "sanitized_title": "multimodal_protein_representation_learning_and_targetaware_variational_autoencoders_for_proteinbinding_ligand_generation"
        },
        {
            "paper_title": "Molgpt: Molecular generation using a transformer-decoder model.",
            "rating": 2,
            "sanitized_title": "molgpt_molecular_generation_using_a_transformerdecoder_model"
        },
        {
            "paper_title": "Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins",
            "rating": 2,
            "sanitized_title": "druggpt_a_gptbased_strategy_for_designing_potential_ligands_targeting_specific_proteins"
        },
        {
            "paper_title": "Guacamol: benchmarking models for de novo molecular design",
            "rating": 1,
            "sanitized_title": "guacamol_benchmarking_models_for_de_novo_molecular_design"
        },
        {
            "paper_title": "Chemberta-2: Towards chemical foundation models.",
            "rating": 1,
            "sanitized_title": "chemberta2_towards_chemical_foundation_models"
        },
        {
            "paper_title": "DiffDock-L: Deep confident steps to new pockets: Strategies for docking generalization.",
            "rating": 1,
            "sanitized_title": "diffdockl_deep_confident_steps_to_new_pockets_strategies_for_docking_generalization"
        }
    ],
    "cost": 0.02184875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Chem42 * : a Family of chemical Language Models for Target-aware Ligand Generation
11 Jun 2025</p>
<p>Aahan Singh 
Inception Institute of Artificial Intelligence
Abu DhabiUAE</p>
<p>Engin Tekin 
Cerebras Systems
SunnyvaleCAUSA</p>
<p>Maryam Nadeem 
Inception Institute of Artificial Intelligence
Abu DhabiUAE</p>
<p>Nancy A Elnaker 
Inception Institute of Artificial Intelligence
Abu DhabiUAE</p>
<p>Mohammad Amaan Sayeed 
Inception Institute of Artificial Intelligence
Abu DhabiUAE</p>
<p>Natalia Vassilieva 
Cerebras Systems
SunnyvaleCAUSA</p>
<p>Boulbaba Ben Amor boulbaba.amor@inceptionai.ai 
Inception Institute of Artificial Intelligence
Abu DhabiUAE</p>
<p>Boulbaba Ben 
Chem42 * : a Family of chemical Language Models for Target-aware Ligand Generation
11 Jun 20251D4BFF01672F325D1BA29A934A527A2AarXiv:2503.16563v2[cs.LG]
Revolutionizing drug discovery demands more than just understanding molecular interactions-it requires generative models that can design novel ligands tailored to specific biological targets.While chemical Language Models (cLMs) have made strides in learning molecular properties, most fail to incorporate target-specific insights, restricting their ability to drive de-novo ligand generation.Chem42, a cutting-edge family of generative chemical Language Models, is designed to bridge this gap.By integrating atomic-level interactions with multimodal inputs from Prot42, a complementary protein Language Model [STN + 25], Chem42 achieves a sophisticated cross-modal representation of molecular structures, interactions, and binding patterns.This innovative framework enables the creation of structurally valid, synthetically accessible ligands with enhanced target specificity.Evaluations across diverse protein targets confirm that Chem42 surpasses existing approaches in chemical validity, target-aware design, and predicted binding affinity.By reducing the search space of viable drug candidates, Chem42 could accelerate the drug discovery pipeline, offering a powerful generative AI tool for precision medicine.The Chem42 models, available at huggingface.co/inceptionai,set a new benchmark in molecule property prediction, conditional molecule generation, and target-aware ligand design.* Chem42 is part of the Omics42 platform which includes also a family of genomic LMs and a family of proteins LMs named Gene42 and Prot42, respectively.Refer to Omics42 blog at huggingface.co/inceptionaifor further details.</p>
<p>Introduction</p>
<p>The discovery and design of novel therapeutic molecules remains one of the most critical challenges in drug development, requiring a precise understanding of molecular interactions and target specificity.Traditional computational methods, such as molecular docking and quantitative structure-activity relationship (QSAR) models, often struggle with the vast chemical space and the dynamic nature of molecular interactions [SK23].Recent advancements in Large Language Models (LLMs) have driven the emergence of chemical Language Models (cLMs) [CHZ + 24, BAVP22, ASC + 22, IDHB22], which learn molecular representations to generate novel structures and predict their properties.However, these models typically lack target-specific awareness, limiting their utility for rational ligand design.</p>
<p>Drug discovery is inherently multimodal, since ligand activity is dictated not only by the structure of a molecule, but also by its complex interactions with protein targets.To address this limitation, we introduce Chem42, a generative chemical-protein modeling framework that integrates our protein Language Model (pLM), named Prot42 [STN + 25], with our chemical Language Model for targetaware ligand generation.This cross-modal approach enhances molecular design by embedding the protein context into the generative process, enabling the model to generate valid, biologically relevant, and target-specific ligands.Figure 1 illustrates how Chem42 generates a ligand for the Y220C p53 mutant protein (a tumor suppressor called also "Guardian of the Genome"), comparing its binding pose to JC769, a known stabilizer (PDB ID: 8a32) [SCDD + 22].Both ligands bind to the same pocket, demonstrating the model's capacity to design structure-guided molecules.Given a target protein such as p53, we prompt Prot42 with its sequence, embedding its molecular features into Chem42 via crossattention, allowing for ligand generation guided by target-specific interactions.To the best of our knowledge, this is the first framework where a cLM and a pLM collaborate to explicitly account for protein context in ligand generation, marking a significant advancement in AI-driven drug design.</p>
<p>Related Works</p>
<p>This section reviews recent advances in chemical Language Models (cLMs) for molecule generation and property prediction, emphasizing the significance of tokenization strategies in developing effective pretrained models.In addition, we discuss how these models facilitate advancements in drug discovery, particularly through approaches that enable target-aware ligand generation.</p>
<p>Chemical Language Models for Molecular Generation</p>
<p>Recent advances in chemical Language Models (cLMs) have significantly impacted molecular generation, leveraging deep learning to propose novel compounds with targeted properties.ChemFM [CHZ + 24], a large-scale foundation model built on the Tiny LLaMA architecture, employs self-supervised causal language modeling on a dataset of 178 million molecules, capturing complex molecular representations without task-specific architectures.With up to 3 billion parameters, ChemFM captures complex molecular representations and generalizes across multiple chemical tasks without relying on task-specific architectures.MolGPT [BAVP22] adopts a GPT-inspired transformer-decoder framework utilizing masked self-attention on SMILES representations, emphasizing molecular generation tasks.Beyond ChemFM and MolGPT, various chemical LLMs have adopted different tokenization strategies and pre-training methodologies to enhance molecular design.SMILES-BERT [WGW + 19] and Chem-BERTa [CGR20] employ masked language modeling (MLM) to refine molecular property prediction, while models such as Chemformer [IDHB22] integrate MLM paired with an autoregressive token generation (ATG tokenization) strategy to improve both property prediction and molecule generation.MolXPT [LZX + 23] expands the capabilities of chemical LLMs by incorporating textual descriptions of molecules, excelling in molecule captioning and molecular property prediction.DrugGPT [LGS + 23] introduces an autoregressive model based on GPT for the discovery of protein specific ligands.</p>
<p>Chemical Language Models (cLMs) employ distinct pretraining strategies depending on their primary objective-molecular property prediction or molecule generation.The two dominant approaches are Masked Language Modeling (MLM) and Autoregressive Token Generation (ATG), each optimized for different tasks.MLM, inspired by BERT-based models, trains by masking random tokens in molecular sequences and predicting them based on surrounding context.This bidirectional training approach enhances deep molecular understanding and improves property prediction.Notable examples include SMILES-BERT [WGW + 19] and ChemBERTa [CGR20], both designed for molecular property prediction, while MG-BERT [ZWY + 21] extends this method by integrating graph adjacency knowledge to refine molecular representations.ATG, on the other hand, generates molecular sequences token-bytoken, predicting each token based on its predecessors.This method is particularly effective for de novo molecular design and reaction prediction.ChemGPT [FSA + 23] specializes in molecular sequence completion using ATG, while MolXPT [LZX + 23] enhances generative tasks by combining SMILES representations with textual descriptions, excelling in molecular captioning and property prediction.Meanwhile, DrugGPT [LGS + 23], built on GPT-2, focuses on controlled generation of novel drug-like molecules.Overall, MLM-based models excel in molecular property prediction, while ATG-based models are more effective for generative tasks [LYMW24].Chemformer [IDHB22], leveraging a fusion of both strategies, stands out as a powerful model for both molecule generation and molecular property prediction.</p>
<p>These models mark a paradigm shift from traditional rule-based approaches to scalable, datadriven strategies, enabling more efficient molecular generation, optimization, and property prediction.By integrating diverse tokenization techniques, pretraining objectives, and model architectures, AIdriven chemical design continues to evolve, pushing the boundaries of computational drug discovery and accelerating the identification of novel molecules with optimized properties.</p>
<p>Chemical Language and Tokenization</p>
<p>The Simplified Molecular Input Line Entry System (SMILES) [Wei88] provides a structured method for representing molecular structures as linear strings, making it well-suited for processing by large language models (LLMs).However, the effectiveness of these models depends heavily on how molecular structures are tokenized-a critical factor influencing both learning efficiency and predictive performance.Several tokenization strategies have been explored, each impacting model performance in distinct ways.In general, SMILES tokenization can be categorized into three main approaches.Character-level tokenization treats each character in a SMILES string as an independent token.While simple and easy to implement, it introduces challenges such as the incorrect splitting of multi-character atomic symbols-for instance, Br (bromine) being mistakenly split into B and r.Atom-level tokenization enhances interpretability and chemical consistency by ensuring that each token corresponds to a meaningful chemical entity, allowing models to better capture molecular substructures.This approach has been adopted by ChemBERTa [CGR20] and Chemformer [IDHB22], while ChemFM models [CHZ + 24] employ a similar strategy but with a predetermined vocabulary of 266 tokens, compared to the 591-token vocabulary used in SmilesTokenizer.1 Motif-level tokenization segments molecules into commonly occurring chemical substructures rather than individual atoms.This method can be guided by expert-defined rules or data-driven techniques such as byte pair encoding (BPE).Models like ChemBERTa-2 [ASC + 22] integrate motif-level tokenization with molecular property prediction (MPP) tasks, improving predictive accuracy.Other models such as X-Mol [XZC + 22] and BARTSmiles [CTT + 22] leverage motif-based tokenization to create more structured molecular representations.Chemformer [IDHB22], which integrates both masked language modeling (MLM) and autoregressive token generation (ATG), benefits from this approach, achieving strong results in both molecular property prediction and molecule generation.The impact of tokenization strategies is evident in downstream tasks, where models that utilize more structured and chemically meaningful tokenization methods tend to exhibit better generalization and efficiency.For instance, Chemformer [IDHB22] outperforms baseline models in molecular property prediction and generative tasks, while BARTSmiles [CTT + 22] excels in reaction prediction and molecular design.</p>
<p>Target-aware Ligand Generation</p>
<p>A major breakthrough in chemical generative models is their ability to design molecules tailored to specific protein targets.The development of new pharmaceuticals-from target identification to regulatory approval-is an extensive and costly process, typically spanning 12 years and costing approximately $2.6 billion [DMTC22].To accelerate this pipeline, machine learning (ML) and deep learning (DL) have increasingly been employed to enhance computational drug discovery.While many computational methods generate bioactive molecules by leveraging known protein-ligand binding sites, their efficacy diminishes when binding sites are undefined or ambiguous.Binding site identification remains an open challenge, limiting the effectiveness of structure-informed approaches that rely solely on predefined binding pockets [VBG + 01].A recent advancement addressing this limitation is TargetVAE, a variational autoencoder introduced by Ngo and Hy [NH24], which employs multimodal protein representations-including amino acid sequences, three-dimensional structures, and residue-level graphs-to generate ligands without explicitly predefined binding sites.In contrast, our approach introduces a fully sequence-driven multimodal framework, integrating our protein Language Model (pLM) with our chemical Language Model (cLM).This enables the direct generation of ligands from protein sequences alone, eliminating the need for explicit structural or residue-level features.By leveraging protein sequence embeddings to guide chemical generation, our method streamlines the computational pipeline while maintaining strong target specificity.This approach significantly expands the scope of targetaware ligand generation, making it possible to design bioactive molecules even when binding sites are unknown or poorly defined.</p>
<p>Methodology</p>
<p>We introduce a novel approach leveraging our family of chemical Language Models (cLMs), specifically designed for drug-like and target-aware ligand generation.Unlike conventional molecular representations, our approach explicitly captures atomic-level interactions, providing a chemically meaningful and information-rich representation of molecules.By embedding molecules within a language that inherently models atomic interactions, we establish a precise and generative framework for designing compounds with desired pharmacological properties.Our contributions in this paper are threefold: (i) We develop a novel chemical language that encodes atomic interactions, enhancing both the representation and generation of drug-like molecules; (ii) We introduce a multi-omics modality integration framework by combining our chemical language model with a protein Language Model (Prot42 [STN + 25]), enabling target-aware ligand generation aligned with structural and functional properties of the target; and (iii) We conduct a comprehensive evaluation of our approach in terms of chemical validity, binding affinity prediction, and target specificity.By bridging chemical and protein Language Models, we advance toward a rational and data-driven drug design paradigm, unlocking new possibilities for highly specific therapeutic development.Our chemical language models (cLMs) were pre-trained on the extensive UniChem dataset [CDG + 13] using a decoder-only, autoregressive LLaMA-inspired architecture [TMS + 23].To determine the optimal model size and hyperparameters, we employed maximal update parametrization (µP) [mup-mutransfer], leveraging a 38M-parameter proxy model.After fine-tuning hyperparameters through this proxy, we conducted full-scale pre-training, ensuring both scalability and efficiency.Evaluations demonstrated that a target tokens-per-parameter (TPP) ratio of 50 provided the optimal balance, achieving the lowest validation loss while maintaining computational efficiency.This approach optimizes performance while reducing computational overhead, setting a new benchmark for efficient and scalable chemical modeling.</p>
<p>Pretraining Data and Tokenization</p>
<p>The data preprocessing pipeline begins with the UniChem database, which contains small molecules as well as larger biomolecular entities, including nucleotides, carbohydrates, lipids, peptides, and chemically modified macromolecules.To standardize molecular representations, we extract molecular structures and convert them into their canonical SMILES format [Wei88].Following previous methodologies [CHZ + 24], SMILES enumeration is applied to enhance data diversity by generating multiple random variations of each molecule.This process yields four distinct dataset partitions, allowing us to train models of different sizes and evaluate their performance across various dataset configurations.For tokenization, we adopt an atom-level tokenization strategy, similar to that used in ChemFM [CHZ + 24], featuring a vocabulary of 268 tokens.This vocabulary includes periodic table elements, digits, SMILESspecific special characters, and start/end sequence markers.The dataset is tokenized with a context length of 512 tokens, ensuring effective representation of molecular sequences.The dataset statistics are presented in Table 1.  3. After identifying the optimal hyperparameter configuration, we apply scaling laws to determine the optimal token-perparameter (TPP) ratio for pre-training.We define five distinct compute budgets, ranging from 5×10 18 to 4.59 × 10 19 , and systematically vary the model size and TPP for each budget.Subsequently, we pretrain these models and evaluate validation accuracy to identify the optimal TPP.For each training run, we utilize µTransfer to configure hyperparameters, employing a learning rate schedule that includes a linear warm-up followed by a 10× cosine decay.The results of our scaling law experiments are depicted in Figure 2. The scaling laws suggest that a TPP of 50 is optimal for our pre-training process.We also present models and hyperparameters used in scaling law experiments in Table 12.</p>
<p>We utilize Cerebras CS-2 for our training runs.The Cerebras CS-2 system is an AI accelerator that features 850,000 AI optimized compute cores, 40GB of on-chip SRAM, 20 PB/s memory bandwidth, and 220 PB/s interconnect [CS21].We present our production run model architecture and configuration in Table 2.</p>
<p>Baseline Family of Models</p>
<p>To further compare Chem42 on downstream prediction tasks and generation tasks, we built a baseline family of models pretrained on the PubChem dataset (77M compounds) and using ChemBERTa-2 tokenization [ASC + 22] which consists of a vocab size of 591 tokens (larger compared to the vocab size used in Chem42 [TRKK24] (268 tokens)).Compared to the UniChem dataset described in   , where the query, key, and value projections are defined as
Q = θ q H l , K = θ k E ′ p , V = θ v E ′ p , with learnable weight matrices θ q , θ k , θ v ∈ R 1280×1280 .
The resulting protein-conditioned ligand representations are given by C l = AV , which are combined with the original ligand embeddings as H ′ l = H l + C l .These transformed representations H ′ l are then passed through the CFM decoder layers.The generation of ligands follows an autoregressive process in which each token lt is predicted based on previously generated tokens and protein-informed features.The probability distribution over the ligand vocabulary V l is given by y t = Softmax(θ h h ′ t + θ c C t ), where θ h , θ c are learnable parameters, and the next ligand token is selected as lt = arg max j (y j t ).This process continues until a termination token is reached.The generated ligand sequence Ŝl = ( l1 , l2 , ..., ln ) is then ranked using a scoring function R( Ŝl ), which evaluates molecular validity, drug-likeness (QED) and synthetic accessibility (SA).Thus, the entire protein-ligand generation process can be represented as: Ŝl = M l (E ′ p , H l ), where the parameters of this model are θ = {θ p , θ q , θ k , θ v , θ h , θ c }.During inference, the protein embeddings E p are passed to M l , where the generation of the ligand sequence begins with a fixed token, "C" (see Figure 4).This token serves as a controlled starting point for generating SMILES string.The "C" (Carbon) token represents a generic initialization that anchors the generation process, ensuring that the ligand sequence starts from a valid structure.By fixing the "C" token, the model can focus on generating the rest of the ligand sequence while keeping the protein context in mind.</p>
<p>Experimental Results</p>
<p>Molecular property prediction</p>
<p>To assess the efficacy of our Chem42 models in downstream tasks, we evaluated them on two widely used benchmarks, Moleculenet[WRF + 18] and Admet[HFG + 21].We adopted the same experimental settings and hyperparameters as specified in the ChemFM GitHub repository2 , ensuring consistency across all tasks.This includes using the same random seed to maintain reproducibility.Tables 4 and 5 present the results, comparing the performance of Chem42 against ChemFM.Our models outperform ChemFM on the majority of benchmarks while maintaining significantly fewer parameters, demonstrating the efficiency and effectiveness of our approach.</p>
<p>Evaluation on MoleculeNet Benchmark</p>
<p>The Moleculenet[WRF + 18] benchmark comprises 12 different datasets with varying sample sizes across 5 different molecular properties and could be either a classification or regression prediction task.Table 4 shows the comparison between ChemFM and our Chem42 models.Our Chem42 models outperform the ChemFM models on 10 of 12 tasks while having significantly fewer model parameters compared to ChemFM.On the BACE task, our best model has an ROC-AUC that is just 0.002 lower than ChemFM.</p>
<p>Evaluation on ADMET Benchmark</p>
<p>The ADMET[HFG + 21] benchmark comprises of 22 datasets of varying sizes across 5 molecular properties.Similar to Moleculenet, this benchmark has tasks split across classification and regression.Our Chem42 models outperform ChemFM on 18 of 22 tasks (table 5) while having significantly fewer model parameters compared to ChemFM.</p>
<p>Table 11 in the Appendix compares Chem42 performances to the Baseline family of models described in Section3.3 on ADMET benchmark.</p>
<p>Unconditional molecular generation</p>
<p>We model unconditional generation as a sequence completion problem.Provided an input sequence S in , we ask the model to complete the sequence by predicting the next token until the end of sequence token [EOS] is encountered.We generate multiple such smiles strings and evaluate the generated strings using 2 metrics: Validity and Uniqueness.We define Validity score as follows, V alidity = |S valid |/|S gen |, where S valid is the set of all valid smiles strings.Validity of a smiles string is found via the RDKit library [BHF + 20].S gen is the complete set of all generated smiles strings.We define Uniqueness score as follows, U niqueness = |S unique |/|S gen |, where S unique is the set of unique smiles strings.We conducted two experiments for unconditional prediction: (1) predicting the second half of the Ozempic molecule and (2) generating a new SMILES string starting from a single carbon atom.These tasks evaluate how the model's performance varies under different generation constraints.In  Table 6 compares the unconditional generation capabilities of our Chem42 models with ChemFM.Starting prediction from a single carbon atom, all models have a comparable performance thereby not providing us with any valuable insights into the generation capabilities of the models.Predicting the 2nd half of Ozempic is a more challenging task, and our models achieve a higher uniqueness score and comparable validity score while being significantly smaller than ChemFM models.</p>
<p>Evaluation on conditional molecule generation</p>
<p>Conditional molecule generation plays a crucial role in designing molecules that satisfy specific property criteria or incorporate predefined scaffold structures.To address this, we fine-tuned one of our best performing Chem42 model -597M on the GuacaMol dataset for property-based generation.The model was trained to optimize four continuous molecular properties: octanol-water partition coefficient (logP), synthetic accessibility score (SAS), topological polar surface area (TPSA), and quantitative estimate of drug-likeness (QED).For property-based generation, we evaluated the model trained on the GuacaMol dataset by generating 10,000 molecules per property combination and assessing validity, uniqueness, novelty, and mean absolute deviation (MAD) between the conditioned and computed properties.Our model demonstrated superior performance across all properties, achieving on average improvements of 0.017 in validity compared to MolGPT and 0.009 compared to ChemFM (Table 7).Our model had a performance comparable or slightly worse than ChemFM on the other performance metrics while having a model size approximately 6 times smaller than ChemFM.Notably, we achieved the highest validity scores in all conditions, ranging from 0.981 to 0.996, surpassing both ChemFM-3B and MolGPT.For single-property generation, our Chem42 model outperformed its counterparts in logP (0.989), TPSA (0.989), SAS (0.996), and QED (0.993) in terms of validity, while also achieving the lowest mean absolute deviation (MAD) in SAS (0.107) and QED (0.043) (Table 7).In multiproperty settings, our Chem42 maintained competitive performance, achieving the highest validity in all combinations, with values ranging from 0.981 to 0.988.We also recorded the lowest Mean Average Deviation (MAD) in SAS for all multi-property conditions, with a minimum of 0.111 (Table 7).</p>
<p>Evaluation on Reaction Prediction</p>
<p>We fine-tuned our models for both reaction synthesis and retro-synthesis using the USPTO-FULL [Low17], USPTO-MIT [JCBJ17], and USPTO-50K [DLC + 20] datasets.These datasets consist of organic chemical reactions extracted through text mining from United States patents.To improve model robustness and generalization, we employed SMILES enumeration as a data augmentation strategy.Finally, we benchmarked our models against existing approaches and present the comparative results in Table 8.Empirical evaluation demonstrates that our Chem42 models achieve performance comparable to the state-of-the-art ChemFM 3B model, despite having six times fewer parameters.Consequently reduced model size enhances deployment efficiency for downstream applications.Furthermore, our findings validates our scaling law experiments, indicating that a token-to-parameter ratio of 50 TPP represents an optimal balance between model capacity and efficiency.</p>
<p>Target-aware Ligand Generation</p>
<p>This task focuses on generating drug-like molecules that can bind to target proteins with unknown binding sites.We used the PDBBind 2020 training set3 , which contains more than 16,000 unique protein-ligand pairs.For docking simulations and computing the binding affinity, we used AutoDock-Vina [TO10].Our approach is evaluated on eight target proteins; • 1IEP4 (the kinase domain of c-Abl ) : The kinase domain of c-Abl is a catalytic domain responsible for its tyrosine kinase activity.It consists of an N-terminal lobe primarily composed of β-sheets and a C-terminal lobe dominated by α-helices, connected by a flexible hinge region.The ATPbinding site is located in the cleft between these lobes, facilitating phosphate transfer to substrate proteins.Regulation of c-Abl activity involves autoinhibitory interactions and phosphorylation events that modulate its conformational state, influencing its role in signal transduction, cell proliferation, and cytoskeletal dynamics.</p>
<p>• 2RGP5 (EGFR (Epidermal Growth Factor Receptor)): The kinase domain of ErbB-2 (HER2) and EGFR is a tyrosine kinase domain that plays a central role in receptor dimerization and signal transduction.It consists of an N-lobe and C-lobe, with an ATP-binding cleft and an activation loop that regulates catalytic activity.In EGFR (ErbB-1), ligand binding induces dimerization, leading to autophosphorylation and activation of downstream signaling pathways like MAPK and PI3K-Akt, which drive cell proliferation and survival.HER2 (ErbB-2), though ligand-independent, is a potent signaling receptor due to its ability to form heterodimers with other ErbB family members.Mutations or overexpression of these kinases are associated with various cancers, making them key targets for cancer therapies like tyrosine kinase inhibitors (TKIs) and monoclonal antibodies.</p>
<p>• 3EML6 (Human A2A Adenosine Receptor) : The human A 2A adenosine receptor (A 2A AR) is a G protein-coupled receptor (GPCR) that belongs to the adenosine receptor family.It is primarily expressed in the central nervous system, cardiovascular system, and immune cells.A 2A AR plays a key role in modulating neurotransmission, vasodilation, and immune response by binding adenosine and activating intracellular signaling pathways via G s proteins, leading to increased cyclic AMP (cAMP) production.Due to its involvement in neurological disorders, inflammation, and cardiovascular regulation, A 2A AR is a significant therapeutic target in conditions such as Parkinson's disease and ischemia.</p>
<p>• 3NY8 7 (the human β 2 -adrenergic receptor): The human β 2 -adrenergic receptor (β 2 AR) is a G protein-coupled receptor (GPCR) that plays a key role in mediating the effects of catecholamines, particularly epinephrine and norepinephrine.Unlike tyrosine kinases, β 2 AR lacks a kinase domain and instead contains a seven-transmembrane domain characteristic of GPCRs.Upon ligand binding, β 2 AR primarily couples to G s proteins, leading to the activation of adenylyl cyclase, increased intracellular cyclic AMP (cAMP) levels, and subsequent activation of protein kinase A (PKA).This signaling pathway regulates smooth muscle relaxation, bronchodilation, and cardiac function.β 2 AR is a major pharmacological target for bronchodilators in asthma and chronic obstructive pulmonary disease (COPD), as well as for cardiovascular therapeutics.</p>
<p>• 4RLU8 ((3R)-hydroxyacyl-ACP dehydratase HadAB hetero-dimer from Mycobacterium tuberculosis): The (3R)-hydroxyacyl-ACP dehydratase HadAB is a heterodimeric enzyme involved in the fatty acid elongation cycle in Mycobacterium tuberculosis.It consists of the HadA and HadB subunits, which function cooperatively to catalyze the dehydration of (3R)-hydroxyacyl-acyl carrier protein (ACP) intermediates during the biosynthesis of mycolic acids, essential components of the mycobacterial cell wall.Structural and functional analyses suggest that HadA provides catalytic activity, while HadB stabilizes the enzyme-substrate complex, ensuring efficient mycolate biosynthesis and bacterial survival.• 5MO410 (ABL1 kinase (T334I D382N)): The ABL1 kinase is a non-receptor tyrosine kinase involved in cell signaling, proliferation, and cytoskeletal dynamics.The T334I D382N double mutation affects the kinase domain, potentially altering its structural conformation and enzymatic activity.Threonine 334 to isoleucine (T334I) substitution may influence ATP binding or substrate interactions, while aspartate 382 to asparagine (D382N) could impact the regulatory mechanisms of the kinase.These mutations may play a role in resistance to tyrosine kinase inhibitors (TKIs) and altered oncogenic potential in diseases such as chronic myeloid leukemia (CML) and acute lymphoblastic leukemia (ALL).</p>
<p>• 7L1111 (Main Protease of SARS-CoV-2): The Main Protease (Mpro) of SARS-CoV-2, also known as 3C-like protease (3CLpro), is a key enzyme in the viral replication cycle.It plays a crucial role in processing the polyproteins translated from the viral RNA by cleaving them at specific sites to generate functional proteins essential for virus maturation.Due to its indispensable role in viral replication and its absence in humans, Mpro is a prime target for antiviral drug development against COVID-19.</p>
<p>These targets were not part of the model's training data.For each target protein, we generate 500 SMILES sequences, evaluating their drug-likeness (QED), synthetic accessibility (SA), both computed using RDKit [BHF + 20].From these, we select the best molecule based on the properties mentioned, perform docking simulations with AutoDock Vina [TO10], and finally get the binding affinities.Table 9 presents a comparative analysis of our multi-omics modality protein-ligand generation model against PMN [NH24] across multiple target proteins.While binding affinities are generally comparable between the two models-with each showing strengths on different targets-our models consistently achieve higher QED and lower SA scores, demonstrating improved molecular quality and ease of synthesis-key factors for successful downstream drug development.Figure 5 visualizes some of our generated ligands.</p>
<p>Conclusion</p>
<p>In this work, we introduced Chem42, a new family of chemical Language Models (CLMs) designed for target-aware ligand generation by integrating atomic interaction modeling with protein Language Models (Prot42).Our approach addresses a critical limitation of existing molecular generative models by incorporating target-specific information through a multimodal framework, enabling the generation of ligands that are both chemically valid and biologically relevant.Through extensive evaluation, we demonstrated that Chem42 outperforms existing CLMs in terms of molecular validity, target specificity, and is comparable in predicting binding affinity.The ability of our model to capture atomic-level interactions enhances the precision of ligand design, while integration with protein embeddings ensures context-aware molecular generation, even in cases where binding sites are not well defined.Our experimental results validate the effectiveness of this approach, showing improved drug-likeness (QED), synthetic accessibility (SA), and on par with binding affinity (BA) in comparison to baseline methods.</p>
<p>The broader impact of this work extends beyond ligand generation.By bridging chemical and protein representations, we have paved the way for more accurate and efficient AI-driven drug discovery pipelines.Future directions include further scaling of the model, optimizing protein-ligand docking accuracy, and extending multimodal learning to incorporate structural and biochemical constraints.By integrating chemistry-aware and biology-aware representations, our work advances toward a datadriven, rational approach to drug design, ultimately accelerating the discovery of novel therapeutics with high specificity and efficacy.</p>
<p>Figure 1 :
1
Figure 1: Example of a Chem42-generated ligand designed to bind the Y220C mutant p53 protein (PDB ID: 8a32), shown on the right (panel (c)).For comparison, panel (a) presents the binding pose of the recently proposed small-molecule stabilizer JC769 as estimated using DiffDock-L [CDF + 24], while panel (b) displays the stabilizer JC769 as originally reported in the literature [SCDD + 22].The protein target context is captured using our protein Language Model (Prot42 [STN + 25]) and integrated into the chemical Language Model (Chem42) to guide ligand generation.Calculated metrics-including Drug-likeness, Synthetic Accessibility, and Binding Affinity-demonstrate the effectiveness of Chem42 in achieving state-of-the-art molecular design results.</p>
<p>Figure 2 :
2
Figure 2: Scaling Law Experiments.We plot validation loss against token per parameter TPP.Colors indicate the compute budget measured in FLOPs, the needed floating-point operations.</p>
<p>Figure 4 :
4
Figure 4: Overview of the our protein-ligand model architecture.(Left) Our multi-omics pipeline integrates protein embeddings into the chemical model via cross-attention before passing through the decoder layers.The embeddings are first projected into the model's hidden space.(Right) Crossattention enables the model to compute interactions between chemical tokens and protein embeddings.After projection, protein embeddings serve as key-value pairs, while token embeddings act as queries.The attended representations are then combined with token inputs and processed by the decoder.</p>
<p>Figure 5 :
5
Figure 5: Examples of generated Ligands for protein targets 2rgp, 5mo4 4rlu and 4unn.Docking results with metrics (QED, SA, BA and Confidence score on pose estimation output of DiffDock-L [CDF + 24]).</p>
<p>Table 1 :
1
Pretraining dataset specifications used to train Chem42 models.3.2ModelArchitecture, Scaling-laws and PretrainingChem42 is an autoregressive transformer decoder model based on the LLaMA architecture [TMS + 23].To optimize hyperparameter selection for training, we employ maximal update parametrization (µP) [YHB + 22].Our hyperparameter tuning process is conducted using a smaller 38M-parameter proxy model.The results of our hyperparameter sweep are presented in Figure
Dataset NameVal SizeTrain Size# Nonpad Training TokensUniChem-Canonical7.65M188.4M9.5BUniChem-Canonical+Random115.3M376.9M19.5BUniChem-Canonical+Random315.3M565.4M29.5BUniChem-Canonical+Random515.3M942.3M49.5B</p>
<p>Figure3: µP Hyperparameter Optimization.We perform a hyperparameter sweep consisting of 200 independent runs to optimize the standard deviation of parameter initializers, the embedding multiplier, the output layer logit multiplier, and the peak learning rate.The optimal hyperparameter values identified through this process are 0.1438, 3.88, 2.2, and 2.98 × 10 −3 , respectively.
0.52500.52500.52500.5250eval loss0.5150 0.5175 0.5200 0.52250.5150 0.5175 0.5200 0.52250.5150 0.5175 0.5200 0.52250.5150 0.5175 0.5200 0.52250.51250.51250.51250.51250.51000.10 0.15 0.20 0.25 init_std0.510024 emd_mult 6 8 100.51002 output_scaler 46lr 0.001 0.002 0.003 0.004 0.005 0.5100ModelChem42-190MChem42-387MChem42-597MChem42-1BBatch size512512512512Base Frequency10k10k10k10kHidden size768108814081792# of hidden layers24242424# of attention heads16161632# KV heads4448Transformer FFN Dim.2816403244806272OptimizerAdamWAdamWAdamWAdamWBetas 0.9, 0.950.9, 0.950.9, 0.950.9, 0.95Eps1e-81e-81e-81e-8Weight Decay0.10.10.10.1Max grad norm1111Learning rate (Linear)0 to 5.9e-40 to 2.9e-40 to 1.9e-40 to 1.1e-4Iterations (Linear)0 to 18000 to 37000 to 56900 to 9650Learning rate (Cosine)5.9e-4 to 5.9e-52.9e-4 to 2.9e-51.9e-4 to 1.9e-51.1e-4 to 1.1e-5Iterations (Cosine)1800 to 362503700 to 742505690 to 1138009650 to 193000</p>
<p>Table 2 :
2
Hyperparameters used for pretraining of the Chem42 models.
VersionModelDatasetSmiles enumerationNum sequencesTokenizerVocabulary SizeTPP140MPubchemCanonical115.6MMotif Level59150Baseline507MPubchemCanonical115.6MMotif Level59150184MUnichemCanonical188.4MMotif Level59150190MUnichemCanonical188.4MAtom Level26850Chem42387M 597MUnichem UnichemCanonical+1xRandom Canonical+2xRandom376.9M 565.4MAtom Level Atom Level268 26850 501002MUnichemCanonical+4xRandom942.3MAtom Level26850
Section.3.1,PubChem consists of three primary databases: Substance, Compound, and BioAssay which makes it richer in biological activities of small molecules.We will report in Section.4 and the Appendix quantitative results of Chem42 models and Baseline models.</p>
<p>Table 3 :
3
Baseline vs Chem42 models.Given a dataset D of protein-ligand pairs, our objective is to generate ligand sequences Ŝl that exhibit high affinity toward a target protein.Let (S p , S l ) ∈ D represent a protein-ligand pair, where S p = (s 1 , s 2 , ..., s m ) is a sequence of m amino acid tokens, and S l = (l 1 , l 2 , ..., l n ) is a sequence of n ligand tokens, each belonging to a predefined vocabulary V l .The protein model (Prot42 [STN + 25] M 2048×1280 .These transformed protein embed-dings are incorporated into ligand representations using a cross-attention mechanism.Attention scores are computed as A = Softmax QK ⊤
3.4 Protein-binding Ligand Generation
p encodes S p into a sequence of hidden representations E p = (e 1 , e 2 , ..., e m ), where each e i ∈ R 2048 captures structural and functional properties of the protein.Similarly, the ligand model (Chem42) M l encodes S l into a sequence of latent embeddings H l = (h 1 , h 2 , ..., h n ), where each h t ∈ R 1280 represents the hidden state of the chemical model.To integrate protein context into ligand generation, protein embeddings are projected into the hidden dimension of the ligand model using a learnable transformation E ′ p = θ p E p , where θ p ∈ R √ 1280</p>
<p>Table 4 :
4
Experimental Results on Moleculenet [WRF
CategoryDatasetMetricChemFM [CHZ + 24] 1B 3B190MChem42 (Ours) 387M 597M1BPharmacokineticBBBPROC-AUC ↑0.7450.7510.7700.7750.7630.762BACEROC-AUC ↑0.8570.8690.8480.8510.8530.867BioactivityHIV MUVROC-AUC ↑ ROC-AUC ↑0.785 0.8120.807 -0.774 0.8240.789 0.8500.774 0.8530.732 0.712PCBAPRC-AUC ↑0.7850.8070.892--0.886TOX21ROC-AUC ↑0.8630.8690.8360.8730.8620.870ToxicitySIDERROC-AUC ↑0.7020.7090.6930.6980.7160.578CLINTOXROC-AUC ↑0.8990.9180.9760.9620.9390.971ESOLRMSE ↓0.5290.5160.4800.4680.5110.470PhysicochemicalFREESOLV LIPORMSE ↓ RMSE ↓0.906 0.5470.830 0.5450.638 0.5680.582 0.5060.673 0.5191.023 0.578Molecular BindingPDBBind FullRMSE ↓0.7000.6970.7430.7040.6970.674
+ 18] benchmark datasets for molecular property prediction following the experimental settings of [CHZ + 24].Bold: best performance; Green indicates best performance within the Chem42 models.</p>
<p>Table 10
10
in the Appendix compares Chem42 performances to the Baseline family of models described in Section3.3 on MoleculeNet benchmark.</p>
<p>Table 5 :
5
Experimental Results on ADMET [HFG + 21] benchmark for property prediction following the experimental setting of [CHZ + 24].Bold: best performance; Green indicates best performance within the Chem42 models.
Input MoleculeTokens to generateGenerated mol.ModelUniqueness ↑Validity ↑3261000ChemFM 3B98.6079.303261000Baseline 507M91.8075.70Ozempic3261000Chem42 387M97.3089.303261000Chem42 597M98.9066.103261000Chem42 1B94.4091.60&lt;= 5121000ChemFM 3B99.80100&lt;= 5121000Baseline 507M99.70100Single Carbon Atom 'C'&lt;= 512 &lt;= 5121000 1000Chem42 387M Chem42 597M99.60 99.70100 100&lt;= 5121000Chem42 1B99.60100
the second task, where generation starts with a single carbon atom (C) and continues until an end-ofsequence ([EOS]) token is reached, the model operates without restrictions, increasing the likelihood of generating valid SMILES string.Conversely, in the first task, predicting the second half of Ozempic imposes a length constraint, making it more challenging to generate valid SMILES.Using a temperature of 1, we generate 1,000 SMILES strings for each task and evaluate their validity and uniqueness.In the first task, the initial half of the Ozempic SMILES string (comprising 326 tokens out of 653) is provided as input, and the model predicts the remaining tokens until an [EOS] token appears.In the second task, we begin with C as input and generate tokens up to a maximum sequence length of 512.In both cases, generation stops if the [EOS] token is predicted or the model reaches its maximum sequence length.</p>
<p>Table 6 :
6
Unconditional generation results.All numbers presented are percentage values.Bold: best performance; Green indicates best performance within the Chem42 models.</p>
<p>Table 7 :
7
[BFSV19]ntal Results on GuacaMol dataset[BFSV19]for conditional generation following the experimental settings of [CHZ + 24].Bold: best performance
PropertyModelValidity ↑Uniqueness ↑Novelty ↑MAD ↓MolGPT0.9710.9980.9770.230logPChemFM -3B0.9811.0000.9850.182Chem42 -597M0.9890.9980.9240.202MolGPT0.9710.9970.9753.562TPSAChemFM -3B0.9790.9990.9842.466Chem42 -597M0.9890.9970.9183.409MolGPT0.9780.9960.9660.133SASChemFM -3B0.9860.9990.9710.126Chem42 -597M0.9960.9950.8970.107MolGPT0.9740.9970.9680.056QEDChemFM -3B0.9821.0000.9800.045Chem42 -597M0.9930.9990.8990.043MolGPT0.9720.9910.9830.147/ 0.253SAS + logPChemFM -3B0.9800.9950.9850.137/ 0.195Chem42 -597M0.9870.9830.9340.111 / 0.228MolGPT0.9710.9880.9840.155/ 3.785SAS + TPSAChemFM -3B0.9800.9910.9850.138/ 2.659Chem42 -597M0.9880.9750.9360.111 / 3.597MolGPT0.9640.9940.9893.715/ 0.243TPSA + logPChemFM -3B0.9730.9970.9922.415/ 0.184Chem42 -597M0.9840.9900.9573.292 / 0.211MolGPT0.9720.9690.9883.797/ 0.268/ 0.180TPSA + logP + SASChemFM -3B0.9750.9710.9892.289/ 0.191/ 0.166Chem42 -597M0.9810.9160.9653.015/ 0.212/ 0.131</p>
<p>Table 8 :
8
Experimental results on the USPTO benchmarks.Bold: best performance and underlined values represent the second best performance.
TaskDataset# Samples(Test/ Val/ Train)ModelTop-1 ↑Top-3 ↑Top-5 ↑ChemFM -3B90.595.796.6SynthesisUSPTO-MIT40,26530,182411,685Chem42 -597M AT90.3 90.495.7 -96.6 96.5R-SMILES90.095.696.4ChemFM-3B58.080.086.3Chem42 -597M58.779.583.6USPTO-50K5,0075,00140,003AT R-SMILES53.5 56.0-79.081.0 86.1RetroXpert50.461.162.3G 2 Retro54.174.181.2ChemFM-3B61.678.783.0Retro-synthesisUSPTO-MIT40,26530,182411,685Chem42-597M R-SMILES62.4 60.378.8 77.982.6 82.8RetroTRAE58.3--ChemFM-3B51.768.072.5Chem42-597M51.768.172.2USPTO-Full96,02396,071768,630AT46.2-73.3R-SMILES48.966.672.0RetroXpert49.463.667.6</p>
<p>•</p>
<p>4UNN 9 (M.tuberculosis thymidylate kinase (Mtb TMK)): Mycobacterium tuberculosis thymidylate kinase (Mtb TMK) is an essential enzyme in the nucleotide biosynthesis pathway, catalyzing the phosphorylation of dTMP to dTDP using ATP as a phosphate donor.It plays a crucial role in DNA replication and repair, making it a potential target for anti-tubercular drug development.Structurally, Mtb TMK consists of a conserved P-loop, which is critical for nucleotide binding, and a lid domain that undergoes conformational changes during catalysis.Its specificity and catalytic efficiency are regulated by interactions with substrates and cofactors.</p>
<p>Table 9 :
9
Performance comparison between PMN, Chem42 (507M), and Chem42 (597M) on eight protein targets.Higher QED and more negative BA are desirable; lower SA indicates easier synthesis.Bold indicates the best performance per metric per target.
TargetQED (↑)SA (↓)BA (↓) (kcal/mol)PMN Chem42 (507M) Chem42 (597M) PMN Chem42 (507M) Chem42 (597M)PMNChem42 (507M) Chem42 (597M)1iep0.3220.6130.3607.6094.1253.152-9.946-10.860-10.352rgp0.4280.5260.5623.3912.1952.334-11.936-10.690-10.103eml0.5840.6240.5347.2681.8953.271-25.939-10.590-10.403ny80.8070.3830.3975.9902.5833.250-11.257-12.070-10.704rlu0.4790.8540.8032.9791.7351.765-11.250-9.100-9.804unn0.1610.3330.4632.3232.3313.628-10.752-10.190-10.405mo40.4320.7820.4156.3302.3212.753-11.812-10.260-10.237l110.1360.5130.6457.9122.9782.540-11.220-7.580-8.43
https://deepchem.readthedocs.io/en/2.4.0/api_reference/tokenizers.html
https://github.com/TheLuoFengLab/ChemFM
https://huggingface.co/datasets/jglaser/binding_affinity
https://www.rcsb.org/structure/1IEP
https://www.rcsb.org/structure/2rgp
https://www.rcsb.org/structure/3EML
https://www.rcsb.org/structure/3ny8
https://www.rcsb.org/structure/4rlu
https://www.rcsb.org/structure/4UNN
https://www.rcsb.org/structure/5MO4
https://www.rcsb.org/structure/7L11
AcknowledgmentThe authors are thankful to the AutoDock Vina team[TO10], a state-of-the-art model for molecular docking.All ligand-protein docking results reported in this paper are obtained using this tool.All protein visualizations reported in this paper were obtained using the UCSF ChimeraX software.
Walid Ahmad, Elana Simon, ASC + 22Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta-2: Towards chemical foundation models. 2022</p>
<p>Molgpt: Molecular generation using a transformer-decoder model. Viraj Bagal, P K Aggarwal, U Deva Vinod, Priyakumar, Journal of Chemical Information and Modeling. 6292022</p>
<p>Guacamol: benchmarking models for de novo molecular design. Nathan Brown, Marco Fiscato, Marwin Hs Segler, Alain C Vaucher, Journal of chemical information and modeling. 5932019</p>
<p>An open source chemical structure curation pipeline using rdkit. A Patrícia Bento, Anne Hersey, Eloy Félix, Greg Landrum, Anna Gaulton, Francis Atkinson, Louisa J Bellis, Marleen De Veij, Andrew R Leach, Journal of Cheminformatics. 122020BHF + 20</p>
<p>Deep confident steps to new pockets: Strategies for docking generalization. Cdf + 24] Gabriele, Arthur Corso, Benjamin Deng, Nicholas Fry, Regina Polizzi, Tommi Barzilay, Jaakkola, ArXiv. 24022024</p>
<p>Unichem: A unified chemical structure cross-referencing and identifier tracking system. Jon Chambers, Mark Davies, Anna Gaulton, Anne Hersey, Sameer Velankar, Robert Petryszak, Janna Hastings, Louisa Bellis, Shaun Mcglinchey, John Overington, CDG + 13. 5</p>
<p>Chemberta: Large-scale self-supervised pretraining for molecular property prediction. Seyone Chithrananda, Gabriel Grand, Bharath Ramsundar, ArXiv, abs/2010.098852020</p>
<p>A foundation model for chemical design and property prediction. Feiyang Cai, Katelin Hanna, Tianyu Zhu, Tzuen-Rong Tzeng, Yongping Duan, Ling Liu, Srikanth Pilla, Gang Li, Feng Luo, arXiv:2410.214222024arXiv preprintCHZ + 24</p>
<p>Cerebras systems: Achieving industry best ai performance through a systems approach. Cerebras Inc, Systems, 2021</p>
<p>Hovhannes Gayane Chilingaryan, Ani Tamoyan, Nelly Tevosyan, Lusine Babayan, Karen Khondkaryan, Zaven Hambardzumyan, Navoyan, arXiv:2211.16349Hrant Khachatrian, and Armen Aghajanyan. Bartsmiles: Generative masked language models for molecular representations. 2022arXiv preprintCTT + 22</p>
<p>Retrosynthesis prediction with conditional graph logic network. Dlc + 20] Hanjun, Chengtao Dai, Connor W Li, Bo Coley, Le Dai, Song, CoRR, abs/2001.014082020</p>
<p>Artificial intelligence in the prediction of protein-ligand interactions: recent advances and future directions. Ashwin Dhakal, Cole Mckay, John J Tanner, Jianlin Cheng, Briefings in bioinformatics. 2314762022</p>
<p>Neural scaling of deep chemical models. Nathan C Frey, Ryan Soklaski, Simon Axelrod, Siddharth Samsi, Rafael Gómez-Bombarelli, Connor W Coley, Vijay Gadepally, FSA + 23. Nov 20235</p>
<p>Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, H Yusuf, Jure Roohani, Connor W Leskovec, Cao Coley, Jimeng Xiao, Marinka Sun, Zitnik, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021HFG + 21. Round 1</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. Ross Irwin, Spyridon Dimitriadis, Jiazhen He, Esben Jannik Bjerrum, Machine Learning: Science and Technology. 3115022jan 2022</p>
<p>Predicting organic reaction outcomes with weisfeiler-lehman network. Wengong Jin, Connor W Coley, Regina Barzilay, Tommi S Jaakkola, CoRR, abs/1709.045552017</p>
<p>Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins. Yuesen Li, Chengyi Gao, Xin Song, Xiangyu Wang, Yungang Xu, Suxia Han, LGS + 23. 2023</p>
<p>Chemical reactions from us patents. Daniel Lowe, 1976-sep2016), 2017</p>
<p>From words to molecules: A survey of large language models in chemistry. Chang Liao, Yemin Yu, Yu Mei, Ying Wei, 2024</p>
<p>MolXPT: Wrapping molecules with text for generative pre-training. Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang, Tie-Yan Liu, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20232Short Papers)</p>
<p>Multimodal protein representation learning and target-aware variational auto-encoders for protein-binding ligand generation. Khang Nhat, Truong Ngo, Hy Son, Machine Learning: Science and Technology. 52250212024</p>
<p>Discovery of nanomolar-affinity pharmacological chaperones stabilizing the oncogenic p53 mutant y220c. Leon R Joseph R Stephenson Clarke, Patrick J Douglas, Dimitrios-Ilias Duriez, Andreas C Balourdas, Raniya Joerger, Emil Khadiullina, Matthias Gj Bulatov, Baud, ACS pharmacology &amp; translational science. 5112022SCDD + 22</p>
<p>Computational approaches streamlining drug discovery. V Anastasiia, Vsevolod Sadybekov, Katritch, Nature. 61679582023</p>
<p>Prot42: a novel family of protein language models for target-aware protein binder generation. Engin Mohammad Amaan Sayeed, Maryam Tekin, Nancy A Nadeem, Aahan Elnaker, Natalia Singh, Boulbaba Vassilieva, Ben Amor, 2025</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Louis Tms + 23] Hugo Touvron, Kevin Martin, Peter Stone, Amjad Albert, Yasmine Almahairi, Nikolay Babaei, Soumya Bashlykov, Prajjwal Batra, Shruti Bhargava, Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Oleg Trott, Arthur J Olson, Journal of computational chemistry. 3122010</p>
<p>Large vocabulary size improves large language models. Sho Takase, Ryokan Ri, Shun Kiyono, Takuya Kato, arXiv:2406.165082024arXiv preprint</p>
<p>Binding energy landscapes of ligand-protein complexes and molecular docking: Principles, methods, and validation experiments. M Gennady, Djamal Verkhivker, Bouzida, Paul A Daniel K Gehlhaar, Sandra Rejto, Arthurs, Stephan T Anthony B Colson, Veda Freer, Brock A Larson, Tami Luty, Marrone, Combinatorial Library Design and Evaluation. CRC Press2001</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, Journal of Chemical Information and Computer Sciences. 2811988</p>
<p>Smiles-bert: Large scale unsupervised pre-training for molecular property prediction. Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, Junzhou Huang, Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics, BCB '19. the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics, BCB '19New York, NY, USAAssociation for Computing Machinery2019WGW + 19</p>
<p>Moleculenet: a benchmark for molecular machine learning. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, S Aneesh, Karl Pappu, Vijay Leswing, Pande, WRF + 18. 20189</p>
<p>X-mol: large-scale pre-training for molecular understanding and diverse molecular analysis. Dongyu Xue, Han Zhang, Xiaohan Chen, Dongling Xiao, Yukang Gong, Guohui Chuai, Yu Sun, Hua Hao Tian, Yukun Wu, Qi Li, Liu, XZC + 22Science Bulletin. 6792022</p>
<p>Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. Yhb + 22] Greg, Edward J Yang, Igor Hu, Szymon Babuschkin, David Sidor, Jakub Farhi, Xiaodong Pachocki, Weizhu Liu, Jianfeng Chen, Gao, 2021. March 2022</p>
<p>Mg-bert: leveraging unsupervised atomic representation learning for molecular property prediction. Xiao-Chen Zhang, Cheng-Kun Wu, Zhi-Jiang Yang, Zhen-Xing Wu, Jia-Cai Yi, Chang-Yu Hsieh, Ting-Jun Hou, Dong-Sheng Cao, ZWY + 21. 22</p>            </div>
        </div>

    </div>
</body>
</html>